<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005204A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005204</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17903621</doc-number><date>20220906</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20110101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20110101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>03</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20130101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>048</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>042</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0304</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>017</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>048</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0425</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2219</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">OBJECT CREATION USING BODY GESTURES</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17145517</doc-number><date>20210111</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11461950</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17903621</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16744715</doc-number><date>20200116</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10916047</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17145517</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15888572</doc-number><date>20180205</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10573049</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16744715</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>14770800</doc-number><date>20150826</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>9928634</doc-number></document-id></parent-grant-document><parent-pct-document><document-id><country>WO</country><doc-number>PCT/CN2013/072067</doc-number><date>20130301</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>15888572</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Microsoft Technology Licensing, LLC</orgname><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Cao</last-name><first-name>Xiang</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Liu</last-name><first-name>Yang</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Han</last-name><first-name>Teng</first-name><address><city>Pittsburg</city><state>PA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Shiratori</last-name><first-name>Takaaki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Umetani</last-name><first-name>Nobuyuki</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Zhang</last-name><first-name>Yupeng</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>Tong</last-name><first-name>Xin</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="07" designation="us-only"><addressbook><last-name>Ren</last-name><first-name>Zhimin</first-name><address><city>Durham</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Microsoft Technology Licensing, LLC</orgname><role>02</role><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An intuitive interface may allow users of a computing device (e.g., children, etc.) to create imaginary three dimensional (3D) objects of any shape using body gestures performed by the users as a primary or only input. A user may make motions while in front of an imaging device that senses movement of the user. The interface may allow first-person and/or third person interaction during creation of objects, which may map a body of a user to a body of an object presented by a display. In an example process, the user may start by scanning an arbitrary body gesture into an initial shape of an object. Next, the user may perform various gestures using his body, which may result in various edits to the object. After the object is completed, the object may be animated, possibly based on movements of the user.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="234.95mm" wi="152.82mm" file="US20230005204A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="255.27mm" wi="156.04mm" file="US20230005204A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="217.17mm" wi="134.79mm" file="US20230005204A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="250.44mm" wi="153.75mm" file="US20230005204A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="254.34mm" wi="152.65mm" file="US20230005204A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="220.30mm" wi="169.42mm" orientation="landscape" file="US20230005204A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="225.13mm" wi="169.33mm" orientation="landscape" file="US20230005204A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="219.63mm" wi="140.80mm" file="US20230005204A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED PATENT APPLICATIONS</heading><p id="p-0002" num="0001">This application is a Continuation application of U.S. patent application Ser. No. 17/145,517, filed Jan. 11, 2021, which is a Continuation application of U.S. patent application Ser. No. 16/744,715, filed Jan. 16, 2020, now U.S. Pat. No. 10,916,047 issued Feb. 9, 2021, which is a Continuation application of U.S. patent application Ser. No. 15/888,572, filed Feb. 5, 2018, now U.S. Pat. No. 10,573,049 issued Feb. 25, 2020, which is a Continuation application of U.S. patent application Ser. No. 14/770,800, filed Aug. 26, 2015, now U.S. Pat. No. 9,928,634 issued Mar. 27, 2018, which is a 371 National Phase of Patent Cooperation Treaty Application No. PCT/CN2013/072067, filed Mar. 1, 2013, each of which is herein incorporated by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Many video games feature avatars that are controllable by one or more players of the games. Some games allow a player to select an avatar that the player then controls during the game. Typically, avatars are selected from existing avatars that may or may not be customized. Customization often includes changing colors, basic attributes, or other features by selecting from various choices provided by the game. Thus, the creation or modification of avatars is often limited to a finite amount of possibilities that are dictated by the designer of the game.</p><p id="p-0004" num="0003">Three dimensional (3D) modeling is traditionally accomplished using sophisticated and expensive 3D computer graphics software that requires some formal training to operate and is not intuitive to the general population. Creating even simple objects, such as a simple avatar, using graphics software can be time consuming and require many steps.</p><p id="p-0005" num="0004">More recently, imaging software allows users to convert two dimensional (2D) images into 3D model. This may allow a user to select an object from the real world, obtain 2D images of the object, and then convert the images into a virtual 3D object. However, modification of the virtual 3D object still presents challenges and may be time consuming and require many steps.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">An intuitive interface may allow users of a computing device (e.g., children, etc.) to create imaginary three dimensional (3D) objects of any shape using body gestures performed by the users as a primary or only input. A user may make gestures (motions) while in front of an imaging device that senses movement of the user.</p><p id="p-0007" num="0006">The interface may employ a mapping perspective that is maintained between the user and the 3D object. The mapping may be a body reference mapping where the user is mapped to a similar location as the 3D object to allow the user to interact with the 3D object in a first-person editing perspective (as if the user were inside of the 3D object or were the 3D object). The mapping may be a spatial mapping where the user is mapped to a location outside of the 3D object to allow the user to interact with the 3D object in a third-person editing perspective (allowing the user to walk around the 3D object, etc.).</p><p id="p-0008" num="0007">In some embodiments, the user may start by scanning an arbitrary body gesture into an initial shape of a 3D object. Next, the user may perform various gestures using his body, which may result in various edits to the 3D object. An edit may include selection of a surface or part of a surface of the 3D object based on a first part of the body gesture performed by the user and modifying the surface of the 3D object based on a second part of the body gesture of the user. After the 3D object is completed, the 3D object may be animated, possibly based on movements of the user.</p><p id="p-0009" num="0008">This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The same reference numbers in different figures indicate similar or identical items.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of an illustrative environment that includes a computing architecture to model three dimensional (3D) objects using body gestures performed by a user.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow diagram of an illustrative process of creating objects using body gestures.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of a computing architecture to enable creation and animation of avatars using body gestures.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram of an illustrative process of creating and animating an avatar using body gestures.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are pictorial flow diagrams of an illustrative process of creating and editing an avatar using body gestures.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of an illustrative environment where the computing device includes network connectivity.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><heading id="h-0006" level="2">Overview</heading><p id="p-0017" num="0016">An intuitive interface may allow users of a computing device (e.g., children or other users) to create imaginary three dimensional (3D) objects and avatars of any shape using body gestures performed by the users as a primary or only input. A user may make motions while in front of an imaging device that senses movement of the user. The interface may allow first-person and/or third person interaction during creation of objects or avatars, which may map a body of a user to a body of an object or avatar presented by a display. In an example process, the user may start by scanning an arbitrary body gesture into an initial shape of an object or avatar. Next, the user may perform various gestures using his or her body, which may result in various edits to the object or avatar. After the object or avatar is completed, the object or avatar may be animated, possibly based on movements of the user.</p><p id="p-0018" num="0017">The processes and systems described herein may be implemented in a number of ways. Example implementations are provided below with reference to the following figures.</p><heading id="h-0007" level="2">Illustrative Environment</heading><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of an illustrative environment <b>100</b> that includes a computing architecture to model 3D objects using body gestures. The environment <b>100</b> includes a computing device <b>102</b> configured with a display <b>104</b>. The computing device <b>102</b> may be a gaming console, a desktop computer, a laptop computer, a tablet, a smart phone, or any other type of computing device capable of receiving inputs from various devices (e.g., camera, microphones, etc.) and causing a visual display. The computing device <b>102</b> may be in communication with an imaging device <b>106</b> and a microphone <b>108</b>. The microphone <b>108</b> is shown as being integrated with the imaging device <b>106</b>, but may be implemented as a separate component. The microphone <b>108</b> may be implemented as a single microphone or as a microphone array that can detect a direction of some sounds. In some instances, the microphone <b>108</b> may be worn by a user <b>110</b> and may communicate with the computing device <b>102</b> using a wired or wireless connection. In some embodiments, the computing device <b>102</b>, the display <b>104</b>, the imaging device <b>106</b>, and/or the microphone <b>108</b> may be integrated together.</p><p id="p-0020" num="0019">In accordance with various embodiments, the computing device <b>102</b> may track movement of the user <b>110</b> within a zone <b>112</b> using the imaging device <b>106</b>. In some embodiments, the imaging device <b>106</b> may include one or more cameras and one or more depth sensors, which may be used individually or together to record imagery in the zone <b>112</b>, sense movement in the zone <b>112</b>, determine relative distances of surfaces in the zone, detect colors, detect shapes (e.g., facial recognition, etc.), and/or perform other imagery functions. In some embodiments, the imaging device <b>106</b> may create a 3D interpretation of objects and people within the zone <b>112</b>. The zone <b>112</b> may be within a room or other predefined range and angle from the imaging device <b>106</b>.</p><p id="p-0021" num="0020">The user <b>110</b>, while being tracked by the imaging device <b>106</b>, may communicate body gestures by movement of the user's appendages and body that, when received by the computing device and interpreted, may cause the computing device <b>102</b> to perform certain actions. For example, the body gestures may be interpreted by the computing device <b>102</b> to cause creation and editing of 3D object while the computing device is operating in a modeling mode. In some embodiments, the object may be an avatar that the user may then animate using body gestures, possibly while the computing device <b>102</b> is operating in an animation mode. In some instances, the computing device <b>102</b> may simultaneously operate in the modeling mode and the animation mode.</p><p id="p-0022" num="0021">Body gestures may be any movements of the user <b>110</b> that are associated with commands that can be interpreted by the computing device <b>102</b>. In some instances, the body gestures may be predefined by the computing device. However, the computing device <b>102</b> may also be capable of learning new body gestures, which may be assigned to events by the user <b>110</b>. For example, when interacting with the computing device <b>102</b>, a body gesture of jumping up and down may cause, after detection, the computing device <b>102</b> to execute a first operation while a body gesture of reaching out with a hand having a closed first may cause, after detection, the computing device <b>102</b> to execute a second operation. Many different types of body gestures may be recognized by the computing device <b>102</b> by way of imagery captured by the imaging device <b>106</b>. The body gestures may be performed by appendages of the user <b>110</b> (e.g., movement of a hand, foot, arm, fingers, etc.) and/or by the entire body movement of the user (e.g., jumping). In some embodiments, the gestures may be indicated by a cue, such as a signal to indicate that a gesture is to be performed and/or to indicate that the gesture has been completed. The cue may be a voice command, another gesture, a hardware input, and/or another type of input.</p><p id="p-0023" num="0022">In addition to body gestures, the computing device <b>102</b> may receive voice commands from the user <b>110</b> by way of the microphone <b>108</b>. The voice commands may be discrete words (e.g., &#x201c;stop,&#x201d; &#x201c;next,&#x201d; &#x201c;menu,&#x201d; etc.), or may be commands parsed from natural language commands (e.g., &#x201c;show me the menu,&#x201d; causing the computing device <b>102</b> to cause display of a menu, etc.).</p><p id="p-0024" num="0023">The computing device <b>102</b>, in a basic configuration, may include a modeling application <b>114</b>, an animation application <b>116</b>, a motion detection module <b>118</b>, and a voice command module <b>120</b>, each discussed in turn.</p><p id="p-0025" num="0024">The modeling application <b>114</b> may receive input from the imaging device <b>106</b> to enable the user <b>110</b> to create a 3D model of an object using body gestures performed by the user <b>110</b>. The modeling application <b>114</b> may create and edit objects formed using voxel modeling, implicit surface modeling, or other types of 3d modeling. The modeling application <b>114</b> may create a non-rigid animated object that can be animated by the user or by other inputs.</p><p id="p-0026" num="0025">For example, the user <b>110</b> may use her body to create an initial object or shape, such as by forming a first pose with her body. The modeling application <b>114</b> may interpret the pose and form the initial object. The computing device <b>102</b> may cause display of the initial object on the display <b>104</b> to provide feedback to the user <b>110</b>. The user <b>110</b> may then edit the initial object using the modeling application <b>114</b>. The modeling application may recognize gestures, tools, voice commands, and/or other inputs, and in response perform operations to modify an object <b>122</b>, such as the initial object. The modeling application <b>114</b> may repeatedly allow the user <b>110</b> to edit and modify the object <b>122</b>. The modeling application <b>114</b> may interpret operations such as push a feature, pull a feature, sculpt a feature, sketch a feature, cut a feature, paint a feature, add a special feature, and/or other operations. These operations are discussed in greater detail below, and specifically with reference to <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref>. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the object <b>122</b>, such as an avatar, may be presented back to the user <b>110</b> by the display <b>104</b>.</p><p id="p-0027" num="0026">The modeling application <b>114</b> may use body-centric coordinate mapping to map the user <b>110</b> to the object being modeled, and thus allowing the user <b>110</b> to interact with the object using body gestures. In some embodiments, the modeling application <b>114</b> may operate in a first person editing mode (body reference mode) or a third person editing mode (spatial mapping mode). The body reference mode may allow the user <b>110</b> to edit the object as if at least a part of the user <b>110</b> is the object. The spatial mapping mode may allow the user <b>110</b> to edit the object as if the user stepped out of the object and is next to the object. In the spatial mapping mode, the user <b>110</b> may freely move about relative to the object to edit the object from outside the object or edit the object from inside of the object. Thus, the modeling application <b>114</b> may determine a reference mode for operation (e.g., the spatial mapping mode or the body reference mapping mode), map locations of the user <b>110</b> to locations of the object or locations near the object, and then perform operations to the object based on the body gestures of the user <b>110</b>. After the user confirms a mapping relation, the user may trigger an action. By triggering an action, whenever the user <b>110</b> touches the surface or a portion of the surface of the object with any part of her body, the object will transform accordingly. With body reference mapping, the modeling application <b>114</b> maps generally to the user's skeleton (e.g., use several joints along the line from the head to the foot, etc.) to a center line of the object (avatar). In this case, the user is able to touch some parts of her body to navigate to a certain part/point on the object or avatar. As discussed herein, the term &#x201c;surface&#x201d; may be used to mean an entire surface or a portion of a surface.</p><p id="p-0028" num="0027">One main difference between these two mappings is in the spatial mapping mode, the user is free to move relative to the object, such as rotating and walking around the object or into the object as long as she does not trigger an editing action, such as by touching, with an editing tool, at least a portion of a surface mapped to at least a portion of a surface of the object. While in the body reference mapping mode, the object will follow the user's action so that the user will feel that she is the object herself. When she triggers an action, the modeling application <b>114</b> will stop the mappings for the editing hand/arm/leg, etc. For instance, when the user is using a right hand to edit the object (touch a surface of the object, etc.), the mapping relation between the right arm and the object may be at least temporarily ignored until the action is completed.</p><p id="p-0029" num="0028">The animation application <b>116</b> may animate the object, such as an avatar. The animation application <b>116</b> may create relationships between the user's movement and the movement of the object. For example, when the object is an avatar that includes three arms, the animation application <b>116</b> may map two of the arms to respective arms of the user <b>110</b> and may map the movement of the third arm to one of the user's arms (e.g., left arm, etc.) or may not map the third arm to the user but instead control the third arm based on other information (e.g., random movement, aggregate movement of both the user's arm, etc.). The animation application <b>116</b> may enable the user to control the avatar (e.g., the object <b>122</b>), such as during game play in an adventure game, a sports game, an action game, and/or in other games or scenarios.</p><p id="p-0030" num="0029">The motion detection module <b>118</b> may capture and interpret movement of the user <b>110</b> that performs the body gestures. The motion detection module <b>118</b> may also capture 3D reference information, such as a pose of the user <b>110</b> that may be used to create the initial object. The motion detection module <b>118</b> may receive signals from the imaging device <b>106</b> and interpret those signals to enable creation of 3D objects, detection of body gestures, and so forth. Thus, the motion detection module <b>118</b> may interpret the movements and information from imagery captured by the imaging device <b>106</b>, which may then be processed by the modeling application <b>114</b> and/or the animation application <b>116</b>.</p><p id="p-0031" num="0030">The voice command module <b>120</b> may capture and interpret sounds from the user <b>110</b> and/or other sounds. The voice command module <b>120</b> may interpret the sounds as voice commands, which may then be processed by the modeling application <b>114</b> and/or the animation application <b>116</b>.</p><heading id="h-0008" level="2">Illustrative Operation</heading><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow diagram of an illustrative process of creating objects using body gestures. The process <b>200</b> is illustrated as a collection of blocks in a logical flow graph, which represent a sequence of operations that can be implemented in hardware, software, or a combination thereof. In the context of software, the blocks represent computer-executable instructions that, when executed by one or more processors, cause the one or more processors to perform the recited operations. Generally, computer-executable instructions include routines, programs, objects, components, data structures, and the like that perform particular functions or implement particular abstract data types. The order in which the operations are described is not intended to be construed as a limitation, and any number of the described blocks can be combined in any order and/or in parallel to implement the process. Other processes described throughout this disclosure, in addition to process <b>300</b>, shall be interpreted accordingly. The process <b>200</b> is described with reference to the environment <b>100</b>. However, the process <b>200</b> may be implemented in other environments.</p><p id="p-0033" num="0032">At <b>202</b>, the modeling application <b>114</b> may create or obtain an initial 3D object (or shape). For example the modeling application <b>114</b> may enable the user <b>110</b> to create the initial object by forming a pose and then creating a basic 3D model representative of the pose of the user. For example, if the user <b>110</b> where to kneel down and make her profile look generally like an egg, the initial object may be rendered in the basic shape of an egg. In some embodiments, the modeling application <b>114</b> may enable the user to select an existing object as the initial object. The existing object may be retrieved from a library of digital objects and/or captured from the real world by the imaging device <b>106</b> (e.g., scanning an object in front of the imaging device <b>106</b> to create a 3d object).</p><p id="p-0034" num="0033">At <b>204</b>, the modeling application <b>114</b> may edit the object <b>122</b> based on body gestures performed by the user <b>110</b>. The modeling application <b>114</b> may, for example, interpret a body gesture as a &#x201c;push a feature&#x201d; operation, and in response, extrude an appendage from the initial object or from a modified form of the object <b>122</b>.</p><p id="p-0035" num="0034">At <b>206</b>, the modeling application <b>114</b> may provide visual feedback to the user <b>110</b> to show changes to the object <b>122</b>. The visual feedback may be presented via the display <b>104</b>. The modeling application <b>114</b> may enable the user <b>110</b> to continually update the object <b>122</b> through repeated edits via the operation <b>204</b>, which may be shown to the user <b>110</b> via the operation <b>206</b>.</p><p id="p-0036" num="0035">At <b>208</b>, the modeling application <b>114</b> may save the object (e.g., an avatar). The modeling application <b>114</b> may enable use of the object <b>122</b> by other applications (e.g., games, resources, etc.), and/or may enable animation of the object, such as by use of the animation application <b>116</b>.</p><heading id="h-0009" level="2">Illustrative Computing Architecture</heading><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of a computing architecture <b>300</b> to enable creation and animation of avatars using body gestures. The computing architecture <b>300</b> shows additional details of the computing device <b>102</b>, which may include additional modules, libraries, and/or hardware.</p><p id="p-0038" num="0037">The computing architecture <b>300</b> may include processor(s) <b>302</b> and memory <b>304</b>. The memory <b>304</b> may store various modules, applications, programs, or other data. The memory <b>304</b> may include instructions that, when executed by the processor(s) <b>302</b>, cause the processor(s) to perform the operations described herein for the computing device <b>102</b>.</p><p id="p-0039" num="0038">The computing device <b>102</b> may have additional features and/or functionality. For example, the computing device <b>102</b> may also include additional data storage devices (removable and/or non-removable) such as, for example, magnetic disks, optical disks, or tape. Such additional storage may include removable storage and/or non-removable storage. Computer-readable media may include, at least, two types of computer-readable media, namely computer storage media and communication media. Computer storage media may include volatile and non-volatile, removable, and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data. The system memory, the removable storage and the non-removable storage are all examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD), or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to store the desired information and which can be accessed by the computing device <b>102</b>. Any such computer storage media may be part of the computing device <b>102</b>. Moreover, the computer-readable media may include computer-executable instructions that, when executed by the processor(s), perform various functions and/or operations described herein.</p><p id="p-0040" num="0039">In contrast, communication media may embody computer-readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave, or other mechanism. As defined herein, computer storage media does not include communication media.</p><p id="p-0041" num="0040">The memory <b>304</b> may store an operating system <b>306</b> as well as the modeling application <b>114</b>, the animation application <b>116</b>, the motion detection module <b>118</b>, and the voice command module <b>120</b>.</p><p id="p-0042" num="0041">The modeling application <b>114</b> may include various modules such as a tool module <b>308</b>, a mapping module <b>310</b>, and an editing module <b>312</b>. Each of these modules is discussed in turn.</p><p id="p-0043" num="0042">The tool module <b>308</b> may allow selection of tools for use by the user <b>110</b> to interact with the modeling application <b>114</b>. The tools may be selected and identified based on object held by the user, by gestures of the user, by voice commands, or by a combination thereof. For example, the tool module <b>308</b> may detect a prop of a pair of scissors (e.g., plastic scissors, etc.) that are a real world object, held by the user <b>110</b>, and detectable by the imaging device <b>106</b>. The scissors may be associated with a type of action, such as an edit feature of cutting. As another example, the tool module <b>308</b> may receive a command &#x201c;cut&#x201d; or &#x201c;use scissors&#x201d; to initiate use of that particular tool. In yet another example, the user may use her fingers to make a scissor action with her index finger and middle finger moving apart and then back together, which may allow the tool module <b>308</b> to identify a desired use of that particular tool. Thus, the tool module <b>308</b> may enable identification and selection of tools based on at least gestures, voice commands, and/or real world objects. In some embodiments, the tools may also be selected from preexisting tools (e.g., from a menu.). The tool module <b>308</b> may store references and associated tools in a tool library <b>314</b>. In some embodiments, the user <b>110</b> may define or modify definitions of the tools in the tool library <b>314</b>.</p><p id="p-0044" num="0043">In various embodiments, the modeling application <b>114</b> may detect tool and/or gestures based on a position of wearable devices, such as a glove. For example, the user may wear a glove, shirt, helmet, or other wearable devices which can be sensed by the imaging device <b>106</b>. However, body gestures may be recognized without use of the wearable devices based on existing algorithms created for such purposes.</p><p id="p-0045" num="0044">The mapping module <b>310</b> may cause the modeling application <b>114</b> to alternate between mappings. As discussed above, the modeling application <b>114</b> may operate in at least two mapping modes: a first person editing mode (body reference mapping mode) and a third person editing mode (spatial mapping mode). The body reference mapping mode may allow the user <b>110</b> to edit the object as if the user <b>110</b> was inside of the object or is the object. Thus, the object is mapped at least partially to the body of the user <b>110</b>. The spatial mapping mode may allow the user <b>110</b> to edit the object as if the user stepped out of the object and is next to the object. Thus the object is mapped to a position that is different than, but accessibly by, the user <b>110</b>. In this position, the user <b>110</b> may walk around the object as if the object is a statue being sculpted or otherwise edited by the user <b>110</b>. The mapping module <b>310</b> may alternate between the modes based on body gestures of the user <b>110</b>, voice commands, and/or other received selections. For example, the user gesture of &#x201c;jumping to a side&#x201d; may cause the mapping module to shift between the mappings. Other gesture commands may be used as well.</p><p id="p-0046" num="0045">The mapping module <b>310</b> may manage reference planes and/or reference axes used by the modeling application to maintain mappings between the object and the user <b>110</b>. In some embodiments, the user <b>110</b> may desire to adjust a mapping such as to rotate the object to enable editing a different side of the object. The mapping module may cause some actions to snap to or operate in certain ways with respect to the planes/axes, such as to snap to a plane or axis (e.g., sketching may snap to a plane and then be transformed to a 3D object after the sketch is completed). Thus, the mapping module <b>310</b> may create and manage reference information for the user <b>110</b> and the object, and may create and maintain a relationship between the reference information via the mappings, based partly on the mode of operation (the spatial mapping mode, the body reference mapping mode, etc.).</p><p id="p-0047" num="0046">The editing module <b>312</b> may cause creation, selection, and edits to the object (e.g., the avatar). The edits may modify surfaces of the object. The edits may include operations such as push a feature, pull a feature, sculpt a feature, sketch a feature, cut a feature, paint a feature, add a special feature, and/or other operations. The edit module <b>312</b> may determine an operation to perform based on various information, such as a tool identified by the tool module <b>308</b>, a mapping determined by the mapping module <b>310</b> (including corresponding mappings between the user <b>110</b> and the object), a location of the tool relative to the object (e.g., on surface of object, near the object), and so forth.</p><p id="p-0048" num="0047">In some embodiments, the editing module <b>312</b> may enable the user <b>110</b> to obtain parts from a part library <b>316</b>. For example, the user <b>110</b> may desire to add eyes, ears, and a mouth to her avatar. The eyes, ears, mouth, and other predetermined objects may be selectable from the part library. In some embodiments, the functionality of these parts may be accessible and/or assignable. For example, the user <b>110</b> may create an eye using the editing module, and then assign the function of the created eye as the function of seeing, which in turn may modify a display during animation of the avatar (e.g., during game play, etc.). Other common parts or user created parts may be stored in the part library and made accessible to the editing module <b>312</b>. In some embodiments, the part library <b>316</b> may be accessible to other computing devices via a network connection.</p><p id="p-0049" num="0048">In various embodiments, the edit module <b>312</b> may provide feedback to the user <b>110</b>, via an output to the display of the avatar, to assist in editing the avatar. For example, the edit module <b>312</b> may identify a selectable portion of the object (e.g., a portion of a surface of the object, etc.) about to be edited before the editing occurs, may indicate a currently selected tool, indicate one or more reference planes and/or axes, and so forth. The user <b>110</b> may then use this information prior to or during the editing of the avatar.</p><p id="p-0050" num="0049">In accordance with one or more embodiments, the modeling application <b>114</b> may enable multi-user operations, such as by enabling two or more users to design an object while working together. For example, a first user may act as the object while a second user performs edit operations on the object (positioned as the first user).</p><p id="p-0051" num="0050">The animation application <b>116</b> may include various modules such as an animation assignment module <b>318</b> and an animation module <b>320</b>. Each of these modules is discussed in turn.</p><p id="p-0052" num="0051">The animation assignment module <b>318</b> may map motion and animation features of the avatar (created via the modeling application <b>114</b>) to body gestures of the user <b>110</b>. In some embodiments, the animation assignment module <b>318</b> may map motion and animation features to related parts (e.g., map left arm of avatar to left hand of the user <b>110</b>). However, when the avatar includes features that do not correspond to features of the user <b>110</b>, then customized assignments may be performed by the animation assignment module <b>318</b>. In various embodiments, the animation assignment module <b>318</b> may allow the user <b>110</b> to specific mappings and animation of these non-corresponding features. For example, when the avatar includes three arms, the animation application <b>116</b> may map two of the arms to respective arms of the user <b>110</b> and may map the movement of the third arm to one of the user's arms (e.g., left arm, etc.) or may not map the third arm to the user but instead control the third arm based on other information (e.g., random movement, aggregate movement of both the user's arm, movement of a leg, etc.). As another example, when the avatar does not include legs, the animation assignment module <b>318</b> may map a gesture such as jumping with movement of the avatar. The animation assignment module <b>318</b> may use a structured process to enable the user to specify mappings of motion to features of the avatar, which may be stored in an animation mapping store <b>322</b>. In some embodiments, the animation mapping store <b>322</b> may include selectable animations, such as movement of a tail, which can be assigned to a feature of a tail included on the avatar and so forth.</p><p id="p-0053" num="0052">The animation module <b>320</b> may cause animation of the avatar by implementing the animation assignments created by the animation assignment module <b>318</b> and stored in the animation mapping store <b>322</b>. In some embodiments, the animation application <b>116</b> may be used by other resources, such as other games, applications, and so forth.</p><heading id="h-0010" level="2">Illustrative Operations</heading><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram of an illustrative process <b>400</b> of creating and animating an avatar using body gestures. Similar to the process <b>200</b>, the process <b>400</b> is illustrated as a collection of blocks in a logical flow graph, which represent a sequence of operations that can be implemented in hardware, software, or a combination thereof. The order in which the operations are described is not intended to be construed as a limitation, and any number of the described blocks can be combined in any order and/or in parallel to implement the process. The process <b>400</b> is described with reference to the environment <b>100</b> and the computing architecture <b>300</b>. However, the process <b>400</b> may be implemented in other environments and/or other computing architectures.</p><p id="p-0055" num="0054">At <b>402</b>, the modeling application <b>114</b> may determine an initial shape for the avatar. For example the modeling application <b>114</b> may enable the user <b>110</b> to create the initial object by forming a pose and then creating a basic 3D model representative of the pose of the user. In some embodiments, the modeling application <b>114</b> may enable the user to select an existing object as the initial shape. For example, the user <b>110</b> may create the initial shape using a toy or other real or virtual object.</p><p id="p-0056" num="0055">At <b>404</b>, the tool module <b>308</b> may determine a tool for use by the user <b>110</b>. The tool module <b>308</b> may determine the tool based on a user selection, which may be a voice command, a body gesture, a menu selection, or another type of selection. For example, the tool module <b>308</b> may determine the tool based on a prop that is picked up by the user (e.g., a pair of scissors, a wand, etc.).</p><p id="p-0057" num="0056">At <b>406</b>, the mapping module <b>310</b> may determine a mapping for interaction between the user <b>110</b> and the avatar. For example, the mapping module <b>310</b> may determine the mapping from a voice command, a gesture, a menu selection, or another type of selection.</p><p id="p-0058" num="0057">At <b>408</b>, the editing module <b>312</b> may cause an edit to the avatar. The edit may change the 3D shape of the initial shape or an edited shape (after one or more edits to the initial shape). The edits may include operations such as push a feature, pull a feature, sculpt a feature, sketch a feature, cut a feature, paint a feature, add a special feature, and/or other operations.</p><p id="p-0059" num="0058">At <b>410</b>, the modeling application <b>114</b> may determine whether to undo an edit. For example, the user <b>110</b> may make create a feature using a gesture and then decide she does not like the newly created feature or that the newly created feature was not created as intended. Thus, the decision operation <b>410</b> may enable quick and easy reversion to a version of the avatar prior to a most recent edit. In some embodiments, the decision operation <b>410</b> may allow the user <b>110</b> to undo any number of edits (features). When the decision operation <b>410</b> determines to undo one or more edits (following the &#x201c;yes&#x201d; route from the decision operation <b>410</b>), then the process <b>400</b> may advance to an operation <b>412</b>.</p><p id="p-0060" num="0059">At <b>412</b>, the one or more edits may be removed, thus reverting back to a prior version of the avatar prior to implementation of one or more edits by the operation <b>408</b>. Following the operation <b>408</b>, the process <b>400</b> may return to the operation <b>404</b>, the operation <b>406</b>, and/or the operation <b>408</b> to allow the user to continue to edit the avatar, or possibly to a save operation <b>416</b>.</p><p id="p-0061" num="0060">When the decision operation <b>410</b> determines not to undo one or more edits (following the &#x201c;no&#x201d; route from the decision operation <b>410</b>), then the process <b>400</b> may advance to an operation <b>414</b>. This route may not require action by the user <b>110</b> (e.g. may be a default route). At <b>414</b>, the modeling application <b>114</b> may determine whether the avatar is completed (typically base on a user selection). When the avatar is not complete (following the &#x201c;no&#x201d; route from the decision operation <b>414</b>), then the process <b>400</b> may advance to the operation <b>404</b>, the operation <b>406</b>, and/or the operation <b>408</b> to continue modifying the avatar.</p><p id="p-0062" num="0061">When the avatar is complete (following the &#x201c;no&#x201d; route from the decision operation <b>414</b>), then the process <b>400</b> may advance to an operation <b>416</b>. At <b>416</b>, the modeling application <b>114</b> may save the avatar. In some embodiments, the avatar may be saved at other times, such as by automatic saving performed by the modeling application <b>114</b>.</p><p id="p-0063" num="0062">At <b>418</b>, the animation assignment module <b>318</b> may allow the user to map motion and animation features of the avatar (created via the modeling application <b>114</b>) to body gestures of the user <b>110</b>. The mapped animation relationships may be stored in the animation mapping store <b>322</b>.</p><p id="p-0064" num="0063">At <b>420</b>, the animation module <b>320</b> may cause animation of the avatar based on user gestures in accordance with the mappings stored in the animation mapping store <b>322</b>. In some embodiments, the process <b>400</b> may enable later modifications and/or edits to the avatar after the operations <b>416</b>, <b>418</b>, and/or <b>420</b> by allowing the process <b>400</b> to return to the operation <b>404</b> or other operations shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are pictorial flow diagrams of an illustrative process <b>500</b> of creating and editing an avatar using body gestures. Similar to the process <b>200</b> and the process <b>400</b>, the process <b>500</b> is illustrated as a collection of blocks in a logical flow graph, which represent a sequence of operations that can be implemented in hardware, software, or a combination thereof. The order in which the operations are described is not intended to be construed as a limitation, and any number of the described blocks can be combined in any order and/or in parallel to implement the process. The process <b>500</b> is described with reference to the environment <b>100</b> and the computing architecture <b>300</b>. However, the process <b>500</b> may be implemented in other environments and/or other computing architectures.</p><p id="p-0066" num="0065">The process <b>500</b> shows a series of edit functions arranged as operations that perform edits to an example object (avatar). The order of the operations is purely illustrative and may occur in any order. In some instances, the same edit operation may be performed many times while other edit operations may not be used to create an object. The process <b>500</b> shows creation of an avatar; however, the process <b>500</b> may be used to create other objects using body gestures made by the user <b>110</b>.</p><p id="p-0067" num="0066">Each operation shown in the process includes an associated representation of an avatar <b>502</b>. Some operations include a user action frame (e.g., a user action frame <b>504</b>) to show how one or more features in the operation may be created based on body gestures of the user <b>110</b>. The user action frame shows an action of the user on the left side of the frame and corresponding example change to the object on the right side of the frame. The user action frames are illustrative of some possible results of body gestures performed by the user <b>110</b>.</p><p id="p-0068" num="0067">At <b>506</b>, the edit module <b>308</b> may create or obtain an initial body <b>508</b> for the avatar <b>502</b>. The user action frame <b>504</b> shows the user <b>110</b> posing in a position to create the body <b>508</b> of the avatar <b>502</b>. However, the edit module <b>308</b> may create the body <b>508</b> using existing real world objects or vertical objects.</p><p id="p-0069" num="0068">At <b>510</b>, the edit module <b>308</b> may create push features or drag features based on respective body gestures performed by the user <b>110</b> and sensed by the motion detection module <b>118</b>. A push operation may occur when the body gesture pushes against a surface (or portion of a surface) of the avatar <b>502</b>, as shown in a user action frame <b>512</b>. A drag (or pull) operation may occur when the body gesture pulls a surface of the avatar <b>502</b>, as shown in a user action frame <b>514</b>. The push and pull operations may create extrusions such as appendages <b>516</b> (e.g., arms, legs, etc.). The push and pull operations may also modify a size or shape of existing features, such as modifying a shape of a head <b>518</b> of the avatar <b>502</b> by widening the head (or possibly by shrinking the head, etc.). The push and pull operations may be performed in the first person editing mode (body reference mapping mode) and/or the third person editing mode (spatial mapping mode). In the body reference mapping mode, the edit module may determine that the user's gesture is a push against a surface of the avatar <b>502</b> outwards as if the user <b>110</b> was inside of the avatar. In the spatial mapping mode, the edit module <b>308</b> may determine that the user's gesture is pull from a surface of the avatar <b>502</b> outwards as if the user <b>110</b> were outside of the avatar and pulling a part of the avatar. The push/pull operation may start and stop following cues. A start cue may be an identification of a tool touching a surface of the avatar. The extrusion may stop after the use performs a gesture to stop the push or pull operation (e.g., releases clamped hand, pauses for a predetermined amount of time, etc.).</p><p id="p-0070" num="0069">At <b>520</b>, the edit module <b>308</b> may add special features <b>522</b> to the avatar <b>502</b>. The special features <b>522</b> may be obtained from the parts library <b>316</b>. For example the user may add special features of eyes and a mouth <b>524</b> to the avatar. The special features <b>522</b> may be added to the avatar using any mapping mode. In some embodiments, the special features <b>522</b> may be added by selecting the special feature (via gestures, voice commands, and/or menu selections) and then touching a location on the avatar to place the special features. In some instances, the special features may include assignment of functionality, such as an ability to hear, to see, and so forth. These sensory attributes may be added to existing features using the editing module <b>308</b> in some embodiments. Thus, the editing module <b>308</b> may assign sensory attributes and/or parts to specific locations on the avatar based on body gestures performed by the user <b>110</b>.</p><p id="p-0071" num="0070">Moving to <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, at <b>524</b>, the edit module <b>308</b> may create sculpt features or sketch features based on respective body gestures performed by the user <b>110</b> and sensed by the motion detection module <b>118</b>. A sculpt operation may occur when the body gesture moves over a surface to define a new profile or location for an adjacent surface. A user action frame <b>528</b> shows an example sculpt operation to create a paunch for an avatar. An example sculpted feature <b>530</b> is shown on the avatar <b>502</b>.</p><p id="p-0072" num="0071">A sketch operation may occur when the body gesture draws an outline on a surface or plane (e.g., a reference plane provided by the mapping module <b>310</b>). A user action frame <b>532</b> shows an example sketch operation to create hair <b>534</b> (or a mane) for the avatar <b>502</b>. In another example, the edit module <b>208</b> may be used to sketch a tail <b>536</b>. The tail may also be created by other edit operations, such as the drag operation. Thus, some features may be created using one or more different edit operations.</p><p id="p-0073" num="0072">When implementing the sketch operation, the edit module <b>308</b> may first place the sketch on a plane or other surface, and then associate or connect the sketch to the avatar <b>502</b>. The edit module <b>308</b> may transform the sketch to a 3D sketch by adding volume to a resultant feature, such as volume to the tail <b>536</b> or the hair <b>534</b>. The sculpt operation and the sketch operation may be performed in the first person editing mode (the body reference mapping mode) and/or the third person editing mode (spatial mapping mode).</p><p id="p-0074" num="0073">At <b>538</b>, the edit module <b>308</b> may cut features based on respective body gestures performed by the user <b>110</b> and sensed by the motion detection module <b>118</b>. A cut operation may occur when the body gesture moves into a surface of the avatar <b>502</b>, such as shown in a user action frame <b>540</b>. The cut operation may be used to reduce a size, shape, or volume of an existing feature. For example, the gestures by a user may motion to cut the size of the arms <b>542</b> of the avatar to make the arms narrower. In some embodiments, the cut operation may be performed using a same or similar algorithm as the push/pull operations, but may reduce the size of a feature rather than increase a size of a feature. The cut operation may be performed in the first person editing mode (body reference mapping mode) and/or the third person editing mode (spatial mapping mode).</p><p id="p-0075" num="0074">At <b>544</b>, the edit module <b>308</b> may paint colors, textures, patterns, clothing, and/or other features on the avatar. The paint operation may be performed by selecting a paint attribute (e.g., colors, textures, patterns, clothing, etc.), and then applying the paint attribute to a surface using a body gesture, such as shown in a user action frame <b>546</b>. For example, the user <b>110</b> may add color, texture, etc. to the hair <b>548</b> and or add color, texture, clothing, markings, etc. to a body <b>550</b> of the avatar. In some embodiments, the paint feature may enable selection of colors, textures, or other paint attributes from attributes of existing object (real objects and/or virtual objects).</p><heading id="h-0011" level="2">Illustrative Operations</heading><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of an illustrative environment <b>600</b> where the computing device <b>102</b> includes network connectivity. The environment <b>600</b> may include communication between the computing device <b>102</b> and one or more remote resources, such as remote resources <b>602</b>(<b>1</b>), <b>602</b>(<b>2</b>), . . . , <b>602</b>(N) through one or more networks <b>604</b>. The networks may include wired or wireless networks, such as Wi-Fi networks, mobile telephone networks, and so forth.</p><p id="p-0077" num="0076">The remote resources <b>602</b>(<b>1</b>)-(N) may host some of the functions shown in the computing architecture <b>300</b>. For example, the remote resources <b>602</b>(<b>1</b>)-(N) may store the avatar for access in other computing environments, may perform the animation processes or portions thereof, may perform the modeling processes or portions thereof, and so forth. The remote resources <b>602</b>(<b>1</b>)-(N) may be representative of a distributed computing environment, such as a cloud services computing environment.</p><heading id="h-0012" level="1">CONCLUSION</heading><p id="p-0078" num="0077">Although the techniques have been described in language specific to structural features and/or methodological acts, it is to be understood that the appended claims are not necessarily limited to the specific features or acts described. Rather, the specific features and acts are disclosed as exemplary forms of implementing such techniques.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>creating, using an imaging device, a three dimensional (3D) object from an initial pose of a user;</claim-text><claim-text>editing the 3D object by:<claim-text>determining an edit operation to be performed on the 3D object;</claim-text><claim-text>performing the edit operation on the 3D object based at least in part on a body gesture of the user that is sensed by the imaging device; and</claim-text><claim-text>causing presentation of the 3D object to the user; and</claim-text></claim-text><claim-text>animating the 3D object based on additional body gestures of the user.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the performing the edit operation includes:<claim-text>selecting at least a portion of a surface of the 3D object based on a first part of the body gesture of the user; and</claim-text><claim-text>modifying the at least a portion of the surface of the 3D object based on a second part of the body gesture of the user.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the editing the 3D object is performed by using a mapping between the user and the 3D object, the mapping being at least one of:<claim-text>a body reference mapping where the user is mapped to a similar location as the 3D object to allow the user to interact with the 3D object in a first-person editing perspective, or</claim-text><claim-text>a spatial mapping where the user is mapped to a location outside of the 3D object to allow the user to interact with the 3D object in a third-person editing perspective.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method as recited in <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising determining the mapping prior to performing the edit operation.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 3D object is an avatar.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the edit operation is at least one of a push operation, a drag operation, a sketch operation, a sculpt operation, a cut operation, or a paint operation.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising mapping features of the 3D object to body parts of the user sensed by the imaging device, and wherein the animating the 3D object includes moving the features of the 3D object based on movement of the body parts of the user.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising storing the 3D object for access by another computing device.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving a selection of an existing 3D object, and</claim-text><claim-text>adding the existing 3D object to the 3D object.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method as recited in <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising obtaining the existing 3D object by scanning a real world object using the imaging device.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining the edit operation includes identifying a tool based on object recognition or based on the body gesture.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising receiving a voice command using a microphone, and wherein the determining the edit operation or the performing the edit operation is based at least partly on the voice command.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A system comprising:<claim-text>an imaging device to detect body gestures of a user;</claim-text><claim-text>one or more processors; and</claim-text><claim-text>memory to store computer readable instructions that, when executed on the one or more processors, cause the one or more processors to perform acts comprising:<claim-text>identifying the body gesture of the user based on information received from the imaging device;</claim-text><claim-text>determining an edit operation to be performed on a three dimensional (3D) object based at least in part on the body gesture; and</claim-text><claim-text>modifying the 3D object using the edit operation.</claim-text></claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system as recited in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the edit operation modifies a location of at least a portion of a surface of the 3D object based on movement of the user during the body gesture.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system as recited in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the determining the edit operation is based at least in part on identification of a tool held by the user and detected by the imaging device.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system as recited in <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising creating the 3D object using at least one of voxel modeling or implicit surface modeling.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system as recited in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the 3D object is an avatar, and wherein the acts further comprise animating the avatar based on movements of the user.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. One or more memory storing computer-executable instructions that, when executed on one or more processors, causes the one or more processors to perform acts comprising:<claim-text>obtaining a three dimensional (3D) object from an initial shape;</claim-text><claim-text>determining an edit operation to be performed on the 3D object;</claim-text><claim-text>performing the edit operation on the 3D object based at least in part on a body gesture of a user that is sensed by a imaging device; and</claim-text><claim-text>causing presentation of the 3D object.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The one or more memory as recited in <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the edit operation modifies a location of at least a portion of a surface of the 3D object based on movement of the user during the body gesture.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The one or more memory as recited in <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising animating the 3D object.</claim-text></claim></claims></us-patent-application>