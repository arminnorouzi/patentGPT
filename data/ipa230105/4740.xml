<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004741A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004741</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364641</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00422</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6228</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">HANDWRITING RECOGNITION METHOD AND APPARATUS EMPLOYING CONTENT AWARE AND STYLE AWARE DATA AUGMENTATION</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>KONICA MINOLTA BUSINESS SOLUTIONS U.S.A., INC.</orgname><address><city>Foster City</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>XU</last-name><first-name>Ting</first-name><address><city>Campbell</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>KONICA MINOLTA BUSINESS SOLUTIONS U.S.A., INC.</orgname><role>02</role><address><city>Foster City</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A content aware and style aware neural network based data augmentation model generates augmented data sets to train neural network based handwriting recognition models to recognize individuals' handwriting. In embodiments, the augmented data sets are generated so as to be artificial, and to lack personal or confidential information. In embodiments, the data augmentation model generates content reference sets of individual characters generated in different fonts, and style reference sets of pluralities of characters of a particular style, for example, an individual's handwriting.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="216.75mm" wi="158.75mm" file="US20230004741A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="232.49mm" wi="174.07mm" file="US20230004741A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="219.88mm" wi="159.51mm" file="US20230004741A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD OF THE INVENTION</heading><p id="p-0002" num="0001">Aspects of the present invention relate to handwriting recognition, and in particular to augmentation of data sets to train handwriting recognition systems.</p><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0003" num="0002">One of the numerous challenges in handwriting recognition is the need to compensate for different handwriting styles. Even the same individual can have different handwriting as they get older. The same individual's handwriting can be different on different surfaces, or when the individual uses different writing instruments, or when the individual is forced to write in cramped spaces (for example, filling out a form that provides very little space in its field).</p><p id="p-0004" num="0003">One term for these changes is called a data distribution shift, where the probability distribution of test samples is different from that of training samples (x, y) that are used to train a handwriting recognition system. When test samples are drawn from a different probability distribution P(x, y), it can cause the handwriting recognition system to vary away from a desired result. One way of addressing this problem is to transfer learning from the original model onto the new data. However, taking this approach can cause issues with sufficiency of data, and with data privacy.</p><p id="p-0005" num="0004">When handwriting samples are used to train a model, it can be difficult to obtain enough useful samples; hence the data sufficiency problem. Moreover, the samples themselves may contain types of information that the handwriter would not want made public; hence the data privacy problem. Depending on the model size and which parts of the model need to be fine-tuned, customer data still may be insufficient, even for transfer learning. In addition, it is necessary for the customer to identify and/or label this customer data. Further, since the data originates with the customer, on one or more customer devices, this data needs to be transferred to a service provider. This transfer is how the data privacy concerns can arise. Recent efforts to advance collaborative distributed learning, such as Federated Learning, provide an approach to address this issue. As an example of this kind of approach, multiple sources (local devices) may receive a model from a central server. Each source provides an update to the model, using data stored locally at the source. The update, but not the locally stored data, is transmitted back to the central server</p><p id="p-0006" num="0005">For handwriting recognition, the issue of data distribution P(x, y) drift is quite pronounced because of unique handwriting styles for each individual. For example, different people are likely to write the same character in different ways, whether with the same instrument or with a different instrument, or in different ways in different places (for example, in a relatively cramped area on a form compared with an open space on a sheet of paper), or at the same time or at different times (since people's handwriting tends to evolve with age). Technically, this is one type of data drift called &#x201c;concept drift,&#x201d; where the conditional probability, or likelihood, P(x|y), meaning that the probability of x (an image) occurring given that y (a character) occurred, can differ from training to production. Another type of drift is &#x201c;prior drift,&#x201d; where the prior probability P(y) changes as a result of a change in content distribution. For example, in kanji, depending on the topic, the text can employ different characters, with different frequencies. All of the foregoing takes into account Bayes theorem, according to which P(x, y)=P(x|y)P(y).</p><p id="p-0007" num="0006">An additional challenge is the ability to obtain sufficient data to train a handwriting recognition system, particularly a neural network-based system. Even the most sophisticated recognition models do not work well if there is not enough training data of suitable relevance.</p><heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0008" num="0007">To address the foregoing, aspects of the present invention provide a data augmentation technique that can synthesize training data similar to customer's data in both style and content to produce augmented data. Style similarity diminishes concept drift, keeping P(x|y) the same or almost the same as the conditional probability for customer data. Content relevance reduces prior drift, and keeps P(y) the same or almost the same as the prior probability for customer data.</p><p id="p-0009" num="0008">Since the augmented data are artificially generated, the data pose fewer privacy issues even if the data are uploaded to the cloud. On the other hand, the substantial availability of synthesized data removes data scarcity as a concern for transfer learning.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">Aspects of the invention now will be described with reference to embodiments as illustrated in the accompanying drawings, in which:</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a high level flow chart for providing augmented data sets according to an embodiment;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a high level block diagram of structure for providing augmented data sets according to an embodiment;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a high level block diagram of an exemplary computing system for handwriting recognition according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading><p id="p-0014" num="0013">Embodiments of the present invention may provide a computer-implemented method comprising:</p><p id="p-0015" num="0014">selecting a text character from a plurality of text characters;</p><p id="p-0016" num="0015">selecting a plurality of fonts from a font set;</p><p id="p-0017" num="0016">generating the text character in each of the plurality of fonts to generate a content reference set;</p><p id="p-0018" num="0017">selecting a plurality of images from an image set to generate a style reference set comprising a style selected from a plurality of styles;</p><p id="p-0019" num="0018">transferring the style to the text character in each of the plurality of fonts to generate a set of stylized characters;</p><p id="p-0020" num="0019">selecting a plurality of stylized characters from the set;</p><p id="p-0021" num="0020">randomly sequencing the plurality of stylized characters to form an augmented data set; and</p><p id="p-0022" num="0021">applying the augmented data set to a handwriting recognition system as a training set.</p><p id="p-0023" num="0022">In embodiments, the listed actions may be repeated for each of the plurality of text characters, or for each of the plurality of styles.</p><p id="p-0024" num="0023">In embodiments, the content reference set may be generated before the style reference set, or the style reference set may be generated before the content reference set.</p><p id="p-0025" num="0024">In an embodiment, at least one of the content reference set and the style reference set is selected from previously-generated reference sets.</p><p id="p-0026" num="0025">In an embodiment, the selected text character may be generated from an augmented data set generator comprising a recurrent neural network/long short term memory language (RNN/LSTM LM) model. In an embodiment, that model may be trained by updating weights using cross-entropy loss calculation.</p><p id="p-0027" num="0026">In an embodiment, the mentioned handwriting recognition system may be used to recognize handwriting. The handwriting recognition model may employ a neural network based system selected from the group consisting of a convolutional neural network (CNN), in conjunction with a bidirectional LSTM (CRNN); and a CNN in combination with a transformer neural network. In embodiments, the handwriting recognition model may be trained by updating weights using a loss calculation selected from the group consisting of cross-entropy loss and connectionist temporal classification (CTC) loss.</p><p id="p-0028" num="0027">Embodiments of the present invention also may provide an apparatus comprising:</p><p id="p-0029" num="0028">a neural network based augmented data set generator to generate an augmented data set, the augmented data set generator comprising:</p><p id="p-0030" num="0029">a content encoder to generate a content reference set comprising a text character selected from a plurality of text characters, the text character generated in each of a plurality of fonts;</p><p id="p-0031" num="0030">a style encoder to generate a style reference set comprising a plurality of images comprising text characters represented in a style selected from a plurality of styles;</p><p id="p-0032" num="0031">a mixer to transfer the style to the text character in each of the plurality of fonts to generate a set of stylized characters;</p><p id="p-0033" num="0032">a selector to select a plurality of stylized characters from the set; and</p><p id="p-0034" num="0033">a randomized stitcher to randomly sequence the plurality of stylized characters to generate the augmented data set;</p><p id="p-0035" num="0034">In an embodiment, the apparatus further may comprise a neural network based handwriting recognition system to receive the augmented data set from the augmented data set generator as a training set, and to recognize handwriting.</p><p id="p-0036" num="0035">In an embodiment, the augmented data set generator may generate a plurality of augmented data sets, one for each of the plurality of text characters, and/or one for each of the plurality of styles.</p><p id="p-0037" num="0036">In an embodiment, the augmented data set generator may comprise a recurrent neural network/long short term memory language (RNN/LSTM LM) model to generate the text character. In an embodiment, the model may be trained by updating weights using cross-entropy loss calculation.</p><p id="p-0038" num="0037">In an embodiment, the handwriting recognition system may comprise a neural network based model selected from the group consisting of: a convolutional neural network (CNN), in conjunction with a bidirectional LSTM (CRNN); and a CNN in combination with a transformer neural network (CNN+transformer).</p><p id="p-0039" num="0038">In an embodiment, the CRNN may be trained using connectionist temporal classification (CTC) loss calculation. In an embodiment, the CNN+transformer may be trained using cross-entropy loss calculation.</p><p id="p-0040" num="0039">In an embodiment, the mixer may comprise a bilinear model to map the content reference set and the style reference set to generate the set of stylized characters.</p><p id="p-0041" num="0040">In an embodiment, the apparatus further may comprise imaging equipment to provide some or all of the plurality of text characters used to generate the content reference set, and/or some or all of the plurality of images used to generate the style reference set.</p><p id="p-0042" num="0041">Aspects of the present invention address the data shift problem through improved data augmentation, generating data that more relevant in both content and style. In this manner, it is possible to improve the effectiveness of transfer learning on data from a given customer, leading to a more accurate handwriting recognition model for that customer&#x2014;effectively, a type of model customization.</p><p id="p-0043" num="0042">Using the described techniques in accordance with aspects of the invention, it is possible to generate relevant content in a user's handwriting style without revealing sensitive customer or user content. Moreover, content and style related data augmentation models need not be trained on customer or user data. In this manner, a customer's or user's data privacy on the server side of the system may be protected, even as a handwriting recognition model is trained to recognize handwriting of a particular user. Thus, for example, transfer learning, a machine learning method in which a model which is developed to accomplish one task, may be used as the starting point for a model being developed to accomplish another task.</p><p id="p-0044" num="0043">The following describes synthesis of line images of characters and/or letters (so-called &#x201c;relevant&#x201d; content) with handwriting style superimposed thereon or combined therewith.</p><p id="p-0045" num="0044">In Asian languages such as Chinese or Japanese, the alphabet contains thousands of characters, mainly Kanji. The words that are spelled out in such characters, for example, may have substantial influence over what characters may appear. Different characters may be pronounced the same way, but may have very different meanings, and so some characters may be less appropriate in some words than in others. As a result, the prior probability distribution of characters (especially Kanji) can vary depending on the subject. Accordingly, generating relevant text can make the prior distribution P(y) more similar to that of actual customer content.</p><p id="p-0046" num="0045">Relevant text does not necessarily mean fully comprehensible, however. In an embodiment, a character-level recurrent neural network (RNN)/long short-term memory (LSTM) language model (LM) may be employed, because the generated text is not, and is not intended to be for human comprehension. Rather, the idea is to be able to train the system to recognize someone's handwriting, and in particular, different characters or letters that a person may write. In that context, whether the word strings make sense need not matter. What matters more is appropriate coverage of text content in terms of characters and combinations of characters in words and/or phrases in the line images. Individual words may make sense. In a given discipline, such as finance, certain types of characters may be expected in words, and so, for individual words, prior probability distribution can be relevant and useful. While individual words may make sense, however, strings of those words may not. Within a word, prior probability distribution can be applied. With random generation of words, for example, to create the augmented data to generate training sets, the presence of a word in a word string may not have an effect on the likelihood of the presence of a following word. Comprehensibility of resulting words or phrases or sentences is not so important in one or more of the described embodiments.</p><p id="p-0047" num="0046">Where individual words are intended to make sense, prior probability distribution can be applied, as individual characters forming character strings will have a dependence and a context in a given discipline. In an embodiment, character strings need not make sense, however.</p><p id="p-0048" num="0047">In an embodiment, a text generator may be a standard RNN/LSTM character-level language model (LM) trained on a topic-specific corpus such as transportation, financial, medical, legal, scientific (engineering, chemistry, biology), or the like. Depending on the size of available topic-specific corpus, a single layer or a multi-layer RNN/LSTM may be used. The corpus, which is a sequence of characters, may be fed into the LM to predict the probability distribution of each character, given the presence of previous characters. A loss function may be computed as a cross-entropy between the predicted probability distribution and the true next character (a one-hot vector). This character-level LM may be trained with standard back propagation through time. The training input may be a simple one-hot vector encoding the character index in the alphabet. Both the loss function and the gradients for a batch of sentences may be computed. In an embodiment, gradient descent or, in a more specific embodiment, stochastic gradient descent may be employed to update the model weights.</p><p id="p-0049" num="0048">In an embodiment, neural style transfer may be employed to good advantage. In neural transfer, a content image and a style image may be combined. There may be an input image to be styled. That may be the content image, or it may be another image. In the context of alphabets, character strings, and the like, a content image may be a letter, or a character. A style image might be a font, or a handwriting style. By imposing, or superimposing, or combining the content image and the style image, it may be possible to provide handwriting style for characters, even if there may be no specific handwriting example from a particular user.</p><p id="p-0050" num="0049">Indeed, since a customer's handwriting style may not be known beforehand, thus preventing the training of a targeted style transfer model, it must be possible to generalize to a new style during testing.</p><p id="p-0051" num="0050">There are bilinear models which are two factor models that can separate and/or fuse content and style in a flexible manner. In an embodiment, this type of property may be favorable in a transfer model. One such model, called EMD (encoder-mixer-decoder), utilizes a bilinear model as a content-style mixer in an encoder-decoder architecture. This model maps the content reference set and the style reference set to a target image with corresponding content (character class) and style.</p><p id="p-0052" num="0051">As will be explained below, in an embodiment such a model may be utilized to transfer the style of some handwriting character images to some font-rendered character images with desired content. In this manner, it is possible to generalize to new styles once the model is trained. In an embodiment, using such a model can obviate the need to retrain for a new style during testing.</p><p id="p-0053" num="0052">This ability to avoid retraining can be particularly useful because of the diversity of handwriting styles. In terms of available style images, for example, handwriting style can be so diverse as to be likely underrepresented in or even absent from training data.</p><p id="p-0054" num="0053">In an embodiment, the model is trained using both font-rendered character images and real handwritten character images. In an embodiment, a font set F may be selected as the style images. A handwritten character image set H also may have style information (such as the identification of the writer), but also provides character images, of course.</p><p id="p-0055" num="0054">In an embodiment, a handwritten character image T may be selected randomly as a target image from an image set S, with a character C in that image written by an individual P. In one aspect, a number of fonts, r, also may be selected randomly in order to render the character C. In this manner, a result is a content reference set comprising r images of the character C. At the same time, the same or a similar number r of images (in which characters may or may not be the character C) written by the individual P also may be randomly selected as the style reference set. These two sets&#x2014;the content reference set and the style reference set&#x2014;may be inputs to the model. A target image T may be taken as the ground truth for training the style transfer model.</p><p id="p-0056" num="0055">In an embodiment, instead of transferring styles among fonts, handwritten style (of handwritten character images) may be transferred to content images rendered by a font. The content images may be sampled using only font rendered images. The style images may be sampled only from handwritten character images. The trained EMD model then may be used to transfer a style of an individual's handwriting image to a font rendered image.</p><p id="p-0057" num="0056">In an embodiment, as discussed previously, in the generation of training samples during testing (using text generation and style transfer), training is not required.</p><p id="p-0058" num="0057">During testing, an initial input of characters may be provided to the character language model. Through repeated sampling, it is possible to generate relevant textual content. In an embodiment, such sampled characters as an output, in turn can become an input for a next iteration or step.</p><p id="p-0059" num="0058">In an embodiment, each character in a generated set of text may be considered to be one content target, to be rendered by r fonts randomly selected from the font set F. The r font images then form the content reference set, which is one of the two inputs to the style transferer. Thus, a content reference set may be a single character reproduced in r styles each corresponding to a different font.</p><p id="p-0060" num="0059">In turn, the targeted style may be specified by the style reference set, consisting of r character images segmented from scanned documents of an individual. In an embodiment, a handwriting character detection model may be used. Alternatively, characters can be segmented from document line images by one or more known character segmentation algorithms. Thus, a style reference set may be a plurality of r characters reproduced in a single style.</p><p id="p-0061" num="0060">After the content reference set and the style reference sets are generated, these may be fed into a style transfer model which will generate a character image with targeted content (character class) and handwriting style.</p><p id="p-0062" num="0061">After generating individual character images, various ones of the individual character images can be assembled (put together, or &#x201c;stitched&#x201d;) as line images. These line images can become final augmented samples to facilitate transfer learning. Corresponding content for each line image may be the ground truth label for that sample. Generating line images involves randomizing location, size, and other characteristics of individual character images as the character images are put into a sequence to form a line image. The randomizing of various character image characteristics can yield a more robust training set.</p><p id="p-0063" num="0062">Image stitching can connote a matching of size and format in order to provide a seamless line image. Such seamlessness may be desirable in some situations according to embodiments of the present invention. However, as just noted, generating lines of character images can involve randomization of various characteristics of individual character images. When the character images are assembled, the resulting line image may or may not reflect seamlessness in stitching. In fact, in some embodiments, because handwriting can have random variations for any number of reasons, it may be desirable not to normalize character images to enable seamless assembly of line images. In the case of kanji and similar characters, this may particularly be the case. In the case of other types of handwriting, for example, for English or other alphabets, seamlessness of stitching may be desirable in order to provide training sets that more nearly resemble cursive writing.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a high level flow chart for generated an augmented training set according to an embodiment. The process may start by selecting a first character C<sub>N</sub>. At <b>105</b> and <b>110</b>, this may be done by setting a counter N=1 and denoting the first character as C<sub>1</sub>. At <b>115</b>, from a selected font set F, a number r of fonts may be selected. At <b>120</b>, the character C<sub>N </sub>may be generated in each of the r fonts to create a content reference set. This generation may be carried out as an encoding process.</p><p id="p-0065" num="0064">In another aspect of the process, a first style M may be selected. At <b>125</b> and <b>130</b>, this may be done by setting a counter M=1. At <b>130</b>, style M is selected. At <b>135</b>, r images in that style M are generated to create a style reference set. In an embodiment, the images may be a plurality of characters or letters. This generation may be carried out as an encoding process.</p><p id="p-0066" num="0065">At <b>140</b>, style M is transferred to character C<sub>N </sub>in the r fonts to generate stylized characters. From the earlier discussion, this transfer may be carried out as a mixing process, using a bilinear model. At <b>145</b> and <b>150</b>, the process may repeat, returning flow to <b>110</b>, until all of the characters C<sub>N </sub>have been accounted for. Once this is done, at <b>155</b> a plurality P of generated stylized characters is selected from Q characters generated by the mixing process. From the earlier discussion, this may be understood as a decoding process. At <b>160</b>, an augmented training set may be formed from a random sequence of the P generated stylized characters. The process repeats by iterating through <b>155</b> and <b>160</b>, P characters at a time, so long as there are still at least P stylized characters to select. P need not be fixed but can be a random number at each iteration in order to generate samples with varied line length.</p><p id="p-0067" num="0066">In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the just-discussed sequence may be repeated for each of a plurality of styles, until all styles have been accounted for. For example, in <figref idref="DRAWINGS">FIGS. <b>1</b></figref>, at <b>180</b> and <b>185</b>, the process may repeat, returning flow to <b>130</b> for selection of another style, until all styles have been accounted for. Once that is accomplished, at <b>195</b> the process ends. However, for purposes of training a handwriting recognition system to recognize a particular handwriting style, the process may focus on just an augmented training set for that particular handwriting style. In this embodiment, it may be unnecessary to select multiple styles and iterate the process on the multiple styles.</p><p id="p-0068" num="0067">Ordinarily skilled artisans will appreciate that creation of a style reference set and a content reference set need not proceed in a particular order. The two sets may be generated in parallel, or one may be generated before the other. As another alternative, a plurality of such sets may be created in advance, with &#x201c;creation&#x201d; of a style reference set and/or a character reference set coming from selection of one such set from a plurality of such sets.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a high level block diagram of various elements of an apparatus <b>200</b> for generating augmented data sets according to embodiments. A content encoder <b>210</b> and a style encoder <b>220</b> respectively provide the content reference set and the style reference set mentioned above. In an embodiment, one or both of these encoders may employ downsampling by a series of convolution layers. In an embodiment, content encoder <b>210</b> may obtain the necessary text characters and fonts from storage (for example, storage <b>360</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>). Similarly, style encoder <b>220</b> may obtain the necessary images and styles from storage <b>360</b>.</p><p id="p-0070" num="0069">Mixer <b>230</b> receives the content reference set from content encoder <b>210</b> and the style reference set from style encoder <b>220</b> to transfer the style reflected in the style reference set to the characters in the content reference set. In an embodiment, the mixer may employ a bilinear model as discussed above.</p><p id="p-0071" num="0070">Decoder <b>240</b> receives an output of mixer <b>230</b>. In an embodiment, the decoder may employ upsampling by a series of deconvolution layers. Decoder <b>240</b> outputs stylized characters. In an embodiment, decoder may include a selector to enable selection of some of the stylized characters being output. Randomized stitcher <b>250</b> receives an output of decoder/selector <b>240</b> and randomizes the sequence of the selected stylized characters to output augmented data set <b>260</b>. Employing randomization can yield an enhanced data set which improves training of the handwriting recognition model.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a high level block diagram of a computing system <b>300</b> which, among other things, can use the augmented data set to train the handwriting recognition model according to an embodiment. System <b>300</b> may include the augmented data set generator <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The augmented data set generator <b>200</b> may communicate with a handwriting recognition system <b>400</b> directly, or may do so through a network <b>320</b>, which may be a wired or a wireless network or, in an embodiment, the cloud. System <b>300</b> also may include processing system <b>340</b> which in turn may include one or more processors, one or more storage devices, and one or more solid-state memory systems (which are different from the storage devices, and which may include both non-transitory and transitory memory). There also may be additional storage <b>360</b>, which may be accessible directly or via network <b>320</b>. Augmented data set generator <b>200</b> may communicate directly with storage <b>360</b>, or may do so via network <b>320</b>.</p><p id="p-0073" num="0072">In an embodiment, as noted earlier, storage <b>360</b> may provide content and fonts for content encoder <b>210</b>, and/or may provide images and styles for style encoder <b>220</b>. Computing system <b>300</b>, an in particular processing system <b>340</b>, may obtain the content and fonts, and images and styles stored in storage <b>360</b> by external means such as scanners, cameras, or other imaging equipment <b>345</b>. In an embodiment, processing system <b>340</b> may provide processing power for augmented data set generator <b>200</b>. Alternatively, the various elements of augmented data set generator <b>200</b> may employ their own processing units.</p><p id="p-0074" num="0073">Where network <b>320</b> is a cloud system for communication, one or more portions of computing system <b>300</b> may be remote from other portions. In an embodiment, even where the various elements are co-located, network <b>320</b> may be a cloud-based system.</p><p id="p-0075" num="0074">In an embodiment, the handwriting recognition system <b>400</b> may be based on a convolutional neural network (CNN), in conjunction with a bidirectional LSTM. Such an architecture is known to ordinarily skilled artisans as CRNN. In an embodiment, connectionist temporal classification (CTC) loss may computed and used to train the CRNN model. Ordinarily skilled artisans understand such a configuration to constitute an encoder, with no corresponding decoder structure. CTC loss computation is known to ordinarily skilled artisans. In a further embodiment, the handwriting recognition system <b>400</b> may be based on a CNN in combination with a transformer neural network. Ordinarily skilled artisans understand such a configuration to constitute an encoder-decoder combination. In an embodiment, cross-entropy loss, mentioned earlier, may be computed, and used to train the CNN+transformer model.</p><p id="p-0076" num="0075">The present application mentions several neural network-based architectures for implementing the handwriting recognition system <b>400</b>, as well as for implementing the augmented data set generator <b>200</b>. Ordinarily skilled artisans will appreciate the combination of augmented data set generator <b>200</b> and handwriting recognition system <b>400</b> yields beneficial effects according to aspects of the invention. Training of the handwriting recognition system <b>400</b> may be facilitated using the data sets that the augmented data set generator <b>200</b> generates. Depending on the specific application, this combination of augmented data set generator <b>200</b> and handwriting recognition system <b>400</b> may inform ordinarily skilled artisans of more specific versions of the neural network-based architectures discussed above. It is the combination itself that yields better results in a handwriting recognition system, without revealing an individual's personally sensitive information, or other confidential information that may be contained in training data sets that may be obtained from natural sources.</p><p id="p-0077" num="0076">Overall, a deep learning model in accordance with aspects of the invention may be implemented by one or more different types of neural networks, possibly in addition to the ones mentioned above, though the ones mentioned above would appear to work particularly well together. As noted, embodiments of the invention focus on the generation of training data to be provided to handwriting recognition models.</p><p id="p-0078" num="0077">While the foregoing describes embodiments according to aspects of the invention, the invention is not to be considered as limited to those embodiments or aspects. Ordinarily skilled artisans will appreciate variants of the invention within the scope and spirit of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method comprising:<claim-text>i) selecting a text character from a plurality of text characters;</claim-text><claim-text>ii) selecting a plurality of fonts from a font set;</claim-text><claim-text>iii) generating the text character in each of the plurality of fonts to generate a content reference set;</claim-text><claim-text>iv) selecting a plurality of images from an image set to generate a style reference set comprising a style selected from a plurality of styles;</claim-text><claim-text>v) transferring the style to the text character in each of the plurality of fonts to generate a set of stylized characters;</claim-text><claim-text>vi) selecting a plurality of stylized characters from the set;</claim-text><claim-text>vii) randomly sequencing the plurality of stylized characters to form an augmented data set; and</claim-text><claim-text>viii) applying the augmented data set to a handwriting recognition system as a training set.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising repeating i) to viii) for each of the plurality of text characters.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising repeating i) to viii) for each of the plurality of styles.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the content reference set is generated before the style reference set.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the style reference set is generated before the content reference set.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein at least one of the content reference set and the style reference set is selected from previously-generated reference sets.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising generating the text character from a language model comprising a recurrent neural network (RNN) or long short term memory (LSTM) model.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer-implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the RNN/LSTM LM model is trained by updating weights using cross-entropy loss calculation.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>ix) using the handwriting recognition system to recognize handwriting, wherein the handwriting recognition model employs a neural network based system selected from the group consisting of a convolutional neural network (CNN), in conjunction with a bidirectional LSTM (CRNN); and a CNN in combination with a transformer neural network.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computer-implemented method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the handwriting recognition model is trained by updating weights using a loss calculation selected from the group consisting of cross-entropy loss and connectionist temporal classification (CTC) loss.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An apparatus comprising:<claim-text>a neural network based augmented data set generator to generate an augmented data set, the augmented data set generator comprising:<claim-text>a content encoder to generate a content reference set comprising a text character selected from a plurality of text characters, the text character generated in each of a plurality of fonts;</claim-text><claim-text>a style encoder to generate a style reference set comprising a plurality of images comprising text characters represented in a style selected from a plurality of styles;</claim-text><claim-text>a mixer to transfer the style to the text character in each of the plurality of fonts to generate a set of stylized characters;</claim-text><claim-text>a selector to select a plurality of stylized characters from the set; and</claim-text><claim-text>a randomized stitcher to randomly sequence the plurality of stylized characters to generate the augmented data set;</claim-text></claim-text><claim-text>the apparatus further comprising a neural network based handwriting recognition system to receive the augmented data set from the augmented data set generator as a training set, and to recognize handwriting.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the augmented data set generator generates a plurality of augmented data sets, one for each of the plurality of text characters.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the augmented data set generator generates a plurality of augmented data sets, one for each of the plurality of styles.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the augmented data set generator comprises a language model implemented by a recurrent neural network (RNN) or long short term memory (LSTM) model to generate the text character.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the RNN/LSTM LM model is trained by updating weights using cross-entropy loss calculation.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the handwriting recognition system comprises a neural network based model selected from the group consisting of: a convolutional neural network (CNN), in conjunction with a bidirectional LSTM (CRNN); and a CNN in combination with a transformer neural network (CNN+transformer).</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the CRNN is trained using connectionist temporal classification (CTC) loss calculation.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the CNN+transformer is trained using cross-entropy loss calculation.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the mixer comprises a bilinear model to map the content reference set and the style reference set to generate the set of stylized characters.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising imaging equipment to provide some or all of the plurality of text characters used to generate the content reference set, and/or some or all of the plurality of images used to generate the style reference set.</claim-text></claim></claims></us-patent-application>