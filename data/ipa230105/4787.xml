<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004788A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004788</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17856530</doc-number><date>20220701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>7</main-group><subgroup>523</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>7</main-group><subgroup>523</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">HARDWARE ARCHITECTURE FOR PROCESSING TENSORS WITH ACTIVATION SPARSITY</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63218354</doc-number><date>20210704</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Numenta, Inc.</orgname><address><city>Redwood City</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Hunter</last-name><first-name>Kevin Lee</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Spracklen</last-name><first-name>Lawrence</first-name><address><city>Boulder Creek</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Ahmad</last-name><first-name>Subutai</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A hardware accelerator that is efficient at performing computations related to tensors. The hardware accelerator may store a complementary dense process tensor that is combined from a plurality of sparse process tensors. The plurality of sparse process tensors have non-overlapping locations of active values. The hardware accelerator may perform elementwise operations between the complementary dense process tensor and an activation tensor to generate a product tensor. The hardware accelerator may re-arrange the product tensor based on a permutation logic to separate the products into groups. Each group corresponds to one of the sparse process tensors. Each group may be accumulated separately to generate a plurality of output values. The output values may be selected in an activation selection. The activation selection may be a dense activation or a sparse activation such as k winner activation that set non-winners to zeros.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="93.56mm" wi="158.75mm" file="US20230004788A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="134.79mm" wi="172.89mm" file="US20230004788A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="230.80mm" wi="175.60mm" file="US20230004788A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="209.13mm" wi="109.81mm" file="US20230004788A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="217.59mm" wi="110.49mm" file="US20230004788A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="217.59mm" wi="110.49mm" file="US20230004788A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="209.13mm" wi="109.81mm" file="US20230004788A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="225.98mm" wi="173.31mm" file="US20230004788A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="241.05mm" wi="176.36mm" file="US20230004788A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="213.02mm" wi="131.83mm" file="US20230004788A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="244.77mm" wi="131.23mm" file="US20230004788A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="236.81mm" wi="133.01mm" file="US20230004788A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="226.82mm" wi="177.97mm" file="US20230004788A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="220.90mm" wi="173.31mm" file="US20230004788A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="220.90mm" wi="173.31mm" file="US20230004788A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="243.76mm" wi="170.94mm" orientation="landscape" file="US20230004788A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="242.40mm" wi="148.93mm" orientation="landscape" file="US20230004788A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="246.80mm" wi="151.05mm" orientation="landscape" file="US20230004788A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="215.98mm" wi="173.06mm" file="US20230004788A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="247.14mm" wi="168.99mm" orientation="landscape" file="US20230004788A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="241.64mm" wi="136.99mm" orientation="landscape" file="US20230004788A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="213.28mm" wi="169.67mm" file="US20230004788A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="214.71mm" wi="155.62mm" orientation="landscape" file="US20230004788A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="247.82mm" wi="168.23mm" orientation="landscape" file="US20230004788A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="244.18mm" wi="146.98mm" orientation="landscape" file="US20230004788A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="250.61mm" wi="132.00mm" orientation="landscape" file="US20230004788A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application claims the benefit of U.S. Provisional Patent Application 63/218,354, filed on Jul. 4, 2021, which is hereby incorporated by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE DISCLOSURE</heading><p id="p-0003" num="0002">The present disclosure relates to learning and processing tensors, and more specifically to hardware architecture that is efficient at performing operations related to sparse tensors.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">The use of artificial neural networks (ANN), or simply neural networks, includes a vast array of technologies. An ANN's complexity, in terms of the number of parameters, is growing exponentially at a faster rate than hardware performance. In many cases, an ANN may have a large number of parameters. Training and inference on these networks are bottlenecked by massive linear tensor operations, multiplication and convolution. Consequently, a large amount of time and/or resources may be used for both ANN creation (e.g., training) and execution (e.g., inference).</p><p id="p-0005" num="0004">Computing systems that execute ANNs often involve extensive computing operations including multiplication and accumulation. For example, CNN is a class of machine learning techniques that primarily uses convolution between input data and kernel data, which can be decomposed into multiplication and accumulation operations. Using a central processing unit (CPU) and its main memory to instantiate and execute machine learning systems or models of various configurations is relatively easy because such systems or models can be instantiated with mere updates to code. However, relying solely on the CPU for various operations of these machine learning systems or models would consume significant bandwidth of a central processing unit (CPU) as well as increase the overall power consumption.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">Embodiments relate to an accelerator for performing operations on tensors. The accelerator may include a plurality of multiply circuits configured to perform multiplications between values in a process tensor and values in an activation tensor to generate a plurality of products. The values in the process tensor are associated with tensor identifiers. The accelerator may also include a routing circuit configured to carry over the tensor identifiers of the values in the process tensor to the plurality of products and divide the plurality of products into subsets based on the tensor identifiers. The accelerator may also include a plurality of adder trees coupled to the routing circuit. Each adder tree is configured to receive a subset of the products that are grouped based on the tensor identifiers and accumulate the subset of the products to generate an output value. The plurality of adder trees is configured to generate a plurality of output values. The accelerator may further include an activation circuit coupled to the plurality of adder trees. The activation circuit is configured to select a subset of the output values as winners of an activation selection and set remaining of the plurality of output values as zero.</p><p id="p-0007" num="0006">In some embodiments, the activation circuit is further configured to boost one or more output values of the plurality of output values before the activation selection.</p><p id="p-0008" num="0007">In some embodiments, the one or more output values that are boosted correspond to one or more nodes that are set to zero in a previous cycle of operation.</p><p id="p-0009" num="0008">In some embodiments, the activation circuit is configured to select K output values as a number of output values in the subset that are selected as the winners and each of the tensor identifiers is used to identify one of the sparse process tensors.</p><p id="p-0010" num="0009">In some embodiments, the process tensor is a complementary dense process tensor that is combined from a plurality of sparse process tensors.</p><p id="p-0011" num="0010">In some embodiments, the routing circuit includes an arbiter circuit that controls routing of a product of the plurality of products to one of the adder trees.</p><p id="p-0012" num="0011">In some embodiments, the plurality of output values correspond to a plurality of channel dimension of the activation tensor.</p><p id="p-0013" num="0012">In some embodiments, the activation circuit includes a histogram memory that is configured to build a histogram that represents a distribution of the plurality of output values.</p><p id="p-0014" num="0013">In some embodiments, the activation circuit includes a sorting circuit configured to select the winners from serial bursts of the output values.</p><p id="p-0015" num="0014">In some embodiments, the activation circuit includes a sorting circuit configured to select the winners from the plurality of output values in parallel.</p><p id="p-0016" num="0015">In some embodiments, a computer-implemented method for operating on tensors may include combining a plurality of sparse process tensors to a complementary dense process tensor. The plurality of sparse process tensors have non-overlapping locations of active values. The method may also include performing computations between the complementary dense process tensor and an activation tensor to generate a plurality of products. The method may further include separating the plurality of products into groups, each group corresponding to one of the sparse process tensors.</p><p id="p-0017" num="0016">In some embodiments, a distribution of the active values in at least one of the sparse process tensors are partitioned.</p><p id="p-0018" num="0017">In some embodiments, the computations between the complementary dense process tensor and the activation tensor are performed by elementwise multiplications between values in the complementary dense process tensor and values in the activation tensor.</p><p id="p-0019" num="0018">In some embodiments, separating the plurality of products into groups includes a pre-multiplication re-arrangement of the activation tensor.</p><p id="p-0020" num="0019">In some embodiments, separating the plurality of products into groups includes a post-multiplication re-arrangement of the plurality of products.</p><p id="p-0021" num="0020">In some embodiments, the method may further include accumulating the groups of products to generate a plurality of accumulated values, each accumulated value corresponding to one of the sparse process tensors.</p><p id="p-0022" num="0021">In some embodiments, the method may further include selecting a subset of the plurality of accumulated values as winners of an activation selection of the sparse neural network; and setting remaining of the plurality of accumulated values as zero.</p><p id="p-0023" num="0022">In some embodiments, separating the plurality of products into groups includes flattening the plurality of products in a form of a tensor into a one-dimensional array and re-arranging the one-dimensional array to the groups of products corresponding to the sparse process tensors.</p><p id="p-0024" num="0023">In some embodiments, the plurality of sparse process tensors corresponds to a plurality of nodes of the sparse neural network.</p><p id="p-0025" num="0024">In some embodiments, the method may further include combining a second plurality of sparse process tensors to a second complementary dense process tensor, wherein the plurality of sparse process tensors and the second plurality of sparse process tensors both correspond to nodes in a layer of the sparse neural network.</p><p id="p-0026" num="0025">In some embodiments, an accelerator for performing operations on tensors may include a memory configured to store a complementary dense process tensor. The complementary dense process tensor may be generated from combining a plurality of sparse process tensors that have non-overlapping locations of active values. The accelerator may also include a computation core coupled to the memory. The computation core is configured to perform computations between two or more tensors to generate a product tensor. The two or more tensors include the complementary dense process tensor. The computation core may include a permutation circuit configured to re-arrange values in one of the two or more tensors or in the product tensor to group the values corresponding to one of the sparse process tensors together.</p><p id="p-0027" num="0026">In some embodiments, the computation core may also include a multiply circuit configured to perform multiplications between two or more tensors; and an adder tree configured to accumulate the values corresponding to the one of the sparse process tensors.</p><p id="p-0028" num="0027">In some embodiments, the permutation circuit is located upstream of the multiply circuit.</p><p id="p-0029" num="0028">In some embodiments, the permutation circuit is located downstream of the multiply circuit.</p><p id="p-0030" num="0029">In some embodiments, the permutation circuit is configured to re-arrange the values in an activation tensor, the activation tensor being one of the two or more tensors.</p><p id="p-0031" num="0030">In some embodiments, the permutation circuit is configured to re-arrange the values in the product tensor.</p><p id="p-0032" num="0031">In some embodiments, the active values in the plurality of sparse process tensors are partitioned, and the permutation circuit includes multiple permutation networks, each of the premutation networks is configured to re-arrange the values correspond a partition.</p><p id="p-0033" num="0032">In some embodiments, the permutation circuit includes a network of switches.</p><p id="p-0034" num="0033">In some embodiments, the values corresponding to the one of the sparse process tensors have the same tensor identifier and the permutation circuit is configured to group the values corresponding to the one of the sparse process tensors based on the tensor identifier.</p><p id="p-0035" num="0034">In some embodiments, the accelerator may further include an activation circuit configured to select k winners of outputs of the computation core as values in an output activation tensor.</p><p id="p-0036" num="0035">The features and advantages described in the specification are not all inclusive and, in particular, many additional features and advantages will be apparent to one of ordinary skill in the art in view of the drawings and specification. Moreover, it should be noted that the language used in the specification has been principally selected for readability and instructional purposes, and may not have been selected to delineate or circumscribe the inventive subject matter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0037" num="0036">The teachings of the embodiments of the present invention can be readily understood by considering the following detailed description in conjunction with the accompanying drawings.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a computing device, according to some embodiments.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a conceptual diagram illustrating an example architecture of a neural network, according to an embodiment.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a block diagram illustrating an example general operation of a node in a neural network, according to an embodiment.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>2</b>C through <b>2</b>F</figref> illustrates the concept of sparsity in a neural network, according to an embodiment.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating circuitry and hardware architecture of an example accelerator, according to an embodiment.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a conceptual diagram illustrating various examples of sparse tensors, according to an embodiment.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates several examples of pairings of sparse tensors, according to an embodiment.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a flowchart depicting an example process for performing operations related to complementary sparsity techniques in a sparse neural network, according to an embodiment.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a conceptual diagram that graphically illustrates a sparse neural network operation using complementary sparsity techniques, according to an embodiment.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> is a conceptual diagram that illustrates complemental sparsity techniques graphically using three 3&#xd7;3 sparse kernels as an example, according to an embodiment.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> illustrates an example accelerator that performs pre-multiplication permutation, according to an embodiment.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> illustrates an example accelerator that performs post-multiplication permutation, according to an embodiment.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>6</b>C</figref> is a conceptual diagram illustrating complementary sparsity techniques and a post-multiplication routing, according to an embodiment.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a conceptual diagram illustrating example circuitry that may be used in the permutation circuit, according to an embodiment.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a block diagram that illustrates circuitry that may be used for pre-multiplication routing, according to an embodiment.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a conceptual diagram that graphically illustrates a sparse neural network process using sparse activation, according to an embodiment.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a conceptual diagram illustrating the fetching of augmented process tensors and multiplications between process tensor values and activation values, according to an embodiment.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> illustrates that elementwise products are serially routed to the appropriate accumulator, according to an embodiment.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrates that elementwise products are routed in parallel to the appropriate adder-trees, according to an embodiment.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>10</b>C</figref> is a block diagram illustrating the structure of an example arbiter circuit, according to an embodiment.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>10</b>D</figref> is a conceptual diagram illustrating a prefix sum block in an arbiter circuit, according to an embodiment.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a conceptual diagram illustrating the circuitry of an example activation circuit and approach used for a parallel global K winner approach, according to an embodiment.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>12</b>A</figref> is a block diagram illustrating the sorting circuit for serially processing complementary sparse convolutions.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> is a block diagram illustrating the sorting circuit for parallel processing complementary sparse convolutions.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading><p id="p-0062" num="0061">In the following description of embodiments, numerous specific details are set forth in order to provide more thorough understanding. However, note that the present invention may be practiced without one or more of these specific details. In other instances, well-known features have not been described in detail to avoid unnecessarily complicating the description.</p><p id="p-0063" num="0062">A preferred embodiment is now described with reference to the figures where like reference numbers indicate identical or functionally similar elements. Also in the figures, the left-most digit of each reference number corresponds to the figure in which the reference number is first used.</p><p id="p-0064" num="0063">Reference in the specification to &#x201c;one embodiment&#x201d; or to &#x201c;an embodiment&#x201d; means that a particular feature, structure, or characteristic described in connection with the embodiments is included in at least one embodiment. The appearances of the phrase &#x201c;in one embodiment&#x201d; in various places in the specification are not necessarily all referring to the same embodiment.</p><p id="p-0065" num="0064">Some portions of the detailed description that follows are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here, and generally, conceived to be a self-consistent sequence of steps (instructions) leading to the desired result. The steps are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of electrical, magnetic or optical signals capable of being stored, transferred, combined, compared and otherwise manipulated. It is convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers, or the like. Furthermore, it is also convenient at times, to refer to certain arrangements of steps requiring physical manipulations of physical quantities as modules or code devices, without loss of generality.</p><p id="p-0066" num="0065">However, all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion, it is appreciated that throughout the description, discussions utilizing terms such as &#x201c;processing&#x201d; or &#x201c;computing&#x201d; or &#x201c;calculating&#x201d; or &#x201c;determining&#x201d; or &#x201c;displaying&#x201d; or &#x201c;determining&#x201d; or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system memories or registers or other such information storage, transmission or display devices.</p><p id="p-0067" num="0066">Certain aspects of the embodiments include process steps and instructions described herein in the form of an algorithm. It should be noted that the process steps and instructions of the embodiments could be embodied in software, firmware or hardware, and when embodied in software, could be downloaded to reside on and be operated from different platforms used by a variety of operating systems.</p><p id="p-0068" num="0067">Embodiments also relate to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for storing electronic instructions, and each coupled to a computer system bus. A computer readable medium is a non-transitory medium that does not include propagation signals and transient waves. Furthermore, the computers referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability. Various embodiments described may also be implemented as field-programmable gate arrays (FPGAs), which include hardware programmable devices that accept programming commands to execute the processing of input data.</p><p id="p-0069" num="0068">The algorithms and displays presented herein are not inherently related to any particular computer or other apparatus. Various general-purpose systems may also be used with programs in accordance with the teachings herein, or it may prove convenient to construct more specialized apparatus to perform the required method steps. The required structure for a variety of these systems will appear from the description below. In addition, embodiments are not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings as described herein, and any references below to specific languages are provided for disclosure of enablement and best mode of the embodiments.</p><p id="p-0070" num="0069">In addition, the language used in the specification has been principally selected for readability and instructional purposes, and may not have been selected to delineate or circumscribe the inventive subject matter. Accordingly, the disclosure set forth herein is intended to be illustrative, but not limiting, of the scope, which is set forth in the claims.</p><p id="p-0071" num="0070">Embodiments relate to architecture of an accelerator that is efficient at processing tensors associated with a sparse node. A sparse node may include a sparse tensor that has a low density of active values. In using a generic processor, the computation operation of a tensor, sparse or dense, may include computing the value in the tensor one by one. However, in a sparse tensor, since many values in the tensor are inactive (e.g., zeros) and computation with such inactive values can be skipped, the accelerator may determine the locations of active values in the tensor and perform computation efficiently so that the number of operations to process the tensor is reduced. In some embodiments, since the tensors may have a high degree of sparsity, the sparse tensors may be combined into a dense tensor so that computations of multiple tensors may be carried out in a single set of operations. The distribution of the active values in the sparse tensors may be arranged such that the active values among the sparse tensors to be combined are non-overlapping. The combined dense tensor may be referred to as a complementary dense tensor. Circuitry that improves the routing and re-arrangement of elements may be used to improve the efficiency in grouping and separating the active values back to sparse tensors after the combination.</p><heading id="h-0007" level="2">Example Computing Device Architecture</heading><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an example computing device <b>100</b> for processing one or more sparse neural networks, according to an embodiment. A computing device <b>100</b> may be a server computer, a personal computer, a portable electronic device, a wearable electronic device (e.g., a smartwatch), an IoT device (e.g., a sensor), smart/connected appliance (e.g., a refrigerator), dongle, a device in edge computing, a device with limited processing power, etc. The computing device <b>100</b> may include, among other components, a central processing unit (CPU) <b>102</b>, an accelerator <b>104</b> for performing tensor operations, a graphical processing unit (GPU) <b>106</b>, system memory <b>108</b>, a storage unit <b>110</b>, an input interface <b>114</b>, an output interface <b>116</b>, a network interface <b>118</b>, and a bus <b>120</b> connecting these components. In various embodiments, computing device <b>100</b> may include additional, fewer or different components. In some embodiments, the accelerator <b>104</b> (including examples of accelerators in subsequent figures) may also be referred to as artificial intelligence accelerator (AI accelerator).</p><p id="p-0073" num="0072">While some of the components in this disclosure may at times be described in a singular form while other components may be described in a plural form, various components described in any system may include one or more copies of the components. For example, a computing device <b>100</b> may include more than one processor such as CPU <b>102</b>, accelerator <b>104</b>, and GPU <b>106</b>, but the disclosure may refer the processors to as &#x201c;a processor&#x201d; or &#x201c;the processor.&#x201d; Also, a processor may include multiple cores.</p><p id="p-0074" num="0073">CPU <b>102</b> may be a general-purpose processor using any appropriate architecture. CPU <b>102</b> retrieves and executes computer code that includes instructions, when executed, that may cause CPU <b>102</b> or another processor, individually or in combination, to perform certain actions or processes that are described in this disclosure. Instructions can be any directions, commands, or orders that may be stored in different forms, such as equipment-readable instructions, programming instructions including source code, and other communication signals and orders. Instructions may be used in a general sense and are not limited to machine-readable codes. CPU <b>102</b> may be used to compile the instructions and also determine which processors may be used to performed certain tasks based on the commands in the instructions. For example, certain machine learning computations may be more efficient to be processed using accelerator <b>104</b> while other parallel computations may be better to be processed using GPU <b>106</b>.</p><p id="p-0075" num="0074">Accelerator <b>104</b> may be a processor that is efficient at performing certain machine learning operations such as tensor multiplications, convolutions, tensor dot products, etc. In various embodiments, accelerator <b>104</b> may have different hardware architectures. For example, in one embodiment, accelerator <b>104</b> may take the form of field-programmable gate arrays (FPGAs). In another embodiment, accelerator <b>104</b> may take the form of application-specific integrated circuits (ASICs), which may include circuits along or circuits in combination with firmware.</p><p id="p-0076" num="0075">GPU <b>106</b> may be a processor that includes highly parallel structures that are more efficient than CPU <b>102</b> at processing large blocks of data in parallel. GPU <b>106</b> may be used to process graphical data and accelerate certain graphical operations. In some cases, owing to its parallel nature, GPU <b>106</b> may also be used to process a large number of machine learning operations in parallel. GPU <b>106</b> is often efficient at performing the same type of workload many times in rapid succession.</p><p id="p-0077" num="0076">While, in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the processors CPU <b>102</b>, accelerator <b>104</b>, and GPU <b>106</b> are illustrated as separated components, in various embodiments the structure of one processor may be embedded in another processor. For example, one or more examples of the circuitry of accelerator <b>104</b> disclosed in different figures of this disclosure may be embedded in a CPU <b>102</b>. The processors may also be included in a single chip such as in a system-on-a-chip (SoC) implementation. In various embodiments, computing device <b>100</b> may also include additional processors for various specific purposes. In this disclosure, the various processors may be collectively referred to as &#x201c;processors&#x201d; or &#x201c;a processor.&#x201d;</p><p id="p-0078" num="0077">System memory <b>108</b> includes circuitry for storing instructions for execution by a processor and for storing data processed by the processor. System memory <b>180</b> may take the form of any type of memory structure including, for example, dynamic random access memory (DRAM), synchronous DRAM (SDRAM), double data rate (DDR, DDR2, DDR3, etc.) RAMBUS DRAM (RDRAM), static RAM (SRAM) or a combination thereof. System memory <b>108</b> usually takes the form of volatile memory.</p><p id="p-0079" num="0078">Storage unit <b>110</b> may be a persistent storage for storing data and software applications in a non-volatile manner. Storage unit <b>110</b> may take the form of read-only memory (ROM), hard drive, flash memory, or another type of non-volatile memory device. Storage unit <b>110</b> stores the operating system of the computing device <b>100</b>, various software applications <b>130</b> and machine learning models <b>140</b>. Storage unit <b>110</b> may store computer code that includes instructions that, when executed, cause a processor to perform one or more processes described in this disclosure.</p><p id="p-0080" num="0079">Applications <b>130</b> may be any suitable software applications that operate at the computing device <b>100</b>. An application <b>130</b> may be in communication with other devices via network interface <b>118</b>. Applications <b>130</b> may be of different types. In one case, an application <b>130</b> may be a web application, such as an application that runs on JavaScript. In another case, an application <b>130</b> may be a mobile application. For example, the mobile application may run on Swift for iOS and other APPLE operating systems or on Java or another suitable language for ANDROID systems. In yet another case, an application <b>130</b> may be a software program that operates on a desktop operating system such as LINUX, MICROSOFT WINDOWS, MAC OS, or CHROME OS. In yet another case, an application <b>130</b> may be a built-in application in an IoT device. An application <b>130</b> may include a graphical user interface (GUI) that visually renders data and information. An application <b>130</b> may include tools for training machine leaning models <b>140</b> and/or perform inference using the trained machine learning models <b>140</b>.</p><p id="p-0081" num="0080">Machine learning models <b>140</b> may include different types of algorithms for making inferences based on the training of the models. Examples of machine learning models <b>140</b> include regression models, random forest models, support vector machines (SVMs) such as kernel SVMs, and artificial neural networks (ANNs) such as convolutional network networks (CNNs), recurrent network networks (RNNs), autoencoders, long short term memory (LSTM), reinforcement learning (RL) models. Some of the machine learning models may include a sparse network structure whose detail will be further discussed with reference to <figref idref="DRAWINGS">FIG. <b>2</b>B through <b>2</b>D</figref>. A machine learning model <b>140</b> may be an independent model that is run by a processor. A machine learning model <b>140</b> may also be part of a software application <b>130</b>. Machine learning models <b>140</b> may perform various tasks.</p><p id="p-0082" num="0081">By way of example, a machine learning model <b>140</b> may receive sensed inputs representing images, videos, audio signals, sensor signals, data related to network traffic, financial transaction data, communication signals (e.g., emails, text messages and instant messages), documents, insurance records, biometric information, parameters for manufacturing process (e.g., semiconductor fabrication parameters), inventory patterns, energy or power usage patterns, data representing genes, results of scientific experiments or parameters associated with the operation of a machine (e.g., vehicle operation) and medical treatment data. The machine learning model <b>140</b> may process such inputs and produce an output representing, among others, identification of objects shown in an image, identification of recognized gestures, classification of digital images as pornographic or non-pornographic, identification of email messages as unsolicited bulk email (&#x2018;spam&#x2019;) or legitimate email (&#x2018;non-spam&#x2019;), prediction of a trend in financial market, prediction of failures in a large-scale power system, identification of a speaker in an audio recording, classification of loan applicants as good or bad credit risks, identification of network traffic as malicious or benign, identity of a person appearing in the image, processed natural language processing, weather forecast results, patterns of a person's behavior, control signals for machines (e.g., automatic vehicle navigation), gene expression and protein interactions, analytic information on access to resources on a network, parameters for optimizing a manufacturing process, predicted inventory, predicted energy usage in a building or facility, web analytics (e.g., predicting which link or advertisement that users are likely to click), identification of anomalous patterns in insurance records, prediction on results of experiments, indication of illness that a person is likely to experience, selection of contents that may be of interest to a user, indication on prediction of a person's behavior (e.g., ticket purchase, no-show behavior), prediction on election, prediction/detection of adverse events, a string of texts in the image, indication representing topic in text, and a summary of text or prediction on reaction to medical treatments. The underlying representation (e.g., photo, audio and etc.) can be stored in system memory <b>108</b> and/or storage unit <b>110</b>.</p><p id="p-0083" num="0082">Input interface <b>114</b> receives data from external sources such as sensor data or action information. Output interface <b>116</b> is a component for providing the result of computations in various forms (e.g., image or audio signals). Computing device <b>100</b> may include various types of input or output interfaces, such as displays, keyboards, cameras, microphones, speakers, antennas, fingerprint sensors, touch sensors, and other measurement sensors. Some input interface <b>114</b> may directly work with a machine learning model <b>140</b> to perform various functions. For example, a sensor may use a machine learning model <b>140</b> to infer interpretations of measurements. Output interface <b>116</b> may be in communication with humans, robotic agents or other computing devices.</p><p id="p-0084" num="0083">The network interface <b>118</b> enables the computing device <b>100</b> to communicate with other computing devices via a network. The networks may include, but are not limited to, Local Area Networks (LANs) (e.g., an Ethernet or corporate network) and Wide Area Networks (WANs). When multiple nodes or components of a single node of a machine learning model <b>140</b> is embodied in multiple computing devices, information associated with various processes in the machine learning model <b>140</b>, such as temporal sequencing, spatial pooling and management of nodes may be communicated between computing devices via the network interface <b>118</b>. Example Neural Network Architecture</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a conceptual diagram illustrating an example architecture of a neural network <b>200</b>, according to an embodiment. The illustrated neural network <b>200</b> shows a generic structure of a neural network. Neural network <b>200</b> may represent different types of neural networks, including convolutional network networks (CNNs), recurrent network networks (RNNs), autoencoders, and long short term memory (LSTM). In various embodiments, customized changes may be made to this general structure. Neural network <b>200</b> may also be a hierarchical temporal memory system as described, for example, in U.S. Patent Application Publication No. 2020/0097857, published on May 26, 2020, which is incorporated hereto by reference in its entirety.</p><p id="p-0086" num="0085">Neural network <b>200</b> includes an input layer <b>202</b>, an output layer <b>204</b> and one or more hidden layers <b>206</b>. Input layer <b>202</b> is the first layer of neural network <b>200</b>. Input layer <b>202</b> receives input data, such as image data, speech data, text, etc. Output layer <b>204</b> is the last layer of neural network <b>200</b>. Output layer <b>204</b> may generate one or more inferences in the form of classifications or probabilities. Neural network <b>200</b> may include any number of hidden layers <b>206</b>. Hidden layer <b>206</b> are intermediate layers in neural network <b>200</b> that perform various operations. Neural network <b>200</b> may include additional or fewer layers than the example shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>. Each layer may include one or more nodes <b>210</b>. The number of nodes in each layer in the neural network <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is an example only. A node <b>210</b> may be associated with certain weights and activation functions. In various embodiments, the nodes <b>210</b> in neural network <b>200</b> may be fully connected or partially connected.</p><p id="p-0087" num="0086">Each node <b>210</b> in neural network <b>200</b> may be associated with different operations. For example, in a simple form, neural network <b>200</b> may be a vanilla neural network whose nodes are each associated with a set of linear weight coefficients and an activation function. In another embodiment, neural network <b>200</b> may be an example convolutional neural network (CNN). In this example CNN, nodes <b>210</b> in one layer may be associated with convolution operations with kernels as weights that are adjustable in the training process. Nodes <b>210</b> in another layer may be associated with spatial pooling operations. In yet another embodiment, neural network <b>200</b> may be a recurrent neural network (RNN) whose nodes may be associated with more complicated structures such as loops and gates. In a neural network <b>200</b>, each node may represent a different structure and have different weight values and a different activation function.</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a block diagram illustrating an example general operation of a node <b>210</b> in neural network <b>200</b>, according to an embodiment. A node <b>210</b> may receive an input activation tensor <b>220</b>, which can be an N-dimensional tensor, where N can be greater than or equal to one. Input activation tensor <b>220</b> may be the input data of neural network <b>200</b> if node <b>210</b> is in the input layer <b>202</b>. Input activation tensor <b>220</b> may also be the output of another node in the preceding layer. Node <b>210</b> may apply a process tensor <b>222</b> to input activation tensor <b>220</b> in a linear operation <b>224</b>, such as addition, scaling, biasing, tensor multiplication, and convolution in the case of a CNN. The result of linear operation <b>224</b> may be processed by a non-linear activation <b>226</b> such as a step function, a sigmoid function, a hyperbolic tangent function (tanh), rectified linear unit functions (ReLU), or a sparsity activation such as a K-winner take all technique that will be discussed below. The result of the activation is an output activation tensor <b>228</b> that is sent to a subsequent connected node that is in the next layer of neural network <b>200</b>. The subsequent node uses output activation tensor <b>228</b> as the input activation tensor <b>220</b>. Here, the process tensor <b>222</b> may be a name for a tensor that includes one or more parameters in an algorithm and the values in the process tensor <b>222</b> often include weight values in a machine learning model but are not limited to weights. A process tensor throughout this disclosure may also be referred to as a weight tensor.</p><p id="p-0089" num="0088">In various embodiments, a wide variety of machine learning techniques may be used in training neural network <b>200</b>. Neural network <b>200</b> may be associated with an objective function (also commonly referred to as a loss function), which generates a metric value that describes the objective goal of the training process. The training may intend to reduce the error rate of the model in generating predictions. In such a case, the objective function may monitor the error rate of neural network <b>200</b>. For example, in object recognition (e.g., object detection and classification), the objective function of neural network <b>200</b> may be the training error rate in classifying objects in a training set. Other forms of objective functions may also be used. In various embodiments, the error rate may be measured as cross-entropy loss, L1 loss (e.g., the sum of absolute differences between the predicted values and the actual value), L2 loss (e.g., the sum of squared distances) or their combinations.</p><p id="p-0090" num="0089">The weights and coefficients in activation functions of neural network may be adjusted by training and also be constrained by sparsity and structural requirements. Sparsity will be further discussed with reference to <figref idref="DRAWINGS">FIG. <b>2</b>C through <b>2</b>F</figref> and example structural requirements will be further discussed with reference to <figref idref="DRAWINGS">FIGS. <b>4</b>A, <b>4</b>B, and <b>5</b>B</figref>. Training of neural network <b>200</b> may include forward propagation and backpropagation. In forward propagation, neural network <b>200</b> performs the computation in the forward direction based on outputs of a preceding layer. The operation of a node <b>210</b> may be defined by one or more functions, such as linear operation <b>224</b> and non-linear activation <b>226</b>. The functions that define the operation of a node <b>210</b> may include various computation operations such as convolution of data with one or more kernels, pooling, recurrent loop in RNN, various gates in LSTM, etc. The functions may also include an activation function that adjusts the output of the node.</p><p id="p-0091" num="0090">Each of the functions in neural network <b>200</b> may be associated with different weights (e.g., coefficients and kernel coefficients) that are adjustable during training. After an input is provided to neural network <b>200</b> and passes through neural network <b>200</b> in the forward direction, the results may be compared to the training labels or other values in the training set to determine the neural network's performance. The process of prediction may be repeated for other samples in the training sets to compute the overall value of the objective function in a particular training round. In turn, neural network <b>200</b> performs backpropagation by using gradient descent such as stochastic gradient descent (SGD) to adjust the coefficients in various functions to improve the value of the objective function.</p><p id="p-0092" num="0091">Multiple rounds of forward propagation and backpropagation may be performed. Training may be completed when the objective function has become sufficiently stable (e.g., neural network <b>200</b> has converged) or after a predetermined number of rounds for a particular set of training samples. The trained neural network <b>200</b> can be used for making inferences or another suitable task for which the model is trained.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>2</b>C through <b>2</b>F</figref> illustrates the concept of sparsity in a neural network <b>200</b>, according to various embodiments. Each of <figref idref="DRAWINGS">FIGS. <b>2</b>C, <b>2</b>D, <b>2</b>E, and <b>2</b>F</figref> shows the operation within a node <b>210</b> with different degrees of sparsity and is a graphical illustration of the flowchart shown in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>. A circle in <figref idref="DRAWINGS">FIGS. <b>2</b>C, <b>2</b>D, <b>2</b>E, and <b>2</b>F</figref> represents a value in a tensor. In a neural network <b>200</b> with L hidden layers, the notation y<sup>l </sup>denotes output activation tensor <b>228</b> from layer l and y<sup>l-1 </sup>denotes the output activation tensor <b>228</b> in the preceding layer l&#x2212;1 or the input activation tensor <b>220</b> of layer l. W<sup>l </sup>and u<sup>l </sup>represent respectively the process tensor <b>222</b> and biases for each node. In a neural network node <b>210</b> that has a dense process tensor W<sup>l</sup>, the feed-forward outputs are calculated as follow:</p><p id="p-0094" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>&#x177;</i><sup>l</sup><i>=W</i><sup>l</sup><i>&#xb7;y</i><sup>l-1</sup><i>+u</i><sup>l</sup>&#x2003;&#x2003;Equation 1<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0095" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>y</i><sup>l</sup><i>=f</i>(<i>&#x177;</i><sup>l</sup>)&#x2003;&#x2003;Equation 2<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0096" num="0000">where f is any activation function, such as tanh or ReLU and &#x177;<sup>l </sup>is the output of the linear operation before an activation function is applied.</p><p id="p-0097" num="0093">The above relationship may be conceptually represented as a block diagram as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>. Graphically, a dense node with dense weights and a dense activation function such as tanh or ReLU is illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>. In <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>, the result &#x177;<sup>l </sup>after the linear operation is dense with most of the values being non-zero. The active values (e.g., non-zero values) are represented by the shaded circles. The activation function also results in a dense output y<sup>l </sup>in which a majority of the values are still active, which are also represented by the shaded circles.</p><p id="p-0098" num="0094">Here, a value being active may refer to a value whose mathematical operation will need to be included in order to perform the overall computation. For example, in the context of matrix multiplication, convolution, or dot product, an active value may be a non-zero value because the mathematical operation, such as addition and multiplication, of the non-zero value will need to be included in order to get to the correct result of the matrix multiplication, convolution, or dot product. A value being inactive may refer to a value whose mathematical operation may be skipped. For example, in the context of matrix multiplication, convolution, or dot product, an inactive value is zero because the mathematical operation involving zero, such as addition and multiplication, may be skipped without affecting the final result. A process tensor is dense if the percentage of active values in the tensor exceeds a threshold. Likewise, an activation is dense if the activation function will result in a number of output values in the output activation tensor y<sup>l </sup>being dense and the percentage of the active values exceeding a threshold. Using ReLU as an example, ReLU sets values that are lower than a level (e.g., 0) as 0 and allows values that are greater than the level to retain the values. Hence, it is expected that ReLU will generate about half active values if the values in the intermediate tensor &#x177;<sup>l </sup>are roughly equally distributed around the level. A tensor output that has about half of the values being non-zero is often considered as dense. In <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>, since the process tensor is dense and the activation layer will also generate a dense value, the node <b>240</b> can be considered as a weight-dense and activation-dense node <b>240</b>, or simply referred to as dense-dense node <b>240</b>.</p><p id="p-0099" num="0095">The degree of sparsity for a tensor to be considered sparse may vary, depending on embodiments. In one embodiment, the number of active values in a tensor is fewer than 50% to be considered a sparse tensor. In one embodiment, the number of active values in a tensor is fewer than 40% to be considered a sparse tensor. In one embodiment, the number of active values in a tensor is fewer than 30% to be considered a sparse tensor. In one embodiment, the number of active values in a tensor is fewer than 20% to be considered a sparse tensor. The number of active values in a tensor is fewer than 15% to be considered a sparse tensor. The number of active values in a tensor is fewer than 10% to be considered a sparse tensor. The number of active values in a tensor is fewer than 5% to be considered a sparse tensor. The number of active values in a tensor is fewer than 4% to be considered a sparse tensor. The number of active values in a tensor is fewer than 3% to be considered a sparse tensor. The number of active values in a tensor is fewer than 3% to be considered a sparse tensor. The number of active values in a tensor is fewer than 2% to be considered a sparse tensor. The number of active values in a tensor is fewer than 1% to be considered a sparse tensor. The number of active values in a tensor is fewer than 0.8% to be considered a sparse tensor. The number of active values in a tensor is fewer than 0.5% to be considered a sparse tensor. The number of active values in a tensor is fewer than 0.2% to be considered a sparse tensor. The number of active values in a tensor is fewer than 0.1% to be considered a sparse tensor. The number of active values in a tensor is fewer than 0.01% to be considered a sparse tensor.</p><p id="p-0100" num="0096"><figref idref="DRAWINGS">FIG. <b>2</b>D</figref> is a conceptual diagram that illustrates a sparse-dense node <b>250</b>, according to an embodiment. Compared to the node <b>240</b> in <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>, node <b>250</b> has sparse weights that are illustrated by having much fewer connected lines. Despite being illustrated as a dense tensor, the input y<sup>l-1 </sup>can be a dense tensor or a sparse tensor, depending on the previous node's sparsity. The weights of this node <b>240</b> are sparse, meaning there are a large number of inactive values (e.g., zeros) in the process tensor. A sparse process tensor may be achieved by imposing a constraint on node <b>240</b> to limit the maximum number of active values in the process tensor. After the linear operation, the intermediate tensor &#x177;<sup>l </sup>is likely to be dense because the linear operation, such as tensor multiplication or convolution, likely spreads the number of active values in the tensor. After the linear operation, the non-linear activation <b>226</b> step is the same as the node <b>240</b> in <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>. For example, the ReLU activation function will get around half of the values as zeros. Overall, the output tensor y<sup>l </sup>is still a dense tensor since about half of the values are dense. In this example, node <b>250</b> may be referred to as a weight-sparse and activation-dense node or simply sparse-dense node. The process tensors being sparse may be referred to as weight sparsity. Techniques and hardware architectures related to weight sparsity such as complementary sparsity are further discussed in <figref idref="DRAWINGS">FIG. <b>5</b>A through <b>7</b>B</figref>.</p><p id="p-0101" num="0097"><figref idref="DRAWINGS">FIG. <b>2</b>E</figref> is a conceptual diagram that illustrates a sparse-sparse node <b>260</b>, according to an embodiment. Compared to node <b>250</b> in <figref idref="DRAWINGS">FIG. <b>2</b>D</figref>, node <b>260</b> also has sparse weights, but it also has a sparse activation function that generates a sparse output. The input y<sup>l-1 </sup>can be a dense tensor or a sparse tensor, depending on the previous node's sparsity. In this example, the input y<sup>l-1 </sup>is illustrated as a sparse tensor. Even though the weights of this node <b>260</b> are sparse, after the linear operation, the intermediate tensor &#x177;<sup>l </sup>is likely to be dense because the linear operation likely spreads the number of non-zero values in the tensor. After the linear operation, a sparse activation function called K-winner activation is used instead of a dense activation such as ReLU activation function. K-winner activation selects the top K values in the intermediate tensor &#x177;<sup>l </sup>and force all other values, non-zero or not, to zeros. K may be a constraint set to maintain the sparsity of node <b>260</b> and may be set as a percentage of the total number of values in a tensor. For example, K may be 30%, 20%, 15%, 10%, 5%, etc., depending on the selection. The output tensor y<sup>l </sup>is a sparse tensor after the K-winner activation function that restrains the number of active values in the tensor. In this example, node <b>260</b> may be referred to as a weight-sparse and activation-sparse node or simply sparse-sparse node. Using a sparse activation function in a node may be referred to as activation sparsity. Techniques and hardware architectures related to activation sparsity are further discussed in <figref idref="DRAWINGS">FIG. <b>8</b> through <b>12</b>B</figref>. <figref idref="DRAWINGS">FIG. <b>2</b>F</figref> is a conceptual diagram that illustrates a dense-sparse node <b>270</b>, according to an embodiment. Node <b>270</b> has dense weights, but it has a sparse activation function that generates a sparse output.</p><p id="p-0102" num="0098">Neural network <b>200</b> with one or more nodes that have the sparse-dense or sparse-sparse structure may be referred to as a sparse neural network. A sparse neural network may be a hierarchical temporal memory system. In various embodiments, while a sparse neural network may include a large number of sparse nodes, the sparse neural network may also include some dense nodes. Also, a sparse node may be a sparse-sparse node <b>260</b> or a sparse-dense node <b>250</b>. In some embodiments, a node may also be with either weight sparsity or activation sparsity.</p><p id="p-0103" num="0099">A sparse neural network often has improved performance in terms of speed in training and inference because the large number of inactive values in the network allows the network to skip many mathematical operations. For example, many common operations in neural networks, such as convolution and tensor multiplication, may be converted to dot products. Oftentimes a processor uses dot products to compute those operations in neural networks. Zeros in the tensors will significantly simplify the number of multiplications and additions needed to perform in a dot product. In many cases, sparse neural networks may model the structure of a human brain, which appears to also rely on a large degree of sparsity. Those sparse neural networks often not only have improved speed compared to dense neural networks but also increase inference accuracy particularly in the cases of noisy environments. For example, sparse neural networks reduce the number of parameters necessary to achieve an equivalent result accuracy, leading to savings in computational infrastructure, execution time, latency, power and therefore costs. They also exhibit increased robustness to noise in real-world situations. In Edge and IoT applications, a sparse network may fit on a limited deployment platform where an equivalent dense network would not.</p><heading id="h-0008" level="2">Example Circuitry for AI Accelerator</heading><p id="p-0104" num="0100"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating circuitry and hardware architecture of an example accelerator <b>300</b>, according to an embodiment. accelerator <b>300</b> may be a circuit that is efficient at performing operations related to a sparse neural network. Accelerator <b>300</b> may be an example of accelerator <b>104</b> or may also be embedded as part of a larger processor, such as CPU <b>102</b>. In various embodiments, accelerator <b>300</b> may include fewer or additional components than the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. For example, in one embodiment, accelerator <b>300</b> shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> only illustrates blocks that are relevant to computations related to accelerating the operation of a sparse neural network and other components may not be shown. In one embodiment, accelerator <b>300</b> includes internal memory <b>310</b> and one or more computation cores <b>320</b> that perform computations in parallel.</p><p id="p-0105" num="0101">Internal memory <b>310</b> may be the dedicated memory for accelerator <b>300</b> that is used for storage of data fetched from system memory <b>108</b> and data outputted by computation cores <b>320</b>. The data stored in internal memory <b>310</b> may include input data of neural network <b>200</b>, weights and other coefficients in neural network <b>200</b>, intermediate data of neural network <b>200</b>, such as output activation tensor <b>228</b> that is outputted by each node <b>210</b>, loss function coefficients, and other suitable data that are related to the operation of neural network <b>200</b>. For each node <b>210</b>, input activation tensor <b>220</b> may be saved in internal memory <b>310</b>. The input activation tensor <b>220</b> may be divided into multiple units and are sent to various computation cores <b>320</b> to process in parallel. The outputs of computation cores <b>320</b><i>s </i>may be recombined as output activation tensor <b>228</b>, which is an output of a node <b>210</b>. After the operations of the nodes <b>210</b> in a layer of neural network <b>200</b> are completed, operations of nodes <b>210</b> in the next layer may begin. The output activation tensor <b>228</b> is then fetched again to one or more computation core <b>320</b> as the input activation tensor <b>220</b> of a succeeding node <b>210</b> in the next layer. The process repeats until the operations reach the output layer <b>204</b>. In some embodiments, the data stored in internal memory <b>310</b> may be sparse tensors that include zeros in various locations. In some embodiments, some data in internal memory <b>310</b> may also be compressed to dense tensors by removing zeros in the tensors. Compression of sparse tensors will be discussed in further detail.</p><p id="p-0106" num="0102">In some embodiments, an accelerator <b>300</b> may not need to include internal memory <b>310</b>. Instead, data are directly fetched and written to the system memory <b>108</b>.</p><p id="p-0107" num="0103">A computation core <b>320</b> is a circuit that performs computations between two or more tensors. The tensors may be a process tensor and an activation tensor. The computation core <b>320</b> may include a number of multiply circuits <b>330</b> that perform tensor operations such as the multiplications part of dot products, tensor multiplications, convolutions. Common machine learning operations such as tensor multiplications and convolutions may be converted to dot products and be performed by multiply circuits <b>330</b>. A computation core <b>320</b> may include a number of multiply circuits for performing computations in parallel.</p><p id="p-0108" num="0104">A multiply circuit <b>330</b> may take various forms. In one embodiment, a multiply circuit <b>330</b> is a multiply-accumulate circuit (MAC) that includes multiply units and accumulators. The multiply units may be used to perform multiplications and additions. A multiply unit is a circuit with a known structure and may be used for binary multiplication or floating-point multiplication. An accumulator is a memory circuit that receives and stores values from the multiply units. The values may be stored individually or added together in the accumulator. In some embodiments, the multiply circuits <b>330</b> may only include multiply units and perform elementwise multiplications.</p><p id="p-0109" num="0105">Computation core <b>320</b> may include circuitry upstream of multiply circuits <b>330</b> for pre-processing of various tensors such as by dividing an input activation tensor into smaller units and by compressing and converting sparse tensors to a form that is efficient for the multiply circuits <b>330</b> to process. An activation buffer <b>352</b> is a buffer circuit and related data-processing circuit for performing data processing of an input activation tensor <b>220</b> for a node <b>210</b>. For example, normally an input activation tensor <b>220</b> may have a size that is significantly larger than the capacity of a multiply circuit <b>330</b>. The input activation tensor <b>220</b> may be divided into multiple data subunits and be processed in parallel by different multiply circuits <b>330</b>. Activation buffer <b>352</b> may include circuitry that divides the input activation tensor <b>220</b> or include different addresses for various multiply circuits <b>330</b> to fetch different portions of the input activation tensor <b>220</b>. In some embodiments, activation buffer <b>352</b> may fetch the tensor values from internal memory <b>310</b>. In some cases, only the active values are fetched to activation buffer <b>352</b>.</p><p id="p-0110" num="0106">Activation buffer <b>352</b> may also perform a transpose operation of the input activation tensor <b>220</b> by fetching data values in the input activation tensor <b>220</b> in an order different from the order in internal memory <b>310</b>. In some cases, an input activation tensor <b>220</b> may be saved in internal memory <b>310</b> under certain dimensions such as X by Y by Z while the division of data subunits may be more efficient under the dimension Y by Z by X. The efficiency of storage and operation of data under certain dimensions may depend on the hardware landscape such as the multiplier arrangement in a multiply circuit <b>330</b> and memory structure.</p><p id="p-0111" num="0107">A weight buffer <b>350</b> and pre-processing circuit <b>354</b> are other examples of circuitry upstream of multiply circuits <b>330</b> for pre-processing of various tensors. For an operation with respect to a given node <b>210</b> in neural network <b>200</b>, weight buffer <b>350</b> fetches the tensor values of process tensor <b>222</b> from internal memory <b>310</b> or system memory <b>108</b>. Similar to activation buffer <b>352</b>, in some cases weight buffer <b>350</b> may only fetch the active values in process tensor <b>222</b>.</p><p id="p-0112" num="0108">Pre-processing circuit <b>354</b> may include different types of circuits that are used to pre-process process tensor <b>222</b> and input activation tensor <b>220</b>. Process tensor <b>222</b> and input activation tensor <b>220</b> may be associated with different degrees of sparsity. For example, in one case, process tensor <b>222</b> may be sparse while input activation tensor <b>220</b> may be dense. In another case, both process tensor <b>222</b> and input activation tensor <b>220</b> may be sparse. In yet another case, process tensor <b>222</b> may be dense and input activation tensor <b>220</b> may be sparse. Pre-processing circuit <b>354</b> may pre-process process tensor <b>222</b> and input activation tensor <b>220</b> in different ways, depending on their sparsity. For example, in some embodiments, process tensor <b>222</b> and input activation tensor <b>220</b> may be processed separately. In some embodiments, when both process tensor <b>222</b> and input activation tensor <b>220</b> are sparse, pre-processing circuit <b>354</b> may process the two tensors together.</p><p id="p-0113" num="0109">In some embodiments, pre-processing carried out by the pre-processing circuit <b>354</b> may include identifying locations of active values in the process tensor <b>222</b> and input activation tensor <b>220</b>. Pre-processing circuit <b>354</b> may scan through a sparse tensor and identify the locations of the active values in the sparse tensor. The locations may take the form of the locations in the tensor (e.g., a location at the third row and the fifth column in the tensor) and may also take the form of memory addresses of active values (e.g., an active value being saved in the memory address of 0xC0010000). Pre-processing circuit <b>354</b> may only transmit the active values to multiply circuits <b>330</b> for computations. In some embodiments, pre-processing circuit <b>354</b> may identify dense pairs that have active values at the same tensor location in both process tensor <b>222</b> and input activation tensor <b>220</b>. Pre-processing circuit <b>354</b> may only transmit the dense pairs to multiply circuits <b>330</b> for computations. In other words, in some cases, pre-processing circuit <b>354</b> may exclude the transmission of inactive values in process tensor <b>222</b> or input activation tensor <b>220</b> to multiply circuits <b>330</b>.</p><p id="p-0114" num="0110">In some embodiments, pre-processing carried out by the pre-processing circuit <b>354</b> may also include compress a sparse tensor or combine multiple sparse tensors to a dense tensor. In various computations such as dot products and other multiplications, the results will be zero if one of the input values is zero. As such, the processing of those inactive values may be skipped in the multiply circuits <b>330</b>. In some cases, when two tensors are multiplied, only multiplications of two active values are to be computed. As such, in some embodiments, pre-processing circuit <b>354</b> may compress a sparse tensor by converting the sparse tensor into a smaller-size dense tensor. In some embodiments, such as in complementary sparsity that will be discussed in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> through <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, pre-processing circuit may combine multiple sparse tensor that have non-overlapping active values to a dense tensor. In some embodiments, the compression or combination of tensors to generate a dense tensor may be performed offline and outside the accelerator <b>300</b> instead of at a pre-processing stage. The compressed or combined tensor may be stored in a memory such as the system memory. The number of multiplication operations to be performed by multiply circuits <b>330</b> may be significantly reduced after inactive values are removed from the tensors. By way of example, if a dot product is performed between two sparse tensors that each has about 10% of active values, it is expected to only 1% of the multiplication operations will need to be performed. The rest of the positions are either the multiplications of two zeros or multiplications of a non-zero value and a zero. By removing the inactive value (e.g., zeros) in the tensors, pre-processing circuit <b>354</b> may speed up the computations for multiply circuits <b>330</b>. In some embodiments, the tensors fetched to pre-processing circuit <b>354</b> may also be structured so that pre-processing circuit <b>354</b> can remove the zeros in those tensors more efficiently. The structure of tensors will be further discussed in <figref idref="DRAWINGS">FIGS. <b>4</b>A, <b>4</b>B, and <b>5</b>A</figref>.</p><p id="p-0115" num="0111">In some embodiments, pre-processing circuit <b>354</b> may also store the addresses of active values in the tensors so that the dense tensors and output tensors generated by multiply circuits <b>330</b> may be convert back to sparse tensors. For example, in complementary sparsity, multiple sparse process tensors may be combined as a dense process tensor. Pre-processing circuit <b>354</b> may use a state vector and tensor identifiers keep track of which locations correspond to which sparse process tensors. Pre-processing circuit <b>354</b> may function as a permutation circuit that re-routes and re-arranges values in various tensors so that values in a combined tensor may be grouped based on the corresponding sparse tensors. Example structures and operations of permutation circuits are further discussed in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> through <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>.</p><p id="p-0116" num="0112">Pre-processing circuit <b>354</b> may also perform other data pre-processing such as transposing process tensor <b>222</b> and input activation tensor <b>220</b>. Pre-processing circuit <b>354</b> may also subdivide the tensors in a way that is efficient for multiply circuits <b>330</b> to process. The pre-processed tensors are fetched and sent to multiply circuits <b>330</b> to perform computations with input activation tensor <b>220</b>.</p><p id="p-0117" num="0113">After results of multiply circuits <b>330</b> are computed, the results are sent to one or more adder trees <b>360</b> to generate an intermediate output tensor &#x177;<sup>l</sup>. The results (products) of the multiply circuits <b>330</b> are then combined in adder trees <b>360</b>. For example, in performing a dot product, multiply circuits <b>330</b> perform the multiplication and accumulation parts of the dot product and the results of different multiply circuits <b>330</b> are added together at the adder tree <b>360</b> to generate the final result. Alternatively, the accumulation parts may be performed in the adder tree, depending on the hardware architecture and the operations. In some embodiments, input activation tensor <b>220</b> is divided into multiple subunits for parallel processing in the multiply circuits <b>330</b>. In some embodiments, for complementary sparsity, the products of the multiply circuits <b>330</b> are re-arranged by permutation circuits so that values corresponding to the same sparse process tensor are sent to the same adder tree <b>360</b>. In some embodiments the computations performed on the sparse tensors are not multiplication and addition, but any pair of computations. In those embodiments, the multiply circuits <b>330</b> may be replaced by or reconfigured to a first computation circuit computing the first operator, and the adder tree may be replaced by or reconfigured to a second computation circuit computing the second operator. In some embodiments, the multiply circuits <b>330</b> (in <figref idref="DRAWINGS">FIG. <b>3</b></figref> and other subsequent figures) may be referred to as a first computation circuit. The adder tree <b>360</b> (in <figref idref="DRAWINGS">FIG. <b>3</b></figref> and other subsequent figures) may be referred to as a second computation circuit or a reduction tree. Likewise, the term &#x201c;products&#x201d; are not limited to multiplication products and includes any results of a computation.</p><p id="p-0118" num="0114">An activation circuit <b>370</b> is a circuit downstream of adder tree <b>360</b> to perform the operation specified in the activation function. Activation may be dense or sparse. Examples of dense activation include more conventional activations such as ReLU and tanh. Examples of sparse activation include K-winner take all (k-WTA) activation that will be discussed in further details. Activation circuit <b>370</b> may include a number of comparator circuits that are used for the ReLU activation function. Activation circuit <b>370</b> may also include comparator trees for determining top K highest values in a tensor in the case of a sparse K-winner activation function. Activation circuit <b>370</b> generates the output activation tensor <b>228</b> from the intermediate output tensor. Activation circuit <b>370</b> may set a number of values in the intermediate output tensor to zero, depending on the type of activation function. Hence, output activation tensor <b>228</b> may be a dense or sparse tensor. In some embodiments, one or more input tensors are previously compressed, activation circuit <b>370</b> may also expand the output activation tensor <b>228</b> back to the original size. Output activation tensor <b>228</b> is transmitted to internal memory <b>310</b> or system memory <b>108</b> as the output of a particular node <b>210</b>. The output activation tensor <b>228</b> is fetched subsequently as input activation tensor <b>220</b> when another round of operations related to a subsequent node <b>210</b> begins. In some embodiments, the output activation tensor <b>228</b> may be directly sent within in the accelerator <b>300</b> as the input activation tensor of the next round, as represented by arrow <b>372</b>. Example structures of the activation circuit <b>370</b> are further discussed in <figref idref="DRAWINGS">FIGS. <b>12</b>A and <b>12</b>B</figref>.</p><p id="p-0119" num="0115">In some cases, bias factors <b>364</b> may also be fetched from internal memory <b>310</b>. The bias factors <b>364</b> may be added or multiplied to some of the output values of some adder trees <b>360</b>. For example, in some cases, boosting techniques may be used in association with a k-WTA activation. The k-WTA activation select the highest k output values among the nodes (or among a set or a partition) and set all other output values to zeros. In some cases, values corresponding certain nodes, such as nodes that are not selected in previous rounds of prediction or training, are manually boosted by increasing the output values. The boosting is used to increase the chances of some less frequently selected nodes to be selected in the k-WTA activation scheme. The magnitude of the boosting for each node may be a hyperparameter that is configurable or may be learned in the training.</p><p id="p-0120" num="0116">The use of a sparse neural network and an accelerator that is efficient at operating with the sparse neural network reduces the number of computations and power consumptions of the accelerator. The sparse neural network also reduces storage requirements and working memory bandwidth. The accelerator improves the speed of a computing device and is suitable for use in computing devices that have limited power or computing capacity, such as IoT devices and in the case of edge computing.</p><heading id="h-0009" level="2">Example Structured Sparse Tensor Configurations</heading><p id="p-0121" num="0117"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a conceptual diagram illustrating various examples of sparse tensors, according to an embodiment. Structured sparse tensors may be used as the data format of a sparse neural network. Such a format is efficient for an accelerator <b>300</b> to perform computations. For example, a properly structured tensor with active values arranged in a certain structured manner may allow pre-processing circuit <b>354</b> to process and compress the tensor in an efficient manner, thereby further speeding up the neural network.</p><p id="p-0122" num="0118">Tensor <b>402</b> is an example unstructured tensor. Tensor <b>402</b> and various tensors in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> are illustrated as 2-dimensional tensors with an x-direction (row) and a y-direction (column). In actual data, the tensors used in neural network <b>200</b> may include in any number of dimensions, from one to many. The discussions using 2-dimensional tensors may be extended to any number of dimensions. Whether a tensor is structured depends on whether the active values are distributed in any specific manner according to one or more constraints. According to an embodiment, the manners to arrange the active values may be referred to as block and partition. Each of those manners will be discussed in further detail in association with tensor <b>404</b> through tensor <b>438</b>. In <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the active values (e.g., non-zero values) are represented by the shaded cells and the inactive values (e.g., zeros) are represented by the white blocks. In unstructured tensor <b>402</b>, the active values are distributed randomly. For example, in the x-direction, the first row of tensor <b>402</b> has 4 active values; the second row of tensor <b>402</b> has 4 active values; and the third row of tensor <b>402</b> has only 1 active value. The active values in unstructured tensor <b>402</b> are generated based on the training of neural network <b>200</b> without any constraints imposed on how the active values should be placed.</p><p id="p-0123" num="0119">The use of unstructured tensors in an accelerator <b>300</b> may significantly slow down the speed of operation due to the sparse marshalling problem in identifying the randomly located active values. As mentioned in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, to speed up a sparse neural network, a sparse tensor may be compressed to a dense tensor so that the computations related to value locations that are zero are skipped. However, in an unstructured tensor <b>402</b> that has active values occurring without an easily identifiable pattern, an accelerator may need to spend significant resources and time to scan through the tensor to identify the locations of active values. The time for scanning through the tensor may sometimes even be longer than performing the tensor multiplication in a brute-force manner such as by performing multiplications of two sparse tensors on all data locations of the tensors without identifying the sparse locations.</p><p id="p-0124" num="0120">The marshalling problem may be illustrated by an example. The expected number of multiply-accumulate operations for a sparse-sparse (both tensors are sparse) dot product is the product of the tensors' densities. In a 1600-element dot product, if the first tensor's density is 5% and the second tensor's density is 12.5%, the expected number of the multiply-accumulate operations between two active values is only 10. This represents 160 times of computation reduction. To realize this computation reduction, the sparse tensors may be distilled by pre-processing circuit <b>354</b> to eliminate the operand pairs that have an inactive value involved and keep only the mutually active operand pairs from each sparse tensor. This distillation process may be referred to as a sparse to dense compression. However, without specific structured tensors and circuitry, rendezvousing these mutually active pairs can be a challenging problem. Also, in an unstructured tensor, the positions of active values within a tensor usually do not follow an algorithmic pattern. During compression from a sparse tensor to a dense tensor, coordinates will need to be associated with the active values. There will be storage and performance overhead in an accelerator for accessing these coordinates. General hardware circuitry, whether conventional CPU, GPU, FPGA, or ASIC, may take a significant time to compare both tensors to determine the locations with active values in both tensors. The time or the hardware footprint needed to perform the searching may rival a dense operation that conducts the dot products in all 1600 locations by vector processing with single instruction multiple data (SIMD) units. The searching of those locations may be referred to as the marshalling problem.</p><p id="p-0125" num="0121">According to an embodiment, the sparsity of tensors in a neural network <b>200</b> may be constrained so that the active values are spatially structured. For example, structured tensors may be achieved in the training of neural network <b>200</b> by imposing one or more constraints on how the active values are distributed. The tensors <b>404</b> through <b>438</b> illustrate two types of structure, which are referred to as block structure and partitioned structure. A tensor may also be in a combination of these two types of structures. In a block structure, a tensor may be divided into blocks, which are a group of data value locations in the tensor. In the block structure, the active values are concentrated in a subset of blocks, leaving the rest of the blocks completely inactive. In a partitioned structure, the tensor may be divided into sub-volumes. One or more constraints may be imposed equally on each sub-volume. For example, the number of active values in each sub-volume may be a fixed number so that the partitions have a balanced number of active values. The partitioned structure results in less variability of the sparsity, which in turn reduces the combinatorics of the marshalling problem. The constraints of blocks and partitions may be imposed on one or more dimensions of the tensor. A tensor may also have both the block and partitioned structures in one or more dimensions.</p><p id="p-0126" num="0122">Tensors <b>404</b> through <b>438</b> illustrate various examples of structures in different dimensions, according to different embodiments. In tensor <b>404</b>, the tensor is divided into blocks in x-dimension. Each block includes 1&#xd7;4 value locations. Each block is either active or inactive. In an active block, at least one of the values is active. In an inactive block, all of the values are inactive. In tensor <b>406</b>, the tensor is divided into partitions in x-dimension. Each row is a partition. A constraint is imposed on tensor <b>404</b> so that each row (each partition) has the same number (4) of active values. In tensor <b>408</b>, both block structure and petitioned structure are imposed in x-dimension. Similar to tensor <b>404</b>, tensor <b>408</b> is divided into 1&#xd7;4 blocks. Each row in tensor <b>408</b> has one and only one active block, which is a condition imposed on the partition.</p><p id="p-0127" num="0123">Tensor <b>412</b> through <b>438</b> illustrate additional structures that are in different dimensions and different combinations. For example, tensor <b>412</b> is a block structure in y-dimension. Tensor <b>414</b> is a block structure in both x and y dimensions. Each block includes 2&#xd7;2 value locations. In tensor <b>416</b>, block structure is imposed in y-dimension while the partition structure is imposed in the x-dimension. As such, each row (x-dimension) has four dense vertical blocks. Tensor <b>418</b> is divided by 2&#xd7;2 x-y blocks. Partitioning is imposed in x-dimension so that each row in tensor <b>418</b> has 2 blocks. Tensors <b>422</b>, <b>424</b>, <b>426</b>, <b>428</b>, <b>432</b>, <b>434</b>, and <b>436</b> are additional examples of different combinations of block and partitioned structures. Tensor <b>438</b> is divided by 2&#xd7;2 x-y blocks. Partitioning is imposed in both x-dimension and y-dimension so that each row in tensor <b>438</b> has 2 blocks. Each column in tensor <b>438</b> also has 2 blocks.</p><p id="p-0128" num="0124">The block and partitioned structures can be applied to both input activation tensor <b>220</b> and process tensor <b>222</b>. Each of the input activation tensor <b>220</b> and process tensor <b>222</b> may be blocked and partitioned in a similar manner but in different dimensions so that the pairing of input activation tensor <b>220</b> and process tensor <b>222</b> can predictably limit the number of computations. <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates several examples of such pairing of tensors. In operation <b>450</b>, partitioned-x tensor <b>406</b> may represent the process tensor <b>222</b> and partitioned-y tensor <b>422</b> may represent the input activation tensor <b>220</b>. The tensor <b>406</b> and tensor <b>422</b> both have a partitioned structure but the former has the partitions in a first dimension and the latter has the partitions in a second dimension different from the first dimension. Rows of tensor <b>406</b> and columns of tensor <b>422</b> have a fixed number of elements. Hence, operation <b>450</b> can have a maximum of 4 multiply-accumulate operations per dot-product.</p><p id="p-0129" num="0125">In operation <b>460</b>, block-x and partitioned-x tensor <b>408</b> may represent the process tensor <b>222</b> and block-y and partitioned-y tensor <b>432</b> may represent the input activation tensor <b>220</b>. The tensor <b>408</b> and tensor <b>432</b> both have block structure and partitioned structure, but both blocks are partitions in different dimensions. In this case, rows of tensor <b>408</b> and columns of tensor <b>432</b> have a fixed number of blocks. Hence, operation <b>460</b> can have the maximum of 1 single instruction multiple data (SIMD) block multiply-accumulate operations per dot-product.</p><p id="p-0130" num="0126">In operation <b>470</b>, block-x and partitioned-xy tensor <b>428</b> may represent the process tensor <b>222</b> and block-y and partitioned-xy tensor <b>436</b> may represent the input activation tensor <b>220</b>. The tensor <b>428</b> and tensor <b>436</b> both have block structure and partitioned structure, but the blocks are divided in different dimensions. In this case, both rows and columns of tensor <b>428</b> and the row and columns of tensor <b>436</b> have a fixed number of blocks. Hence, operation <b>470</b> can have the maximum of 1 single instruction multiple data (SIMD) block multiply-accumulate operations per dot-product.</p><heading id="h-0010" level="2">Example Complementary Sparsity Techniques</heading><p id="p-0131" num="0127"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a flowchart depicting an example process <b>500</b> for performing operations related to complementary sparsity techniques in a sparse neural network, according to an embodiment. <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a conceptual diagram that graphically illustrates a sparse neural network operation using complementary sparsity techniques, according to an embodiment. <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> are discussed in conjunction with each other.</p><p id="p-0132" num="0128">The process <b>500</b> may be performed by a computing device, such as computing device <b>100</b>. The computing device may be equipped with an accelerator <b>300</b>, <b>600</b>, or <b>650</b> and may perform one or more steps of this process using the accelerator <b>300</b>, <b>600</b>, or <b>650</b>. However, in some embodiments, the process may also be performed using a CPU, a GPU, or any combination of processors. The process may be embodied as software algorithm that may be stored as computer instructions that are executable by one or more processors and certain hardware architecture described in this disclosure may speed up the computation. The instructions, when executed by the processors, cause the processors to perform various steps illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>.</p><p id="p-0133" num="0129">The computing device initializes <b>505</b> a neural network with a plurality of nodes. The structure of the neural network may depend on its type, which can be CNN, RNN, LSTM, etc. The structures and operations of the nodes can be different among the nodes. The nodes may each be associated with a process tensor and an activation tensor. The structure and operation related to the tensors are discussed in <figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref>. The initialized neural network may be saved in system memory <b>108</b> or storage unit <b>110</b>. The process tensor in each node may include a plurality of data values. The initial data values may be initialized randomly or based on expected values. In some cases, the initial data values in the process tensor may be initialized with a number of zeros. In <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, the process tensors are illustrated as sparse kernels <b>540</b>, <b>542</b>, <b>544</b>, <b>546</b>, and <b>548</b> and the activation tensor is illustrated as an activation matrix <b>550</b>. The sparse kernels <b>540</b>, <b>542</b>, <b>544</b>, <b>546</b>, and <b>548</b> may correspond to the weights of five different nodes. Without loss of generality, the process tensors and the activation tensor may be in any dimensions and sizes and are not limited to each as a 5&#xd7;5 two-dimensional matrix. For example, the technique can be applied to convolutional kernels by overlaying multiple 3D sparse tensors from a layer's 4D sparse process tensor. Also, there can be more or fewer than five nodes in each layer of the neural network.</p><p id="p-0134" num="0130">The computing device imposes <b>510</b> one or more structural constraints to limit the distribution of active values of the process tensor. The constraints may be based on one or more code instructions in training the neural network that defines the configuration of the neural network. In complementary sparsity, the constraints may include the locations of active values so that no two sparse process tensors within a subset contain an active at precisely the same location. In some embodiments, the constraints do not dictate the relative positions of the active values or the permissible sparsity levels except may be a minimum sparsity. Given the flexibility of the constraints, experimental results show that neural networks trained with the complementary sparsity constraints do not compromise on accuracy when compared to unstructured sparsity. In some embodiments, additional constraints may additionally be imposed. For example, one or more blocky or partitioned constraints may also be applied.</p><p id="p-0135" num="0131">One or more structural constraints may also be imposed for an activation tensor by way of the K-winner activation function. Referring temporarily back to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, for the nodes in the input layer <b>202</b>, the input activation tensor <b>220</b> may likely be a dense tensor because the input data is often data such as image data, speech data, etc. As such, a node in the input layer may be a sparse-weight, dense-activation node, or simply a sparse-dense node. After process tensor <b>222</b> and a K-winner activation function are applied, the output activation tensor <b>228</b> can be sparse. For example, the K-winner activation function can limit the number of active values in the output activation tensor <b>228</b> and force the loser data values to zeros. The output activation tensor <b>228</b> becomes the input activation tensor <b>220</b> of the next node. The next node can be a sparse-sparse node. The K-winner activation function may be used in both training the neural network that defines the configuration of the neural network and in inference. In situations that involve boosting that gives favor to nodes that are not activated in previous cycles by manually increasing the values of the nodes, boosting may be applied before K-winner selection.</p><p id="p-0136" num="0132">While the K-winner activation function is described as an example of a sparse activation function, other sparse activation functions may also be used in various embodiments. A sparse activation function is an activation function that results in a sparse output. The activation function is applied to the computation result in a neural network node. For example, in the K-winner activation function, the number of active values in the output may be limited by K. Alternatively, or additionally, a threshold approach may be used as a sparse activation function. Values that are below the threshold are set to inactive (e.g., set to zeros). The threshold may be global or local, static or dynamic. The threshold is applied to an entire tensor in the global approach while the threshold is only applied to a certain subset of data (e.g., a block or a partition) in a local approach. In a static approach, a predetermined threshold value may be used. In a dynamic approach, a threshold value may vary based on factors to be determined during the training. For example, statistics may be performed on a set of values on the fly to determine a dynamic threshold cutoff to set some of the values to zeros.</p><p id="p-0137" num="0133">The structure constraint for the K-winner approach for the activation tensor can be global or local. If K-winner is applied to an entire tensor, the K-winner approach may be referred to as a global K-winner. If K-winner is applied to a subset of the tensor, such as a dimension, a block, or a partition of the data, the K-winner approach may be referred to as local K-winner. The computing device may train <b>515</b> the neural network using one or more structural constraints. The computing device may use one or more processors, such as an accelerator <b>300</b>, a CPU, or in combination, to perform different computations associated with training of the neural network. The training <b>515</b> may include forward propagation <b>520</b> and backpropagation <b>530</b>. In forward propagation <b>520</b>, the processor performs computations as defined by each node in the forward direction as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>. In one or more nodes, the computation may include a linear operation between an input activation tensor <b>220</b> and a process tensor <b>222</b> followed by a non-linear operation, as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>. In backpropagation <b>530</b>, the processors may adjust the weight values in the process tensors using techniques such as coordinate descent and also based on the structural constraints imposed on one or more nodes.</p><p id="p-0138" num="0134">In forward propagation <b>520</b>, different operations may be performed based on the sparsity of a node. The operations may include combining sparse process tensors to a dense process tensor, multiply-accumulation, and post-processing of tensors. The computing device may combine <b>522</b> a plurality of sparse process tensors to a dense process tensor. <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates the use of complementary sparsity to combine the sparse process tensors <b>540</b>, <b>542</b>, <b>544</b>, <b>546</b>, and <b>548</b> to a dense process tensor <b>560</b>. Since the active value patterns in the sparse process tensors <b>540</b>, <b>542</b>, <b>544</b>, <b>546</b>, and <b>548</b> are non-overlapping, the sparse process tensors <b>540</b>, <b>542</b>, <b>544</b>, <b>546</b>, and <b>548</b> can be combined as a single dense process tensor <b>560</b>. The dense process tensor <b>560</b> may also be referred to as a complementary tensor. In various embodiments, while the active value patterns are non-overlapping, the inactive values of the combined tensors do not need to completely fill the dense process tensor <b>560</b>. In other words, the dense process tensor <b>560</b> may have one or more inactive values, although the particular example of dense process tensor <b>560</b> shown in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is completely filled with active values. For example, temporarily referring to <figref idref="DRAWINGS">FIG. <b>6</b>C</figref>, an example of a complementary tensor <b>660</b> has a cross-hatched block that represents an inactive value location that is common to the corresponding sparse tensors.</p><p id="p-0139" num="0135">In the particular example shown in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, each of the sparse process tensors <b>540</b>, <b>542</b>, <b>544</b>, <b>546</b>, and <b>548</b> is 80% sparse. A set of 5 non-overlapping patterns of active values is overlaid to form a single dense process tensor <b>560</b>. The number of sparse process tensors that can be combined scales proportionally with their sparsity. A constraint is that the non-zero elements in each set do not collide with each other. In some embodiments, however, it is not necessary that all the process tensors in a layer are non-overlapping. The constraint applies only to each set being combined. Using this 80% sparsity example, if a convolutional layer contains 20 channels, there can be 4 dense combined process tensors <b>560</b> each corresponding to a set of 5 sparse process tensors. The values in the sparse process tensors are non-overlapping within a set, but there are no restrictions across the 4 sets. In some embodiments, some of the nodes may be complementary in nature while other nodes are not. For example, some of the nodes may be dense nodes.</p><p id="p-0140" num="0136">The computing device may control <b>524</b> the permutation logic of the combined dense process tensor <b>560</b>. Since the dense process tensor <b>560</b> is combined from multiple sparse process tensors, the computing device needs to track the positions of values that correspond to different sparse process tensors. The computing device also routes the appropriate computation products separately for each output. Each dense process tensor <b>560</b> is associated with a state vector that controls the permutation logic to produce the grouping of the sparse process tensors. Collectively the dense process tensor <b>560</b> and the state vector can be described as an augmented process tensor. The organization of dense process tensor <b>560</b> (complementary tensor) computation may be performed before or after multiplication. For example, step <b>524</b> may be performed before or after step <b>526</b>. In some embodiments, in a pre-multiplication routing, the processor lines up the values in the activation tensor <b>550</b> with the weights that are clustered into groups. The pre-multiplication permutation may be referred to as a gather operation. In some embodiments, in a post-multiplication routing of elementwise products, the processor may steer the elementwise products into groups. The post-multiplication permutation may be referred to as a scatter operation. The computing device performs either pre-multiplication routing or post-multiplication routing to segregate the product results and steer the product results toward independent adder trees to be accumulated.</p><p id="p-0141" num="0137">The computing device may perform <b>526</b> elementwise operations between the dense process tensor <b>560</b> and the activation tensor <b>550</b>. The elementwise operations may be multiplication operations to generate multiply products (e.g., Hadamard products) and may be performed in parallel by a number of multiply circuits <b>330</b> in parallel. Similar to the discussion of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the elementwise operations may simply be referred to as the first computation or a binary operation and the operations may be carried by the first computation circuit, of which the multiply circuit <b>330</b> is an example. Combining multiple sparse process tensors into a dense process tensor <b>560</b> reduces the number of operations and speeds up the process. Rather than serially multiplying a set of sparse process tensors (e.g., <b>540</b> through <b>548</b>) to subsets of the activation tensor <b>550</b>, sparse process tensors are interleaved into a single multiply operation with the entire activation tensor <b>550</b>. The higher the sparsity for each sparse process tensor, the greater the number of sparse process tensors that can be multiplied simultaneously.</p><p id="p-0142" num="0138">The computing device separates the elementwise products into different results <b>570</b>, <b>572</b>, <b>574</b>, <b>576</b>, and <b>578</b> based on the permutation logic. The processor may perform <b>528</b> accumulations of elementwise products that correspond to sparse process tensors <b>540</b> through <b>548</b>. For example, the elementwise products of the multiply circuits <b>330</b> are aggregated in adder trees <b>360</b>. Each accumulated result corresponds to an original sparse process tensor. As such, multiple sparse process tensors <b>540</b> through <b>548</b> are multiplied with the activation tensor <b>550</b> in a single multiplication operation through the dense process tensor <b>560</b> and the accumulated results, which correspond to results of different nodes, are separately generated. Similar to the discussion of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the accumulation operation may simply be referred to as the second computation and the operations may be carried by the second computation circuit, of which an adder tree <b>360</b> is an example. The second computation circuit may also include one or more reduction trees.</p><p id="p-0143" num="0139">The computing device may also apply <b>529</b> activation functions to the accumulated results generated by the adder trees <b>360</b>. The activation function may be a dense activation function such as ReLU or tanh. The activation function may also be a sparse activation function such as K-winner. The activation function may further be a sparse and structured activation function such as blocky K-winner or partitioned K-winner. Blocky K-winner may refer to a division of the tensor by blocks and selection of top K blocks. Partitioned K-winner may refer to a division of the tensor by partitions and selection of top K values in each partition. After completing the computations of a node, the processor may perform computations on a subsequent node in the forward direction of the neural network until an inference result is made. The inference result is compared to the actual label of a training sample.</p><p id="p-0144" num="0140">In backpropagation <b>530</b>, the computing device may adjust <b>552</b> the weight values in process tensors of various nodes under the structural constraints, such as the complementary sparsity constraints. For example, the weight values may be adjusted using techniques such as coordinate descent to change the values in directions that will more likely for the neural network to generate the correct inference result.</p><p id="p-0145" num="0141">After the neural network is trained with training samples, the neural network may be used to make <b>535</b> inferences from actual samples. The inference may be performed using the steps described in the forward propagation <b>530</b>. Since the sparsity distribution and the active values in a process tensor may be fixed during the training, the combination of multiple sparse process tensors <b>540</b>, <b>542</b>, <b>544</b>, <b>546</b>, and <b>548</b> may be performed offline as a preprocessing step. The inference is also accelerated because the trained sparse process tensors <b>540</b>, <b>542</b>, <b>544</b>, <b>546</b>, and <b>548</b> are combined as the dense process tensor <b>560</b>.</p><p id="p-0146" num="0142"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> is a conceptual diagram that illustrates complemental sparsity techniques graphically using three 3&#xd7;3 sparse kernels as an example, according to an embodiment. Again, the exact dimensions and sizes of the sparse process tensors (3&#xd7;3 sparse kernels in this example) vary, depending on embodiments. In <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, the active values in a tensor are illustrated as shaded blocks and inactive values are illustrated as white blocks. The computation of a forward propagation step may include five different steps, which may be referred to as &#x201c;combine,&#x201d; &#x201c;multiply,&#x201d; &#x201c;route,&#x201d; &#x201c;sum,&#x201d; and &#x201c;activation.&#x201d;</p><p id="p-0147" num="0143">In the combine step, multiple sparse process tensors <b>580</b>, <b>582</b>, and <b>584</b>, each having 33% sparsity in this example, are combined and overlaid to form a single dense process tensor <b>586</b>. The active values in each of the sparse process tensors <b>580</b>, <b>582</b>, and <b>584</b> remain in the same positions in the single dense process tensor <b>586</b>.</p><p id="p-0148" num="0144">In the multiply step, each value in the combined dense process tensor <b>586</b> is multiplied with the corresponding value in the activation tensor <b>590</b> in elementwise operations to generate elementwise products (e.g., Hadamard products). The elementwise products may be represented as a tensor form <b>592</b>.</p><p id="p-0149" num="0145">In the route step, the appropriate elementwise products are routed separately for each output. For example, the elementwise products that correspond to the first sparse process tensor <b>580</b> are routed together based on the permutation logic in the state vector. Likewise, the elementwise products that correspond to the second sparse process tensor <b>582</b> and the elementwise products that correspond to the third sparse process tensor <b>584</b> are respectively routed together based on the permutation logic.</p><p id="p-0150" num="0146">In the sum step, the routed products are aggregated to form a separate result that corresponds to each sparse process tensor. Each separate result is a result of a node in a layer of the neural network. The sum step is an accumulation step that may be performed by the adder trees <b>360</b>.</p><p id="p-0151" num="0147">In the activation step, one or more activation criteria may be applied to the results of those nodes, which are aggregated in the sum steps. The activation criteria may be ReLU, tanh, LSTM gates, or other common activation criteria in a dense activation neural network. In a sparse activation neural network, the activation criteria may be a form of K-winner. The values of the results of those nodes are compared and top K values are selected as the winners of an activation selection. Other values are set to zero.</p><heading id="h-0011" level="2">Example Circuitry for Complementary Sparsity</heading><p id="p-0152" num="0148"><figref idref="DRAWINGS">FIGS. <b>6</b>A and <b>6</b>B</figref> are block diagrams illustrating circuitry and hardware architecture of example accelerators that are designed for improving the performance of using complementary sparsity techniques, according to some embodiments. <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> illustrates an example accelerator <b>600</b> that performs pre-multiplication permutation. <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> illustrates an example accelerator <b>650</b> that performs post-multiplication permutation. Most of the circuit components are similar to those in <figref idref="DRAWINGS">FIG. <b>3</b></figref> and those components are not repeatedly discussed. The circuitry for <figref idref="DRAWINGS">FIGS. <b>6</b>A and <b>6</b>B</figref> may be used for sparse-dense networks.</p><p id="p-0153" num="0149">A group of permutation state registers <b>610</b> are added to the accelerator <b>600</b> and the accelerator <b>650</b>. As discussed in association with the process <b>500</b>, multiple sparse process tensors are combined into a dense process tensor. In making an inference after training, the combination and generation of the dense process tensor may be performed as a pre-processing step and the dense process tensor may be stored in a memory such as internal memory <b>310</b> or system memory <b>108</b>. The permutation state registers <b>610</b> are used to store the state vector that tracks the permutation logic when combining multiple sparse process tensors into a sense process tensor. For example, the permutation logic may store the corresponding active values of sparse process tensors as a sequence of sparse process tensor identifiers.</p><p id="p-0154" num="0150">The permutation circuit <b>605</b> performs the routing of values based on the permutation logic stored in the permutation state registers <b>610</b>. In pre-multiplication routing, the permutation circuit <b>605</b> may be an example of the pre-processing circuit <b>354</b>. In the accelerator <b>600</b>, a gather operation may be performed as a pre-multiplication routing operation. In a pre-processing stage, the values in the dense process tensor stored in a memory such as system memory <b>108</b> may be saved based on the order of the sparse process tensors (e.g., the order of the nodes in a layer of the neural network). For example, the values in the dense process tensor may have been re-routed in a pre-processing stage so that the active values in a first sparse process tensor will go first, then the active values in a second sparse process tensor, and so forth, even though such an order is not the actual order of the values in the dense process tensor. To perform the elementwise operations, the permutation circuit <b>605</b> may re-route and group the values in the activation tensor stored in activation buffer <b>352</b> based on the corresponding permutation and ordering of the routed dense process tensor. Elementwise operations may then be performed between the routed dense process tensor and the routed activation tensor in multiply circuit <b>330</b>. The elementwise products are already gathered and ordered based on a certain order of the nodes in the neural network. As such, accumulations may be performed separately for each node.</p><p id="p-0155" num="0151">In the accelerator <b>650</b>, a scatter operation may be performed as a post-multiplication routing operation. The activation tensor and the dense process tensor may be directly multiplied in an elementwise manner using the multiply circuits <b>330</b> without re-routing. Hence, for example, the dense process tensor <b>586</b> shown in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref> may be multiplied by the activation tensor without re-ordering. The elementwise products may be represented in a tensor form, for example, as tensor <b>592</b> in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>. The permutation circuit <b>605</b> in the accelerator <b>650</b> is located downstream of the multiply circuits <b>330</b>. Based on the permutation logic stored in the permutation state registers <b>610</b>, the permutation circuit <b>605</b> re-arranges the order of the values in a routing circuit to break the values in the elementwise products into different groups. Each group corresponds to a sparse process tensor (e.g., corresponds to a node in a layer of the neural network).</p><p id="p-0156" num="0152">Whether the pre-multiplication routing or post-multiplication routing is used may depend on embodiments. In some embodiments, if the multiplication operands are floating-point numbers, the pre-multiplication routing or post-multiplication routing consumes equal or similar resources. In some embodiments, if the multiplication operands are fixed-point numbers, pre-multiplication routing may be preferable, in which the values in the activation tensors are re-arranged. In fixed-point post-multiplication routing, the product values are often twice the width of the activation operand values, and therefore require twice the resources (e.g., multiplexors, wires) to re-route the values to group the values based on nodes of the neural network.</p><p id="p-0157" num="0153"><figref idref="DRAWINGS">FIG. <b>6</b>C</figref> is a conceptual diagram illustrating complementary sparsity techniques and a post-multiplication routing, according to an embodiment. <figref idref="DRAWINGS">FIG. <b>6</b>C</figref> illustrates the combination of two sparse process tensors <b>650</b> and <b>655</b>. The shaded blocked in the sparse process tensors <b>650</b> and <b>655</b> represent active values in the tensors and the white blocks represent inactive values. The cross-hatched blocks represent an inactive value whose location is common to both sparse process tensors <b>650</b> and <b>655</b>. While in this example the sparsity is about 50% and there are two sparse process tensors, in various embodiments the sparsity and the number of sparse process tensors to be combined can be higher.</p><p id="p-0158" num="0154">The two sparse process tensors <b>650</b> and <b>655</b> are combined to form a complementary tensor <b>660</b>, which is multiplied with an activation tensor <b>665</b> in elementwise operations to generate an elementwise product tensor <b>670</b>. The elementwise product tensor <b>670</b> has a white block that represents the location where both sparse process tensors <b>650</b> and <b>655</b> have an inactive value. The elementwise product tensor <b>670</b> is flattened to a linear array <b>675</b>. The linear array <b>675</b> has the same order of values as the elementwise product tensor <b>670</b> and, hence, has the elementwise product values of both sparse process tensors <b>650</b> and <b>655</b> mixed. In flattening the elementwise product tensor <b>670</b>, the processor, such as accelerator <b>650</b>, may remove any common inactive position(s). For example, since the elementwise product tensor <b>670</b> contains a white block, the 5&#xd7;5 tensor is flattened to a 1&#xd7;24 array with one value removed. Additional values may be removed if more common inactive positions are presented. The linear array <b>675</b> is then re-arranged to form a permuted linear array <b>680</b> by the permutation circuit <b>605</b>. Values in linear array <b>675</b> are routed to as groups based on the sparse process tensors <b>650</b> and <b>655</b>. Each group can be sent to an adder tree for accumulation. Example circuitry of the permutation circuit <b>605</b> is discussed in <figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref>.</p><p id="p-0159" num="0155">The pre-multiplication routing may be carried out in a similar fashion illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>C</figref> but will be carried out on the activation tensor <b>665</b>. The complementary tensor <b>660</b> may be generated in a pre-processing step that is routed and grouped. In other words, instead of having the order of values of the complementary tensor <b>660</b> shown in <figref idref="DRAWINGS">FIG. <b>6</b>C</figref>, in pre-multiplication routing, the complementary tensor's values may be grouped like the values in the permuted linear array <b>680</b> and the complementary tenor may be flattened. In pre-multiplication routing, as the activation tensor <b>665</b> is generated (e.g., generated as the output of a preceding layer of the neural network), the activation tensor <b>665</b> is re-arranged by the permutation circuit <b>605</b> before the multiplication and may also be flattened.</p><heading id="h-0012" level="2">Example Permutation Circuits</heading><p id="p-0160" num="0156"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a conceptual diagram illustrating example circuitry that may be used in the permutation circuit <b>605</b>, according to an embodiment. The circuitry shown in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> are examples of sub-units of a routing circuit. The subunits may be a switch circuit <b>710</b> and a permutation network circuit <b>720</b>. In various embodiments, the actual routing circuit can be scaled by using a combination of one or more switch circuits <b>710</b> and one or more permutation network circuits <b>720</b> based on the size of the processor and anticipated tensor work unit size (e.g., size of a dataset in an operating cycle).</p><p id="p-0161" num="0157">The switch circuit <b>710</b> is a simple circuit unit that maps 2 inputs to 2 outputs using a control bit. A first value of the control bit directs the switch circuit <b>710</b> to simply pass the 2 inputs to 2 outputs. The second value of the control bit directs the witch circuit <b>710</b> to swap the inputs. The permutation network circuit <b>720</b> is a combination of multiple switch circuits <b>710</b> in a particular order so that N inputs can be permuted in any order as N outputs. The example permutation network circuit <b>720</b> shown in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a particular arrangement that allows 5 inputs to be permuted. In many embodiments, since values in an AI processor are often stored as bytes or multiples of bytes (e.g., floating-point (FP) <b>16</b>, FP <b>32</b>, etc.), a permutation network circuit <b>720</b> may be of the size that is configured to handle 8 bits. The number of switch circuits <b>710</b> needed for N&#xd7;N permutation network circuit <b>720</b> may follow this pattern: &#x250c;N*log 2(N)&#x2510;&#x2212;N+1. The number of stages needed for N&#xd7;N permutation network circuit <b>720</b> may follow this pattern: 2*&#x250c;log 2(N)&#x2510;&#x2212;1. The control of the permutation may be based on a lookup table that sends control bits to various switch circuits <b>710</b> (e.g., 8 switches in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>) in a permutation network circuit <b>720</b>. The control of the permutation may be saved in permutation state registers <b>610</b> in <figref idref="DRAWINGS">FIGS. <b>6</b>A and <b>6</b>B</figref>.</p><p id="p-0162" num="0158">The permutation network circuit <b>720</b> is less resource-intensive to implement the desired reordering than parallel operations that may be used to permute a vector into a particular order. The permutation network circuit <b>720</b>, such as the Waksman network, takes multiple nodes and logic stages to effect a permutation. In some embodiments, the permutation circuit <b>605</b> may be subdivided into multiple smaller permutation networks. The number of subdivisions corresponds to the number of samples for each sparse kernel.</p><p id="p-0163" num="0159"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a block diagram that illustrates the circuitry <b>700</b> that may be used for pre-multiplication routing, according to an embodiment. The components of the circuitry <b>700</b> may be examples of the permutation circuit <b>605</b>, the multiply circuits <b>330</b> and the adder trees <b>360</b> of the accelerator <b>600</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>. In <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, the activation array <b>750</b> is flattened from an activation tensor (not shown) that may be generated from a preceding layer of the neural network or input data. The activation array <b>750</b> is flattened but has not been re-arranged yet. The weight array <b>760</b> is also flattened from a dense complementary process tensor (not shown) and has already been re-arranged so that values corresponding to different sparse process tensors are grouped. The generation of weight array <b>760</b> may be performed in a pre-processing step. While the size of process tensors and the activation tensor in this example is 5&#xd7;5, the circuitry <b>700</b> may be expanded to any size without the loss of generality. In this example, the neural network is subject to a sparsity constraint in addition to complementary sparsity. The additional sparsity constraint in this example is partition sparsity. For example, the corresponding 5&#xd7;5 dense process tensor in this example may be combined from 5 sparse process tensors with the partition constraint that each sparse process tensor has a single active value in each row.</p><p id="p-0164" num="0160">The order of the values of activation array <b>750</b> is re-arranged by different permutation network circuits <b>720</b>. In this example, the maximum length of activation array <b>750</b> is 25. In the particular example of circuitry <b>700</b>, five different permutation network circuits <b>720</b> are included in the circuitry <b>700</b>. In various embodiments, other numbers of permutation network circuits <b>720</b>, such as a single one, may also be used. After the values in the activation array <b>750</b> are re-arranged, the values are multiplied with the weight array <b>760</b> in an elementwise fashion to generate elementwise products <b>770</b>. The elementwise products <b>770</b> may be statically routed to the adder trees <b>360</b>. As the example dense process tensor is combined from 5 sparse process tensors, the elementwise products <b>770</b> are routed to five adder trees <b>360</b>.</p><p id="p-0165" num="0161">The partition sparsity constraint may further improve the improvement of the neural network and the associated hardware. The process tensor is subject to a partition sparsity constraint so that the process tensor is divisible into five (or N for other sizes in other examples) different partitions. As such, smaller permutation network circuits <b>720</b>, such as the one illustrated in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, that handle the re-arrangement of a smaller number of values may be used. In some embodiments, if a partition sparsity constraint is not added, instead of multiple smaller permutation network circuits <b>720</b>, a large permutation network circuit may be used. However, the number of switch circuits <b>710</b> used in a permutation network circuit &#x250c;N*log 2(N)&#x2510;&#x2212;N+1 so the number of switch circuits <b>710</b> needed for a larger permutation network circuit is often higher than having multiple smaller permutation network circuits.</p><heading id="h-0013" level="2">Example Sparse Activation Processes</heading><p id="p-0166" num="0162"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a conceptual diagram that graphically illustrates a sparse neural network process <b>800</b> using sparse activation, according to an embodiment. The sparse activation example illustrated is a K-winner take-all technique, but in some embodiments, other sparse activation methods may also be used. The process <b>800</b> may be performed by a computing device, such as computing device <b>100</b>. The computing device may be equipped with an accelerator <b>300</b>, <b>600</b>, or <b>650</b> and may perform one or more steps of this process using the accelerator <b>300</b>, <b>600</b>, or <b>650</b>. However, in some embodiments, the process may also be performed using a CPU, a GPU, or any combination of processors. The process may be embodied as a software algorithm that may be stored as computer instructions that are executable by one or more processors and certain hardware architecture described in this disclosure may speed up the computation. The instructions, when executed by the processors, cause the processors to perform various steps illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0167" num="0163">For each of the K non-zero activation values in the activation tensor, the index of the value may be used to extract the relevant weight values, which are then multiplied in an elementwise fashion. The individual terms of the elementwise products are routed separately to compute the sums for each output channel.</p><p id="p-0168" num="0164">To compute the operation efficiently, a preprocessing step that combines sets of sparse process tensors <b>810</b> into smaller sets of combined dense tensors may be used in a fashion described in <figref idref="DRAWINGS">FIGS. <b>5</b>B and <b>5</b>C</figref>. The combined dense tensors are associated with state vectors that include sparse tensor identifiers for the active weight values. The collection of the combined dense tensors and the state vectors may be denoted as augmented weight tensors (AWT). The complementary sparse tensors <b>810</b> are combined into a smaller number (L) of dense complementary sparse filter blocks (CSFBs) <b>820</b>. The dense CSFBs <b>820</b> are examples of combined dense tensors. Each of these dense CSFBs is flattened into a one-dimensional column. The collection of the one-dimensional columns are concatenated horizontally into an AWT <b>830</b> that has K ports. In some embodiments, the construction of this multi-ported AWT <b>830</b> may be an offline process done once for each convolutional layer. In addition, in the AWT <b>830</b>, each active weight value has a sparse tensor identifier (TID) co-located with the active value. The sparse tensor identifier flows through to each of the resulting product terms and is used for subsequent routing.</p><p id="p-0169" num="0165"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a conceptual diagram illustrating the fetching of augmented weight tensors <b>830</b> and multiplications between process tensor values and activation values, according to an embodiment. The operation of <figref idref="DRAWINGS">FIG. <b>9</b></figref> may be implemented as circuitry that operates based on the principles illustrated below. In some embodiments, to compute the elementwise products, an accelerator may serially access the AWT <b>830</b>, once for each of the K active values in the activation tensors. In some embodiments as illustrated by <figref idref="DRAWINGS">FIG. <b>9</b></figref>, K instances of the AWT <b>830</b> may be preloaded into a set of separate memories on an accelerator, which may be implemented as FPGA. Each instance of the AWT <b>830</b> may have a length L that includes L CSFBs <b>820</b>. Each memory has multiple output ports that can support outputting L values at a time. Each output port of the memory delivers a value from each of the L CSFBs <b>820</b> in each AWT <b>830</b> in parallel. The activation-aligned weights can be read out in parallel from this multi-ported AWT <b>830</b>. The activation values and the sparse tensor identifiers, TIDs, are also fetched from the memory. As a result, the elementwise products <b>840</b> for each column of the CSFB <b>820</b> can be computed in a single cycle. The sparse tensor identifiers flow along with the elementwise products <b>840</b> and are used for subsequent routing.</p><p id="p-0170" num="0166">At inference time, the following formula generates the lookup address for the AWT <b>830</b>, where (W<sub>x</sub>, W<sub>y</sub>) are the coordinates of columns in the CSFB <b>820</b>, is the index associated with j'th non-zero activation value, and C<sub>in </sub>is the number of channels in the input activation tensor to the layer:</p><p id="p-0171" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Address=<i>I</i><sub>j</sub><i>+W</i><sub>x</sub><i>*C</i><sub>in</sub><i>+W</i><sub>y</sub><i>*C</i><sub>in</sub><i>*W</i>&#x2003;&#x2003;Equation (3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0172" num="0167">A scaling issue with this scheme is the amount of memory consumed by the complete AWT structure. The total number of bits for the multi-ported AWT <b>830</b> is:</p><p id="p-0173" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>B</i><sub>M</sub><i>=C</i><sub>in</sub><i>*W</i><sup>2</sup><i>*K*L*B</i><sub>E</sub>&#x2003;&#x2003;Equation (4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0174" num="0168">Here, B<sub>E </sub>is the size of each element and is the sum of the bit size of the weight element value, B<sub>W</sub>, plus the bit size of the associated TID, B<sub>ID</sub>. In some embodiments, 8-bit weights are used so B<sub>W</sub>=8. To determine B<sub>ID</sub>, the number of sparse tensors that can fit into a single CSFB <b>820</b> is calculated. The active weight values in each sparse tensor may be distributed using partitioned weight sparsity along the C<sub>in </sub>dimension. With N active values in each column of the sparse process tensor, the number of sparse process tensors in a single CSFB <b>820</b> is C<sub>in</sub>/N. Therefore, B<sub>ID</sub>=[log 2(C<sub>in</sub>/N)]. If Gout is the number of output channels produced by the layer, the number of CSFBs <b>820</b> in an AWT <b>830</b>, L, is equal to Cout/(C<sub>in</sub>/N). Plugging this into Equation (4) yields:</p><p id="p-0175" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>B</i><sub>M</sub><i>=W</i><sup>2</sup><i>*C</i><sub>out</sub><i>*N*K*B</i><sub>E</sub>&#x2003;&#x2003;Equation (5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0176" num="0169">In some embodiments, the size of memory decreases as activation sparsity is increased (decreasing K). Similarly, the size of memory decreases as the weight sparsity is increased (decreasing/V). Therefore, the memory savings with weight and activation sparsity are multiplicative. Overall, with sparse-sparse networks, this approach of replicating weights enables far higher throughput with favorable memory scaling.</p><heading id="h-0014" level="2">Example Routing Operations in Sparse Activation</heading><p id="p-0177" num="0170">In some embodiments, an accelerator is further designed for efficient routing of the elementwise products from the elementwise operations. After an activation value is multiplied by a weight value, to complete the computation such as convolution, each resulting elementwise product is combined with the other products corresponding to the same sparse process tensor to generate an accumulated value. The relevant products are identified using the TIDs, which are copied and carried along with the computation, as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0178" num="0171"><figref idref="DRAWINGS">FIGS. <b>10</b>A and <b>10</b>B</figref> are conceptual diagrams illustrating two different ways of sparse-sparse product term routing using the sparse tensor identifiers, TIDs, according to some embodiments. The weight values in an AWT <b>830</b> may belong to a single sparse process tensor so that values have identical TID, or might be distributed across several sparse weights tensors that have different TIDs. Each of the K elementwise products can be processed serially, in which case the results for each of the products can be simply routed via a multiplexor network to a designated accumulator, based upon its Sparse Tensor ID. <figref idref="DRAWINGS">FIG. <b>10</b>A</figref> illustrates that elementwise products are serially routed to the appropriate accumulator. The P<sub>i </sub>represent the product terms along with their associated TID<sub>j</sub>. The TID<sub>j </sub>are used to successively index a single multiplexer to route the product term to the relevant accumulator Accum<sub>j </sub>to be summed. The solid black arrow indicates the selection process. This operation is performed serially K times.</p><p id="p-0179" num="0172"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrates that elementwise products are routed in parallel to the appropriate adder trees. In some embodiments, for greater performance, the elementwise products can be processed in parallel. In <figref idref="DRAWINGS">FIG. <b>10</b>B</figref>, the elementwise products are routed simultaneously to adder trees for summing, rather than to a single accumulator. To illustrate, the active routes are marked by solid black arrows. In the particular case shown in <figref idref="DRAWINGS">FIG. <b>10</b>B</figref>, the three elementwise products P<sub>0</sub>, P<sub>1</sub>, and P<sub>2 </sub>have identical TIDs of 1. As such, the elementwise products P<sub>0</sub>, P<sub>1</sub>, and P<sub>2 </sub>are routed to ATree1. In some embodiments, the adder trees have the capacity to handle the possibility of all K product terms being routed to a single adder tree.</p><p id="p-0180" num="0173">In parallel routing, routing of multiple elementwise products to non-conflicting inputs in an adder tree may introduce additional complexity. The parallel routing routes elementwise products based upon the TIDs. Additionally, destination address bits may be needed to designate the specific input port of the adder in which an elementwise product should land. This may be resolved with an arbiter <b>1010</b>, which provides these additional address bits before the elementwise product is passed to a larger multiplexer network. This is indicated in <figref idref="DRAWINGS">FIG. <b>10</b>B</figref> by a dotted line terminating on the arbiter <b>1010</b>. The arbiter <b>1010</b> may generate low-order address bits from the set of TIDs, using a prefix sum algorithm. Each occurrence of an elementwise product with the same TID increments the value of the lower order bits so that elementwise products with the same TID are assigned to a non-conflicting slot in the adder tree. The generated low-order bits may be concatenated to the TID. The fine-grained fan-out to individual ports of the adder tree is not illustrated in <figref idref="DRAWINGS">FIG. <b>10</b>B</figref>.</p><p id="p-0181" num="0174">Various factors may be used to improve or adjust the efficiency of the computation. For example, in some embodiments, a partition sparsity constraint may be used. Sparsity partitioned in the channel dimension, as reflected in the range of TIDs, may reduce the bit size of the TIDs since only sufficient bits are needed to identify the sparse process tensor within the channel dimension, not the location within the W<sup>2</sup>*C<sub>in </sub>locations of a dense process tensor. Other factors that may affect the computation efficiency may include K and N. Small values for K, reflecting high activation sparsity, reduces the number of low order bits needed for adder tree input port assignment in the parallel implementation. Small values of N, reflecting high weight sparsity, also reduce the number of low order bits needed, since the number of product terms which can be directed towards a single adder tree is min(K, N).</p><p id="p-0182" num="0175"><figref idref="DRAWINGS">FIG. <b>10</b>C</figref> is a block diagram illustrating the structure of an example arbiter circuit <b>1010</b>, according to an embodiment. The function of the arbiter circuit <b>1010</b> is to generate LSB address bits that are used to assign non-conflicting port addresses for each kernel filter's respective adder tree. In <figref idref="DRAWINGS">FIGS. <b>10</b>C and <b>10</b>D</figref>, K may be denoted as the number of non-zero activations and number of product terms being computed. F may be denoted as the number of filter kernels per CFSB. B<sub>ID </sub>may be denoted as the number of bits to represent the range of F. B<sub>K </sub>may be denoted as the number of bits to represent the range of K(&#x250c;log<sub>2</sub>(K)&#x2510;). T may be denoted as the number product terms per filter kernel. B<sub>T </sub>may be denoted as the number of bits to represent the range of T(&#x250c;log<sub>2</sub>(T)&#x2510;).</p><p id="p-0183" num="0176">The arbiter circuit <b>1010</b> generates the low order address bits from the set of K Tensor IDs (TIDs). Each occurrence of a product with the same TID effectively increments a count associated with that TID. This is done with a bit-wise prefix sum module for each TID, where the positions of the input &#x201c;1&#x201d; bits correspond to the storage order of the products and their TIDs. Referring to <b>10</b>C, each of the K TIDs is fed into the select lines of a 1-to-F single bit demultiplexer. The inputs to the demultiplexers are tied to logical &#x201c;1&#x201d;, while the outputs of each demultiplexer is distributed to F instances of Prefix Sum circuits shown in <figref idref="DRAWINGS">FIG. <b>10</b>D</figref>.</p><p id="p-0184" num="0177">The K bits inputs to each Prefix Sum circuit are summed to produce a K*B<sub>T </sub>bit wide output. <figref idref="DRAWINGS">FIG. <b>10</b>D</figref> is a conceptual diagram illustrating a prefix sum circuit in the arbiter circuit <b>1010</b> shown in <figref idref="DRAWINGS">FIG. <b>10</b>C</figref>, according to an embodiment. Note that the outputs are gated such that only inputs which have a &#x201c;1&#x201d; will produce non-zero B<sub>T </sub>outputs. In some embodiments, the gating is used because in the next stage all F sets of LSBs generated in the prefix sum stage are logically &#x201c;OR&#x201d;ed together to produce a single K*B<sub>T </sub>wide bit vector. In some embodiments, K*B<sub>T </sub>OR gates are used, each with F inputs, one input from each of the Prefix Sum circuits. The K sets of BT bit-wide LSBs are paired and concatenated with the B<sub>ID </sub>bits of their respective TID, completing the address generation function of the arbiter circuit <b>1010</b>.</p><heading id="h-0015" level="2">Activation Sparsity Using K-WTA and Example Circuit Configurations</heading><p id="p-0185" num="0178">In some embodiments, for K-winner take all (k-WTA) techniques, activation sparsity may be induced by explicitly restricting the number of active output elements to the K largest values that are produced by a layer. In some cases, determining these top K values efficiently can represent a significant obstacle to the effective use of activation sparsity. The time and resources expended performing the sorting operation may erode the performance benefits associated with leveraging the resulting sparsity in subsequent processing. k-WTA implementations may fall into two broad categories. In a global k-WTA, all output elements in an output activation tensor are examined to determine the K largest to be selected and the rest to be set as zeros. In some embodiments, global k-WTA may be used in linear layers of a neural network. In a local k-WTA, the activation is partitioned into smaller units, and only the elements belonging to a partition are compared to each other. In some embodiments, local k-WTA may be used in convolutional layers of a neural network, where the winner-take-all competition happens along a specific dimension, such as the channel dimension. The process illustrated for determining k-WTA may be carried in the activation circuit <b>370</b>.</p><p id="p-0186" num="0179"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a conceptual diagram illustrating the circuitry of an example activation circuit <b>370</b> and the approach used for a parallel global k-WTA approach, according to an embodiment. The process performs a histogram-based search of the entire output to determine the threshold yielding K active output elements. The activation circuit <b>370</b> may include an activation memory <b>1110</b> that is used to store output values and histogram memories <b>1120</b> that are used to build a histogram that represents a distribution of the values of the output elements. Using 8-bit output values as an example, in some embodiments, a 256-element array in memory may be used to build the histogram, with each output value being used to increment a count at a location addressed by that value. After all of the output values have been processed, the histogram array represents the distribution of the output values. For a specified value of K, the histogram values can be read, largest first, to determine the appropriate minimum value cutoff. Output values above this threshold are retained as part of the top-k in the activation selection and the remaining output values are discarded. The activation circuit <b>370</b> may include a simple comparator circuit to compare the output values against the threshold. The winners are passed to the next layer as the activation values in an activation tensor for the next layer.</p><p id="p-0187" num="0180">The activation memory <b>1110</b> may also receive biases <b>364</b> as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In a boosting operation, output values that correspond to some of the nodes that are previously inactive may receive a boost in the values before the values are compared in the k-WTA operation. The boost may be a linear boost by adding a value to the output or a scaling factor that multiplies the output.</p><p id="p-0188" num="0181">For improved performance, an implementation may process multiple output elements in parallel. In this scenario, multiple histograms are built in parallel and then combined to determine the overall cutoff value. An example of this implementation is illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, for 1500-outputs, 5-way parallelism, and activation sparsity of 85%. In this example, the 1500-element output is stored as 300 5-element blocks in the activation memory <b>1110</b>. Each block is read out and the element values are used to address and increment counts in 5 separate histogram memories <b>1120</b>, A-E. The counts are then cumulatively summed into variable Accum, starting with the largest value location, until Accum reaches a total count of K, establishing a threshold value. Values in the activation memory <b>1110</b> are then compared to the threshold. The values are sent through to the next layer if the values are greater than or equal to the threshold, along with the corresponding indices in the original 1500 element vector. The specific numbers used in <figref idref="DRAWINGS">FIG. <b>11</b></figref> are only examples. In various embodiments, other data sizes and sparsity levels may also be used.</p><p id="p-0189" num="0182"><figref idref="DRAWINGS">FIGS. <b>12</b>A and <b>12</b>B</figref> are block diagrams illustrating example structures of sorting circuits <b>1200</b> and <b>1250</b> that are used for k-winner take all activation function, according to some embodiments. The sorting circuits <b>1200</b> and <b>1250</b> may be examples of the activation circuit <b>370</b>.</p><p id="p-0190" num="0183">The use of partition sparsity constraint may provide significant efficiency benefits in a sparse activation operation. In some embodiments, such as for convolutional layers, activation tensors and outputs may have a natural partitioning in the channel dimension. When the top-k operation in k-WTA is implemented as a sorting operation, which may have the complexity O(N*log(N)) either in time or hardware resources, partitioning may provide significant efficiency benefits. The position of each result value produced by the convolutional layer may be tracked through the sorting process. This is achieved by appending an index to each data value entering the sorting function.</p><p id="p-0191" num="0184">In some embodiments, sorting may be performed in several stages. Since it is only needed to find the top K values in each set of output values, the ordering of the low-valued elements is immaterial. As K decreases with increasing activation sparsity, the cost of sorting implementation may fall accordingly. First, each set of output values may be subdivided into M sub-vectors. Each sub-vector is sent through a sorting network. The sorted sub-vector is subsequently loaded into one of M first-in-first-out (FIFO) circuits, with each sub-vector's largest value at the front of the FIFO queue.</p><p id="p-0192" num="0185">A vector composed of the M top-of-FIFO values is then passed through a log<sub>2</sub>(M) stage comparator tree, in order to determine the maximum value in the output set. The maximum value is retained, and its associated indexing information (which indicates in which FIFO the value was located) is used to pop that element from the appropriate FIFO, exposing the FIFO's next largest element. This process is repeated K times, at which point the output vector has been filled with the top K elements and is passed to the next processing layer. In some embodiments, a 64-element output set is subdivided into eight 8-element sub-vectors. The sorting network may include 19 comparators, arranged into depth 6 layers. There are 8 FIFO circuits, and a 3-level comparator tree is used to determine the maximum value in the 8-element top-of-FIFO vector.</p><p id="p-0193" num="0186">To prevent bottlenecks, the performance of the k-WTA implementation may be matched to the performance of the convolutional operator. The incoming results can either arrive in serial bursts or as complete result vectors. A k-WTA implementation could wait until all bursts have been concatenated and a complete output result is available, or take advantage of the burst intervals and combinationally sort the burst values before loading the values into one of the FIFOs. <figref idref="DRAWINGS">FIG. <b>12</b>A</figref> is a block diagram illustrating the sorting circuit <b>1200</b> for serially processing complementary sparse convolutions. Alternatively, all the activation results could be computed in parallel, partitioned into M groups, and pushed through M instances of a combinational sort before being loaded in parallel to the M FIFOs. <figref idref="DRAWINGS">FIG. <b>12</b>B</figref> is a block diagram illustrating the sorting circuit <b>1250</b> for parallel processing complementary sparse convolutions.</p><p id="p-0194" num="0187">Upon reading this disclosure, those of skill in the art will appreciate still additional alternative designs for processing nodes. Thus, while particular embodiments and applications have been illustrated and described, it is to be understood that the invention is not limited to the precise construction and components disclosed herein and that various modifications, changes and variations which will be apparent to those skilled in the art may be made in the arrangement, operation and details of the method and apparatus disclosed herein without departing from the spirit and scope of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An accelerator for performing operations on tensors, the accelerator comprising:<claim-text>a plurality of first computation circuits configured to perform first computations between values in a process tensor and values in an activation tensor to generate a plurality of products, wherein the values in the process tensor are associated with tensor identifiers;</claim-text><claim-text>a plurality of second computation circuits, each second computation circuit configured to receive a subset of the products that are grouped based on the tensor identifiers and perform a second computation for the subset of the products to generate an output value, the plurality of second computation circuits configured to generate a plurality of output values; and</claim-text><claim-text>an activation circuit coupled to the plurality of second computation circuits, the activation circuit configured to select a subset of the output values as winners of an activation selection and set remaining of the plurality of output values as zero.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The accelerator of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the activation circuit is further configured to boost one or more output values of the plurality of output values before the activation selection.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The accelerator of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the one or more output values that are boosted correspond to one or more nodes that are set to zero in a previous cycle of operation.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The accelerator of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the activation circuit is configured to select a fixed number of output values as a number of output values in the subset that are selected as the winners.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The accelerator of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the process tensor is a complementary dense process tensor that is combined from a plurality of sparse process tensors, and each of the tensor identifiers is used to identify one of the sparse process tensors.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The accelerator of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a routing circuit coupled to the plurality of first computation circuits, the routing circuit configured to:<claim-text>carry over the tensor identifiers of the values in the process tensor to the plurality of products, and</claim-text><claim-text>divide the plurality of products into subsets based on the tensor identifiers; and</claim-text></claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The accelerator of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the routing circuit comprises an arbiter circuit that controls routing of a product of the plurality of products to one of the adder trees.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The accelerator of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the activation circuit comprises a histogram memory that is configured to build a histogram that represents a distribution of the plurality of output values.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The accelerator of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the routing circuit comprises a sorting circuit configured to select the winners from serial bursts of the output values.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The accelerator of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the routing circuit comprises a sorting circuit configured to select the winners from the plurality of output values in parallel.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A method comprising:<claim-text>performing first computations between values in a process tensor and values in an activation tensor to generate a plurality of products, wherein the values in the process tensor are associated with tensor identifiers;</claim-text><claim-text>grouping the plurality of products into a plurality of subsets of the products based on the tensor identifiers;</claim-text><claim-text>performing a second computation for each subset of the products to generate an output value, the plurality of subsets of the products generating a plurality of output values;</claim-text><claim-text>selecting a subset of the output values as winners of an activation selection; and</claim-text><claim-text>setting remaining of the plurality of output values as zero.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising boosting one or more output values of the plurality of output values before the activation selection.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the one or more output values that are boosted correspond to one or more nodes that are set to zero in a previous cycle of operation.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising selecting a fixed number of output values as a number of output values in the subset that are selected as the winners.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the process tensor is a complementary dense process tensor that is combined from a plurality of sparse process tensors, and each of the tensor identifiers is used to identify one of the sparse process tensors.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising routing the plurality of products, wherein routing the plurality of products comprises:<claim-text>carrying over the tensor identifiers of the values in the process tensor to the plurality of products, and</claim-text><claim-text>dividing the plurality of products into subsets based on the tensor identifiers; and</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein routing the plurality of products is performed by an arbiter circuit that controls routing of a product of the plurality of products to an adder tree.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein selecting the subset of the output values as the winners comprises building a histogram that represents a distribution of the plurality of output values.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein routing the plurality of products is performed by a sorting circuit configured to select the winners from serial bursts of the output values.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein routing the plurality of products is performed by a sorting circuit configured to select the winners from the plurality of output values in parallel.</claim-text></claim></claims></us-patent-application>