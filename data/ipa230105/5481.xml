<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005482A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005482</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940732</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>M</subclass><main-group>3</main-group><subgroup>51</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>222</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>M</subclass><main-group>3</main-group><subgroup>5158</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>M</subclass><main-group>2201</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>M</subclass><main-group>2201</main-group><subgroup>39</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">AUTOMATED CALLING SYSTEM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17049689</doc-number><date>20201022</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11468893</doc-number></document-id></parent-grant-document><parent-pct-document><document-id><country>WO</country><doc-number>PCT/US2020/031507</doc-number><date>20200505</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17940732</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62843660</doc-number><date>20190506</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>GOOGLE LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Aharoni</last-name><first-name>Asaf</first-name><address><city>Ramat Hasharon</city><country>IL</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Segalis</last-name><first-name>Eyal</first-name><address><city>Tel Aviv</city><country>IL</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Leviathan</last-name><first-name>Yaniv</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods, systems, and apparatus for an automated calling system are disclosed. Some implementations are directed to using a bot to initiate telephone calls and conduct telephone conversations with a user. The bot may be interrupted while providing synthesized speech during the telephone call. The interruption can be classified into one of multiple disparate interruption types, and the bot can react to the interruption based on the interruption type. Some implementations are directed to determining that a first user is placed on hold by a second user during a telephone conversation, and maintaining the telephone call in an active state in response to determining the first user hung up the telephone call. The first user can be notified when the second user rejoins the call, and a bot associated with the first user can notify the first user that the second user has rejoined the telephone call.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="109.64mm" wi="158.75mm" file="US20230005482A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="225.72mm" wi="156.38mm" file="US20230005482A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="227.58mm" wi="157.82mm" orientation="landscape" file="US20230005482A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="213.02mm" wi="127.59mm" file="US20230005482A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="222.59mm" wi="127.76mm" file="US20230005482A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="206.93mm" wi="145.03mm" orientation="landscape" file="US20230005482A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">Users may need to collect types of information that is not easily obtained without human interaction. For example, in order to verify or collect data from multiple places of business or organizations, a user may need to call each of the businesses or organizations in order to gather the information. While web search engines can assist users with such tasks by providing contact information for a service or business, the user must still call the service or business themselves to complete the task themselves.</p><p id="p-0003" num="0002">In order to maintain a database of information gathered from multiple places of business or organizations, a human operator can initiate automated calls to large numbers of businesses to collect data, but selecting the callees (e.g., all restaurants in a particular town that serve the same cuisine) and placing the calls can be time-consuming when performed manually. Moreover, determining when and whether to place the calls generally requires human analysis of existing data in order to identify a need for verification, updating, or supplemental information.</p><p id="p-0004" num="0003">Users may also wish to perform tasks such as make appointments or hire a service. However, there is generally a person with whom a user must interact to complete the desired task. For example, a user may be required to call and speak with a hostess in order to make a reservation at a small restaurant that does not have a website. In some cases, even when users place the calls themselves, they may encounter automated phone trees that often accept only a limited set of user responses.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0005" num="0004">Some implementations are directed to using a bot to initiate telephone calls and conduct telephone conversations with a user. The bot may be interrupted while providing synthesized speech during the telephone call. The interruption can be classified into one of multiple disparate interruption types, and the bot can react to the interruption based on the interruption type. Some implementations are directed to determining that a first user is placed on hold by a second user during a telephone conversation, and maintaining the telephone call in an active state in response to determining the first user hung up the telephone call. The first user can be notified when the second user rejoins the call, and a bot associated with the first user can notify the first user that the second user has rejoined the telephone call.</p><p id="p-0006" num="0005">In some implementations, a method implemented by one or more processors is provided, and includes initiating a telephone call with a user using a bot that is configured to initiate telephone calls and conduct telephone conversations, and providing, for output at a corresponding computing device of the user, synthesized speech of the bot. The method further includes, while providing the synthesized speech of the bot, receiving, from the user, a user utterance that interrupts the synthesized speech the bot, in response to receiving the user utterance that interrupts the synthesized speech, classifying the received user utterance as a given type of interruption of multiple disparate types of interruptions, and determining, based on the given type of interruption, whether to continue providing, for output at the corresponding computing device of the user, the synthesized speech of the bot.</p><p id="p-0007" num="0006">These and other implementations of technology disclosed herein can optionally include one or more of the following features.</p><p id="p-0008" num="0007">In some implementations, the given type of interruption is a non-meaningful interruption. Classifying the received user utterance as the non-meaningful interruption includes processing audio data corresponding to the received user utterance or a transcription corresponding to the received user utterance to determine that the received user utterance includes one or more of: background noise, affirmation words or phrases, or filler words or phrases, and classifying the received user utterance as the non-meaningful interruption based on determining that the received user utterance includes one or more of: background noise, affirmation words or phrases, or filler words or phrases.</p><p id="p-0009" num="0008">In some versions of those implementations, determining whether to continue providing the synthesized speech of the bot includes determining to continue providing the synthesized speech of the bot based on classifying the received user utterance as the non-meaningful interruption.</p><p id="p-0010" num="0009">In some implementations, the given type of interruption is a non-critical meaningful interruption. Classifying the received user utterance as the non-critical meaningful interruption includes processing audio data corresponding to the received user utterance or a transcription corresponding to the received user utterance to determine that the received user utterance includes a request for information that is known by the bot, and that is yet to be provided, and classifying the received user utterance as the non-critical meaningful interruption based on determining that the received user utterance includes the request for the information that is known by the bot, and that is yet to be provided.</p><p id="p-0011" num="0010">In some versions of those implementations, determining whether to continue providing the synthesized speech of the bot includes, based on classifying the user utterance as the non-critical meaningful interruption, determining a temporal point in a remainder portion of the synthesized speech to cease providing, for output, the synthesized speech of the bot, determining whether the remainder portion of the synthesized speech is responsive to the received utterance, and in response to determining that the remainder portion is not responsive to the received user utterance: providing, for output, an additional portion of the synthesized speech that is responsive to the received user utterance, and that is yet to be provide, and after providing, for output, the additional portion of the synthesized speech, continuing providing, for output, the remainder portion of the synthesized speech of the bot from the temporal point.</p><p id="p-0012" num="0011">In some further versions of those implementations, the method further includes, in response to determining that the remainder portion is responsive to the received user utterance, continuing providing, for output, the remainder portion of the synthesized speech of the bot from the temporal point.</p><p id="p-0013" num="0012">In some implementations, the given type of interruption is a critical meaningful interruption. Classifying the received user utterance as the critical meaningful interruption includes processing audio data corresponding to the received user utterance or a transcription corresponding to the received user utterance to determine that the received user utterance includes a request for the bot to repeat the synthesized speech or a request to place the bot on hold, and classifying the received user utterance as the non-critical meaningful interruption based on determining that the received user utterance includes the request for the bot to repeat the synthesized speech or the request to place the bot on hold.</p><p id="p-0014" num="0013">In some versions of those implementations, determining whether to continue providing the synthesized speech of the bot includes providing, for output, a remainder portion of a current word or term of the synthesized speech of the bot, and after providing, for output, the remainder portion of the current word or term, cease providing, for output, the synthesized speech of the bot.</p><p id="p-0015" num="0014">In some implementations, classifying the received user utterance as the given type of interruption includes processing audio data corresponding to the received user utterance or a transcription corresponding to the received user utterance using a machine learning model to determine the given type of interruption.</p><p id="p-0016" num="0015">In some versions of those implementations, the method further includes training the machine learning model using a plurality of training instances. Each of the training instances include training instance input and corresponding training instance output, each training instance input including training audio data corresponding to an interruption utterance or a transcription corresponding to the interruption utterance, and each corresponding training instance output including a ground truth label corresponding the type of interruption included in the interruption utterance. In some further versions of those implementations, processing the audio data corresponding to the received user utterance or the transcription corresponding to the received user utterance using the machine learning model further includes processing the synthesized speech being output when the user utterance was received along with the audio data or the transcription.</p><p id="p-0017" num="0016">In some implementations, classifying the received user utterance as the given type of interruption includes processing audio data corresponding to the received user utterance or a transcription corresponding to the received user utterance using one or more rules that match tokens of the received user utterance to one or more terms associated with each of the multiple disparate interruption types.</p><p id="p-0018" num="0017">In some implementations, initiating the telephone call with the user using the bot is responsive to receiving user input, from a given user associated with the bot, to initiate the telephone call. In some versions of those implementations, the user input to initiate the telephone call includes information points that are to be included in the synthesized speech that is provided for output at the corresponding computing device of the user.</p><p id="p-0019" num="0018">In some implementations, a method implemented by one or more processors is provided, and includes determining that a first user and a second user are engaged in a telephone call. The first user being associated with a corresponding first computing device, and the second user being associated with a corresponding second computing device. The method further includes determining that the second user has placed the first user on hold, determining that the first user has hung up the corresponding first computing device, and, in response to determining that the first user has hung up the corresponding first phone and while the first user is on hold: maintaining the telephone call in an active state, and determining whether the second user has rejoined the telephone call. The method further includes, in response to determining that the second user has rejoined the telephone call: using a bot associated with the corresponding first computing device of the first user to continue the telephone call with the second user, providing, for output at the corresponding first computing device, a notification for the first user to rejoin the telephone call, determining that the first user has rejoined the telephone call, and, in response to determining that the first user has rejoined the telephone call, ceasing the telephone conversation between the second user and the bot.</p><p id="p-0020" num="0019">These and other implementations of technology disclosed herein can optionally include one or more of the following features.</p><p id="p-0021" num="0020">In some implementations, using the bot associated with the corresponding first computing device of the first user to continue the telephone call with the second user includes generating synthesized speech of the bot that indicates the first user is not an active participant on the telephone call, and providing, for output at the corresponding second computing device of the second user, the synthesized speech of the bot.</p><p id="p-0022" num="0021">In some implementations, the method further includes prior to the first user rejoining the telephone call, receiving, from the second user, a user utterance, generating a transcription of the user utterance, and including the transcription of the user utterance in the notification for the first user to rejoin the telephone call.</p><p id="p-0023" num="0022">In some implementations, maintaining the telephone call in the active state includes muting one or more microphones of the corresponding first computing device, and maintaining a telephonic connection between the corresponding first computing device and the corresponding second computing device.</p><p id="p-0024" num="0023">In addition, some implementations include one or more processors (e.g., central processing unit(s) (CPU(s)), graphical processing unit(s) (GPU(s)), and/or tensor processing unit(s) (TPU(s)) of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the aforementioned methods. Some implementations also include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the aforementioned methods. Some implementations also include a computer program product including instructions executable by one or more processors to perform the aforementioned methods.</p><p id="p-0025" num="0024">It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.</p><p id="p-0026" num="0025">The above description is provided as an overview of only some implementations disclosed herein. Those implementations, and other implementations, are described in additional detail herein.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example system for handling interruptions received from a user while the user and a bot of a call initiating system are having a telephone conversation.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example system that monitors a telephone call where one user is on hold and that notifies the user on hold when the other user has rejoined the telephone call.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is a flowchart of an example process for handling interruptions received from a user while the user and a bot of a call initiating system are having a telephone conversation</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is a flowchart of an example process for monitoring a telephone call where one user is on hold and for notifying the user on hold when the other user has rejoined the call.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example of a computing device and a mobile computing device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0032" num="0031">Like reference numbers and designations in the various drawings indicate like elements.</p><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example system <b>100</b> for handling interruptions received from a human representative <b>102</b> while the representative <b>102</b> and a bot of the call initiating system <b>104</b> are having a telephone conversation. Briefly, and as described in more detail below, the call initiating system <b>104</b> is performing a task for a user by placing a telephone call (e.g., initiating an assisted call on behalf of the user). The representative <b>102</b> answers the telephone call and has a telephone conversation with the bot of the call initiating system <b>104</b> on behalf of the user. During the telephone call, the representative <b>102</b> may interrupt the bot (e.g., during the assisted call). The bot can identify the interruption, classify the interruption into one of multiple disparate interruption types, and continue the telephone conversation on behalf of the user based on the type of interruption.</p><p id="p-0034" num="0033">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a user may interact with a digital assistant by requesting that the digital assistant make a reservation for Burger Palace the following day for two people at 7:00 pm. Burger Palace may be a small restaurant that does not have an online portal where the digital assistant can request the reservation. Instead, prospective patrons must call Burger Palace on the telephone to make a reservation. In this case, the digital assistant requests that the call initiating system <b>104</b> initiate a telephone call with the restaurant and make the reservation on behalf of the user. Although the system <b>100</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is illustrated as being implemented by server(s) over network(s) (e.g., LAN, WAN, WiFi, Bluetooth, and/or other network(s)), it should be understood that is for the sake of example and is not meant to be limiting. For example, the system <b>100</b> can be implemented locally at a computing device of the user that initiated the assisted call and/or implemented by the computing device and the server(s) in a distributed manner over the network(s).</p><p id="p-0035" num="0034">The digital assistant can provide the call initiating system <b>104</b> with data included in the request provided by the user. The information may include the requested date and time of the reservation (e.g., tomorrow at 7:00 pm), the requested business (e.g., Burger Palace), and number of people in the party (e.g., two). For requests other than restaurant reservations, the information may include the name of a requested service provider (e.g., an airline company, a utilities provider, and/or any other service provider), a description of the request for the service provider (e.g., making/modifying/discontinuing a service or reservation), and/or any other information that may be solicited by the representative <b>102</b> in performing the task on behalf of the user. The call initiating system <b>104</b> may store this data as information points <b>106</b>. The information points <b>106</b> include, for example, information that the call initiating system <b>104</b> should provide to the representative <b>102</b> during the telephone call or that the call initiating system <b>104</b> should request that the representative <b>102</b> provide during the telephone call.</p><p id="p-0036" num="0035">For example, assume the call initiating system <b>104</b> initiates a telephone call with Burger Palace, and assume the representative <b>102</b> answers the phone <b>108</b>. Further assume, the representative <b>102</b> speaks the utterance <b>110</b> of &#x201c;Thank you for calling Burger Palace. How can I help you?&#x201d; The bot of the call initiating system <b>104</b> can detect the utterance <b>110</b>, and provide the audio data of the utterance <b>110</b> to the speech recognizer <b>112</b>. The speech recognizer <b>112</b> can generate a transcription of the utterance <b>110</b>, and provide the transcription of the utterance <b>110</b> to the transcription generator <b>114</b>.</p><p id="p-0037" num="0036">The transcription generator <b>114</b> can generate a transcription that responsive to the utterance <b>110</b> of the representative <b>102</b>. Further, the transcription generator <b>114</b> may access the information points <b>106</b> to determine whether providing information included in one of the information points <b>106</b> is an appropriate response to the utterance <b>110</b> of the representative <b>102</b>. To determine whether the information included in one of the information points <b>106</b> is an appropriate response to the utterance <b>110</b> of the representative <b>102</b>, the transcription generator <b>114</b> may use a variety of techniques, including a rules based approach and/or a machine learning based approach. In some implementations, the transcription generator <b>114</b> may identify keywords or phrases in the transcription of the utterance <b>110</b> of the representative <b>102</b>. The transcription generator <b>114</b> may tokenize the transcription of the utterance <b>110</b> of the representative <b>102</b>, and identify keywords among the tokenized terms. The transcription generator <b>114</b> may then use those keywords or phrases to determine the subject matter of the utterance <b>110</b> of the representative <b>102</b>. The transcription generator <b>114</b> may use the subject matter of the utterance <b>110</b> of the representative <b>102</b> to generate a transcription of a response.</p><p id="p-0038" num="0037">In some implementations, the transcription generator <b>114</b> uses a model trained using machine learning to determine subject matter of and/or an appropriate response to the utterance <b>110</b> of the representative <b>102</b>. The call initiating system <b>104</b> may access training data that includes a log of previous conversations. The previous conversations may be specific to a type of business or organization, such as a restaurant business, an airline business, a government agency, and/or conversations specific to other businesses or organizations. Each of the utterances in the corresponding conversations may include keyword labels. The keyword labels can include the terms in the utterances, semantic representations of the terms in the utterances, and/or other types of labels for annotating utterances or transcriptions thereof. The call initiating system <b>104</b> can use the training data to train a machine learning model to identify keywords of an utterance when audio data that captures the utterance (and/or a transcription thereof) is applied as input across the machine learning model. In some additional and/or alternative implementations, the call initiating system <b>104</b> can use the training data to train the machine learning model to generate keywords for an appropriate response when audio data that captures the utterance (and/or a transcription thereof) is applied as input across the machine learning model.</p><p id="p-0039" num="0038">The transcription generator <b>114</b> may use any combination of these machine learning models and/or a rule based approach to generate an appropriate response to the utterance <b>110</b> in combination with the information points <b>106</b>. More particularly, the transcription generator <b>114</b> may use the machine learning models to generate the appropriate response, and use the information points <b>106</b> to fill in any details that may be unique to the current conversation. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the transcription generator <b>114</b> can analyze the transcription of the utterance <b>110</b> of the representative <b>102</b> to generate a response shell to which the transcription generator <b>114</b> applies one or more of the information points <b>106</b>. For example, the transcription generator <b>114</b> may generate a response shell such as &#x201c;I'd like to &#x3c;blank&#x3e;.&#x201d; The transcription generator <b>114</b> may fill in the blank with the information points <b>106</b> that include the requested task (e.g., make a reservation), the date and time (e.g., tomorrow at 7:00 PM), and the party size (e.g., two people). In this example, the transcription generator <b>114</b> can generate the transcription of &#x201c;I'd like to make a reservation for two people tomorrow at 7 pm&#x201d; responsive to processing the transcription of the utterance <b>110</b> of the representative <b>102</b>.</p><p id="p-0040" num="0039">The transcription generator <b>114</b> can provide the transcription generated by transcription generator <b>114</b> that is responsive to the utterance <b>110</b> of the representative <b>102</b> to the speech synthesizer <b>116</b>. The speech synthesizer <b>116</b> can generate synthesized speech that includes audio data corresponding to the received transcription that is responsive to the utterance <b>110</b> of the representative <b>102</b>. In some implementations, the speech synthesizer <b>116</b> may be configured to output synthesized speech in several different voices. For example, the speech synthesizer <b>116</b> may be configured to output synthesized speech (or a portion thereof during the conversation) in a voice similar to the user who requested the reservation, a reserved voice for the digital assistant, a voice of an operator who may take over for the bot if the transcription generator <b>114</b> is unable to generate an appropriate response, or another voice selected by the user who requested the reservation or selected by the call initiating system <b>104</b>.</p><p id="p-0041" num="0040">The call initiating system <b>104</b> can output the synthesized speech <b>118</b> of &#x201c;I'd like to make a reservation for two people tomorrow at 7 pm&#x201d; that was generated by the speech synthesizer <b>116</b> in response to receiving the utterance <b>110</b> of the representative <b>102</b>. At this point, the transcription generator <b>114</b> may mark the information points <b>106</b> of date and time provided <b>122</b>, reservation requested <b>124</b>, and party size provided <b>126</b> as satisfied because the bot provided that information to the representative <b>102</b> as part of synthesized speech <b>118</b>. Further assume in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref> that the representative <b>102</b> hears the synthesized speech <b>118</b> through the telephone <b>108</b> and responds with the utterance <b>120</b>. In response to detecting the utterance <b>120</b>, the call initiating system <b>104</b> uses the speech recognizer <b>112</b> to generate a transcription of the utterance <b>120</b> of the representative <b>102</b> and the transcription generator <b>114</b> to generate a transcription of an appropriate response in the same or similar manner described above with respect to utterance <b>110</b>.</p><p id="p-0042" num="0041">In this instance, the transcription generator <b>114</b> determines that the representative <b>102</b> is requesting information that has already been provided. The transcription generator <b>114</b> may determine that the transcription of the utterance <b>120</b> includes keywords or phrases such as &#x201c;what time&#x201d; and &#x201c;how many people.&#x201d; The transcription generator <b>114</b> may determine that these two phrases correspond to the information points <b>106</b> of date and time provided <b>122</b> and party size provided <b>126</b> that were included in the synthesized speech <b>118</b>. Nonetheless, the transcription generator <b>114</b> can generate another transcription to provide these particular information points <b>106</b> to the representative <b>102</b> again. For example, the transcription generator <b>114</b> may generate the transcription of &#x201c;The reservation should be for two people at 7 pm&#x201d; response to receiving the utterance <b>120</b>. Further, the transcription generator <b>114</b> can provide this transcription to the speech synthesizer <b>116</b>.</p><p id="p-0043" num="0042">The speech synthesizer <b>116</b> can generate synthesized speech <b>128</b> that includes audio data corresponding to the transcription that is generated responsive to the utterance <b>120</b>. Moreover, the call initiating system <b>104</b> can output the synthesized speech <b>128</b> of &#x201c;The reservation should be for two people at 7 pm&#x201d; that was generated by the speech synthesizer <b>116</b> in response to receiving the utterance <b>120</b> of the representative <b>102</b>. Further assume in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref> that the representative <b>102</b> hears the synthesized speech <b>128</b> through the telephone <b>108</b>, and, in the middle of outputting the synthesized speech <b>128</b>, the representative <b>102</b> interrupts and speaks the utterance <b>130</b> of &#x201c;Oh I'm sorry, you already said two people at 7 pm.&#x201d;</p><p id="p-0044" num="0043">The interruption detector <b>132</b> may be continuously analyzing incoming audio data from the representative <b>102</b> and/or corresponding transcriptions thereof to determine whether an interruption has occurred. The interruption detector <b>132</b> can compare a signal strength of the incoming audio data to a threshold. If the signal strength of the incoming audio is above a certain threshold while the call initiating system <b>104</b> is outputting synthesized speech (e.g., above an ambient noise level), then the interruption detector <b>132</b> may determine that an interruption has occurred. In some implementations, the interruption detector <b>132</b> may analyze the transcription of the utterance <b>130</b> generated by the speech recognizer <b>112</b> and/or an energy level of the utterance <b>130</b>. In this instance, if the speech recognizer <b>112</b> is able to generate a transcription of the utterance <b>130</b>, then the call initiating system <b>104</b> may determine that the representative <b>102</b> or another person is speaking and that the telephone <b>108</b> is detecting that speech, and the interruption detector <b>132</b> may determine that there is an interruption. By using this technique, the interruption detector <b>132</b> may not identify an interruption if there is only background noise, somebody is speaking in the background, or the representative <b>102</b> is speaking in a lower volume, which likely indicates that the representative <b>102</b> is not speaking to the bot.</p><p id="p-0045" num="0044">For example, assume the signal strength of the corresponding audio data for the utterance <b>130</b> is fifty-five decibels, and assume the threshold is forty decibels. In this example, the speech recognizer <b>112</b> can generate a transcription of the utterance <b>130</b> of the representative <b>102</b> based on the signal strength (e.g., fifty-five decibels) satisfying the threshold (e.g., forty decibels). Based on the speech recognizer <b>112</b> generating a transcription responsive to detecting the utterance <b>130</b> and/or the signal strength of the corresponding audio data for the utterance <b>130</b> satisfying the threshold, the interruption detector <b>132</b> can determine that the utterance <b>130</b> is an interruption utterance during the conversation (referred to hereinafter as &#x201c;the interruption utterance <b>130</b>&#x201d;).</p><p id="p-0046" num="0045">The interruption classifier <b>136</b> can classify the interruption as a given type of interruption from multiple disparate interruption types <b>138</b>. The interruption types <b>138</b> can be mutually exclusive, and can include, for example, a non-meaningful interruption, a non-critical meaningful interruption, a critical meaningful interruption, and/or other types of interruptions. A non-meaningful interruption may be an interruption that does not necessitate the call initiating system <b>104</b> to change the course of the conversation. For example, a given interruption detected by the interruption detector <b>132</b> can be classified as a non-meaningful interruption if it is determined that the detected interruption includes background noise, a whisper, background conversation, or the representative <b>102</b> confirming what the bot is saying by using words such as &#x201c;right,&#x201d; &#x201c;ok,&#x201d; uh-huh,&#x201d; or other similar affirmation/filler words and/or phrases. As another example, a given interruption detected by the interruption detector <b>132</b> can be classified as a non-meaningful interruption if it is determined that the representative <b>102</b> is repeating what the bot said. For instance, assume that the bot says &#x201c;I'd like to make a reservation for two people tomorrow&#x201d;, and further assume, before the bot finishes and says, &#x201c;at 7 pm,&#x201d; the representative <b>102</b> says &#x201c;two people, tomorrow.&#x201d; In this instance, because the representative <b>102</b> repeated what the bot said, the interruption classifier <b>136</b> can classify the interruption as a non-meaningful interruption.</p><p id="p-0047" num="0046">A non-critical meaningful interruption may be an interruption that necessitates the call initiating system <b>104</b> to change the course of the conversation, but does not necessitate that the bot immediately stop speaking. For example, a given interruption detected by the interruption detector <b>132</b> can be classified as a non-critical interruption if it is determined that the detected interruption includes a request for information points <b>106</b> that are yet to be provided. For instance, assume that the bot says &#x201c;I'd like to make a reservation for two people tomorrow&#x201d;, and further assume, before the bot finishes and says, &#x201c;at 7 pm,&#x201d; the representative <b>102</b> says &#x201c;at what time?&#x201d; In this instance, because the representative <b>102</b> is requesting information that is known to the bot (e.g., time and date provided <b>122</b>), the interruption classifier <b>136</b> can classify the interruption as a non-critical interruption, and output synthesized speech of &#x201c;7 PM&#x201d; without pausing for the interruption. In some implementations, after detecting a non-critical meaningful interruption, the bot may reach a natural pause in the synthesized speech being before outputting all of the synthesized speech, and may cease outputting a remainder of the synthesized speech. For example, a given interruption detected by the interruption detector <b>132</b> can be classified as a non-meaningful interruption if it is determined that the detected interruption includes the representative <b>102</b> stating &#x201c;excuse me,&#x201d; &#x201c;please repeat,&#x201d; &#x201c;slow down,&#x201d; and/or other words and/or phrases requesting that the bot clarifies or re-state preceding synthesized speech. For instance, assume that the bot says &#x201c;I'd like to make a reservation for two people tomorrow&#x201d;, and further assume, before the bot finishes and says, &#x201c;at 7 pm,&#x201d; the representative <b>102</b> says &#x201c;pardon me.&#x201d; In this instance, because the representative <b>102</b> is requesting the bot to output the synthesized speech again, the interruption classifier <b>136</b> can classify the interruption as a non-critical interruption, and the bot can finish outputting the remainder of the synthesized speech before outputting the synthesized speech again.</p><p id="p-0048" num="0047">A critical meaningful interruption may be an interruption that necessitates the call initiating system <b>104</b> to change the course of the conversation while necessitating that the bot immediately stop speaking. In some implementations, after a critical meaningful interruption is detected, the bot may finish the word that the bot is currently speaking and then cease outputting the remainder of the synthesized speech. For instance, assume that the bot says &#x201c;I'd like to make a reservation for two people tomorrow&#x201d;, and further assume, before the bot finishes and says, &#x201c;at 7 pm,&#x201d; the representative <b>102</b> says &#x201c;please hold,&#x201d; &#x201c;stop talking,&#x201d; &#x201c;wait, wait, wait,&#x201d; and/or other similar words and/or phrases that indicate the bot should cease outputting of the synthesized speech. In this instance, because the representative <b>102</b> is requesting that the bot cease providing of the synthesized speech, the interruption classifier <b>136</b> can classify the interruption as a critical meaning interruption. Accordingly, after the representative <b>102</b> has finished speaking the interrupting utterance, the bot may respond to the interrupting utterance with the appropriate action (e.g., ending the conversation, cease outputting of synthesized speech, repeating most recently output synthesized speech, and/or other actions responsive to the detected interruption).</p><p id="p-0049" num="0048">The interruption classifier <b>136</b> may classify the detected interruption using various classification techniques. In some implementations, the interruption classifier <b>136</b> can process the interrupting utterance of the representative <b>102</b> using one or more machine learning models to classify the interrupting utterance of the representative <b>102</b> as one of the interruption types <b>136</b>. The call initiating system <b>104</b> may can train the one or more machine learning models using training instances. Each of the training instances can include training instance input and corresponding training instance output. The training instance input can include audio data capturing interrupting utterances (and/or transcriptions thereof), and the corresponding training instance output can include ground truth labels that correspond to a classification of the interrupting utterance (e.g., non-meaningful interruption, a non-critical meaningful interruption, a critical meaningful interruption, and/or other types of interruptions). For example, a first training instance input can include audio data capturing an utterance of &#x201c;wait, I can't hear you&#x201d; (and/or a transcription thereof), and first training instance output can include a ground truth label corresponding to a critical meaningful interruption. Further, a second training instance input can audio data capturing an utterance of &#x201c;tomorrow at&#x201d; (and/or a transcription thereof), and first training instance output can include a ground truth label corresponding to a non-meaningful interruption. In some implementations, the training instance input may further include audio data capturing synthesized speech (and/or a transcription thereof) of the synthesized speech that was being output when the interrupting utterance was detected.</p><p id="p-0050" num="0049">In some additional and/or alternative implementations, the interruption classifier <b>136</b> can process the interrupting utterance of the representative <b>102</b> using one or more rules to classify the interrupting utterance of the representative <b>102</b> as one of the interruption types <b>136</b>. In some versions of those implementations, the interruption classifier <b>136</b> may tokenize the interrupting utterance, and compare the tokens to various groups of terms. If the tokens include terms of a first group, then the interruption classifier <b>136</b> may classify the interruption as a critical meaningful interruption. Further, if the tokens include terms from a second group that are distinct from the terms included in the first group, then the interruption classifier <b>136</b> may classify the interruption as a non-critical meaningful interruption. Even further, if the tokens include terms from a third group that are distinct from the terms of both the first group and the second group, then the interruption classifier <b>136</b> may classify the interruption as a non-meaningful interruption. By using this tiered approach for classifying the interruptions, the interruption classifier <b>136</b> may err on the side of over classifying interruptions because additional pauses in the conversation caused by classifying a given interruption as a non-critical meaningful interruption rather than a non-meaningful interruption allows both the call initiating system <b>104</b> and the representative to process exchanged subject matter of the conversation, thereby concluding the conversation in a quick and efficient manner. In various implementations, the interruption classifier <b>136</b> may not use a third group of terms because the interruption classifier <b>136</b> may classify the interruption as a non-meaningful interruption if the terms of the interruption are not included in either the first or second group of terms. Moreover, in various implementations, the interruption classifier <b>136</b> may determine that there is no interruption if no terms of the interruption are in the first, second, or third group of terms.</p><p id="p-0051" num="0050">In some versions of those implementations, the terms in each of the groups that the interruption classifier <b>136</b> compares the tokens to may be fixed, while in other versions of those implementations, the tokens may change depending on the telephone conversation. For example, the first group of terms may include &#x201c;stop talking&#x201d; and &#x201c;can't hear,&#x201d; whereas the second group of terms may include &#x201c;excuse me,&#x201d; &#x201c;I'm sorry,&#x201d; and/or terms that are semantically similar to those included in the synthesized speech. The terms that are semantically similar to those included in the synthesized speech may include terms that are semantically similar to the information points <b>106</b>. For example, the information points <b>106</b> depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref> can include information provided the user, such as &#x201c;two people,&#x201d; &#x201c;7 pm,&#x201d; and &#x201c;tomorrow.&#x201d; In this example, the semantically similar terms for the second group may include &#x201c;three people (as opposed to &#x201c;two people&#x201d;), &#x201c;8 pm&#x201d; (as opposed to &#x201c;7 pm&#x201d;), and &#x201c;next week&#x201d; (as opposed to &#x201c;tomorrow&#x201d;). In this manner, if the terms of the interruption include any semantically similar terms that suggest the representative <b>102</b> misunderstood the bot, then the interruption classifier <b>136</b> may classify the interruption as a non-critical meaningful interruption. The third group of terms may include terms such as &#x201c;right,&#x201d; &#x201c;uh huh,&#x201d; and/or other filler/affirmation terms that conform with the representative <b>102</b> understanding information included in the information points <b>106</b> (e.g., &#x201c;two people,&#x201d; &#x201c;7 pm,&#x201d; and &#x201c;tomorrow&#x201d;).</p><p id="p-0052" num="0051">Moreover, in the example shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the representative <b>102</b> may begin to speak the interruption utterance <b>130</b> after the bot says &#x201c;for two&#x201d; of the synthesized speech <b>128</b>. The interruption detector <b>132</b> may detect an interruption based on the speech recognizer <b>112</b> recognizing speech in the interruption utterance <b>130</b>, and/or based on the energy level of the audio data corresponding to the interruption utterance <b>130</b> being above a threshold as discussed above. The interruption classifier <b>136</b> may apply an initial portion of the transcription of the interruption utterance <b>130</b> and/or the synthesized speech <b>128</b> of the bot as input across a trained machine learning model and/or rules based model. The initial portion may include &#x201c;oh, I'm sorry,&#x201d; of the interruption utterance <b>130</b>. In implementations where the interruption classifier <b>136</b> utilizes a machine learning model, the interruption classifier <b>136</b> may classify the interruption included in the interruption utterance <b>130</b> is a non-critical meaningful interruption based on processing the audio data corresponding to the interruption utterance <b>130</b> (and optionally the synthesized speech <b>128</b> that immediately preceded the interruption utterance <b>130</b>). In implementations where the interruption classifier <b>136</b> includes one or more rules, the interruption classifier <b>136</b> may tokenize the initial portion of the interruption utterance <b>130</b>, and the tokens may include the terms &#x201c;oh&#x201d; and &#x201c;I'm sorry.&#x201d; Further, the interruption classifier <b>136</b> may continue to tokenize the interruption utterance <b>130</b> as the speech recognizer transcribes additional words. Although the terms &#x201c;oh&#x201d; and &#x201c;I'm sorry&#x201d; may not match any terms in the first group the term &#x201c;I'm sorry&#x201d; may match a term in the second group. In this case, the interruption classifier <b>136</b> may classify the interruption depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref> as a non-critical meaningful interruption.</p><p id="p-0053" num="0052">With the interruption classified, the pause inserter <b>142</b> can identify whether and/or when the bot should cease outputting synthesized speech. For example, if the interruption is a non-meaningful interruption, then the pause inserter <b>142</b> may determine that there is no need to insert a pause in the synthesized speech of the bot, and determine that the bot may continue outputting synthesized speech as if the interruption did not occur. As another example, if the interruption is a critical meaningful interruption, then the pause inserter <b>142</b> may determine that a pause needs to be inserted in the synthesized speech when the bot completes outputting of a current word and/or phrase. In this example, the pause inserter <b>142</b> can provide an indication to the transcription generator <b>114</b> to cease providing terms and/or words to the speech synthesizer <b>116</b> to be output to the representative <b>102</b>. Further, the transcription generator <b>114</b> need not provide an additional transcription to the speech synthesizer <b>116</b> until the representative <b>102</b> completes speaking of the interruption utterance <b>130</b>.</p><p id="p-0054" num="0053">Upon receiving a pause instruction from the pause inserter <b>142</b>, the transcription generator <b>114</b> may update the information points <b>106</b> to indicate whether or not they have been provided to the representative. For instance, the transcription generator <b>114</b> may update a given information point of the information points <b>106</b> as satisfied after outputting of synthesized speech that includes the given information point. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the transcription generator <b>114</b> may update the information points for the time and date provided <b>122</b>, the reservation requested <b>124</b>, and the party size provided <b>126</b> to indicate that they are satisfied because the bot has already output synthesized speech that includes those information points <b>106</b>. Notably, although those information points <b>106</b> are updated to indicate that they are satisfied, the bot may still repeat those information points <b>106</b> if requested by the representative <b>102</b> and/or if there is a misunderstanding by the representative <b>102</b>, but those information points <b>106</b> may not be voluntarily provided by the bot twice.</p><p id="p-0055" num="0054">As noted above, in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref> the interruption classifier <b>136</b> classifies the interruption as a non-critical meaningful interruption. In this example, the pause identifier <b>142</b> may determine a position in the synthesized speech <b>128</b> for a natural pause (i.e., a pause in the synthesized speech being output when the interruption utterance <b>130</b> is detected). The natural pause may be at the end of a sentence, at the end of a prepositional phrase, before a prepositional phrase, before a conjunction, and/or any other similar part of speech where a speaker may naturally pause. The pause identifier <b>142</b> may identify a natural pause location after the preposition phrase &#x201c;for two people&#x201d; in synthesized speech <b>128</b> (i.e., before providing the portion of &#x201c;at 7 pm&#x201d; responsive to the utterance <b>120</b>). Further, the pause inserter <b>142</b> may instruct the transcription generator <b>114</b> to cease providing a transcription of a response to the utterance <b>120</b> to the speech synthesizer <b>116</b>.</p><p id="p-0056" num="0055">Moreover, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the bot may cease outputting synthesized speech <b>128</b> after outputting &#x201c;for two people&#x201d;, and the representative <b>102</b> may continue speaking the interruption utterance <b>130</b>. The speech recognizer <b>112</b> can transcribe the remaining portion of the interruption utterance <b>130</b>, and the transcription generator <b>114</b> can receive the transcription of the interruption utterance <b>130</b> of &#x201c;Oh, I'm sorry, you already said two people at 7 pm.&#x201d; Further, the transcription generator <b>114</b> can generate a response to the interruption utterance <b>130</b> by generating the transcription &#x201c;that's correct,&#x201d; and the speech synthesizer <b>116</b> can generates the synthesized speech <b>144</b> of &#x201c;That's correct.&#x201d; The call initiating system <b>104</b> can then transmit the synthesized speech <b>144</b> to the telephone <b>108</b> of the representative.</p><p id="p-0057" num="0056">After the call initiating system <b>104</b> transmits the synthesized speech <b>144</b> to the telephone <b>108</b> of the representative <b>102</b>, the call initiating system <b>104</b> may determine that the interruption is complete. At this point, the transcription generator <b>114</b> can determine whether all of the information points <b>106</b> are satisfied. If they are, then the transcription generator <b>114</b> can generate further transcriptions to complete the telephone call. If there are information points <b>106</b> that still remain incomplete after the interruption, then the transcription generator <b>114</b> can generate transcriptions that provide the information of the incomplete information points to the representative <b>102</b>, the speech synthesizer <b>116</b> can generate synthesized speech corresponding to the transcriptions, and the synthesized speech can be output at the telephone <b>108</b> of the representative <b>102</b>.</p><p id="p-0058" num="0057">In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the representative <b>102</b> responds to the synthesized speech <b>144</b> by speaking the utterance <b>146</b> of &#x201c;Great, I made the reservation. See you tomorrow.&#x201d; The speech recognizer can perform speech recognition on the utterance <b>146</b>, the speech recognizer <b>112</b> can provide the transcription of the utterance <b>146</b> to the transcription generator <b>114</b>, and the transcription generator <b>114</b> can determine that that the information points <b>106</b> are satisfied. In response to determining that all of the information points <b>106</b> are satisfied and/or in response to determining that the representative is ending the telephone call (e.g., based on recognized text included in the utterance <b>146</b>), the transcription generator <b>114</b> can generate a transcription to end the telephone call. The transcription generator <b>114</b> can provide the transcription to the speech synthesizer <b>116</b>. The call initiating system <b>104</b> transmits the synthesized utterance to the telephone <b>108</b> and terminates the telephone call (e.g., &#x201c;Thank you, see you tomorrow&#x201d;).</p><p id="p-0059" num="0058">In some implementations, the transcription generator <b>114</b> may steer the telephone conversation back to the subject of synthesized speech <b>128</b> before the interruption utterance <b>130</b> was detected. In this case, the transcription generator <b>114</b> may generate a transcription that completes any incomplete information points <b>106</b> (e.g., &#x201c;7 pm&#x201d;) of synthesized speech <b>128</b> and generate further synthesized speech based on the unsatisfied information points <b>106</b>. The transcription generator <b>114</b> can provide the transcription to the speech synthesizer <b>116</b>. The call initiating system <b>104</b> transmits the synthesized speech to the telephone <b>108</b>. For example, if the interruption utterance <b>130</b> only indicated &#x201c;you already said two people&#x201d;, but did not acknowledge the prior synthesized speech <b>118</b> that included &#x201c;7 pm&#x201d;, then the synthesized speech <b>144</b> may also include synthesized speech that includes the time and date provided <b>122</b> (e.g., &#x201c;That's correct, and 7 pm&#x201d;).</p><p id="p-0060" num="0059">In various implementations of the call initiating system <b>104</b>, the bot can cause synthesized speech that requests the representative <b>102</b> to consent to having the conversation with the bot on behalf of the user to be output at the telephone <b>108</b> of the representative <b>102</b>. In some implementations, the bot can cause the synthesized speech to be output when the representative <b>102</b> answers the assisted call. For example, in response to detecting the utterance <b>110</b> of the representative <b>102</b>, the bot can cause synthesized speech of &#x201c;Hello, this is bot calling on behalf of John Doe, do you consent to monitoring of this call&#x201d;. If the bot receives consent from the representative <b>102</b>, then the bot can then output the synthesized speech <b>118</b>. However, if the bot does not receive consent from the representative <b>102</b>, then the bot can terminate the assisted call, and can notify the user that the representative <b>102</b> did not consent to the assisted call.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example system <b>200</b> that monitors a telephone call between a first user <b>202</b> a second user <b>204</b>. For example, the first user <b>202</b> of the telephone call may be on hold, and the system <b>200</b> can notify the first user <b>202</b> that is on hold when the second user <b>204</b> has rejoined the telephone call. Briefly, and as described in more detail below, the first user <b>202</b> may call the second user <b>204</b>, and the second user <b>204</b> may place the first user <b>202</b> on hold. The first user <b>202</b> may hang up the phone <b>206</b> in response to being placed on hold. The call monitoring system <b>210</b> can monitor the telephone call to detect that the first user <b>202</b> hangs up the phone <b>206</b> while on hold. Although the first user <b>202</b> hangs up the phone <b>206</b>, the call monitoring system <b>210</b> can maintain the telephone call in an active state in response to the first user <b>206</b> hanging up the phone <b>206</b>, and can notify the first user <b>202</b> when the second user <b>204</b> rejoins the telephone call. Moreover, although the system <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is illustrated as being implemented by server(s) over network(s) (e.g., LAN, WAN, Bluetooth, and/or other network(s)), it should be understood that is for the sake of example and is not meant to be limiting. For example, the system <b>200</b> can be implemented locally at a computing device of the first user <b>202</b> and/or implemented by the computing device of the first user <b>202</b> and the server(s) in a distributed manner over the network(s).</p><p id="p-0062" num="0061">The first user <b>202</b> can configure the call settings on the phone <b>206</b> to enable the call monitoring system <b>210</b> to monitor telephone calls of the user <b>202</b>. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, assume the first user <b>202</b> places a telephone call to the second user <b>204</b> using a corresponding computing device (e.g., phone <b>206</b>), and that the call monitoring system <b>210</b> is enabled. In this example, the call monitoring system <b>210</b> can monitor the telephone call between the first user <b>202</b> and the second user <b>204</b>. During the initial portion of the telephone call, the phone <b>206</b> may be in a first state <b>226</b> where the phone <b>206</b> is connected to telephone <b>208</b>. Further assume that the second user <b>204</b> answers the telephone call using telephone <b>208</b> and speaks the utterance <b>212</b> of &#x201c;Thank you for calling Cable Company. How may I direct your call,&#x201d; assume that the first user <b>202</b> responds to the second user <b>204</b> with the utterance <b>214</b> of &#x201c;I'd like to cancel my account,&#x201d; and assume that the second user <b>204</b> responds to the first user <b>202</b> with the utterance <b>216</b> of &#x201c;Thank you, please hold.&#x201d;</p><p id="p-0063" num="0062">During this exchange between the first user <b>202</b> and the second user <b>204</b>, the call monitoring system <b>210</b> can monitor the telephone call, and can use the speech recognizer <b>244</b> to perform speech recognition on the utterances <b>212</b>, <b>214</b>, and <b>216</b> of both the first user <b>202</b> and the second user <b>204</b>. The call monitoring system <b>210</b> can analyze corresponding transcriptions of the utterances <b>212</b>, <b>214</b>, and <b>216</b> to determine the subject matter of the telephone call. Further, the call monitoring system <b>210</b> can store the subject matter of the telephone call and/or other details of the telephone call in the information points <b>248</b>. In this example, the call monitoring system <b>210</b> can store the information point of &#x201c;cancel account for cable company,&#x201d; and can label this information point as initially unsatisfied since the call was placed on hold prior to the first user <b>202</b> receiving confirmation from the second user <b>204</b> that the cable account was cancelled.</p><p id="p-0064" num="0063">The call monitor <b>236</b> of the call monitoring system <b>210</b> can include a hold detector <b>238</b>. The hold detector <b>238</b> can determine that the second user <b>204</b> has placed the first user <b>202</b> on hold. The hold detector <b>238</b> can determine that the second user <b>204</b> has placed the first user <b>202</b> on hold based on, for example, detecting an utterance from the second user <b>204</b> that explicitly states the first user <b>202</b> is being placed on hold (e.g., &#x201c;please hold&#x201d; in the utterance <b>216</b>), detecting a threshold duration of silence (e.g., no utterances of the first user <b>202</b> or the second user <b>204</b> are detected for 30 seconds, 45 seconds, and/or other durations of time), detecting that another bot has taken over the call on behalf of the second user (e.g., based on signal energy, voice identification, and so on), detecting metadata associated with the call that indicates the first user <b>202</b> has been placed on hold, and/or based on other techniques for determining that a call has been placed on hold. At this point, the call monitoring system <b>210</b> can maintain the call in an active state even if the user attempts to hang up.</p><p id="p-0065" num="0064">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the first user <b>202</b> attempts to hang up the phone <b>206</b> while the phone <b>206</b> is in the first state <b>226</b> because the first user <b>202</b> does not want to wait on hold. Notably, even though the first user <b>202</b> hangs up the phone <b>206</b>, the call monitoring system <b>210</b> can prevent the call from disconnecting entirely. Rather, the call monitoring system <b>210</b> can cause the phone <b>206</b> to transition from the first state <b>226</b> (e.g., connected) to the second state <b>228</b> (e.g., mute). In the second state <b>228</b>, the call between the first user <b>202</b> and the second user <b>204</b> is still active, but microphone(s) of the phone <b>206</b> are muted. The phone <b>206</b> may indicate that there is a telephone call active in the background, and also indicate that the microphone(s) are muted. The first user <b>202</b> may be able to use the phone <b>206</b> to perform other activities while the call monitoring system <b>210</b> maintains the telephone call in the second state <b>228</b>. In some implementations, the first user <b>202</b> can provide additional input (e.g., spoken, touch, and/or typed) while the phone is in the first state <b>226</b> or the second state <b>228</b> that causes the call monitoring system <b>210</b> to disconnect the call entirely.</p><p id="p-0066" num="0065">In some implementations, the call monitoring system <b>210</b> may not require that the first user <b>202</b> attempt to hang up the phone <b>206</b> to cause the phone <b>206</b> to transition from the first state <b>226</b> (e.g., connected) to the second state <b>228</b> (e.g., mute). Rather, the hold detector <b>238</b> can determine that the second user <b>204</b> has placed the first user <b>202</b> on hold, and, in response, the call monitor <b>236</b> can automatically cause the phone <b>206</b> to transition from the first state <b>226</b> (e.g., connected) to the second state <b>228</b> (e.g., mute), thereby maintaining the telephone call in the active state. The active state can be, for example, the phone <b>206</b> of the first user <b>202</b> maintaining the connection with the phone <b>208</b> of the second user <b>204</b> over network(s) (Voice over Internet Protocol (VoIP), public switched telephone network (PSTN), and/or other telephonic communication protocols).</p><p id="p-0067" num="0066">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, assume that the second user <b>204</b> rejoins the telephone call. At this point, the hold detector <b>238</b> determines that the first user <b>202</b> is no longer on hold, and the hold detector <b>238</b> can provide an indication to the notification generator <b>242</b> that the first user <b>202</b> is no longer on hold. In this example, the notification generator <b>242</b> causes the phone <b>206</b> to output an audible notification <b>234</b> (in addition to or in lieu of a visual notification) for the first user <b>202</b> that indicates the first user <b>228</b> is no longer on hold (e.g., the phone <b>206</b> vibrates/rings like an incoming call, the phone <b>206</b> vibrates/dings like an incoming notification, the phone <b>206</b> causes a visual notification to be rendered on a display of the phone <b>206</b>, and so on). In some additional and/or alternative implementations, the notification generator <b>242</b> may determine that the first user <b>202</b> is using the phone <b>206</b>. In this case, the notification generator <b>242</b> can present a visual notification on the display of the phone <b>206</b> (in addition to or in lieu of an audible notification).</p><p id="p-0068" num="0067">When the second user <b>204</b> rejoins the telephone call, assume that the second user <b>204</b> speaks the utterance <b>220</b> of &#x201c;Hello, I understand you want to cancel your account.&#x201d; In some implementations, and as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the first user <b>202</b> may not have rejoined the call yet responsive to receiving the notification <b>234</b>. In this case, a bot of the call monitoring system <b>210</b> may generate synthesized speech <b>218</b> of &#x201c;Hello, sorry. She will be right back&#x201d; to output to the phone <b>208</b> of the second user <b>204</b> while waiting for the first user <b>202</b> to rejoin the telephone call. The call monitoring system <b>210</b> can generate the synthesized speech in a similar manner described in more detail above with respect to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For example, the speech recognizer <b>244</b> can perform speech recognition on the utterance <b>220</b> of the second user <b>204</b> using the speech recognizer <b>244</b>, the transcription generator <b>240</b> can generate a transcription that is responsive to the utterance <b>220</b>, the speech synthesizer <b>246</b> can generate synthesized speech that includes audio data corresponding to the transcription that is responsive to the utterance, and the call monitoring system can cause the synthesized speech to be output at the phone <b>208</b> of the second user. In some versions of those implementations, the call monitoring system <b>210</b> can cause the transcription of the utterance <b>220</b> of the second user <b>204</b> to be visually rendered and/or cause audio data that captures the utterance <b>220</b> of the second user <b>204</b> to be audibly rendered at the phone <b>206</b> of the first user.</p><p id="p-0069" num="0068">In some implementations, the transcription that is responsive to the to the utterance <b>220</b> of the second user <b>204</b> can indicate that the notification <b>234</b> was output to first user <b>202</b> and/or indicate that the first user <b>202</b> will return to the telephone call shortly. As noted above, the transcription generator <b>240</b> can generate the transcription of &#x201c;Hello, sorry. She will be right back&#x201d; and provide the transcription to the speech synthesizer <b>246</b>, and the speech synthesizer <b>246</b> can generate synthesized speech that includes audio data <b>224</b> corresponding to the synthesized speech <b>218</b>. The call monitoring system <b>210</b> can transmit the audio data <b>224</b> directly to the telephone <b>208</b>, or to the phone <b>206</b> for transmission to the telephone <b>208</b>.</p><p id="p-0070" num="0069">In some implementations, the transcription generator <b>240</b> may use the transcription of the utterance <b>220</b> of the second user <b>204</b>, and the information points <b>248</b> of the telephone call to generate the transcription that is provided to the speech synthesizer <b>246</b>. In this instance, the transcription generator <b>240</b> may access the information points <b>248</b> that include details related to cancelling an account with Cable Company. As another example, the transcription generator <b>240</b> can generate the transcription, &#x201c;Hello, sorry, she is looking for her account information and will be right back&#x201d; and provide the transcription to the speech synthesizer <b>246</b>, and the speech synthesizer <b>246</b> can generate synthesized speech that includes audio data corresponding to the synthesized speech. Again, the call monitoring system <b>210</b> can transmit the audio data to the telephone <b>208</b>, or to the phone <b>206</b> for transmission to the telephone <b>208</b>.</p><p id="p-0071" num="0070">In some implementations, the call monitoring system <b>210</b> can continue the conversation with the second user <b>204</b> on behalf of the first user until the first user <b>202</b> rejoins the call. The transcription generator <b>240</b> may use a technique similar to the one described in <figref idref="DRAWINGS">FIG. <b>1</b></figref> for generating transcriptions that are relevant to additional information points and/or additional utterances of the second user <b>204</b> that are detected at the phone <b>206</b>. In this case, the transcription generator <b>240</b> may generate transcriptions indicating that the first user <b>202</b> is not ready to continue the conversation with second user <b>204</b>. For example, the transcription generator <b>240</b> may generate the transcription, &#x201c;Hello, sorry. I'm still looking for my account information&#x201d; or &#x201c;Hello. Please give me a moment while my computer boots up.&#x201d; In some versions of those implementations, the bot can access a user profile of the first user <b>202</b> and provide information that is included in the user profile (e.g., an address of the first user <b>202</b>, a name of the first user <b>202</b>, an account number of the first user <b>202</b> that is associated with the cable company, and so on).</p><p id="p-0072" num="0071">In some additional and/or alternative implementations, the bot may place the second user <b>204</b> on hold and/or terminate the telephone call if the first user does not rejoin the call within a threshold duration of time. The threshold duration of time may be fixed or dynamic. In implementations where the threshold duration of time is dynamic, the threshold duration of time may be based on interactions of the first user <b>202</b> with the phone <b>206</b>. For example, if the first user <b>202</b> does not interact with the notification <b>234</b> for 30 seconds, then the bot may place the second user <b>204</b> on hold. In this example, if the first user <b>202</b> does not rejoin the call within 60 seconds of the second user <b>204</b> being placed on hold, then the bot may determine that the call should be terminated. As another example, if the first user <b>202</b> interacts with the notification <b>234</b> but has not yet rejoined the call, then the bot may place the second user <b>204</b> on hold after 60 seconds. In this example, if the first user <b>202</b> does not rejoin the call within an additional 60 seconds of the second user <b>204</b> being placed on hold, then the bot may determine that the call should be terminated.</p><p id="p-0073" num="0072">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, assume that the first user <b>202</b> rejoins the telephone call responsive to receiving the notification <b>234</b> and speaks the utterance <b>222</b> of &#x201c;Hi, I'm here.&#x201d; When the first user <b>202</b> rejoins the call, the phone <b>206</b> can transition from the second state <b>228</b> (e.g., &#x201c;mute&#x201d;) to the third state <b>230</b> (e.g., &#x201c;connected&#x201d; or &#x201c;re-connected&#x201d;) where the phone <b>206</b> remains connected to the telephone call and the microphone(s) of the phone <b>206</b> of the first user <b>202</b> are un-muted. The call monitoring system <b>210</b> and the call monitor <b>236</b> can return to monitoring the telephone call.</p><p id="p-0074" num="0073">In some additional and/or alternative implementations, and although not depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the call monitoring system <b>210</b> can determine that the first user <b>202</b> rejoined the call responsive to receiving the notification <b>234</b> before (or during) detection of the utterance <b>220</b> of the second user <b>204</b>. The call monitoring system <b>210</b> can determine that the first user <b>202</b> rejoined the call before (or during) detection of the utterance <b>220</b> of the second user <b>202</b> based on, for example, determining that the phone <b>206</b> transitioned from the second state <b>228</b> to the third state <b>230</b>, determining that a spoken utterance detected at the phone <b>206</b> corresponds to the first user <b>202</b> using voice identification (and optionally determining that the spoken utterance is responsive to the utterance <b>220</b> of the second user <b>204</b>), and/or using other techniques to determine that the first user <b>202</b> has rejoined the call. In some versions of those implementations, the call monitoring system <b>210</b> can determine that there is no need for the bot to generate the synthesized speech <b>218</b> and/or to engage in conversation with the second user <b>204</b>. For example, assume that the first user <b>202</b> rejoins the phone call immediately upon receiving the notification <b>234</b> and while the second user <b>204</b> is speaking the utterance <b>220</b>. In this example, the call monitoring system <b>210</b> may not invoke the bot to engage in conversation with the second user <b>204</b> since the first user <b>202</b> has already rejoined the call. In some further versions of those implementations, the call monitoring system <b>210</b> may still cause a transcription of the utterance <b>220</b> of the second user <b>204</b> to be visually rendered on the phone <b>206</b> of the first user <b>202</b> to ensure the first user <b>202</b> is aware of the entirety of the utterance <b>220</b> of the second user <b>204</b>.</p><p id="p-0075" num="0074">In various implementations of the call monitoring system <b>210</b>, the bot can cause synthesized speech that requests the second user <b>204</b> to consent to having the conversation between the first user <b>202</b> and the second user <b>204</b> monitored by the bot. In some implementations, the bot can cause the synthesized speech to be output when the second user <b>204</b> answers the call. For example, in response to detecting the utterance <b>212</b> of the of the second user, the bot can cause synthesized speech of &#x201c;Hello, do you consent to the bot joining this call&#x201d; to be rendered. If the bot receives consent from the second user <b>204</b>, then the bot can join the conversation. However, if the bot does not receive consent from the second user <b>204</b>, then the bot may not join the call. Even if the bot does not receive consent from the second user <b>204</b> to join the call, the bot may still monitor the call. For example, the bot may still monitor the call to determine whether and/or the second user <b>204</b> rejoins the call using a voice activity detector (VAD) trained to detect voice activity, a hotword detector trained to detect particular words and/or phrases (e.g., &#x201c;Hello,&#x201d; &#x201c;I'm sorry for wait,&#x201d;, and so on). However, the bot may not use speech recognizer <b>244</b> to process any utterances of the second user <b>204</b>.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is a flowchart of an example process <b>300</b><i>a </i>for handling interruptions received from a user (or representative) while the user and a bot of a call initiating system are engaged in a telephone conversation (e.g., as described in more detail above with respect to <figref idref="DRAWINGS">FIG. <b>1</b></figref>). In general, the process <b>300</b><i>a </i>can initiate a telephone conversation with the user (or representative) using a bot associated with a corresponding user of a computing device. During the telephone conversation, the user (or representative) may interrupt the bot while the bot is outputting synthesized speech. The bot can identify and classify the interruption to determine the appropriate way to handle the interruption. After handling the interruption, the bot can steer the conversation back to the subject matter discussed before the interruption or proceed with the conversation. For the sake of simplicity, operations of the process <b>300</b><i>a </i>will be described as being performed by a system that includes one or more processors (e.g., the system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). The system can be implemented, for example, by server(s), a computing device of a corresponding user associated with the bot, and/or a combination thereof.</p><p id="p-0077" num="0076">At block <b>310</b><i>a</i>, the system initiates, by a call initiating system that is configured to initiate telephone calls and conduct telephone conversations using a bot, a telephone call with a user. The system can initiate the call in response to detecting user input (e.g., spoken, typed, and/or touch) at a corresponding computing device of a given user that requests the call be initiated on behalf of the given user. The bot can engage in the conversation with the user on behalf of the given user of the corresponding computing device. In some implementations, the bot can solicit information from the given user prior initiating the telephone call with the user. For example, if the given user provides user input of &#x201c;Make a reservation at Burger Palace,&#x201d; then the bot can solicit time and date information and party size information from the given user that requested the bot make the reservation. The user can be another bot or a human representative associated with a business or agency that is engaged with during the conversation. Continuing with the above example, the user can be another bot or human employee associated with Burger Palace. Initiating the call with the user is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0078" num="0077">At block <b>320</b><i>a</i>, the system provides, for output by the call initiating system, synthesized speech of the bot. The synthesized speech can include audio data corresponding to, for example, a request that the user consent to engaging with the bot during the conversation upon initiating of the call, information related to a task to be performed on behalf of a given user that provided user input to initiate the call, and/or other synthesized speech to facilitate the conversation between the bot and user. The synthesized speech can be output via speaker(s) of a computing device associated with the user such that the synthesized speech can be audibly perceived by the user. Providing the synthesized speech is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0079" num="0078">At block <b>330</b><i>a</i>, while providing, for output, the synthesized speech of the bot, the system receives, from the user and by the call initiating system, a user utterance. The system can continuously monitor for utterances of the user even while the bot is outputting the synthesized speech. Put another way, the system can detect an utterance of the user while the bot is outputting the synthesized speech at the computing device associated with the user. Receiving the user utterance while the bot is outputting the synthesized speech is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0080" num="0079">At block <b>340</b><i>a</i>, while providing, for output, the synthesized speech of the bot and based on the user utterance and on an outputted portion of the synthesized speech of the bot, the system determines whether to continue providing, for output, the synthesized speech of the bot or to cease providing, for output, the synthesized speech of the bot. The system can classify the user utterance received at block <b>330</b><i>a </i>into one of a plurality of disparate types of interruptions (e.g., a non-meaningful interruption, a non-critical meaningful interruption, a critical meaningful interruption, and/or other types of interruptions). Further, the system can determine whether to continue or cease providing of the synthesized speech based on the classifying of the user utterance. The system can classify the user utterance into one of the plurality of disparate types of interruptions based on processing, using machine learning model(s) and/or one or more rules, the user utterance received at block <b>330</b><i>a </i>(and optionally the synthesized speech). Moreover, the system can cause the bot to steer the conversation back to subject matter included in the synthesized speech prior to receiving the user utterance at block <b>330</b><i>a</i>, continue the conversation as if the user utterance was not received at block <b>330</b><i>a</i>, and/or change direction of the conversation to cause further synthesized speech that is responsive to the user utterance received at block <b>330</b><i>a</i>. Classifying the user utterance into one of the plurality of disparate interruptions types, and determining whether or not to cease providing of the synthesized speech is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is a flowchart of an example process for monitoring a telephone call where one user is on hold and for notifying the user on hold when the other user has rejoined the call. In general, the process <b>300</b><i>b </i>monitors a telephone conversation between two users. One of the users places the other user on hold. Instead of waiting, the user on hold attempts to hang up the phone. The process <b>300</b><i>b </i>maintain the phone call in an active state and notifies the user on hold when the other user has rejoined the call. The process <b>300</b><i>b </i>may use a bot to conduct a conversation with the user who has rejoined while waiting for the user who was on hold to return to the call. For the sake of simplicity, operations of the process <b>300</b><i>b </i>will be described as being performed by a system that includes one or more processors (e.g., the system <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>). The system can be implemented, for example, by server(s), a computing device of a corresponding user associated with the bot, and/or a combination thereof.</p><p id="p-0082" num="0081">At block <b>310</b><i>b</i>, the system determines that a first user and a second user are engaged in a telephone call. The system can determine that the first user and the second user are engaged in the telephone call based on a state of a corresponding first phone associated with the first user. The state of the corresponding first phone can indicate, for example, that the first user and the second user are connected using a telephonic communication protocol (e.g., VoIP, PSTN, and/or other protocols). Determining that the first user and the second user are engaged in a telephone call is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0083" num="0082">At block <b>320</b><i>b</i>, the system determines that the second user has placed the first user on hold. The system can determine that the second user has placed the first user on hold based on processing user utterances of the conversation. For example, the system can process a stream of audio data corresponding to the user utterances of the conversation. Based on processing the stream of audio data (e.g., using a speech recognizer), the system can determine that recognized text corresponding to the stream of audio data indicates that the second user placed the first user on hold. For example, the system can process the user utterances of the conversation to determine that the second user stated &#x201c;Please hold,&#x201d; &#x201c;Will you please hold while I transfer your call,&#x201d; and/or other phrases that indicate the second user placed the first user on hold. Determining that the second user has placed the first user on hold is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0084" num="0083">At block <b>330</b><i>b</i>, the system determines that the first user has hung up a corresponding first phone. The system can determine that the first user has hung up the corresponding first phone based on determining that the first user provided user input at the corresponding first to terminate the phone call. For example, the system can determine that the first user has hung up the corresponding first input based on user input directed to a button (e.g., physical button and/or soft button on a graphical user interface) that, when selected, causes the telephone call to be terminated, based on the first user placing the corresponding first phone in a locked state, and/or based on other interactions with the corresponding first phone. Determining that the first user has hung up a corresponding first phone is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0085" num="0084">At block <b>340</b><i>b</i>, based on determining that the first user has hung up the corresponding first phone, and while the first user is on hold, the system maintains the telephone call in an active state. The system can maintain the active state of the telephone call by causing the corresponding first phone to transition from a first state to a second state. In the second state, microphone(s) of the corresponding first computing device can be muted, and the first user can interact with the corresponding first phone while the system maintains the telephonic connection with a corresponding second phone of the second user in the background. Maintaining the corresponding first phone in the active state is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0086" num="0085">At block <b>350</b><i>b</i>, the system determines that the second user has rejoined the telephone call. The system can determine that the second user has rejoined the call. In some implementations, the system can determine that the second user has rejoined the call based on detecting a further user utterance of the second user. The system can use a voice activity detector, speech recognizer, and/or other components to determine that the second user has rejoined the call. In some versions of those implementations, the system can determine that the user utterance originated from the second user (i.e., as opposed to audio data corresponding music being played while, audio data corresponding to that of an interactive voice response (IVR) system, and so on) using voice identification. Determining that the second user has rejoined the telephone call is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>). Notably, there is a break between block <b>340</b><i>b </i>and block <b>350</b><i>b</i>. This break indicates that the system can maintain the active state of the telephone call at block <b>340</b><i>b </i>while actively monitoring for the second user to rejoin the telephone call at block <b>350</b><i>b. </i></p><p id="p-0087" num="0086">At block <b>360</b><i>b</i>, based on determining that the second user has rejoined the telephone call, the system conducts a telephone conversation between the second user and a bot that is configured to conduct telephone conversations with humans. The system can cause the bot to output synthesized speech in response to determining that the second user has provided a user utterance upon rejoining the telephone call. In some implementations, the system only causes the bot to output the synthesized speech in response to determining that the first user has not yet rejoined the telephone call. The synthesized speech can indicate that bot is an active participant in the conversation, and on behalf of the first user. In some additional and/or alternative implementations, the bot can output synthesized speech that includes information points that are to be conveyed to the second user during the telephone call (e.g., name information, address information, account information, and/or other information associated with the first user). Conducting the telephone conversation between the bot and the second user is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0088" num="0087">At block <b>370</b><i>b</i>, while conducting the telephone conversation between the second user and the bot, the system provides, for output to the corresponding first phone, a notification for the first user to rejoin the telephone call. The notification for the first user can indicate that the second user has rejoined the call, and can include, for example, an audible and/or visual notification rendered at the corresponding first phone. Further, the synthesized speech output at block <b>360</b><i>b </i>can indicate that the first user has been notified that the second user has rejoined the telephone call. In some implementations, the notification can further include a transcription (and/or audio data corresponding thereto) of user utterances of the second user and/or synthesized speech of the bot subsequent to the second user rejoining the telephone call. Providing the notification for the first user to rejoin the telephone call is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0089" num="0088">At block <b>380</b><i>b</i>, the system determines that the first user has rejoined the telephone call. The system can determine that the first user has rejoined the telephone call based on the corresponding first phone transitioning from a second state to a third state that indicates the first user is now an active participant in the conversation. Determining that the first user has rejoined the telephone call is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0090" num="0089">At block <b>390</b><i>b</i>, based on determining that the first user has rejoined the telephone call, the system ceases conducting the telephone conversation between the second user and the bot. The bot can cease conducting of the telephone conversation in response to determining that the first user has rejoined the telephone call. In some implementations, if the bot is outputting synthesized speech when the first user rejoins the call, the bot can cease providing the synthesized speech upon concluding outputting of the synthesized speech and/or upon reaching a natural pause in the synthesized speech (e.g., similar to discussed in <figref idref="DRAWINGS">FIG. <b>1</b></figref> with respect to pause inserter <b>142</b>). Ceasing conducting of the telephone conversation is described in more detail herein (e.g., with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example of a computing device <b>400</b> and a mobile computing device <b>450</b> that can be used to implement the techniques described here. The computing device <b>400</b> is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The mobile computing device <b>450</b> is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart-phones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be examples only, and are not meant to be limiting.</p><p id="p-0092" num="0091">The computing device <b>400</b> includes a processor <b>402</b>, a memory <b>404</b>, a storage device <b>406</b>, a high-speed interface <b>408</b> connecting to the memory <b>404</b> and multiple high-speed expansion ports <b>410</b>, and a low-speed interface <b>412</b> connecting to a low-speed expansion port <b>414</b> and the storage device <b>406</b>. Each of the processor <b>402</b>, the memory <b>404</b>, the storage device <b>406</b>, the high-speed interface <b>408</b>, the high-speed expansion ports <b>410</b>, and the low-speed interface <b>412</b>, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor <b>402</b> can process instructions for execution within the computing device <b>400</b>, including instructions stored in the memory <b>404</b> or on the storage device <b>406</b> to display graphical information for a GUI on an external input/output device, such as a display <b>416</b> coupled to the high-speed interface <b>408</b>. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).</p><p id="p-0093" num="0092">The memory <b>404</b> stores information within the computing device <b>400</b>. In some implementations, the memory <b>404</b> is a volatile memory unit or units. In some implementations, the memory <b>404</b> is a non-volatile memory unit or units. The memory <b>404</b> may also be another form of computer-readable medium, such as a magnetic or optical disk.</p><p id="p-0094" num="0093">The storage device <b>406</b> is capable of providing mass storage for the computing device <b>400</b>. In some implementations, the storage device <b>406</b> may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. Instructions can be stored in an information carrier. The instructions, when executed by one or more processing devices (for example, processor <b>402</b>), perform one or more methods, such as those described above. The instructions can also be stored by one or more storage devices such as computer- or machine-readable mediums (for example, the memory <b>404</b>, the storage device <b>406</b>, or memory on the processor <b>402</b>).</p><p id="p-0095" num="0094">The high-speed interface <b>408</b> manages bandwidth-intensive operations for the computing device <b>400</b>, while the low-speed interface <b>412</b> manages lower bandwidth-intensive operations. Such allocation of functions is an example only. In some implementations, the high-speed interface <b>408</b> is coupled to the memory <b>404</b>, the display <b>416</b> (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports <b>410</b>, which may accept various expansion cards (not shown). In the implementation, the low-speed interface <b>412</b> is coupled to the storage device <b>406</b> and the low-speed expansion port <b>414</b>. The low-speed expansion port <b>414</b>, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.</p><p id="p-0096" num="0095">The computing device <b>400</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server <b>420</b>, or multiple times in a group of such servers. In addition, it may be implemented in a personal computer such as a laptop computer <b>422</b>. It may also be implemented as part of a rack server system <b>424</b>. Alternatively, components from the computing device <b>400</b> may be combined with other components in a mobile device (not shown), such as a mobile computing device <b>450</b>. Each of such devices may contain one or more of the computing device <b>400</b> and the mobile computing device <b>450</b>, and an entire system may be made up of multiple computing devices communicating with each other.</p><p id="p-0097" num="0096">The mobile computing device <b>450</b> includes a processor <b>452</b>, a memory <b>464</b>, an input/output device such as a display <b>454</b>, a communication interface <b>466</b>, and a transceiver <b>468</b>, among other components. The mobile computing device <b>450</b> may also be provided with a storage device, such as a micro-drive or other device, to provide additional storage. Each of the processor <b>452</b>, the memory <b>464</b>, the display <b>454</b>, the communication interface <b>466</b>, and the transceiver <b>468</b>, are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.</p><p id="p-0098" num="0097">The processor <b>452</b> can execute instructions within the mobile computing device <b>450</b>, including instructions stored in the memory <b>464</b>. The processor <b>452</b> may be implemented as a chipset of chips that include separate and multiple analog and digital processors. The processor <b>452</b> may provide, for example, for coordination of the other components of the mobile computing device <b>450</b>, such as control of user interfaces, applications run by the mobile computing device <b>450</b>, and wireless communication by the mobile computing device <b>450</b>.</p><p id="p-0099" num="0098">The processor <b>452</b> may communicate with a user through a control interface <b>458</b> and a display interface <b>456</b> coupled to the display <b>454</b>. The display <b>454</b> may be, for example, a TFT (Thin-Film-Transistor Liquid Crystal Display) display or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology. The display interface <b>456</b> may comprise appropriate circuitry for driving the display <b>454</b> to present graphical and other information to a user. The control interface <b>458</b> may receive commands from a user and convert them for submission to the processor <b>452</b>. In addition, an external interface <b>462</b> may provide communication with the processor <b>452</b>, so as to enable near area communication of the mobile computing device <b>450</b> with other devices. The external interface <b>462</b> may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.</p><p id="p-0100" num="0099">The memory <b>464</b> stores information within the mobile computing device <b>450</b>. The memory <b>464</b> can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units. An expansion memory <b>474</b> may also be provided and connected to the mobile computing device <b>450</b> through an expansion interface <b>472</b>, which may include, for example, a SIMM (Single In Line Memory Module) card interface. The expansion memory <b>474</b> may provide extra storage space for the mobile computing device <b>450</b>, or may also store applications or other information for the mobile computing device <b>450</b>. Specifically, the expansion memory <b>474</b> may include instructions to carry out or supplement the processes described above, and may include secure information also. Thus, for example, the expansion memory <b>474</b> may be provide as a security module for the mobile computing device <b>450</b>, and may be programmed with instructions that permit secure use of the mobile computing device <b>450</b>. In addition, secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.</p><p id="p-0101" num="0100">The memory may include, for example, flash memory and/or NVRAM memory (non-volatile random access memory), as discussed below. In some implementations, instructions are stored in an information carrier. that the instructions, when executed by one or more processing devices (for example, processor <b>452</b>), perform one or more methods, such as those described above. The instructions can also be stored by one or more storage devices, such as one or more computer- or machine-readable mediums (for example, the memory <b>464</b>, the expansion memory <b>474</b>, or memory on the processor <b>452</b>). In some implementations, the instructions can be received in a propagated signal, for example, over the transceiver <b>468</b> or the external interface <b>462</b>.</p><p id="p-0102" num="0101">The mobile computing device <b>450</b> may communicate wirelessly through the communication interface <b>466</b>, which may include digital signal processing circuitry where necessary. The communication interface <b>466</b> may provide for communications under various modes or protocols, such as GSM voice calls (Global System for Mobile communications), SMS (Short Message Service), EMS (Enhanced Messaging Service), or MMS messaging (Multimedia Messaging Service), CDMA (code division multiple access), TDMA (time division multiple access), PDC (Personal Digital Cellular), WCDMA (Wideband Code Division Multiple Access), CDMA2000, or GPRS (General Packet Radio Service), among others. Such communication may occur, for example, through the transceiver <b>468</b> using a radio-frequency. In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, a GPS (Global Positioning System) receiver module <b>470</b> may provide additional navigation- and location-related wireless data to the mobile computing device <b>450</b>, which may be used as appropriate by applications running on the mobile computing device <b>450</b>.</p><p id="p-0103" num="0102">The mobile computing device <b>450</b> may also communicate audibly using an audio codec <b>460</b>, which may receive spoken information from a user and convert it to usable digital information. The audio codec <b>460</b> may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of the mobile computing device <b>450</b>. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on the mobile computing device <b>450</b>.</p><p id="p-0104" num="0103">The mobile computing device <b>450</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone <b>480</b>. It may also be implemented as part of a smart-phone <b>482</b>, personal digital assistant, or other similar mobile device.</p><p id="p-0105" num="0104">Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.</p><p id="p-0106" num="0105">These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms machine-readable medium and computer-readable medium refer to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term machine-readable signal refers to any signal used to provide machine instructions and/or data to a programmable processor.</p><p id="p-0107" num="0106">To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.</p><p id="p-0108" num="0107">The systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (LAN), a wide area network (WAN), and the Internet. In some implementations, the systems and techniques described here can be implemented on an embedded system where speech recognition and other processing is performed directly on the device.</p><p id="p-0109" num="0108">The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p><p id="p-0110" num="0109">Although a few implementations have been described in detail above, other modifications are possible. For example, while a client application is described as accessing the delegate(s), in other implementations the delegate(s) may be employed by other applications implemented by one or more processors, such as an application executing on one or more servers. In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other actions may be provided, or actions may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other implementations are within the scope of the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method implemented by one or more processors, the method comprising:<claim-text>determining that a first user and a second user are engaged in a telephone call, wherein the first user is associated with a corresponding first computing device, and wherein the second user is associated with a corresponding second computing device;</claim-text><claim-text>determining that the second user has placed the first user on hold;</claim-text><claim-text>determining that the first user has hung up the corresponding first computing device;</claim-text><claim-text>in response to determining that the first user has hung up the corresponding first phone and while the first user is on hold:<claim-text>maintaining the telephone call in an active state; and</claim-text><claim-text>determining whether the second user has rejoined the telephone call; and</claim-text></claim-text><claim-text>in response to determining that the second user has rejoined the telephone call:<claim-text>causing a bot associated with the corresponding first computing device of the first user to continue the telephone call with the second user;</claim-text><claim-text>causing the bot to provide, for output at the corresponding first computing device, a notification for the first user to rejoin the telephone call;</claim-text><claim-text>determining that the first user has rejoined the telephone call; and</claim-text><claim-text>in response to determining that the first user has rejoined the telephone call:<claim-text>causing the bot to cease the telephone conversation with the second user.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein causing the bot associated with the corresponding first computing device of the first user to continue the telephone call with the second user comprises:<claim-text>causing the bot to generate synthesized speech of the bot that indicates the first user is not an active participant on the telephone call; and</claim-text><claim-text>causing the bot to provide, for output at the corresponding second computing device of the second user, the synthesized speech of the bot.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein causing the bot associated with the corresponding first computing device of the first user to continue the telephone call with the second user comprises:<claim-text>causing the bot to generate synthesized speech of the bot that conveys one or more details with respect to the first user initiating the telephone call; and</claim-text><claim-text>causing the bot to provide, for output at the corresponding second computing device of the second user, the synthesized speech of the bot.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>in response to determining that the first user has not rejoined the telephone call:<claim-text>causing the bot associated with the corresponding first computing device of the first user to place the second user on hold.</claim-text></claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:<claim-text>in response to determining that the first user has not rejoined the telephone call within a threshold duration of time of causing the bot associated with the corresponding first computing device of the first user to place the second user on hold:<claim-text>causing the bot to terminate the telephone call.</claim-text></claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>prior to the first user rejoining the telephone call, receiving, from the second user, a user utterance;</claim-text><claim-text>causing the bot to generate a transcription of the user utterance; and</claim-text><claim-text>causing the bot to include the notification for the first user to rejoin the telephone call in the transcription of the user utterance.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein maintaining the telephone call in the active state comprises:<claim-text>muting one or more microphones of the corresponding first computing device; and</claim-text><claim-text>maintaining a telephonic connection between the corresponding first computing device and the corresponding second computing device.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, in response to determining that the first user has rejoined the telephone call, further comprising:<claim-text>unmuting the one or more microphones of the corresponding first computing device.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein causing the bot associated with the corresponding first computing device of the first user to continue the telephone call with the second user comprises:<claim-text>refraining from causing the bot to generate any synthesized speech of the bot based on determining that the first user has rejoined the telephone call within a threshold duration of time of causing the bot to provide the notification for the first user to rejoin the telephone call for output at the corresponding first computing device.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the bot is implemented at a remote server that is remote from the first computing device.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the bot is implemented locally at the first computing device.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A system comprising:<claim-text>one or more processors; and</claim-text><claim-text>memory storing instructions that, when executed by the one or more processors, cause the one or more processors to:<claim-text>determine that a first user and a second user are engaged in a telephone call, wherein the first user is associated with a corresponding first computing device, and wherein the second user is associated with a corresponding second computing device;</claim-text><claim-text>determine that the second user has placed the first user on hold;</claim-text><claim-text>determine that the first user has hung up the corresponding first computing device; and</claim-text><claim-text>in response to determining that the first user has hung up the corresponding first phone and while the first user is on hold:<claim-text>maintain the telephone call in an active state; and</claim-text><claim-text>determine whether the second user has rejoined the telephone call; and</claim-text></claim-text><claim-text>in response to determining that the second user has rejoined the telephone call:<claim-text>cause a bot associated with the corresponding first computing device of the first user to continue the telephone call with the second user;</claim-text><claim-text>cause the bot to provide, for output at the corresponding first computing device, a notification for the first user to rejoin the telephone call;</claim-text><claim-text>determine that the first user has rejoined the telephone call; and</claim-text><claim-text>in response to determining that the first user has rejoined the telephone call:<claim-text>causing the bot to cease the telephone conversation with the second user.</claim-text></claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions to cause the bot associated with the corresponding first computing device of the first user to continue the telephone call with the second user comprise instructions to:<claim-text>cause the bot to generate synthesized speech of the bot that indicates the first user is not an active participant on the telephone call; and</claim-text><claim-text>cause the bot to provide, for output at the corresponding second computing device of the second user, the synthesized speech of the bot.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions to cause the bot associated with the corresponding first computing device of the first user to continue the telephone call with the second user comprise instructions to:<claim-text>cause the bot to generate synthesized speech of the bot that conveys one or more details with respect to the first user initiating the telephone call; and</claim-text><claim-text>cause the bot to provide, for output at the corresponding second computing device of the second user, the synthesized speech of the bot.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions further cause the one or more processors to:<claim-text>in response to determining that the first user has not rejoined the telephone call:<claim-text>cause the bot associated with the corresponding first computing device of the first user to place the second user on hold.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions further cause the one or more processors to:<claim-text>in response to determining that the first user has not rejoined the telephone call within a threshold duration of time of causing the bot associated with the corresponding first computing device of the first user to place the second user on hold:<claim-text>cause the bot to terminate the telephone call.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions further cause the one or more processors to:<claim-text>prior to the first user rejoining the telephone call, receive, from the second user, a user utterance;</claim-text><claim-text>cause the bot to generate a transcription of the user utterance; and</claim-text><claim-text>cause the bot to include the notification for the first user to rejoin the telephone call in the transcription of the user utterance.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions to maintain the telephone call in the active state comprise instructions to:<claim-text>mute one or more microphones of the corresponding first computing device; and</claim-text><claim-text>maintain a telephonic connection between the corresponding first computing device and the corresponding second computing device.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, in response to determining that the first user has rejoined the telephone call, wherein the instructions further cause the one or more processors to:<claim-text>unmute the one or more microphones of the corresponding first computing device.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer-readable storage medium storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations, the operations comprising:<claim-text>determining that a first user and a second user are engaged in a telephone call, wherein the first user is associated with a corresponding first computing device, and wherein the second user is associated with a corresponding second computing device;</claim-text><claim-text>determining that the second user has placed the first user on hold;</claim-text><claim-text>determining that the first user has hung up the corresponding first computing device;</claim-text><claim-text>in response to determining that the first user has hung up the corresponding first phone and while the first user is on hold:<claim-text>maintaining the telephone call in an active state; and</claim-text><claim-text>determining whether the second user has rejoined the telephone call; and</claim-text></claim-text><claim-text>in response to determining that the second user has rejoined the telephone call:<claim-text>causing a bot associated with the corresponding first computing device of the first user to continue the telephone call with the second user;</claim-text><claim-text>causing the bot to provide, for output at the corresponding first computing device, a notification for the first user to rejoin the telephone call;</claim-text><claim-text>determining that the first user has rejoined the telephone call; and</claim-text><claim-text>in response to determining that the first user has rejoined the telephone call:<claim-text>causing the bot to cease the telephone conversation with the second user.</claim-text></claim-text></claim-text></claim-text></claim></claims></us-patent-application>