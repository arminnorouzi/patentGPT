<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005119A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005119</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17809749</doc-number><date>20220629</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2021-108801</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>776</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>001</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>776</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>761</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30168</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30108</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD FOR DETERMINING QUALITY OF INSPECTION DATA USING MACHINE LEARNING MODEL, INFORMATION PROCESSING APPARATUS, AND NON-TRANSITORY COMPUTER READABLE STORAGE MEDIUM STORING COMPUTER PROGRAM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SEIKO EPSON CORPORATION</orgname><address><city>TOKYO</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KURASAWA</last-name><first-name>Hikaru</first-name><address><city>Matsumoto-shi</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A quality determination method includes: (a) generating a plurality of pieces of training data by classifying a plurality of pieces of non-defective product data into a plurality of classes; (b) executing learning of a machine learning model using the plurality of pieces of training data; (c) preparing a known feature spectrum group; and (d) executing quality determination processing of inspection data using the machine learning model and the known feature spectrum group. The (d) includes (d1) calculating a feature spectrum related to the inspection data, (d2) calculating a similarity between the feature spectrum and the known feature spectrum group, and (d3) determining the inspection data to be non-defective when the similarity is equal to or greater than a threshold value and determining the inspection data to be defective when the similarity is less than the threshold value.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="114.89mm" wi="129.29mm" file="US20230005119A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="183.30mm" wi="137.67mm" file="US20230005119A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="210.82mm" wi="92.79mm" orientation="landscape" file="US20230005119A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="165.18mm" wi="82.30mm" file="US20230005119A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="156.63mm" wi="112.27mm" file="US20230005119A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="171.62mm" wi="116.92mm" file="US20230005119A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="128.27mm" wi="131.32mm" file="US20230005119A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="172.38mm" wi="129.96mm" file="US20230005119A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="164.08mm" wi="136.48mm" file="US20230005119A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="207.60mm" wi="121.33mm" orientation="landscape" file="US20230005119A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="207.18mm" wi="119.97mm" orientation="landscape" file="US20230005119A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="93.56mm" wi="82.13mm" file="US20230005119A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">The present application is based on, and claims priority from JP Application Serial Number 2021-108801, filed Jun. 30, 2021, the disclosure of which is hereby incorporated by reference herein in its entirety.</p><heading id="h-0001" level="1">BACKGROUND</heading><heading id="h-0002" level="1">1. Technical Field</heading><p id="p-0003" num="0002">The present disclosure relates to a method for determining the quality of inspection data using a machine learning model, an information processing apparatus, and a computer program.</p><heading id="h-0003" level="1">2. Related Art</heading><p id="p-0004" num="0003">JP-A-2021-42994 discloses an appearance inspection system that determines the quality of a product based on an image thereof. In the appearance inspection system, quality determination is executed using a learning model generated by an autoencoder. In the learning model using the autoencoder, the learning can be executed using only data of a non-defective product. For example, when the learning of the autoencoder is executed using only image data of the non-defective product and image data to be inspected is decoded by the learned autoencoder, image data to be output close to the learned image data of the non-defective product is obtained. At this time, when the difference between the image data to be inspected and the image data to be output is large, the inspection data can be determined to be defective.</p><p id="p-0005" num="0004">However, in the learning model using the autoencoder, unless the processing of compression in an encoding unit is appropriately set, it may not be possible to distinguish whether the difference between the image data to be inspected and the image data to be output is due to a defect or due to inappropriate compression processing. Therefore, a technique of improving the determination accuracy of the quality determination using a machine learning model other than the autoencoder is desired.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">A first aspect of the present disclosure provides a method for determining quality of inspection data using a machine learning model of a vector neural network type including a plurality of vector neuron layers. This method includes: (a) generating a plurality of pieces of training data by classifying a plurality of pieces of data of a non-defective product into a plurality of classes and assigning a plurality of labels distinguishing the plurality of classes to the plurality of pieces of data of the non-defective product; (b) executing learning of the machine learning model using the plurality of pieces of training data; (c) preparing a known feature spectrum group obtained based on an output of at least one specific layer among the plurality of vector neuron layers when the plurality of pieces of training data are input to the learned machine learning model; and (d) executing quality determination processing of the inspection data using the learned machine learning model and the known feature spectrum group. The (d) includes (d1) calculating a feature spectrum based on the output of the specific layer in response to an input of the inspection data to the machine learning model, (d2) calculating a similarity between the feature spectrum and the known feature spectrum group, and (d3) determining the inspection data to be non-defective when the similarity is equal to or greater than a preset threshold value, and determining the inspection data to be defective when the similarity is less than the threshold value.</p><p id="p-0007" num="0006">A second aspect of the present disclosure provides an information processing apparatus configured to execute quality determination processing for determining quality of inspection data using a machine learning model of a vector neural network type including a plurality of vector neuron layers. The information processing apparatus includes a memory configured to store the machine learning model, and a processor configured to execute a calculation using the machine learning model. The processor is configured to (a) generate a plurality of pieces of training data by classifying a plurality of pieces of data of a non-defective product into a plurality of classes and assigning a plurality of labels distinguishing the plurality of classes to the plurality of pieces of data of the non-defective product, (b) execute learning of the machine learning model using the plurality of pieces of training data, (c) prepare a known feature spectrum group obtained based on an output of at least one specific layer among the plurality of vector neuron layers when the plurality of pieces of training data are input to the learned machine learning model, and (d) execute quality determination processing of the inspection data using the learned machine learning model and the known feature spectrum group. The (d) includes (d1) calculating a feature spectrum based on the output of the specific layer in response to an input of the inspection data to the machine learning model, (d2) calculating a similarity between the feature spectrum and the known feature spectrum group, and (d3) determining the inspection data to be non-defective when the similarity is equal to or greater than a preset threshold value, and determining the inspection data to be defective when the similarity is less than the threshold value.</p><p id="p-0008" num="0007">A third aspect of the present disclosure provides a non-transitory computer readable storage medium storing a computer program configured to execute, by a processor, quality determination processing for determining quality of inspection data using a machine learning model of a vector neural network type including a plurality of vector neuron layers. The computer program includes: (a) generating a plurality of pieces of training data by classifying a plurality of pieces of data of a non-defective product into a plurality of classes and assigning a plurality of labels distinguishing the plurality of classes to the plurality of pieces of data of the non-defective product; (b) processing of executing learning of the machine learning model using the plurality of pieces of training data; (c) preparing a known feature spectrum group obtained based on an output of at least one specific layer among the plurality of vector neuron layers when the plurality of pieces of training data are input to the learned machine learning model; and (d) executing quality determination processing of the inspection data using the learned machine learning model and the known feature spectrum group. The (d) includes (d1) calculating a feature spectrum based on the output of the specific layer in response to an input of the inspection data to the machine learning model, (d2) calculating a similarity between the feature spectrum and the known feature spectrum group, and (d3) determining the inspection data to be non-defective when the similarity is equal to or greater than a preset threshold value, and determining the inspection data to be defective when the similarity is less than the threshold value.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a quality determination system.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram showing a configuration example of a machine learning model.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart showing a preparation process of the machine learning model.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram showing an example of a product to be inspected.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram showing training data subjected to clustering processing.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram showing a feature spectrum.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram showing a configuration of a known feature spectrum group.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart showing a processing procedure of a quality determination process.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram showing a state in which a class similarity related to inspection data is obtained.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic diagram showing a first calculation method of the class similarity.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram showing a second calculation method of the class similarity.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic diagram showing a third calculation method of the class similarity.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EXEMPLARY EMBODIMENTS</heading><heading id="h-0007" level="1">A. Embodiment</heading><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram showing a quality determination system according to an embodiment. The quality determination system includes an information processing apparatus <b>100</b> and a camera <b>400</b>. The camera <b>400</b> is for capturing an image of a product to be inspected. As the camera <b>400</b>, a camera that captures a color image may be used, or a camera that captures a monochrome image or a spectral image may be used. In the present embodiment, an image captured by the camera <b>400</b> is used as training data or inspection data, and data other than the image may be used as training data or inspection data. In this case, an inspection data acquisition apparatus corresponding to the type of data is used instead of the camera <b>400</b>.</p><p id="p-0022" num="0021">The information processing apparatus <b>100</b> includes a processor <b>110</b>, a memory <b>120</b>, an interface circuit <b>130</b>, and an input device <b>140</b> and a display unit <b>150</b> that are coupled to the interface circuit <b>130</b>. The camera <b>400</b> is also coupled to the interface circuit <b>130</b>. Although not limited, for example, the processor <b>110</b> has not only a function of executing processing described in detail below but also a function of displaying, on the display unit <b>150</b>, data obtained by the processing and data generated during the processing.</p><p id="p-0023" num="0022">The processor <b>110</b> functions as a class determination processing unit <b>112</b> that executes class determination processing of the inspection data. The class determination processing unit <b>112</b> includes a similarity calculation unit <b>310</b> and a quality determination unit <b>320</b>. The class determination processing unit <b>112</b> is implemented by executing a computer program stored in the memory <b>120</b> by the processor <b>110</b>. Alternatively, the class determination processing unit <b>112</b> may be implemented by a hardware circuit. The term &#x201c;processor&#x201d; in the present description includes such a hardware circuit. The processor that executes the class determination processing may be a processor included in a remote computer that is connected to the information processing apparatus <b>100</b> via a network.</p><p id="p-0024" num="0023">A machine learning model <b>200</b>, a training data group TD, and a known feature spectrum group KSp are stored in the memory <b>120</b>. The machine learning model <b>200</b> is used for processing by the class determination processing unit <b>112</b>. A configuration example and an operation of the machine learning model <b>200</b> will be described later. The training data group TD is a set of labeled data used for learning of the machine learning model <b>200</b>. In the present embodiment, the training data group TD is a set of image data. The known feature spectrum group KSp is a set of feature spectra obtained when the training data group TD is input to the learned machine learning model <b>200</b>. The feature spectrum will be described later.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram showing a configuration example of the machine learning model <b>200</b>. The machine learning model <b>200</b> includes, in order from an input data IM side, a convolutional layer <b>210</b>, a prime vector neuron layer <b>220</b>, a first convolutional vector neuron layer <b>230</b>, a second convolutional vector neuron layer <b>240</b>, and a classification vector neuron layer <b>250</b>. Among these five layers <b>210</b> to <b>250</b>, the convolutional layer <b>210</b> is the lowest layer, and the classification vector neuron layer <b>250</b> is the highest layer. In the following descriptions, the layers <b>210</b> to <b>250</b> are also referred to as a &#x201c;Conn layer <b>210</b>&#x201d;, a &#x201c;PrimeVN layer <b>220</b>&#x201d;, a &#x201c;ConvVN1 layer <b>230</b>&#x201d;, a &#x201c;ConvVN2 layer <b>240</b>&#x201d;, and a &#x201c;ClassVN layer <b>250</b>&#x201d;, respectively.</p><p id="p-0026" num="0025">The convolutional vector neuron layers <b>230</b> and <b>240</b> are used in the example in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Alternatively, the number of convolutional vector neuron layers may be any number, and the convolutional vector neuron layer may be omitted. It is preferable to use one or more convolutional vector neuron layers.</p><p id="p-0027" num="0026">Configurations of the layers <b>210</b> to <b>250</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref> can be described as follows.</p><heading id="h-0008" level="2">Description of Configurations of Machine Learning Model <b>200</b></heading><p id="p-0028" num="0027">Conv layer <b>210</b>: Conv[32, 5, 2]</p><p id="p-0029" num="0028">PrimeVN layer <b>220</b>: PrimeVN [16, 1, 1]</p><p id="p-0030" num="0029">ConvVN1 layer <b>230</b>: ConvVN1 [12, 3, 2]</p><p id="p-0031" num="0030">ConvVN2 layer <b>240</b>: ConvVN2 [6, 3, 1]</p><p id="p-0032" num="0031">ClassVN layer <b>250</b>: ClassVN [Nm, <b>4</b>, <b>1</b>]</p><p id="p-0033" num="0032">Vector Dimension VD: VD=16</p><p id="p-0034" num="0033">In the descriptions of the layers <b>210</b> to <b>250</b>, character strings before parentheses are layer names, and numbers in the parentheses are the number of channels, surface sizes of kernels, and strides in order. For example, the layer name of the Conv layer <b>210</b> is &#x201c;Cony&#x201d;, the number of channels is 32, the surface size of a kernel is 5&#xd7;5, and the stride is 2. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, these descriptions are shown below the layers. A hatched rectangle drawn in each layer represents the surface size of the kernel used when an output vector of an adjacent upper layer is calculated. In the present embodiment, since the input data IM is image data, the surface size of the kernel is also two-dimensional. Values of parameters used in the descriptions of the layers <b>210</b> to <b>250</b> are examples, and can be changed to any value.</p><p id="p-0035" num="0034">The Conv layer <b>210</b> is a layer including scalar neurons. The other four layers <b>220</b> to <b>250</b> are layers including vector neurons. The vector neuron is a neuron in which a vector is used as an input or an output. In the above descriptions, the dimension of the output vector of each vector neuron is constant at <b>16</b>. Hereinafter, a term &#x201c;node&#x201d; is used as a superordinate concept of the scalar neuron and the vector neuron.</p><p id="p-0036" num="0035">In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, regarding the Conv layer <b>210</b>, a first axis x and a second axis y that define plane coordinates of node arrays, and a third axis z that represents a depth are shown. <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows that the sizes of the Conv layer <b>210</b> in x, y, and z directions are 13, 13, and 32. The size in the x direction and the size in the y direction are referred to as &#x201c;resolution&#x201d;. The size in the z direction is the number of channels. These three axes x, y, and z are also used as coordinate axes that indicate the positions of the nodes in the other layers. However, in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the axes x, y, and z are not shown in the layers other than the Conv layer <b>210</b>.</p><p id="p-0037" num="0036">As is well known, a resolution W<b>1</b> after convolution is obtained by the following equation.</p><p id="p-0038" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i>1=Ceil{(<i>W</i>0&#x2212;<i>Wk+</i>1)/<i>S}</i>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0039" num="0037">Here, W0 is the resolution before convolution, Wk is the surface size of the kernel, S is the stride, and Ceil{X} is a function for executing an operation of rounding up the decimal part of X.</p><p id="p-0040" num="0038">The resolution of each layer shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is an example when the resolution of the input data IM is 29, and the actual resolution of each layer is appropriately changed according to the size of the input data IM.</p><p id="p-0041" num="0039">The ClassVN layer <b>250</b> has Nm channels. In the example in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, Nm=2. In general, Nm is an integer of 2 or more, and is the number of classes that can be determined using the machine learning model <b>200</b>. The number Nm of classes that can be determined can be set to a different value for each machine learning model <b>200</b>. Determination values Class 1 and Class 2 corresponding to two classes are output from two channels of the ClassVN layer <b>250</b>. In general, the class having the largest value among the determination values Class 1 and Class 2 is used as a class determination result of the input data IM. When the largest value in the determination values Class 1 and Class 2 is less than a predetermined threshold value, it can also be determined that the class of the input data IM is unknown.</p><p id="p-0042" num="0040">In the present disclosure, as will be described later, instead of using the determination values Class 1 and Class 2 of the ClassVN layer <b>250</b> that is the output layer, a method for determining whether the input data is known or unknown using the similarity calculated based on the output of a specific vector neuron layer is used.</p><p id="p-0043" num="0041">In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, partial regions Rn in the layers <b>210</b>, <b>220</b>, <b>230</b>, <b>240</b>, and <b>250</b> are further depicted. The subscript &#x201c;n&#x201d; of each partial region Rn is the numeral number of each layer. For example, a partial region R<b>210</b> indicates a partial region in the Conv layer <b>210</b>. The &#x201c;partial region Rn&#x201d; is a region that is specified by a plane position (x, y) defined by the position of the first axis x and the position of the second axis y in each layer and includes a plurality of channels along the third axis z. Each partial region Rn has dimensions of &#x201c;Width&#x201d;&#xd7;&#x201c;Height&#x201d;&#xd7;&#x201c;Depth&#x201d; corresponding to the first axis x, the second axis y, and the third axis z. In the present embodiment, the number of nodes included in one &#x201c;partial region Rn&#x201d; is &#x201c;1&#xd7;1&#xd7;depth number&#x201d;, that is, &#x201c;1&#xd7;1&#xd7;number of channels&#x201d;.</p><p id="p-0044" num="0042">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a feature spectrum Sp_ConvVN1 to be described later is calculated based on an output of the ConvVN1 layer <b>230</b>, and is input to the similarity calculation unit <b>310</b>. Similarly, feature spectra Sp_ConvVN2 and Sp_ClassVN are calculated based on outputs of the ConvVN2 layer <b>240</b> and the ClassVN layer <b>250</b>, respectively, and are input to the similarity calculation unit <b>310</b>. The similarity calculation unit <b>310</b> calculates a similarity between the feature spectra Sp_ConvVN1, Sp_ConvVN, and Sp_ClassVN and the known feature spectrum group KSp generated in advance.</p><p id="p-0045" num="0043">In the present disclosure, the vector neuron layer used to calculate the similarity is also referred to as a &#x201c;specific layer&#x201d;. As the specific layer, any number of one or more vector neuron layers can be used. The configuration of the feature spectrum and the method for calculating the similarity using the feature spectrum will be described later.</p><p id="p-0046" num="0044"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart showing a processing procedure of a preparation process of a machine learning model. In step S<b>110</b>, a plurality of pieces of data of non-defective products are generated by capturing a plurality of non-defective product samples.</p><p id="p-0047" num="0045"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram showing an example of a product to be inspected. Here, examples of a non-defective product PDa and a defective product PDb are shown. The defective product PDb has a flaw DF that is not present in the non-defective product PDa. Each of the non-defective product samples captured in step S<b>110</b> is a product having no defect such as the flaw DF.</p><p id="p-0048" num="0046">In step S<b>120</b>, preprocessing is applied to the data of the non-defective products. As the preprocessing, for example, processing such as size adjustment and data normalization (min-max normalization) can be executed. <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example in which, as the size adjustment, the image of the non-defective product PDa is divided into a plurality of patch images PPDa by division lines indicated by broken lines. Each of the patch images PPDa is used as the data of the non-defective product. Alternatively, the preprocessing may be omitted. In step S<b>130</b>, the class determination processing unit <b>112</b> executes processing of dividing the plurality of pieces of data of the non-defective products into a plurality of groups.</p><p id="p-0049" num="0047"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows the grouped data of the non-defective products. In this example, the plurality of pieces of data of the non-defective products are grouped, by clustering processing, into a first group TD<b>1</b> of the data of the non-defective products and a second group TD<b>2</b> of the data of the non-defective products. In the clustering processing, for example, image data that is the data of the non-defective products is converted into grayscale data, two-dimensional data is converted into one-dimensional data by raster-scanning and arranging luminance from an upper left pixel, and the one-dimensional data is classified into a plurality of groups by the k-mean method. Alternatively, a method may be used in which image data is converted into one-dimensional data, the dimension is reduced by applying principal component analysis (PCA), and the one-dimensional data is classified into a plurality of groups by the k-mean method. In the example in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the one-dimensional data is classified into two groups TD<b>1</b> and TD<b>2</b> centered on centers of gravities G<b>1</b> and G<b>2</b>, respectively. Grouping may be executed using a method other than the k-means method. For example, the data of the non-defective products may be arranged and classified into a plurality of groups according to ordinal numbers thereof.</p><p id="p-0050" num="0048">In step S<b>140</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a label is assigned to each group to generate a plurality of training data groups. In the present embodiment, labels &#x201c;1&#x201d; and &#x201c;2&#x201d; are assigned to two groups of the data of the non-defective products, respectively, to generate two training data groups. These labels correspond to the two classes Class 1 and Class 2 of the machine learning model <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In the present disclosure, &#x201c;label&#x201d; and &#x201c;class&#x201d; mean the same.</p><p id="p-0051" num="0049">In step S<b>150</b>, the class determination processing unit <b>112</b> executes the learning of the machine learning model <b>200</b> using the plurality of training data groups. When the learning using the plurality of training data groups is completed, the learned machine learning model <b>200</b> is stored in the memory <b>120</b>.</p><p id="p-0052" num="0050">In step S<b>160</b>, a plurality of pieces of training data are reinput to the learned machine learning model <b>200</b> to generate the known feature spectrum group KSp. The known feature spectrum group KSp is a set of feature spectra to be described below.</p><p id="p-0053" num="0051"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram showing the feature spectrum Sp obtained by inputting any input data to the learned machine learning model <b>200</b>. Here, the feature spectrum Sp obtained based on the output of the ConvVN1 layer <b>230</b> will be described. A horizontal axis in <figref idref="DRAWINGS">FIG. <b>6</b></figref> indicates positions of vector elements related to output vectors of a plurality of nodes included in one partial region R<b>230</b> of the ConvVN1 layer <b>230</b>. The position of the vector element is represented by a combination of an element number ND of the output vector at each node and a channel number NC. In the present embodiment, since the vector dimension is 16 (the number of elements of the output vector output by each node), the element number ND of the output vector is 16 from 0 to 15. Since the number of channels of the ConvVN1 layer <b>230</b> is 12, the channel number NC is 12 from 0 to 11. In other words, the feature spectrum Sp is obtained by arranging a plurality of element values of an output vector of each vector neuron included in one partial region R<b>230</b> over a plurality of channels along the third axis z.</p><p id="p-0054" num="0052">A vertical axis in <figref idref="DRAWINGS">FIG. <b>6</b></figref> indicates a feature value C<sub>V </sub>at each spectral position. In this example, the feature value C<sub>V </sub>is a value V<sub>ND </sub>of each element of the output vector. The feature value C<sub>V </sub>may be subjected to statistical processing such as centering to an average value 0. As the feature value C<sub>V</sub>, a value obtained by multiplying the value V<sub>ND </sub>of each element of the output vector by a normalization coefficient to be described later may be used, or the normalization coefficient may be used as it is. In the latter case, the number of feature values C<sub>V </sub>included in the feature spectrum Sp is equal to the number of channels and is 12. The normalization coefficient is a value corresponding to a vector length of the output vector of the node.</p><p id="p-0055" num="0053">Since the number of feature spectra Sp obtained based on the output of the ConvVN1 layer <b>230</b> for one piece of input data is equal to the number of planar positions (x, y) of the ConvVN1 layer <b>230</b>, that is, the number of partial regions R<b>230</b>, the number is 36. Similarly, for one piece of input data, 16 feature spectra Sp are obtained based on the output of the ConvVN2 layer <b>240</b>, and one feature spectrum Sp is obtained based on the output of the ClassVN layer <b>250</b>.</p><p id="p-0056" num="0054">When the training data is reinput to the learned machine learning model <b>200</b>, the similarity calculation unit <b>310</b> calculates the feature spectrum Sp shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> and registers the feature spectrum Sp as the known feature spectrum group KSp in the memory <b>120</b>.</p><p id="p-0057" num="0055"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram showing a configuration of the known feature spectrum group KSp. In this example, a known feature spectrum group KSp_ConvVN1 obtained based on the output of the ConvVN1 layer <b>230</b> of the machine learning model <b>200</b> is shown. A known feature spectrum group KSp_ConvVN2 obtained based on the output of the ConvVN2 layer <b>240</b> and a known feature spectrum group KSp_ConvVN1 obtained based on the output of the ClassVN layer <b>250</b> also have the same configuration, and are not shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. As the known feature spectrum group KSp, one obtained based on an output of at least one vector neuron layer may be registered.</p><p id="p-0058" num="0056">Each record of the known feature spectrum group KSp_ConvVN1 includes a parameter i indicating the order of labels or classes, a parameter j indicating the order of specific layers, a parameter k indicating the order of partial regions Rn, a parameter q indicating a data number, and the known feature spectrum KSp. The known feature spectrum KSp is the same as the feature spectrum Sp in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0059" num="0057">The parameter i of the class takes the same value of 1 and 2 as the label. The parameter j of the specific layer takes values of 1 to 3 indicating which of the three specific layers <b>230</b>, <b>240</b>, and <b>250</b> is. The parameter k of the partial region Rn takes a value indicating which of the plurality of partial regions Rn included in the specific layers is, that is, which of the planar positions (x, y) is. For the ConvVN1 layer <b>230</b>, since the number of partial regions R<b>230</b> is 36, k=1 to 36. The parameter q of the data number indicates the number of the training data to which the same label is assigned, and takes a value of 1 to max1 for class 1 and a value of 1 to max2 for class 2. The known feature spectrum group KSp does not need to be classified into classes. That is, the known feature spectrum group KSp may include known feature spectra for all classes without distinguishing the plurality of classes.</p><p id="p-0060" num="0058">The plurality of pieces of training data TD used in step S<b>160</b> are not necessarily the same as the plurality of pieces of training data used in step S<b>150</b>. However, also in step S<b>160</b>, if a part or all of the plurality of pieces of training data used in step S<b>150</b> are used, there is an advantage in that it is not necessary to prepare additional training data.</p><p id="p-0061" num="0059"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart showing a processing procedure of a quality determination process using the learned machine learning model <b>200</b>. In step S<b>210</b>, the class determination processing unit <b>112</b> obtains inspection data by capturing the image of the product to be inspected by the camera <b>400</b>. In step S<b>220</b>, the class determination processing unit <b>112</b> applies preprocessing to the inspection data. The preprocessing is the same as the preprocessing used in step S<b>120</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. As described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in the present embodiment, in the preprocessing, the image of the product to be inspected is divided into a plurality of patch images, and each patch image is generated as the inspection data.</p><p id="p-0062" num="0060">In step S<b>230</b>, the class determination processing unit <b>112</b> inputs the inspection data to the learned machine learning model <b>200</b>, and calculates the feature spectrum Sp based on an output of the machine learning model <b>200</b>. In step S<b>240</b>, the similarity calculation unit <b>310</b> calculates the similarity based on the feature spectrum Sp obtained according to the input of the inspection data and the registered known feature spectrum group KSp. When the known feature spectrum group KSp is configured for each class as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the similarity between the known feature spectrum group KSp and the feature spectrum Sp is calculated for each class in step S<b>240</b>. Hereinafter, the similarity is referred to as a &#x201c;class similarity&#x201d;. On the other hand, when the known feature spectrum group KSp includes the known feature spectra for all the classes without distinguishing the plurality of classes, the similarity between the entire known feature spectrum group KSp and the feature spectrum Sp is calculated in step S<b>240</b>. In the present embodiment, a method using the class similarity will be mainly described.</p><p id="p-0063" num="0061"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram showing a state in which a class similarity Sclass related to the inspection data is obtained. When the inspection data is input to the machine learning model <b>200</b>, the class determination processing unit <b>112</b> calculates the feature spectra Sp_ConvVN1, Sp_ConvVN2, and Sp_ClassVN based on the outputs of the ConvVN1 layer <b>230</b>, the ConvVN2 layer <b>240</b>, and the ClassVN layer <b>250</b> of the machine learning model <b>200</b>, respectively. The similarity calculation unit <b>310</b> calculates a class similarity Sclass_ConvVN1 using the feature spectrum Sp_ConvVN1 and the known feature spectrum group KSp_ConvVN1 that are obtained based on the output of the ConvVN1 layer <b>230</b>. A specific method for calculating the class similarity will be described later. Similarly, for the ConvVN2 layer <b>240</b> and the ClassVN layer <b>250</b>, class similarities Sclass_ConvVN2 and Sclass_ClassVN are calculated.</p><p id="p-0064" num="0062">Although it is not necessary to generate all of the class similarities Sclass_ConvVN1, Sclass_ConvVN2, and Sclass_ClassVN using the three vector neuron layers <b>230</b>, <b>240</b>, and <b>250</b>, it is preferable to calculate the class similarities using one or more of these vector neuron layers. As described above, in the present disclosure, the vector neuron layer used to calculate the similarity is referred to as a &#x201c;specific layer&#x201d;.</p><p id="p-0065" num="0063">The similarity calculation unit <b>310</b> calculates a final class similarity Sclass(i) using at least one of the class similarities Sclass_ConvVN1, Sclass_ConvVN2, and Sclass_ClassVN. For example, one of the three class similarities Sclass_ConvVN1, Sclass_ConvVN2, and Sclass_ClassVN, which is selected in advance, may be used as the final class similarity Sclass(i). Alternatively, results of appropriate statistical processing of the three class similarities Sclass_ConvVN1, Sclass_ConvVN2, and Sclass_ClassVN may be used as the final class similarity Sclass(i). The &#x201c;result of appropriate statistical processing&#x201d; is a statistically representative value. As the statistically representative value, for example, a maximum value, an average value, or a minimum value can be used. Alternatively, a weighted average by a weight determined based on a range of the similarity for each layer can be used. In general, it is preferable to adopt the minimum value among the plurality of class similarities obtained from the plurality of specific layers. The class similarity Sclass(i) is an index indicating a degree to which the inspection data is similar to the training data related to each class i.</p><p id="p-0066" num="0064">In step S<b>250</b>, the quality determination unit <b>320</b> determines whether the similarity calculated in step S<b>240</b> is equal to or greater than a predetermined threshold value. As described above, in the present embodiment, the class similarity Sclass(i) described with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref> is used as the similarity. In the determination of step S<b>250</b>, one of the following two determination methods can be adopted.</p><heading id="h-0009" level="2">Determination Method J1</heading><p id="p-0067" num="0065">When the class similarities Sclass(i) for all the classes are less than the threshold value, the inspection data is determined to be defective, and when the class similarity Sclass(i) for at least one class is equal to or greater than the threshold value, the inspection data is determined to be non-defective.</p><heading id="h-0010" level="2">Determination Method J2</heading><p id="p-0068" num="0066">When the class similarity Sclass(i) corresponding to one determination class indicated by the determination values Class 1 and Class 2 of the ClassVN layer <b>250</b> is equal to or greater than the threshold value, the inspection data is determined to be non-defective, and when the class similarity Sclass(i) corresponding to the determination class is less than the threshold value, the inspection data is determined to be defective.</p><p id="p-0069" num="0067">In the determination method J1, since the determination is executed for all the classes, the determination can be executed more reliably. In the determination method J2, since it is only necessary to execute the calculation of the class similarity in step S<b>240</b> for one determination class, the processing can be speeded up.</p><p id="p-0070" num="0068">When it is determined in step S<b>250</b> that the similarity is equal to or greater than the threshold value, the processing proceeds to step S<b>260</b>, and the inspection data is determined to be non-defective. On the other hand, when it is determined that the similarity is less than the threshold value, the processing proceeds to step S<b>270</b>, and the inspection data is determined to be defective. As described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, when an image of the product to be inspected is divided into a plurality of patch images and it is determined that the similarity is equal to or greater than the threshold value for all of the patch images, the inspection data is determined to be non-defective. When the non-defective product PDa and the defective product PDb as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> are respectively inspected as the product to be inspected using the machine learning model <b>200</b> according to the present embodiment, the quality of each product can be correctly determined.</p><p id="p-0071" num="0069">As described above, the known feature spectrum group KSp may include the known feature spectra for all classes without distinguishing the plurality of classes. In this case, in step S<b>240</b>, the similarity between the entire known feature spectrum group KSp and the feature spectrum Sp is calculated, and in step S<b>250</b>, the similarity is compared with the threshold value. This method also makes it possible to correctly determine the quality of the inspection data. Since the similarity is calculated without considering the class, the similarity calculation processing is simplified.</p><p id="p-0072" num="0070">As described above, in the present embodiment, since the learning of the machine learning model <b>200</b> is executed using, as training data, the plurality of pieces of data of the non-defective products classified into the plurality of classes and the quality determination of the inspection data is executed using the similarity calculated using the learned machine learning model <b>200</b> and the known feature spectrum group KSp, the quality determination can be executed without preparing data of defective products as the training data.</p><heading id="h-0011" level="1">B. Method for Calculating Similarity</heading><p id="p-0073" num="0071">As a method for calculating the above class similarity, for example, any one of the following four methods can be adopted.</p><p id="p-0074" num="0072">(1) a first calculation method M1 for obtaining the class similarity without considering correspondence between the feature spectrum Sp and the partial region Rn in the known feature spectrum group KSp</p><p id="p-0075" num="0073">(2) a second calculation method M2 for obtaining the class similarity by the corresponding partial regions Rn of the feature spectrum Sp and the known feature spectrum group KSp</p><p id="p-0076" num="0074">(3) a third calculation method M3 for obtaining the class similarity without considering the partial region Rn at all</p><p id="p-0077" num="0075">(4) a fourth calculation method M4 for obtaining the similarity using the known feature spectrum group KSp including known feature spectra for all classes</p><p id="p-0078" num="0076">Hereinafter, a method for calculating the similarity based on the output of the ConvVN1 layer <b>230</b> according to the four calculation methods M1, M2, M3, and M4 will be sequentially described. In the following descriptions, the parameter q of the inspection data is omitted.</p><p id="p-0079" num="0077"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic diagram showing the first calculation method M1 for the class similarity. In the first calculation method M1, first, a local similarity S(i, j, k) indicating the similarity to each class i is calculated for each partial region k based on the output of the ConvVN1 layer <b>230</b> that is the specific layer. In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the number of partial regions k is six for simplification of illustration, and in the machine learning model <b>200</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, k=36. One of the three class similarities Sclass(i, j) shown on the right side of <figref idref="DRAWINGS">FIG. <b>10</b></figref> is calculated based on the local similarities S(i, j, k). The class similarity Sclass(i, j) is the same as the class similarity Sclass_ConvVN1 shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0080" num="0078">In the first calculation method M1, the local similarity S(i, j, k) is calculated using the following equation.</p><p id="p-0081" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i>(<i>i,j,k</i>)=max[<i>G</i>{Sp(<i>j,k</i>),<i>K</i>Sp(<i>i,j,k</i>=all,<i>q</i>=all)}]&#x2003;&#x2003; (c1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0082" num="0079">Here, i is a parameter indicating the class, j is a parameter indicating the specific layer, k is a parameter indicating the partial region Rn, q is a parameter indicating the data number, G{a, b} is a function for obtaining the similarity between a and b, Sp(j, k) is the feature spectrum obtained based on the output of the specific partial region k of the specific layer j according to the inspection data, KSp(i, j, k=all, q=all) is the known feature spectrum of all the data numbers q in all the partial regions k of the specific layer j associated with the class i in the known feature spectrum group KSp shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, and max[X] is a logical operation that takes the maximum value among the values of X.</p><p id="p-0083" num="0080">As the function G{a, b} for obtaining the similarity, for example, an equation for obtaining a cosine similarity or an equation for obtaining the similarity according to the distance can be used.</p><p id="p-0084" num="0081">The three types of class similarities Sclass(i, j) shown on the right side of <figref idref="DRAWINGS">FIG. <b>10</b></figref> are obtained by, for each class i, taking the maximum value, the average value, or the minimum value of the local similarities S(i, j, k) for the plurality of partial regions k. Which calculation of the maximum value, the average value, and the minimum value to be used depends on the purpose of use of the class determination processing, and in general, it is considered that the minimum value is preferably used. However, which of the three types of calculations to be used is experimentally or empirically set in advance by a user.</p><p id="p-0085" num="0082">As described above, in the first calculation method M1 for the class similarity,</p><p id="p-0086" num="0083">(1) the local similarity S(i, j, k), which is the similarity between the feature spectrum Sp obtained based on the output of the specific partial region k of the specific layer j and all known feature spectra KSp associated with the specific layer j and each class i, is obtained according to the inspection data, and</p><p id="p-0087" num="0084">(2) the class similarity Sclass(i, j) is obtained, for each class i, by taking the maximum value, the average value, or the minimum value of the local similarities S(i, j, k) for the plurality of partial regions k.</p><p id="p-0088" num="0085">According to the first calculation method M1, the class similarity Sclass(i, j) can be obtained by a relatively simple calculation and procedure.</p><p id="p-0089" num="0086"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram showing the second calculation method M2 for the class similarity. In the second calculation method M2, the local similarity S(i, j, k) is calculated using the following equation instead of the above equation (c1).</p><p id="p-0090" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i>(<i>i,j,k</i>)=max[<i>G</i>{Sp(<i>j,k</i>),<i>K</i>Sp(<i>i,j,k,q</i>=all)}]&#x2003;&#x2003;(c2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0091" num="0087">Here, KSp(i, j, k, q=all) is the known feature spectrum of all the data numbers q in the specific partial region k of the specific layer j associated with the class i in the known feature spectrum group KSp shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0092" num="0088">In the above first calculation method M1, the known feature spectrum KSp(i, j, k=all, q=all) in all the partial regions k of the specific layer j is used, whereas in the second calculation method M2, only the known feature spectrum KSp(i, j, k, q=all) for the same partial region k as the partial region k of the feature spectrum Sp(j, k) is used. Other methods in the second calculation method M2 are the same as those in the first calculation method M1.</p><p id="p-0093" num="0089">In the second calculation method M2 for the class similarity,</p><p id="p-0094" num="0090">(1) the local similarity S(i, j, k), which is the similarity between the feature spectrum Sp obtained based on the output of the specific partial region k of the specific layer j and all known feature spectra KSp associated with the specific partial region k of the specific layer j and each class i, is obtained according to the inspection data, and</p><p id="p-0095" num="0091">(2) the class similarity Sclass(i, j) is obtained, for each class i, by taking the maximum value, the average value, or the minimum value of the local similarities S(i, j, k) for the plurality of partial regions k.</p><p id="p-0096" num="0092">According to the second calculation method M2, the class similarity Sclass(i, j) can also be obtained by a relatively simple calculation and procedure.</p><p id="p-0097" num="0093"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic diagram showing the third calculation method M3 for the class similarity. In the third calculation method M3, the class similarity Sclass(i, j) is calculated based on the output of the ConvVN1 layer <b>230</b>, which is the specific layer, without obtaining the local similarity S(i, j, k).</p><p id="p-0098" num="0094">The class similarity Sclass(i, j) obtained by the third calculation method M3 is calculated using the following equation.</p><p id="p-0099" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i>class(<i>i,j</i>)=max[<i>G</i>{Sp(<i>j,k</i>=all),<i>K</i>Sp(<i>i,j,k</i>=all,<i>q</i>=all)}]&#x2003;&#x2003;(c3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0100" num="0095">Here, Sp(j, k=all) is the feature spectrum obtained based on the outputs of all the partial regions k of the specific layer j according to the inspection data.</p><p id="p-0101" num="0096">As described above, in the third calculation method M3 for the class similarity,</p><p id="p-0102" num="0097">(1) the class similarity Sclass(i, j), which is the similarity between all the feature spectra Sp obtained based on the output of the specific layer j according to the inspection data and all the known feature spectra KSp associated with the specific layer j and each class i, is obtained for each class.</p><p id="p-0103" num="0098">According to the third calculation method M3, the class similarity Sclass(i, j) can be obtained by a simpler calculation and procedure.</p><p id="p-0104" num="0099">Each of the three calculation methods M1 to M3 described above is a method for calculating the class similarity for each specific layer i. As described above, in the present embodiment, the class similarity can be calculated using one or more of the plurality of vector neuron layers <b>230</b>, <b>240</b>, and <b>250</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> as the specific layer. When a plurality of specific layers are used, for example, it is preferable to use the minimum value among the plurality of class similarities obtained based on the plurality of specific layers as the final similarity.</p><p id="p-0105" num="0100">In the fourth calculation method M4, the similarity with the feature spectrum Sp is calculated using the known feature spectrum group KSp including the known feature spectra for all the classes. A similarity S_all(j) obtained by the fourth calculation method M4 is calculated using the following equation similar to the above equation (c3).</p><p id="p-0106" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i>_all(<i>j</i>)=max[<i>G</i>{Sp(<i>j,k</i>=all),<i>K</i>Sp(<i>i</i>=all,<i>j,k</i>=all,<i>q</i>=all)}]&#x2003;&#x2003;(c4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0107" num="0101">Here, Sp(j, k=all) is a feature spectrum obtained based on the outputs of all the partial regions k of the specific layer j according to the inspection data, and KSp(i=all, j, k=all, q=all) is a known feature spectrum of all data numbers q in all partial regions k of the specific layer j with respect to all classes i in the known feature spectrum group KSp shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0108" num="0102">Also in the fourth calculation method M4, when a plurality of specific layers are used, it is preferable to use the minimum value among the plurality of similarities obtained based on the plurality of specific layers as the final similarity.</p><heading id="h-0012" level="1">C. Calculation Method for Output Vector of Each Layer of Machine Learning Model</heading><p id="p-0109" num="0103">A method for calculating the output of each layer in the machine learning model <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is as follows.</p><p id="p-0110" num="0104">Each node of the PrimeVN layer <b>220</b> regards scalar outputs of 1&#xd7;1&#xd7;32 nodes of the Conv layer <b>210</b> as a 32-dimensional vector, and multiplies the vector by a transformation matrix to obtain a vector output of the nodes. The transformation matrix is an element of a kernel having a surface size of 1&#xd7;1, and is updated by the learning of the machine learning model <b>200</b>. The processing of the Conv layer <b>210</b> and the PrimeVN layer <b>220</b> may be integrated to form one prime vector neuron layer.</p><p id="p-0111" num="0105">When the PrimeVN layer <b>220</b> is referred to as a &#x201c;lower layer L&#x201d; and the ConvVN1 layer <b>230</b> adjacent to the upper side of the PrimeVN layer <b>220</b> is referred to as an &#x201c;upper layer L+1&#x201d;, the output of each node of the upper layer L+1 is determined using the following equations.</p><p id="p-0112" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>v</mi>      <mi>ij</mi>     </msub>     <mo>=</mo>     <mrow>      <msubsup>       <mi>W</mi>       <mi>ij</mi>       <mi>L</mi>      </msubsup>      <mo>&#x2062;</mo>      <msubsup>       <mi>M</mi>       <mi>i</mi>       <mi>L</mi>      </msubsup>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>E</mi>      <mo>&#x2062;</mo>      <mn>1</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>u</mi>      <mi>j</mi>     </msub>     <mo>=</mo>     <mrow>      <msub>       <mo>&#x2211;</mo>       <mi>i</mi>      </msub>      <msub>       <mi>v</mi>       <mi>ij</mi>      </msub>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mi>E2</mi>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-3" num="00001.3"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>a</mi>      <mi>j</mi>     </msub>     <mo>=</mo>     <mrow>      <mi>F</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mo>&#xf605;</mo>       <msub>        <mi>u</mi>        <mi>j</mi>       </msub>       <mo>&#xf606;</mo>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mi>E3</mi>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-4" num="00001.4"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msubsup>      <mi>M</mi>      <mi>j</mi>      <mrow>       <mi>L</mi>       <mo>+</mo>       <mn>1</mn>      </mrow>     </msubsup>     <mo>=</mo>     <mrow>      <msub>       <mi>a</mi>       <mi>j</mi>      </msub>      <mo>&#xd7;</mo>      <mfrac>       <mn>1</mn>       <mrow>        <mo>&#xf605;</mo>        <msub>         <mi>u</mi>         <mi>j</mi>        </msub>        <mo>&#xf606;</mo>       </mrow>      </mfrac>      <mo>&#x2062;</mo>      <msub>       <mi>u</mi>       <mi>j</mi>      </msub>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mi>E4</mi>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0113" num="0106">Here, M<sup>L</sup><sub>i </sub>is an output vector of an i-th node in the lower layer L, M<sup>L+1</sup><sub>j </sub>is an output vector of a j-th node in the upper layer L+1, v<sub>ij </sub>is a prediction vector of the output vector M<sup>L+1</sup><sub>j</sub>, W<sup>L</sup><sub>ij </sub>is a prediction matrix for calculating the prediction vector v<sub>ij </sub>based on the output vector M<sup>L</sup><sub>i </sub>of the lower layer L, u<sub>j </sub>is a sum vector that is a sum of the prediction vectors v<sub>ij</sub>, that is, a linear combination, a<sub>j </sub>is an activation value that is a normalization coefficient obtained by normalizing the norm |u<sub>j</sub>| of the sum vector u<sub>j</sub>, and F(X) is a normalization function that normalizes X.</p><p id="p-0114" num="0107">As the normalization function F(X), for example, the following equation (E3a) or (E3b) can be used.</p><p id="p-0115" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>a</mi>      <mi>j</mi>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <mi>F</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mo>&#xf605;</mo>        <msub>         <mi>u</mi>         <mi>j</mi>        </msub>        <mo>&#xf606;</mo>       </mrow>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mrow>       <mrow>        <mi>softmax</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mo>&#xf605;</mo>         <msub>          <mi>u</mi>          <mi>j</mi>         </msub>         <mo>&#xf606;</mo>        </mrow>        <mo>)</mo>       </mrow>       <mo>=</mo>       <mfrac>        <mrow>         <mi>exp</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>&#x3b2;</mi>          <mo>&#x2062;</mo>          <mrow>           <mo>&#xf605;</mo>           <msub>            <mi>u</mi>            <mi>j</mi>           </msub>           <mo>&#xf606;</mo>          </mrow>         </mrow>         <mo>)</mo>        </mrow>        <mrow>         <msub>          <mo>&#x2211;</mo>          <mi>k</mi>         </msub>         <mrow>          <mi>exp</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mi>&#x3b2;</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>&#xf605;</mo>            <msub>             <mi>u</mi>             <mi>k</mi>            </msub>            <mo>&#xf606;</mo>           </mrow>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </mfrac>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mi>E3a</mi>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>a</mi>      <mi>j</mi>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <mi>F</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mo>&#xf605;</mo>        <msub>         <mi>u</mi>         <mi>j</mi>        </msub>        <mo>&#xf606;</mo>       </mrow>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mfrac>       <mrow>        <mo>&#xf605;</mo>        <msub>         <mi>u</mi>         <mi>j</mi>        </msub>        <mo>&#xf606;</mo>       </mrow>       <mrow>        <msub>         <mo>&#x2211;</mo>         <mi>k</mi>        </msub>        <mrow>         <mo>&#xf605;</mo>         <msub>          <mi>u</mi>          <mi>k</mi>         </msub>         <mo>&#xf606;</mo>        </mrow>       </mrow>      </mfrac>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mi>E3b</mi>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0116" num="0108">Here, k is an ordinal number for all nodes of the upper layer L+1, and &#x3b2; is an adjustment parameter that is any positive coefficient, and for example, &#x3b2;=1.</p><p id="p-0117" num="0109">In the above equation (E3a), the activation value a<sub>j </sub>is obtained by normalizing the norm |u<sub>j</sub>| of the sum vector u<sub>j </sub>for all nodes of the upper layer L+1 by the softmax function. On the other hand, in the equation (E3b), the activation value a<sub>j </sub>is obtained by dividing the norm |u<sub>j</sub>| of the sum vector u<sub>j </sub>by the sum of the norms |u<sub>j</sub>| of all the nodes of the upper layer L+1. As the normalization function F(X), a function other than the equation (E3a) and the equation (E3b) may be used.</p><p id="p-0118" num="0110">For convenience, the ordinal number i of the above equation (E2) is assigned to the nodes of the lower layer L used for determining the output vector M<sup>L+1</sup><sub>j </sub>of the j-th node in the upper layer L+1, and takes values of 1 to n. The integer n is the number of nodes in the lower layer L used to determine the output vector M<sup>L+1</sup><sub>j </sub>of the j-th node in the upper layer L+1. Therefore, the integer n is given by the following equation.</p><p id="p-0119" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>n=Nk&#xd7;Nc</i>&#x2003;&#x2003;(E5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0120" num="0111">Here, Nk is the surface size of the kernel, and Nc is the number of channels of the PrimeVN layer <b>220</b> that is the lower layer. In the example in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, since Nk=9 and Nc=16, n=144.</p><p id="p-0121" num="0112">One kernel used to obtain the output vector of the ConvVN1 layer <b>230</b> has 3&#xd7;3&#xd7;16=144 elements in which the kernel size is 3&#xd7;3 as the surface size and the number of channels of the lower layer is 16 as the depth, and each of these elements is the prediction matrix W<sup>L</sup><sub>ij</sub>. In order to generate the output vectors of 12 channels of the ConvVN1 layer <b>230</b>, <b>12</b> sets of kernels are required. Therefore, the number of the prediction matrices W<sup>L</sup><sub>ij </sub>of the kernel used to obtain the output vector of the ConvVN1 layer <b>230</b> is 144&#xd7;12=1728. These prediction matrices W<sup>L</sup><sub>ij </sub>are updated by the learning of the machine learning model <b>200</b>.</p><p id="p-0122" num="0113">As is clear from the above equations (E1) to (E4), the output vector M<sup>L+1</sup><sub>j </sub>of each node of the upper layer L+1 is obtained by the following calculation:</p><p id="p-0123" num="0114">(a) the output vector M<sup>L</sup><sub>i </sub>of each node of the lower layer L is multiplied by the prediction matrix W<sup>L</sup><sub>ij </sub>to obtain the prediction vector v<sub>ij</sub>,</p><p id="p-0124" num="0115">(b) the sum vector u<sub>j</sub>, which is the sum of the prediction vectors v<sub>ij </sub>obtained from the nodes of the lower layer L, that is, the linear combination, is obtained,</p><p id="p-0125" num="0116">(c) the activation value a<sub>j</sub>, which is the normalization coefficient, is obtained by normalizing the norm |u<sub>j</sub>| of the sum vector u<sub>j</sub>, and</p><p id="p-0126" num="0117">(d) the sum vector u<sub>j </sub>is divided by the norm |u<sub>j</sub>|, and further multiplied by the activation value a<sub>j</sub>.</p><p id="p-0127" num="0118">The activation value a<sub>j </sub>is a normalization coefficient obtained by normalizing the norm |u<sub>j</sub>| for all nodes of the upper layer L+1. Therefore, the activation value a can be considered as an index indicating a relative output intensity of each node among all nodes in the upper layer L+1. The norm used in the equations (E3), (E3a), (E3b), and (4) is an L2 norm representing a vector length in a typical example. At this time, the activation value a<sub>j </sub>corresponds to the vector length of the output vector M<sup>L+1</sup><sub>j</sub>. Since the activation value a<sub>j </sub>is merely used in the above equations (E3) and (E4), it is not necessary to output the activation value a<sub>j </sub>from the node. Alternatively, the upper layer L+1 can output the activation value a<sub>j </sub>to the outside.</p><p id="p-0128" num="0119">A configuration of a vector neural network is substantially the same as a configuration of a capsule network, and vector neurons of the vector neural network correspond to capsules of the capsule network. However, calculation according to the above equations (E1) to (E4) used in the vector neural network is different from calculation used in the capsule network. The largest difference between the two calculations is that, in the capsule network, the prediction vector v<sub>ij </sub>on the right side of the above equation (E2) is multiplied by a weight, and the weight is searched by repeating dynamic routing a plurality of times. On the other hand, in the vector neural network according to the present embodiment, the output vector M<sup>L+1</sup><sub>j </sub>is obtained by sequentially calculating the above equations (E1) to (E4) once. Therefore, there is an advantage that it is not necessary to repeat the dynamic routing and the calculation is executed at a higher speed. The vector neural network according to the present embodiment has an advantage that, the memory amount required for the calculation is smaller than that of the capsule network, and according to the experiment of the inventor of the present disclosure, the memory amount of the vector neural network is only approximately &#xbd; to &#x2153; of that of the capsule network.</p><p id="p-0129" num="0120">The vector neural network is the same as the capsule network in that a node that inputs and outputs a vector is used. Therefore, the advantage of using vector neurons is also common to the capsule network. The plurality of layers <b>210</b> to <b>250</b> are the same as a normal convolutional neural network in that, the higher the level, the larger the feature of the region, and the lower the level, the smaller the feature of the region. Here, the &#x201c;feature&#x201d; means a characteristic portion included in input data to the neural network. The vector neural network or the capsule network is superior to the normal convolutional neural network in that the output vector of a certain node includes spatial information representing spatial information on a feature represented by the node. That is, the vector length of the output vector of a certain node represents a presence probability of the feature represented by the node, and a vector direction represents the spatial information such as the direction and the scale of the feature. Therefore, the vector directions of the output vectors of two nodes belonging to the same layer represent positional relation of the features. Alternatively, it can be said that the vector directions of the output vectors of the two nodes represent variations of features. For example, in the case of a node corresponding to the feature of &#x201c;eye&#x201d;, the direction of the output vector may represent variations such as the fineness and the lifting manner of the eye. In the normal convolutional neural network, it is said that spatial information on a feature is lost due to pooling processing. As a result, the vector neural network and the capsule network have an advantage that the performance of identifying input data is superior to that of the normal convolutional neural network.</p><p id="p-0130" num="0121">The advantage of the vector neural network can also be considered as follows. That is, the vector neural network has an advantage in that an output vector of a node expresses a feature of input data as coordinates in a continuous space. Therefore, the output vector can be evaluated such that the features are similar if the vector directions are close. There is also an advantage that, for example, even when the feature included in the input data is not covered by the training data, the feature can be determined by interpolation. On the other hand, the normal convolutional neural network has a disadvantage that, disorderly compression is applied due to the pooling processing, and thus a feature of input data cannot be expressed as coordinates in a continuous space.</p><p id="p-0131" num="0122">The outputs of the nodes of the ConvVN2 layer <b>240</b> and the ClassVN layer <b>250</b> are also determined in the same manner using the above equations (E1) to (E4). Therefore, detailed descriptions thereof will be omitted. The resolution of the ClassVN layer <b>250</b>, which is the uppermost layer, is 1&#xd7;1, and the number of channels is Nm.</p><p id="p-0132" num="0123">The output of the ClassVN layer <b>250</b> is converted into a plurality of determination values Class 0 to Class 2 for a known class. These determination values are normally values normalized by the softmax function. Specifically, for example, the calculation is executed, that is, the vector length of the output vector is calculated based on the output vector of each node of the ClassVN layer <b>250</b>, and the vector length of each node is normalized by the softmax function, thereby obtaining a determination value for each class. As described above, the activation value a obtained by the above equation (E3) is a value corresponding to the vector length of the output vector M<sup>L+1</sup><sub>j</sub>, and is normalized. Therefore, the activation value a<sub>j </sub>in each node of the ClassVN layer <b>250</b> may be output and used as it is as a determination value for each class.</p><p id="p-0133" num="0124">In the above embodiment, as the machine learning model <b>200</b>, the vector neural network that obtains the output vector by the calculation of the above equations (E1) to (E4) is used. Alternatively, instead of the vector neural network, a capsule network disclosed in U.S. Pat. No. 5,210,798 or International Publication No. 2009/083553 may be used.</p><heading id="h-0013" level="1">Other Embodiments</heading><p id="p-0134" num="0125">The present disclosure is not limited to the above embodiment, and can be implemented in various forms without departing from the spirit of the present disclosure. For example, the present disclosure can also be implemented by the following aspects. Technical features in the above embodiment corresponding to technical features in the embodiments described below can be appropriately replaced or combined in order to solve a part or all of the problems of the present disclosure or in order to achieve a part or all of the effects of the present disclosure. Any of the technical features can be appropriately deleted unless the technical feature is described as essential herein.</p><p id="p-0135" num="0126">(1) A first aspect of the present disclosure provides a method for determining quality of inspection data using a machine learning model of a vector neural network type including a plurality of vector neuron layers. This method includes: (a) generating a plurality of pieces of training data by classifying a plurality of pieces of data of a non-defective product into a plurality of classes and assigning a plurality of labels distinguishing the plurality of classes to the plurality of pieces of data of the non-defective product; (b) executing learning of the machine learning model using the plurality of pieces of training data; (c) preparing a known feature spectrum group obtained based on an output of at least one specific layer among the plurality of vector neuron layers when the plurality of pieces of training data are input to the learned machine learning model; and (d) executing quality determination processing of the inspection data using the learned machine learning model and the known feature spectrum group. The (d) includes (d1) calculating a feature spectrum based on the output of the specific layer in response to an input of the inspection data to the machine learning model, (d2) calculating a similarity between the feature spectrum and the known feature spectrum group, and (d3) determining the inspection data to be non-defective when the similarity is equal to or greater than a preset threshold value, and determining the inspection data to be defective when the similarity is less than the threshold value.</p><p id="p-0136" num="0127">According to this method, since the learning of the machine learning model is executed using, as the training data, the plurality of pieces of data of the non-defective product classified into the plurality of classes and the quality determination of the inspection data is executed using the similarity calculated using the learned machine learning model and the known feature spectrum group, the quality determination can be executed without preparing data of a defective product as the training data.</p><p id="p-0137" num="0128">(2) In the above method, the (c) may include preparing the known feature spectrum group for each of the plurality of classes, the similarity calculated in the (d2) may be a class-based similarity between the feature spectrum and the known feature spectrum group related to a respective one of the plurality of classes, and in the (d3), the inspection data may be determined to be defective when the similarity related to all of the plurality of classes is less than the preset threshold value, and the inspection data may be determined to be non-defective when the similarity related to at least one class is equal to or greater than the threshold value.</p><p id="p-0138" num="0129">According to this method, the quality determination can be executed using the class-based similarity.</p><p id="p-0139" num="0130">(3) In the above method, the (c) may include preparing the known feature spectrum group for each of the plurality of classes, and the similarity calculated in the (d2) may be a similarity between the feature spectrum and the known feature spectrum group related to a determination class indicated by an output of the machine learning model in response to the inspection data.</p><p id="p-0140" num="0131">According to this method, the quality determination can be executed using the similarity related only to the determination class.</p><p id="p-0141" num="0132">(4) In the above method, the known feature spectrum group may include known feature spectra for all classes without distinguishing the plurality of classes.</p><p id="p-0142" num="0133">According to this method, the similarity can be calculated by a simple calculation.</p><p id="p-0143" num="0134">(5) In the above method, in the specific layer, vector neurons arranged in a plane defined by two axes including a first axis and a second axis may be arranged as a plurality of channels along a third axis in a direction different from the two axes, and in the specific layer, when a region that is specified by a plane position defined by a position of the first axis and a position of the second axis and includes the plurality of channels along the third axis is referred to as partial regions, for each of a plurality of partial regions included in the specific layer, the feature spectrum may be obtained as any one of (i) a first type of feature spectrum in which a plurality of element values of an output vector of each vector neuron included in the corresponding partial region are arranged over the plurality of channels along the third axis, (ii) a second type of feature spectrum obtained by multiplying each element value of the first type of feature spectrum by a normalization coefficient corresponding to a vector length of the output vector, and (iii) a third type of feature spectrum in which the normalization coefficient is arranged over the plurality of channels along the third axis.</p><p id="p-0144" num="0135">According to this method, the similarity can be obtained using any one of the three types of feature spectra obtained based on the output vector of the specific layer.</p><p id="p-0145" num="0136">(6) In the above method, the (d2) may include obtaining a plurality of local similarities that are similarities with respect to the classes related to the plurality of partial regions of the specific layer by obtaining a local similarity that is a similarity between the feature spectrum obtained based on an output of a specific partial region of the specific layer and all known feature spectra associated with the specific layer and the classes in response to the inspection data, and obtaining a class-based similarity that is a similarity on a class basis by obtaining, on a class basis, a maximum value, an average value, or a minimum value of the plurality of local similarities related to the plurality of partial regions.</p><p id="p-0146" num="0137">According to this method, the class similarity can be calculated by a relatively simple calculation.</p><p id="p-0147" num="0138">(7) In the above method, the (d2) may include obtaining a plurality of local similarities that are similarities with respect to the classes related to the plurality of partial regions of the specific layer by obtaining a local similarity that is a similarity between the feature spectrum obtained based on an output of a specific partial region of the specific layer and all known feature spectra associated with the specific partial region of the specific layer and the classes in response to the inspection data, and obtaining a class-based similarity that is a similarity on a class basis by obtaining, on a class basis, a maximum value, an average value, or a minimum value of the plurality of local similarities related to the plurality of partial regions.</p><p id="p-0148" num="0139">According to this method, the class similarity can be calculated by a relatively simple calculation.</p><p id="p-0149" num="0140">(8) In the above method, the (d2) may include obtaining a class-based similarity that is a similarity on a class basis by obtaining, on a class basis, a similarity between all feature spectra obtained based on the output of the specific layer and all known feature spectra associated with the specific layer and each class in response to the inspection data.</p><p id="p-0150" num="0141">According to this method, the class similarity can be calculated by a relatively simple calculation.</p><p id="p-0151" num="0142">(9) A second aspect of the present disclosure provides an information processing apparatus configured to execute quality determination processing for determining quality of inspection data using a machine learning model of a vector neural network type including a plurality of vector neuron layers. The information processing apparatus includes a memory configured to store the machine learning model, and a processor configured to execute a calculation using the machine learning model. The processor is configured to (a) generate a plurality of pieces of training data by classifying a plurality of pieces of data of a non-defective product into a plurality of classes and assigning a plurality of labels distinguishing the plurality of classes to the plurality of pieces of data of the non-defective product, (b) execute learning of the machine learning model using the plurality of pieces of training data, (c) prepare a known feature spectrum group obtained based on an output of at least one specific layer among the plurality of vector neuron layers when the plurality of pieces of training data are input to the learned machine learning model, and (d) execute quality determination processing of the inspection data using the learned machine learning model and the known feature spectrum group. The (d) includes (d1) calculating a feature spectrum based on the output of the specific layer in response to an input of the inspection data to the machine learning model, (d2) calculating a similarity between the feature spectrum and the known feature spectrum group, and (d3) determining the inspection data to be non-defective when the similarity is equal to or greater than a preset threshold value, and determining the inspection data to be defective when the similarity is less than the threshold value.</p><p id="p-0152" num="0143">According to this information processing apparatus, the learning of the machine learning model is executed using, as the training data, the plurality of pieces of data of the non-defective product classified into the plurality of classes, and the quality determination of the inspection data is executed using the similarity calculated using the learned machine learning model and the known feature spectrum group. Therefore, the quality determination can be executed without preparing data of a defective product as the training data.</p><p id="p-0153" num="0144">(10) A third aspect of the present disclosure provides a non-transitory computer readable storage medium storing a computer program configured to execute, by a processor, quality determination processing for determining quality of inspection data using a machine learning model of a vector neural network type including a plurality of vector neuron layers. The computer program includes: (a) generating a plurality of pieces of training data by classifying a plurality of pieces of data of a non-defective product into a plurality of classes and assigning a plurality of labels for distinguishing the plurality of classes to the plurality of pieces of data of the non-defective product; (b) executing learning of the machine learning model using the plurality of pieces of training data; (c) preparing a known feature spectrum group obtained based on an output of at least one specific layer among the plurality of vector neuron layers when the plurality of pieces of training data are input to the learned machine learning model; and (d) executing quality determination processing of the inspection data using the learned machine learning model and the known feature spectrum group. The (d) includes (d1) calculating a feature spectrum based on the output of the specific layer in response to an input of the inspection data to the machine learning model, (d2) calculating a similarity between the feature spectrum and the known feature spectrum group, and (d3) determining the inspection data to be non-defective when the similarity is equal to or greater than a preset threshold value, and determining the inspection data to be defective when the similarity is less than the threshold value.</p><p id="p-0154" num="0145">According to this non-transitory computer readable storage medium storing a computer program, the learning of the machine learning model is executed using, as the training data, the plurality of pieces of data of the non-defective product classified into the plurality of classes, and the quality determination of the inspection data is executed using the similarity calculated using the learned machine learning model and the known feature spectrum group. Therefore, the quality determination can be executed without preparing data of a defective product as the training data.</p><p id="p-0155" num="0146">The present disclosure can also be implemented in various aspects other than the above aspects. For example, the present disclosure can also be implemented in an aspect of a non-transitory computer readable storage medium storing a computer program for implementing functions of a class determination apparatus.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2 MATH-US-00001-3 MATH-US-00001-4" nb-file="US20230005119A1-20230105-M00001.NB"><img id="EMI-M00001" he="20.83mm" wi="76.20mm" file="US20230005119A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US20230005119A1-20230105-M00002.NB"><img id="EMI-M00002" he="16.59mm" wi="76.20mm" file="US20230005119A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for determining quality of inspection data using a machine learning model of a vector neural network type including a plurality of vector neuron layers, the method comprising:<claim-text>(a) generating a plurality of pieces of training data by classifying a plurality of pieces of data of a non-defective product into a plurality of classes and assigning a plurality of labels distinguishing the plurality of classes to the plurality of pieces of data of the non-defective product;</claim-text><claim-text>(b) executing learning of the machine learning model using the plurality of pieces of training data;</claim-text><claim-text>(c) preparing a known feature spectrum group obtained based on an output of at least one specific layer among the plurality of vector neuron layers when the plurality of pieces of training data are input to the learned machine learning model; and</claim-text><claim-text>(d) executing quality determination processing of the inspection data using the learned machine learning model and the known feature spectrum group, wherein</claim-text><claim-text>the (d) includes<claim-text>(d1) calculating a feature spectrum based on the output of the specific layer in response to an input of the inspection data to the machine learning model,</claim-text><claim-text>(d2) calculating a similarity between the feature spectrum and the known feature spectrum group, and</claim-text><claim-text>(d3) determining the inspection data to be non-defective when the similarity is equal to or greater than a preset threshold value, and determining the inspection data to be defective when the similarity is less than the threshold value.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the (c) includes preparing the known feature spectrum group for each of the plurality of classes,</claim-text><claim-text>the similarity calculated in the (d2) is a class-based similarity between the feature spectrum and the known feature spectrum group related to a respective one of the plurality of classes, and</claim-text><claim-text>in the (d3), the inspection data is determined to be defective when the similarity related to all of the plurality of classes is less than the preset threshold value, and the inspection data is determined to be non-defective when the similarity related to at least one class is equal to or greater than the threshold value.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the (c) includes preparing the known feature spectrum group for each of the plurality of classes, and</claim-text><claim-text>the similarity calculated in the (d2) is a similarity between the feature spectrum and the known feature spectrum group related to a determination class indicated by an output of the machine learning model in response to the inspection data.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the known feature spectrum group includes known feature spectra for all classes without distinguishing the plurality of classes.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>in the specific layer, vector neurons arranged in a plane defined by two axes including a first axis and a second axis are arranged as a plurality of channels along a third axis in a direction different from the two axes, and</claim-text><claim-text>in the specific layer, when a region that is specified by a plane position defined by a position of the first axis and a position of the second axis and includes the plurality of channels along the third axis is referred to as a partial region, for each of a plurality of partial regions included in the specific layer, the feature spectrum is obtained as one of<claim-text>(i) a first type of feature spectrum in which a plurality of element values of an output vector of each vector neuron included in the corresponding partial region are arranged over the plurality of channels along the third axis,</claim-text><claim-text>(ii) a second type of feature spectrum obtained by multiplying each element value of the first type of feature spectrum by a normalization coefficient corresponding to a vector length of the output vector, and</claim-text><claim-text>(iii) a third type of feature spectrum in which the normalization coefficient is arranged over the plurality of channels along the third axis.</claim-text></claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the (d2) includes<claim-text>obtaining a plurality of local similarities that are similarities with respect to the classes related to the plurality of partial regions of the specific layer by obtaining a local similarity that is a similarity between the feature spectrum obtained based on an output of a specific partial region of the specific layer and all known feature spectra associated with the specific layer and the classes in response to the inspection data, and</claim-text><claim-text>obtaining a class-based similarity that is a similarity on a class basis by obtaining, on a class basis, a maximum value, an average value, or a minimum value of the plurality of local similarities related to the plurality of partial regions.</claim-text></claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the (d2) includes<claim-text>obtaining a plurality of local similarities that are similarities with respect to the classes related to the plurality of partial regions of the specific layer by obtaining a local similarity that is a similarity between the feature spectrum obtained based on an output of a specific partial region of the specific layer and all known feature spectra associated with the specific partial region of the specific layer and the classes in response to the inspection data, and</claim-text><claim-text>obtaining a class-based similarity that is a similarity on a class basis by obtaining, on a class basis, a maximum value, an average value, or a minimum value of the plurality of local similarities related to the plurality of partial regions.</claim-text></claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the (d2) includes<claim-text>obtaining a class-based similarity that is a similarity on a class basis by obtaining, on a class basis, a similarity between all feature spectra obtained based on the output of the specific layer and all known feature spectra associated with the specific layer and the classes in response to the inspection data.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An information processing apparatus configured to execute quality determination processing for determining quality of inspection data using a machine learning model of a vector neural network type including a plurality of vector neuron layers, the information processing apparatus comprising:<claim-text>a memory configured to store the machine learning model; and</claim-text><claim-text>a processor configured to execute a calculation using the machine learning model, wherein</claim-text><claim-text>the processor is configured to<claim-text>(a) generate a plurality of pieces of training data by classifying a plurality of pieces of data of a non-defective product into a plurality of classes and assigning a plurality of labels distinguishing the plurality of classes to the plurality of pieces of data of the non-defective product,</claim-text><claim-text>(b) execute learning of the machine learning model using the plurality of pieces of training data,</claim-text><claim-text>(c) prepare a known feature spectrum group obtained based on an output of at least one specific layer among the plurality of vector neuron layers when the plurality of pieces of training data are input to the learned machine learning model, and</claim-text><claim-text>(d) execute quality determination processing of the inspection data using the learned machine learning model and the known feature spectrum group, and</claim-text><claim-text>the (d) includes<claim-text>(d1) calculating a feature spectrum based on the output of the specific layer in response to an input of the inspection data to the machine learning model,</claim-text><claim-text>(d2) calculating a similarity between the feature spectrum and the known feature spectrum group, and</claim-text><claim-text>(d3) determining the inspection data to be non-defective when the similarity is equal to or greater than a preset threshold value, and determining the inspection data to be defective when the similarity is less than the threshold value.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A non-transitory computer readable storage medium storing a computer program configured to execute, by a processor, quality determination processing for determining quality of inspection data using a machine learning model of a vector neural network type including a plurality of vector neuron layers, the computer program comprising:<claim-text>(a) generating a plurality of pieces of training data by classifying a plurality of pieces of data of a non-defective product into a plurality of classes and assigning a plurality of labels distinguishing the plurality of classes to the plurality of pieces of data of the non-defective product;</claim-text><claim-text>(b) executing learning of the machine learning model using the plurality of pieces of training data;</claim-text><claim-text>(c) preparing a known feature spectrum group obtained based on an output of at least one specific layer among the plurality of vector neuron layers when the plurality of pieces of training data are input to the learned machine learning model; and</claim-text><claim-text>(d) executing quality determination processing of the inspection data using the learned machine learning model and the known feature spectrum group, wherein</claim-text><claim-text>the (d) includes<claim-text>(d1) calculating a feature spectrum based on the output of the specific layer in response to an input of the inspection data to the machine learning model,</claim-text><claim-text>(d2) calculating a similarity between the feature spectrum and the known feature spectrum group, and</claim-text><claim-text>(d3) determining the inspection data to be non-defective when the similarity is equal to or greater than a preset threshold value, and determining the inspection data to be defective when the similarity is less than the threshold value.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>