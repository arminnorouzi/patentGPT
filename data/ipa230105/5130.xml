<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005131A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005131</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17853304</doc-number><date>20220629</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2022-0079388</doc-number><date>20220629</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0008</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30248</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">VISION INSPECTION SYSTEM BASED ON DEEP LEARNING AND VISION INSPECTING METHOD THEREOF</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217010</doc-number><date>20210630</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>HYUNDAI MOBIS Co., Ltd.</orgname><address><city>Seoul</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Auburn University</orgname><address><city>Auburn</city><state>AL</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Park</last-name><first-name>Jong Min</first-name><address><city>Yongin-si</city><country>KR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>HYUNDAI MOBIS Co., Ltd.</orgname><role>03</role><address><city>Seoul</city><country>KR</country></address></addressbook></assignee><assignee><addressbook><orgname>Auburn University</orgname><role>02</role><address><city>Auburn</city><state>AL</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure relates to a vision inspection system based on deep learning and a vision inspection method of. The vision inspection system based on deep learning according to the present disclosure includes a GT generation module that generates a GT for a region of interest of a car part image, a learning module that receives learning data from the GT generation module, performs learning based on deep learning, and outputs a weight file, and an interface module that detects a defect with respect to an image file received from a vision program by using the weight file, and returns a defect detection result to the vision program.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="99.91mm" wi="158.75mm" file="US20230005131A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="158.50mm" wi="71.71mm" file="US20230005131A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="202.18mm" wi="127.93mm" orientation="landscape" file="US20230005131A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="202.27mm" wi="104.73mm" orientation="landscape" file="US20230005131A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="164.76mm" wi="112.69mm" orientation="landscape" file="US20230005131A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="199.47mm" wi="136.40mm" orientation="landscape" file="US20230005131A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="106.43mm" wi="142.07mm" file="US20230005131A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="200.24mm" wi="138.68mm" orientation="landscape" file="US20230005131A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="164.76mm" wi="125.81mm" orientation="landscape" file="US20230005131A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="168.99mm" wi="125.73mm" orientation="landscape" file="US20230005131A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="200.32mm" wi="125.65mm" orientation="landscape" file="US20230005131A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="149.78mm" wi="98.72mm" file="US20230005131A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="202.27mm" wi="125.73mm" orientation="landscape" file="US20230005131A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="184.40mm" wi="143.85mm" orientation="landscape" file="US20230005131A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="198.46mm" wi="100.16mm" orientation="landscape" file="US20230005131A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="167.47mm" wi="112.52mm" orientation="landscape" file="US20230005131A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="167.47mm" wi="114.30mm" orientation="landscape" file="US20230005131A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="49.78mm" wi="118.28mm" file="US20230005131A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims priority to and the benefit of U.S. Application No. 63/217,010, filed on Jun. 30, 2021, and Korean Patent Application No. 10-2022-0079388, filed on Jun. 29, 2022, the disclosure of which is incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">1. Technical Field</heading><p id="p-0003" num="0002">The present disclosure relates to a vision inspection system based on deep learning and a vision inspection method thereof.</p><heading id="h-0004" level="1">2. Related Art</heading><p id="p-0004" num="0003">According to the prior art, final inspection of a module line is automatically performed using a vision inspection system. The vision inspection system uses pattern, blob and color tools to check the quality of an assembled module and detect a defect of the module.</p><p id="p-0005" num="0004">When a vision program determines that the module is defective, the module line has to be be stopped immediately, and an operation thereof has to be stopped until an operator identifies and handles the defect. In addition, there is a problem regarding an error detection rate in the module line such as a chassis module, and the current error detection rate for NG is counted at approximately 5%.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0006" num="0005">The present disclosure has been made in an effort to solve the problems in the related art, and an object of the present disclosure is to provide a chassis module vision inspection system based on deep learning capable of rapidly performing an entire process of acquiring a defective image from a vision program and returning an inspection completion result to the vision program within a preset time per single image, and converging a detection error rate to 0%, and a vision inspection method of the chassis module vision inspection system.</p><p id="p-0007" num="0006">Various embodiments are directed to a vision inspection system based on deep learning including: a GT generation module that generates a GT for a region of interest of a car part image; a learning module that receives learning data from the GT generation module, performs learning based on deep learning, and outputs a weight file; and an interface module that detects a defect with respect to an image file received from a vision program by using the weight file, and returns a defect detection result to the vision program.</p><p id="p-0008" num="0007">The GT generation module stores information, in which a bounding box is generated on an inspection part image, in a learning folder.</p><p id="p-0009" num="0008">OK data, NG data, a Cfg file, a pretrained weights file, and object name information are stored in the learning folder.</p><p id="p-0010" num="0009">The learning module performs learning on designated car parts by using a YOLO learning model, and outputs the weight file as a learning result.</p><p id="p-0011" num="0010">The learning module stores the weight file in a designated directory folder.</p><p id="p-0012" num="0011">The interface module is connected to the vision program by using an Ethernet TCP/IP protocol interface.</p><p id="p-0013" num="0012">The interface module receives the weight file and a configuration file from the learning module, detects a part defect by using a YOLO detection model, and controls a detection result to be displayed on a screen.</p><p id="p-0014" num="0013">Various embodiments are directed to a vision inspection method of based on deep learning, the method including: step (a) of generating a ground truth for a region of interest of a car part image; step (b) of performing learning based on deep learning by using learning data generated in step (a), and outputting a weight file; and step (c) of detecting a defect in the car part image by using the weight file, and providing a bounding box and a determination result on whether there is the defect in the car part image.</p><p id="p-0015" num="0014">Step (a) includes generating and storing GT information as a text file by using an image annotation open source.</p><p id="p-0016" num="0015">Step (b) includes performing learning on designated car parts by using a YOLO learning model, and outputting the weight file as a learning result.</p><p id="p-0017" num="0016">Step (c) includes obtaining the car part image, which is a raw image file from a vision program using Ethernet TCP/IP protocol, determining whether there is a defect in the car part image by using a YOLO detection model, controlling the determination result to be displayed on a screen, and returning the determination result to the vision program.</p><p id="p-0018" num="0017">According to the embodiments of the present disclosure, all parts of a car model may be learned, a defect may be detected through image analysis, a detection error rate may be reduced close to 0%, downtime of a module line may be dramatically shortened, and production efficiency may increase.</p><p id="p-0019" num="0018">The effects of the present disclosure are not limited to the aforementioned effects, and other effects, which are not mentioned above, may be clearly understood by those skilled in the art to which the present disclosure pertains from the following descriptions.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a vision inspection system based on deep learning according to an embodiment of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a GT generation screen according to an embodiment of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> to <figref idref="DRAWINGS">FIG. <b>3</b>D</figref> illustrate a GT generation process according to an embodiment of the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a learning screen according to an embodiment of the present disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> to <figref idref="DRAWINGS">FIG. <b>5</b>D</figref> illustrate a learning process according to an embodiment of the present disclosure.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a dashboard screen according to an embodiment of the present disclosure.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> to <figref idref="DRAWINGS">FIG. <b>7</b>D</figref> illustrate an inspection process according to an embodiment of the present disclosure.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a vision inspection method of based on deep learning according to an embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0028" num="0027">The above-described objects, and other objects, advantages and features of the present disclosure and methods of achieving the objects, advantages and features will be clear with reference to embodiments described in detail below together with the accompanying drawings.</p><p id="p-0029" num="0028">However, the present disclosure is not limited to the embodiments disclosed herein but will be implemented in various forms. The embodiments of the present disclosure are provided so that a person with ordinary skill in the art can easily understand the objects, configurations and effects of the present disclosure. The present disclosure will be defined only by the scope of the appended claims.</p><p id="p-0030" num="0029">Meanwhile, the terms used in the present specification are for explaining the embodiments, not for limiting the present disclosure. Unless particularly stated otherwise in the present specification, a singular form also includes a plural form. The terms &#x201c;comprise (include)&#x201d; and/or &#x201c;comprising (including)&#x201d; used in the specification are intended to specify the presence of the mentioned constituent elements, steps, operations, and/or elements, but do not exclude the presence or addition of one or more other constituent elements, steps, operations, and/or elements.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a vision inspection system based on deep learning according to an embodiment of the present disclosure.</p><p id="p-0032" num="0031">A GT generation module <b>100</b> includes a GT program executing unit <b>110</b> that executes a GT program. The GT program organizes data on the basis of a car model and parts.</p><p id="p-0033" num="0032">The GT program executing unit <b>110</b> outputs an OK file and an NG file, and the OK file and the NG file each include an image file and a bounding box.</p><p id="p-0034" num="0033">A learning folder management unit <b>120</b> of the GT generation module <b>100</b> stores and manages the OK file and NG file, and OK data, NG data, a Cfg file, a pretrained weights file and object names are stored in a learning folder.</p><p id="p-0035" num="0034">A learning module <b>200</b> includes a learning performing unit <b>210</b> and a learning result management unit <b>220</b>.</p><p id="p-0036" num="0035">The learning performing unit <b>210</b> performs learning by using a YOLO-v3 training model, i.e., darknet.exe, and the learning result management unit <b>220</b> stores a weight file for each car model, and transmits the learning result to a detection performing unit <b>320</b> of an interface module <b>300</b>.</p><p id="p-0037" num="0036">YOLO uses a one stage detector to pass one image through a neural network once, and thus has a significantly high detection speed. The image is divided into several grid cells, and each of the cells has a pixel size and a center point. A plurality of anchor boxes with a fixed width and height are generated in each of the cells. Parameters, which are information of each of the anchor box, include a probability that there is an object in a corresponding cell, x and y values of the corresponding cell, and width and height values of an anchor box in the corresponding cell. Information of each of the cells consists of class probabilities, i.e., a percentage for each class of a corresponding bounding box, and bounding box parameters.</p><p id="p-0038" num="0037">The order in which the results are derived is as follows. A letter box image is generated from network input, and trained data, i.e., weight, a network model and an image are inputted to GoogLeNet as an input. A fully connected layer is transformed to fit the size of a grid cell, and a class confidence score for each bounding box is calculated. Subsequently, an overlapping box is removed through non-maximum suppression (NMS), and a final box is drawn.</p><p id="p-0039" num="0038">The interface module <b>300</b> performs a dashboard function, and an image file acquisition unit <b>310</b> acquires a raw image file from a vision program by using an Ethernet TCP/IP protocol.</p><p id="p-0040" num="0039">The detection performing unit <b>320</b> reads information on the car model and parts, and calls an appropriate weight and configuration file, by using a YOLO v3-detection model.</p><p id="p-0041" num="0040">A detection result management unit <b>330</b> receives a defect detection result (output from detection) from the detection performing unit <b>320</b>, and controls the defect detection result to be displayed on a screen. At this time, an image including a bounding box and the information on the car model are provided.</p><p id="p-0042" num="0041">The detection result management unit <b>330</b> stores an irradiated image in a saving folder in real time, and transmits an output to the vision program by using the Ethernet TCP/IP protocol.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a GT generation screen according to an embodiment of the present disclosure, and <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> to <figref idref="DRAWINGS">FIG. <b>3</b>D</figref> illustrate a GT generation process according to an embodiment of the present disclosure.</p><p id="p-0044" num="0043">The GT generation module <b>100</b> provides generation and deletion functions for a model and parts, and generates a GT, i.e., a region of interest (ROI).</p><p id="p-0045" num="0044">The GT generation module <b>100</b> generates one GT for each image, and it takes 10 minutes on average to generate <b>100</b> GTs for one part.</p><p id="p-0046" num="0045">The GT generation module <b>100</b> generates and stores GT information as a text file by using an image annotation open source.</p><p id="p-0047" num="0046">The GT generation module <b>100</b> generates a box on an image in which an inspection part is present.</p><p id="p-0048" num="0047">This box is called a GT (Ground Truth), and YOLO v3 requires locational information of a training object as one of supervised learning.</p><p id="p-0049" num="0048">Through the GT generation module <b>100</b>, it is possible to add and delete a car model and parts depending on user needs, and a list of car information is managed as an INI file in the background. When an inspection part is added, a directory folder for learning is automatically generated, and a class file and a configuration file are generated at this time. A collected training image is saved in designated folders OK and NG. When dataset is prepared and a car model and parts are selected, the GT is generated through a GT program, i.e., a annotation program.</p><p id="p-0050" num="0049">Referring to <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, a form model is inputted, and the model is selected and loaded.</p><p id="p-0051" num="0050">Referring to <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, an inspection part and a class, i.e., OK or NG, is selected.</p><p id="p-0052" num="0051">Referring to <figref idref="DRAWINGS">FIG. <b>3</b>C</figref>, the GT program is loaded according to &#x201c;Start GT Program&#x201d;.</p><p id="p-0053" num="0052">Referring to <figref idref="DRAWINGS">FIG. <b>3</b>D</figref>, a GT box is generated through a mouse drag.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a learning screen according to an embodiment of the present disclosure, and <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a learning process according to an embodiment of the present disclosure.</p><p id="p-0055" num="0054">The learning module <b>200</b> performs learning on a designated part, and generates a weight file according to the learning.</p><p id="p-0056" num="0055">It takes about 30 minutes to 1 hour to perform learning for one part.</p><p id="p-0057" num="0056">The learning module <b>200</b> performs the learning by using a Darknet learning program, i.e., Darknet.exe, which is a learning program of a YOLO algorithm, as an open source.</p><p id="p-0058" num="0057">After the GT is generated, the selected inspection part is learned by the YOLO algorithm.</p><p id="p-0059" num="0058">Unlike a vision tool based on a rule, a deep learning model operates on the basis of data, and thus model performance is determined by how well the model is trained.</p><p id="p-0060" num="0059">The learning is performed using Darknet.exe, which is a program for a YOLO model created on a Darknet platform, i.e., C code.</p><p id="p-0061" num="0060">In order to execute this program, a Cfg file, object data and an image directory, i.e., .txt file, are set as parameters.</p><p id="p-0062" num="0061">When the inspection part has been previously learned, the existing weight file is automatically loaded, and when the inspection part has not been previously learned, the pretrained weights file is used.</p><p id="p-0063" num="0062">During the learning, a weight file is generated in a designated directory folder, which makes it possible to perform real-time detection.</p><p id="p-0064" num="0063">Referring to <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, a car model and inspection parts to be learned are selected.</p><p id="p-0065" num="0064">Referring to <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, &#x201c;Ready to train&#x201d; is clicked, and it is checked whether Cfg, obj.names and train.txt files are set.</p><p id="p-0066" num="0065">Referring to <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, &#x201c;Start&#x201d; is clicked, and the learning program, i.e., Darknet. exe, is executed.</p><p id="p-0067" num="0066">Referring to <figref idref="DRAWINGS">FIG. <b>5</b>D</figref>, after the learning, a weight file is automatically generated in a specific directory.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a dashboard screen according to an embodiment of the present disclosure, and <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an inspection process according to an embodiment of the present disclosure.</p><p id="p-0069" num="0068">An interface module is connected to a vision inspection program through Ethernet, and provides auto inspection and manual inspection functions.</p><p id="p-0070" num="0069">The auto inspection is performed within 2 seconds per image, the manual inspection is performed within 1 second per image, and the automatic inspection and the manual inspection use a YOLO wrapper, i.e., wrapping code from C to C#, which is an open source.</p><p id="p-0071" num="0070">After learning, a test is performed manually through the interface module, and when the performance of a learning model is confirmed to be good, the learning model is applied to the automatic inspection of a module line.</p><p id="p-0072" num="0071">A deep learning program using a TCP IP protocol is connected to the vision inspection program, and transmits and receives data.</p><p id="p-0073" num="0072">The dashboard screen displays the type of inspection, loads the weight file, and uses a YOLO detection model.</p><p id="p-0074" num="0073">During the manual inspection, a test may be performed on all images in a specific folder, and the auto inspection is used for real-time inspection with a vision program.</p><p id="p-0075" num="0074">Referring to <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, when &#x201c;MANUAL TEST&#x201d; is clicked, a manual test form is loaded.</p><p id="p-0076" num="0075">Referring to <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, a car model and parts are selected, and the manual inspection is performed.</p><p id="p-0077" num="0076">Referring to <figref idref="DRAWINGS">FIG. <b>7</b>C</figref>, &#x201c;AUTO&#x201d; and &#x201c;Connect OP.PC&#x201d; are clicked, and a log box at the bottom is checked to confirm the connection.</p><p id="p-0078" num="0077">Referring to <figref idref="DRAWINGS">FIG. <b>7</b>D</figref>, when the connection is confirmed, the program automatically inspects the image and outputs the result.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a vision inspection method of based on deep learning according to an embodiment of the present disclosure.</p><p id="p-0080" num="0079">In step S<b>810</b>, inspection based on a video is performed. The inspection is performed on all parts for each stage, and when a defective part is found, the inspection stops, and a defective part image and part information are transmitted.</p><p id="p-0081" num="0080">In step S<b>820</b>, inspection based on learning is performed. At this time, the inspection based on learning using a YOLO-v3 training model, i.e., darknet.exe, is performed. The inspection starts to be performed on an image received from a vision program, a detection result and the part information are displayed, and the detection result is transmitted to the vision program.</p><p id="p-0082" num="0081">In step S<b>830</b>, a final vision inspection result is obtained. The detection result obtained from a deep learning program is applied, the inspection is continuously performed on the other parts, and the final detection result in a module line is obtained.</p><p id="p-0083" num="0082">Meanwhile, the method according to the embodiments of the present disclosure may be implemented in a computer system or recorded in a recording medium. The computer system may include at least one processor, a memory, a user input device, a data communication bus, a user output device, and storage. Each of the above-described components performs data communication through the data communication bus.</p><p id="p-0084" num="0083">The computer system may further include a network interface coupled to a network. The processor may be a central processing unit (CPU) or a semiconductor device that processes instructions stored in a memory and/or storage.</p><p id="p-0085" num="0084">The memory and storage may include various types of volatile or non-volatile storage media. Examples of the memory may include a read only memory (ROM) and a random access memory (RAM).</p><p id="p-0086" num="0085">Accordingly, the method according to the embodiments of the present disclosure may be implemented as a computer-executable method. When the method according to the embodiments of the present disclosure is performed in a computer device, computer-readable instructions may perform the method according to the embodiments of the present disclosure.</p><p id="p-0087" num="0086">The method according to the embodiments of the present disclosure may be implemented as a computer-readable code in a computer-readable recording medium. The computer-readable recording medium includes all types of recording medium in which data that can be read by a computer system is stored. Examples of the medium may include a ROM, a RAM, a magnetic tape, a magnetic disk, a flash memory and an optical data storage. In addition, the computer-readable recording medium may be dispersed to the computer system connected through a computer communication network, and stored and executed as a readable code in a dispersed manner.</p><p id="p-0088" num="0087">According to the embodiments of the present disclosure, all parts of a car model may be learned, a defect may be detected through image analysis, a detection error rate may be reduced close to 0%, downtime of a module line may be dramatically shortened, and production efficiency may increase.</p><p id="p-0089" num="0088">While the configuration of the present disclosure has been described above in detail with reference to the accompanying drawings, the description of the configuration is for illustrative purposes only, and various modifications and alterations may of course be made by those skilled in the art without departing from the technical spirit of the present disclosure. Accordingly, the protection scope of the present disclosure should not be limited by the above-mentioned embodiments but should be determined by the appended claims.</p><p id="p-0090" num="0089">The components described in the example embodiments may be implemented by hardware components including, for example, at least one digital signal processor (DSP), a processor, a controller, an application-specific integrated circuit (ASIC), a programmable logic element, such as an FPGA, other electronic devices, or combinations thereof. At least some of the functions or the processes described in the example embodiments may be implemented by software, and the software may be recorded on a recording medium. The components, the functions, and the processes described in the example embodiments may be implemented by a combination of hardware and software.</p><p id="p-0091" num="0090">The method according to example embodiments may be embodied as a program that is executable by a computer, and may be implemented as various recording media such as a magnetic storage medium, an optical reading medium, and a digital storage medium.</p><p id="p-0092" num="0091">Various techniques described herein may be implemented as digital electronic circuitry, or as computer hardware, firmware, software, or combinations thereof. The techniques may be implemented as a computer program product, i.e., a computer program tangibly embodied in an information carrier, e.g., in a machine-readable storage device (for example, a computer-readable medium) or in a propagated signal for processing by, or to control an operation of a data processing apparatus, e.g., a programmable processor, a computer, or multiple computers. A computer program(s) may be written in any form of a programming language, including compiled or interpreted languages and may be deployed in any form including a stand-alone program or a module, a component, a subroutine, or other units suitable for use in a computing environment. A computer program may be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.</p><p id="p-0093" num="0092">Processors suitable for execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. Elements of a computer may include at least one processor to execute instructions and one or more memory devices to store instructions and data. Generally, a computer will also include or be coupled to receive data from, transfer data to, or perform both on one or more mass storage devices to store data, e.g., magnetic, magneto-optical disks, or optical disks. Examples of information carriers suitable for embodying computer program instructions and data include semiconductor memory devices, for example, magnetic media such as a hard disk, a floppy disk, and a magnetic tape, optical media such as a compact disk read only memory (CD-ROM), a digital video disk (DVD), etc. and magneto-optical media such as a floptical disk, and a read only memory (ROM), a random access memory (RAM), a flash memory, an erasable programmable ROM (EPROM), and an electrically erasable programmable ROM (EEPROM) and any other known computer readable medium. A processor and a memory may be supplemented by, or integrated into, a special purpose logic circuit.</p><p id="p-0094" num="0093">The processor may run an operating system (OS) and one or more software applications that run on the OS. The processor device also may access, store, manipulate, process, and create data in response to execution of the software. For purpose of simplicity, the description of a processor device is used as singular; however, one skilled in the art will be appreciated that a processor device may include multiple processing elements and/or multiple types of processing elements. For example, a processor device may include multiple processors or a processor and a controller. In addition, different processing configurations are possible, such as parallel processors.</p><p id="p-0095" num="0094">Also, non-transitory computer-readable media may be any available media that may be accessed by a computer, and may include both computer storage media and transmission media.</p><p id="p-0096" num="0095">The present specification includes details of a number of specific implements, but it should be understood that the details do not limit any invention or what is claimable in the specification but rather describe features of the specific example embodiment. Features described in the specification in the context of individual example embodiments may be implemented as a combination in a single example embodiment. In contrast, various features described in the specification in the context of a single example embodiment may be implemented in multiple example embodiments individually or in an appropriate sub-combination.</p><p id="p-0097" num="0096">Furthermore, the features may operate in a specific combination and may be initially described as claimed in the combination, but one or more features may be excluded from the claimed combination in some cases, and the claimed combination may be changed into a sub-combination or a modification of a sub-combination.</p><p id="p-0098" num="0097">Similarly, even though operations are described in a specific order on the drawings, it should not be understood as the operations needing to be performed in the specific order or in sequence to obtain desired results or as all the operations needing to be performed. In a specific case, multitasking and parallel processing may be advantageous. In addition, it should not be understood as requiring a separation of various apparatus components in the above described example embodiments in all example embodiments, and it should be understood that the above-described program components and apparatuses may be incorporated into a single software product or may be packaged in multiple software products.</p><p id="p-0099" num="0098">It should be understood that the example embodiments disclosed herein are merely illustrative and are not intended to limit the scope of the invention. It will be apparent to one of ordinary skill in the art that various modifications of the example embodiments may be made without departing from the spirit and scope of the claims and their equivalents.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A vision inspection system based on deep learning, the vision inspection system comprising:<claim-text>a ground truth (GT) generation module that generates a GT box for a region of interest on a car part image;</claim-text><claim-text>a learning module that receives learning data from the GT generation module, performs learning based on deep learning, and outputs a weight file; and</claim-text><claim-text>an interface module that detects a part defect with respect to an image file received from a vision program by using the weight file, and returns a part defect detection result to the vision program.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The vision inspection system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the GT generation module stores information, in which a bounding box is generated on an inspection part image, in a learning folder.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The vision inspection system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein OK data, NG data, a configuration (Cfg) file, a pretrained weights file, and object name information are stored in the learning folder.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The vision inspection system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the learning module performs learning on designated car parts by using a YOLO training model, and outputs the weight file as a learning result.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The vision inspection system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the learning module stores the weight file in a designated directory folder.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The vision inspection system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interface module is connected to the vision program by using an Ethernet TCP/IP protocol interface.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The vision inspection system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interface module receives the weight file and a configuration file from the learning module, detects the part defect by using a YOLO detection model, and controls the part defect detection result to be displayed on a screen.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A vision inspection method of based on deep learning, the method comprising:<claim-text>step (a) of generating a ground truth box for a region of interest on a car part image;</claim-text><claim-text>step (b) of performing learning based on deep learning by using learning data generated in step (a), and outputting a weight file; and</claim-text><claim-text>step (c) of detecting a defect in the car part image by using the weight file, and providing a bounding box and a determination result on whether there is the defect in the car part image.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein step (a) includes generating and storing GT information as a text file by using an image annotation open source.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein step (b) includes performing learning on designated car parts by using a YOLO learning model, and outputting the weight file as a learning result.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein step (c) includes obtaining the car part image, which is a raw image file from a vision program using Ethernet TCP/IP protocol, determining whether there is the defect in the car part image by using a YOLO detection model, controlling the determination result to be displayed on a screen, and returning the determination result to the vision program.</claim-text></claim></claims></us-patent-application>