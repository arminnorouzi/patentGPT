<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000338A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000338</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942132</doc-number><date>20220910</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="regional"><country>EP</country><doc-number>18 214 689.4</doc-number><date>20181220</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>0025</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>102</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">Detection of Pathologies in Ocular Images</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16721902</doc-number><date>20191219</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11503994</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17942132</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>OPTOS PLC</orgname><address><city>Dunfermline</city><country>GB</country></address></addressbook><residence><country>GB</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>van Hemert</last-name><first-name>Jano</first-name><address><city>Dunfermline</city><country>GB</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>OPTOS PLC</orgname><role>03</role><address><city>Dunfermline</city><country>GB</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A computer-implemented method of searching for a region indicative of a pathology in an image of a portion of an eye acquired by an ocular imaging system, the method comprising: receiving image data defining the image; searching for the region in the image by processing the received image data using a learning algorithm; and in case a region in the image that is indicative of the pathology is found: determining a location of the region in the image; generating an instruction for an eye measurement apparatus to perform a measurement on the portion of the eye to generate measurement data, using a reference point based on the determined location for setting a location of the measurement on the portion of the eye; and receiving the measurement data from the eye measurement apparatus.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="192.87mm" wi="140.29mm" file="US20230000338A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="194.23mm" wi="150.54mm" file="US20230000338A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="190.75mm" wi="164.00mm" file="US20230000338A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="215.98mm" wi="142.32mm" file="US20230000338A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="202.78mm" wi="125.65mm" file="US20230000338A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="194.06mm" wi="116.50mm" file="US20230000338A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="193.63mm" wi="163.75mm" file="US20230000338A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="180.76mm" wi="137.75mm" file="US20230000338A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="172.64mm" wi="139.87mm" file="US20230000338A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="203.12mm" wi="150.54mm" file="US20230000338A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="206.84mm" wi="142.41mm" file="US20230000338A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">Example aspects herein generally relate to the field of image processing and, more particularly, to the processing of an ocular image to facilitate identification of the presence and/or location of a pathology in an imaged portion of an eye.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">An ocular image of a portion of a subject's eye acquired by an ocular imaging system may have one or more regions containing some indication of a pathology that would not be present in a comparable image of a healthy eye, and the presence of such region(s) may be indicative of a disease or an otherwise unhealthy state of the subject. By way of an example, a fluorescein angiogram of the subject's retina may contain one or more regions where blood vessel leakage appears to have occurred, which can be a sign of macular oedema. Ocular images require an experienced ophthalmologist to acquire and assess. However, existing techniques are time-consuming for the ophthalmologist and/or the patient, and the assessment performed may be prone to error.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">The present inventors have devised, in accordance with a first example aspect herein, a computer-implemented method of searching for a region indicative of a pathology in an image of a portion of an eye acquired by an ocular imaging system. The method comprises receiving image data defining the image, and searching for the region in the image by processing the received image data using a learning algorithm trained on image data defining images of the portion of healthy eyes, and image data defining images of the portion of unhealthy eyes each having at least one region that is indicative of the pathology. In case a region in the image that is indicative of the pathology is found in the searching, a location of the region in the image is determined, an instruction is generated for an eye measurement apparatus to perform a measurement on the portion of the eye to generate measurement data, using a reference point based on the determined location for setting a location of the measurement on the portion of the eye, and the measurement data is received from the eye measurement apparatus.</p><p id="p-0005" num="0004">The present inventors have also devised, in accordance with a second example aspect herein, a computer-implemented method of searching for the presence of a pathology in an image of a portion of an eye acquired by an ocular imaging system. The method comprises receiving image data defining the image, and searching for the presence of at least one of a plurality of different types of pathology in the image by processing the received image data using a learning algorithm trained on image data defining images of healthy eyes, and images of unhealthy eyes each having a respective one of the different types of pathology. The method further comprises, in case at least one of the plurality of different types of pathology is found to be present in the image, processes of: selecting, for each of at least one type of pathology found to be present in the image, a respective one of a plurality of different types of measurement modality which is to be used to perform a measurement on the portion of the eye; generating, for each of the at least one type of pathology found to be present in the image, a respective instruction for an eye measurement apparatus of the respective selected measurement modality to perform the measurement on the portion of the eye; and receiving measurement data of the measurement performed by the eye measurement apparatus of each selected measurement modality.</p><p id="p-0006" num="0005">The present inventors have further devised, in accordance with a third example aspect herein, a computer program which, when executed by a computer, causes the computer to perform the method according to the first or the second example aspects herein.</p><p id="p-0007" num="0006">The present inventors have further devised, in accordance with a fourth example aspect herein, a non-transitory computer-readable storage medium storing the computer program according to the third example aspect herein.</p><p id="p-0008" num="0007">The present inventors have further devised, in accordance with a fifth example aspect herein, a signal carrying the computer program according to the third example aspect herein.</p><p id="p-0009" num="0008">The present inventors have further devised, in accordance with a sixth example aspect herein, an apparatus for searching for a region indicative of a pathology in an image of a portion of an eye acquired by an ocular imaging system. The apparatus comprises a receiver module configured to receive image data defining the image. The apparatus further comprises a search module configured to search for the region in the image by processing the received image data using a learning algorithm trained on image data defining images of the portion of healthy eyes, and image data defining images of the portion of unhealthy eyes each having at least one region that is indicative of the pathology. The apparatus further comprises an instruction generating module configured to perform, in response to a region in the image that is indicative of the pathology being found by the search module, processes of: determining a location of the region in the image; and generating an instruction for an eye measurement apparatus to perform a measurement on the portion of the eye to generate measurement data, using a reference point based on the determined location for setting a location of the measurement on the portion of the eye. The receiver module is further configured to receive the measurement data from the eye measurement apparatus.</p><p id="p-0010" num="0009">The present inventors have further devised, in accordance with a seventh example aspect herein, an apparatus for searching for the presence of a pathology in an image of a portion of an eye acquired by an ocular imaging system. The apparatus comprises a receiver module configured to receive image data defining the image and a search module configured to search for the presence of at least one of a plurality of different types of pathology in the image by processing the received image data using a learning algorithm trained on image data defining images of healthy eyes, and images of unhealthy eyes each having a respective one of the different types of pathology. The apparatus further comprises an instruction generating module configured to perform, in response to at least one of the plurality of different types of pathology being found to be present in the image by the search module, processes of: selecting, for each of at least one type of pathology found to be present in the image, a respective one of a plurality of different types of measurement modality which is to be used to perform a measurement on the portion of the eye; and generating, for each of the at least one type of pathology found to be present in the image, a respective instruction for an eye measurement apparatus of the respective selected measurement modality to perform the measurement on the portion of the eye. The receiver module is further configured to receive measurement data of the measurement performed by the eye measurement apparatus of each selected measurement modality.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010">Embodiments of the invention will now be explained in detail, by way of non-limiting example only, with reference to the accompanying figures described below. Like reference numerals appearing in different ones of the figures can denote identical or functionally similar elements, unless indicated otherwise.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic illustration of an apparatus for searching for a region indicative of a pathology in an ocular image, in accordance with a first example embodiment herein.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating an example signal processing hardware configuration of the apparatus of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, according to an example embodiment herein.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow diagram illustrating a process by which the apparatus of <figref idref="DRAWINGS">FIG. <b>1</b></figref> searches for a region indicative of a pathology in an ocular image, in accordance with the first example embodiment herein.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b>(<i>a</i>)</figref> is a schematic illustration of an image of a portion of the eye acquired by an ocular imaging system.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b>(<i>b</i>)</figref> is an annotated version of the image shown in <figref idref="DRAWINGS">FIG. <b>4</b>(<i>a</i>)</figref>, showing a region which is indicative of a pathology that has been found, in accordance with an example aspect herein.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic illustrating how a learning algorithm may be trained on image data defining images of the portion of healthy eyes, and image data defining images of the portion of unhealthy eyes each having at least one region that is indicative of a pathology, in accordance with a first example aspect herein.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic illustrating how a learning algorithm may be trained on image data defining images of a portion of healthy eyes, and image data defining images of the portion of unhealthy eyes each having at least one region that is indicative of a respective one of a plurality of different types of pathology, in accordance with a second example aspect herein.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic illustration of a convolutional neural network comprising artificial neurons in an input layer, a hidden layer, and an output layer.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow diagram illustrating a process by which a search module of the apparatus of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may search for a region indicative of pathology in the image, according to an example embodiment herein.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic illustration of an apparatus for searching for the presence of a pathology in an ocular image, in accordance with an example embodiment herein.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flow diagram illustrating a process by which the apparatus of <figref idref="DRAWINGS">FIG. <b>9</b></figref> searches for the presence of a pathology in an ocular image, in accordance with an example embodiment herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading><p id="p-0023" num="0022">In certain example embodiments described herein, an ocular image is processed to automatically detect a pathology in the ocular image using the image processing techniques hereinafter described, and then a further measurement on the eye is automatically instructed in order to acquire supplementary data that can allow the presence of the pathology to be determined (and/or to allow the pathology to be characterised) with a higher degree of confidence. For example, the eye may be imaged using a different imaging modality (for example, optical coherence tomography, OCT), or its functional response to light stimulation measured. By way of further example, in the case of a fluorescein angiogram in which the image processing detects a possible indication that blood vessel leakage has occurred in a region of the retina, an OCT scan of the region may be instructed, and thickness or volume measurements may be obtained from the data of the OCT scan in order to confirm the presence of the macular oedema and, where present, optionally diagnose its severity. Embodiments can thus facilitate fast and reliable detection of pathologies for early detection of disease, their advantages being most pronounced where the ocular image is a wide-field or ultra-wide-field image covering a large part of the retina or other portion of the eye.</p><p id="p-0024" num="0023">Embodiments of the present invention will now be described in detail with reference to the accompanying drawings.</p><heading id="h-0006" level="1">Embodiment 1</heading><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic illustration of an apparatus <b>100</b> according to a first example embodiment, for searching for a region indicative of a pathology in an image of a portion of an eye acquired by an ocular imaging system (shown at <b>520</b> in the schematic illustration of <figref idref="DRAWINGS">FIG. <b>5</b></figref>). The apparatus <b>100</b> comprises a receiver module <b>110</b> configured to receive image data defining an image produced by a ocular imaging system <b>520</b>, and a search module <b>120</b> configured to automatically search for at least one region indicative of a pathology in the image, by processing the received image data using a learning algorithm which is described in detail below. The apparatus <b>100</b> further comprises an instruction generating module <b>130</b> configured to automatically perform, in response to a region in the image that is indicative of the pathology being found by the search module <b>120</b>, processes of: determining a location of the region in the image and generating an instruction for an eye measurement apparatus to perform a measurement on the portion of the eye to generate measurement data, using a reference point based on the determined location for setting a location of the measurement on the portion of the eye. In cases where two or more such regions are found by the search module <b>120</b>, these processes may be performed for each of the regions. The receiver module <b>110</b> is further configured to receive the measurement data from the eye measurement apparatus. Optionally, the apparatus <b>100</b> may, as in the present illustrated embodiment, comprise a display control signal generator <b>140</b> (this optional component being shown by the dashed lines in <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0026" num="0025">A region indicative of a pathology may be a region of the image defined by the received image data which contains image features indicative of a lesion, structural damage or any other abnormality that would not be present in a similarly-acquired image of a healthy eye.</p><p id="p-0027" num="0026">The apparatus <b>100</b> may be configured to search for a region indicative of any pathology that may occur in the eye, which may include (but is not limited to), for example, glaucoma, moderate diabetic retinopathy, severe diabetic retinopathy, tumour, drusen, oedema and atrophy.</p><p id="p-0028" num="0027">In the present example embodiment, the receiver module <b>110</b> is configured to receive image data defining an image of a portion of a retina of the eye, which has been generated by the ocular imaging system <b>520</b>. However, in other example embodiments, the received image data may define an image of a portion of the eye other than the retina, for example a portion of the anterior segment of the eye, or a portion of the posterior segment of the eye. Furthermore, the received image data may, as in the present example embodiment, define a two-dimensional image, or it may alternatively define a three-dimensional image of the imaged portion of the eye. The received image data may be provided in any suitable format (whether compressed or uncompressed) known to those skilled in the art. The image data received by the receiver module <b>110</b> is of a first imaging modality (discussed in more detail below), and represents a result of imaging the retina of the eye using appropriately selected values for imaging parameters, which may include imaging resolution, aperture size, and wavelength.</p><p id="p-0029" num="0028">The ocular imaging system <b>520</b> may be any ocular imaging system that is suitable for imaging the retina (or other selected portion) of the eye. The ocular imaging system <b>520</b> may, for example, be a fundus camera, or a type of scanning imaging system. By way of example, the ocular imaging system <b>520</b> of the present example embodiment is a scanning imaging system in the exemplary form of a scanning laser ophthalmoscope (SLO), which is configured to acquire images of the retina of a subject's eye. The SLO of the present example embodiment is configured to capture autofluorescence (AF) images (it may be configured to capture Red-Green (RG) reflectance images or images from other fluorescence modes), although it may alternatively or additionally be configured to acquire one or more other types of images. The SLO may, for example, be an ultra-wide field SLO (UWF-SLO) capable of generating an ultra-wide field image of up to 80% of a retinal surface. Alternatively, the ocular imaging system <b>520</b> may be of another imaging modality, for example an optical coherence tomography (OCT) scanner, in which case the image processing techniques described herein are applicable to the tomographic images acquired by the OCT scanner. As a further alternative, the ocular imaging system <b>520</b> may be a combined SLO-OCT scanner, in which case the image processing techniques described herein are applicable to both the SLO retinal scans and the OCT scans acquired by the combined SLO-OCT scanner.</p><p id="p-0030" num="0029">The receiver module <b>110</b> is configured to receive image data defining the image acquired by the ocular imaging system <b>520</b> by any suitable means known to those versed in the art. For example, the receiver module <b>110</b> may receive the image data from the ocular imaging system <b>520</b> via a direct communication link (which may be provided by any suitable wired or wireless connection, e.g. a Universal Serial Bus (USB) or a Bluetooth&#x2122; connection), or an indirect communication link (which may be provided by a network comprising a Local Area Network (LAN), a Wide Area Network (WAN) and/or the Internet). Furthermore, the image data may be received by the receiver module <b>110</b> receiving (e.g. by reading from a storage medium such as a CD or hard disk, or receiving via a network such as the Internet) such image data after it has been acquired by the ocular imaging system.</p><p id="p-0031" num="0030">Furthermore, the image data may be received by the receiver module <b>110</b> (and may furthermore subsequently be processed to search for a region indicative of a pathology in an image of a portion of an eye, as described below) as this image data is being generated by the ocular imaging system, i.e. the image data may be acquired &#x201c;on the fly&#x201d;, without waiting for the ocular imaging system <b>520</b> to finish generating all of the image data that forms the image of the portion of the retina. However, in the present example embodiment, and for the purposes of this description, the receiver module <b>110</b> is configured to receive all of the image data defining the image of the portion of the eye before the search module <b>120</b> begins to process this image data.</p><p id="p-0032" num="0031">In embodiments like the present illustrated embodiment, where the apparatus <b>100</b> comprises a display control signal generator <b>140</b>, the display control signal generator <b>140</b> may be arranged to generate display control signals for controlling a display device (as shown at <b>215</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>), such as an LCD screen or other type of visual display unit, to display both the location of the region indicative of the pathology in the image of the portion of the eye, and a representation of the received measurement data.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic illustration of a programmable signal processing hardware <b>200</b>, which may, as in the present example embodiment, be configured to function as the apparatus <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The programmable signal processing hardware <b>200</b> comprises a communication interface (I/F) <b>210</b> for receiving the image data described above, generating the instruction for the eye measurement apparatus <b>300</b> to perform the measurement on the portion of the eye to generate the measurement data, receiving the measurement data from the eye measurement apparatus <b>300</b>, and, optionally, for outputting display control signals for controlling the display device <b>215</b> to display both the image of the portion of the eye and a representation of the measurement data. The signal processing apparatus <b>200</b> further comprises a processor (e.g. a Central Processing Unit, CPU, or Graphics Processing Unit, GPU) <b>220</b>, a working memory <b>230</b> (e.g. a random access memory) and an instruction store <b>240</b> storing a computer program comprising the computer-readable instructions which, when executed by the processor <b>220</b>, cause the processor <b>220</b> to perform various functions including those of the search module <b>120</b>, instruction generating module <b>130</b> and, optionally, the display control signal generator <b>140</b> described above. The instruction store <b>240</b> may comprise a ROM (e.g. in the form of an electrically-erasable programmable read-only memory (EEPROM) or flash memory) which is pre-loaded with the computer-readable instructions. Alternatively, the instruction store <b>240</b> may comprise a RAM or similar type of memory, and the computer-readable instructions of the computer program can be input thereto from a computer program product, such as a non-transitory, computer-readable storage medium <b>250</b> in the form of a CD-ROM, DVD-ROM, etc. or a computer-readable signal <b>260</b> carrying the computer-readable instructions. In any case, the computer program, when executed by the processor, causes the processor to execute at least one of the methods of searching for a region indicative of a pathology, or the presence of a pathology, in an image of a portion of an eye acquired by an ocular imaging system described herein. It should be noted, however, that the apparatus <b>100</b> may alternatively be implemented in non-programmable hardware, such as an application-specific integrated circuit (ASIC).</p><p id="p-0034" num="0033">In the present example embodiment, a combination <b>270</b> of the hardware components shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, comprising the processor <b>220</b>, the working memory <b>230</b> and the instruction store <b>240</b>, is configured to perform functions of the search module <b>120</b> and instruction generating module <b>130</b>, which functions will now be described in detail below. In embodiments like the present illustrated embodiment, where the apparatus <b>100</b> comprises a display control signal generator <b>140</b>, the functionality of this optional component also be provided by the combination <b>270</b> of the hardware components, together with the communication I/F <b>210</b>.</p><p id="p-0035" num="0034">As will become more apparent from the following description of the operations performed by the apparatus <b>100</b> of the present example embodiment, the apparatus <b>100</b> automatically processes image data defining an image of the portion of an eye acquired by an ocular imaging system <b>520</b> to find, and record the location in the image of, each region that is indicative of pathology, and generates a corresponding instruction for an eye measurement apparatus <b>300</b> to perform a measurement on the portion of the eye to generate measurement data, using a reference point based on the recorded location of the respective region for setting a respective location of the measurement on the portion of the eye. The apparatus <b>100</b> thus acquires both the location of each image region suspected to contain a pathology, and additional measurement data related to the respective region. These different but complementary kinds of information that are automatically acquired by the apparatus <b>100</b> can be presented together to the medical practitioner for inspection, and may thus allow any pathology-containing regions of the received ocular image to be identified more quickly and with a greater degree of confidence than by inspecting the ocular image alone. For example, in example embodiments where the recorded location of the region in the image and the representation of the received measurement data are both displayed on the display device <b>215</b> (preferably overlaid on one another), a viewer of the display device <b>215</b> may be able to easily recognize whether, and where, such region is to be found in the image. It may not be as easy to do so from viewing either the recorded location of the region in the image, or the representation of the received measurement data, when either type of information is displayed alone on the display device <b>215</b>. Similar advantages may occur in alternative example embodiments, where the recorded location of the region in the image and the received measurement data are processed automatically to identify regions of interest.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow diagram illustrating a process by which the apparatus <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> searches for a region that is indicative of a pathology in an image of a portion of an eye acquired by an ocular imaging system <b>520</b>, in accordance with the first example embodiment herein.</p><p id="p-0037" num="0036">In process S<b>10</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the receiver module <b>110</b> receives image data of a first imaging modality, which has been acquired by the ocular imaging system <b>520</b>. The imaging modality of the ocular imaging system may, for example, take one of the many different forms known to those versed in the art, including OCT, colour fundus photography, fluorescein angiography (FA), indocyanine green angiography (ICG) and autofluorescence (AF), among others. The received image data defines the image of a portion of the eye (in this example embodiment, the retina) acquired by the ocular imaging system <b>520</b>. <figref idref="DRAWINGS">FIG. <b>4</b>(<i>a</i>)</figref> is a schematic illustration of an image <b>400</b> of the portion of the retina defined by the received image data that has been acquired by an ocular imaging system <b>520</b>.</p><p id="p-0038" num="0037">In process S<b>12</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the search module <b>120</b> searches for the region (<b>410</b> in <figref idref="DRAWINGS">FIG. <b>4</b>(<i>a</i>)</figref>) indicative of a single type of pathology in the image <b>400</b> by processing the received image data using a learning algorithm which has been trained on image data defining images of the retina of healthy eyes, and image data defining images of the retina of unhealthy eyes each having at least one region that is indicative of the pathology. The location in the image <b>400</b> of each region <b>410</b> found in the search is recorded by the search module <b>120</b>, for example in the working memory <b>230</b> in case the apparatus <b>100</b> is implemented in the programmable signal processing hardware <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic representation illustrating how a learning algorithm may be trained on image data <b>502</b> defining images of healthy eyes, and image data <b>501</b> defining images of unhealthy eyes. The learning algorithm <b>530</b> is configured to learn from, and make predictions based on, input data by building a model <b>540</b> from an example training set <b>500</b> of input data, comprising the image data <b>502</b> defining images of the retina of healthy eyes, and the image data <b>501</b> defining images of the retina of unhealthy eyes. By way of example, image data <b>501</b> defines images of the portion of unhealthy eyes, each of which has a region indicative of severe diabetic retinopathy. The images defined by the image data <b>501</b> in the example training set <b>500</b> may be collected by acquiring images of the retinas of multiple subjects. More generally, each image is of the same portion of the eye (or of substantially the same portion of the eye <b>510</b> or of a part of the eye containing the same portion) as the image defined by the received image data. Furthermore, each image defined by the image data in the example training set <b>500</b> is acquired by the ocular imaging system <b>520</b> or by the same type of ocular imaging system and operating in the same imaging modality. By way of example, image data <b>501</b> defines images of the portion of unhealthy eyes, each of which has a region indicative of severe diabetic retinopathy.</p><p id="p-0040" num="0039">In embodiments where the learning algorithm <b>530</b> is a supervised learning algorithm (such as a neural network, a support vector machine or an evolutionary algorithm, for example), each example image in the example training set <b>500</b> is a pair consisting of input image data defining an image of the portion of the eye and a desired output value indicating whether the image is of a portion of a &#x201c;healthy&#x201d; or &#x201c;unhealthy&#x201d; eye. The supervised learning algorithm <b>530</b> analyses the image data in the example training set <b>500</b> and produces a model <b>540</b>, which can be used to classify new unseen image data defining an image of the portion of the eye as &#x201c;healthy&#x201d; or &#x201c;unhealthy&#x201d;. As the learning algorithm <b>530</b> is trained on image data <b>501</b> defining images of unhealthy eyes having a single type of pathology only, the model <b>540</b> cannot distinguish between pathologies. It can only determine whether a region indicative of the pathology, for example, severe diabetic retinopathy, is present or not.</p><p id="p-0041" num="0040">In process S<b>12</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the search module <b>120</b> may, in a modification of the present example embodiment, search for the region <b>410</b> in the image by searching for a region <b>410</b> in the image that is indicative not of a single type of pathology, but of one of a plurality of different types of pathology, by processing the received image data using the learning algorithm. Therefore, in the modification of the present example embodiment, the learning algorithm may be trained on image data defining images of the portion of unhealthy eyes each having a respective one of the different types of pathology.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic illustrating how the modified learning algorithm <b>630</b> may be trained on image data <b>604</b> defining images of the retina of healthy eyes, and image data <b>601</b>, <b>602</b>, <b>603</b> defining images of the retina of unhealthy eyes, each having at least one region that is indicative of a respective one of a plurality of different types of pathology. By way of example, image data <b>601</b> defines images of the portion of unhealthy eyes which have glaucoma, image data <b>602</b> defines images of the portion of unhealthy eyes which have moderate diabetic retinopathy, and image data <b>603</b> defines images of the portion of unhealthy eyes which have severe diabetic retinopathy.</p><p id="p-0043" num="0042">The learning algorithm <b>630</b> is configured to learn from, and make predictions based on, input data by building a model <b>640</b> from an example training set <b>600</b> of input data, comprising the image data <b>604</b> defining images of the retina of healthy eyes, and the image data <b>601</b>, <b>602</b>, <b>603</b> defining images of the retina of unhealthy eyes. The images defined by the image data <b>601</b> to <b>604</b> in the example training set <b>600</b> may be collected by acquiring images of the retinas of multiple subjects. More generally, each image is of the same portion of the eye (or of substantially the same portion of the eye <b>510</b> or of a part of the eye containing the same portion) as the image defined by the received image data. Furthermore, each image defined by the image data <b>610</b> to <b>604</b> in the example training set <b>600</b> is acquired by the ocular imaging system <b>520</b> or by the same type of ocular imaging system and operating in the same imaging modality.</p><p id="p-0044" num="0043">The learning algorithm <b>630</b> may be a supervised learning algorithm. Thus, each example image of the portion of the eye <b>510</b> in the example training set <b>600</b> is associated with an indicator indicating whether that image is of a portion of a &#x201c;healthy&#x201d; or an &#x201c;unhealthy&#x201d; eye and, in cases where the image is of a portion of a &#x201c;unhealthy&#x201d; eye, also a second indicator indicating which one of the plurality of pathologies is present in the image. The supervised learning algorithm analyses the image data in the example training set <b>600</b> and produces a model <b>640</b>, which can be used to classify new (previously unseen) image data defining an image of the portion of the eye as one of, for example: &#x201c;healthy&#x201d;; &#x201c;unhealthy&#x2014;glaucoma&#x201d;; &#x201c;unhealthy&#x2014;moderate diabetic retinopathy&#x201d;; and &#x201c;unhealthy&#x2014;severe diabetic retinopathy&#x201d;.</p><p id="p-0045" num="0044">In will be evident to one skilled in the art that the apparatus <b>100</b> may be adapted to classify additional pathologies by expanding the training set <b>600</b> to include, for each of the additional pathologies, image data defining images of the retina (or other portion) of unhealthy eyes having that pathology, and associated indicators as described above. For example, the training set <b>600</b> may be expanded to include image data defining images of the portion of unhealthy eyes having a tumour and/or image data defining images of the portion of unhealthy eyes having oedema. Furthermore, any of the image data <b>601</b>, <b>602</b>, <b>603</b> defining images of the portion of unhealthy eyes may be removed or replaced with image data defining images of the portion of unhealthy eyes having a different pathology. A revised version of the model <b>640</b> may then be produced based on the modified training set <b>600</b>.</p><p id="p-0046" num="0045">The supervised learning algorithm <b>630</b> may, as in the present example embodiment, be a neural network. Neural networks automatically generate identifying characteristics by processing the input data, such as the image data in the example training set <b>600</b>, without any prior knowledge.</p><p id="p-0047" num="0046">The neural network may, as in the present example embodiment, be a convolutional neural network. Convolutional neural networks are particularly suitable to image and video recognition tasks.</p><p id="p-0048" num="0047">As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in general, a convolution neural network consists of an input layer and an output layer, as well as multiple hidden layers. Each of the layers is composed of a plurality of artificial neurons (labelled A to F in <figref idref="DRAWINGS">FIG. <b>7</b></figref>), and each layer may perform different kinds of transformations on their inputs. Each artificial neuron may be connected to multiple artificial neurons in adjacent layers. The output of each artificial neuron is computed by some non-linear function of the sum of its inputs. Artificial neurons and the connections therebetween typically have respective weights (WAD, WAE, etc. in <figref idref="DRAWINGS">FIG. <b>7</b></figref>) which determined the strength of the signal at a given connection. These weights are adjusted as learning proceeds, thereby adjusting the output of the convolutional neural network. Signals travel from the first layer (the input layer), to the last layer (the output layer), and may traverse the layers multiple times.</p><p id="p-0049" num="0048">The output of the neural network may be viewed as a probability of the input image data containing identifying characteristics of the pathology and the classification may, as in the present example embodiment, comprise determining whether the output of the trained model <b>640</b> exceeds a predetermined threshold. The predetermined threshold may represent an acceptably low probability that the input image data contains identifying characteristics of a pathology and, therefore, a high probability that the eye of the subject is healthy.</p><p id="p-0050" num="0049">In the case where the learning algorithm is a neural network, as in the present example embodiment, the search module <b>120</b> may be configured, as part of process S<b>12</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, to search for the region <b>410</b> indicative of the pathology for in the image by deconstructing the neural network.</p><p id="p-0051" num="0050">When image data defining an image <b>410</b> of the portion of the eye is input to the trained model <b>640</b>, the trained model <b>640</b> classifies this image as &#x201c;healthy&#x201d;, or as &#x201c;unhealthy&#x201d; and having a particular pathology (that is, the model <b>640</b> determines whether the input image data contains identifying characteristics of a pathology or not). Conventional convolutional neural networks do not output an explanation of this classification. Accordingly, the convolutional neural network can be deconstructed, that is, processed in order to determine which input variables of the input image data (that is, pixels of the image <b>400</b>) were relevant to the output of the neural network. The input variables most relevant to the classification as &#x201c;unhealthy&#x201d; and having a certain pathology of the image <b>400</b> defined by the received image data correspond to the region <b>410</b> (or regions) that are indicative of the pathology.</p><p id="p-0052" num="0051">In case the trained model <b>640</b> classified the image <b>400</b> defined by the received image data as &#x201c;healthy&#x201d; (that is, not containing any regions indicative of a pathology), the search module <b>120</b> may, as in the present example embodiment, be configured not to deconstruct the neural network.</p><p id="p-0053" num="0052">In embodiments like the present illustrated embodiment, where the neural network is a convolutional neural network, the search module <b>120</b> may, as in the present example embodiment, be configured to deconstruct the convolutional neural network by performing the processes illustrated in the flow diagram of <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0054" num="0053">In S<b>122</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the search module <b>120</b> performs, for each of a plurality of different sections of the image <b>400</b>, a process of masking the section of the image to generate a masked image.</p><p id="p-0055" num="0054">In S<b>124</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the search module <b>120</b> performs, for each of a plurality of different sections of the image <b>400</b>, a process of searching for the region in the masked image by processing image data defining the masked image using the learning algorithm.</p><p id="p-0056" num="0055">In S<b>126</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the search module <b>120</b> performs, for each of a plurality of different sections of the image <b>400</b>, a process of determining a difference between a first result which is a result of the search performed using the image data defining the masked image, and a second result which is a result of a search performed using the received image data (that is, the image data as received, in which the section of the image is not masked).</p><p id="p-0057" num="0056">In S<b>128</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the search module <b>120</b> determines, as the location of the region <b>410</b> in the image <b>400</b>, a location of a section for which the determined difference is largest.</p><p id="p-0058" num="0057">Alternatively, where the neural network is a convolutional neural network, the search module <b>120</b> may be configured to deconstruct the convolutional neural network by:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0058">(i) determining a relevance of each input variable of the neural network to an output of the neural network by applying a Taylor decomposition to each layer of the neural network, from a top layer of the neural network to an input layer of the neural network; and</li>        <li id="ul0002-0002" num="0059">(ii) determining the location to be recorded based on at least one section of the received image data corresponding to the most relevant input variables of the neural network.</li>    </ul>    </li></ul></p><p id="p-0059" num="0060">Taylor decomposition is a method of explaining individual neural network predictions by decomposing the output of the neural network on its input variables. This method views the image <b>400</b> as a set of pixel values x={x<sub>p</sub>}, where p denotes a particular pixel. A function &#x192;(x) quantifies the presence of a certain type of object, here a region <b>410</b> indicative of a pathology, in the image <b>400</b>. A function value &#x192;(x)=0 indicates an absence of it. On the other hand, a function value &#x192;(x)&#x3e;0 expresses its presence with a certain degree of certainty, or in a certain amount. The Taylor decomposition method assigns each pixel p in the image a relevance score R<sub>p</sub>(x), that indicates for an image x to what extent the pixel p contributes to explaining the classification decision &#x192;(x). The relevance of each pixel can be stored in a heat map denoted by R(x)={R<sub>p</sub>(x)} of same dimensions as x and can be visualized as an image. Accordingly, the location of the region can be determined as the location of the most relevant pixels in the heat map.</p><p id="p-0060" num="0061">As a further alternative, where the neural network is a convolutional neural network, the search module <b>120</b> may be configured to deconstruct the convolutional neural network by determining a deconvolution of the convolutional neural network. In particular, the convolution neural network may be deconstructed by training a multi-layer deconvolution network which takes, as an input, the output of the convolutional neural network and provides, as an output, a probability map of the same size as the image input to the convolutional neural network which indicates the probability of each pixel belonging to one of the predefined classes. Accordingly, the location of the region may be determined as the location of the pixels having the highest probability of belonging to one of the predetermined classes (that is, the highest probability of having one of the plurality of different types of pathology).</p><p id="p-0061" num="0062">Therefore, the search module <b>120</b> may, as in the present example embodiment, be configured to search for the region <b>410</b> indicative of a pathology in the image by identifying, as the region <b>410</b> indicative of a pathology, the pixels of the image <b>400</b> defined by the received image data that are most relevant to the output of the neural network.</p><p id="p-0062" num="0063">Referring again to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in process S<b>13</b>, the search module <b>120</b> determines whether a region in the image that is indicative of the pathology has been found as a result of the search performed in process S<b>12</b>. If a region in the image that is indicative of the pathology has been found as a result of the search performed in process S<b>12</b>, the process proceeds to S<b>14</b>, otherwise the process ends.</p><p id="p-0063" num="0064">In process S<b>14</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the instruction generating module <b>130</b> determines the location in the image of the region <b>410</b> which has been found by the search module <b>120</b>. The location may, for example, be recorded by the search module <b>120</b> in the working memory <b>230</b> of the hardware configuration <b>200</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and read from the working memory by the instruction generating module <b>130</b> in S<b>14</b>.</p><p id="p-0064" num="0065">In embodiments like the present example embodiment, where process results of the apparatus <b>100</b> are displayed on a display device <b>215</b> as herein described, the determined location may, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>(<i>a</i>)</figref>, serve to center or otherwise position an overlay graphical element, such as the boundary box <b>440</b> illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>(<i>b</i>)</figref>, over the region <b>410</b> of the image <b>400</b> found to contain the pathology.</p><p id="p-0065" num="0066">In process S<b>16</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the instruction generating module <b>130</b> generates an instruction for an eye measurement apparatus <b>300</b> to perform a measurement on the portion of the eye to generate measurement data, using a reference point based on the determined location for setting a location of the measurement on the portion of the eye. The instruction generated in process S<b>16</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be provided to the eye measurement apparatus <b>300</b> by any suitable means known to those versed in the art.</p><p id="p-0066" num="0067">In embodiments like the present illustrated embodiment, the determined location is a location in the image <b>400</b> of a group of one or more pixels constituting the region <b>410</b>. In order to determine a reference point that can be used to set a location of the measurement to be performed by the eye measurement apparatus <b>300</b> on the portion of the eye, the location recorded by the search module <b>120</b> in S<b>12</b> can be transformed into a corresponding set of one or more control parameters for steering the eye measurement apparatus <b>300</b> to perform its measurement at substantially the same location in/on the eye as that imaged by the ocular imaging system <b>520</b>. This can be done in one of a number of different ways.</p><p id="p-0067" num="0068">For example, the instruction generating module <b>130</b> may use a mapping between pixel coordinate values from images acquired by the SLO and corresponding values of the control parameters, which may be provided in the form of a look-up table or a function defined by a set of parameters, for example. The mapping may be determined by calibration, using techniques known to those skilled in the art.</p><p id="p-0068" num="0069">In process S<b>18</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the receiver module <b>110</b> receives the measurement data from the eye measurement apparatus <b>300</b>, and the process then ends.</p><p id="p-0069" num="0070">The eye measurement apparatus <b>300</b> may be of any kind of apparatus that allows a supplementary measurement of the subject's eye to be made, which could be used to verify that the region found by the search module <b>120</b> does (or is highly likely to) contain an imaged pathology. The eye measurement apparatus <b>300</b> may, for example, measure a functional response of the subject's eye to stimulation by light in response to an instruction generated by the instruction generating module <b>130</b>, and communicate data representing the measured functional response to the receiver module <b>110</b>.</p><p id="p-0070" num="0071">Alternatively, the eye measurement apparatus <b>300</b> may, as in the present embodiment, be configured to perform, as the measurement on the retina (or other selected portion of the eye, as noted above), an image capture process of a second imaging modality to image a region of the retina, using the aforementioned reference point for setting, as the location of the measurement, a location of the region of the retina to be imaged in the image capture process, wherein the second imaging modality is different to the first imaging modality (i.e. SLO in the present example embodiment). Thus, in the present example embodiment, where a region in the image that is indicative of the pathology is found in S<b>12</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the instruction generating module <b>120</b> generates in S<b>16</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, as the instruction for the eye measurement apparatus <b>300</b> to perform the measurement on the portion of the eye, an instruction for the eye measurement apparatus <b>300</b> to perform the image capture process using the reference point for setting, as the location of the measurement, a location of a region in the portion of the eye to be imaged in the image capture process. The receiver module <b>110</b> then receives in process S<b>18</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, as the measurement data, image data of the second imaging modality acquired by the eye measurement apparatus <b>300</b>. It is noted that a single, multi-modal ocular imaging system capable of acquiring ocular images of two or more different imaging modalities (e.g. a combined SLO-OCT imaging system) may provide the functionality of the ocular imaging system <b>520</b> and the eye measurement apparatus <b>300</b>. The ocular imaging system <b>520</b> and the eye measurement apparatus may alternatively be provided as separate components.</p><p id="p-0071" num="0072">The mapping discussed above may rely on the subject's eye being positioned and oriented in relation to the eye measuring apparatus in a predetermined way. While this may be a good assumption in any practical applications, it may not hold true in all cases. Where this assumption does not hold, it may be preferable to define the location of the region found by the search module <b>120</b> in relation to an anatomical feature (e.g. the fovea) that is recognisable in both the image acquired by the ocular imaging system <b>520</b> and the image of a different modality that is acquired by the eye measurement apparatus <b>300</b> of the present example embodiment. The location of the region relative to the position of the fovea in the image <b>400</b> may then be transformed into a corresponding location relative to the position of the fovea in an image acquired by the eye measurement apparatus <b>300</b>.</p><p id="p-0072" num="0073">The reference point may be used to set the location of the measurement on the portion of the eye in any suitable way. For example, the eye measurement apparatus <b>300</b> may be controlled to image (or more generally perform any other kind of measurement on) a region of a predetermined size that is centred on a location on the subject's retina corresponding to the reference point.</p><p id="p-0073" num="0074">In the modification of the first example embodiment described above, where the search module <b>120</b> searches for a region <b>410</b> in the image that is indicative of one of a plurality of different types of pathology by processing the received image data using the learning algorithm, the instruction generating module <b>130</b> performs, in response to a region <b>410</b> in the image <b>400</b> that is indicative of one of the plurality of different types of pathology being found by the search module <b>120</b>, a process of selecting, for the one of the plurality of different types of pathology and as the second imaging modality, a respective one of a plurality of different types of imaging modality which is to be used to perform the image capture process on the retina of the eye. In the modification of the first embodiment, the instruction generating module <b>130</b> may, for example, select the respective one of a plurality of different types of imaging modality, which is to be used to perform the image capture process on the subject's retina, by consulting a look-up table (LUT) in which an indicator of each of the plurality of different types of pathology is stored in association with a respective indicator of one of the plurality of different types of imaging modality, and using the one of the plurality of different types of pathology found by the search module <b>120</b> as a search key.</p><p id="p-0074" num="0075">For example, in a case where the one of the plurality of different types of pathology is glaucoma, the instruction generating module <b>130</b> may generate an instruction for the eye measurement apparatus <b>300</b> to perform, as the image capture process of the second imaging modality, an OCT scan of the region on the retina of the subject's eye. The receiver module <b>110</b> may then receive in S<b>18</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, as the measurement data, image data of the OCT scan, which the apparatus <b>100</b> can process to obtain complementary data that is useful for detecting the glaucoma (and optionally estimating its severity), namely a measurement of a retinal nerve fibre layer and/or an optic nerve head of the eye.</p><p id="p-0075" num="0076">As a further example, in a case where the one of the plurality of different types of pathology is severe diabetic retinopathy, the instruction generating module <b>130</b> may generate an instruction for the eye measurement apparatus <b>300</b> to perform, as the image capture process of the second imaging modality, an OCT scan of a region of a retina of the eye. The receiver module <b>110</b> may then receive in S<b>18</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, as the measurement data, image data of the OCT scan, which the apparatus <b>100</b> can process to obtain complementary data that is useful for detecting the severe diabetic retinopathy (and optionally estimating its severity), namely measurements of macular thickness.</p><p id="p-0076" num="0077">As another example, in a case where the one of the plurality of different types of pathology is a tumour, the instruction generating module <b>130</b> may generate an instruction for the eye measurement apparatus <b>300</b> to perform, as the image capture process of the second imaging modality, a high-density OCT B-scan of the region in the retina of the eye. The receiver module <b>110</b> may then receive in S<b>18</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, as the measurement data, image data of the high-density OCT B-scan, which can be useful for detecting the tumour (and optionally estimating its size).</p><p id="p-0077" num="0078">As a yet further example, in a case where the one of the plurality of different types of pathology is drusen, the instruction generating module <b>130</b> may generate an instruction for the eye measurement apparatus <b>300</b> to perform, as the image capture process of the second imaging modality, an OCT B-scan of the region in the retina of the eye. The receiver module <b>110</b> may then receive in S<b>18</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, as the measurement data, image data of the OCT B-scan, which can be useful for detecting the presence of drusen or estimating their size and/or number. This data can be valuable in diagnosing age-related macular degeneration at an early stage, for example.</p><p id="p-0078" num="0079">Furthermore, in a case where the one of the plurality of different types of pathology is oedema or atrophy, the instruction generating module <b>130</b> may generate an instruction for the eye measurement apparatus <b>300</b> to perform, as the image capture process of the second imaging modality, an OCT scan of the region in the retina of the eye. The receiver module <b>110</b> may then receive in S<b>18</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, as the measurement data, image data of the OCT scan, which can be useful for detecting the oedema (or atrophy, as the case may be), and optionally estimating its severity.</p><p id="p-0079" num="0080">In the modification of the first example embodiment described above, the search module <b>120</b> searches for a region <b>410</b> in the image that is indicative of one of a plurality of different types of pathology by processing the received image data using the learning algorithm, and the instruction generating module <b>130</b> performs, in response to a region <b>410</b> in the image <b>400</b> that is indicative of one of the plurality of different types of pathology being found by the search module <b>120</b>, a process of selecting, for the one of the plurality of different types of pathology and as the second imaging modality, a respective one of a plurality of different types of imaging modality which is to be used to perform the image capture process on the retina of the eye. However, as noted above, the eye measurement apparatus <b>300</b> need not be configured to acquire an image of the retina (or, more generally, any other selected portion of the eye) but may instead perform a different kind of measurement on the eye. Moreover, the eye measurement apparatus <b>300</b> may be multi-modal in the sense of being operable in a selected one of a plurality of different measurement modalities. In such variants, the instruction generating module <b>130</b> may perform, in response to a region <b>410</b> in the image <b>400</b> that is indicative of one of the plurality of different types of pathology being found by the search module <b>120</b>, processes of: selecting, for the one of the plurality of different types of pathology, a respective one of a plurality of different types of measurement modality for a measurement to be performed on the eye; and generating, as the instruction for the eye measurement apparatus <b>300</b> to perform the measurement on the portion of the eye, an instruction for an eye measurement apparatus <b>300</b> of the selected measurement modality to perform the measurement on the portion of the eye, using the reference point for setting the location of the measurement on the portion of the eye.</p><p id="p-0080" num="0081">In a second modification of the first example embodiment, the eye measurement apparatus <b>300</b> may be an ocular imaging apparatus of the same imaging modality as the ocular imaging system <b>520</b> which, similar to the first embodiment, does not distinguish between different types of pathology. In this second modification, the receiver module <b>110</b> is configured to receive image data representing a result of imaging the portion of the eye using a first value of an imaging parameter, the imaging parameter being an imaging resolution, an aperture size or wavelength used in the imaging. The instruction generating module <b>130</b> is configured to perform, in response to a region <b>410</b> in the image <b>400</b> that is indicative of the pathology being found by the search module <b>120</b>, a process of generating, as the instruction for the eye measurement apparatus <b>300</b> to perform the measurement on the portion of the eye, an instruction for the eye measurement apparatus <b>300</b> to perform an image capture process using a second value of the imaging parameter to image a region in the portion of the eye, again using the reference point for setting, as the location of the measurement, a location of a region in the portion of the eye to be imaged in the image capture process, wherein the second value of the imaging parameter is different from the first value of the imaging parameter. The receiver module <b>110</b> is configured to receive from the eye measurement apparatus <b>300</b>, as the measurement data, the image data representing the result of imaging the region in the portion of the eye using the second value of the imaging parameter and the received image data and the received measurement data are of the same imaging modality.</p><p id="p-0081" num="0082">For example, in the second modification, the ocular imaging system <b>520</b> may be configured to acquire image data defining the image by imaging the portion of the eye at a first imaging resolution, and the instruction generating module <b>130</b> may be configured to generate, in response to a region in the image that is indicative of pathology being found by the search module <b>120</b>, an instruction for the eye measurement apparatus <b>300</b> to perform an image capture process at a second, higher imaging resolution to image the region in more detail.</p><heading id="h-0007" level="1">Embodiment 2</heading><p id="p-0082" num="0083"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic illustration of an apparatus <b>800</b> for searching for the presence of a pathology in an image of a portion of an eye acquired by an ocular imaging system (<b>520</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>), in accordance with a second example embodiment herein. The apparatus <b>800</b> of the second example embodiment differs from the apparatus <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in that the apparatus of <figref idref="DRAWINGS">FIG. <b>9</b></figref> is not required to determine the location of a region indicative of a pathology in the received image <b>400</b>.</p><p id="p-0083" num="0084">The apparatus <b>800</b> comprises a receiver module <b>810</b> configured to receive image data defining an image produced by a ocular imaging system <b>520</b>, and a search module <b>820</b> configured to search for the presence of at least one of a plurality of different types of pathology in the image. The apparatus <b>800</b> further comprises an instruction generating module <b>830</b> configured to perform, in response to at least one of the plurality of different types of pathology being found to be present in the image by the search module <b>820</b>, processes of: selecting, for each of at least one type of pathology found to be present in the image, a respective one of a plurality of different types of measurement modality which is to be used to perform a measurement on the portion of the eye; and generating, for each of the at least one type of pathology found to be present in the image, a respective instruction for an eye measurement apparatus of the respective selected measurement modality to perform the measurement on the portion of the eye. The receiver module <b>810</b> is further configured to receive measurement data of the measurement performed by the eye measurement apparatus of each selected measurement modality. Optionally, the apparatus <b>800</b> may, as in the present illustrated embodiment comprise a display control signal generator <b>840</b> (this optional component being shown by the dashed lines in <figref idref="DRAWINGS">FIG. <b>9</b></figref>).</p><p id="p-0084" num="0085">The apparatus <b>800</b> may be configured to search for the presence of any pathology, (including, for example, glaucoma, moderate diabetic retinopathy, severe diabetic retinopathy, tumour, drusen, oedema and atrophy), as discussed above in relation to the first embodiment.</p><p id="p-0085" num="0086">In the present example embodiment, the receiver module <b>810</b> is configured to receive image data defining an image of a portion of a retina of the eye, which has been generated by the ocular imaging system <b>520</b>. However, in other example embodiments, the received image data may define an image of a portion of the eye other than the retina, for example a portion of the anterior segment of the eye, or a portion of the posterior segment of the eye.</p><p id="p-0086" num="0087">The ocular imaging system <b>520</b> may, as in the present example embodiment, be a scanning laser ophthalmoscope. Alternatively, the ocular imaging system may be any ocular imaging system described above in relation to the first embodiment.</p><p id="p-0087" num="0088">The receiver module <b>810</b> may be configured to receive image data defining the image acquired by the ocular imaging system <b>520</b> by any of the means discussed above in relation to the first embodiment.</p><p id="p-0088" num="0089">As will become more apparent from the following description of the operations performed by the apparatus <b>800</b> of the example embodiment, the apparatus <b>800</b> automatically processes image data defining an image of the portion of an eye acquired by an ocular imaging system <b>520</b> to search for the presence of at least one of a plurality of different types of pathology in the image and, when at least one of the plurality of different types of pathology is found to be present in the image, selects for each of at least one type of pathology found to be present in the image a respective one of a plurality of different types of measurement modality which is to be used to perform a measurement on the portion of the eye. A respective instruction for an eye measurement apparatus of the respective selected measurement modality to perform the measurement on the portion of the eye is then generated for each of the at least one type of pathology found to be present in the image. The apparatus <b>800</b> thus acquires both the image suspected to contain a particular type of pathology, and additional measurement data of a measurement modality that is related to the suspected type of pathology. These different but complementary kinds of information that are automatically acquired by the apparatus <b>800</b> may allow any pathology-containing ocular image to be identified quickly and with a high degree of confidence.</p><p id="p-0089" num="0090">The apparatus <b>800</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref> may be implemented by a signal processing hardware configuration, such as that shown in the <figref idref="DRAWINGS">FIG. <b>2</b></figref>, or by any other suitable means.</p><p id="p-0090" num="0091"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flow diagram illustrating a process by which the apparatus <b>800</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref> searches for the presence of a pathology in an image of a portion of an eye acquired by an ocular imaging system <b>520</b>, in accordance with the second example embodiment herein.</p><p id="p-0091" num="0092">In process S<b>20</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the receiver module <b>810</b> receives image data defining the image of the portion of the eye acquired by the SLO (as an example of the ocular imaging system <b>520</b>).</p><p id="p-0092" num="0093">In process S<b>22</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the search module <b>820</b> searches for the presence of at least one of a plurality of different types of pathology in the image by processing the received image data using a learning algorithm trained on image data defining images of healthy eyes, and images of unhealthy eyes each having a respective one of the different types of pathology. The learning algorithm may be trained as discussed above in relation to <figref idref="DRAWINGS">FIG. <b>6</b></figref> in order to classify input image data defining an image of the portion of an eye as &#x201c;healthy&#x201d; or as &#x201c;unhealthy&#x201d; and having a particular pathology.</p><p id="p-0093" num="0094">The apparatus <b>800</b> is not required to record a location of a region indicative of pathology in the image. Therefore, the apparatus <b>800</b> may, as in the present example embodiment, not process the learning algorithm in order to determine which input variables of the input image data (that is, pixels of the image defined by the received image data) were relevant to the output (that is, the finding that one of a plurality of different types of pathology is present). Alternatively, in other embodiments, such processing may optionally be carried out by the apparatus <b>800</b> as part of S<b>22</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref> in order to identify a reference point.</p><p id="p-0094" num="0095">In process S<b>23</b>, the search module <b>820</b> determines whether at least one of the different types of pathology has been found to be present in the image, as a result of the search performed in process S<b>22</b>. If at least one of the different types of pathology has been found to be present in the image as a result of the search performed in process S<b>22</b>, the process proceeds to S<b>24</b>, otherwise the process ends.</p><p id="p-0095" num="0096">In process S<b>24</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, in response to at least one of the plurality of different types of pathology being found to be present in the image by the search module <b>820</b>, the instruction generating module <b>830</b> performs the process of selecting, for each of at least one type of pathology found to be present in the image, a respective one of a plurality of different types of measurement modality which is to be used to perform a measurement on the portion of the eye.</p><p id="p-0096" num="0097">The instruction generating module <b>830</b> may select a respective one of a plurality of different types of measurement modality by any of the means described above in relation to the apparatus <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0097" num="0098">In process S<b>26</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, in response to at least one of the plurality of different types of pathology being found to be present in the image by the search module <b>820</b>, the instruction generating module <b>830</b> performs the process of generating, for each of the at least one type of pathology found to be present in the image, a respective instruction for an eye measurement apparatus of the respective selected measurement modality to perform the measurement on the portion of the eye.</p><p id="p-0098" num="0099">The instruction may be generated substantially as discussed above in relation to apparatus <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. However, in embodiments like the present illustrated embodiment where the apparatus <b>800</b> does not search for a region indicative of a pathology, the instruction generating module <b>830</b> need not use a reference point based on a recorded location of a region indicative of pathology for setting a location of the measurement on the portion of the eye. As part of process S<b>26</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the instruction generating module <b>130</b> may, as in the present example embodiment, generate, for each of the at least one type of pathology found to be present in the image, a respective instruction for an eye measurement apparatus of the respective selected measurement modality to perform the measurement on the same portion of the eye as imaged by the ocular imaging system.</p><p id="p-0099" num="0100">In process S<b>28</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref> the receiver module <b>810</b> receives measurement data of the measurement performed by the eye measurement apparatus of each selected measurement modality. The receiver module <b>810</b> may receive measurement data by any of the means discussed above in relation of the first embodiment.</p><p id="p-0100" num="0101">Some of the embodiments described above are summarised in the following examples E1 to E52:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0102">E1. A computer-implemented method of searching for a region (<b>410</b>) indicative of a pathology in an image (<b>400</b>) of a portion of an eye acquired by an ocular imaging system (<b>520</b>), the method comprising:        <ul id="ul0005" list-style="none">            <li id="ul0005-0001" num="0103">receiving (S<b>10</b>) image data defining the image (<b>400</b>);</li>            <li id="ul0005-0002" num="0104">searching (S<b>12</b>) for the region (<b>410</b>) in the image (<b>400</b>) by processing the received image data using a learning algorithm (<b>530</b>; <b>630</b>) trained on image data (<b>502</b>; <b>604</b>) defining images of the portion of healthy eyes, and image data (<b>601</b>, <b>602</b>, <b>603</b>; <b>501</b>) defining images of the portion of unhealthy eyes each having at least one region that is indicative of the pathology; and</li>            <li id="ul0005-0003" num="0105">in case a region (<b>410</b>) in the image (<b>400</b>) that is indicative of the pathology is found in the searching:            <ul id="ul0006" list-style="none">                <li id="ul0006-0001" num="0106">determining (S<b>14</b>) a location of the region (<b>410</b>) in the image (<b>400</b>);</li>                <li id="ul0006-0002" num="0107">generating (S<b>16</b>) an instruction for an eye measurement apparatus (<b>300</b>) to perform a measurement on the portion of the eye to generate measurement data, using a reference point based on the determined location for setting a location of the measurement on the portion of the eye; and</li>            </ul>            </li>            <li id="ul0005-0004" num="0108">receiving (S<b>18</b>) the measurement data from the eye measurement apparatus (<b>300</b>).</li>        </ul>        </li>        <li id="ul0004-0002" num="0109">E2. The computer-implemented method of E1, wherein        <ul id="ul0007" list-style="none">            <li id="ul0007-0001" num="0110">searching (S<b>12</b>) for the region (<b>410</b>) in the image (<b>400</b>) comprises searching for a region (<b>410</b>) in the image (<b>400</b>) that is indicative of one of a plurality of different types of pathology by processing the received image data using the learning algorithm (<b>630</b>), the learning algorithm being trained on image data (<b>601</b>, <b>602</b>, <b>603</b>) defining images of the portion of unhealthy eyes each having a respective one of the different types of pathology, and</li>            <li id="ul0007-0002" num="0111">in case a region (<b>410</b>) in the image (<b>400</b>) that is indicative of one of the plurality of different types of pathology is found in the searching (S<b>12</b>):            <ul id="ul0008" list-style="none">                <li id="ul0008-0001" num="0112">the method further comprises selecting, for the one of the plurality of different types of pathology, a respective one of a plurality of different types of measurement modality for a measurement to be performed on the eye; and</li>                <li id="ul0008-0002" num="0113">the method comprises generating (S<b>16</b>), as the instruction for the eye measurement apparatus (<b>300</b>) to perform the measurement on the portion of the eye, an instruction for an eye measurement apparatus (<b>300</b>) of the selected measurement modality to perform the measurement on the portion of the eye, using the reference point for setting the location of the measurement on the portion of the eye.</li>            </ul>            </li>        </ul>        </li>        <li id="ul0004-0003" num="0114">E3. The computer-implemented method of E1, wherein        <ul id="ul0009" list-style="none">            <li id="ul0009-0001" num="0115">the received image data is image data of a first imaging modality,</li>            <li id="ul0009-0002" num="0116">the eye measurement apparatus (<b>300</b>) is configured to perform, as the measurement on the portion of the eye, an image capture process of a second imaging modality to image a region in the portion of the eye, and to acquire, as the measurement data, image data of the second imaging modality, the second imaging modality being different than the first imaging modality, and</li>            <li id="ul0009-0003" num="0117">in the case that a region (<b>410</b>) in the image (<b>400</b>) that is indicative of the pathology is found in the searching (S<b>12</b>), the method comprises:            <ul id="ul0010" list-style="none">                <li id="ul0010-0001" num="0118">generating (S<b>16</b>), as the instruction for the eye measurement apparatus (<b>300</b>) to perform the measurement on the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform the image capture process using the reference point for setting, as the location of the measurement, a location of a region in the portion of the eye to be imaged in the image capture process; and</li>                <li id="ul0010-0002" num="0119">receiving (S<b>18</b>) from the eye measurement apparatus (<b>300</b>), as the measurement data, image data of the second imaging modality defining an image of the region in the portion of the eye.</li>            </ul>            </li>        </ul>        </li>        <li id="ul0004-0004" num="0120">E4. The computer-implemented method of E3, wherein        <ul id="ul0011" list-style="none">            <li id="ul0011-0001" num="0121">searching (S<b>12</b>) for the region (<b>410</b>) in the image (<b>400</b>) comprises searching for a region (<b>410</b>) in the image (<b>400</b>) that is indicative of one of a plurality of different types of pathology by processing the received image data using the learning algorithm (<b>630</b>), the learning algorithm being trained on the image data (<b>601</b>, <b>602</b>, <b>603</b>) defining images of the portion of unhealthy eyes each having a respective one of the different types of pathology, and</li>            <li id="ul0011-0002" num="0122">in case a region (<b>410</b>) in the image (<b>400</b>) that is indicative of one of the plurality of different types of pathology is found in the searching (S<b>12</b>), the method further comprises selecting, for the one of the plurality of different types of pathology and as the second imaging modality, a respective one of a plurality of different types of imaging modality which is to be used to perform the image capture process on the portion of the eye.</li>        </ul>        </li>        <li id="ul0004-0005" num="0123">E5. The computer-implemented method of E4, wherein:        <ul id="ul0012" list-style="none">            <li id="ul0012-0001" num="0124">in a case where the one of the plurality of different types of pathology is glaucoma, the method comprises generating (S<b>16</b>) an instruction for the eye measurement apparatus to perform, as the image capture process of the second imaging modality, an optical coherence tomography, OCT, scan of the region in the portion of the eye, and to acquire, as the measurement data, image data of the OCT scan;</li>            <li id="ul0012-0002" num="0125">in a case where the one of the plurality of different types of pathology is severe diabetic retinopathy, the method comprises generating (S<b>16</b>) an instruction for the eye measurement apparatus to perform, as the image capture process of the second imaging modality, an optical coherence tomography, OCT, scan of a region of a retina of the eye, and to acquire, as the measurement data, image data of the OCT scan;</li>            <li id="ul0012-0003" num="0126">in a case where the one of the plurality of different types of pathology is a tumour, the method comprises generating (S<b>16</b>) an instruction for the eye measurement apparatus to perform, as the image capture process of the second imaging modality, a high-density optical coherence tomography, OCT, B-scan of the region in the portion of the eye, and to acquire, as the measurement data, image data of the high-density OCT B-scan;</li>            <li id="ul0012-0004" num="0127">in a case where the one of the plurality of different types of pathology is drusen, the method comprises generating (S<b>16</b>) an instruction for the eye measurement apparatus to perform, as the image capture process of the second imaging modality, an optical coherence tomography, OCT, B-scan of the region in the portion of the eye, and to acquire, as the measurement data, image data of the OCT B-scan;</li>            <li id="ul0012-0005" num="0128">in a case where the one of the plurality of different types of pathology is oedema or atrophy, the method comprises generating (S<b>16</b>) an instruction for the eye measurement apparatus to perform, as the image capture process of the second imaging modality, an optical coherence tomography, OCT, scan of the region in the portion of the eye, and to acquire, as the measurement data, image data of the OCT scan.</li>        </ul>        </li>        <li id="ul0004-0006" num="0129">E6. The computer-implemented method of E1, wherein, in case the region (<b>410</b>) in the image (<b>400</b>) that is indicative of the pathology is found in the searching, the method comprises generating (S<b>16</b>), as the instruction for the eye measurement apparatus to perform the measurement on the portion of the eye, an instruction for the eye measurement apparatus to measure a functional response of the eye to light stimulation, using the reference point for setting the location of the measurement which is based on the determined location.</li>        <li id="ul0004-0007" num="0130">E7. The computer-implemented method of E1, wherein        <ul id="ul0013" list-style="none">            <li id="ul0013-0001" num="0131">the received image data represents a result of imaging the portion of the eye using a first value of an imaging parameter, the imaging parameter being one of an imaging resolution, an aperture size and wavelength used in the imaging,</li>            <li id="ul0013-0002" num="0132">the eye measurement apparatus is configured to perform, as the measurement on the portion of the eye, an image capture process using a second value of the imaging parameter to image a region in the portion of the eye, and to acquire, as the measurement data, image data representing a result of imaging the region using the second value of the imaging parameter, wherein the second value of the imaging parameter is different from the first value of the imaging parameter, and the received image data and the acquired image data are of the same imaging modality, and</li>            <li id="ul0013-0003" num="0133">in the case that a region (<b>410</b>) in the image (<b>400</b>) that is indicative of the pathology is found in the searching, the method comprises:            <ul id="ul0014" list-style="none">                <li id="ul0014-0001" num="0134">generating (S<b>16</b>), as the instruction for the eye measurement apparatus to perform the measurement on the portion of the eye, an instruction for the eye measurement apparatus to perform the image capture process, using the reference point for setting, as the location of the measurement, a location of a region in the portion of the eye to be imaged in the image capture process; and</li>                <li id="ul0014-0002" num="0135">receiving (S<b>18</b>) from the eye measurement apparatus, as the measurement data, the image data representing the result of imaging the region (<b>410</b>) in the portion of the eye using the second value of the imaging parameter.</li>            </ul>            </li>        </ul>        </li>        <li id="ul0004-0008" num="0136">E8. The computer-implemented method of any of E1 to E7, further comprising generating instructions for controlling a display unit (<b>215</b>) to display the location of the region (<b>410</b>) in the image (<b>400</b>) of the portion of the eye and a representation of the received measurement data.</li>        <li id="ul0004-0009" num="0137">E9. The computer-implemented method of any of E1 to E8, wherein the learning algorithm (<b>530</b>; <b>630</b>) is a supervised learning algorithm.</li>        <li id="ul0004-0010" num="0138">E10. The computer-implemented method of E9, wherein the supervised learning algorithm comprises a neural network, and the region (<b>410</b>) indicative of the pathology is searched for in the image (<b>400</b>) by deconstructing the neural network.</li>        <li id="ul0004-0011" num="0139">E11. The computer-implemented method of E10, wherein the neural network is a convolutional neural network, and the neural network is deconstructed by:        <ul id="ul0015" list-style="none">            <li id="ul0015-0001" num="0140">performing, for each of a plurality of different sections of the image (<b>400</b>) that is defined by the received image data, processes of:            <ul id="ul0016" list-style="none">                <li id="ul0016-0001" num="0141">masking (S<b>122</b>) the section of the image (<b>400</b>) to generate a masked image;</li>                <li id="ul0016-0002" num="0142">searching (S<b>124</b>) for the region in the masked image by processing image data defining the masked image using the learning algorithm; and</li>                <li id="ul0016-0003" num="0143">determining (S<b>126</b>) a difference between a result of the search performed using the image data defining the masked image and a result of a search performed using the received image data; and</li>            </ul>            </li>            <li id="ul0015-0002" num="0144">determining (S<b>128</b>), as the location of the region (<b>410</b>) in the image (<b>400</b>), a location of a section for which the determined difference is largest.</li>        </ul>        </li>        <li id="ul0004-0012" num="0145">E12. The computer-implemented method of E10, wherein the neural network is a convolutional neural network, and the convolutional neural network is deconstructed by:        <ul id="ul0017" list-style="none">            <li id="ul0017-0001" num="0146">determining a relevance of each input variable of the neural network to an output of the neural network by applying a Taylor decomposition to each layer of the neural network, from a top layer of the neural network to an input layer of the neural network; and</li>            <li id="ul0017-0002" num="0147">determining the location of the region (<b>410</b>) in the image (<b>400</b>) based on at least one section of the received image data corresponding to the most relevant input variables of the neural network.</li>        </ul>        </li>        <li id="ul0004-0013" num="0148">E13. The computer-implemented method of E10, wherein the neural network is a convolutional neural network, and the convolutional neural network is deconstructed by determining a deconvolution of the convolutional neural network.</li>        <li id="ul0004-0014" num="0149">E14. A computer-implemented method of searching for the presence of a pathology in an image (<b>400</b>) of a portion of an eye acquired by an ocular imaging system (<b>520</b>), the method comprising:        <ul id="ul0018" list-style="none">            <li id="ul0018-0001" num="0150">receiving (S<b>20</b>) image data defining the image (<b>400</b>);</li>            <li id="ul0018-0002" num="0151">searching (S<b>22</b>) for the presence of at least one of a plurality of different types of pathology in the image (<b>400</b>) by processing the received image data using a learning algorithm (<b>630</b>) trained on image data defining images (<b>502</b>) of healthy eyes, and images (<b>601</b>, <b>602</b>, <b>603</b>) of unhealthy eyes each having a respective one of the different types of pathology; and</li>            <li id="ul0018-0003" num="0152">in case at least one of the plurality of different types of pathology is found to be present in the image (<b>400</b>):            <ul id="ul0019" list-style="none">                <li id="ul0019-0001" num="0153">selecting (S<b>24</b>), for each of at least one type of pathology found to be present in the image (<b>400</b>), a respective one of a plurality of different types of measurement modality which is to be used to perform a measurement on the portion of the eye;</li>                <li id="ul0019-0002" num="0154">generating (S<b>26</b>), for each of the at least one type of pathology found to be present in the image (<b>400</b>), a respective instruction for an eye measurement apparatus (<b>300</b>) of the respective selected measurement modality to perform the measurement on the portion of the eye; and</li>                <li id="ul0019-0003" num="0155">receiving (S<b>28</b>) measurement data of the measurement performed by the eye measurement apparatus (<b>300</b>) of each selected measurement modality.</li>            </ul>            </li>        </ul>        </li>        <li id="ul0004-0015" num="0156">E15. The computer-implemented method of E14, further comprising, in case at least one of the plurality of different types of pathology is found to be present in the image (<b>400</b>):        <ul id="ul0020" list-style="none">            <li id="ul0020-0001" num="0157">for each of the at least one of the different types of pathology found to be present in the image, searching for a respective region (<b>410</b>) in the image (<b>400</b>) that is indicative of the respective type of pathology, by processing the received image data using the learning algorithm (<b>630</b>), and recording a location of the respective region (<b>410</b>) in the image (<b>400</b>), wherein</li>            <li id="ul0020-0002" num="0158">a respective instruction for the eye measurement apparatus (<b>300</b>) of each selected measurement modality to perform a measurement on the eye using a reference point for locating the measurement which is based on the respective recorded location is generated.</li>        </ul>        </li>        <li id="ul0004-0016" num="0159">E16. The computer-implemented method of E15, wherein, in case at least one of the plurality of different types of pathology is found to be present in the image (<b>400</b>),        <ul id="ul0021" list-style="none">            <li id="ul0021-0001" num="0160">for each of the at least one of the different types of pathology found to be present in the image (<b>400</b>):            <ul id="ul0022" list-style="none">                <li id="ul0022-0001" num="0161">a respective one of a plurality of different types of imaging modality which is to be used to image the portion of the eye is selected as the respective one of the plurality of different types of measurement modality;</li>                <li id="ul0022-0002" num="0162">a respective instruction for an eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, using a reference point for locating a region of the eye to be imaged which is based on the respective recorded location, is generated as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected measurement modality; and</li>                <li id="ul0022-0003" num="0163">respective image data of the imaging performed by the eye measurement apparatus (<b>300</b>) of the selected imaging modality is received as the measurement data of the measurement performed by the eye measurement apparatus (<b>300</b>).</li>            </ul>            </li>        </ul>        </li>        <li id="ul0004-0017" num="0164">E17. The computer-implemented method of E16, wherein:        <ul id="ul0023" list-style="none">            <li id="ul0023-0001" num="0165">in a case where one of the plurality of different types of pathology found to be present in the image (<b>400</b>) is glaucoma, the method comprises generating (S<b>26</b>), as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform an optical coherence tomography, OCT, scan of the region in the portion of the eye, and to acquire, as the measurement data, image data of the OCT scan;</li>            <li id="ul0023-0002" num="0166">in a case where one of the plurality of different types of pathology found to be present in the image (<b>400</b>) is severe diabetic retinopathy, the method comprises generating (S<b>26</b>), as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform an optical coherence tomography, OCT, scan of a region of a retina of the eye, and to acquire, as the measurement data, image data of the OCT scan;</li>            <li id="ul0023-0003" num="0167">in a case where one of the plurality of different types of pathology found to be present in the image (<b>400</b>) is a tumour, the method comprises generating (S<b>26</b>), as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform a high-density optical coherence tomography, OCT, B-scan of the region in the portion of the eye, and to acquire, as the measurement data, image data of the high-density OCT B-scan;</li>            <li id="ul0023-0004" num="0168">in a case where the one of the plurality of different types of pathology found to be present in the image (<b>400</b>) is drusen, the method comprises generating (S<b>26</b>), as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform an optical coherence tomography, OCT, B-scan of the region in the portion of the eye, and to acquire, as the measurement data, image data of the OCT B-scan; and</li>            <li id="ul0023-0005" num="0169">in a case where the one of the plurality of different types of pathology found to be present in the image (<b>400</b>) is oedema or atrophy, the method comprises generating (S<b>26</b>), as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform an optical coherence tomography, OCT, scan of the region in the portion of the eye, and to acquire, as the measurement data, image data of the OCT scan.</li>        </ul>        </li>        <li id="ul0004-0018" num="0170">E18. The computer-implemented method of E14, wherein, in case at least one of the plurality of different types of pathology is found to be present in the image (<b>400</b>), at least one of the instructions generated is an instruction for an eye measurement apparatus (<b>300</b>) of a selected measurement modality to measure a functional response of the eye to light stimulation.</li>        <li id="ul0004-0019" num="0171">E19. The computer-implemented method of any of E15 to E17, further comprising generating instructions for controlling a display unit (<b>215</b>) to display the recorded location of the region (<b>410</b>) in the image (<b>400</b>) of the portion of the eye and a representation of the received measurement data.</li>        <li id="ul0004-0020" num="0172">E20. The computer-implemented method of any of E15 to E17, wherein the learning algorithm (<b>630</b>) is a supervised learning algorithm.</li>        <li id="ul0004-0021" num="0173">E21. The computer-implemented method of E20, wherein the supervised learning algorithm comprises a neural network, and a region indicative of one of the different types of pathology found to be present in the image (<b>400</b>) is searched for in the image by deconstructing the neural network.</li>        <li id="ul0004-0022" num="0174">E22. The computer-implemented method of E21, wherein the neural network is a convolutional neural network, and the neural network is deconstructed by:        <ul id="ul0024" list-style="none">            <li id="ul0024-0001" num="0175">performing, for each of a plurality of different sections of the image (<b>400</b>) that is defined by the received image data, processes of:            <ul id="ul0025" list-style="none">                <li id="ul0025-0001" num="0176">masking (S<b>122</b>) the section of the image (<b>400</b>) to generate a masked image;</li>                <li id="ul0025-0002" num="0177">searching (S<b>124</b>) for the region in the masked image by processing image data defining the masked image using the learning algorithm; and</li>                <li id="ul0025-0003" num="0178">determining (S<b>126</b>) a difference between a result of the search performed using the image data defining the masked image and a result of a search performed using the received image data; and</li>            </ul>            </li>            <li id="ul0024-0002" num="0179">determining (S<b>128</b>), as the location to be recorded, a location of a section for which the determined difference is largest.</li>        </ul>        </li>        <li id="ul0004-0023" num="0180">E23. The computer-implemented method of E21, wherein the neural network is a convolutional neural network, and the convolutional neural network is deconstructed by:        <ul id="ul0026" list-style="none">            <li id="ul0026-0001" num="0181">determining a relevance of each input variable of the neural network to an output of the neural network by applying a Taylor decomposition to each layer of the neural network, from a top layer of the neural network to an input layer of the neural network; and</li>            <li id="ul0026-0002" num="0182">determining the location to be recorded based on at least one section of the received image data corresponding to the most relevant input variables of the neural network.</li>        </ul>        </li>        <li id="ul0004-0024" num="0183">E24. The computer-implemented method of E21, wherein the neural network is a convolutional neural network, and the convolutional neural network is deconstructed by determining a deconvolution of the convolutional neural network.</li>        <li id="ul0004-0025" num="0184">E25. A computer program which, when executed by a computer, causes the computer to perform a method as set out in at least one of E1 to E24.</li>        <li id="ul0004-0026" num="0185">E26. A computer-readable storage medium (<b>250</b>) storing the computer program of E25.</li>        <li id="ul0004-0027" num="0186">E27. A signal (<b>260</b>) carrying the computer program of E25.</li>        <li id="ul0004-0028" num="0187">E28. An apparatus (<b>100</b>) for searching for a region indicative of a pathology in an image (<b>400</b>) of a portion of an eye acquired by an ocular imaging system (<b>520</b>; <b>720</b>), the apparatus (<b>100</b>) comprising:        <ul id="ul0027" list-style="none">            <li id="ul0027-0001" num="0188">a receiver module (<b>110</b>) configured to receive image data defining the image (<b>400</b>);</li>            <li id="ul0027-0002" num="0189">a search module (<b>120</b>) configured to search for the region in the image (<b>400</b>) by processing the received image data using a learning algorithm (<b>530</b>; <b>630</b>) trained on image data (<b>502</b>; <b>604</b>) defining images of the portion of healthy eyes, and image data (<b>601</b>, <b>602</b>, <b>603</b>; <b>501</b>) defining images of the portion of unhealthy eyes each having at least one region that is indicative of the pathology; and</li>            <li id="ul0027-0003" num="0190">an instruction generating module (<b>130</b>) configured to perform, in response to a region (<b>410</b>) in the image (<b>400</b>) that is indicative of the pathology being found by the search module (<b>120</b>), processes of:            <ul id="ul0028" list-style="none">                <li id="ul0028-0001" num="0191">determining a location of the region (<b>410</b>) in the image (<b>400</b>); and</li>                <li id="ul0028-0002" num="0192">generating an instruction for an eye measurement apparatus (<b>300</b>) to perform a measurement on the portion of the eye to generate measurement data, using a reference point based on the determined location for setting a location of the measurement on the portion of the eye,</li>            </ul>            </li>            <li id="ul0027-0004" num="0193">wherein the receiver module (<b>110</b>) is further configured to receive the measurement data from the eye measurement apparatus (<b>300</b>).</li>        </ul>        </li>        <li id="ul0004-0029" num="0194">E29. The apparatus (<b>100</b>) of E28, wherein        <ul id="ul0029" list-style="none">            <li id="ul0029-0001" num="0195">the search module (<b>120</b>) is configured to search for the region (<b>410</b>) in the image (<b>400</b>) by searching for a region (<b>410</b>) in the image (<b>400</b>) that is indicative of one of a plurality of different types of pathology by processing the received image data using the learning algorithm (<b>630</b>), the learning algorithm (<b>630</b>) being trained on image data (<b>601</b>, <b>602</b>, <b>603</b>) defining images of the portion of unhealthy eyes each having a respective one of the different types of pathology,</li>            <li id="ul0029-0002" num="0196">the instruction generating module (<b>130</b>) is configured to perform, in response to a region (<b>410</b>) in the image (<b>400</b>) that is indicative of one of the plurality of different types of pathology being found by the search module (<b>120</b>), processes of:            <ul id="ul0030" list-style="none">                <li id="ul0030-0001" num="0197">selecting, for the one of the plurality of different types of pathology, a respective one of a plurality of different types of measurement modality for a measurement to be performed on the eye; and</li>                <li id="ul0030-0002" num="0198">generating, as the instruction for the eye measurement apparatus (<b>300</b>) to perform the measurement on the portion of the eye, an instruction for an eye measurement apparatus (<b>300</b>) of the selected measurement modality to perform the measurement on the portion of the eye, using the reference point for setting the location of the measurement on the portion of the eye.</li>            </ul>            </li>        </ul>        </li>        <li id="ul0004-0030" num="0199">E30. The apparatus (<b>100</b>) of E28, wherein        <ul id="ul0031" list-style="none">            <li id="ul0031-0001" num="0200">the receiver module (<b>110</b>) is configured to receive, as the image data, image data of a first imaging modality,</li>            <li id="ul0031-0002" num="0201">the instruction generating module (<b>130</b>) is configured to generate, in response to a region (<b>410</b>) in the image (<b>400</b>) that is indicative of the pathology being found by the search module (<b>120</b>), and as the instruction for the eye measurement apparatus (<b>300</b>) to perform the measurement on the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform an image capture process of a second imaging modality to image a region in the portion of the eye, using the reference point for setting, as the location of the measurement, a location of a region in the portion of the eye to be imaged in the image capture process, the second imaging modality being different to the first imaging modality, and</li>            <li id="ul0031-0003" num="0202">the receiver module (<b>110</b>) is configured to receive from the eye measurement apparatus (<b>300</b>), as the measurement data, image data of the second imaging modality acquired by the eye measurement apparatus (<b>300</b>).</li>        </ul>        </li>        <li id="ul0004-0031" num="0203">E31. The apparatus (<b>100</b>) of E30, wherein        <ul id="ul0032" list-style="none">            <li id="ul0032-0001" num="0204">the search module (<b>120</b>) is configured to search for a region (<b>410</b>) in the image (<b>400</b>) that is indicative of one of a plurality of different types of pathology by processing the received image data using the learning algorithm (<b>630</b>), the learning algorithm being trained on the image data (<b>601</b>, <b>602</b>, <b>603</b>) defining images of the portion of unhealthy eyes each having a respective one of the different types of pathology, and</li>            <li id="ul0032-0002" num="0205">the instruction generating module (<b>130</b>) is configured to perform, in response to a region (<b>410</b>) in the image (<b>400</b>) that is indicative of one of the plurality of different types of pathology being found by the search module (<b>120</b>), a process of selecting, for the one of the plurality of different types of pathology and as the second imaging modality, a respective one of a plurality of different types of imaging modality which is to be used to perform the image capture process on the portion of the eye.</li>        </ul>        </li>        <li id="ul0004-0032" num="0206">E32. The apparatus (<b>100</b>) of E31, wherein        <ul id="ul0033" list-style="none">            <li id="ul0033-0001" num="0207">the instruction generating module (<b>130</b>) is configured to generate, in a case where the one of the plurality of different types of pathology is glaucoma, an instruction for the eye measurement apparatus (<b>300</b>) to perform, as the image capture process of the second imaging modality, an optical coherence tomography, OCT, scan of the region in the portion of the eye, and the receiver module (<b>110</b>) is configured to receive, as the measurement data, image data of the OCT scan;</li>            <li id="ul0033-0002" num="0208">the instruction generating module (<b>130</b>) is configured to generate, in a case where the one of the plurality of different types of pathology is severe diabetic retinopathy, an instruction for the eye measurement apparatus (<b>300</b>) to perform, as the image capture process of the second imaging modality, an optical coherence tomography, OCT, scan of a region of a retina of the eye, and the receiver module (<b>110</b>) is configured to receive, as the measurement data, image data of the OCT scan;</li>            <li id="ul0033-0003" num="0209">the instruction generating module (<b>130</b>) is configured to generate, in a case where the one of the plurality of different types of pathology is a tumour, an instruction for the eye measurement apparatus (<b>300</b>) to perform, as the image capture process of the second imaging modality, a high-density optical coherence tomography, OCT, B-scan of the region in the portion of the eye, and the receiver module (<b>110</b>) is configured to receive, as the measurement data, image data of the high-density OCT B-scan;</li>            <li id="ul0033-0004" num="0210">the instruction generating module (<b>130</b>) is configured to generate, in a case where the one of the plurality of different types of pathology is drusen, an instruction for the eye measurement apparatus (<b>300</b>) to perform, as the image capture process of the second imaging modality, an optical coherence tomography, OCT, B-scan of the region in the portion of the eye, and the receiver module (<b>110</b>) is configured to receive, as the measurement data, image data of the OCT B-scan; and</li>            <li id="ul0033-0005" num="0211">the instruction generating module (<b>130</b>) is configured to generate, in a case where the one of the plurality of different types of pathology is oedema, an instruction for the eye measurement apparatus (<b>300</b>) to perform, as the image capture process of the second imaging modality, an optical coherence tomography, OCT, scan of the region in the portion of the eye, and the receiver module (<b>110</b>) is configured to receive, as the measurement data, image data of the OCT scan.</li>        </ul>        </li>        <li id="ul0004-0033" num="0212">E33. The apparatus (<b>100</b>) of E28, wherein, the instruction generating module (<b>130</b>) is configured to generate, in response to the region (<b>410</b>) in the image (<b>400</b>) that is indicative of the pathology being found by the search module (<b>120</b>), and as the instruction for the eye measurement apparatus (<b>300</b>) to perform the measurement on the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to measure a functional response of the eye to light stimulation, using the reference point for setting the location of the measurement which is based on the determined location.</li>        <li id="ul0004-0034" num="0213">E34. The apparatus (<b>100</b>) of E28, wherein        <ul id="ul0034" list-style="none">            <li id="ul0034-0001" num="0214">the receiver module (<b>110</b>) is configured to receive image data representing a result of imaging the portion of the eye using a first value of an imaging parameter, the imaging parameter being one of an imaging resolution, an aperture size and wavelength used in the imaging,</li>            <li id="ul0034-0002" num="0215">the instruction generating module (<b>130</b>) is configured to perform, in response to a region (<b>410</b>) in the image (<b>400</b>) that is indicative of the pathology being found by the search module (<b>120</b>), a processes of generating, as the instruction for the eye measurement apparatus (<b>300</b>) to perform the measurement on the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform an image capture process using a second value of the imaging parameter to image a region in the portion of the eye, using the reference point for setting, as the location of the measurement, a location of a region in the portion of the eye to be imaged in the image capture process, wherein the second value of the imaging parameter is different from the first value of the imaging parameter, and</li>            <li id="ul0034-0003" num="0216">the receiver module (<b>110</b>) is configured to receive from the eye measurement apparatus (<b>300</b>), as the measurement data, the image data representing the result of imaging the region in the portion of the eye using the second value of the imaging parameter and the received image data and the received measurement data are of the same imaging modality.</li>        </ul>        </li>        <li id="ul0004-0035" num="0217">E35. The apparatus (<b>100</b>) of any of E28 to E34, wherein the instruction generating module (<b>130</b>) is further configured to generate instructions for controlling a display unit (<b>215</b>) to display the determined location of the region (<b>410</b>) in the image (<b>400</b>) of the portion of the eye and a representation of the received measurement data.</li>        <li id="ul0004-0036" num="0218">E36. The apparatus (<b>100</b>) of any of E28 to E35, wherein the learning algorithm (<b>630</b>) is a supervised learning algorithm.</li>        <li id="ul0004-0037" num="0219">E37. The apparatus (<b>100</b>) of E36, wherein the supervised learning algorithm comprises a neural network, and the search module (<b>120</b>) is configured to search for the region indicative of the pathology in the image by deconstructing the neural network.</li>        <li id="ul0004-0038" num="0220">E38. The apparatus (<b>100</b>) of E37, wherein the neural network is a convolutional neural network, and the search module (<b>120</b>) is configured to deconstruct the neural network by:        <ul id="ul0035" list-style="none">            <li id="ul0035-0001" num="0221">performing, for each of a plurality of different sections of the image (<b>400</b>) that is defined by the received image data, processes of:            <ul id="ul0036" list-style="none">                <li id="ul0036-0001" num="0222">masking the section of the image (<b>400</b>) to generate a masked image;</li>                <li id="ul0036-0002" num="0223">searching for the region in the masked image by processing image data defining the masked image using the learning algorithm; and</li>                <li id="ul0036-0003" num="0224">determining a difference between a result of the search performed using the image data defining the masked image and a result of a search performed using the received image data; and</li>            </ul>            </li>            <li id="ul0035-0002" num="0225">determining, as the location of the region (<b>410</b>) in the image (<b>400</b>), a location of a section for which the determined difference is largest.</li>        </ul>        </li>        <li id="ul0004-0039" num="0226">E39. The apparatus (<b>100</b>) of E37, wherein the neural network is a convolutional neural network, and the search module (<b>120</b>) is configured to deconstruct the convolutional neural network by:        <ul id="ul0037" list-style="none">            <li id="ul0037-0001" num="0227">determining a relevance of each input variable of the neural network to an output of the neural network by applying a Taylor decomposition to each layer of the neural network, from a top layer of the neural network to an input layer of the neural network; and</li>            <li id="ul0037-0002" num="0228">determining the location of the region (<b>410</b>) in the image (<b>400</b>) based on at least one section of the received image data corresponding to the most relevant input variables of the neural network.</li>        </ul>        </li>        <li id="ul0004-0040" num="0229">E40. The apparatus (<b>100</b>) of E37, wherein the neural network is a convolutional neural network, and the search module (<b>120</b>) is configured to deconstruct the convolutional neural network by determining a deconvolution of the convolutional neural network.</li>        <li id="ul0004-0041" num="0230">E41. An apparatus (<b>800</b>) for searching for the presence of a pathology in an image (<b>400</b>) of a portion of an eye acquired by an ocular imaging system (<b>520</b>), the apparatus (<b>800</b>) comprising:        <ul id="ul0038" list-style="none">            <li id="ul0038-0001" num="0231">a receiver module (<b>810</b>) configured to receive image data defining the image (<b>400</b>);</li>            <li id="ul0038-0002" num="0232">a search module (<b>820</b>) configured to search for the presence of at least one of a plurality of different types of pathology in the image (<b>400</b>) by processing the received image data using a learning algorithm (<b>630</b>) trained on image data defining images (<b>604</b>) of healthy eyes, and images (<b>601</b>, <b>602</b>, <b>603</b>) of unhealthy eyes each having a respective one of the different types of pathology; and</li>            <li id="ul0038-0003" num="0233">an instruction generating module (<b>830</b>) configured to perform, in response to at least one of the plurality of different types of pathology being found to be present in the image (<b>400</b>) by the search module (<b>820</b>), processes of:            <ul id="ul0039" list-style="none">                <li id="ul0039-0001" num="0234">selecting, for each of at least one type of pathology found to be present in the image (<b>400</b>), a respective one of a plurality of different types of measurement modality which is to be used to perform a measurement on the portion of the eye; and generating, for each of the at least one type of pathology found to be present in</li>                <li id="ul0039-0002" num="0235">the image (<b>400</b>), a respective instruction for an eye measurement apparatus (<b>300</b>) of the respective selected measurement modality to perform the measurement on the portion of the eye,</li>            </ul>            </li>            <li id="ul0038-0004" num="0236">wherein the receiver module (<b>810</b>) is further configured to receive measurement data of the measurement performed by the eye measurement apparatus (<b>300</b>) of each selected measurement modality.</li>        </ul>        </li>        <li id="ul0004-0042" num="0237">E42. The apparatus (<b>800</b>) of E41, wherein the search module (<b>820</b>) is further configured to perform, in response to finding at least one of the plurality of different types of pathology to be present in the image (<b>400</b>), a process of:        <ul id="ul0040" list-style="none">            <li id="ul0040-0001" num="0238">for each of the at least one of the different types of pathology found to be present in the image (<b>400</b>), searching for a respective region (<b>410</b>) in the image (<b>400</b>) that is indicative of the respective type of pathology, by processing the received image data using the learning algorithm (<b>630</b>), and recording a location of the respective region (<b>410</b>) in the image (<b>400</b>), and</li>            <li id="ul0040-0002" num="0239">the instruction generating module (<b>830</b>) is configured to generate a respective instruction for the eye measurement apparatus (<b>300</b>) of each selected measurement modality to perform a measurement on the eye using a reference point for locating the measurement which is based on the respective location.</li>        </ul>        </li>        <li id="ul0004-0043" num="0240">E43. The apparatus (<b>800</b>) of E42, wherein the instruction generating module (<b>830</b>) is configured to perform, in response to at least one of the plurality of different types of pathology being found to be present in the image (<b>400</b>) by the search module (<b>820</b>), and for each of the at least one of the different types of pathology found to be present in the image (<b>400</b>), processes of:        <ul id="ul0041" list-style="none">            <li id="ul0041-0001" num="0241">selecting a respective one of a plurality of different types of imaging modality which is to be used to image the portion of the eye as the respective one of the plurality of different types of measurement modality; and</li>            <li id="ul0041-0002" num="0242">generating, as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected measurement modality, a respective instruction for an eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, using a reference point for locating a region of the eye to be imaged which is based on the respective recorded location, and wherein</li>            <li id="ul0041-0003" num="0243">the receiver module (<b>810</b>) is configured to receive respective image data of the imaging performed by the eye measurement apparatus (<b>300</b>) of the selected imaging modality as the measurement data of the measurement performed by the eye measurement apparatus (<b>300</b>).</li>        </ul>        </li>        <li id="ul0004-0044" num="0244">E44. The apparatus (<b>800</b>) of E43, wherein:        <ul id="ul0042" list-style="none">            <li id="ul0042-0001" num="0245">the instruction generating module (<b>830</b>) is configured to generate, in a case where one of the plurality of different types of pathology found to be present in the image (<b>400</b>) is glaucoma, and as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform an optical coherence tomography, OCT, scan of the region in the portion of the eye, and the receiver module (<b>810</b>) is configured to receive, as the measurement data, image data of the OCT scan;</li>            <li id="ul0042-0002" num="0246">the instruction generating module (<b>830</b>) is configured to generate, in a case where one of the plurality of different types of pathology found to be present in the image (<b>400</b>) is severe diabetic retinopathy, and as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform an optical coherence tomography, OCT, scan of a region of a retina of the eye, and the receiver module (<b>810</b>) is configured to receive, as the measurement data, image data of the OCT scan;</li>            <li id="ul0042-0003" num="0247">the instruction generating module (<b>830</b>) is configured to generate, in a case where one of the plurality of different types of pathology found to be present in the image (<b>400</b>) is a tumour, and as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform a high-density optical coherence tomography, OCT, B-scan of the region in the portion of the eye, and the receiver module (<b>810</b>) is configured to receive, as the measurement data, image data of the high-density OCT B-scan;</li>            <li id="ul0042-0004" num="0248">the instruction generating module (<b>830</b>) is configured to generate, in a case where the one of the plurality of different types of pathology found to be present in the image (<b>400</b>) is drusen, and as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform an optical coherence tomography, OCT, B-scan of the region in the portion of the eye, and the receiver module (<b>810</b>) is configured to receive, as the measurement data, image data of the OCT B-scan; and</li>            <li id="ul0042-0005" num="0249">the instruction generating module (<b>830</b>) is configured to generate, in a case where the one of the plurality of different types of pathology found to be present in the image (<b>400</b>) is oedema, and as the respective instruction for the eye measurement apparatus (<b>300</b>) of the selected imaging modality to image the portion of the eye, an instruction for the eye measurement apparatus (<b>300</b>) to perform an optical coherence tomography, OCT, scan of the region in the portion of the eye, and the receiver module (<b>810</b>) is configured to receive, as the measurement data, image data of the OCT scan.</li>        </ul>        </li>        <li id="ul0004-0045" num="0250">E45. The apparatus (<b>800</b>) of E41, wherein the instruction generating module (<b>830</b>) is configured to generate, in response to at least one of the plurality of different types of pathology being found to be present in the image by the search module (<b>820</b>), an instruction for an eye measurement apparatus (<b>300</b>) of a selected measurement modality to measure a functional response of the eye to light stimulation.</li>        <li id="ul0004-0046" num="0251">E46. The apparatus (<b>800</b>) of any of E42 to E44, wherein the instruction generating module (<b>830</b>) is further configured to generate instructions for controlling a display unit (<b>215</b>) to display the recorded location of the region (<b>410</b>) in the image (<b>400</b>) of the portion of the eye and a representation of the received measurement data.</li>        <li id="ul0004-0047" num="0252">E47. The apparatus (<b>800</b>) of any of E42 to E44, wherein the learning algorithm (<b>630</b>) is a supervised learning algorithm.</li>        <li id="ul0004-0048" num="0253">E48. The apparatus (<b>800</b>) of E47, wherein the supervised learning algorithm comprises a neural network, and the search module (<b>820</b>) is configured to search for a region (<b>410</b>) indicative of one of the different types of pathology found to be present in the image (<b>400</b>) by deconstructing the neural network.</li>        <li id="ul0004-0049" num="0254">E49. The apparatus of E48, wherein the neural network is a convolutional neural network, and the search module (<b>820</b>) is configured to deconstruct the neural network by:        <ul id="ul0043" list-style="none">            <li id="ul0043-0001" num="0255">performing, for each of a plurality of different sections of the image (<b>400</b>) that is defined by the received image data, processes of:            <ul id="ul0044" list-style="none">                <li id="ul0044-0001" num="0256">masking the section of the image (<b>400</b>) to generate a masked image;</li>                <li id="ul0044-0002" num="0257">searching for the region in the masked image by processing image data defining the masked image using the learning algorithm; and</li>                <li id="ul0044-0003" num="0258">determining a difference between a result of the search performed using the image data defining the masked image and a result of a search performed using the received image data; and</li>            </ul>            </li>            <li id="ul0043-0002" num="0259">determining, as the location to be recorded, a location of a section for which the determined difference is largest.</li>        </ul>        </li>        <li id="ul0004-0050" num="0260">E50. The apparatus (<b>800</b>) of E48, wherein the neural network is a convolutional neural network, and the search module (<b>820</b>) is configured to deconstruct the convolutional neural network by:        <ul id="ul0045" list-style="none">            <li id="ul0045-0001" num="0261">determining a relevance of each input variable of the neural network to an output of the neural network by applying a Taylor decomposition to each layer of the neural network, from a top layer of the neural network to an input layer of the neural network; and</li>            <li id="ul0045-0002" num="0262">determining the location to be recorded based on at least one section of the received image data corresponding to the most relevant input variables of the neural network.</li>        </ul>        </li>        <li id="ul0004-0051" num="0263">E51. The apparatus (<b>800</b>) of E48, wherein the neural network is a convolutional neural network, and the search module (<b>820</b>) is configured to deconstruct the convolutional neural network by determining a deconvolution of the convolutional neural network.</li>        <li id="ul0004-0052" num="0264">E52. An apparatus for searching for a region indicative of a pathology in an image (<b>400</b>) of a portion of an eye acquired by an ocular imaging system (<b>520</b>; <b>720</b>), the apparatus (<b>100</b>) comprising a processor and a memory storing computer program instructions which, when executed by the processor, cause the processor to perform a method as set out in at least one of E1 to E24.</li>    </ul>    </li></ul></p><p id="p-0101" num="0265">In the foregoing description, example aspects are described with reference to several example embodiments. Accordingly, the specification should be regarded as illustrative, rather than restrictive. Similarly, the figures illustrated in the drawings, which highlight the functionality and advantages of the example embodiments, are presented for example purposes only. The architecture of the example embodiments is sufficiently flexible and configurable, such that it may be utilized (and navigated) in ways other than those shown in the accompanying figures.</p><p id="p-0102" num="0266">Software embodiments of the examples presented herein may be provided as, a computer program, or software, such as one or more programs having instructions or sequences of instructions, included or stored in an article of manufacture such as a machine-accessible or machine-readable medium, an instruction store, or computer-readable storage device, each of which can be non-transitory, in one example embodiment. The program or instructions on the non-transitory machine-accessible medium, machine-readable medium, instruction store, or computer-readable storage device, may be used to program a computer system or other electronic device. The machine- or computer-readable medium, instruction store, and storage device may include, but are not limited to, floppy diskettes, optical disks, and magneto-optical disks or other types of media/machine-readable medium/instruction store/storage device suitable for storing or transmitting electronic instructions. The techniques described herein are not limited to any particular software configuration. They may find applicability in any computing or processing environment. The terms &#x201c;computer-readable&#x201d;, &#x201c;machine-accessible medium&#x201d;, &#x201c;machine-readable medium&#x201d;, &#x201c;instruction store&#x201d;, and &#x201c;computer-readable storage device&#x201d; used herein shall include any medium that is capable of storing, encoding, or transmitting instructions or a sequence of instructions for execution by the machine, computer, or computer processor and that causes the machine/computer/computer processor to perform any one of the methods described herein. Furthermore, it is common in the art to speak of software, in one form or another (e.g., program, procedure, process, application, module, unit, logic, and so on), as taking an action or causing a result. Such expressions are merely a shorthand way of stating that the execution of the software by a processing system causes the processor to perform an action to produce a result.</p><p id="p-0103" num="0267">Some embodiments may also be implemented by the preparation of application-specific integrated circuits, field-programmable gate arrays, or by interconnecting an appropriate network of conventional component circuits.</p><p id="p-0104" num="0268">Some embodiments include a computer program product. The computer program product may be a storage medium or media, instruction store(s), or storage device(s), having instructions stored thereon or therein which can be used to control, or cause, a computer or computer processor to perform any of the procedures of the example embodiments described herein. The storage medium/instruction store/storage device may include, by example and without limitation, an optical disc, a ROM, a RAM, an EPROM, an EEPROM, a DRAM, a VRAM, a flash memory, a flash card, a magnetic card, an optical card, nanosystems, a molecular memory integrated circuit, a RAID, remote data storage/archive/warehousing, and/or any other type of device suitable for storing instructions and/or data.</p><p id="p-0105" num="0269">Stored on any one of the computer-readable medium or media, instruction store(s), or storage device(s), some implementations include software for controlling both the hardware of the system and for enabling the system or microprocessor to interact with a human user or other mechanism utilizing the results of the example embodiments described herein. Such software may include without limitation device drivers, operating systems, and user applications. Ultimately, such computer-readable media or storage device(s) further include software for performing example aspects of the invention, as described above.</p><p id="p-0106" num="0270">Included in the programming and/or software of the system are software modules for implementing the procedures described herein. In some example embodiments herein, a module includes software, although in other example embodiments herein, a module includes hardware, or a combination of hardware and software.</p><p id="p-0107" num="0271">While various example embodiments of the present invention have been described above, it should be understood that they have been presented by way of example, and not limitation. It will be apparent to persons skilled in the relevant art(s) that various changes in form and detail can be made therein. Thus, the present invention should not be limited by any of the above described example embodiments, but should be defined only in accordance with the following claims and their equivalents.</p><p id="p-0108" num="0272">Further, the purpose of the Abstract is to enable the Patent Office and the public generally, and especially the scientists, engineers and practitioners in the art who are not familiar with patent or legal terms or phraseology, to determine quickly from a cursory inspection the nature and essence of the technical disclosure of the application. The Abstract is not intended to be limiting as to the scope of the example embodiments presented herein in any way. It is also to be understood that the procedures recited in the claims need not be performed in the order presented.</p><p id="p-0109" num="0273">While this specification contains many specific embodiment details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular embodiments described herein. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable sub-combination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a sub-combination or variation of a sub-combination.</p><p id="p-0110" num="0274">In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.</p><p id="p-0111" num="0275">Having now described some illustrative embodiments and embodiments, it is apparent that the foregoing is illustrative and not limiting, having been presented by way of example. In particular, although many of the examples presented herein involve specific combinations of apparatus or software elements, those elements may be combined in other ways to accomplish the same objectives. Acts, elements and features discussed only in connection with one embodiment are not intended to be excluded from a similar role in other embodiments or embodiments.</p><p id="p-0112" num="0276">The apparatus and computer programs described herein may be embodied in other specific forms without departing from the characteristics thereof. The foregoing embodiments are illustrative rather than limiting of the described systems and methods. Scope of the apparatus and computer programs described herein is thus indicated by the appended claims, rather than the foregoing description, and changes that come within the meaning and range of equivalency of the claims are embraced therein.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-01-17" num="01-17"><claim-text><b>1</b>-<b>17</b>. (canceled)</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A computer-implemented method of searching for a region indicative of a pathology in an image of a portion of an eye acquired by an ocular imaging system, the method comprising:<claim-text>receiving image data defining the image;</claim-text><claim-text>searching for the region in the image by processing the received image data using a learning algorithm trained on image data defining images of the portion of healthy eyes, and image data defining images of the portion of unhealthy eyes each having at least one region that is indicative of the pathology; and</claim-text><claim-text>in case a region in the image that is indicative of the pathology is found in the searching:<claim-text>determining a location of the region in the image;</claim-text><claim-text>generating an instruction for an eye measurement apparatus to perform a measurement on the portion of the eye to generate measurement data, using a reference point based on the determined location for setting a location of the measurement on the portion of the eye; and</claim-text><claim-text>receiving the measurement data from the eye measurement apparatus, wherein</claim-text></claim-text><claim-text>the received image data represents a result of imaging the portion of the eye using a first value of an imaging parameter, the imaging parameter being one of an imaging resolution, an aperture size and wavelength used in the imaging,</claim-text><claim-text>the eye measurement apparatus is configured to perform, as the measurement on the portion of the eye, an image capture process using a second value of the imaging parameter to image a region in the portion of the eye, and to acquire, as the measurement data, image data representing a result of imaging the region using the second value of the imaging parameter, wherein the second value of the imaging parameter is different from the first value of the imaging parameter, and the received image data and the acquired image data are of the same imaging modality, and</claim-text><claim-text>in the case that a region in the image that is indicative of the pathology is found in the searching, the method comprises:<claim-text>generating, as the instruction for the eye measurement apparatus to perform the measurement on the portion of the eye, an instruction for the eye measurement apparatus to perform the image capture process, using the reference point for setting, as the location of the measurement, a location of a region in the portion of the eye to be imaged in the image capture process; and</claim-text><claim-text>receiving from the eye measurement apparatus, as the measurement data, the image data representing the result of imaging the region in the portion of the eye using the second value of the imaging parameter.</claim-text></claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer-implemented method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising generating instructions for controlling a display unit to display the location of the region in the image of the portion of the eye and a representation of the received measurement data.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer-implemented method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the learning algorithm is a supervised learning algorithm.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The computer-implemented method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the supervised learning algorithm comprises a neural network, and the region indicative of the pathology is searched for in the image by deconstructing the neural network.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The computer-implemented method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the neural network is a convolutional neural network, and the neural network is deconstructed by:<claim-text>performing, for each of a plurality of different sections of the image that is defined by the received image data, processes of:</claim-text><claim-text>masking the section of the image to generate a masked image;</claim-text><claim-text>searching for the region in the masked image by processing image data defining the masked image using the learning algorithm; and</claim-text><claim-text>determining a difference between a result of the search performed using the image data defining the masked image and a result of a search performed using the received image data; and</claim-text><claim-text>determining, as the location of the region in the image, a location of a section for which the determined difference is largest.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The computer-implemented method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the neural network is a convolutional neural network, and the convolutional neural network is deconstructed by:<claim-text>determining a relevance of each input variable of the neural network to an output of the neural network by applying a Taylor decomposition to each layer of the neural network, from a top layer of the neural network to an input layer of the neural network; and</claim-text><claim-text>determining the location of the region in the image based on at least one section of the received image data corresponding to the most relevant input variables of the neural network.</claim-text></claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The computer-implemented method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the neural network is a convolutional neural network, and the convolutional neural network is deconstructed by determining a deconvolution of the convolutional neural network.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. A non-transitory computer-readable storage medium storing a computer program which, when executed by a processor, causes the processor to execute a computer-implemented method according to <claim-ref idref="CLM-00018">claim 18</claim-ref>.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. An apparatus for searching for a region indicative of a pathology in an image of a portion of an eye acquired by an ocular imaging system, the apparatus comprising:<claim-text>a receiver module configured to receive image data defining the image;</claim-text><claim-text>a search module configured to search for the region in the image by processing the received image data using a learning algorithm trained on image data defining images of the portion of healthy eyes, and image data defining images of the portion of unhealthy eyes each having at least one region that is indicative of the pathology; and</claim-text><claim-text>an instruction generating module configured to perform, in response to a region in the image that is indicative of the pathology being found by the search module, processes of:</claim-text><claim-text>determining a location of the region in the image; and</claim-text><claim-text>generating an instruction for an eye measurement apparatus to perform a measurement on the portion of the eye to generate measurement data, using a reference point based on the determined location for setting a location of the measurement on the portion of the eye,</claim-text><claim-text>wherein the receiver module is further configured to receive the measurement data from the eye measurement apparatus, wherein</claim-text><claim-text>the receiver module is configured to receive image data representing a result of imaging the portion of the eye using a first value of an imaging parameter, the imaging parameter being one of an imaging resolution, an aperture size and wavelength used in the imaging,</claim-text><claim-text>the instruction generating module is configured to perform, in response to a region in the image that is indicative of the pathology being found by the search module, a processes of generating, as the instruction for the eye measurement apparatus to perform the measurement on the portion of the eye, an instruction for the eye measurement apparatus to perform an image capture process using a second value of the imaging parameter to image a region in the portion of the eye, using the reference point for setting, as the location of the measurement, a location of a region in the portion of the eye to be imaged in the image capture process, wherein the second value of the imaging parameter is different from the first value of the imaging parameter, and</claim-text><claim-text>the receiver module is configured to receive from the eye measurement apparatus, as the measurement data, the image data representing the result of imaging the region in the portion of the eye using the second value of the imaging parameter and the received image data and the received measurement data are of the same imaging modality.</claim-text></claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The apparatus of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the instruction generating module is further configured to generate instructions for controlling a display unit to display the determined location of the region in the image of the portion of the eye and a representation of the received measurement data.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The apparatus of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the learning algorithm is a supervised learning algorithm.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The apparatus of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the supervised learning algorithm comprises a neural network, and the search module is configured to search for the region indicative of the pathology in the image by deconstructing the neural network.</claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The apparatus of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the neural network is a convolutional neural network, and the search module is configured to deconstruct the neural network by:<claim-text>performing, for each of a plurality of different sections of the image that is defined by the received image data, processes of:<claim-text>masking the section of the image to generate a masked image;</claim-text><claim-text>searching for the region in the masked image by processing image data defining the masked image using the learning algorithm; and</claim-text><claim-text>determining a difference between a result of the search performed using the image data defining the masked image and a result of a search performed using the received image data; and</claim-text></claim-text><claim-text>determining, as the location of the region in the image, a location of a section for which the determined difference is largest.</claim-text></claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The apparatus of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the neural network is a convolutional neural network, and the search module is configured to deconstruct the convolutional neural network by:<claim-text>determining a relevance of each input variable of the neural network to an output of the neural network by applying a Taylor decomposition to each layer of the neural network, from a top layer of the neural network to an input layer of the neural network; and</claim-text><claim-text>determining the location of the region in the image based on at least one section of the received image data corresponding to the most relevant input variables of the neural network.</claim-text></claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The apparatus of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the neural network is a convolutional neural network, and the search module is configured to deconstruct the convolutional neural network by determining a deconvolution of the convolutional neural network.</claim-text></claim></claims></us-patent-application>