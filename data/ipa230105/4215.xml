<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004216A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004216</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17305219</doc-number><date>20210701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>013</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6218</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>0093</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>0101</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>0172</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>2027</main-group><subgroup>0138</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>2027</main-group><subgroup>0178</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">EYE GAZE CLASSIFICATION</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>GOOGLE LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Rodgers</last-name><first-name>Ivana Tosic</first-name><address><city>Redwood City</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Fanello</last-name><first-name>Sean Ryan Francesco</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Bouaziz</last-name><first-name>Sofien</first-name><address><city>Los Gatos</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Pandey</last-name><first-name>Rohit Kumar</first-name><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Aboussouan</last-name><first-name>Eric</first-name><address><city>Campbell</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Kowdle</last-name><first-name>Adarsh Prakash Murthy</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques of tracking a user's gaze includes identifying a region of a display at which a gaze of a user is directed, the region including a plurality of pixels. By determining a region rather than a point, when the regions correspond to elements of a user interface, the improved technique enables a system to activate the element to which a determined region is selected. In some implementations, the system makes the determination using a classification engine including a convolutional neural network; such an engine takes as input images of the user's eye and outputs a list of probabilities that the gaze is directed to each of the regions.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="68.07mm" wi="120.82mm" file="US20230004216A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="120.99mm" wi="122.85mm" file="US20230004216A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="196.93mm" wi="164.25mm" file="US20230004216A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="181.27mm" wi="161.97mm" file="US20230004216A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="181.27mm" wi="159.34mm" orientation="landscape" file="US20230004216A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="217.68mm" wi="172.38mm" orientation="landscape" file="US20230004216A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="213.36mm" wi="170.35mm" orientation="landscape" file="US20230004216A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="225.72mm" wi="165.35mm" orientation="landscape" file="US20230004216A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="193.97mm" wi="123.36mm" file="US20230004216A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="240.96mm" wi="161.12mm" orientation="landscape" file="US20230004216A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">This description relates to determining a region of a display at which a user's eye gaze is directed.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Some augmented reality (AR) systems track eye gaze direction, i.e., a direction at which the eyes of a user are oriented. For example, an AR system may include smartglasses for displaying content to the user on a transparent display. Some smartglasses include a camera on the glasses frame that is configured to produce images of a user's eyes for tracking an eye gaze direction.</p><p id="p-0004" num="0003">Such an AR system may track eye gaze direction to enable a user interface on the transparent display. For example, there may be first content and second content rendered on the transparent display. The AR system may deduce whether the user is looking at the first content or the second content by determining the eye gaze direction of the user.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">Implementations disclosed herein provide an improved technique for tracking a user's gaze with respect to a display. In some implementations, the display is a transparent display such as that embedded in smartglasses used in an AR system. In some implementations, the display is that used in a mobile computing system, e.g., smartphone, tablet computer, or the like. Rather than track the user's gaze to a particular point on the display, however, the improved technique involves determining at which region of the display the user's gaze is directed. By determining a region rather than a point, when the regions correspond to elements of a user interface, the improved technique enables a system to activate the element to which a determined region is selected. In some implementations, the system makes the determination using a classification engine including a convolutional neural network; such an engine takes as input images of the user's eye and outputs a list of probabilities that the gaze is directed to each of the regions.</p><p id="p-0006" num="0005">In one general aspect, a method can include receiving image data representing at least one image of an eye of a user looking at a display at an instant of time, the display including a plurality of regions and being configured to operate in an augmented reality (AR) application, each of the plurality of regions including a plurality of pixels and corresponding to a respective element of a user interface. The method can also include identifying, based on the image data, a region of the plurality of regions of the display at which a gaze of a user is directed at the instant of time, the identifying including inputting the at least one image of the eye of the user into a classification engine configured to classify the gaze as being directed to one of the plurality of regions. The method can further include activating the element of the user interface to which the identified region corresponds.</p><p id="p-0007" num="0006">In another general aspect, a computer program product comprises a non-transitory storage medium, the computer program product including code that, when executed by processing circuitry of a computing device, causes the processing circuitry to perform a method. The method can include receiving image data representing at least one image of an eye of a user looking at a display at an instant of time, the display including a plurality of regions and being configured to operate in an augmented reality (AR) application, each of the plurality of regions including a plurality of pixels and corresponding to a respective element of a user interface. The method can also include identifying, based on the image data, a region of the plurality of regions of the display at which a gaze of a user is directed at the instant of time, the identifying including inputting the at least one image of the eye of the user into a classification engine configured to classify the gaze as being directed to one of the plurality of regions. The method can further include activating the element of the user interface to which the identified region corresponds.</p><p id="p-0008" num="0007">In another general aspect, an electronic apparatus comprises memory and controlling circuitry coupled to the memory. The controlling circuitry can be configured to receive image data representing at least one image of an eye of a user looking at a display at an instant of time, the display including a plurality of regions and being configured to operate in an augmented reality (AR) application, each of the plurality of regions including a plurality of pixels and corresponding to a respective element of a user interface. The controlling circuitry can also be configured to identify, based on the image data, a region of the plurality of regions of the display at which a gaze of a user is directed at the instant of time, the identifying including inputting the at least one image of the eye of the user into a classification engine configured to classify the gaze as being directed to one of the plurality of regions. The controlling circuitry can further be configured to activate the element of the user interface to which the identified region corresponds.</p><p id="p-0009" num="0008">The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a diagram that illustrates example smartglasses used in an augmented reality (AR) system.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a diagram that illustrates an example electronic environment in which improved techniques described herein may be implemented.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a diagram that illustrates example regions on a display, including an activated region.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a diagram that illustrates example regions on a display, including an activated region; in this case, the regions may not be contiguous.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is a diagram that illustrates an example convolutional neural network (CNN) configured to classify an image of a user's eye as having a gaze directed to a particular region.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is a diagram that illustrates an example convolutional neural network (CNN) configured to classify an image of a user's eye and determine a particular point where the gaze is directed.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram that illustrates example convolutional layers forming another branch of a CNN, in this case configured to adapt for different region or tile geometries.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart that illustrates an example method of performing the improved technique within the electronic environment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram that illustrates an example of a computer device and a mobile computer device that can be used to implement the described techniques.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">Conventional eye gaze direction trackers are configured to estimate a pixel of the transparent display at which the eye of the user is most likely to be looking. For example, a conventional eye gaze direction tracker may derive a pixel-based heatmap for the transparent display, with each pixel having a color based on a probability that the user is looking at that pixel.</p><p id="p-0020" num="0019">Because the output of the conventional eye gaze direction trackers is directed to an identification of a pixel of the transparent display, a technical problem with the above-described conventional approaches to tracking a user's gaze is that the conventional eye gaze direction trackers can be resource-intensive and error-prone. For example, while a conventional eye gaze direction tracker may identify a pixel at which a user is most likely looking, such a pixel may not be identified with any content rendered on the transparent display. Accordingly, a system employing such an eye gaze direction tracker may also need to map that pixel to displayed content; such a mapping may consume computing resources needed for other tasks and may interfere with the user's experience.</p><p id="p-0021" num="0020">In accordance with the implementations described herein, a technical solution to the above-described technical problem includes identifying a region of a display at which a gaze of a user is directed, the region including a plurality of pixels. By determining a region rather than a point, when the regions correspond to elements of a user interface, the improved technique enables a system to activate the element to which a determined region is selected. In some implementations, the system makes the determination using a classification engine including a convolutional neural network; such an engine takes as input images of the user's eye and outputs a list of probabilities that the gaze is directed to each of the regions.</p><p id="p-0022" num="0021">A technical advantage of disclosed implementations is that such implementations use fewer computing resources and is less error prone. For example, in some implementations a region may be associated with a user interface element, e.g., a window containing content rendered on the display; such an association uses less computing resources to activate the window on the display than mapping an identified pixel to such a window as done in the conventional eye gaze direction trackers.</p><p id="p-0023" num="0022">It is noted that, in contrast to the above-described conventional approaches, the output of the improved techniques is a vector of likelihoods corresponding to regions rather than individual pixels. The output is accordingly of much smaller size than that of the conventional approaches.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a diagram that illustrates example smartglasses <b>110</b> used in an augmented reality (AR) system as a head. <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> shows a world side <b>112</b>(<i>a</i>) of a transparent display <b>112</b> of the smartglasses <b>110</b>. The smartglasses <b>110</b> may be used as a head-mounted display (HMD) within an AR system. The smartglasses <b>110</b> include a frame <b>111</b>, with a transparent display <b>112</b> coupled in the frame <b>111</b>. In some implementations, an audio output device <b>113</b> is coupled to the frame <b>111</b>. In some implementations, a touch surface <b>114</b> allows for user control, input and the like of the smartglasses <b>110</b>. The smartglasses <b>110</b> may include a sensing system <b>116</b> including various sensing system devices and a control system <b>117</b> including various control system devices to facilitate operation of the smartglasses <b>110</b>. The control system <b>117</b> may include a processor <b>119</b> operably coupled to the components of the control system <b>117</b> and a communication module <b>115</b> providing for communication with external devices and/or networks. The smartglasses <b>110</b> may also include an image sensor <b>118</b> (i.e., a camera <b>118</b>), a depth sensor, a light sensor, and other such sensing devices. In some implementations, the image sensor <b>118</b>, or camera <b>118</b> is capable of capturing still and/or moving images, patterns, features, light and the like.</p><p id="p-0025" num="0024">It is noted that, in some implementations, the smartglasses <b>110</b> may be replaced with any sort of HMD that includes a transparent display, in which the form of the HMD is not necessarily wearable glasses or goggles. For example, one such HMD may take the form of a camera with a viewfinder configured to display AR content and allow viewing of the world-side environment.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a diagram that illustrates an example electronic environment <b>100</b> in which the above-described technical solution may be implemented. The computer <b>120</b> is configured to determine a region of a display at which a user's gaze is directed.</p><p id="p-0027" num="0026">The computer <b>120</b> includes a network interface <b>122</b>, one or more processing units <b>124</b>, and memory <b>126</b>. The network interface <b>122</b> includes, for example, Ethernet adaptors and the like, for converting electronic and/or optical signals received from the network <b>150</b> to electronic form for use by the computer <b>120</b>. The set of processing units <b>124</b> include one or more processing chips and/or assemblies. The memory <b>126</b> includes both volatile memory (e.g., RAM) and non-volatile memory, such as one or more ROMs, disk drives, solid state drives, and the like. The set of processing units <b>124</b> and the memory <b>126</b> together form control circuitry, which is configured and arranged to carry out various methods and functions as described herein.</p><p id="p-0028" num="0027">In some implementations, one or more of the components of the computer <b>120</b> can be, or can include processors (e.g., processing units <b>124</b>) configured to process instructions stored in the memory <b>126</b>. Examples of such instructions as depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref> include an input manager <b>130</b>, a classification manager <b>140</b>, and an activation manager. Further, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the memory <b>126</b> is configured to store various data, which is described with respect to the respective managers that use such data.</p><p id="p-0029" num="0028">The input manager <b>130</b> is configured to receive input data such as image data <b>132</b>, region data <b>134</b>, slippage data <b>136</b>, and user data <b>138</b>. In some implementations, the various input data are captured via hardware connected to a display, e.g., transparent display <b>112</b> (<figref idref="DRAWINGS">FIG. <b>1</b>A</figref>). For example, the hardware may include a camera, e.g., camera <b>118</b>, configured to capture images of a user's eye. In some implementations, the hardware includes any of a gyroscope, a magnetometer, a GPS receiver, and the like to acquire input data such as the slippage data <b>136</b> and user data <b>138</b>. In some implementations, the input manager <b>130</b> is configured to receive the input data over the network interface <b>122</b>.</p><p id="p-0030" num="0029">The image data <b>132</b> represents at least one image of a user's eye. The image data <b>132</b> is arranged for input into the classification manager <b>140</b>. In some implementations, the image data <b>132</b> represents a sequence of images of the user's eye for tracking a gaze direction. In some implementations, the sequence of images are frames of a video that tracks the motion of the user's eye.</p><p id="p-0031" num="0030">The region data <b>134</b> represents the regions of the display. Each of the regions includes a plurality of pixels of the display. In some implementations, each region corresponds with a respective element of a user interface, e.g., a window containing content to be viewed by the user. In some implementations, each region has a rectangular shape that includes an array of pixels. In some implementations, at least one region is non-contiguous and includes multiple rectangles. In some implementations, the region data <b>134</b> includes identifiers that identify each region.</p><p id="p-0032" num="0031">The slippage data <b>136</b> represents, for implementations in which the display is a transparent display used in smartglasses, parameter values corresponding to a degree of slippage of the smartglasses from a nominal location on a user's face. In a nominal configuration of a gaze tracking system, the eye is at a nominal location with a known (designed) pose relative to the display. During wearing, the position of the glasses can change from this nominal location (slipping on the nose, adjustment by the user, etc.). When slippage occurs, the pose between the eye and the eye tracking camera changes and the images of the user's eye appear different from the case of the nominal configuration. Moreover, the gaze angle will be different because the position of the display changes with the slippage of smartglasses. Accordingly, in some implementations, the slippage data <b>136</b> includes, as a parameter value, an estimate of the eye position relative to the camera. In some implementations, such a relative eye position is expressed as a three-dimensional vector. In some implementations, such a relative eye position is expressed as an angular coordinate on a sphere.</p><p id="p-0033" num="0032">The user data <b>138</b> represents parameter values describing physical differences between users that may affect the determination of the region at which the user's gaze is directed. For example, user differences in eye appearance, visual axis, head shape, etc. can all affect the accuracy of the determination. Accordingly, in some implementations, the user data <b>138</b> represents value of parameters that define eye appearance, visual axis, and head shape. In some implementations, such parameter values are deduced directly from the image data <b>134</b>.</p><p id="p-0034" num="0033">The classification manager <b>140</b> is configured to determine the region of the display at which the user's gaze is directed and thereby produce classification data <b>144</b> based on at least the image data <b>132</b>. In some implementations, the classification data <b>144</b> is based on any of the region data <b>134</b>, the slippage data <b>136</b>, and the user data <b>138</b>. The classification manager <b>140</b>, in some implementations, includes at least one branch of a convolutional neural network (CNN) that acts as a classification engine. The classification manager <b>140</b> includes a training manager <b>141</b> and an identification manager <b>144</b>. The classification data <b>144</b> includes training data <b>145</b> and identification data <b>146</b>.</p><p id="p-0035" num="0034">In some implementations, the CNN takes as input the image data <b>132</b>. In some implementations, the CNN also takes as input any of the region data <b>134</b>, the slippage data <b>136</b>, and the user data <b>138</b>. In some implementations, the CNN has a specified number of layers, one of which is an output layer producing an output. Such a neural network is configured to produce a classification result: which region of the display is the region at which the user's gaze is directed. In some implementations, the output includes a vector of values indicative of a likelihood that the user's gaze is directed to each region.</p><p id="p-0036" num="0035">The training manager <b>141</b> is configured to generate the classification engine, i.e., a neural network, based on the training data <b>145</b>. For example, in some implementations the training data <b>145</b> represents images of the user's eye along with corresponding identifiers of regions at which the user's gaze was directed when that image was taken. The training manager <b>141</b> then adjusts weights of nodes within hidden layers of the neural network to optimize a specified loss function. In some implementations, a loss function includes a categorical cross entropy appropriate for multi-class classification engines such as the CNN described above. In some implementations, a loss function includes a Kullback-Leibler divergence loss. In some implementations, the classification engine learns a calibration to different region layouts based on region data <b>134</b>; further details are shown with regard to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The weights adjusted by the training manager <b>141</b> and other data representing an architecture of the neural network are included in the classification data <b>144</b>.</p><p id="p-0037" num="0036">The identification manager <b>142</b> is configured to classify the image data <b>132</b> to identify a region of the display at which the user's gaze is directed. As shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the classification data <b>144</b> includes identification data <b>146</b> output from the identification manager <b>142</b>. In some implementations, the identification data <b>146</b> represents a vector of likelihood values, e.g., probabilities that the user's gaze is directed to each of the regions of the display.</p><p id="p-0038" num="0037">In some implementations, the identification manager <b>142</b> is configured to produce the classification data based on a loss function as described above used to train the classification engine. In some implementations, the identification manager <b>142</b> is configured to accept as any of input region data <b>134</b>, slippage data <b>136</b>, and user data <b>138</b>. In some implementations, the training data <b>145</b> includes region, slippage, and user data coupled with identifiers of regions at which the user's gaze is directed. In some implementations, each of the above type of data is used to generate additional branches of the classification engine; further details are shown with regard to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0039" num="0038">In some implementations, each of the regions described in the region data <b>134</b> corresponds to an element of a user interface, e.g., a window. The activation manager <b>150</b> is configured to activate a user interface element on the display in response to that user interface element corresponding to a region determined to be that at which the user's gaze is directed. For example, when it is determined that a user's gaze is directed to a region of the display corresponding to a window, e.g., the window is contained in the region, the window is the region, etc., the activation manager <b>150</b> is configured to activate the window, i.e., make the window active by highlighting its title bar while dimming the title bars of the other windows. In the implementations involving the display being embedded in smartglasses, a user may perform operations on content displayed in the window using, e.g., voice commands. When the user directs their gaze toward another region and the classification manager <b>140</b> identifies the other region as the region at which the user's gaze is directed, the activation manager <b>150</b> activates the window corresponding to the other region.</p><p id="p-0040" num="0039">The components (e.g., modules, processing units <b>124</b>) of the user device <b>120</b> can be configured to operate based on one or more platforms (e.g., one or more similar or different platforms) that can include one or more types of hardware, software, firmware, operating systems, runtime libraries, and/or so forth. In some implementations, the components of the computer <b>120</b> can be configured to operate within a cluster of devices (e.g., a server farm). In such an implementation, the functionality and processing of the components of the computer <b>120</b> can be distributed to several devices of the cluster of devices.</p><p id="p-0041" num="0040">The components of the computer <b>120</b> can be, or can include, any type of hardware and/or software configured to process attributes. In some implementations, one or more portions of the components shown in the components of the computer <b>120</b> in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> can be, or can include, a hardware-based module (e.g., a digital signal processor (DSP), a field programmable gate array (FPGA), a memory), a firmware module, and/or a software-based module (e.g., a module of computer code, a set of computer-readable instructions that can be executed at a computer). For example, in some implementations, one or more portions of the components of the computer <b>120</b> can be, or can include, a software module configured for execution by at least one processor (not shown). In some implementations, the functionality of the components can be included in different modules and/or different components than those shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, including combining functionality illustrated as two components into a single component.</p><p id="p-0042" num="0041">Although not shown, in some implementations, the components of the computer <b>120</b> (or portions thereof) can be configured to operate within, for example, a data center (e.g., a cloud computing environment), a computer system, one or more server/host devices, and/or so forth. In some implementations, the components of the computer <b>120</b> (or portions thereof) can be configured to operate within a network. Thus, the components of the computer <b>120</b> (or portions thereof) can be configured to function within various types of network environments that can include one or more devices and/or one or more server devices. For example, the network can be, or can include, a local area network (LAN), a wide area network (WAN), and/or so forth. The network can be, or can include, a wireless network and/or wireless network implemented using, for example, gateway devices, bridges, switches, and/or so forth. The network can include one or more segments and/or can have portions based on various protocols such as Internet Protocol (IP) and/or a proprietary protocol. The network can include at least a portion of the Internet.</p><p id="p-0043" num="0042">In some implementations, one or more of the components of the computer <b>120</b> can be, or can include, processors configured to process instructions stored in a memory. For example, an input manager <b>130</b> (and/or a portion thereof), a classification manager <b>140</b> (and/or a portion thereof), and an activation manager <b>150</b> (and/or a portion thereof can be a combination of a processor and a memory configured to execute instructions related to a process to implement one or more functions.</p><p id="p-0044" num="0043">In some implementations, the memory <b>126</b> can be any type of memory such as a random-access memory, a disk drive memory, flash memory, and/or so forth. In some implementations, the memory <b>126</b> can be implemented as more than one memory component (e.g., more than one RAM component or disk drive memory) associated with the components of the VR server computer <b>120</b>. In some implementations, the memory <b>126</b> can be a database memory. In some implementations, the memory <b>126</b> can be, or can include, a non-local memory. For example, the memory <b>126</b> can be, or can include, a memory shared by multiple devices (not shown). In some implementations, the memory <b>126</b> can be associated with a server device (not shown) within a network and configured to serve the components of the computer <b>120</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the memory <b>126</b> is configured to store various data, including image training data <b>131</b>, reference object data <b>136</b> and prediction engine data <b>150</b>.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a diagram that illustrates example regions <b>220</b> (<b>1</b>-<b>4</b>) on a display <b>200</b> including an activated region <b>220</b> (<b>1</b>). In some implementations, the display <b>200</b> is a transparent display embedded in smartglasses for an AR application. In some implementations, the display <b>200</b> is a display on a portable computing device such as a smartphone or a tablet computer.</p><p id="p-0046" num="0045">The display <b>200</b> includes an array of pixels <b>210</b>; each pixel represents a color or grayscale level that is a building block of displayed content. Each of the regions <b>220</b> (<b>1</b>-<b>4</b>) includes a respective array of pixels. As shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the regions <b>210</b> (<b>1</b>-<b>4</b>) have rounded corners but this is by no means a requirement.</p><p id="p-0047" num="0046">The region <b>220</b>(<b>1</b>) is shown as identified as being the region at which the user's gaze is directed, while the other regions <b>220</b>(<b>2</b>-<b>4</b>) are not identified as such. This means that it was determined by the identification manager <b>142</b> that a user's gaze was directed to the region <b>220</b>(<b>1</b>). The classification engine <b>140</b> is configured to perform the identification of a region in real time so that regions are identified with very little latency when the user changes the direction of their gaze. Accordingly, the arrangement of the regions shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> implies that the user is currently looking at region <b>220</b>(<b>1</b>).</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a diagram that illustrates example regions <b>260</b>(<b>1</b>-<b>4</b>) on a display, including an activated region <b>260</b>(<b>1</b>); in this case, the regions may not be contiguous. For example, as shown in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, the region <b>260</b>(<b>1</b>) includes two separate subregions separate from one another. Such a noncontiguous region may occur, for example, when the subregions correspond to different windows in a single application. In some implementations, the region <b>260</b>(<b>1</b>) may not have a rectangular shape but is contiguous; in such an implementation, the region may be decomposed into several rectangles or defined as a polygon over the grid of pixels 210.</p><p id="p-0049" num="0048">It is noted that the identification manager <b>142</b> does not require the eye tracker to provide the exact pixel of the gaze, but only to find the region on which the gaze is directed. In the examples shown in <figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref>, the camera would need to provide input data if the gaze is on regions <b>220</b>(<b>1</b>-<b>4</b>). By formulating the problem this way, the accuracy requirements for tracking a user's gaze may be relaxed to only the accuracy needed for the given user interface design. Furthermore, by doing so the computation time, power consumption, memory can be reduced and possibly make the gaze tracking more robust.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is a diagram that illustrates an example convolutional neural network (CNN) <b>300</b> configured to classify an image of a user's eye as having a gaze directed to a particular region. This is a convolutional neural network (CNN) that consists of CNN layers, pooling layers and dense layers. The input is an image obtained from a camera that is mounted on smartglasses frames and that images the user's eye. The output of the network is a vector of length N, where N is the number of regions. The index of the largest value in an output vector <b>350</b> gives the identified region of the gaze. Optionally, a softmax layer <b>340</b> may be added after the last dense layer, which normalizes each value inside the output vector to be between 0 and 1 and that all values sum up to 1. Each value then represents the probability that the gaze is directed to a particular region. In some implementations, each CNN layer includes an activation function, such as a Rectified Linear Unit, a Sigmoid, or a hyperbolic tangent.</p><p id="p-0051" num="0050">In some implementations in which there are two classes, i.e., regions, with similar probabilities (i.e., probabilities that differ less than a specified threshold difference), the computer <b>120</b> may render and display only those two regions. In some implementations, the computer <b>120</b> asks the user to manually select a region from the two displayed.</p><p id="p-0052" num="0051">In some implementations, the region data <b>134</b> includes an identifier for the space outside of the display, i.e., defines a new region not included in any region or including any pixels. In such an implementation, it may be determined that the user is not looking at the display.</p><p id="p-0053" num="0052">As shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, input data <b>305</b> is introduced into a 2D convolution layer <b>310</b>(<b>1</b>), which is followed by a pooling layer <b>320</b>(<b>1</b>). In the example illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, there are four 2D convolution layers <b>310</b>(<b>1</b>-<b>4</b>), each followed by a respective pooling layer <b>320</b>(<b>1</b>-<b>4</b>). Each of the four convolution layers <b>310</b>(<b>1</b>-<b>4</b>) shown have a kernel size of 3&#xd7;3, with two strides. The output size from 2D convolution layers <b>310</b>(<b>1</b>-<b>4</b>) are, respectively, <b>16</b>, <b>32</b>, <b>64</b>, and <b>128</b>.</p><p id="p-0054" num="0053">Following these convolution layers <b>310</b>(<b>1</b>-<b>4</b>) and their respective pooling layers <b>320</b>(<b>1</b>-<b>4</b>) are dense layers <b>330</b>(<b>1</b>) and <b>330</b>(<b>2</b>). In some implementations, the dense layers <b>330</b>(<b>1</b>) and <b>330</b>(<b>2</b>) are used to merge other branches of the CNN into the branch defined as having the convolution layers <b>310</b>(<b>1</b>-<b>4</b>) and their respective pooling layers <b>320</b>(<b>1</b>-<b>4</b>). For example, a second branch of the CNN may be trained to generate output values based on region data <b>134</b>, slippage data <b>136</b>, and/or user data <b>138</b>. The dense layers <b>330</b>(<b>1</b>) and <b>330</b>(<b>2</b>), then, may provide adjustments to a classification model defined by the first branch of the CNN based on the arrangement of the regions, any slippage of smartglasses down a user's face, or other user characteristics.</p><p id="p-0055" num="0054">The classification model is generated by the training manager <b>141</b> using training data <b>145</b>. The training data <b>145</b> represents training datasets and identifiers for regions at which the gazes represented in the training datasets are directed. The training datasets include images of the user's eye over time. In some implementations, the images are generated on a periodic basis, e.g., every second, every 0.5 seconds, every 0.1 seconds, or so on. These images and the corresponding region identifiers are input to a loss function, and the values of the layer nodes are generated by optimizing the loss function.</p><p id="p-0056" num="0055">In some implementations, the region data <b>134</b> is expressed in terms of pixel coordinates or angles. In such implementations, the training manager <b>141</b> converts the coordinates or angles into region identifiers. In some implementation, this conversion may be accomplished using a lookup table. In some implementation, this conversion is accomplished by computing the closest tile center according to, e.g., Euclidean distance or a cosine distance between a gaze vector and a vector representing the center of the region.</p><p id="p-0057" num="0056">The loss function includes, in some implementations, a categorical cross entropy appropriate for multi-class classification problems. Such a cross-entropy loss may be represented mathematically as follows:</p><p id="p-0058" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>&#x2112;</mi>    <mi>CE</mi>   </msub>   <mo>=</mo>   <mrow>    <mo>-</mo>    <mrow>     <munderover>      <mo>&#x2211;</mo>      <mrow>       <mi>i</mi>       <mo>=</mo>       <mn>1</mn>      </mrow>      <mi>C</mi>     </munderover>     <mrow>      <msub>       <mi>p</mi>       <mi>i</mi>      </msub>      <mo>&#x2062;</mo>      <mi>log</mi>      <mo>&#x2062;</mo>      <mtext>  </mtext>      <msub>       <mi>q</mi>       <mi>i</mi>      </msub>     </mrow>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0059" num="0000">where C is the number of classes (e.g., the number of regions over the display), p<sub>i </sub>is the label for class i and q<sub>i </sub>is the output of the network; in some arrangements, the output of a softmax layer. In some implementations, the labels for the classes are in a one-hot representation in which p<sub>i</sub>=1 only for the class to which an example belongs, and p<sub>i</sub>=0 otherwise. The above equation represents a loss per example (i.e., image from training data <b>145</b>); a total loss is obtained by summing the cross-entropy loss over all examples in a batch and dividing by the batch size.</p><p id="p-0060" num="0057">In some implementations, the loss function includes a Kullback-Leibler divergence loss. Such a Kullback-Leibler divergence loss may be represented mathematically as follows:</p><p id="p-0061" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>&#x2112;</mi>    <mi>KL</mi>   </msub>   <mo>=</mo>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>C</mi>    </munderover>    <mrow>     <msub>      <mi>p</mi>      <mi>i</mi>     </msub>     <mo>&#x2062;</mo>     <mrow>      <mi>log</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mfrac>       <msub>        <mi>p</mi>        <mi>i</mi>       </msub>       <msub>        <mi>q</mi>        <mi>i</mi>       </msub>      </mfrac>      <mo>)</mo>     </mrow>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0062" num="0000">where C is the number of classes (e.g., the number of regions over the display), p<sub>i </sub>is the label for class i and q<sub>i </sub>is the output of the network; in some arrangements, the output of a softmax layer. The above equation represents a loss per example (i.e., image from training data); a total loss is obtained by summing the Kullback-Leibler divergence loss over all examples in a batch and dividing by the batch size</p><p id="p-0063" num="0058">In some implementations, the loss function includes a triplet loss used to optimize a metric space defined by an area encompassing the gaze space, i.e., all the locations that the user can look at; in such a metric space image clusters may be defined. These image clusters in turn define anchor points, and the loss function may be based in part on a distance from the anchor points. Such a triplet loss may be represented mathematically as follows:</p><p id="p-0064" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>&#x2112;</mi>    <mi>T</mi>   </msub>   <mo>=</mo>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>k</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>K</mi>    </munderover>    <mrow>     <mi>max</mi>     <mo>&#x2062;</mo>     <mrow>      <mo>{</mo>      <mrow>       <mrow>        <msup>         <mrow>          <mo>&#xf605;</mo>          <mrow>           <mrow>            <mi>f</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <msub>             <mi>A</mi>             <mi>k</mi>            </msub>            <mo>)</mo>           </mrow>           <mo>-</mo>           <mrow>            <mi>f</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <msub>             <mi>P</mi>             <mi>k</mi>            </msub>            <mo>)</mo>           </mrow>          </mrow>          <mo>&#xf606;</mo>         </mrow>         <mn>2</mn>        </msup>        <mo>-</mo>        <msup>         <mrow>          <mo>&#xf605;</mo>          <mrow>           <mrow>            <mi>f</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <msub>             <mi>A</mi>             <mi>k</mi>            </msub>            <mo>)</mo>           </mrow>           <mo>-</mo>           <mrow>            <mi>f</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <msub>             <mi>N</mi>             <mi>k</mi>            </msub>            <mo>)</mo>           </mrow>          </mrow>          <mo>&#xf606;</mo>         </mrow>         <mn>2</mn>        </msup>        <mo>+</mo>        <mi>&#x3b1;</mi>       </mrow>       <mo>,</mo>       <mn>0</mn>      </mrow>      <mo>}</mo>     </mrow>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0065" num="0000">where f(x) represents a neural network transform, A<sub>k </sub>represents an anchor input (i.e., an image), P<sub>k </sub>is a positive example, and N<sub>k </sub>is a negative example. The summation is over all possible triplets of (anchor, positive example, negative example). &#x3b1; represents a small number that introduces a margin between positive and negative examples, used in avoiding trivial solutions of all zeroes.</p><p id="p-0066" num="0059"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is a diagram that illustrates an example convolutional neural network (CNN) <b>360</b> configured to classify an image of a user's eye as having a gaze directed to a particular point. The CNN <b>360</b> is similar to CNN <b>300</b> (<figref idref="DRAWINGS">FIG. <b>3</b>A</figref>), except that there are new dense layers <b>370</b>(<b>1</b>-<b>4</b>). Dense layers <b>370</b>(<b>1</b>,<b>2</b>) are similar to the dense layers <b>330</b>(<b>1</b>,<b>2</b>) of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>. Dense layers <b>370</b>(<b>3</b>,<b>4</b>), however, are configured to produce coordinates of a most likely point on the display at which the user's gaze is directed.</p><p id="p-0067" num="0060">As stated above, the dense layers <b>330</b>(<b>1</b>) and <b>330</b>(<b>2</b>) of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, or <b>370</b>(<b>1</b>-<b>4</b>) of <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, allow for an adjustment to the classification model based on other input data such as region data <b>134</b>, slippage data <b>136</b>, and user data <b>138</b>. An example branch that provides output to a dense layer, e.g., dense layer <b>330</b>(<b>1</b>) is shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0068" num="0061"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram that illustrates example convolutional layers forming another branch of a CNN <b>400</b>, in this case configured to adapt for different region or tile geometries. It is noted that similar branches may be defined for the slippage and user adjustments. The output of the other branch is fed into dense layers that adjust the output from that using the image data <b>132</b> only.</p><p id="p-0069" num="0062">The CNN <b>400</b> takes input <b>405</b> into convolution layers <b>410</b> of a first branch and provides a first output via a first dense layer to a concatenation layer <b>440</b>. In a second branch, the CNN inputs a tile design <b>420</b>, i.e., an arrangement of regions, into a learned embedding layer <b>430</b>. The output of this learned embedding layer <b>430</b> is also input into the concatenation layer <b>440</b>. The concatenating layer <b>440</b> concatenates the output of the first dense layer with the output of the embedding layer <b>430</b>. The concatenated output is used to determine the region at which the gaze is directed.</p><p id="p-0070" num="0063">There are several alternative approaches to determining the region at which a gaze is directed than that illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In some implementations, multiple models are trained, each model corresponds to a respective arrangement. In some implementations, training is performed on small regions and the output is binned for each arrangement. In some implementations, a network is trained such that the network has fixed convolutional layers for all arrangements but different dense layers for each arrangement.</p><p id="p-0071" num="0064">Similar approaches may be employed with regard to other input data. For example, with regard to user calibration based on slippage data <b>136</b>, the eye is at a nominal location with a known (designed) pose relative to the display. During wearing, the position of the glasses can change from this nominal location (slipping on the nose, adjustment by the user, etc.). When slippage happens, the pose between the eye and the eye tracking camera changes and the images of the eyes appear different from the case of the nominal configuration. Moreover, the gaze angle will be different because the position of the display changes with the slippage of glasses. Because of that, the neural network might produce wrong classifications.</p><p id="p-0072" num="0065">In some implementations, one infers a pose (position and orientation) of the eye relative to the camera. To do this, one may take the following approach: Train a network that learns to decouple the gaze classification and estimation of the eye position relative to the camera (or display). The estimation of the eye position relative to the display could use parts of the eye image that do not change with gaze (such as the corners of the eye). Selection of these image parts could be done with an attention neural network model, for example. These image parts could be compared in two sets of images: the first set could be from the calibration phase, and the second set from the images captured during gaze classification runtime.</p><p id="p-0073" num="0066">Other alternatives to adjusting the classification model with the slippage data <b>136</b> includes an assumption of a finite set of possible slippage positions and perform a classification of eye positions based on the finite set. User cues may also be used to detect whether slippage has occurred. Finally, one may employ a brute-force approach in which the CNN is trained to be invariant with respect to position changes due to slippage.</p><p id="p-0074" num="0067">With regard to user data <b>138</b>, the CNN may be calibrated to account for differences in eye appearance, visual axis, head shape, and the like. For example, a calibration scheme involves directing the user to look at specific targets on the display with known region associations. During calibration, eye camera images are recorded and region identifiers of the targets are saved as labels as part of the user data <b>138</b>. The user data <b>138</b> then may be used in the following ways:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0068">To fine tune the existing, pre-trained neural network,</li>        <li id="ul0002-0002" num="0069">To train additional network branches or structures added specifically for calibration,</li>        <li id="ul0002-0003" num="0070">To learn an encoder during training that takes as input only the calibration data and predicts a user-specific embedding layer.,</li>        <li id="ul0002-0004" num="0071">To align images such that eye landmarks in the user data <b>138</b> are aligned with eye landmarks in the training data <b>141</b>.</li>    </ul>    </li></ul></p><p id="p-0075" num="0072">In some implementations, a smooth, stable temporal gaze is desired. In such implementations, temporal filtering may be used to accomplish the smooth, stable temporal gaze, as follows:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0073">compute a mean or median score from softmax layer outputs from consecutive video frames in the image data <b>132</b>,</li>        <li id="ul0004-0002" num="0074">incorporate a recursive neural network (RNN) layer configured to take outputs of a convolutional layer and train a RNN cell on top (e.g., long short-term memory cell, GRU cell, or the like),</li>        <li id="ul0004-0003" num="0075">classify, in some implementations, eye movement as a fixation, a saccade, or a pursuit.</li>    </ul>    </li></ul></p><p id="p-0076" num="0076">In some implementations, the output of the CNN includes a region identifier as well as an estimated location within the display at which the gaze is directed.</p><p id="p-0077" num="0077"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart depicting an example method <b>500</b> of determining a region at which a user's gaze is directed. The method <b>500</b> may be performed by software constructs described in connection with <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, which reside in memory <b>126</b> of the computing circuitry <b>120</b> and are run by the set of processing units <b>124</b> or may be performed by software constructs which reside in memory of the computing circuitry <b>120</b>.</p><p id="p-0078" num="0078">At <b>502</b>, the computer <b>120</b> receives image data (e.g., image data <b>132</b>) representing at least one image of an eye of a user looking at a display at an instant of time, the display including a plurality of regions (e.g., regions <b>220</b>(<b>1</b>-<b>4</b>)) and being configured to operate in an augmented reality (AR) application (e.g., in smartglasses <b>110</b>), each of the plurality of regions including a plurality of pixels (e.g., pixels <b>210</b>) and corresponding to a respective element of a user interface.</p><p id="p-0079" num="0079">At <b>504</b>, the computer <b>120</b> identifies, based on the image data, a region of the plurality of regions of the display at which a gaze of a user is directed at the instant of time, the identifying including inputting the at least one image of the eye of the user into a classification engine configured to classify the gaze as being directed to one of the plurality of regions.</p><p id="p-0080" num="0080">At <b>506</b>, the computer <b>120</b> activates the element of the user interface to which the identified region corresponds.</p><p id="p-0081" num="0081"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example of a generic computer device <b>600</b> and a generic mobile computer device <b>650</b>, which may be used with the techniques described here. Computer device <b>600</b> is one example configuration of computer <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0082" num="0082">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, computing device <b>600</b> is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. Computing device <b>650</b> is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart phones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.</p><p id="p-0083" num="0083">Computing device <b>600</b> includes a processor <b>602</b>, memory <b>604</b>, a storage device <b>606</b>, a high-speed interface <b>608</b> connecting to memory <b>604</b> and high-speed expansion ports <b>610</b>, and a low speed interface <b>612</b> connecting to low speed bus <b>614</b> and storage device <b>606</b>. Each of the components <b>602</b>, <b>604</b>, <b>606</b>, <b>608</b>, <b>610</b>, and <b>612</b>, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor <b>602</b> can process instructions for execution within the computing device <b>600</b>, including instructions stored in the memory <b>604</b> or on the storage device <b>606</b> to display graphical information for a GUI on an external input/output device, such as display <b>616</b> coupled to high speed interface <b>608</b>. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices <b>600</b> may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).</p><p id="p-0084" num="0084">The memory <b>604</b> stores information within the computing device <b>600</b>. In one implementation, the memory <b>604</b> is a volatile memory unit or units. In another implementation, the memory <b>604</b> is a non-volatile memory unit or units. The memory <b>604</b> may also be another form of computer-readable medium, such as a magnetic or optical disk.</p><p id="p-0085" num="0085">The storage device <b>606</b> is capable of providing mass storage for the computing device <b>600</b>. In one implementation, the storage device <b>606</b> may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. A computer program product can be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory <b>604</b>, the storage device <b>606</b>, or memory on processor <b>602</b>.</p><p id="p-0086" num="0086">The high speed controller <b>608</b> manages bandwidth-intensive operations for the computing device <b>500</b>, while the low speed controller <b>612</b> manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only. In one implementation, the high-speed controller <b>608</b> is coupled to memory <b>604</b>, display <b>616</b> (e.g., through a graphics processor or accelerator), and to high-speed expansion ports <b>610</b>, which may accept various expansion cards (not shown). In the implementation, low-speed controller <b>612</b> is coupled to storage device <b>506</b> and low-speed expansion port <b>614</b>. The low-speed expansion port, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.</p><p id="p-0087" num="0087">The computing device <b>600</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server <b>620</b>, or multiple times in a group of such servers. It may also be implemented as part of a rack server system <b>624</b>. In addition, it may be implemented in a personal computer such as a laptop computer <b>622</b>. Alternatively, components from computing device <b>600</b> may be combined with other components in a mobile device (not shown), such as device <b>650</b>. Each of such devices may contain one or more of computing device <b>600</b>, <b>650</b>, and an entire system may be made up of multiple computing devices <b>600</b>, <b>650</b> communicating with each other.</p><p id="p-0088" num="0088">Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.</p><p id="p-0089" num="0089">These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms &#x201c;machine-readable medium&#x201d; &#x201c;computer-readable medium&#x201d; refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term &#x201c;machine-readable signal&#x201d; refers to any signal used to provide machine instructions and/or data to a programmable processor.</p><p id="p-0090" num="0090">To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.</p><p id="p-0091" num="0091">The systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (&#x201c;LAN&#x201d;), a wide area network (&#x201c;WAN&#x201d;), and the Internet.</p><p id="p-0092" num="0092">The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p><p id="p-0093" num="0093">A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the specification.</p><p id="p-0094" num="0094">It will also be understood that when an element is referred to as being on, connected to, electrically connected to, coupled to, or electrically coupled to another element, it may be directly on, connected or coupled to the other element, or one or more intervening elements may be present. In contrast, when an element is referred to as being directly on, directly connected to or directly coupled to another element, there are no intervening elements present. Although the terms directly on, directly connected to, or directly coupled to may not be used throughout the detailed description, elements that are shown as being directly on, directly connected or directly coupled can be referred to as such. The claims of the application may be amended to recite exemplary relationships described in the specification or shown in the figures.</p><p id="p-0095" num="0095">While certain features of the described implementations have been illustrated as described herein, many modifications, substitutions, changes and equivalents will now occur to those skilled in the art. It is, therefore, to be understood that the appended claims are intended to cover all such modifications and changes as fall within the scope of the implementations. It should be understood that they have been presented by way of example only, not limitation, and various changes in form and details may be made. Any portion of the apparatus and/or methods described herein may be combined in any combination, except mutually exclusive combinations. The implementations described herein can include various combinations and/or sub-combinations of the functions, components and/or features of the different implementations described.</p><p id="p-0096" num="0096">In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, or steps may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other implementations are within the scope of the following claims.</p><p id="p-0097" num="0097">In the following some examples are described.</p><p id="p-0098" num="0098">Example 1: A method, comprising:</p><p id="p-0099" num="0099">receiving image data representing at least one image of an eye of a user looking at a display at an instant of time, the display including a plurality of regions and being configured to operate in an augmented reality (AR) application, each of the plurality of regions including a plurality of pixels and corresponding to a respective element of a user interface;</p><p id="p-0100" num="0100">identifying, based on the image data, a region of the plurality of regions of the display at which a gaze of a user is directed at the instant of time, the identifying including inputting the at least one image of the eye of the user into a classification engine configured to classify the gaze as being directed to one of the plurality of regions; and</p><p id="p-0101" num="0101">activating the element of the user interface to which the identified region corresponds.</p><p id="p-0102" num="0102">Example 2: The method as in example 1, wherein the classification engine includes a first branch representing a convolutional neural network (CNN).</p><p id="p-0103" num="0103">Example 3: The method as in claim <b>2</b>, wherein the classification engine is configured to produce, as an output, a vector having a number of elements equal to a number of regions of the plurality of regions, each element of the vector including a number corresponding to a respective region of the plurality of regions, the number representing a likelihood of the gaze of the user being directed to the region to which the number corresponds.</p><p id="p-0104" num="0104">Example 4: The method as in example 3, wherein the classification engine includes a softmax layer configured to produce, as an output of the classification engine, as the likelihood corresponding to for each region of the plurality of regions, a probability between zero and unity, and wherein identifying the region further includes selecting, as the identified region, a region of the plurality of regions having a probability greater than the probability of each of the other regions of the plurality of regions.</p><p id="p-0105" num="0105">Example 5: The method as in example 3, wherein identifying the region further includes generating image cluster data representing a set of image clusters corresponding the plurality of regions on the display, and wherein the classification engine includes a loss function based on distances from the set of image clusters.</p><p id="p-0106" num="0106">Example 6: The method as in example 1, further comprising training the classification engine, the training being based on a mapping between images of the eye of the user and a region identifier identifying a region of the plurality of regions at which the gaze of the user is directed.</p><p id="p-0107" num="0107">Example 7: The method as in example 1, wherein the display is a transparent display embedded in smartglasses.</p><p id="p-0108" num="0108">Example 8: The method as in example 7, wherein the classification engine further includes a second branch representing a neural network, and wherein the method further comprises outputting, from the second branch and based on the image data, a pose of the eye of the user with respect to a camera mounted on the smartglasses.</p><p id="p-0109" num="0109">Example 9: The method as in example 8, wherein the classification engine includes an attention layer, and wherein identifying the region further includes causing the attention layer to adjust probabilities of the gaze being directed to the regions of the display based on the outputted pose of the eye.</p><p id="p-0110" num="0110">Example 10: The method as in example 1, wherein the user is a first user, wherein the classification engine further includes a second branch representing a neural network, and wherein the method further comprises inputting into the second branch a parameter value indicating a difference between the first user and a second user; and causing the second branch to adjust probabilities of the gaze being directed to the regions of the display based on the parameter value.</p><p id="p-0111" num="0111">Example 11: The method as in example 1, wherein the user is a first user, wherein the classification engine further includes a second branch representing a neural network, and wherein the method further comprises inputting into the second branch a parameter value indicating a geometrical configuration of the plurality of regions; and causing the second branch to adjust probabilities of the gaze being directed to the regions of the display based on the parameter value.</p><p id="p-0112" num="0112">Example 12: The method as in example 1, wherein the user is a first user, wherein the classification engine further includes a second branch representing a neural network, and wherein the method further comprises inputting into the second branch a parameter value indicating a temporal smoothness of the image data; and causing the second branch to adjust probabilities of the gaze being directed to the regions of the display based on the parameter value</p><p id="p-0113" num="0113">Example 13: A computer program product comprising a nontransitory storage medium, the computer program product including code that, when executed by processing circuitry of a computer, causes the processing circuitry to perform a method, the method comprising:</p><p id="p-0114" num="0114">receiving image data representing at least one image of an eye of a user looking at a display at an instant of time, the display including a plurality of regions and being configured to operate in an augmented reality (AR) application, each of the plurality of regions including a plurality of pixels and corresponding to a respective element of a user interface;</p><p id="p-0115" num="0115">identifying, based on the image data, a region of the plurality of regions of the display at which a gaze of a user is directed at the instant of time, the identifying including inputting the at least one image of the eye of the user into a classification engine configured to classify the gaze as being directed to one of the plurality of region; and</p><p id="p-0116" num="0116">activating the element of the user interface to which the identified region corresponds.</p><p id="p-0117" num="0117">Example 14: The computer program product as in example 13, wherein the classification engine includes a first branch representing a convolutional neural network (CNN).</p><p id="p-0118" num="0118">Example 15: The computer program product as in example 13, wherein the classification engine is configured to produce, as an output, a number corresponding to each of the plurality of regions, the number representing a likelihood of the gaze of the user being directed to the region to which the number corresponds.</p><p id="p-0119" num="0119">Example 16: The computer program product as in example 13, wherein the method further comprises training the classification engine, the training being based on a mapping between images of the eye of the user and a region identifier identifying a region of the plurality of regions at which the gaze of the user is directed.</p><p id="p-0120" num="0120">Example 17: The computer program product as in example 13, wherein the display is a transparent display embedded in smartglasses.</p><p id="p-0121" num="0121">Example 18: The computer program product as in example 17, wherein the classification engine further includes a second branch representing a neural network, and wherein the method further comprises outputting, from the second branch and based on the image data, a position and orientation of the eye of the user with respect to a camera mounted on the smartglasses.</p><p id="p-0122" num="0122">Example 19: The computer program product as in example 18, wherein the classification engine includes an attention layer, and wherein identifying the region further includes causing the attention layer to adjust probabilities of the gaze being directed to the regions of the display based on the outputted position and orientation of the eye.</p><p id="p-0123" num="0123">Example 20: An electronic apparatus, the electronic apparatus comprising:</p><p id="p-0124" num="0124">memory; and</p><p id="p-0125" num="0125">processing circuitry coupled to the memory, the processing circuitry being configured to:</p><p id="p-0126" num="0126">receive image data representing at least one image of an eye of a user looking at a display at an instant of time, the display including a plurality of regions and being configured to operate in an augmented reality (AR) application, each of the plurality of regions including a plurality of pixels and corresponding to a respective element of a user interface;</p><p id="p-0127" num="0127">identify, based on the image data, a region of the plurality of regions of the display at which a gaze of a user is directed at the instant of time, the identifying including inputting the at least one image of the eye of the user into a classification engine configured to classify the gaze as being directed to one of the plurality of regions; and</p><p id="p-0128" num="0128">activate the element of the user interface to which the identified region corresponds.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004216A1-20230105-M00001.NB"><img id="EMI-M00001" he="8.13mm" wi="76.20mm" file="US20230004216A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004216A1-20230105-M00002.NB"><img id="EMI-M00002" he="8.13mm" wi="76.20mm" file="US20230004216A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230004216A1-20230105-M00003.NB"><img id="EMI-M00003" he="8.13mm" wi="76.20mm" file="US20230004216A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>receiving image data representing at least one image of an eye of a user looking at a display at an instant of time, the display including a plurality of regions and being configured to operate in an augmented reality (AR) application, each of the plurality of regions including a plurality of pixels and corresponding to a respective element of a user interface;</claim-text><claim-text>identifying, based on the image data, a region of the plurality of regions of the display at which a gaze of a user is directed at the instant of time, the identifying including inputting the at least one image of the eye of the user into a classification engine configured to classify the gaze as being directed to one of the plurality of regions; and</claim-text><claim-text>activating the element of the user interface to which the identified region corresponds.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the classification engine includes a first branch representing a convolutional neural network (CNN).</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the classification engine is configured to produce, as an output, a vector having a number of elements equal to a number of regions of the plurality of regions, each element of the vector including a number corresponding to a respective region of the plurality of regions, the number representing a likelihood of the gaze of the user being directed to the region to which the number corresponds.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method as in <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the classification engine includes a softmax layer configured to produce, as an output of the classification engine, as the likelihood corresponding to each region of the plurality of regions, a probability between zero and unity, and<claim-text>wherein identifying the region further includes:</claim-text><claim-text>selecting, as the identified region, a region of the plurality of regions having a probability greater than the probability of each of the other regions of the plurality of regions.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method as in <claim-ref idref="CLM-00003">claim 3</claim-ref>,<claim-text>wherein identifying the region further includes:<claim-text>generating image cluster data representing a set of image clusters corresponding the plurality of regions on the display, and</claim-text></claim-text><claim-text>wherein the classification engine includes a loss function based on distances from the set of image clusters.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>training the classification engine, the training being based on a mapping between images of the eye of the user and a region identifier identifying a region of the plurality of regions at which the gaze of the user is directed.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the display is a transparent display embedded in smartglasses.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method as in <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the classification engine further includes a second branch representing a neural network, and<claim-text>wherein the method further comprises:<claim-text>outputting, from the second branch and based on the image data, a pose of the eye of the user with respect to a camera mounted on the smartglasses.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method as in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the classification engine includes an attention layer, and<claim-text>wherein identifying the region further includes:<claim-text>causing the attention layer to adjust probabilities of the gaze being directed to the regions of the display based on the outputted pose of the eye.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the user is a first user,<claim-text>wherein the classification engine further includes a second branch representing a neural network, and</claim-text><claim-text>wherein the method further comprises:<claim-text>inputting into the second branch a parameter value indicating a difference between the first user and a second user; and</claim-text></claim-text><claim-text>causing the second branch to adjust probabilities of the gaze being directed to the regions of the display based on the parameter value.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the user is a first user,<claim-text>wherein the classification engine further includes a second branch representing a neural network, and</claim-text><claim-text>wherein the method further comprises:<claim-text>inputting into the second branch a parameter value indicating a geometrical configuration of the plurality of regions; and</claim-text><claim-text>causing the second branch to adjust probabilities of the gaze being directed to the regions of the display based on the parameter value.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the user is a first user,<claim-text>wherein the classification engine further includes a second branch representing a neural network, and</claim-text><claim-text>wherein the method further comprises:<claim-text>inputting into the second branch a parameter value indicating a temporal smoothness of the image data; and</claim-text><claim-text>causing the second branch to adjust probabilities of the gaze being directed to the regions of the display based on the parameter value.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A computer program product comprising a nontransitive storage medium, the computer program product including code that, when executed by processing circuitry, causes the processing circuitry to perform a method, the method comprising:<claim-text>receiving image data representing at least one image of an eye of a user looking at a display at an instant of time, the display including a plurality of regions and being configured to operate in an augmented reality (AR) application, each of the plurality of regions including a plurality of pixels and corresponding to a respective element of a user interface;</claim-text><claim-text>identifying, based on the image data, a region of the plurality of regions of the display at which a gaze of a user is directed at the instant of time, the identifying including inputting the at least one image of the eye of the user into a classification engine configured to classify the gaze as being directed to one of the plurality of regions; and</claim-text><claim-text>activating the element of the user interface to which the identified region corresponds.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer program product as in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the classification engine includes a first branch representing a convolutional neural network (CNN).</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer program product as in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the classification engine is configured to produce, as an output, a number corresponding to each of the plurality of regions, the number representing a likelihood of the gaze of the user being directed to the region to which the number corresponds.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer program product as in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the method further comprises:<claim-text>training the classification engine, the training being based on a mapping between images of the eye of the user and a region identifier identifying a region of the plurality of regions at which the gaze of the user is directed.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer program product as in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the display is a transparent display embedded in smartglasses.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer program product as in <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the classification engine further includes a second branch representing a neural network, and<claim-text>wherein the method further comprises:<claim-text>outputting, from the second branch and based on the image data, a pose of the eye of the user with respect to a camera mounted on the smartglasses.</claim-text></claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer program product as in <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the classification engine includes an attention layer, and<claim-text>wherein identifying the region further includes:<claim-text>causing the attention layer to adjust probabilities of the gaze being directed to the regions of the display based on the outputted pose of the eye.</claim-text></claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. An electronic apparatus, the electronic apparatus comprising:<claim-text>memory; and</claim-text><claim-text>controlling circuitry coupled to the memory, the controlling circuitry being configured to:<claim-text>receive image data representing at least one image of an eye of a user looking at a display at an instant of time, the display including a plurality of regions and being configured to operate in an augmented reality (AR) application, each of the plurality of regions including a plurality of pixels and corresponding to a respective element of a user interface;</claim-text><claim-text>identify, based on the image data, a region of the plurality of regions of the display at which a gaze of a user is directed at the instant of time, the identifying including inputting the at least one image of the eye of the user into a classification engine configured to classify the gaze as being directed to one of the plurality of regions; and</claim-text><claim-text>activate the element of the user interface to which the identified region corresponds.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>