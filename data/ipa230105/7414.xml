<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007415A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007415</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940537</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>25</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>25</main-group><subgroup>606</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>25</main-group><subgroup>554</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>2225</main-group><subgroup>41</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>2225</main-group><subgroup>43</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">INDIVIDUALIZED OWN VOICE DETECTION IN A HEARING PROSTHESIS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16961536</doc-number><date>20200710</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11477587</doc-number></document-id></parent-grant-document><parent-pct-document><document-id><country>WO</country><doc-number>PCT/IB2019/050164</doc-number><date>20190109</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17940537</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62617750</doc-number><date>20180116</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Cochlear Limited</orgname><address><city>Macquarie University</city><country>AU</country></address></addressbook><residence><country>AU</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Brown</last-name><first-name>Matthew</first-name><address><city>South Coogee</city><country>AU</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Presented herein are techniques for training a hearing prosthesis to classify/categorize received sound signals as either including a recipient's own voice (i.e., the voice or speech of the recipient of the hearing prosthesis) or external voice (i.e., the voice or speech of one or more persons other than the recipient). The techniques presented herein use the captured voice (speech) of the recipient to train the hearing prosthesis to perform the classification of the sound signals as including the recipient's own voice or external voice.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="139.62mm" wi="140.38mm" file="US20230007415A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="159.85mm" wi="142.41mm" file="US20230007415A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="231.82mm" wi="159.94mm" orientation="landscape" file="US20230007415A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="224.79mm" wi="153.67mm" orientation="landscape" file="US20230007415A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="89.49mm" wi="148.93mm" file="US20230007415A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="232.92mm" wi="158.41mm" orientation="landscape" file="US20230007415A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="114.47mm" wi="118.79mm" file="US20230007415A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="170.94mm" wi="130.73mm" orientation="landscape" file="US20230007415A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="231.39mm" wi="152.40mm" orientation="landscape" file="US20230007415A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="174.24mm" wi="114.81mm" orientation="landscape" file="US20230007415A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="74.85mm" wi="126.15mm" file="US20230007415A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="196.00mm" wi="128.95mm" file="US20230007415A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="99.99mm" wi="102.02mm" file="US20230007415A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="162.98mm" wi="142.58mm" file="US20230007415A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="136.91mm" wi="128.78mm" file="US20230007415A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><heading id="h-0002" level="1">Field of the Invention</heading><p id="p-0002" num="0001">The present invention generally relates to individualized own voice detection in a hearing prosthesis.</p><heading id="h-0003" level="1">RELATED ART</heading><p id="p-0003" num="0002">Hearing loss, which may be due to many different causes, is generally of two types, conductive and/or sensorineural. Conductive hearing loss occurs when the normal mechanical pathways of the outer and/or middle ear are impeded, for example, by damage to the ossicular chain or ear canal. Sensorineural hearing loss occurs when there is damage to the inner ear, or to the nerve pathways from the inner ear to the brain.</p><p id="p-0004" num="0003">Individuals who suffer from conductive hearing loss typically have some form of residual hearing because the hair cells in the cochlea are undamaged. As such, individuals suffering from conductive hearing loss typically receive an auditory prosthesis that generates motion of the cochlea fluid. Such auditory prostheses include, for example, acoustic hearing aids, bone conduction devices, and direct acoustic stimulators.</p><p id="p-0005" num="0004">In many people who are profoundly deaf, however, the mason for their deafness is sensorineural hearing loss. Those suffering from some forms of sensorineural hearing loss are unable to derive suitable benefit from auditory prostheses that generate mechanical motion of the cochlea fluid. Such individuals can benefit from implantable auditory prostheses that stimulate nerve cells of the recipient's auditory system in other ways (e.g., electrical, optical and the like). Cochlear implants are often proposed when the sensorineural hearing loss is due to the absence or destruction of the cochlea hair cells, which transduce acoustic signals into nerve impulses. An auditory brainstem stimulator is another type of stimulating auditory prosthesis that might also be proposed when a recipient experiences sensorineural hearing loss due to damage to the auditory nerve.</p><p id="p-0006" num="0005">Certain individuals suffer from only partial sensorineural hearing loss and, as such, retain at least some residual hearing. These individuals may be candidates for electro-acoustic hearing prostheses.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">In one aspect, a method is provided. The method comprises: at one or more microphones of a hearing prosthesis, capturing input audio signals that include a voice of a recipient of the hearing prosthesis; calculating, on the hearing prosthesis, time-varying features from the input audio signals; and updating, based on an analysis of a plurality of the time-varying features, operation of an own voice detection decision tree of the hearing prosthesis.</p><p id="p-0008" num="0007">In another aspect, a method is provided. The method comprises: receiving input audio signals at a hearing prosthesis, wherein the input audio signals include speech of a recipient of the hearing prosthesis; calculating, on the hearing prosthesis, time-varying features from the input audio signals; analyzing a plurality of the time-varying features with an own voice detection decision tree on the hearing prosthesis; receiving label data associated the input audio signals, wherein the label data indicates which time segments of the input audio signals include the voice of a recipient; analyzing the plurality of time-varying features and the label data to generate updated weights for the own voice detection decision tree; and updating the own voice detection decision tree with the updated weights.</p><p id="p-0009" num="0008">In another aspect, a method is provided. The method comprises: receiving time-varying features generated from input audio signals captured at one or more microphones of a hearing prosthesis, wherein the input audio signals include a voice of a recipient of the hearing prosthesis; receiving label data associated the input audio signals, wherein the label data indicates which of the plurality of time segments of the input audio signals include the voice of a recipient; analyzing the plurality of time-varying features and the label data to generate updated weights for an own voice detection decision tree on the hearing prosthesis; and updating the own voice detection decision tree with the updated weights to generate an updated an own voice detection decision tree.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">Embodiments of the present invention are described herein in conjunction with the accompanying drawings, in which:</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a schematic diagram illustrating a cochlear implant, in accordance with certain embodiments presented herein;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a block diagram of the cochlear implant of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>:</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of a totally implantable cochlear implant, in accordance with certain embodiments presented herein;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic block diagram illustrating operation of an environmental classifier and individualized own voice detector, in accordance with certain embodiments presented herein;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a schematic block diagram illustrating updating of an individualized own voice detection decision tree, in accordance with certain embodiments presented herein;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a schematic block diagram illustrating further details of one implementation of the arrangement of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic block diagram illustrating a technique for dynamically updating an environmental classification decision tree on a hearing prosthesis, in accordance with certain embodiments presented herein;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic block diagram illustrating a technique for dynamically updating an environmental classification decision tree and an own voice detection tree on a hearing prosthesis, in accordance with certain embodiments presented herein;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic block diagram illustrating use of supplemental signal features in an environmental classification, in accordance with certain embodiments presented herein;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram illustrating a cloud-based arrangement for dynamically updating an own voice detection tree, in accordance with certain embodiments presented herein;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram of a fitting system for implementation of certain techniques presented herein;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart of method, in accordance with embodiments presented herein;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart of another method, in accordance with embodiments presented herein; and</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart of another method, in accordance with embodiments presented herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0025" num="0024">Presented herein are techniques for training a hearing prosthesis to classify/categorize captured/received input audio signals as either including a recipient's own voice (i.e., the voice or speech of the recipient of the hearing prosthesis) or external voice (i.e., the voice or speech of one or more persons other than the recipient). The techniques presented herein use the captured voice (speech) of the recipient to train the hearing prosthesis to perform the classification of the input audio signals as including the recipient's own voice or external voice.</p><p id="p-0026" num="0025">There are a number of different types of hearing prostheses in which embodiments of the present invention may be implemented. However, merely for ease of illustration, the techniques presented herein are primarily described with reference to one type of hearing prosthesis, namely a cochlear implant. It is to be appreciated that the techniques presented herein may be used with, or implemented on/by, other hearing prostheses, such as auditory brainstem stimulators, hearing aids, electro-acoustic hearing prostheses, bimodal hearing prosthesis, bilateral hearing prosthesis, etc.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a schematic diagram of an exemplary cochlear implant <b>100</b> configured to implement aspects of the techniques presented herein, while <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a block diagram of the cochlear implant <b>100</b>. For ease of illustration, <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> will be described together.</p><p id="p-0028" num="0027">The cochlear implant <b>100</b> comprises an external component <b>102</b> and an internal/implantable component <b>104</b>. The external component <b>102</b> is directly or indirectly attached to the body of the recipient and typically comprises an external coil <b>106</b> and, generally, a magnet (not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) fixed relative to the external coil <b>106</b>. The external component <b>102</b> also comprises one or more input elements/devices <b>113</b> for receiving input signals at a sound processing unit <b>112</b>. In this example, the one or more one or more input devices <b>113</b> include sound input devices <b>108</b> (e.g., microphones positioned by auricle <b>110</b> of the recipient, telecoils, etc.) configured to capture/receive input signals, one or more auxiliary input devices <b>109</b> (e.g., audio ports, such as a Direct Audio Input (DAI), data ports, such as a Universal Serial Bus (USB) port, cable port, etc.), and a wireless transmitter/receiver (transceiver) <b>111</b>, each located in, on, or near the sound processing unit <b>112</b>.</p><p id="p-0029" num="0028">The sound processing unit <b>112</b> also includes, for example, at least one battery <b>107</b>, a radio-frequency (RF) transceiver <b>121</b>, and a processing module <b>125</b>. The processing module <b>125</b> comprises a number of elements, including an environmental classifier <b>131</b>, a sound processor <b>135</b>, and an individualized own voice detector <b>135</b>. Each of the environmental classifier <b>131</b>, the sound processor <b>135</b>, and the individualized own voice detector <b>135</b> may be formed by one or more processors (e.g., one or more Digital Signal Processors (DSPs), one or more uC cores, etc.), firmware, software, etc. arranged to perform operations described herein. That is, the environmental classifier <b>131</b>, the sound processor <b>135</b>, and the individualized own voice detector <b>135</b> may each be implemented as firmware elements, partially or fully implemented with digital logic gates in one or more application-specific integrated circuits (ASICs), partially or fully in software, etc.</p><p id="p-0030" num="0029">As described further below, the individualized own voice detector <b>135</b> includes a decision tree, sometimes referred to herein as an own voice detection decision tree, that can be trained/updated. Similarly, the environmental classifier <b>131</b> includes a decision tree, sometimes referred to as an environmental classifier decision tree that, in certain embodiments, can also be trained/updated. To provide the ability to train/update the own voice detection decision tree and/or the environmental classifier decision tree, the decision trees are stored in volatile memory and exposed to, for example, other process for updating thereof. As such, the environmental classifier <b>131</b> and the individualized own voice detector <b>135</b> are at least partially implemented in volatile memory.</p><p id="p-0031" num="0030">In the examples of <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>, the sound processing unit <b>112</b> is a behind-the-ear (BTE) sound processing unit configured to be attached to, and worn adjacent to, the recipient's ear. However, it is to be appreciated that embodiments of the present invention may be implemented by sound processing units having other arrangements, such as by a button sound processing unit (i.e., a component having a generally cylindrical shape and which is configured to be magnetically coupled to the recipient's head), etc., a mini or micro-BTE unit, an in-the-canal unit that is configured to be located in the recipient's ear canal, a body-worn sound processing unit, etc.</p><p id="p-0032" num="0031">Returning to the example embodiment of <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>, the implantable component <b>104</b> comprises an implant body (main module) <b>114</b>, a lead region <b>116</b>, and an intra-cochlear stimulating assembly <b>118</b>, all configured to be implanted under the skin/tissue (tissue) <b>105</b> of the recipient. The implant body <b>114</b> generally comprises a hermetically-sealed housing <b>115</b> in which RF interface circuitry <b>124</b> and a stimulator unit <b>120</b> are disposed. The implant body <b>114</b> also includes an internal/implantable coil <b>122</b> that is generally external to the housing <b>115</b>, but which is connected to the RF interface circuitry <b>124</b> via a hermetic feedthrough (not shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>).</p><p id="p-0033" num="0032">As noted, stimulating assembly <b>118</b> is configured to be at least partially implanted in the recipient's cochlea <b>137</b>. Stimulating assembly <b>118</b> includes a plurality of longitudinally spaced intra-cochlear electrical stimulating contacts (electrodes) <b>126</b> that collectively form a contact or electrode array <b>128</b> for delivery of electrical stimulation (current) to the recipient's cochlea. Stimulating assembly <b>118</b> extends through an opening in the recipient's cochlea (e.g., cochleostomy, the round window, etc.) and has a proximal end connected to stimulator unit <b>120</b> via lead region <b>116</b> and a hermetic feedthrough (not shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>). Lead region <b>116</b> includes a plurality of conductors (wires) that electrically couple the electrodes <b>126</b> to the stimulator unit <b>120</b>.</p><p id="p-0034" num="0033">As noted, the cochlear implant <b>100</b> includes the external coil <b>106</b> and the implantable coil <b>122</b>. The coils <b>106</b> and <b>122</b> are typically wire antenna coils each comprised of multiple turns of electrically insulated single-strand or multi-strand platinum or gold wire. Generally, a magnet is fixed relative to each of the external coil <b>106</b> and the implantable coil <b>122</b>. The magnets fixed relative to the external coil <b>106</b> and the implantable coil <b>122</b> facilitate the operational alignment of the external coil with the implantable coil. This operational alignment of the coils <b>106</b> and <b>122</b> enables the external component <b>102</b> to transmit data, as well as possibly power, to the implantable component <b>104</b> via a closely-coupled wireless link formed between the external coil <b>106</b> with the implantable coil <b>122</b>. In certain examples, the closely-coupled wireless link is a radio frequency (RF) link. However, various other types of energy transfer, such as infrared (IR), electromagnetic, capacitive and inductive transfer, may be used to transfer the power and/or data from an external component to an implantable component and, as such, <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates only one example arrangement.</p><p id="p-0035" num="0034">As noted above, sound processing unit <b>112</b> includes the processing module <b>125</b>. The processing module <b>125</b> is configured to convert input audio signals into stimulation control signals <b>136</b> for use in stimulating a first ear of a recipient (i.e., the processing module <b>125</b> is configured to perform sound processing on input audio signals received at the sound processing unit <b>112</b>). Stated differently, the sound processor <b>133</b> (e.g., one or more processing elements implementing firmware, software, etc.) is configured to convert the captured input audio signals into stimulation control signals <b>136</b> that represent electrical stimulation for delivery to the recipient. The input audio signals that are processed and converted into stimulation control signals may be audio signals received via the sound input devices <b>108</b>, signals received via the auxiliary input devices <b>109</b>, and/or signals received via the wireless transceiver <b>111</b>.</p><p id="p-0036" num="0035">In the embodiment of <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the stimulation control signals <b>136</b> are provided to the RF transceiver <b>121</b>, which transcutaneously transfers the stimulation control signals <b>136</b> (e.g., in an encoded manner) to the implantable component <b>104</b> via external coil <b>106</b> and implantable coil <b>122</b>. That is, the stimulation control signals <b>136</b> are received at the RF interface circuitry <b>124</b> via implantable coil <b>122</b> and provided to the stimulator unit <b>120</b>. The stimulator unit <b>120</b> is configured to utilize the stimulation control signals <b>136</b> to generate electrical stimulation signals (e.g., current signals) for delivery to the recipient's cochlea via one or more stimulating contacts <b>126</b>. In this way, cochlear implant <b>100</b> electrically stimulates the recipient's auditory nerve cells, bypassing absent or defective hair cells that normally transduce acoustic vibrations into neural activity, in a manner that causes the recipient to perceive one or more components of the input audio signals.</p><p id="p-0037" num="0036">As noted, in addition to the sound processor <b>133</b>, the processing module <b>125</b> also includes the environmental classifier <b>131</b>. As described further below, the environmental classifier <b>131</b> (e.g., one or more processing elements implementing firmware, software, etc.) is configured to determine an environmental classification of the sound environment (i.e., determines the &#x201c;class&#x201d; or &#x201c;category&#x201d; of the sound environment) associated with the input audio signals received at the cochlear implant <b>100</b>. In addition, also as described further below, the processing module <b>125</b> comprises the individualized own voice detector <b>135</b> (e.g., one or more processing elements implementing firmware, software, etc.) that is configured to perform individualized own voice detection (OVD). As used herein, own voice detection (OVD) generally refers to a process in which speech signals received at a hearing prosthesis are classified as either including the speech of the recipient of the hearing prosthesis (referred to herein as the recipient's own voice or simply own voice) or speech generated by one or persons other than the recipient (referred to herein as external voice). Also as used herein, individualized own voice detection (or individualized OVD) refers to own voice detection that is recipient-specific, meaning the own voice detection is at least partly trained to perform the own voice detection using (based on) the specific voice (speech) of the recipient of the hearing prosthesis, as captured by the hearing prosthesis itself. As a result, the individualized own voice detection is specific/customized to the recipient of the hearing prosthesis and to the hearing prosthesis itself.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> illustrate an arrangement in which the cochlear implant <b>100</b> includes an external component. However, it is to be appreciated that embodiments of the present invention may be implemented in cochlear implants having alternative arrangements. For example, <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram of an exemplary totally implantable cochlear implant <b>200</b> configured to implement embodiments of the present invention. Since the cochlear implant <b>200</b> is totally implantable, all components of cochlear implant <b>200</b> are configured to be implanted under skin/tissue <b>205</b> of a recipient. Because all components are implantable, cochlear implant <b>200</b> operates, for at least a finite period of time, without the need of an external device. An external device <b>202</b> can be used to, for example, charge an internal power source (battery) <b>207</b>. External device <b>202</b> may be a dedicated charger or a conventional cochlear implant sound processor.</p><p id="p-0039" num="0038">Cochlear implant <b>200</b> includes an implant body (main implantable component) <b>214</b>, one or more input elements <b>213</b> for capturing/receiving input audio signals (e.g., one or more implantable microphones <b>208</b> and a wireless transceiver <b>211</b>), an implantable coil <b>222</b>, and an elongate intra-cochlear stimulating assembly <b>118</b> as described above with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>. The microphone <b>208</b> and/or the implantable coil <b>222</b> may be positioned in, or electrically connected to, the implant body <b>214</b>. The implant body <b>214</b> further comprises the battery <b>207</b>, RF interface circuitry <b>224</b>, a processing module <b>225</b>, and a stimulator unit <b>220</b> (which is similar to stimulator unit <b>120</b> of <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). The processing module <b>225</b> may be similar to processing module <b>125</b> of <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>, and includes environmental classifier <b>231</b>, sound processor <b>233</b>, and individualized own voice detector <b>235</b>, which are similar to the environmental classifier <b>131</b>, sound processor <b>133</b>, the individualized own voice detector <b>135</b>, respectively, described with reference to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>.</p><p id="p-0040" num="0039">In the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the one or more implantable microphones <b>208</b> are configured to receive input audio signals. The processing module <b>225</b> is configured to convert received signals into stimulation control signals <b>236</b> for use in stimulating a first ear of a recipient. Stated differently, sound processor <b>233</b> is configured to convert the input audio signals into stimulation control signals <b>236</b> that represent electrical stimulation for delivery to the recipient.</p><p id="p-0041" num="0040">As noted above, <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> illustrate an embodiment in which the external component <b>102</b> includes the processing module <b>125</b>. As such, in the illustrative arrangement of <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>, the stimulation control signals <b>136</b> are provided to the implanted stimulator unit <b>120</b> via the RF link between the external coil <b>106</b> and the internal coil <b>122</b>. However, in the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref> the processing module <b>225</b> is implanted in the recipient. As such, in the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the stimulation control signals <b>236</b> do not traverse the RF link, but instead are provided directly to the stimulator unit <b>220</b>. The stimulator unit <b>220</b> is configured to utilize the stimulation control signals <b>236</b> to generate electrical stimulation signals that are delivered to the recipient's cochlea via one or more stimulation channels.</p><p id="p-0042" num="0041">In addition to the sound processing operations, as described further below, the environmental classifier <b>231</b> is configured to determine an environmental classification of the sound environment associated with the input audio signals and the individualized own voice detector <b>235</b> is configured to perform individualized own voice detection (OVD).</p><p id="p-0043" num="0042">As noted, the techniques presented herein may be implemented in a number of different types of hearing prostheses. However, for ease of description, further details of the techniques presented herein will generally be described with reference to cochlear implant <b>100</b> of <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>B</figref>.</p><p id="p-0044" num="0043">As noted above, own voice detection (OVD) generally refers to a process in which speech signals received at a hearing prosthesis are classified as either including the &#x201c;voice&#x201d; or &#x201c;speech&#x201d; of the recipient of the hearing prosthesis (referred to herein as the recipient's own voice or simply &#x201c;own voice&#x201d;) or the speech by one or persons other than the recipient (referred to herein as &#x201c;external voice&#x201d;). A classification of received speech signals as own voice or external voice may be helpful in, for example, providing information about how well the recipient performs with the hearing prosthesis (i.e., by indicating how much the recipient speaks and, accordingly, providing information of how &#x201c;actively&#x201d; the recipient uses the prosthesis). If a recipient speaks a large percentage of time, then the recipient is active and, accordingly, the recipient can understand other the speech of others (i.e., the recipient is hearing well) and the hearing prosthesis is operating as intended to improve the recipient's life. Own voice detection may enable the determination of a percentage of time a person's own voice is detected, a percentage of time an external voice is detected, and a percentage of time otherwise (e.g., in quiet or noise)</p><p id="p-0045" num="0044">However, it is non-trivial task to distinguish between own voice and external voice and conventional own voice detection techniques attempt to do so with generic algorithms/processes. These generic algorithms/processes can be inaccurate and unreliable (e.g., when the recipient speaks, a generic own voice detection process may incorrectly decide that an external speaker is speaking). To address these problems, the techniques presented herein use a &#x201c;recipient-specific&#x201d; or &#x201c;individualized&#x201d; own voice detector that is trained/updated using the speech of the specific-recipient of the hearing prosthesis. That is, as described further below, the recipient's own voice is used to train, and potentially dynamically update, the individualized own voice detector. As a result, the individualized own voice detector is specific/customized to the recipient of the hearing prosthesis and to the hearing prosthesis. Since, own voice detection is tailored specifically to the speech of the recipient (and to the specific hearing prosthesis), the result is improved accuracy in classifying input audio signals as own voice or external voice. In turn, this improved classification accuracy enables more accurate data to be logged from the device, which is important so that clinicians have reliable data for prescription/therapy of the recipient. Incorrect data can lead to false evidence of a recipient's understanding of speech/conversations/ability to hear/engagement in life.</p><p id="p-0046" num="0045">Before describing training of an individualized own voice detector, the general operation of the individualized own voice detector is described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. More specifically, <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a functional block diagram illustrating further details of the sound processing module <b>125</b> of cochlear implant <b>100</b>, including the environmental classifier <b>131</b> and individualized own voice detector <b>135</b>. For ease of illustration, elements that are not related to the environmental classification and own voice detection have been omitted from <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0047" num="0046">As noted, the cochlear implant <b>100</b> comprises one or more input devices <b>113</b>. In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the input elements <b>113</b> comprise a first microphone <b>108</b>A, a second microphone <b>108</b>B, and at least one auxiliary input <b>109</b> (e.g., an audio input port, a cable port, a telecoil, etc.). If not already in an electrical form, input devices <b>113</b> convert received/input audio signals into electrical signals <b>153</b>, referred to herein as electrical input signals, which represent the input audio signals. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the electrical input signals <b>153</b> include electrical input signal <b>153</b>A from microphone <b>108</b>A, electrical input signal <b>153</b>B from microphone <b>108</b>B, and electrical input signal <b>153</b>C from auxiliary input <b>115</b>.</p><p id="p-0048" num="0047">The electrical input signals <b>153</b> are provided to the environmental classifier <b>131</b>. The environmental classifier <b>131</b> is configured to evaluate/analyze attributes of the input audio signals (represented by the electrical input signals <b>153</b>) and, based on the analysis, determine a &#x201c;class&#x201d; or &#x201c;category&#x201d; of the sound environment associated with the input audio signals. The environmental classifier <b>131</b> may be configured to categorize the sound environment into a number of classes/categories. In one illustrative example, the environmental classifier <b>131</b> is configured to categorize the sound environment into one of five (5) categories, including &#x201c;Speech,&#x201d; &#x201c;Speech in Noise,&#x201d; &#x201c;Quiet,&#x201d; &#x201c;Noise.&#x201d; and &#x201c;Music.&#x201d; although other categories are possible.</p><p id="p-0049" num="0048">In certain embodiments, the environmental classifier <b>131</b> operates to determine a category for the set of input audio signals by calculating, in real-time, a plurality of time-varying features from the input audio signals and analyzing the calculated time-varying features using a using a type of decision structure tree. As a result of the analysis, the environmental classifier <b>131</b> determines the most likely category for the set of input audio signals. Stated differently, the environmental classifier <b>131</b> includes a number of processes/algorithms that calculate time-varying features from the input audio signals. The environmental classifier <b>131</b> also includes a decision tree that uses all or some of these time-varying features as inputs. The decision tree includes a number of hierarchical/linked branches/nodes that each perform evaluations/comparisons/checks using at least one of the time-varying features to determine the classification at the branch ends (leaves).</p><p id="p-0050" num="0049">As noted above, own voice detection is a process in which speech signals received at a hearing prosthesis, such as cochlear implant <b>100</b>, are classified as either including the voice/speech of the recipient or speech generated by one or more persons other than the recipient. As such, own voice detection is only relevant for the categories of the input audio signals, as determined by the environmental classifier <b>131</b>, that include speech, namely the &#x201c;Speech&#x201d; and &#x201c;Speech in Noise&#x201d; categories (sometimes collectively referred to herein as speech classes or categories). Stated differently, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, when the environmental classifier <b>131</b> determines the input audio signals are associated with a speech class (e.g., &#x201c;Speech&#x201d; or &#x201c;Speech in Noise&#x201d;), then the input audio signals are further classified by the individualized own voice detector <b>135</b> as either being own voice (i.e., the hearing prosthesis recipient is speaking within the set of input audio signals) or as external voice (i.e., someone other than the hearing prosthesis recipient is speaking within the set of input audio signals</p><p id="p-0051" num="0050">The individualized own voice detector <b>135</b> operates by calculating, in real-time, a plurality of time-varying features from the input audio signals (as represented by the electrical input signals <b>153</b>) and analyzing the calculated time-varying features using a using a type of decision tree. As a result of the analysis, the individualized own voice detector <b>135</b> determines the most likely category (i.e., either own voice or external voice) for the set of input audio signals. Stated differently, the individualized own voice detector <b>135</b> includes a number of processes/algorithms that calculate time-varying features from the input audio signals. The individualized own voice detector <b>135</b> also includes a decision tree that uses all or some of these time-varying features as inputs. The decision tree includes a number of hierarchical/linked branches/nodes that each perform evaluations/comparisons/checks using at least one of the time-varying features to determine the classification (i.e., own or external voice) at the branch ends (leaves). That is, the decision tree traverses its &#x201c;branches&#x201d; until it arrives at a &#x201c;leaf&#x201d; and decides &#x201c;own&#x201d; or &#x201c;external.&#x201d;</p><p id="p-0052" num="0051">In accordance with embodiments presented herein, the individualized own voice detector <b>135</b> can calculate a number of different time-varying features from the input audio signals and the specific features may vary for different implementations. For example, the own voice detector <b>135</b> may calculate time-varying features such as amplitude modulations, spectral profile, harmonicity, amplitude onsets, etc.</p><p id="p-0053" num="0052">In general, the decision tree of the own voice detector <b>135</b> checks the values of different time-varying features and the combination of the values of the various time-varying features, relative to pre-determined conditions (weights), determine the result. For example, in one illustrative arrangement, the own voice detector <b>135</b> may utilize five (5) time-varying features and three (3) of these features need to have values of 0.5 or greater, and 2 of these need to have values of 0.7 or greater, in order to generate a determination of own voice, otherwise the resulting determination is external voice.</p><p id="p-0054" num="0053">In order to create a decision tree that is accurate, the decision tree needs appropriate weighting for evaluation of each relevant time-varying feature (i.e., the evaluation condition(s) for the nodes need to be set correctly). To create these weightings (evaluation condition or conditions of a node), in accordance with embodiments presented herein, the decision tree is initially trained in advance using the voice (speech) of the recipient of the hearing prosthesis in a machine learning process. As a result, the weightings used in the nodes of the own voice decision tree in accordance with embodiments presented herein are specifically individualized/personalized for the specific recipient.</p><p id="p-0055" num="0054">As noted, the environmental classifier <b>131</b> and the individualized own voice detector <b>135</b> each make use of decision trees. For ease of illustration and description, the environmental classifier <b>131</b> and the individualized own voice detector <b>135</b>, as well as the corresponding decision trees, are described as separate functional entities. However, it is to be appreciated that the environmental classifier <b>131</b> and the individualized own voice detector <b>135</b> may be implemented as a single element using two decision trees or decision tree segments that operate in a parent/child relationship to generate the different classifications (i.e., the environmental classification and the own voice classification).</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a schematic diagram illustrating training of an individualized own voice detector in accordance with embodiments presented herein. For ease of illustration, <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> will be described with reference to the cochlear implant <b>100</b> of <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>1</b>B, and <b>3</b></figref>.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates a clinical setting in which the cochlear implant <b>100</b> is in communication (e.g., wired or wireless communication) with a computing device <b>150</b>, such as a fitting system. In this example, the recipient, generally represented in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> at <b>140</b>, of the cochlear implant <b>100</b> is instructed to speak. The recipient's speech may be elicited in a number of different manners. For example, the recipient <b>140</b> may be instructed to provide free-form speech, to describe a picture or other item, to read one or more sentences, paragraphs, etc. In certain examples, the recipient's speech is elicited so as to include questions, statements, singing, and normal conversations.</p><p id="p-0058" num="0057">The recipient's speech results in the generation of a &#x201c;recipient own voice waveform,&#x201d; which is generally represented in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> at <b>142</b>. While the recipient <b>140</b> is speaking (i.e., during generation of the recipient own voice waveform <b>142</b>), the recipient, clinician, or other user provides a user input at the cochlear implant <b>100</b>, computing device <b>150</b>, or another device to generate a &#x201c;speech label&#x201d; <b>144</b>. In one example, the speech label <b>144</b> is created by the recipient, clinician, or other user by pressing and holding a button while the recipient <b>140</b> is actually speaking and releasing the button while the recipient <b>140</b> is not speaking. The result is a time-varying (e.g., square-wave) speech label <b>144</b> that is generated in real-time and provided to the computing device <b>150</b>. As described further below, the time-varying label <b>144</b> that is time synchronized with the speech of the recipient.</p><p id="p-0059" num="0058">In <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the recipient own voice waveform <b>142</b> (i.e., the recipient's speech) is also captured/received by one or more sound input devices <b>108</b> of the cochlear implant <b>100</b>, along with any other ambient sounds in the clinical environment. The input audio signals, including the recipient own voice waveform <b>142</b>, are provided (in electrical form) to the environmental classifier <b>131</b>. The environmental classifier <b>131</b> operates to classify the input audio signals, for given time periods, within one of the predetermined categories (e.g., &#x201c;Speech,&#x201d; &#x201c;Speech in Noise,&#x201d; &#x201c;Noise,&#x201d; &#x201c;Quiet,&#x201d; &#x201c;Music,&#x201d; etc.).</p><p id="p-0060" num="0059">If the environmental classifier <b>131</b> determines that the input audio signals are associated with a speech class (e.g., are classified as &#x201c;Speech&#x201d; or &#x201c;Speech in Noise&#x201d; signals), then the input audio signals are provided to the individualized own voice detector <b>135</b>. The individualized own voice detector <b>135</b> includes a plurality of feature calculators <b>143</b> (i.e., processes/algorithms) that, for a given time period, calculate a plurality of different time-varying features from the input audio signals. The time-varying features vary over time, but are not necessarily linearly with the input (e.g., if the audio signal is very loud, then very soft, one time-varying feature may not change at all, while another time-varying feature may change rapidly). In general, each time-varying feature is the output of a specifically engineered feature-calculation algorithm that operates blind and independent of the other feature-calculation algorithm and comprise the data used by the decision tree to determine whether or not the input audio signals include own voice. These time-varying features may include, for example, volume level, proximity level, modulation depth, etc.</p><p id="p-0061" num="0060">In certain embodiments, the time-varying features are continuously generated and the outputs of the feature calculators <b>143</b> are sampled at discrete intervals (e.g., every 100 values, every 10 milliseconds, etc.) and these samples are subsequently used, as described below, by the environmental classifier <b>131</b> and/or in a training process.</p><p id="p-0062" num="0061">The environmental classifier <b>131</b> includes an own voice detection decision tree <b>148</b> that uses the time-varying features (as calculated by the plurality of feature calculators <b>143</b> and sampled at the outputs thereof) to classify the input audio signals within a predetermined time period/segment as either Own Voice or External Voice. The time segments may have different lengths (e.g., 100 milliseconds, a second, several seconds, etc.) in different embodiments.</p><p id="p-0063" num="0062">It is to be appreciated that, at the beginning of a clinical fitting process, the individualized own voice detector <b>135</b> has not yet been &#x201c;individualized&#x201d; or &#x201c;personalized&#x201d; for the recipient. Instead, the own voice detection decision tree <b>148</b> is initially programmed as a generic (i.e., not individualized) decision tree that operates to make the initial own voice or external voice classification at the outset of a fitting process based on (using) standard (non-recipient specific) voice samples. The initial programming of the own voice detection decision tree <b>148</b> using standard voice samples is simply to provide a baseline for operation of the decision tree upon receipt of the input audio signals within the clinic. Therefore, at the beginning of a clinical fitting process, the own voice detection decision tree <b>148</b> can be referred to as a &#x201c;generic&#x201d; or &#x201c;standard&#x201d; own voice detection. However, as described below, as the training process continues, the own voice detection decision tree <b>148</b> becomes personalized to the recipient and the hearing prosthesis.</p><p id="p-0064" num="0063">Returning to the example of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, as noted above, for each of the analyzed time periods/segments, the individualized own voice detector <b>135</b> (i.e., the own voice detection decision tree <b>148</b>) generates a classification of the signals within the associated period as being either own voice or external voice. After the individualized own voice detector <b>135</b> generates one or more these classifications, the individualized own voice detector <b>135</b> sends the calculated time-varying features, generated by the feature calculators <b>143</b>, to the computing device <b>135</b>. In <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the calculated time-varying features are generally represented by arrows <b>152</b>.</p><p id="p-0065" num="0064">In the arrangement of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the computing device <b>150</b> includes a decision tree update module <b>154</b>. The decision tree update module <b>154</b> is configured to execute machine learning, using the time-varying features <b>152</b> received from the cochlear implant <b>100</b> and the speech label <b>144</b>, to train/update the own voice detection decision tree. Stated differently, the computing device <b>150</b> performs machine learning to generate, in real-time, updated own voice decision tree weights <b>156</b> (i.e., updates to the conditions for evaluation of a time varying feature at nodes of the own voice detection decision tree <b>148</b>). Further details of the machine learning process at the decision tree update module <b>154</b> are described further below.</p><p id="p-0066" num="0065">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the updated decision tree weights <b>156</b> (i.e., updated evaluation conditions) are then provided back to the individualized own voice detector <b>135</b>. The individualized own voice detector <b>135</b> then updates the current implementation of the own voice detection decision tree <b>148</b> with the updated weights <b>156</b> received from the computing device <b>150</b> (i.e., the updated decision tree weights are sent back to the processing module and the running decision tree is updated in real time the received weights). In general, updating weights of the updated decision tree include, for example, setting a hierarchy of checks for time-varying features in or across nodes and/or setting values for features to trigger different decisions at one or more nodes.</p><p id="p-0067" num="0066">The process shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> may be repeated a number of times, where the same or different recipient speech is analyzed with each iteration. In general, with each iteration, the own voice decision tree <b>148</b> is further customized for the recipient.</p><p id="p-0068" num="0067">As noted above, the decision tree update module <b>154</b> is configured to execute machine learning, using the time-varying features <b>152</b> received from the cochlear implant <b>100</b> and the speech label <b>144</b>, to train/update the own voice detection decision tree weights. <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a schematic diagram illustrating further details of one example implementation of the decision tree update module <b>154</b>. It is to be appreciated that the machine learning at decision tree update module <b>154</b> may be implemented in a number of different manners and, as such, the implementation of <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is merely illustrative.</p><p id="p-0069" num="0068">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the decision tree update module <b>154</b> includes own voice decision tree <b>160</b> and a loss function <b>162</b>. As noted above, the decision tree update module <b>154</b> receives the time-varying features <b>152</b> generated by the own voice detection tree <b>148</b>, as well as the time-varying speech label <b>144</b>. The time-varying features <b>152</b> and the speech label <b>144</b> are time synchronized (i.e., linked/associated) so as to form, for a given time period, a data-label pair <b>163</b>. That is, a data-label pair <b>163</b> is comprised of the time-varying features <b>152</b> determined for a time period (i.e., the data) and the time-varying speech label <b>144</b> (i.e., the label) for the same corresponding time period. As noted, the time-varying features <b>152</b> are features calculated from the input audio signals and analyzed by the own voice detection decision tree <b>148</b> and, as such, generally illustrate the &#x201c;classification&#x201d; of the input audio signals made by the own voice detection decision tree <b>148</b> for the corresponding time period. The speech label <b>144</b> is the authoritative (actual) classification of the input audio signals at the same time period generated, for example, by the recipient, clinician, or other user button push.</p><p id="p-0070" num="0069">In the example of <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the own voice decision tree <b>160</b> uses the time-varying features <b>152</b> received from the cochlear implant <b>100</b> to generate a predicted classification <b>164</b> for a given time period. The loss function <b>162</b> then calculates a &#x201c;loss&#x201d; using the predicted classification <b>164</b> and the speech label <b>144</b> associated with the same time period. The generated loss represents the error/difference between the predicted classification <b>164</b> generated by the own voice machine learning model <b>160</b> (i.e., using the data in the data-label pair) and the associated speech label <b>144</b> (the label in a data-label pair). The determined loss <b>165</b> is then fed back to the own voice decision tree <b>160</b> and used to adapt the decision tree weights until the predicted classification <b>164</b> matches the speech label <b>144</b> (i.e., the system trains itself by minimizing the loss/error).</p><p id="p-0071" num="0070">Eventually, after updating using one or more data-label pairs <b>163</b>, the weights of the own voice decision tree <b>160</b> (i.e., the updated decision tree weights <b>156</b> of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) are sent to the cochlear implant <b>100</b> for instantiation as the own voice detection decision tree <b>148</b> (i.e., replacement of the previous instance of the own voice detection decision tree <b>148</b>). At this time instance, the own voice detection decision tree <b>148</b> generally matches the own voice decision tree <b>160</b> that has been updated at the computing device <b>150</b>. However, through subsequent machine learning training, the own voice decision tree <b>160</b> will eventually evolve and begin to operate differently from the own voice detection decision tree <b>148</b>, at least until the own voice detection decision tree <b>148</b> is updated/replaced using further updated weights received from the computing device <b>150</b>.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> have generally been described with reference to one implementation in which speech of the recipient is captured, labeled, and used to update the own voice detection decision tree <b>148</b>. However, it is to be appreciated that the processes of <figref idref="DRAWINGS">FIGS. <b>4</b>A</figref> and/or <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> can be executed with other types of inputs. For example, in one alternative arrangement, external speech (e.g., speech of the clinician or a caregiver) may be captured, labeled (as described above) and used to update the own voice detection decision tree <b>148</b>. In such examples, a data-label pair received at the decision tree update module <b>154</b> will still include the time-varying parameters <b>152</b>, but the label will indicate external voice at the corresponding time period, rather than indicate own voice as in the arrangement of <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref>. In still other examples, the recipient or external speech may be captured with or without the presence of background noise to train the own voice detection decision tree <b>148</b> to operate in different environments. In other examples, the own voice detection decision tree <b>148</b> may be updated using previously recorded &#x201c;external speech,&#x201d; &#x201c;own speech in noise&#x201d; and &#x201c;external noisy speech in noise&#x201d; (i.e., recorded audio samples) to at least initialize the decision tree.</p><p id="p-0073" num="0072">In summary, <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> generally illustrate arrangements in which the individualized own voice detector <b>135</b> and, more specifically, the own voice detection decision tree <b>148</b> is trained using the recipient's own voice/speech. <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> illustrate a supervised learning approach, where labeled input data is used to train the algorithm(s). Through the training of <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref>, the own voice detection decision tree <b>148</b> (i.e., the decision tree weights) is specifically customized to the characteristics of the recipient's voice (speech).</p><p id="p-0074" num="0073">In addition, it should be noted that the cochlear implant <b>100</b> includes a number of electrical components (e.g., microphones, processors, etc.) that have associated operating characteristics/properties that be different from the electrical components on other devices. In the embodiments of <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref>, these electrical components, and thus the associated operating characteristics, are implicitly used in the training process, e.g., through the receiving of the input audio signals, generating the environmental and own voice classifications, etc. As a result, the training process described above will inherently account for the electrical properties of the individual sound processors, thus improving the accuracy of the decision tree relative to generic own voice detection by removing electrical idiosyncrasies from the real-time operational analysis.</p><p id="p-0075" num="0074">After initial training of the individualized own voice detector <b>135</b>, such as that described with reference to <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref>, the recipient is sent home and the individualized own voice detector <b>135</b> operates to classify input audio signals as either own voice or external voice. However, in accordance with further embodiments presented herein, operation of the own voice detection decision tree <b>148</b> may also be updated outside of a clinical setting. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram illustrating one example arrangement for updating operation of the own voice detection decision tree <b>148</b> by dynamically updating operation of the environmental classifier <b>131</b>.</p><p id="p-0076" num="0075">More specifically <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates that the environmental classifier <b>131</b> includes an environmental classifier decision tree <b>166</b> and an environmental classifier analysis module <b>167</b>. In this example, the recipient speaks and the speech is captured by the sound input device(s) <b>108</b>. While the recipient is speaking, the recipient or other user provides a user input (e.g., deliberately presses a button) to override operation of the environmental classifier decision tree <b>166</b>. This button press, which is generally represented in <figref idref="DRAWINGS">FIG. <b>5</b></figref> by arrow <b>168</b>, indicates that the signals received during that time period (i.e., while the button is pressed) should be classified as speech (e.g., either &#x201c;Speech&#x201d; or &#x201c;Speech in Noise&#x201d; signals). That is, the input audio signals received during the time period are labeled by the user as speech and, as such, the environmental classifier <b>131</b> has the opportunity to update the environmental classifier decision tree <b>166</b> based on the content of the input audio signals.</p><p id="p-0077" num="0076">When the user overrides operation of the environmental classifier decision tree <b>166</b>, the time-varying feature analyzed by the environmental classifier decision tree <b>166</b>, during the override time period, are provided to the environmental classifier analysis module <b>167</b>. Similar to the above examples, these time-varying features, which are represented in <figref idref="DRAWINGS">FIG. <b>5</b></figref> by arrow <b>169</b>, are calculated by feature extractors <b>173</b> (e.g., processes/algorithms) which operate on the input audio signals. The values of the time-varying features analyzed by the environmental classifier decision tree <b>166</b> during the override time period are sometimes referred to herein as &#x201c;manual&#x201d; feature values because they are the values of the time-varying feature when the user manually sets the classification to a speech class (e.g., &#x201c;Speech&#x201d; or &#x201c;Speech in Noise&#x201d;). The environmental classifier analysis module <b>167</b> operates by analyzing the values of the time-varying features during the override time period, and the resulting classification by the decision tree <b>166</b> (i.e., ether own or external voice) in view of the so-called &#x201c;automated&#x201d; time-varying feature values and automated decision tree operation. That is, a comparison is done on the existing feature checks by the environmental classifier decision tree <b>166</b> (i.e., referred to as the automated feature values) and the manual feature checks. Because the user has manually set the environmental classifier decision tree to a speech class, the system determines that all incoming signals are going to be speech. As such, the analysis module <b>167</b> checks to determine whether the time-varying feature values that are provided by the environmental classifier decision tree <b>166</b> in this override (manual) period are different to those in the existing checks done by the decision tree (i.e., automated). If they are significantly different by some threshold or other definable metric (e.g., by more than 50% of the original setting), then the operation of the environmental classifier decision tree <b>166</b> can be adjusted. Adjustment of the environmental classifier decision tree <b>166</b> is generally represented in <figref idref="DRAWINGS">FIG. <b>5</b></figref> by arrow <b>172</b>.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an arrangement in with operation of the environmental classifier decision tree <b>166</b>, rather than the own voice detection decision tree <b>148</b> (<figref idref="DRAWINGS">FIG. <b>4</b>A</figref>), is adjusted. However, the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref> improves the performance of the own voice detection decision tree <b>148</b> because the certainty of speech is higher when the input audio signals are received at the individualized own voice detector <b>135</b>. In a further embodiment, the own voice detection decision tree <b>148</b> may also be updated along with the environmental classifier tree <b>166</b>. Such an arrangement is illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref></p><p id="p-0079" num="0078">More specifically, <figref idref="DRAWINGS">FIG. <b>6</b></figref> first illustrates the environmental classifier <b>131</b> implemented as described above with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. As described above with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the environmental classifier decision tree <b>166</b> can be adjusted, on the cochlear implant <b>100</b>, in response to a user input (e.g., button press) <b>168</b>.</p><p id="p-0080" num="0079">However, <figref idref="DRAWINGS">FIG. <b>6</b></figref> also illustrates the individualized own voice detector <b>135</b>, which in this example includes the own voice detection decision tree <b>148</b> and an own voice detector analysis module <b>176</b>. As noted above with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the recipient speaks and the speech is captured by the sound input device(s) <b>108</b>. While the recipient is speaking, the recipient or other user provides a user input <b>168</b> (e.g., deliberately presses a button). In the example of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, this user input <b>168</b> overrides both the environmental classifier decision tree <b>166</b> and the own voice detection decision tree <b>148</b>.</p><p id="p-0081" num="0080">As noted above, this user input <b>168</b> indicates that the signals received during that time period (i.e., while the button is pressed) are speech signals. However, in the specific arrangement of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the user input <b>168</b> also indicates that the signals are own voice, meaning that the indicated speech is the recipient's own speech. As a result, the individualized own voice detector <b>135</b> has the opportunity to update the own voice detection decision tree <b>148</b> based on the content of the input audio signals.</p><p id="p-0082" num="0081">When the user overrides operation of the own voice detection decision tree <b>148</b> (via user input <b>168</b>), the calculated time-varying features, represented in <figref idref="DRAWINGS">FIG. <b>6</b></figref> by arrow <b>152</b>, are provided to the own voice detector analysis module <b>176</b>. The feature values of the own voice detection decision tree <b>148</b> during the override time period are sometimes referred to herein as &#x201c;manual&#x201d; feature values because they are the feature values when the user manually sets the classification to a class that includes own voice. The own voice detector analysis module <b>176</b> operates by comparing the &#x201c;manual&#x201d; feature values to so-called &#x201c;automated&#x201d; feature values of the own voice detection decision tree <b>148</b>. That is, a comparison is done on the existing feature checks by the own voice detection decision tree <b>148</b> (i.e., referred to as the automated feature values) and the manual feature values (i.e., feature values calculated override (manual) time period). Because the user has manually set the own voice detection decision tree <b>148</b> to the own voice class, the system determines that all incoming signals are going to be own voice. As such, the analysis module <b>176</b> checks to determine whether the feature values that are provided by the own voice detection decision tree <b>148</b> in this override (manual) period are different to those in the existing checks done by the decision tree (i.e., automated). If they are significantly different by some threshold or definable metric (e.g., by more than 50% of the original setting) then the operation of the own voice detection decision tree <b>148</b> can be adjusted. Adjustment of the own voice detection decision tree <b>148</b> is generally represented in <figref idref="DRAWINGS">FIG. <b>5</b></figref> by arrow <b>180</b>.</p><p id="p-0083" num="0082">As noted, <figref idref="DRAWINGS">FIG. <b>5</b></figref> generally illustrates an embodiment in which operation of the environmental classifier <b>131</b> is dynamically updated based on a user input. <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates another embodiment in which the operation of the environmental classifier <b>131</b> is dynamically updated based on one or more time-varying features calculated from input audio signals.</p><p id="p-0084" num="0083">More specifically, some value calculated on the device has a relationship with the input signal(s) that can be used in the environmental classifier. In the example of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a supplemental feature delivery module <b>181</b> is provided. The supplemental feature delivery <b>181</b> is configured to receive the input audio signals (represented by the electrical input signals <b>153</b> described above) and is configured implement a process to calculated and deliver time-varying features (e.g., fundamental frequency (F0), an estimate of a harmonic signal power-to-total power ratio (STR), etc.) to the environmental classifier <b>131</b> to provide further information regarding the probability of the signal being &#x201c;Speech.&#x201d; As such, in these examples, the classification of a current sound environment associated with input audio signals is based, at least in part, on one or more supplemental time-varying features, such estimate of a harmonic signal power-to-total power ratio (STR) associated with the input audio signals, an estimate of a fundamental frequency (F0) associated with the input audio signals, etc. In certain arrangements, these time-varying features can be included one or more of the above embodiments to improve the decision tree at mapping time in clinic, or dynamically post clinic. The example of <figref idref="DRAWINGS">FIG. <b>7</b></figref> improves the performance of the own voice detection decision tree <b>148</b> because the certainty of speech is higher when the input audio signals are received at the individualized own voice detector <b>135</b>.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic diagram illustrating that, in certain embodiments, the individualized own voice detector <b>135</b> can be dynamically updated using a remote or cloud-based arrangement. For example, the recipient's speech <b>182</b> may be recorded (e.g., during a fitting session, off-line, etc.) and stored, for example, in the cloud and, post-clinic, the individualized own voice detector <b>135</b> can be trained further using this recorded speech. For example, if new data <b>183</b> has been mined which improves &#x201c;external speech&#x201d; detection, a cloud-based machine learning decision tree calculator <b>184</b> may take the existing/recorded speech <b>182</b> (e.g., from the recipient's clinical session) and combine it with the new external speech data <b>183</b> to generate a new own voice detection decision tree <b>185</b>. This new own voice detection decision tree <b>185</b> may be provided to, and instantiated at, the individualized own voice detector <b>135</b>. In this example, the cloud-based machine learning decision tree calculator <b>184</b> can be Internet-based, or can be implemented on a local server with a local database.</p><p id="p-0086" num="0085">As noted above, in accordance with embodiments presented herein, an own voice detection decision tree and/or an environmental classification decision tree may be dynamically updated on/by a hearing prosthesis itself, such as cochlear implant <b>100</b>, or updated using an external computing device, such as external computing device <b>150</b> described above with reference to <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref>. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating one example arrangement for external computing device <b>150</b> configured to perform one or more operations in accordance with certain embodiments presented herein.</p><p id="p-0087" num="0086">External computing device <b>150</b> comprises a plurality of interfaces/ports <b>192</b>(<b>1</b>)-<b>192</b>(N), a memory <b>193</b>, a processor <b>194</b>, and a user interface <b>195</b>. The interfaces <b>192</b>(<b>1</b>)-<b>192</b>(N) may comprise, for example, any combination of network ports (e.g., Ethernet ports), wireless network interfaces, Universal Serial Bus (USB) ports, Institute of Electrical and Electronics Engineers (IEEE) 1394 interfaces, PS/2 ports, etc. In the example of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, interface <b>192</b>(<b>1</b>) is connected to cochlear implant <b>100</b> having components implanted in a recipient <b>140</b>. Interface <b>192</b>(<b>1</b>) may be directly connected to the cochlear implant <b>100</b> or connected to an external device that is communication with the cochlear implant <b>100</b>. Interface <b>192</b>(<b>1</b>) may be configured to communicate with the cochlear implant <b>100</b> via a wired or wireless connection.</p><p id="p-0088" num="0087">The user interface <b>195</b> includes one or more output devices, such as a liquid crystal display (LCD) and a speaker, for presentation of visual or audible information to a clinician, audiologist, or other user. The user interface <b>195</b> may also comprise one or more input devices that include, for example, a keypad, keyboard, mouse, touchscreen, etc. that can accept a user input.</p><p id="p-0089" num="0088">The memory <b>193</b> comprises decision tree update <b>196</b> that may be executed to generate or update an own voice detection decision tree (i.e., generate updated decision tree weights), as described elsewhere herein. It would be appreciated that memory <b>193</b> may include other logic elements that, for ease of illustration, have been omitted from <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0090" num="0089">Memory <b>193</b> may comprise read only memory (ROM), random access memory (RAM), magnetic disk storage media devices, optical storage media devices, flash memory devices, electrical, optical, or other physical/tangible memory storage devices. The processor <b>194</b> is, for example, a microprocessor or microcontroller that executes instructions for the apical protection logic <b>196</b>. Thus, in general, the memory <b>193</b> may comprise one or more tangible (non-transitory) computer readable storage media (e.g., a memory device) encoded with software comprising computer executable instructions and when the software is executed (by the processor <b>194</b>) it is operable to perform operations described herein.</p><p id="p-0091" num="0090">It is to be appreciated that the arrangement for external computing device <b>150</b> shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> is illustrative and that an external computing device <b>150</b> in accordance with embodiments presented herein may include any combination of hardware, software, and firmware configured to perform the functions described herein. For example, the external computing device <b>150</b> may be a personal computer, handheld device (e.g., a tablet computer), a mobile device (e.g., a mobile phone), and/or any other electronic device having the capabilities to perform the associated operations described elsewhere herein.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart of method <b>1000</b>, in accordance with embodiments presented herein. Method <b>1000</b> begins at <b>1002</b> where one or more microphones of a hearing prosthesis capture input audio signals that include a voice of a recipient of the hearing prosthesis. At <b>1004</b>, the hearing prosthesis calculates time-varying features from the input audio signals. At <b>1006</b>, based on an analysis of a plurality of the time-varying features, the operation of an own voice detection decision tree of the hearing prosthesis is updated.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart of another method <b>1100</b>, in accordance with embodiments presented herein. Method <b>1100</b> begins at <b>1102</b> where a bearing prosthesis receives input audio signals that include speech of a recipient of the hearing prosthesis. At <b>1104</b>, the hearing prosthesis calculates time-varying features from the input audio signals. At <b>1106</b>, a plurality of the time-varying features are analyzed with an own voice detection decision tree on the hearing prosthesis. At <b>1108</b>, label data associated the input audio signals is received, where the label data indicates which time segments of the input audio signals include the voice of a recipient. At <b>1110</b>, the plurality of time-varying features and the label data are analyzed to generate updated weights for the own voice detection decision tree. At <b>1112</b>, the own voice detection decision tree is updated with the updated weights.</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart of another method <b>1200</b>, in accordance with embodiments presented herein. Method <b>1200</b> begins at <b>1202</b> where time-varying features generated from input audio signals captured at one or more microphones of a hearing prosthesis are received. The input audio signals include a voice of a recipient of a hearing prosthesis. At <b>1204</b>, label data associated the input audio signals is received, wherein the label data indicates which of the plurality of time segments of the input audio signals include the voice of a recipient. At <b>1206</b>, the plurality of time-varying features and the label data are analyzed to generate updated weights for an own voice detection decision tree on the hearing prosthesis. At <b>1208</b>, the own voice detection decision tree is updated with the updated weights to generate an updated an own voice detection decision tree.</p><p id="p-0095" num="0094">It is to be appreciated that the above described embodiments are not mutually exclusive and that the various embodiments can be combined in various manners and arrangements.</p><p id="p-0096" num="0095">The invention described and claimed herein is not to be limited in scope by the specific preferred embodiments herein disclosed, since these embodiments are intended as illustrations, and not limitations, of several aspects of the invention. Any equivalent embodiments are intended to be within the scope of this invention. Indeed, various modifications of the invention in addition to those shown and described herein will become apparent to those skilled in the art from the foregoing description. Such modifications are also intended to fall within the scope of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>capturing input audio signals that include a voice of a user;</claim-text><claim-text>determining, from the input audio signals, a primary classification of a current sound environment associated with the input audio signals, wherein the primary classification indicates that the current sound environment includes speech signals;</claim-text><claim-text>after determining the primary classification, calculating time-varying features from the input audio signals; and</claim-text><claim-text>updating, based on an analysis of a plurality of the time-varying features, operation of an own voice detector.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the own voice detector is configured for classification of one or more time segments of the input audio signals as either including the voice of the user or as including an external voice.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein updating, based on an analysis of the plurality of the time-varying features, operation of an own voice detector, comprises:<claim-text>obtaining a time-varying label that is time synchronized with the plurality of the time-varying features; and</claim-text><claim-text>analyzing the plurality of the time-varying features and the time-varying label to generate updated decision tree weights for a decision tree.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein analyzing the plurality of the time-varying features and the time-varying label to generate updated decision tree weights comprises:<claim-text>executing a machine learning process to analyze the plurality of the time-varying features representative of the user's voice relative to values of the time-varying label at corresponding times.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein obtaining a time-varying label that is time synchronized with the plurality of the time-varying features comprises:<claim-text>receiving a user input indicating which time segments of the input audio signals include the voice of the user.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the primary classification of a current sound environment associated with the input audio signals, comprises:<claim-text>determining the primary classification of the current sound environment based in part on an estimate of a harmonic signal power-to-total power ratio (STR) associated with the input audio signals.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the primary classification of a current sound environment associated with the input audio signals, comprises:<claim-text>determining the primary classification of the current sound environment based in part on an estimate of a fundamental frequency (F0) associated with the input audio signals.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method, comprising:<claim-text>receiving input audio signals at a device, wherein the input audio signals include speech of a user of the device;</claim-text><claim-text>calculating time-varying features from the input audio signals; and</claim-text></claim-text><claim-text>analyzing a plurality of the time-varying features with an own voice detection module to classify one or more time segments of the input audio signals as either including the voice of the user or as including an external voice.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>receiving label data associated the input audio signals, wherein the label data indicates which time segments of the input audio signals include the voice of the user; and</claim-text><claim-text>analyzing the plurality of the time-varying features and the label data to update operation of the own voice detection module.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the own voice detection module includes a decision tree, and wherein analyzing the plurality of the time-varying features and the label data to update operation of the own voice detection module comprises:<claim-text>generating updated weights for the decision tree; and</claim-text><claim-text>updating decision tree with the updated weights.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the label data is time-varying and time synchronized with the plurality of the time-varying features.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein generating updated weights for the decision tree comprises:<claim-text>executing a machine learning process to generate the updated weights for the decision tree based on the plurality of the time-varying features and the label data.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein receiving label data associated the input audio signals comprises:<claim-text>receiving a user input indicating which time segments of the input audio signals received at the device include the voice of the user.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein prior to analyzing a plurality of the time-varying features, the method comprises:<claim-text>determining, from the input audio signals, a primary classification of a current sound environment associated with the input audio signals, wherein the primary classification indicates that the current sound environment includes speech signals.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein determining the primary classification of a current sound environment associated with the input audio signals, comprises:<claim-text>determining the primary classification of the current sound environment based in part on an estimate of a harmonic signal power-to-total power ratio (STR) associated with the input audio signals.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein determining the primary classification of a current sound environment associated with the input audio signals, comprises:<claim-text>determining the primary classification of the current sound environment based in part on an estimate of a fundamental frequency (F0) associated with the input audio signals.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. One or more non-transitory computer readable storage media comprising instructions that, when executed by a processor, cause the processor to:<claim-text>obtain a plurality of time-varying features generated from input audio signals received at a device, wherein the input audio signals include a voice of a user of the device;</claim-text><claim-text>receiving label data associated the input audio signals, wherein the label data indicates which of a plurality of time segments of the input audio signals include the voice of the user; and</claim-text><claim-text>analyzing the plurality of time-varying features and the label data to update operation of an own voice detection module.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The one or more non-transitory computer readable storage media of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the own voice detection module is configured for classification of one or more time segments of input audio signals received at the device as either including the voice of the user or as including an external voice.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The one or more non-transitory computer readable storage media of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the label data is time-varying and time synchronized with the plurality of time-varying features.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The one or more non-transitory computer readable storage media of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein analyzing the plurality of time-varying features and the label data to generate updated operation of the own voice detection module comprises:<claim-text>executing a machine learning process to generate updated weights for an own voice detection decision tree based on the plurality of time-varying features and the label data.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The one or more non-transitory computer readable storage media of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein receiving label data associated the input audio signals comprises:<claim-text>receiving a user input indicating which time segments of the input audio signals received at the device include the voice of the user.</claim-text></claim-text></claim></claims></us-patent-application>