<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004747A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004747</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17365045</doc-number><date>20210701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>32</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>34</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>3258</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>344</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6268</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>628</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>2209</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">CHARACTER RECOGNITION OF LICENSE PLATE UNDER COMPLEX BACKGROUND</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>International Business Machines Corporation</orgname><address><city>Armonk</city><state>NY</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Zhu</last-name><first-name>Sheng Nan</first-name><address><city>Shanghai</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>HU</last-name><first-name>GUOQIANG</first-name><address><city>Shanghai</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Ding</last-name><first-name>Yuan Yuan</first-name><address><city>Shanghai</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Ji</last-name><first-name>Peng</first-name><address><city>Nanjing</city><country>CN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Li</last-name><first-name>Fan</first-name><address><city>Shanghai</city><country>CN</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Xu</last-name><first-name>Jian</first-name><address><city>Shanghai</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system, method, and computer program product provides a way to separate connected or adhered adjacent characters of a digital image for license plate recognition. As a threshold processing, the method performs a recognition of character adhesion by obtaining character parameters using an image processor. The parameters include a horizontal max crossing and a ratio of width and height. A first rule-based module is used responsive to the character parameters to distinguish the adhered characters (character adhesions) that are easy to judge, leaving the uncertain part to a character adhesion classifier model for discrimination. Character adhesion data is obtained by data augmentation including the adding of a random distance between two single characters to create class like adhered characters. Then the character adhesion classifier model of single character and character adhesion data is trained. Any uncertain part can be distinguished by the trained character adhesion classifier model.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="175.09mm" wi="151.81mm" file="US20230004747A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="202.01mm" wi="153.84mm" file="US20230004747A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="233.26mm" wi="135.64mm" file="US20230004747A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="226.57mm" wi="139.36mm" orientation="landscape" file="US20230004747A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="231.39mm" wi="172.72mm" orientation="landscape" file="US20230004747A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="162.98mm" wi="51.05mm" orientation="landscape" file="US20230004747A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="205.06mm" wi="166.20mm" orientation="landscape" file="US20230004747A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="229.95mm" wi="166.20mm" orientation="landscape" file="US20230004747A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="195.24mm" wi="106.76mm" orientation="landscape" file="US20230004747A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="227.08mm" wi="145.80mm" file="US20230004747A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="231.06mm" wi="83.06mm" orientation="landscape" file="US20230004747A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="133.18mm" wi="155.70mm" file="US20230004747A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">The present disclosure relates to systems and methods for recognizing license plate characters.</p><p id="p-0003" num="0002">Character recognition of license plates in natural scene images remains challenging due to the following facts:</p><p id="p-0004" num="0003">1) In one instance, the main problem is lack of license plate data of a particular jurisdiction. Due to easy access to public binary char data, a classical algorithm for single character recognition after segmentation is adopted to solve the problem.</p><p id="p-0005" num="0004">2) Following use of the classical algorithm, character adhesion caused by low-resolution, distortion, etc. often happens in segmentation and makes the recognition difficult.</p><p id="p-0006" num="0005">Moreover, diverse fonts and unfixed font spacing take great challenges to the separation of character adhesion in license plates of certain jurisdictions.</p><heading id="h-0002" level="1">BRIEF SUMMARY</heading><p id="p-0007" num="0006">A system, method, and computer program product described herein provide a method and framework to improve the recognition of characters adhesion by a character adhesion classifier.</p><p id="p-0008" num="0007">The system, method, and computer program product first implements a rule-based method is used to first identify the characters adhesion based on one or more obtained character parameters.</p><p id="p-0009" num="0008">Those uncertain characters will be judged subsequently by a character adhesion classifier model trained with characters adhesion data including data obtained by data augmentation.</p><p id="p-0010" num="0009">Any adhered characters are subject to an incremental split and hard split techniques which can be combined to separate adhered characters.</p><p id="p-0011" num="0010">In one aspect, there is provided a method implemented by at least one hardware processor comprising: receiving, at the at least one hardware processor, a digital image comprising a sequence of characters; evaluating, implementing the at least one hardware processor, the digital image to determine a connectivity of one or more adjacent characters; for connected adjacent characters, performing image processing using the at least one hardware processor to split and segment the connected characters; and performing, using the at least one hardware processor, character recognition to determine the sequence of characters including the segmented characters of the original digital image.</p><p id="p-0012" num="0011">In a further aspect, there is provided a system comprising at least one processor comprising hardware, the at least one processor configured to: receive a digital image comprising a sequence of characters; evaluate the digital image to determine a connectivity of one or more adjacent characters; for connected adjacent characters, perform image processing to split and segment the connected characters; and perform character recognition to determine the sequence of characters including the segmented characters of the original digital image.</p><p id="p-0013" num="0012">In some aspects, a computer readable storage medium is disclosed that includes instructions that, when executed by at least one processor including hardware, configures the at least one processor to: receive a digital image comprising a sequence of characters; evaluate the digital image to determine a connectivity of one or more adjacent characters; for connected adjacent characters, perform image processing to split and segment the connected characters; and perform character recognition to determine the sequence of characters including the segmented characters of the original digital image.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013">The details of the present disclosure, both as to its structure and operation, can be understood by referring to the accompanying drawings, in which like reference numbers and designations refer to like elements.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example methodology for recognizing characters of license plates from vehicles;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows example instances of character adhesion/disturbances in original vehicle license plate images;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a character adhesion separation framework and method <b>100</b> implemented on a computer system or server, according to an embodiment;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> shows an example evaluation of character connectivity using a rule-based method in an embodiment;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> shows an example overlaying of a horizontal line to determine a maximum crossing parameter in an evaluation of adhered characters of an original character sequence in an embodiment;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a table depicting the relationship between the determined parameters resulting from the analysis of the evaluated characters of the license plate image character sequence in an embodiment;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example of a character adhesion classifier performing a data augmentation method that can be used to create a class-like character adhesion entity in an embodiment;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example original license plate image including adjacent low-resolution characters subject to an incremental split method for discriminating the characters using an image binarization method in an embodiment;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an example application of the hard split segmentation method using x-axis projection implemented in character evaluation in an embodiment;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts a further hard split segmentation method example including use of an &#x2018;x-project&#x2019; showing the location of the minimum character connectivity as a gap in an embodiment;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts a general end-to-end method <b>700</b> for vehicle license plate detection and single character recognition;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a license plate recognition system incorporating the system and methods of character adhesion separation according to the embodiments described herein; and</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an exemplary block diagram of a computer system in which processes involved in the system, method, and computer program product described herein may be implemented.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0028" num="0027">The aspects disclosed herein provide a system, method, and computer program product implementing a method and framework to improve the recognition of characters adhesion by a character adhesion classifier.</p><p id="p-0029" num="0028">The system, method, and computer program product first implements a rule-based method is used to identify the characters adhesion. Those uncertain characters will be judged subsequently by a character adhesion classifier.</p><p id="p-0030" num="0029">The method incudes performing an incremental split method and a hard split method which may be combined to separate adhesion characters.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example methodology <b>50</b> for recognizing characters of license plates from vehicles. A first step <b>52</b> comprises the actual acquiring of an RGB image of a license plate from a subject vehicle. This may be performed by any video camera, digital camera (whether mounted or fixed in position such as a surveillance camera or a handheld mobile device, e.g., Android Smartphone) or any like device adapted to take digital images and/or collect data of license plates from roadside parking lots and streets. It is noted that images from handheld devices particularly exhibit strong variations due to the uncertain position and shooting angle of handheld devices, as well as varying illuminations, weather conditions, and different backgrounds at different hours of the day. Then, at <b>54</b>, a computer-implemented algorithm is performed on the RGB image to detect the actual license plate of that vehicle. In an embodiment, at <b>56</b>, <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the image of the license plate can be initially de-skewed, e.g., to correct for horizontal tilt or vertical tilt, and at <b>58</b>, an algorithm is performed to segment the actual numbers/characters of the license plate image. Then, at <b>60</b>, <figref idref="DRAWINGS">FIG. <b>1</b></figref>, character recognition software is invoked to determine/recognize the actual characters <b>62</b> once they are split according to the embodiments herein.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows instances <b>70</b> of character adhesion which can be due to: 1) obtaining a low resolution original image <b>72</b> resulting in an evaluation of a corresponding character adhesions <b>73</b>A, <b>73</b>B in the resulting character sequence of the license plate image; 2) an image distortion of an original image <b>74</b> resulting in an evaluation of a corresponding character adhesions <b>75</b> in the resulting character sequence of the license plate image <b>80</b>; and 3) edges <b>76</b> in the image leading to a corresponding character adhesion <b>77</b> in the resulting character sequence of the license plate image.</p><p id="p-0033" num="0032">In addition to character adhesions, other practical challenges in license plate recognition is the presence of &#x201c;noise&#x201d; disturbances such as the presence of besmirch, license plate edge disturbances, blur, etc. that potentially result in character adhesions/distortions. For example, as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an image distortion in the form of edge disturbances in respective original images <b>82</b> and <b>85</b> results in an evaluating of corresponding unrecognizable characters <b>83</b>, <b>86</b>. The blurry original license plate image <b>90</b> results in distorted character sequence including a distorted character <b>93</b>. The original license plate image <b>95</b> includes besmirched characters that results in distorted character sequence including distorted character <b>96</b>.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a character adhesion separation framework and method <b>100</b> implemented on a computer system or server, according to an embodiment of the present invention.</p><p id="p-0035" num="0034">At <b>102</b>, <figref idref="DRAWINGS">FIG. <b>3</b></figref>, there is depicted the step of receiving an image of a license plate. Such an image can be obtained by a sensor device, e.g., a camera, video cam, or an Internet of Things (IoT) sensor proximately located to a vehicle and/or vehicle's license plate and which image is received as a digital file over a communications network. Based on the received image of the license plate, a rule-based method is invoked to evaluate the connectivity of the license plate characters (evaluate char connectivity). Based on the rule-based evaluation performed at <b>105</b>, a determination is made as to whether the image includes adhesion, i.e., two or more characters are connected to each other, e.g., adjacent characters that overlap or have connected edges such as the image of characters &#x201c;34&#x201d; of license plate <b>74</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. If there is no character adhesion determined at <b>110</b>, meaning the characters are clearly separated and the certainty of the characters is ascertainable, then the process proceeds to <b>150</b> to determine the resulting characters from the initial image as each character of the license plate character sequence is clearly separate and delineated (not adhered) and in suitable for determination. Returning to <b>105</b>, <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the rule-based evaluation may still reveal at <b>112</b> that the characters themselves are not clear or it is uncertain whether character adhesion is present or not in the character sequence, i.e., the uncertain part does not satisfy the rule-based method. In this scenario, a trained character adhesion classifier is invoked at <b>125</b> which character adhesion classifier is trained to distinguish or discriminate a single character from character adhesion. That is, the character adhesion classifier detects adhesion characters and enables the isolation and/or identifying of the adhered characters in the sequence, which results in a sequence having no character adhesion at <b>135</b>. If the character adhesion classifier is able to split the characters, i.e., eliminate the character adhesion or isolate the characters, then the process proceeds to step <b>150</b> in order to obtain the license plate character sequence result.</p><p id="p-0036" num="0035">In an embodiment, as shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the evaluating of char connectivity using a rule-based method performed at <b>105</b>, <figref idref="DRAWINGS">FIG. <b>3</b></figref> involves first determining of or isolating the identity of individual characters of the license plate image character sequence. For example, as shown in the license plate image character sequence <b>80</b> example shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the processing performed by image processing software results in determining individual characters, e.g., characters &#x2018;A&#x2019;, &#x2018;B&#x2019;, &#x2018;1&#x2019; &#x2018;2&#x2019; and &#x2018;34&#x2019;. Further, each isolated individual character or adhesion characters is processed to determine a variety of parameters used in the rule-based evaluation including, in an embodiment, a ratio, a mean ratio (mra) value and a maximum crossing (max crossing) value which lead to determining a connected-chars indicator(cci) for the evaluated character. For example, to determine a connected-chars indicator(cci) of a current evaluated character, there is determined for the evaluated character a max crossing, a ratio and a mean ratio (mra).</p><p id="p-0037" num="0036">In an embodiment, shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the current character or adhesion characters being evaluated can be aligned with respect to a reference and overlayed with a bounding box having a corresponding height and width dimension. From this bounding box, a ratio is determined, i.e., ratio(ra)=width/height of the evaluated character's corresponding bounding box. For example, in the sequence <b>80</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the box <b>201</b> bounding character &#x2018;A&#x2019; has a first determined ratio (ratio 1), the box <b>202</b> bounding character &#x2018;B&#x2019; has a second determined ratio (ratio 2), the box <b>203</b> bounding character &#x2018;1&#x2019; has a third determined ratio (ratio 3), and the box <b>204</b> bounding character &#x2018;2&#x2019; has a fourth determined ratio (ratio 4). A parameter mean ratio (mra) represents the mean of the ratios of the characters of ratio1, ratio2, ratio4, i.e., ratios of boxes for characters excepting for the box <b>203</b> associated with char &#x201c;1&#x201d;. The determining of the character connectivity associated with current evaluated character/adhesion character can then be processed by determining a maximum (horizontal) crossing parameter representing a number of overlapping lines of a single horizontal line overlayed across the current evaluated character of the image.</p><p id="p-0038" num="0037">For example, as shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, for the representation of the adhered characters <b>75</b>&#x2032;, i.e., adjacent characters &#x2018;34&#x2019; of the original character sequence, a horizontal line <b>210</b> is overlayed, using image processing software, across the adhered character image portion <b>75</b>&#x2032; being evaluated in the license plate image character sequence. From this single line overlay <b>210</b>, the processing software determines that there exists line portions <b>211</b> and <b>213</b> intersecting or crossing over portions of the adhered characters <b>75</b>&#x2032;. In this example, based on these intersections, it is determined that a maximum crossing of two (2) lines <b>211</b>, <b>213</b> is present.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shown a table <b>300</b> depicting the relationship between the determined parameters, e.g., max crossing <b>307</b>, ratio <b>309</b>, mra, etc. resulting from the analysis of the evaluated characters of the license plate image character sequence. For example, for an evaluated character of the image, in a first column, there is determined an associated connected-chars indicator (cci) <b>304</b> indicating whether the evaluated character is already segmented/split, is adhered to another character, or whether its adhesion status is indeterminable. For example, the cci indicator for an evaluated character can have a value of: &#x2212;1 for False (i.e., no character adhesion), 0 for uncertain, 1 for True (i.e., character adhesion present). The determined cci values are determined based on rules that depend upon the respective other parameter values determined for that current evaluated character. For example, using table <b>300</b>, a cci of &#x2212;1 (false) is the character connectivity evaluation based on a rule corresponding to either a determination for that character a max crossing parameter of 1, or a max crossing parameter value of 2 and if the determined ratio value is less than (&#x3c;) a value of a quantity of mra*C where mra is the mean ratio: mean of ratio1,ratio2, ratio4, except for char &#x201c;1&#x201d; and where C is an adjustable parameter, typically having a value equal to 1.</p><p id="p-0040" num="0039">Alternatively, using table <b>300</b>, a cci of 0 is a character connectivity evaluation rule corresponding to either a determination for that character of a max crossing parameter of 2 and if the determined ratio value is greater than or equal to (&#x3e;=) a value of a quantity of mra*C, or that character is determined as having a max crossing parameter of 3.</p><p id="p-0041" num="0040">Alternatively, using table <b>300</b>, a cci of 1 (true) is the character connectivity evaluation rule corresponding to a determination for that character of a max crossing parameter of 4 or more.</p><p id="p-0042" num="0041">In a further embodiment, shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the character adhesion classifier performs a data augmentation method that includes adding a random distance between two characters that can be used to create class like character adhesion entity. For example, for two current characters, a data augmentation is applied to make new data, e.g., make a new data class, e.g., &#x2018;zout&#x2019; is a &#x2018;class like character adhesion&#x2019;. As depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a random distance can be applied between each of two (or more) characters to simulate potential adhesion characters for training, e.g., make the data more real for training purposes. For example, given adjacent characters <b>401</b>, <b>402</b> as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> which are already discriminated (i.e., isolated or segmented), a random distance <b>410</b> can be added to result in a character adhesion class <b>420</b> or a character adhesion class <b>421</b>, the classes <b>421</b>, <b>422</b> having a respective different distance between the characters. Further types of augmentations to generate more real data for training purposes, i.e., creating classes to increase the robustness of the classifier for character recognition, can include: adding/cutting upper and lower edges (e.g., only for partial data, error case: add upper edge for &#x201c;1&#x201d; will result in a &#x201c;T&#x201d;); erasing some pixels to imitate the blur situation; or adding some random noise to a character. Each of these techniques may be used to form ground-truth labels for classifier training.</p><p id="p-0043" num="0042">In an embodiment, in an example case of obtaining a low resolution image of a license plate, two, three, or even multiple characters may be connected (adhered). As it is difficult for data augmentation to cover all cases, the rule-based method is used to first distinguish the characters that are easy to judge at step <b>105</b>, <figref idref="DRAWINGS">FIG. <b>3</b></figref>, leaving the uncertain part to the classifier for discrimination at <b>125</b>, <figref idref="DRAWINGS">FIG. <b>3</b></figref>, which reduces the cost of training data augmentation.</p><p id="p-0044" num="0043">Returning to step <b>105</b>, <figref idref="DRAWINGS">FIG. <b>3</b></figref>, if the license plate characters evaluation reveals that character adhesion exists at <b>108</b> (i.e., cci value is +1), or alternatively, if at <b>112</b> the character adhesion evaluation determines an uncertainty (i.e., cci value is 0) and may not isolate or detect the individual characters, then the process proceeds to <b>120</b> where a method is run to incrementally split the characters of the license plate characters sequence.</p><p id="p-0045" num="0044">At <b>120</b>, an incremental split process is run to separate characters that are adhered to each other (separate adhesion).</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example original license plate image <b>500</b> including adjacent low-resolution characters <b>525</b>, e.g., adjacent letters &#x201c;JD&#x201d;, to be subject to an incremental split method for discriminating the characters using a computer vision image binarization method. In one embodiment, the classic &#x201c;Otsu&#x201d; image thresholding method is performed upon image <b>500</b>, that, as known, runs an algorithm for separating pixels into two classes, e.g., foreground and background. In an embodiment, the Otsu method run is an adaptive thresholding method for segmentation in image processing by finding an optimal threshold value of the input image going through all possible pixel threshold values. In particular, at <b>120</b>, <figref idref="DRAWINGS">FIG. <b>3</b></figref>, Otsu methods are run to iterate through possible threshold values and calculate a measure of spread for the pixel levels each side of the threshold, i.e., the pixels that either fall in foreground or background. In an embodiment, assuming the image is an RGB license plate image, this image can be converted into a grayscale image first. Then, statistical data of an image is used, e.g., using a historgram representation of the image. Then a threshold value is used to separate the image into two classes, e.g., a first class representing image pixel intensity values less than a threshold (e.g., image background or foreground), and a second class representing image pixel intensity values greater than a threshold (e.g., image foreground or background). Then, a &#x201c;within-class&#x201d; variance value indicating a distribution of the data that is a function of the pixel value, the mean pixel value of the image and the number of pixels is obtained. To obtain an optimal threshold value is to find a minimum value of within-class variance. Further, a &#x201c;between-class&#x201d; variance value indicating a variance between the two classes is obtained. To obtain an optimal threshold value is to find a minimum value of within-class variance or a maximum value of the &#x201c;between-class&#x201d; variance. The Otsu adaptive thresholding method finds an optimum threshold value, e.g., from 0 to 255 by calculating and evaluating their between-class (or within-class) variance.</p><p id="p-0047" num="0046">As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, original image <b>500</b> having low resolution characters <b>525</b> is subject to Otsu binarization threshold processing at <b>520</b>. An initial or first threshold &#x201c;Thresh 1&#x201d; is applied in the Otsu method resulting in a low resolution image <b>530</b> which characters are still adhered and subject to further discrimination. Upon increasing the Otsu image processing threshold, i.e., an amount of pixels &#x3e;0, can result in a further image <b>535</b> which show subject characters <b>525</b> are still adhered. Applying a further threshold, e.g., &#x201c;Thresh 2&#x201d; eventually results in the image <b>540</b> which show characters that are separated and can be detected. Generally, thresh 2&#x3e;thresh 1. In <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a line graphic <b>550</b> depicts the application of the &#x2018;Otsu&#x2019; adaptive thresholding method showing the adaptive thresholding steps <b>552</b> including first step <b>555</b> (Thresh 1) and the resulting first low-resolution image <b>535</b> corresponding to the original adhered characters <b>525</b>. In the corresponding first low-resolution image <b>535</b> a number representative of the number of characters detected in the image is a &#x2018;blob&#x2019; number. In the example shown, the &#x2018;blob&#x2019; number is 1 as the characters is still adhered. In &#x201c;incremental split&#x201d; method performed, a separation is used to find the suitable threshold. The thresholding is adaptive, i.e., the steps <b>555</b> adjust the segmentation threshold dynamically in the binarization method performed. Application of a subsequent segmentation threshold at a further step <b>555</b> results in a corresponding further low-resolution image <b>540</b>. The &#x2018;blob&#x2019; number characterization <b>541</b> representative of the number of characters detected in the image remains unchanged, i.e., the characters are still adhered. As long as the &#x2018;blob&#x2019; number does not change as the threshold changes, the number of characters does not change. Finally, as the adaptive thresholding step <b>555</b> in the Otsu method is increased, eventually a second segmentation threshold <b>558</b> is reached that results in a corresponding low-resolution image <b>545</b>. However, at this threshold, the &#x2018;blob&#x2019; number characterization <b>546</b> representative of the number of characters detected in the image is changed, e.g., the &#x2018;blob&#x2019; number is 2 as the characters <b>525</b> in the original low-resolution image are now split at <b>545</b> and thus can be individually detected.</p><p id="p-0048" num="0047">Referring back to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, after performing an incremental split method at <b>120</b> such as the Otsu binarization segmentation method, if the blob number has changed before reaching an upper limit threshold, then this indicates a successful split <b>128</b> and the split characters can be determined at <b>130</b>. However, if after performing an incremental split method at <b>120</b> such as the Otsu binarization segmentation method, the &#x2018;blob&#x2019; number remains unchanged after reaching an upper limit threshold, then the characters are determined at <b>138</b> as not being split, and a hard split segmentation method <b>140</b> is performed.</p><p id="p-0049" num="0048">If incremental split fails, the hard split method <b>140</b> is executed to find a gap or trough between the characters based on an x-projection representation of the original low-resolution image. In an embodiment, the &#x2018;x-project&#x2019; is the number of white pixels in vertical direction. The gap is the location of a trough in the &#x2018;x-project&#x2019; graph.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>8</b></figref> show an example application of the hard split segmentation method <b>650</b>. In an embodiment, this hard split segmentation method <b>650</b> occurs after the incremental split method is performed that results in the &#x2018;blob&#x2019; number remaining unchanged after reaching an upper limit threshold. For example, as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, given an original low-resolution image <b>600</b> of adhered characters &#x201c;34&#x201d;, and after applying adaptive thresholding (in the incremental split method) such as performed by the Otsu method, there is shown successive image results <b>610</b>, <b>620</b>, <b>630</b> after application of successive thresholds indicating that there is no threshold that can split &#x2018;3&#x2019; and &#x2018;4&#x2019; from the original image <b>600</b>.</p><p id="p-0051" num="0050">Thus, given the adhered characters &#x201c;34&#x201d; resulting from the image <b>610</b> after application of the first incremental threshold, an x-project is performed that is a visualization graph <b>650</b> representing a density or concentration along the x-axis of the pixels corresponding to the adhered characters, e.g., the concentration of &#x201c;white&#x201d; pixels when taken against the contrasting background. In an embodiment, a vertical histogram projection of the pixels of the connected characters on an x-axis (&#x2018;x-project&#x2019;) is performed resulting in an &#x2018;x-project&#x2019; visualization graph <b>650</b> showing the adhered character &#x2018;3&#x2019; represented as a histogram <b>640</b>, i.e., a concentration of the corresponding pixels in a vertical direction of that character taken along the x-axis, and the adhered character &#x2018;4&#x2019; represented as a histogram <b>645</b>, i.e., a concentration of the corresponding pixels in a vertical direction of that character taken along the x-axis. The computer-implemented method then locates a local minimum or trough <b>660</b> in the visualization graph <b>650</b> which demarcates the point for character segmentation or the logical location to perform the character split. As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, an example &#x2018;x-project&#x2019; <b>680</b> of the low-resolution characters &#x201c;JD&#x201d; <b>670</b> shows the location of the minimum character connectivity as a gap <b>690</b> corresponding to the location of the trough for use as the demarcation point for splitting the characters.</p><p id="p-0052" num="0051">Returning back to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, once the gap or trough location is found in the &#x2018;x-project&#x2019; visualization of the characters, i.e., where the character connectivity is a minimum, then the characters can be split at <b>130</b> for character recognition. In an embodiment, the incremental split <b>120</b> and hard split <b>140</b> methods are combined together to take advantage of each other that can effectively split the characters adhesion. If upon determining a split or not at <b>130</b>, the method returns at <b>133</b> to the character connectivity evaluation step <b>105</b> in order to repeat the process in the event several other characters are deemed connected, e.g., when there are more than two adjacent characters, or to repeat the process in the event character splitting is unsuccessful in the first round of processing.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts a general end-to-end method <b>700</b> for vehicle license plate detection and single character recognition. Step <b>702</b> depicts the ability of a computer imaging method to recognize, from a digital image of a license plate, e.g., low-resolution license plate characters that may be distorted, besmirched, noisy, or generally have adhered or connected characters rendering them unrecognizable. At <b>704</b>, a decision is made as to whether the license plate characters are adhered or not segmented, i.e., the connected-chars indicator or cci is &#x2212;1 for False, cci=0 for uncertain, or cci=1 for True. In one embodiment, the evaluation of the license plate characters include the computing of the character &#x2018;blob&#x2019;'s maximum crossing (max_c) such as shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>. If cci=0, then the method proceeds to step <b>707</b> to run the character adhesion classifier in order to determine whether the characters can be recognized.</p><p id="p-0054" num="0053">In an embodiment, the character adhesion classifier is run at <b>707</b> for character discrimination when the uncertain portions cannot satisfy the rule-based methods. In an embodiment, a convolution neural network (CNN) classification model can be used to train this model, e.g., using Inception, VGG, ResNet CNN neural network model architectures, for example. This model can be trained based on a TensorFlow architecture. The training data of the model is a variety of characters including 0&#x2dc;9, A&#x2dc;Z and augmented characters that are newly created classes of adhered characters obtained by adding random distances, noise, edges, etc. However, before input to the network, the data has been binarized and converted into a binary graph. Ground-truth labels of the dataset is obtained when data is collected over time as the adhesion classifier is used. The output of the model is the recognized result of a character. Thus, the character adhesion classifier distinguishes single characters from character adhesion. If the character is a single character then it can be directly sent to next stage and if the character is adhesion, then it should be separated.</p><p id="p-0055" num="0054">Thus, returning to step <b>702</b>, <figref idref="DRAWINGS">FIG. <b>10</b></figref>, if the character connectivity evaluation determination results in the cci=1, then the process proceeds to <b>710</b> where the incremental split algorithm can be performed at <b>711</b>. Likewise, if the character adhesion classifier result is indeterminable, i.e., at <b>709</b>, <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the method determines a connectivity evaluation of cci=1, then the process proceeds to the incremental split algorithm to be performed at <b>711</b>. The adaptive incremental split algorithm of <figref idref="DRAWINGS">FIG. <b>7</b></figref> is then performed at <b>711</b> and a determination is made at <b>715</b> whether the &#x2018;blob&#x2019; number has changed, indicating that the characters have been suitably segmented/discretized or not. If, at <b>715</b>, it is determined that the adaptive incremental split algorithm results in a &#x2018;blob&#x2019; number change, then the process returns to <b>707</b> where the character adhesion classifier is again run to distinguish single characters from character adhesion.</p><p id="p-0056" num="0055">Retuning to step <b>715</b>, <figref idref="DRAWINGS">FIG. <b>10</b></figref>, if it is determined that the performing of the incremental split algorithm does not result in the &#x2018;blob&#x2019; number changing, then the process proceeds to <b>718</b> where the hard split algorithm is performed as described herein with respect to <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref>. Upon performing the hard split method, the character adhesion classifier is run at <b>720</b> for character discrimination to distinguish single character from character adhesion. A determination is made at <b>722</b>, <figref idref="DRAWINGS">FIG. <b>10</b></figref> as to whether a single character can be discriminated or whether further processing is necessary to isolate the characters, i.e., cci=1. If it is determined at <b>722</b> that cci=1, then the process returns to <b>720</b> to further run the hard split method to discern additional individual characters. Otherwise, the character adhesion classifier run at <b>720</b> has distinguished each single character from the adhered characters and the process proceeds to <b>725</b> indicating that all characters have been split and can be directly sent to a next stage for character recognition.</p><p id="p-0057" num="0056">Both the methods depicted in <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>10</b></figref> significantly improve the accuracy and robustness of the character recognition model in the license plate recognition. That is, the methods of <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>10</b></figref> are incorporated and used in conjunction within license plate recognition system <b>900</b> such as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. In particular, <figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts a portion of a unified deep neural network <b>900</b>, which can localize license plates in original image <b>902</b> and recognize the letters simultaneously in a single forward pass that jointly solves two separate tasks of license plate detection and recognition. In system <b>900</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, several convolutional layers <b>905</b>, using a VGG network model, are used to extract low level CNN features. For example, the VGG network can consist of 13 layers of 3&#xd7;3 convolutions followed by Rectified Linear Unit (ReLU) non-linearity, 5 layers of 2&#xd7;2 max-pooling, and fully connected layers.</p><p id="p-0058" num="0057">These extracted license plate features are processed in a region proposal network <b>910</b> tailored specifically for car license plates and is particularly implemented to extract the license plate candidate areas (e.g., bounding boxes) <b>915</b>. From the license plate candidate areas, the character segmentation framework and methods of <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>10</b></figref> are implemented at <b>925</b>. Finally, character recognition techniques, e.g., optical character recognition (OCR) techniques, can be employed at <b>930</b> for character recognition. Thus, rather than treating plate recognition as a sequence labeling problem such as described in a reference to Li H, Wang P, Shen C. entitled &#x201c;Toward end-to-end car license plate detection and recognition with deep neural networks&#x201d;, IEEE Transactions on Intelligent Transportation Systems, 2018, 20(3): 1126-1136 that addresses both license plate detection and recognition using a single deep network, the system <b>900</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref> incorporating the character adhesion separation framework and methods of <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>10</b></figref> improve the accuracy and robustness of the character recognition model in the license plate recognition significantly and can replace and/or enhance a state-of-the-art license plate recognition model that employs a sliding window manner to extract a sequence of feature vectors from license plate bounding box and that employs Recurrent Neural Networks (RNNs) with Connectionist Temporal Classification (CTC) adopted to label the sequential data and plate decoding without character separation. That is, at step <b>925</b>, <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the rule-based method is first used to identify the characters adhesion and those uncertain characters will be judged by a trained character adhesion classifier. The method provides an enhancement to the state-of-art LP recognition approaches that do not rely on heavy license plate data for model training like end-to-end neural-network models.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example computing system in accordance with the present invention. It is to be understood that the computer system depicted is only one example of a suitable processing system and is not intended to suggest any limitation as to the scope of use or functionality of embodiments of the present invention. For example, the system shown may be operational with numerous other general-purpose or special-purpose computing system environments or configurations. Examples of well-known computing systems, environments, and/or configurations that may be suitable for use with the system shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may include, but are not limited to, personal computer systems, server computer systems, thin clients, thick clients, handheld or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputer systems, mainframe computer systems, and distributed cloud computing environments that include any of the above systems or devices, and the like.</p><p id="p-0060" num="0059">In some embodiments, the computer system may be described in the general context of computer system executable instructions, embodied as program modules stored in memory <b>16</b>, being executed by the computer system. Generally, program modules may include routines, programs, objects, components, logic, data structures, and so on that perform particular tasks and/or implement particular input data and/or data types in accordance with the present invention (see e.g., <figref idref="DRAWINGS">FIGS. <b>3</b>, <b>10</b>, <b>11</b></figref>).</p><p id="p-0061" num="0060">The components of the computer system may include, but are not limited to, one or more processors or processing units <b>12</b>, a memory <b>16</b>, and a bus <b>14</b> that operably couples various system components, including memory <b>16</b> to processor <b>12</b>. In some embodiments, the processor <b>12</b> may execute one or more modules <b>10</b> that are loaded from memory <b>16</b>, where the program module(s) embody software (program instructions) that cause the processor to perform one or more method embodiments of the present invention. In some embodiments, module <b>10</b> may be programmed into the integrated circuits of the processor <b>12</b>, loaded from memory <b>16</b>, storage device <b>18</b>, network <b>24</b> and/or combinations thereof.</p><p id="p-0062" num="0061">Bus <b>14</b> may represent one or more of any of several types of bus structures, including a memory bus or memory controller, a peripheral bus, an accelerated graphics port, and a processor or local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnects (PCI) bus.</p><p id="p-0063" num="0062">The computer system may include a variety of computer system readable media. Such media may be any available media that is accessible by computer system, and it may include both volatile and non-volatile media, removable and non-removable media.</p><p id="p-0064" num="0063">Memory <b>16</b> (sometimes referred to as system memory) can include computer readable media in the form of volatile memory, such as random access memory (RAM), cache memory an/or other forms. Computer system may further include other removable/non-removable, volatile/non-volatile computer system storage media. By way of example only, storage system <b>18</b> can be provided for reading from and writing to a non-removable, non-volatile magnetic media (e.g., a &#x201c;hard drive&#x201d;). Although not shown, a magnetic disk drive for reading from and writing to a removable, non-volatile magnetic disk (e.g., a &#x201c;floppy disk&#x201d;), and an optical disk drive for reading from or writing to a removable, non-volatile optical disk such as a CD-ROM, DVD-ROM or other optical media can be provided. In such instances, each can be connected to bus <b>14</b> by one or more data media interfaces.</p><p id="p-0065" num="0064">The computer system may also communicate with one or more external devices <b>26</b> such as a keyboard, a pointing device, a display <b>28</b>, etc.; one or more devices that enable a user to interact with the computer system; and/or any devices (e.g., network card, modem, etc.) that enable the computer system to communicate with one or more other computing devices. Such communication can occur via Input/Output (I/O) interfaces <b>20</b>.</p><p id="p-0066" num="0065">Still yet, the computer system can communicate with one or more networks <b>24</b> such as a local area network (LAN), a general wide area network (WAN), and/or a public network (e.g., the Internet) via network adapter <b>22</b>. As depicted, network adapter <b>22</b> communicates with the other components of computer system via bus <b>14</b>. It should be understood that although not shown, other hardware and/or software components could be used in conjunction with the computer system. Examples include, but are not limited to: microcode, device drivers, redundant processing units, external disk drive arrays, RAID systems, tape drives, and data archival storage systems, etc.</p><p id="p-0067" num="0066">The present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration. The computer program product may include a computer readable storage medium (or media) having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.</p><p id="p-0068" num="0067">The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be, for example, but is not limited to, an electronic storage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing. A non-exhaustive list of more specific examples of the computer readable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a portable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable combination of the foregoing. A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through a waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.</p><p id="p-0069" num="0068">Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the Internet, a local area network, a wide area network and/or a wireless network. The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. A network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing/processing device.</p><p id="p-0070" num="0069">Computer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware instructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++, or the like, and procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider). In some embodiments, electronic circuitry including, for example, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry, in order to perform aspects of the present invention.</p><p id="p-0071" num="0070">Aspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.</p><p id="p-0072" num="0071">These computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0073" num="0072">The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or other device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0074" num="0073">The flowcharts and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s). In some alternative implementations, the functions noted in the blocks may occur out of the order noted in the Figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.</p><p id="p-0075" num="0074">The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein, the singular forms &#x201c;a&#x201d;, &#x201c;an&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms &#x201c;comprises&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof. The corresponding structures, materials, acts, and equivalents of all elements in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.</p><p id="p-0076" num="0075">The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s). In some alternative implementations, the functions noted in the blocks may occur out of the order noted in the Figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.</p><p id="p-0077" num="0076">Although specific embodiments of the present invention have been described, it will be understood by those of skill in the art that there are other embodiments that are equivalent to the described embodiments. Accordingly, it is to be understood that the invention is not to be limited by the specific illustrated embodiments, but only by the scope of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method implemented by at least one hardware processor comprising:<claim-text>receiving, at the at least one hardware processor, a digital image comprising a sequence of characters;</claim-text><claim-text>evaluating, implementing the at least one hardware processor, the digital image to determine a connectivity of one or more adjacent characters, said evaluating the digital image comprising:<claim-text>processing, using the at least one hardware processor, the characters of the digital image to obtain character parameters; and</claim-text><claim-text>generating, using the at least one hardware processor, a connection indicator value of a character as a function of said character parameters, said generated character connection indicator indicating an uncertainty as to a character being connected to another character in the sequence; and responsive to the generated character connection indicator indicating an uncertainty as to a character being connected to another adjacent character in the sequence, said method further comprising:</claim-text><claim-text>running, using the at least one hardware processor, a character adhesion classifier model trained to recognize, from said image, a presence or not of connected adjacent character classes of adjacent adhered characters of the sequence; and</claim-text></claim-text><claim-text>for connected adjacent characters, performing image processing using the at least one hardware processor to incrementally split and segment the connected characters; and</claim-text><claim-text>performing, using the at least one hardware processor, character recognition to determine the sequence of characters including the segmented characters of the original digital image.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said generated character connection indicator alternately indicates the character as being one of: a segmented character in the sequence, or a character connected to another adjacent character in the sequence.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein said character parameters of a character comprises:<claim-text>a max horizontal crossing value of the character;</claim-text><claim-text>a ratio value of the character, the ratio being a measure of a width/height of the character; and</claim-text><claim-text>a means ratio being a function of the ratio of each of the characters of the sequence.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein responsive to the generated character connection indicator indicating the character as being a segmented character, said method further performing a character recognition of the character using a character recognition model.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. (canceled)</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein responsive to the generated character connection indicator indicating a connection to another adjacent character in the sequence, said method further comprising:<claim-text>running, using the at least one hardware processor, an image processor adapted to split the connected characters of the sequence.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein said running an image processor adapted to split the connected characters comprises:<claim-text>performing, using the at least one hardware processor, a first character splitting method using adaptive thresholding for image binarization, said first character splitting method dynamically adjusting a segmenting threshold for distinguishing among two classes of data used for character segmentation; or</claim-text><claim-text>performing a second character splitting method using a vertical histogram projection of said connected characters on an x-axis and locating a gap or trough on the projection as a location for character segmenting; or</claim-text><claim-text>performing both the first character splitting method and second character splitting method for segmenting the connected characters.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said training said character adhesion classifier model uses single alphabet and numeric characters labels, said method further comprising:<claim-text>adding random distance between two single characters to create new connected adhesion character data, wherein said character adhesion classifier model is further trained using said single characters and said new connected adhesion character data.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A system comprising at least one processor comprising hardware, the at least one processor configured to:<claim-text>receive a digital image comprising a sequence of characters;</claim-text><claim-text>evaluate the digital image to determine a connectivity of one or more adjacent characters wherein to evaluate the digital image, said at least one processor is further configured to:<claim-text>process the characters of the digital image to obtain character parameters; and</claim-text><claim-text>generate a connection indicator value of a character as a function of said character parameters, said generated character connection indicator indicating an uncertainty as to a character being connected to another character in the sequence; and responsive to the generated character connection indicator indicating an uncertainty as to a character being connected to another adjacent character in the sequence, said at least one processor is further configured to:</claim-text><claim-text>run a character adhesion classifier model trained to recognize, from said image, a presence or not of connected adjacent character classes of adjacent adhered characters of the sequence; and</claim-text></claim-text><claim-text>for connected adjacent characters, perform image processing to incrementally split and segment the connected characters; and</claim-text><claim-text>perform character recognition to determine the sequence of characters including the segmented characters of the original digital image.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>said generated character connection indicator alternatively indicating the character as being one of: a segmented character in the sequence, or a character connected to another character in the sequence.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein responsive to the generated character connection indicator indicating the character as being a segmented character, the at least one processor is further configured to:<claim-text>perform a character recognition of the character using a character recognition model.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>responsive to the generated character connection indicator indicating a connection to another adjacent character in the sequence, the at least one processor is further configured to:</claim-text><claim-text>run an image processor adapted to split the connected characters of the sequence.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein to run an image processor adapted to split the connected characters, the at least one processor is further configured to:<claim-text>perform a first character splitting method using adaptive thresholding for image binarization, said first character splitting method dynamically adjusting a segmenting threshold for distinguishing among two classes of data used for character segmentation; or</claim-text><claim-text>perform a second character splitting method using a vertical histogram projection of said connected characters on an x-axis and locating a gap or trough on the projection as a location for character segmenting; or</claim-text><claim-text>perform both the first character splitting method and second character splitting method for segmenting the connected characters.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein said training said character adhesion classifier model uses single alphabet and numeric characters labels, said at least one processor is further configured to:<claim-text>add random distance between two single characters to create new connected adhesion character data, wherein said character adhesion classifier model is further trained using said single characters and said new connected adhesion character data.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer readable storage medium comprising instructions that, when executed by at least one processor comprising hardware, configures the at least one hardware processor to:<claim-text>receive a digital image comprising a sequence of characters;</claim-text><claim-text>evaluate the digital image to determine a connectivity of one or more adjacent characters wherein to evaluate the digital image, said instructions further configuring said at least one processor to:<claim-text>process the characters of the digital image to obtain character parameters; and</claim-text><claim-text>generate a connection indicator value of a character as a function of said character parameters, said generated character connection indicator indicating an uncertainty as to a character being connected to another character in the sequence; and responsive to the generated character connection indicator indicating an uncertainty as to a character being connected to another adjacent character in the sequence, said instructions further configuring said at least one processor to:</claim-text><claim-text>run a character adhesion classifier model trained to recognize, from said image, a presence or not of connected adjacent character classes of adjacent adhered characters of the sequence; and</claim-text><claim-text>for connected adjacent characters, perform image processing to incrementally split and segment the connected characters; and</claim-text><claim-text>perform character recognition to determine the sequence of characters including the segmented characters of the original digital image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein<claim-text>said generated character connection indicator alternatively indicating the character as being one of: a segmented character in the sequence, or a character connected to another character in the sequence.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein responsive to the generated character connection indicator indicating the character as being a segmented character, the at least one processor is further configured to:<claim-text>perform a character recognition of the character using a character recognition model.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein<claim-text>responsive to the generated character connection indicator indicating a connection to another adjacent character in the sequence, the at least one processor is further configured to:</claim-text><claim-text>run an image processor adapted to split the connected characters of the sequence.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer readable storage medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein to run an image processor adapted to split the connected characters, the at least one processor is further configured to:<claim-text>perform a first character splitting method using adaptive thresholding for image binarization, said first character splitting method dynamically adjusting a segmenting threshold for distinguishing among two classes of data used for character segmentation; or</claim-text><claim-text>perform a second character splitting method using a vertical histogram projection of said connected characters on an x-axis and locating a gap or trough on the projection as a location for character segmenting; or</claim-text></claim-text><claim-text>perform both the first character splitting method and second character splitting method for segmenting the connected characters.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable storage medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said training said character adhesion classifier model uses single alphabet and numeric characters labels, said at least one processor is further configured to:<claim-text>add random distance between two single characters to create new connected adhesion character data, wherein said character adhesion classifier model is further trained using said single characters and said new connected adhesion character data.</claim-text></claim-text></claim></claims></us-patent-application>