<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005281A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005281</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17931271</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>69</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>N</subclass><main-group>21</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>141</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>64</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>695</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>N</subclass><main-group>21</main-group><subgroup>251</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>141</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>647</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ADAPTIVE SENSING BASED ON DEPTH</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16851830</doc-number><date>20200417</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11482021</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17931271</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/IL2018/051117</doc-number><date>20181018</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>16851830</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62574289</doc-number><date>20171019</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SCOPIO LABS LTD.</orgname><address><city>Tel Aviv</city><country>IL</country></address></addressbook><residence><country>IL</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LESHEM</last-name><first-name>Ben</first-name><address><city>Tel Aviv</city><country>IL</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SMALL</last-name><first-name>Eran</first-name><address><city>Yahood</city><country>IL</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>HAYUT</last-name><first-name>Itai</first-name><address><city>Tel Aviv</city><country>IL</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>NA'AMAN</last-name><first-name>Erez</first-name><address><city>Tel Aviv</city><country>IL</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>BEN-BASSAT</last-name><first-name>Eyal</first-name><address><city>Tel Aviv</city><country>IL</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>SCOPIO LABS LTD.</orgname><role>03</role><address><city>Tel Aviv</city><country>IL</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A microscope for adaptive sensing may comprise an illumination assembly, an image capture device configured to collect light from a sample illuminated by the assembly, and a processor. The processor may be configured to execute instructions which cause the microscope to capture, using the image capture device, an initial image set of the sample, identify, in response to the initial image set, an attribute of the sample, determine, in response to identifying the attribute, a three-dimensional (3D) process for sensing the sample, and generate, using the determined 3D process, an output image set comprising more than one focal plane. Various other methods, systems, and computer-readable media are also disclosed.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="190.25mm" wi="158.75mm" file="US20230005281A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="227.58mm" wi="162.98mm" file="US20230005281A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="229.79mm" wi="121.24mm" file="US20230005281A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="240.88mm" wi="176.78mm" file="US20230005281A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="240.28mm" wi="138.18mm" file="US20230005281A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="240.28mm" wi="159.34mm" file="US20230005281A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="230.97mm" wi="98.81mm" file="US20230005281A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="193.38mm" wi="115.23mm" file="US20230005281A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="222.67mm" wi="117.18mm" file="US20230005281A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="214.55mm" wi="147.66mm" file="US20230005281A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="223.69mm" wi="104.14mm" file="US20230005281A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="209.55mm" wi="171.28mm" file="US20230005281A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 16/851,830, filed Apr. 17, 2020, which is a continuation of International Application No. PCT/IL2018/051117, filed Oct. 18, 2018, published as WO 2019/077610, on Apr. 25, 2019, which claims the benefit under 35 U.S.C. &#xa7; 119(e) of U.S. Provisional Patent Application No. 62/574,289, filed Oct. 19, 2017, the disclosures of which are incorporated, in their entirety, by this reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The present disclosure relates generally to digital microscopy and/or computational microscopy and, more specifically, to systems and methods for adaptive sensing of a sample.</p><p id="p-0004" num="0003">Today's commercial microscopes may rely on expensive and delicate optical lenses and typically rely on additional hardware to share and process acquired images. Moreover, for scanning optical microscopy, additional expensive equipment such as accurate mechanics and scientific cameras can be utilized. A new generation of microscope technology, known as computational microscopy, has begun to emerge, and makes use of advanced image-processing algorithms (usually with hardware modifications) to overcome limitations of conventional microscopes. For example, microscopic objectives often rely on a high optical resolution, therefore a high numerical aperture which may result in a very shallow depth of focus. The depth of focus may typically be in the order of microns or less for microscopic applications. However, some samples may not be thin enough to fit inside the depth of focus of the lens. This may result in one focused slice of the sample whereas the other slices are out of focus slices, which may appear blurred and add noise to the image. Moreover, more than one plane of focus may be present in the same field of view if the sample is not flat enough for the lens.</p><p id="p-0005" num="0004">A conventional solution to this issue includes focus stacking (also referred to as Z-stacking). In focus stacking, the sample may be measured in a stack of different focal planes for which different slices of the sample are focused. For example, focus stacking may be based on defining a range for focus scanning and a region of interest (ROI) of the sample, and imaging the entire ROI at the defined range. The sample is scanned in several focal planes by changing the distance between the optics and the sample. However, focus stacking may disadvantageously be time consuming, as the measuring or acquisition time is multiplied by the number of focal planes inside the focus stack. Focus stacking may rely on increased data storage and with thicker samples, the resolution and quality of the image may deteriorate. For automatic microscopes, such as slide scanners, acquisition time may take longer than would be ideal. Also, for computational microscopy, the sampling time and computational time to generate images can be less than ideal.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">As will be described in greater detail below, the instant disclosure describes various systems and methods for adaptive sensing of a sample by detecting an attribute of the sample, and determining an acquisition process in response to the attribute. This adaptive approach to the image acquisition and computational process can decrease the acquisition time, storage requirements, costs and processing times. In addition, larger focus spans may be enabled where they are needed, thus improving the quality of the data and reducing a risk of human error in choosing the area and in analyzing the data.</p><p id="p-0007" num="0006">The presently disclosed systems and methods relate to the fields of computational microscopy and digital microscopy. Some disclosed embodiments are directed to systems and methods for focusing a microscope using images acquired under a plurality of illumination conditions. The disclosed embodiments may also comprise systems and methods for acquiring images under a plurality of illumination conditions to generate a high- resolution image of a sample. Although reference is made to computational microscopy, the methods and apparatus disclosed herein will find application in many fields, such as 3D sampling with conventional microscopes, slide scanners and confocal microscopy.</p><p id="p-0008" num="0007">In some embodiments, a microscope may comprise an illumination assembly, an image capture device configured to collect light from a sample illuminated by the illumination assembly, and a processor configured to execute instructions. The instructions may cause the microscope to capture, using the image capture device, an initial image set of the sample, identify, in response to the initial image set, an attribute of the sample, determine, in response to identifying the attribute, a three-dimensional (3D) process, and generate, using the determined 3D process, an output image set comprising more than one focal plane.</p><p id="p-0009" num="0008">In some embodiments, the 3D process may comprise a process for sensing the sample. In some embodiments, the 3D process may comprise a reconstruction process for reconstructing the sample in response to the initial image set. In some embodiments, the 3D process may not rely on additional images beyond the initial image set.</p><p id="p-0010" num="0009">In some embodiments, the 3D process may comprise capturing one or more subsequent images of the sample using one or more illumination conditions of the illumination assembly and one or more image capture settings for the image capture device. In some embodiments a number of illumination conditions for the 3D process is greater than a number of illumination conditions for capturing the initial image set. The 3D process may comprise determining a plurality of focal planes for capturing the one or more subsequent images based at least on the attribute, and the one or more illumination conditions and the one or more image capture settings correspond to the plurality of focal planes. The one or more subsequent images may be taken at one or more locations of the sample determined based at least on the attribute.</p><p id="p-0011" num="0010">In some embodiments, the 3D process may comprise performing a 3D reconstruction of the sample based at least on a subset of images captured by the image capture device. The 3D process may comprise performing a 2.5D reconstruction of the sample based at least on a subset of images captured by the image capture device in order to generate 3D data from the sample.</p><p id="p-0012" num="0011">In some embodiments, the 3D process comprises performing focus stacking for the sample based at least on a subset of images captured by the image capture device. The 3D process may comprise capturing a plurality of images at a respective plurality of focal planes.</p><p id="p-0013" num="0012">In some embodiments, identifying the attribute may comprise estimating the attribute corresponding to one or more locations of the sample. The 3D process may be performed within a threshold time from capturing the initial image set. The threshold time may be one of 5 microseconds, 10 microseconds, 1 second, 5 seconds, 10 seconds, 1 minute, or 5 minutes.</p><p id="p-0014" num="0013">In some embodiments, the attribute may comprise a thickness of the sample at one or more locations of the sample. The attribute may comprise a depth of the sample at one or more locations of the sample.</p><p id="p-0015" num="0014">In some embodiments, the 3D process may comprise capturing images at a plurality of distances between the image capture device and the sample, and a number of focal planes for the 3D process is greater than a number of distances in the plurality of distances.</p><p id="p-0016" num="0015">In some embodiments, capturing the initial image set may comprise capturing the initial image set of the sample using a plurality of illumination conditions for illuminating the sample, and the plurality of illumination conditions comprise at least one of an illumination angle, an illumination wavelength, or an illumination pattern. The 3D process may comprise capturing one or more subsequent images of the sample using a plurality of illumination conditions for illuminating the sample, and the plurality of illumination conditions may comprise at least one of an illumination angle, an illumination wavelength, or an illumination pattern. The 3D process may comprise a plurality of focus levels for adjusting the image capture device.</p><p id="p-0017" num="0016">In some embodiments, the attribute may comprise at least one of a thickness, a density, a depth, a color, a stain structure of the sample, a distance from a lens of the image capture device, a plurality of distances between the lens of the image capture device and the sample, a plurality of focal planes of the sample in relation to the image capture device, a sample structure, a convergence value, a pattern, or a frequency determined based at least on one of color analysis, analysis of optical aberrations, computational reconstruction, pattern recognition, Fourier transformation, or light field analysis.</p><p id="p-0018" num="0017">In some embodiments, the sample may be stained and the color analysis may comprise determining the attribute for an area of the sample based at least on comparing a color or stain structure of the area with a color or stain structure of another area of the sample. The color analysis may comprise determining the attribute for an area of the sample based at least on comparing a color or stain structure of the area with a color or stain structure from empirical data. The sample may be stained using any stain, for example, it may be at least one of a Romanowsky stain, a Gram stain, a hematoxylin and eosin (H&#x26;E) stain, an immunohistochemistry (IHC) stain, a methylene blue stain, or a DAPI stain.</p><p id="p-0019" num="0018">In some embodiments, the analysis of optical aberrations may comprise identifying an optical aberration from the initial image set, and determining, in response to identifying the optical aberration, the attribute. Determining, in response to identifying the optical aberration, the attribute may further comprise determining, in response to identifying the optical aberration, the sample's distance from the lens, and determining the attribute in response to determining the sample's distance from the lens.</p><p id="p-0020" num="0019">In some embodiments, determining the 3D process may comprise determining the sample structure based at least on the computational reconstruction, and determining an illumination condition and an image capture setting in response to determining the sample structure. Determining the illumination condition and the image capture setting may comprise identifying, in response to the computational reconstruction, an area of the sample having a convergence value that indicates low convergence.</p><p id="p-0021" num="0020">In some embodiments, the illumination assembly may comprise a laser and identifying the attribute may comprise identifying the pattern from illuminating the sample with the laser. Identifying the attribute comprises determining, using pattern recognition, the attribute of the sample.</p><p id="p-0022" num="0021">In some embodiments, identifying the attribute may comprise performing the Fourier transformation using the initial image set, determining frequencies represented in the sample based at least on the Fourier transformation, and identifying the attribute in response to determining the frequencies.</p><p id="p-0023" num="0022">In some embodiments, identifying the attribute may comprise performing the light field analysis in response to capturing the initial image set, and identifying the attribute by identifying characteristics of the light field analysis.</p><p id="p-0024" num="0023">In some embodiments, capturing the initial image set may comprise capturing a first plurality of images, the 3D process may comprise capturing a second plurality of images, and a number of images in the second plurality of images may be greater than a number of images in the first plurality of images. The first plurality of images and the second plurality of images may correspond to a same area of the sample. The second plurality of images may be captured at a same relative location of the image capture device with respect to the sample as the first plurality of images. The 3D process may comprise a computational process based at least on capturing the initial image set.</p><p id="p-0025" num="0024">In some embodiments, the illumination assembly may comprise an LED array. The illumination assembly may comprise one or more of a halogen lamp, an LED, an incandescent lamp, a laser or a sodium lamp. A resolution of the output image set may be higher than a resolution of the initial image set.</p><p id="p-0026" num="0025">In some embodiments, the image capture device may comprise a plurality of image capture devices and the illumination assembly may comprise a plurality of illumination assemblies. The plurality of image capture devices may comprise a first image capture device and a second image capture device and the plurality of illumination assemblies may comprise a first illumination assembly and a second illumination assembly. The first illumination assembly may comprise a light source for backlight illumination and the second illumination assembly may comprise a plurality of light sources. The first image capture device may comprise a preview camera and the second image capture device may comprise a microscope objective and detector.</p><p id="p-0027" num="0026">In one example, a method for adaptive sampling may comprise any combination of steps described herein.</p><p id="p-0028" num="0027">Features from any of the above-mentioned embodiments may be used in combination with one another in accordance with the general principles described herein. These and other embodiments, features, and advantages will be more fully understood upon reading the following detailed description in conjunction with the accompanying drawings and claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0029" num="0028">The accompanying drawings illustrate a number of exemplary embodiments and are a part of the specification. Together with the following description, these drawings demonstrate and explain various principles of the instant disclosure.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of an exemplary microscope, in accordance with some embodiments of the present disclosure.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a diagram of the optical paths of two beam pairs when the microscope of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Is out of focus, in accordance with some embodiments of the present disclosure.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a diagram of the optical paths of two beam pairs when the microscope of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is in focus, in accordance with some embodiments of the present disclosure.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a workflow diagram showing an exemplary process for adaptive sensing of a sample, in accordance with some embodiments of the present disclosure.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart showing an exemplary process for adaptive sensing of a sample, in accordance with some embodiments of the present disclosure.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a graph corresponding to an initial acquisition of a sample, in accordance with some embodiments of the present disclosure.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is an illustration of exemplary image data acquired at a first depth, in accordance with some embodiments of the present disclosure.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is an illustration of exemplary image data acquired at a second depth, in accordance with some embodiments of the present disclosure.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>6</b>C</figref> is an illustration of exemplary image data acquired at a third depth, in accordance with some embodiments of the present disclosure.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a diagram of a configuration for a plurality of illumination conditions, in accordance with some embodiments of the present disclosure.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a diagram of a configuration for a plurality of illumination conditions, in accordance with some embodiments of the present disclosure.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>7</b>C</figref> is a diagram of a configuration for a plurality of illumination conditions, in accordance with some embodiments of the present disclosure.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>7</b>D</figref> is a diagram of a configuration for a plurality of illumination conditions, in accordance with some embodiments of the present disclosure.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>7</b>E</figref> is a diagram of a configuration for a plurality of illumination conditions, in accordance with some embodiments of the present disclosure.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>7</b>F</figref> is a diagram of a configuration for a plurality of illumination conditions, in accordance with some embodiments of the present disclosure.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a top-down view of a sample in accordance with some embodiments of the present disclosure.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is a side view of the sample of <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> when the sample is a 2D sample.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>8</b>C</figref> is a side view of the sample of <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> when the sample is a 2.5D sample.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>8</b>D</figref> is a side view of the sample of <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> showing a single layer when the sample is a 3D sample.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>8</b>E</figref> is a side view of the sample of <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> showing multiple layers when the sample is a 3D sample.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0050" num="0049">Throughout the drawings, identical reference characters and descriptions indicate similar, but not necessarily identical, elements. While the exemplary embodiments described herein are susceptible to various modifications and alternative forms, specific embodiments have been shown by way of example in the drawings and will be described in detail herein. However, the exemplary embodiments described herein are not intended to be limited to the particular forms disclosed. Rather, the instant disclosure covers all modifications, equivalents, and alternatives falling within the scope of the appended claims.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0051" num="0050">The present disclosure is generally directed to systems and methods for adaptive sensing of a sample. As will be explained in greater detail below, embodiments of the instant disclosure may be configured to perform image captures using a larger focus depth or at several distances from the sample or locations or with added illumination conditions when it is determined to be necessary for imaging the sample, for example based on a detected attribute of the sample. The acquisition procedure and/or the computational process may be adapted. The resulting images may advantageously comprise most or all of the focus planes with important data at every point. Acquisition time may be significantly reduced by avoiding unnecessary image captures using focal planes which may not contribute additional data or by avoiding capturing images with illumination conditions which aren't necessary for the computational process. The user experience may be improved, for example, because the system may provide high-quality images without requiring the user to determine in advance how to adjust the acquisition procedure.</p><p id="p-0052" num="0051">Tomography refers generally to methods where a three-dimensional (3D) sample is sliced computationally into several 2D slices. Confocal microscopy refers to methods for blocking out-of-focus light in the image formation which improves resolution and contrast but tends to lead to focusing on a very thin focal plane and small field of view. Both tomography and confocal microscopy as well as other methods used in 3D imaging may be used in conjunction with aspects of the present disclosure to produce improved results. Another method may be staggered line scan sensors, where the sensor has several line scanners at different heights and or angles, and the sensor may take images at several focus planes at the same time.</p><p id="p-0053" num="0052">The following will provide, with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>8</b>E</figref>, detailed descriptions of adaptive sensing. <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>, and <b>7</b>A-<b>7</b>E</figref> illustrate a microscope and various microscope configurations. <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>4</b></figref> illustrate exemplary processes for adaptive sensing of a sample. <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an exemplary graph for attribute estimation. <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>C</figref> illustrate exemplary images of a sample at different depths. <figref idref="DRAWINGS">FIG. <b>8</b>A-<b>8</b>E</figref> illustrate exemplary sample geometries.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagrammatic representation of a microscope <b>100</b> consistent with the exemplary disclosed embodiments. The term &#x201c;microscope&#x201d; as used herein generally refers to any device or instrument for magnifying an object which is smaller than easily observable by the naked eye, i.e., creating an image of an object for a user where the image is larger than the object. One type of microscope may be an &#x201c;optical microscope&#x201d; that uses light in combination with an optical system for magnifying an object. An optical microscope may be a simple microscope having one or more magnifying lens. Another type of microscope may be a &#x201c;computational microscope&#x201d; that comprises an image sensor and image-processing algorithms to enhance or magnify the object's size or other properties. The computational microscope may be a dedicated device or created by incorporating software and/or hardware with an existing optical microscope to produce high-resolution digital images. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, microscope <b>100</b> comprises an image capture device <b>102</b>, a focus actuator <b>104</b>, a controller <b>106</b> connected to memory <b>108</b>, an illumination assembly <b>110</b>, and a user interface <b>112</b>. An example usage of microscope <b>100</b> may be capturing images of a sample <b>114</b> mounted on a stage <b>116</b> located within the field-of-view (FOV) of image capture device <b>102</b>, processing the captured images, and presenting on user interface <b>112</b> a magnified image of sample <b>114</b>.</p><p id="p-0055" num="0054">Image capture device <b>102</b> may be used to capture images of sample <b>114</b>. In this specification, the term &#x201c;image capture device&#x201d; as used herein generally refers to a device that records the optical signals entering a lens as an image or a sequence of images. The optical signals may be in the near-infrared, infrared, visible, and ultraviolet spectrums. Examples of an image capture device comprise a CCD camera, a CMOS camera, a photo sensor array, a video camera, a mobile phone equipped with a camera, a webcam, a preview camera, a microscope objective and detector, etc. Some embodiments may comprise only a single image capture device <b>102</b>, while other embodiments may comprise two, three, or even four or more image capture devices <b>102</b>. In some embodiments, image capture device <b>102</b> may be configured to capture images in a defined field-of-view (FOV). Also, when microscope <b>100</b> comprises several image capture devices <b>102</b>, image capture devices <b>102</b> may have overlap areas in their respective FOVs. Image capture device <b>102</b> may have one or more image sensors (not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) for capturing image data of sample <b>114</b>. In other embodiments, image capture device <b>102</b> may be configured to capture images at an image resolution higher than VGA, higher than 1 Megapixel, higher than 2 Megapixels, higher than 5 Megapixels, 10 Megapixels, higher than 12 Megapixels, higher than 15 Megapixels, or higher than 20 Megapixels. In addition, image capture device <b>102</b> may also be configured to have a pixel size smaller than 15 micrometers, smaller than 10 micrometers, smaller than 5 micrometers, smaller than 3 micrometers, or smaller than 1.6 micrometer.</p><p id="p-0056" num="0055">In some embodiments, microscope <b>100</b> comprises focus actuator <b>104</b>. The term &#x201c;focus actuator&#x201d; as used herein generally refers to any device capable of converting input signals into physical motion for adjusting the relative distance between sample <b>114</b> and image capture device <b>102</b>. Various focus actuators may be used, including, for example, linear motors, electrostrictive actuators, electrostatic motors, capacitive motors, voice coil actuators, magnetostrictive actuators, etc. In some embodiments, focus actuator <b>104</b> may comprise an analog position feedback sensor and/or a digital position feedback element. Focus actuator <b>104</b> is configured to receive instructions from controller <b>106</b> in order to make light beams converge to form a clear and sharply defined image of sample <b>114</b>. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, focus actuator <b>104</b> may be configured to adjust the distance by moving image capture device <b>102</b>.</p><p id="p-0057" num="0056">However, in other embodiments, focus actuator <b>104</b> may be configured to adjust the distance by moving stage <b>116</b>, or by moving both image capture device <b>102</b> and stage <b>116</b>. Microscope <b>100</b> may also comprise controller <b>106</b> for controlling the operation of microscope <b>100</b> according to the disclosed embodiments. Controller <b>106</b> may comprise various types of devices for performing logic operations on one or more inputs of image data and other data according to stored or accessible software instructions providing desired functionality. For example, controller <b>106</b> may comprise a central processing unit (CPU), support circuits, digital signal processors, integrated circuits, cache memory, or any other types of devices for image processing and analysis such as graphic processing units (GPUs). The CPU may comprise any number of microcontrollers or microprocessors configured to process the imagery from the image sensors. For example, the CPU may comprise any type of single- or multi-core processor, mobile device microcontroller, etc. Various processors may be used, including, for example, processors available from manufacturers such as Intel&#xae;, AMD&#xae;, etc. and may comprise various architectures (e.g., x86 processor, ARM&#xae;, etc.). The support circuits may be any number of circuits generally well known in the art, including cache, power supply, clock and input-output circuits. Controller <b>106</b> may be at a remote location, such as a computing device communicatively coupled to microscope <b>100</b>.</p><p id="p-0058" num="0057">In some embodiments, controller <b>106</b> may be associated with memory <b>108</b> used for storing software that, when executed by controller <b>106</b>, controls the operation of microscope <b>100</b>. In addition, memory <b>108</b> may also store electronic data associated with operation of microscope <b>100</b> such as, for example, captured or generated images of sample <b>114</b>. In one instance, memory <b>108</b> may be integrated into the controller <b>106</b>. In another instance, memory <b>108</b> may be separated from the controller <b>106</b>.</p><p id="p-0059" num="0058">Specifically, memory <b>108</b> may refer to multiple structures or computer-readable storage mediums located at controller <b>106</b> or at a remote location, such as a cloud server. Memory <b>108</b> may comprise any number of random access memories, read only memories, flash memories, disk drives, optical storage, tape storage, removable storage and other types of storage.</p><p id="p-0060" num="0059">Microscope <b>100</b> may comprise illumination assembly <b>110</b>. The term &#x201c;illumination assembly&#x201d; as used herein generally refers to any device or system capable of projecting light to illuminate sample <b>114</b>.</p><p id="p-0061" num="0060">Illumination assembly <b>110</b> may comprise any number of light sources, such as light emitting diodes (LEDs), LED array, lasers, and lamps configured to emit light, such as a halogen lamp, an incandescent lamp, or a sodium lamp. In one embodiment, illumination assembly <b>110</b> may comprise only a single light source. Alternatively, illumination assembly <b>110</b> may comprise four, sixteen, or even more than a hundred light sources organized in an array or a matrix. In some embodiments, illumination assembly <b>110</b> may use one or more light sources located at a surface parallel to illuminate sample <b>114</b>. In other embodiments, illumination assembly <b>110</b> may use one or more light sources located at a surface perpendicular or at an angle to sample <b>114</b>.</p><p id="p-0062" num="0061">In addition, illumination assembly <b>110</b> may be configured to illuminate sample <b>114</b> in a series of different illumination conditions. In one example, illumination assembly <b>110</b> may comprise a plurality of light sources arranged in different illumination angles, such as a two-dimensional arrangement of light sources. In this case, the different illumination conditions may comprise different illumination angles. For example, <figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a beam <b>118</b> projected from a first illumination angle &#x3b1;<b>1</b>, and a beam <b>120</b> projected from a second illumination angle &#x3b1;<b>2</b>. In some embodiments, first illumination angle &#x3b1;<b>1</b> and second illumination angle &#x3b1;<b>2</b> may have the same value but opposite sign. In other embodiments, first illumination angle &#x3b1;<b>1</b> may be separated from second illumination angle &#x3b1;<b>2</b>. However, both angles originate from points within the acceptance angle of the optics. In another example, illumination assembly <b>110</b> may comprise a plurality of light sources configured to emit light in different wavelengths. In this case, the different illumination conditions may comprise different wavelengths. In yet another example, illumination assembly <b>110</b> may configured to use a number of light sources at predetermined times. In this case, the different illumination conditions may comprise different illumination patterns. Accordingly and consistent with the present disclosure, the different illumination conditions may be selected from a group including: different durations, different intensities, different positions, different illumination angles, different illumination patterns, different wavelengths, or any combination thereof.</p><p id="p-0063" num="0062">Consistent with disclosed embodiments, microscope <b>100</b> may comprise, be connected with, or in communication with (e.g., over a network or wirelessly, e.g., via Bluetooth) user interface <b>112</b>. The term &#x201c;user interface&#x201d; as used herein generally refers to any device suitable for presenting a magnified image of sample <b>114</b> or any device suitable for receiving inputs from one or more users of microscope <b>100</b>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates two examples of user interface <b>112</b>. The first example is a smartphone or a tablet wirelessly communicating with controller <b>106</b> over a Bluetooth, cellular connection or a Wi-Fi connection, directly or through a remote server. The second example is a PC display physically connected to controller <b>106</b>. In some embodiments, user interface <b>112</b> may comprise user output devices, including, for example, a display, tactile device, speaker, etc. In other embodiments, user interface <b>112</b> may comprise user input devices, including, for example, a touchscreen, microphone, keyboard, pointer devices, cameras, knobs, buttons, etc. With such input devices, a user may be able to provide information inputs or commands to microscope <b>100</b> by typing instructions or information, providing voice commands, selecting menu options on a screen using buttons, pointers, or eye-tracking capabilities, or through any other suitable techniques for communicating information to microscope <b>100</b>. User interface <b>112</b> may be connected (physically or wirelessly) with one or more processing devices, such as controller <b>106</b>, to provide and receive information to or from a user and process that information. In some embodiments, such processing devices may execute instructions for responding to keyboard entries or menu selections, recognizing and interpreting touches and/or gestures made on a touchscreen, recognizing and tracking eye movements, receiving and interpreting voice commands, etc.</p><p id="p-0064" num="0063">Microscope <b>100</b> may also comprise or be connected to stage <b>116</b>. Stage <b>116</b> comprises any horizontal rigid surface where sample <b>114</b> may be mounted for examination. Stage <b>116</b> may comprise a mechanical connector for retaining a slide containing sample <b>114</b> in a fixed position. The mechanical connector may use one or more of the following: a mount, an attaching member, a holding arm, a clamp, a clip, an adjustable frame, a locking mechanism, a spring or any combination thereof. In some embodiments, stage <b>116</b> may comprise a translucent portion or an opening for allowing light to illuminate sample <b>114</b>. For example, light transmitted from illumination assembly <b>110</b> may pass through sample <b>114</b> and towards image capture device <b>102</b>. In some embodiments, stage <b>116</b> and/or sample <b>114</b> may be moved using motors or manual controls in the XY plane to enable imaging of multiple areas of the sample.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> depict a closer view of microscope <b>100</b> in two cases. Specifically, <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> illustrates the optical paths of two beams pairs when microscope <b>100</b> is out of focus, and <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> illustrates the optical paths of two beams pairs when microscope <b>100</b> is in focus. In cases where the sample is thicker than the depth of focus or the change in depth is rapid, some portions of the sample may be in focus, while other portions may not be in focus.</p><p id="p-0066" num="0065">As shown in <figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref>, image capture device <b>102</b> comprises an image sensor <b>200</b> and a lens <b>202</b>. In microscopy, lens <b>202</b> may be referred to as an objective lens of microscope <b>100</b>. The term &#x201c;image sensor&#x201d; as used herein generally refers to a device capable of detecting and converting optical signals into electrical signals. The electrical signals may be used to form an image or a video stream based on the detected signals. Examples of image sensor <b>200</b> may comprise semiconductor charge-coupled devices (CCD), active pixel sensors in complementary metal-oxide-semiconductor (CMOS), or N-type metal-oxide- semiconductor (NMOS, Live MOS). The term &#x201c;lens&#x201d; as used herein refers to a ground or molded piece of glass, plastic, or other transparent material with opposite surfaces either or both of which are curved, by means of which light rays are refracted so that they converge or diverge to form an image. The term &#x201c;lens&#x201d; may also refer to an element containing one or more lenses as defined above, such as in a microscope objective. The lens is positioned at least generally transversely of the optical axis of image sensor <b>200</b>. Lens <b>202</b> may be used for concentrating light beams from sample <b>114</b> and directing them towards image sensor <b>200</b>. In some embodiments, image capture device <b>102</b> may comprise a fixed lens or a zoom lens.</p><p id="p-0067" num="0066">When sample <b>114</b> is located at a focal-plane <b>204</b>, the image projected from lens <b>202</b> is completely focused. The term &#x201c;focal-plane&#x201d; is used herein to describe a plane that is perpendicular to the optical axis of lens <b>202</b> and passes through the lens's focal point. The distance between focal-plane <b>204</b> and the center of lens <b>202</b> is called the focal length and is represented by D<b>1</b>. In some cases, sample <b>114</b> may not be completely flat, and there may be small differences between focal-plane <b>204</b> and various regions of sample <b>114</b>. Accordingly, the distance between focal-plane <b>204</b> and sample <b>114</b> or a region of interest (ROI) of sample <b>114</b> is marked as D<b>2</b>. The distance D<b>2</b> corresponds with the degree in which an image of sample <b>114</b> or an image of ROI of sample <b>114</b> is out of focus. For example, distance D<b>2</b> may be between 0 and about 3 mm. In some embodiments, D<b>2</b> may be greater than 3 mm. When distance D<b>2</b> equals to zero, the image of sample <b>114</b> (or the image of ROI of sample <b>114</b>) is completely focused. In contrast, when D<b>2</b> has a value other than zero, the image of sample <b>114</b> (or the image of ROI of sample <b>114</b>) is out of focus.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> depicts a case where the image of sample <b>114</b> is out of focus. For example, the image of sample <b>114</b> may be out of focus when the beams of light received from sample <b>114</b> do not converge on image sensor <b>200</b>. <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> depicts a beams pair <b>206</b> and a beams pair <b>208</b>. Neither pair converges on image sensor <b>200</b>. For the sake of simplicity, the optical paths below sample <b>114</b> are not shown. Consistent with the present disclosure, beams pair <b>206</b> may correspond with beam <b>120</b> projected from illumination assembly <b>110</b> at illumination angle &#x3b1;<b>2</b>, and beams pair <b>208</b> may correspond with beam <b>118</b> projected from illumination assembly <b>110</b> at illumination angle &#x3b1;<b>1</b>. In addition, beams pair <b>206</b> may concurrently hit image sensor <b>200</b> with beams pair <b>208</b>. The term &#x201c;concurrently&#x201d; in this context means that image sensor <b>200</b> has recorded information associated with two or more beams pairs during coincident or overlapping time periods, either where one begins and ends during the duration of the other, or where a later one starts before the completion of the other. In other embodiments, beams pair <b>206</b> and beams pair <b>208</b> may sequentially contact image sensor <b>200</b>. The term &#x201c;sequentially&#x201d; means that image sensor <b>200</b> has started recording information associated with, for example, beam pair <b>206</b> after the completion of recording information associated with, for example, beam pair <b>208</b>.</p><p id="p-0069" num="0068">As discussed above, D<b>2</b> is the distance between focal-plane <b>204</b> and sample <b>114</b>, and it corresponds with the degree in which sample <b>114</b> is out of focus. In one example, D<b>2</b> may have a value of 50 micrometers. Focus actuator <b>104</b> is configured to change distance D<b>2</b> by converting input signals from controller <b>106</b> into physical motion. In some embodiments, in order to focus the image of sample <b>114</b>, focus actuator <b>104</b> may move image capture device <b>102</b>. In this example, to focus the image of sample <b>114</b> focus actuator <b>104</b> may move image capture device <b>102</b> 50 micrometers up. In other embodiments, in order to focus the image of sample <b>114</b>, focus actuator <b>104</b> may move stage <b>116</b> down. Therefore, in this example, instead of moving image capture device <b>102</b> 50 micrometers up, focus actuator <b>104</b> may move stage <b>116</b> 50 micrometers down.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> illustrates a case where the image of sample <b>114</b> is in focus. In this case, both beam pairs <b>206</b> and <b>208</b> converge on image sensor <b>200</b>, and distance D<b>2</b> equals to zero. In other words, focusing the image of sample <b>114</b> (or the image of ROI of sample <b>114</b>) may rely on adjusting the relative distance between image capture device <b>102</b> and sample <b>114</b>. The relative distance may be represented by D<b>1</b>-D<b>2</b>, and when distance D<b>2</b> equals to zero, the relative distance between image capture device <b>102</b> and sample <b>114</b> equals to distance D<b>1</b>, which means that the image of sample <b>114</b> is focused.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an exemplary workflow <b>300</b> for adaptive sensing of a sample, such as sample <b>114</b>, using a microscope, such as microscope <b>100</b>. Samples on a slide may have varying or otherwise non-homogeneous thickness and/or flatness, or a homogenous thickness but unknown prior to the acquisition, which may complicate imaging the samples. Samples may have varying thicknesses based on type of sample. Examples of samples include but are not limited to blood, fine needle aspirates (FNA), histopathology tissue samples, frozen sections, sperm, PAP smear, fecal, urine, petroleum, cream, algae, and brain slices. Blood samples may be smeared, which may result in some areas being thinner or thicker than other areas of the sample. Thin samples may be approximately 2 microns in thickness. Thick areas may be approximately 10 or more microns.</p><p id="p-0072" num="0071">FNA samples may be made using fluid and/or soft material from a syringe, which may be sprayed and smeared on a slide. FNA samples may typically have randomly dispersed thick areas, which may be more difficult to image and analyze. FNA samples may have thicknesses ranging from 2 microns to 20 or more microns.</p><p id="p-0073" num="0072">Tissue samples used for histopathology may typically be embedded with paraffin. These tissue samples may be cut with a microtome. These tissue samples may have thicknesses ranging from two microns to ten microns, with three microns to six microns being very common. However, they may occasionally be cut thicker.</p><p id="p-0074" num="0073">Frozen section tissue samples may be frozen in order to facilitate processing. Frozen section tissue samples may be thicker than histopathology samples due to technical difficulties. Although thicknesses ranging from four microns to six microns may be desirable, samples may be ten microns to thirty or more microns.</p><p id="p-0075" num="0074">Brain slices may sometimes be cut thicker than other sample types. For example, brain slices may have thicknesses of 5-50 microns, but may even have thicknesses ranging from 50-100 microns, and may be as thick as 200 microns.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIGS. <b>8</b>A-<b>8</b>E</figref> illustrate possible sample geometries of a sample <b>814</b>, which may correspond to sample <b>114</b>. Sample <b>814</b> may be mounted on a slide <b>815</b>, which may be a slide suitable for mounting on a microscope. <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> shows a top-down view of sample <b>814</b>, which also illustrates how the sample geometry may not be easily determined without an initial acquisition of the sample. In some embodiments, the microscope comprises a depth of field inside the field of view of the microscope, and the sample is imaged along a layer <b>817</b> corresponding to the depth of field of the microscope. The entire area of the sample imaged at a given moment can be within layer <b>817</b>, e.g. a 2D sample, or the sample can extend beyond the layer <b>817</b> corresponding to the depth of field of the microscope, e.g. 2.5 D and 3D samples.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> shows sample <b>814</b> having a 2D geometry. In this example, a single layer <b>817</b> may be used to capture sample <b>814</b>. Layer <b>817</b> may correspond to a depth of field of a focal plane. <figref idref="DRAWINGS">FIG. <b>8</b>C</figref> shows sample <b>814</b> having a 2.5D geometry. As seen in <figref idref="DRAWINGS">FIG. <b>8</b>C</figref>, a single layer <b>817</b> may capture much of sample <b>814</b> in focus, but may miss some portions, such as the peaks and valleys extending outside of layer <b>817</b>. To properly capture sample <b>814</b> in <figref idref="DRAWINGS">FIG. <b>8</b>C</figref>, a 2.5D reconstruction may rely on additional images corresponding to the portions outside of layer <b>817</b>. For instance, additional layers <b>817</b> may be used. However, additional efficiency may be gained by determining where the peaks and valleys are located (e.g., based on an attribute of sample <b>814</b>) to specifically target the missing portions, according to embodiments described herein. <figref idref="DRAWINGS">FIGS. <b>8</b>D and <b>8</b>E</figref> show sample <b>814</b> having a 3D geometry. In <figref idref="DRAWINGS">FIG. <b>8</b>D</figref>, a single layer <b>817</b> may be sufficient to capture the information in sample <b>814</b> which is of interest for a certain case. The 3D process may be used to enable imaging that layer without interference from the other parts of the sample. As seen in <figref idref="DRAWINGS">FIG. <b>8</b>E</figref>, multiple layers <b>817</b> may be used to cover the depth range of sample <b>814</b>. A 3D reconstruction may reconstruct layers <b>817</b> to produce an output image for sample <b>814</b>. However, additional efficiency may be gained by specifically selecting layers <b>814</b> based on the attribute of sample <b>814</b>. For example, the topmost layer <b>817</b> may only cover a small portion of sample <b>814</b>, and therefore may be reduced in size.</p><p id="p-0078" num="0077">At block <b>310</b>, the microscope performs an initial acquisition of the sample. The microscope may capture images of the sample at a single depth <b>320</b>A, multiple depths <b>320</b>B, and/or multiple locations <b>320</b>C. Single depth <b>320</b>A may comprise capturing images using one or more illuminations at a single depth of the sample. Similarly, multiple depths <b>320</b>B may comprise capturing images using one or more illuminations at multiple depths of the sample. Multiple locations <b>320</b>C may comprise capturing images using one or more illuminations at multiple locations of the sample.</p><p id="p-0079" num="0078">The sample may be scanned with some granularity in order to estimate an attribute, such as thickness, depth, and/or density, at different locations and to further determine a benefit from acquiring and/or calculating information about a larger depth of the sample. In addition, the acquired images may be part of a computational reconstruction of the sample, using a computational reconstruction algorithm such a two dimensional (2D), 2.5D, or 3D reconstruction to analyze the sample. Whereas 3D reconstruction may reconstruct a sample having 3D shapes and/or features, 2.5D reconstruction may reconstruct a sample having a generally thin, flat shape but uneven or non-smooth, similar to a crumpled sheet of paper. An example of that may be a histopathology slice of a tissue, which may not have been placed completely flat on the surface of the slide.</p><p id="p-0080" num="0079">At block <b>330</b>, the microscope may calculate an attribute of the sample, which could indicate that the sample has 2.5D or 3D structure, and therefore may require more than one focal plane. The attribute may be calculated for a single location <b>340</b>A, multiple locations in a field of view (&#x201c;FOV&#x201d;) <b>340</b>B, and/or multiple FOVs <b>340</b>C. The attribute may comprise a thickness, a depth, a density, a color, a stain structure of the sample (e.g., a structure formed or made visible when sample is stained), one or more distances from a lens, one or more focal planes of the sample, a sample structure of the sample, a convergence value, a pattern, or a frequency represented by the sample, in one or more locations of the sample. The attribute may be calculated in various ways.</p><p id="p-0081" num="0080">The depth of the sample may be determined from physical attributes of the acquisition or the image attributes resulting from the physical attributes of, for instance, defocus. For example, a sharpness level in the image at one location may be compared with a sharpness level at other locations of the captured image or a threshold sharpness value. In another example, the sample may have been illuminated from different angles. A difference in shift of features under the different illumination angles may be used to calculate the depth.</p><p id="p-0082" num="0081">The depth of the sample, or the benefit from additional depth information may be determined from the contents of the image. For example, a particular feature or object, such as a specific type of cell (e.g., white blood cells in a blood smear sample) may be recognized in an area of the image. The recognized feature may indicate the depth, and/or may indicate the benefit from additional depth information. For example, the feature may be larger than what was initially captured or may be a feature of interest. The acquisition process may be adapted to acquire additional images in several focus planes near the feature or acquire information to perform a 3D reconstruction in order to assure the feature is in perfect focus and to obtain the 3D information at that location.</p><p id="p-0083" num="0082">The sample may be stained using, for example, a Romanowsky stain, a Gram stain, a hematoxylin and eosin (H&#x26;E) stain, an immunohistochemistry (IHC) stain, a methylene blue stain, a DAPI stain, a fluorescent stain, or any other suitable stain.</p><p id="p-0084" num="0083">Image analysis may be used, for example on preview or magnified images, to determine the attribute. A color, a contrast, an edge, a diffraction, and/or an absorption may be analyzed in the images. Different parts of the sample may respond differently to staining or illumination with different wavelengths or may otherwise appear different in imaging due to changes in focus and/or other characteristics. These differences may be used to estimate which areas of the sample are thicker or thinner than others. For example, if an area is darker than its surroundings, the area may be thicker than its surroundings. Moreover, knowledge from previously analyzed samples and/or prior analysis may be available. For example, some samples made by a consistent process, such as blood samples made with a stainer. More accurate results may be achieved by using the prior analysis.</p><p id="p-0085" num="0084">The attribute may be determined from changes in aberrations. When samples are in different focal planes, the optics may introduce different optical aberrations. These aberrations may produce artifacts in the images, such as stretching, contracting, shifting, etc. The aberrations may suggest the sample's distance from the lens, which may be used to estimate the depth and/or thickness of the sample. In addition, the pupil function of the optics at each location may be used directly.</p><p id="p-0086" num="0085">The attribute may be determined using computational reconstruction. For example, the sample may be analyzed using the results from a computational reconstruction algorithm (such as 2D, 2.5D, or 3D reconstruction) applied to the initial acquisition. For instance, 3D reconstruction may be used with a fewer number of layers than would be utilized for a complete 3D reconstruction in order to assess a structure of the sample. Additional layers and/or illumination conditions as well as locations for the additional layers may be determined based on the structure of the sample.</p><p id="p-0087" num="0086">The attribute may be determined using values of convergence. Reconstruction algorithms may track the algorithm's convergence. Places of low convergence may indicate thick or complex areas which may benefit from further imaging.</p><p id="p-0088" num="0087">The attribute may be determined using a laser. The sample may be illuminated using one or more lasers. The resulting patterns may be analyzed to determine the attribute.</p><p id="p-0089" num="0088">The attribute may be determined using pattern recognition, such as computer vision or deep learning. For known types of samples, a database of expected patterns and shapes in images may be available. For instance, a white blood cell's appearance when covered by different thicknesses of other fluids in an FNA sample may be available and a subsequent FNA sample may be analyzed based on the expected patterns to determine the attribute.</p><p id="p-0090" num="0089">The attribute may be determined from the properties of a Fourier transformation of the images. The Fourier transformation of the images may reveal the frequencies represented in the images. Thick samples may have less high frequencies because of the multiple scatters of light. An &#x201c;envelope&#x201d; of decreased of energy with increasing frequencies may suggest thickness. A change of frequencies between adjacent tiles of an image may indicate differences between the tiles.</p><p id="p-0091" num="0090">The attribute may be determined from digital refocusing using light field analysis. Light field analysis may be based on knowledge of the conditions under which the images were taken to form a basic refocusing approximation. The resultant refocusing approximation may be used to estimate the thickness and location of the sample.</p><p id="p-0092" num="0091">The attribute may be determined from the statistics of lateral shifts in areas of the images. Changes in focus may result in lateral shifts of details. Analyzing how changes in focus affect different areas in the images may be used to determine local depth.</p><p id="p-0093" num="0092">The attribute may be determined using machine learning (e.g., neural networks, convolutional neural networks, deep learning, computer vision, etc.). A database of images under known acquisition conditions may be used to train a machine learning algorithm to identify the thickness and/or location of the sample.</p><p id="p-0094" num="0093">At block <b>350</b>, the microscope may determine a process for adapting the acquisition process. The process may be an acquisition procedure that may perform 3D measurement, such as focus stacking or 3D reconstruction. The process may comprise a 3D reconstruction on initial acquisition <b>360</b>A, which in some embodiments may be a continuation of the initial acquisition. In other words, the initial acquisition may be part of the 3D reconstruction process. The process may comprise a 3D acquisition <b>360</b>B, such as focus stacking. The process may be performed after or concurrently with the attribute estimation.</p><p id="p-0095" num="0094">The attribute, such as thickness and/or density in different locations of the sample, may be used to determine where to place additional layers and/or voxels for 2.5D analysis or 3D analysis. 3D acquisition may comprise, for example, focus stack on a same lateral area <b>370</b>A, focus stack in layers <b>370</b>B, and/or a single distance from the sample <b>370</b>C. Thus, the 3D acquisition <b>360</b>B may be able to analyze different focal planes in a field of view, expand autofocus parameters for 2.5D and 3D samples, and determine a location of maximum data in a thick sample etc.</p><p id="p-0096" num="0095">At block <b>380</b>, 3D data may be prepared. For example, the 3D data may be registered so the layers are in the correct relative locations or use image processing to improve the display or creating an &#x201c;all in focus&#x201d; image for cases such as 2.5D samples. When the 3D data has been prepared, the data may be displayed or stored as shown at block <b>390</b>A. The 3D computational reconstruction can be performed from any subset of images taken as shown at block <b>390</b>B. For example, a 3D reconstruction process may be started, modified, and completed.</p><p id="p-0097" num="0096">The workflow <b>300</b> may comprise a method for adaptive sensing of the sample, and each of the blocks of workflow <b>300</b> may comprise steps of the method for adaptive sensing of the sample.</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of an exemplary process <b>400</b> for determining adaptive sensing, for example, sample <b>114</b>. The steps of process <b>400</b> may be performed by a microscope, such as an autofocus microscope. The term &#x201c;autofocus microscope&#x201d; as used herein generally refers to any device for magnifying sample <b>114</b> with the capability to focus the image of sample <b>114</b> (or the image of ROI of sample <b>114</b>) in an automatic or semiautomatic manner. In the following description, reference is made to components of microscope <b>100</b> for purposes of illustration. It will be appreciated, however, that other implementations are possible and that other components may be utilized to implement the example process.</p><p id="p-0099" num="0098">At step <b>410</b>, image capture device <b>102</b> may be used to capture an initial image set of sample <b>114</b>. Capturing the initial image set may comprise capturing more than one image, such as a first plurality of images. For example, a plurality of illumination conditions for illumination assembly <b>110</b> to illuminate sample <b>114</b>. Examples or illumination conditions comprise illumination angles, illumination wavelengths, or illumination patterns. The illumination conditions and image capture settings may be default settings for microscope <b>100</b>, or may be default settings based on the type of sample <b>114</b>. The default settings may correspond to settings which may minimize how many initial images are captured to determine an attribute of sample <b>114</b>. The default settings may correspond to preliminary steps of a 3D measurement process. Alternatively, the default settings may be determined by a user.</p><p id="p-0100" num="0099">At step <b>420</b>, controller <b>106</b> may identify, in response to the initial image set, an attribute of sample <b>114</b>. The attribute may correspond to one or more aspects of sample <b>114</b>. In some cases, several attributes may be determined, for example, the depth (e.g., distance of the sample or certain layers of the sample from the microscope) and thickness of the sample in the analysis process may be determined. The attribute may comprise a thickness of sample <b>114</b> at one or more locations. The attribute may comprise a depth of sample <b>114</b> at one or more locations. The attribute may comprise a density of sample <b>114</b> at one or more locations. Other attributes of sample <b>114</b> may comprise a color, a stain structure of sample <b>114</b> (e.g., a structure formed or made visible when sample <b>114</b> is stained), a distance from lens <b>202</b> of image capture device <b>102</b>, a plurality of distances between lens <b>202</b> and sample <b>114</b>, a plurality of focal planes of sample <b>114</b> in relation to image capture device <b>102</b>, a sample structure of sample <b>114</b>, a convergence value, a pattern, or a frequency determined based at least on one of color analysis, analysis of optical aberrations, computational reconstruction, pattern recognition, Fourier transformation, or light field analysis.</p><p id="p-0101" num="0100">In some embodiments, sample <b>114</b> may be stained using, for example, a Romanowsky stain, a Gram stain, a hematoxylin and eosin (H&#x26;E) stain, an immunohistochemistry (IHC) stain, a methylene blue stain, a DAPI stain, a fluorescent stain, or any other suitable stain. When sample <b>114</b> is stained, controller <b>106</b> may perform color analysis to determine the attribute of sample <b>114</b>. For example, the attribute for an area of sample <b>114</b> may be determined based on comparing a color or stain structure of the area with a color or stain structure of another area of the sample. In other examples, the attribute for the area may be determined based on comparing the color or stain structure of the area with a color or stain structure from empirical data. For instance, memory <b>108</b> may comprise a database of colors and/or stain structures and how they may correlate to attributes. The comparison of colors and/or stain structures may be based on pattern recognition, machine learning, or any other suitable comparison process.</p><p id="p-0102" num="0101">In some embodiments, controller <b>106</b> may perform analysis of optical aberrations to determine the attribute of sample <b>114</b>. This analysis may comprise identifying an optical aberration from the initial set and determining, in response to identifying the optical aberration, the attribute. For example, controller <b>106</b> may determine a distance of sample <b>114</b> from lens <b>202</b> in response to identifying the optical aberration. Controller <b>106</b> may then determine the attribute in response to determining this distance.</p><p id="p-0103" num="0102">In some embodiments, identifying the attribute may comprise using pattern recognition. For example, illumination assembly <b>110</b> may illuminate sample <b>114</b> with a laser, and controller <b>106</b> may perform pattern recognition on the resulting pattern to identify the attribute.</p><p id="p-0104" num="0103">In some embodiments, identifying the attribute may comprise performing a Fourier transformation. For example, controller <b>106</b> may perform the Fourier transformation using the initial image set. Controller <b>106</b> may then determine frequencies represented in sample <b>114</b> based at least on the Fourier transformation. Controller <b>106</b> may identify the attribute in response to determining the frequencies.</p><p id="p-0105" num="0104">In some embodiments, identifying the attribute may comprise performing a light field analysis. For example, in response to capturing the initial image set, controller <b>106</b> may perform the light field analysis. The initial image set may have been illuminated under particular illumination settings, for example by a laser of illumination assembly <b>110</b>. Controller <b>106</b> may then identify the attribute by identifying characteristics of the light field analysis.</p><p id="p-0106" num="0105">In addition, calculating the attribute may comprise estimating the attribute corresponding to one or more locations of sample <b>114</b> as described herein. For example, the attribute may refer to one or more specific points and/or areas of sample <b>114</b>.</p><p id="p-0107" num="0106">At step <b>430</b>, controller <b>106</b> may determine, in response to identifying the attribute, a three-dimensional (3D) process for sensing sample <b>114</b>. The 3D process may comprise a process for sensing the sample. In some embodiments, the 3D process may comprise a reconstruction process, such as a 3D reconstruction process, for reconstructing the sample in response to the initial image set. In some examples, the 3D process may not require images beyond the initial image set.</p><p id="p-0108" num="0107">In some embodiments, determining the 3D process may comprise controller <b>106</b> determining the sample structure based at least on the computational reconstruction, and determining an illumination condition and an image capture setting in response to determining the sample structure.</p><p id="p-0109" num="0108">The term &#x201c;sample structure&#x201d; as used herein may refer to a structure and/or structural features of the sample, including, for example, biomolecules, whole cells, portions of cells such as various cell components (e.g., cytoplasm, mitochondria, nucleus, chromosomes, nucleoli, nuclear membrane, cell membrane, Golgi apparatus, lysosomes), cell-secreted components (e.g., proteins secreted to intercellular space, proteins secreted to body fluids, such as serum, cerebrospinal fluid, urine), microorganisms, and more. Controller <b>106</b> may determine the illumination condition and the image capture setting based on a benefit from capturing additional detail at a location of the sample structure. Determining the illumination condition and the image capture setting may also comprise identifying, in response to the computational reconstruction, an area of the sample having a convergence value that indicates low convergence.</p><p id="p-0110" num="0109">At step <b>440</b>, controller <b>106</b> may generate, in response to identifying the attribute, an output image set comprising more than one focal plane using a three-dimensional (3D) process. The 3D process may comprise a computational process based at least on capturing the initial image set. For example, the 3D process may be a continuation of a computational process that started with capturing the initial image set.</p><p id="p-0111" num="0110">In some embodiments, the 3D process may comprise capturing one or more subsequent images of the sample using one or more illumination conditions of illumination assembly <b>110</b> and one or more image capture settings for image capture device <b>102</b>. A number of illumination conditions for the 3D process may be greater than a number of illumination conditions for capturing the initial image set.</p><p id="p-0112" num="0111">In some embodiments, the 3D process may comprise determining a plurality of focal planes for capturing the one or more subsequent images based at least on the attribute. The one or more illumination conditions and the one or more image capture settings may correspond to the plurality of focal planes. The one or more subsequent images may be taken at one or more locations of the sample determined based at least on the attribute. For example, the attribute may indicate one or more ROIs. Controller <b>106</b> may have determined focal planes to ensure that all ROIs can be captured in focus.</p><p id="p-0113" num="0112">In some embodiments, the 3D process may comprise performing a 3D reconstruction of the sample based at least on a subset of images captured by the image capture device. In some embodiments, the 3D process may comprise performing a 2.5D reconstruction of the sample based at least on a subset of images captured by the image capture device in order to generate 3D data from the sample. In other words, the 3D process may not be restricted to 3D reconstructions.</p><p id="p-0114" num="0113">In some embodiments, the 3D process comprises performing focus stacking for the sample based at least on a subset of images captured by the image capture device. For example, the 3D process may comprise capturing a plurality of images at a respective plurality of focal planes. In some embodiments, the 3D process may comprise capturing images at a plurality of distances between the image capture device and the sample. A number of focal planes for the 3D process may be greater than a number of distances in the plurality of distances. However, controller <b>106</b> may determine the number of focal planes to efficiently capture ROIs of sample <b>114</b>, without using extraneous focal planes and also without neglecting any ROI. In some embodiments, the 3D process may comprise a plurality of focus levels for adjusting the image capture device.</p><p id="p-0115" num="0114">In some embodiments, the 3D process may comprise capturing one or more subsequent images of the sample using a plurality of illumination conditions for illuminating the sample. The plurality of illumination conditions may comprise at least one of an illumination angle, an illumination wavelength, or an illumination pattern. The illumination conditions may be determined based on the attribute, for example, to ensure that ROIs are properly captured with sufficient detail.</p><p id="p-0116" num="0115">In some embodiments, the 3D process may comprise capturing a second plurality of images. A number of images in the second plurality of images may be greater than a number of images in the first plurality of images. In some embodiments, the first plurality of images and the second plurality of images may correspond to a same area of the sample. In some embodiments, the second plurality of images may be captured at a same relative location of the image capture device with respect to the sample as the first plurality of images. For example, controller <b>106</b> may capture a minimal number of images for the initial image set in order to minimize extraneous captures. Controller <b>106</b> may detect the attribute and determine that the initial image set insufficiently captures the ROIs of sample <b>114</b>. Controller <b>106</b> may then determine how many additional images can be used to sufficiently capture sample <b>114</b>.</p><p id="p-0117" num="0116">In some embodiments, the 3D process may be performed within a threshold time from capturing the initial image set. The threshold time may be one of 5 microseconds, 10 microseconds, 1 second, 5 seconds, 10 seconds, 1 minute, 3 minutes or 5 minutes, or within a range defined by any two of the preceding values. For example, the threshold time can be within a range from 5 microseconds to 10 second, from 5 microseconds to 1 minute, from 5 microseconds to 3 minutes, or 5 microseconds to 5 minutes. In some embodiments, the 3D process may be performed concurrently with capturing the initial image set, or the 3D process may comprise the initial image set.</p><p id="p-0118" num="0117">Any of the steps of method <b>400</b> can be combined with any method step corresponding to a block of workflow <b>300</b> as described herein.</p><p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a graph <b>500</b> which may represent an identified attribute. The initial image set may comprise images taken at various heights, corresponding to the height axis or may comprise a set of images corresponding to illumination conditions. The data density axis may correspond to the attribute. For example, higher values along the data density axis may indicate greater complexity or detail present at that height, which may benefit from additional imaging to properly capture sufficient detail or merit adding a layer in a focus stack or 3D process. The density may indicate the depth of layers in the sample and the span of depths may show the thickness of the sample. The processor may be configured to image regions of the sample with higher data density at higher resolution. Alternatively or in combination, the processor can be configured to process data from the regions of the sample with increased data density in order to provide images with sufficient resolution. Also, the process can be configured not to image regions of the image with lower data density, for example regions of the image below a threshold amount. This can provide images with increased resolution, proper representation of the layers of data, and decreased scan time and storage for the images.</p><p id="p-0120" num="0119">The data density may correspond to sample structure. The image capture settings may be based on, for example, heights corresponding to local peaks in the graph. By prioritizing image capture based on the local peaks, detailed areas of sample <b>114</b> may be captured. In other words, the local peaks in <figref idref="DRAWINGS">FIG. <b>5</b></figref> may correspond to ROIs, and the 3D process can be directed toward the peaks of data density in the attribute.</p><p id="p-0121" num="0120"><figref idref="DRAWINGS">FIGS. <b>6</b>A to <b>6</b>C</figref> show images from a 3D reconstruction based on the attribute shown in graph <b>500</b>. Each image corresponds to a layer in the sample. As can be seen in the figures, there may be good agreement between the data density in the graph and the data in the images of the sample. These images may also be used to demonstrate how the attribute may be determined from a focus stack. In this example, the images may be images taken at different heights, as part of the initial image set. Capturing images at different focal planes may capture different features, which may be used to determine the attribute of the sample. For example, in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, a feature on the left of the image may be in focus and sufficiently captured, whereas a feature on the upper right of the image may be out of focus, insufficiently captured, or tissue from the sample not present at the corresponding depth. In contrast, in <figref idref="DRAWINGS">FIG. <b>6</b>C</figref>, the feature on the upper right may be in focus and sufficiently captured, whereas the feature on the left may be out of focus, insufficiently captured, or tissue from the sample not present at the corresponding depth. The images shown in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is at an intermediate depth as compared with the depth of images <figref idref="DRAWINGS">FIGS. <b>6</b>A and <b>6</b>C</figref>. Thus, by determining the attribute comprising the data density in the initial image set at different locations, the 3D process can be adapted to image regions with higher amounts of data density in order to provide an output image with higher resolution and decreased time. Although <figref idref="DRAWINGS">FIGS. <b>6</b>A to <b>6</b>C</figref> are shown with respect to a substantially fixed lateral location, additional lateral locations can be scanned with a mosaic or tile pattern or a line scan or staggered line scan at each of a plurality of depths in order to determine the attribute and regions to be subsequently scanned with the adaptive 3D process as described herein.</p><p id="p-0122" num="0121"><figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>7</b>F</figref> illustrate different configurations of microscope <b>100</b> for determining phase information under a variety of illumination conditions. According to one embodiment that may be implemented in the configuration of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, microscope <b>100</b> may comprise illumination assembly <b>110</b>, focus actuator <b>104</b>, lens <b>202</b>, and image sensor <b>200</b>. In this embodiment, controller <b>106</b> may acquire a group of images from different focal-planes for each illumination condition, which may be compared to determine the attribute.</p><p id="p-0123" num="0122">According to another embodiment that may be implemented in the configuration of <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, microscope <b>100</b> may comprise illumination assembly <b>110</b>, lens <b>202</b>, a beam splitter <b>600</b>, a first image sensor <b>200</b>A, and a second image sensor <b>200</b>B. In this embodiment, first image sensor <b>200</b>A and second image sensor <b>200</b>B may capture different types of images, and controller <b>106</b> may combine the information from first image sensor <b>200</b>A and second image sensor <b>200</b>B. In one example, image sensor <b>200</b>A may capture Fourier-plane images and second image sensor <b>200</b>B may capture real-plane images. Accordingly, controller <b>106</b> may acquire, for each illumination condition, a Fourier-plane image from first image sensor <b>200</b>A and a real-plane image from second image sensor <b>200</b>B. Therefore, controller <b>106</b> may combine information from the Fourier-plane image and the real-plane image in order to determine the attribute. In another example, image sensor <b>200</b>A may be configured to capture focused images and second image sensor <b>200</b>B is configured to capture unfocused images. It is also possible that additional sensors may be added. For example, 2, 3 or more different sensors may be configured to capture images in 2, 3 or more different focal planes simultaneously.</p><p id="p-0124" num="0123">According to another embodiment that may be implemented in the configurations of <figref idref="DRAWINGS">FIG. <b>7</b>C</figref> and <figref idref="DRAWINGS">FIG. <b>7</b>D</figref>, microscope <b>100</b> may comprise a light source <b>602</b>, a beam splitter <b>600</b>, lens <b>202</b>, and image sensor <b>200</b>. In this embodiment, light source <b>602</b> may project a light beam (coherent or at least partially coherent) towards beam splitter <b>600</b>, the beam splitter generates two light beams that travel through two different optical paths and create an interference pattern. In the configuration of <figref idref="DRAWINGS">FIG. <b>7</b>C</figref>, the interference pattern is created on sample <b>114</b>, and in <figref idref="DRAWINGS">FIG. <b>7</b>D</figref>, the interference pattern is created on image sensor <b>200</b>. In the case presented in <figref idref="DRAWINGS">FIG. <b>7</b>D</figref>, controller <b>106</b> may identify, for each illumination condition, the interference pattern between the two light beams traveling through the different optical paths, and determine, from the interference pattern, the attribute.</p><p id="p-0125" num="0124">According to yet another embodiment that may be implemented in the configurations of <figref idref="DRAWINGS">FIG. <b>7</b>E</figref> and <figref idref="DRAWINGS">FIG. <b>7</b>F</figref>, microscope <b>100</b> may comprise illumination assembly <b>110</b>, lens <b>202</b>, an optical element <b>604</b>, and at least one image sensor <b>200</b>. In this embodiment, optical element <b>604</b> is configured to impose some form of modulation on the light received from sample <b>114</b>. The modulation may be imposed on the phase, the frequency, the amplitude, or the polarization of the beam. In the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>7</b>E</figref>, microscope <b>100</b> may comprise a dynamic optical element, such as spatial light modulator (SLM), that may dynamically change the modulation. Controller <b>106</b> may use the different information caused by the dynamic optical element to determine the attribute.</p><p id="p-0126" num="0125">Alternatively, in the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>7</b>F</figref>, microscope <b>100</b> may comprise a fixed optical element, such as phase-shift mask, beam splitter <b>600</b>, first image sensor <b>200</b>A, and second image sensor <b>200</b>B. Controller <b>106</b> may combine information from first image sensor <b>200</b>A and second image sensor <b>200</b>B to determine the phase information under each illumination condition.</p><p id="p-0127" num="0126">As detailed above, the computing devices and systems described and/or illustrated herein broadly represent any type or form of computing device or system capable of executing computer-readable instructions, such as those contained within the modules described herein. In their most basic configuration, these computing device(s) may each comprise at least one memory device and at least one physical processor.</p><p id="p-0128" num="0127">The term &#x201c;memory&#x201d; or &#x201c;memory device,&#x201d; as used herein, generally represents any type or form of volatile or non-volatile storage device or medium capable of storing data and/or computer-readable instructions. In one example, a memory device may store, load, and/or maintain one or more of the modules described herein. Examples of memory devices comprise, without limitation, Random Access Memory (RAM), Read Only Memory (ROM), flash memory, Hard Disk Drives (HDDs), Solid-State Drives (SSDs), optical disk drives, caches, variations or combinations of one or more of the same, or any other suitable storage memory.</p><p id="p-0129" num="0128">In addition, the term &#x201c;processor&#x201d; or &#x201c;physical processor,&#x201d; as used herein, generally refers to any type or form of hardware-implemented processing unit capable of interpreting and/or executing computer-readable instructions. In one example, a physical processor may access and/or modify one or more modules stored in the above-described memory device. Examples of physical processors comprise, without limitation, microprocessors, microcontrollers, Central Processing Units (CPUs), Field-Programmable Gate Arrays (FPGAs) that implement softcore processors, Application-Specific Integrated Circuits (ASICs), portions of one or more of the same, variations or combinations of one or more of the same, or any other suitable physical processor.</p><p id="p-0130" num="0129">Although illustrated as separate elements, the method steps described and/or illustrated herein may represent portions of a single application. In addition, in some embodiments one or more of these steps may represent or correspond to one or more software applications or programs that, when executed by a computing device, may cause the computing device to perform one or more tasks, such as the method step.</p><p id="p-0131" num="0130">In addition, one or more of the devices described herein may transform data, physical devices, and/or representations of physical devices from one form to another. For example, one or more of the devices recited herein may receive image data of a sample to be transformed, transform the image data, output a result of the transformation to determine a 3D process, use the result of the transformation to perform the 3D process, and store the result of the transformation to produce an output image of the sample. Additionally or alternatively, one or more of the modules recited herein may transform a processor, volatile memory, non-volatile memory, and/or any other portion of a physical computing device from one form to another by executing on the computing device, storing data on the computing device, and/or otherwise interacting with the computing device.</p><p id="p-0132" num="0131">The term &#x201c;computer-readable medium,&#x201d; as used herein, generally refers to any form of device, carrier, or medium capable of storing or carrying computer-readable instructions. Examples of computer-readable media comprise, without limitation, transmission-type media, such as carrier waves, and non-transitory-type media, such as magnetic-storage media (e.g., hard disk drives, tape drives, and floppy disks), optical-storage media (e.g., Compact Disks (CDs), Digital Video Disks (DVDs), and BLU-RAY disks), electronic-storage media (e.g., solid-state drives and flash media), and other distribution systems.</p><p id="p-0133" num="0132">The process parameters and sequence of the steps described and/or illustrated herein are given by way of example only and can be varied as desired. For example, while the steps illustrated and/or described herein may be shown or discussed in a particular order, these steps do not necessarily need to be performed in the order illustrated or discussed. The various exemplary methods described and/or illustrated herein may also omit one or more of the steps described or illustrated herein or comprise additional steps in addition to those disclosed.</p><p id="p-0134" num="0133">The processor as disclosed herein can be configured to perform any one or more steps of a method as disclosed herein.</p><p id="p-0135" num="0134">The preceding description has been provided to enable others skilled in the art to best utilize various aspects of the exemplary embodiments disclosed herein. This exemplary description is not intended to be exhaustive or to be limited to any precise form disclosed. Many modifications and variations are possible without departing from the spirit and scope of the instant disclosure. The embodiments disclosed herein should be considered in all respects illustrative and not restrictive. Reference should be made to the appended claims and their equivalents in determining the scope of the instant disclosure.</p><p id="p-0136" num="0135">Unless otherwise noted, the terms &#x201c;connected to&#x201d; and &#x201c;coupled to&#x201d; (and their derivatives), as used in the specification and claims, are to be construed as permitting both direct and indirect (i.e., via other elements or components) connection. In addition, the terms &#x201c;a&#x201d; or &#x201c;an,&#x201d; as used in the specification and claims, are to be construed as meaning &#x201c;at least one of&#x201d; Finally, for ease of use, the terms &#x201c;including&#x201d; and &#x201c;having&#x201d; (and their derivatives), as used in the specification and claims, are interchangeable with and have the same meaning as the word &#x201c;comprising.&#x201d; The terms &#x201c;based on&#x201d; and &#x201c;in response to&#x201d; are used interchangeably in the present disclosure.</p><p id="p-0137" num="0136">While preferred embodiments of the present invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only. Numerous variations, changes, and substitutions will now occur to those skilled in the art without departing from the invention. It should be understood that various alternatives to the embodiments of the invention described herein may be employed in practicing the invention. It is intended that the following claims define the scope of the invention and that methods and structures within the scope of these claims and their equivalents be covered thereby.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A microscope comprising:<claim-text>an illumination assembly;</claim-text><claim-text>an image capture device configured to collect light from a sample illuminated by the illumination assembly; and</claim-text><claim-text>a processor configured to execute instructions which cause the microscope to:</claim-text><claim-text>capture, using the image capture device, an initial image set of the sample;</claim-text><claim-text>identify, by the processor in response to the initial image set, an attribute of the sample by performing analysis on the initial image set independently from a user;</claim-text><claim-text>determine, in response to identifying the attribute, a three-dimensional (3D) process; and</claim-text><claim-text>generate, using the determined 3D process, an output image set comprising more than one focal plane.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The microscope of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 3D process comprises capturing one or more subsequent images of the sample using one or more illumination conditions of the illumination assembly and one or more image capture settings for the image capture device.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The microscope of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein a number of illumination conditions for the 3D process is greater than a number of illumination conditions for capturing the initial image set.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The microscope of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the 3D process comprises determining a plurality of focal planes for capturing the one or more subsequent images based at least on the attribute, and the one or more illumination conditions and the one or more image capture settings correspond to the plurality of focal planes.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The microscope of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the one or more subsequent images are taken at one or more locations of the sample determined based at least on the attribute.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The microscope of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the 3D process comprises performing a 3D reconstruction of the sample based at least on a subset of images captured by the image capture device.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The microscope of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the 3D process comprises performing a 2.5D reconstruction of the sample based at least on a subset of images captured by the image capture device in order to generate 3D data from the sample.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The microscope of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the 3D process comprises performing focus stacking for the sample based at least on a subset of images captured by the image capture device.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The microscope of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the 3D process comprises capturing a plurality of images at a respective plurality of focal planes.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The microscope of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein identifying the attribute comprises estimating the attribute corresponding to one or more locations of the sample.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The microscope of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 3D process is performed within a threshold time from capturing the initial image set.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The microscope of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the attribute comprises a thickness of the sample at one or more locations of the sample.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The microscope of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the attribute comprises a depth of the sample at one or more locations of the sample.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The microscope of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 3D process comprises capturing images at a plurality of distances between the image capture device and the sample, and a number of focal planes for the 3D process is greater than a number of distances in the plurality of distances.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The microscope of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein capturing the initial image set comprises capturing the initial image set of the sample using a plurality of illumination conditions for illuminating the sample, and the plurality of illumination conditions comprises at least one of an illumination angle, an illumination wavelength, or an illumination pattern.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The microscope of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 3D process comprises capturing one or more subsequent images of the sample using a plurality of illumination conditions for illuminating the sample, and the plurality of illumination conditions comprises at least one of an illumination angle, an illumination wavelength, or an illumination pattern.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The microscope of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 3D process comprises a plurality of focus levels for adjusting the image capture device.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The microscope of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the attribute comprises at least one of a thickness, a density, a depth, a color, a stain structure of the sample, a distance from a lens of the image capture device, a plurality of distances between the lens of the image capture device and the sample, a plurality of focal planes of the sample in relation to the image capture device, a sample structure, a convergence value, a pattern, or a frequency determined based at least on one of color analysis, analysis of optical aberrations, computational reconstruction, pattern recognition, Fourier transformation, or light field analysis.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The microscope of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the sample is stained and the color analysis comprises determining the attribute for an area of the sample based at least on comparing a color or stain structure of the area with a color or stain structure of another area of the sample.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The microscope of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein identifying the attribute comprises determining, using pattern recognition, the attribute of the sample.</claim-text></claim></claims></us-patent-application>