<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004819A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004819</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17930221</doc-number><date>20220907</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202111168520.X</doc-number><date>20210930</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>2457</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>003</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>24578</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND APPARATUS FOR TRAINING SEMANTIC RETRIEVAL NETWORK, ELECTRONIC DEVICE AND STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Qu</last-name><first-name>Yingqi</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Ding</last-name><first-name>Yuchen</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Liu</last-name><first-name>Jing</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Wu</last-name><first-name>Hua</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Haifeng</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.</orgname><role>03</role><address><city>Beijing</city><country>CN</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The disclosure provides a method for training a semantic retrieval network, an electronic device and a storage medium. The method includes: obtaining a training sample including a search term and n candidate files corresponding to the search term, where n is an integer greater than 1; inputting the training sample into the ranking model, to obtain n first correlation degrees output by the ranking model, in which each first correlation degree represents a correlation between a candidate document and the search term; inputting the training sample into the semantic retrieval model, to obtain n second correlation degrees output by the semantic retrieval model, wherein each second correlation degree represents a correlation between a candidate document and the search term; and training the semantic retrieval model and the ranking model jointly based on the n first correlation degrees and the n second correlation degrees.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="115.23mm" wi="153.25mm" file="US20230004819A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="229.87mm" wi="155.62mm" file="US20230004819A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="123.95mm" wi="150.62mm" file="US20230004819A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="191.94mm" wi="156.13mm" file="US20230004819A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="206.59mm" wi="153.42mm" file="US20230004819A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is based on and claims priority to Chinese patent applications Serial No. 202111168520.X filed on Sep. 30, 2021, the entire contents of which are incorporated herein by reference.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The disclosure relates to a field of data processing, especially a field of artificial intelligence such as natural language processing and deep learning, and in particular to a method for training a semantic retrieval network, an apparatus for training a semantic retrieval network, an electronic device and a storage medium.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In the related art, an information retrieval system generally has a recalling stage and a ranking stage. In the recalling stage, K candidates are obtained from a large-scale candidate database based on a semantic model. In the ranking stage, a more complex model is used to resort the K candidates, and return a sorted answer list at last. The semantic retrieval model and the ranking model are interdependent. The ranking model needs to adapt to a distribution of candidates produced by the semantic retrieval model, and the semantic retrieval model needs to continuously learn from the ranking model to enhance its own ability.</p><p id="p-0005" num="0004">However, the retrieval system in the related art has low efficiency for training the semantic retrieval model and the ranking model, and a poor training effect.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">Embodiments of the disclosure provide a method for training a semantic retrieval network, an electronic device and a storage medium.</p><p id="p-0007" num="0006">According to a first aspect of the disclosure, a method for training a semantic retrieval network is provided. The semantic retrieval network includes a semantic retrieval model and a ranking model, and the method includes: obtaining a training sample comprising a search term and n candidate documents corresponding to the search term, where n is an integer greater than 1; inputting the training sample into the ranking model, to obtain n first correlation degrees output by the ranking model, wherein each first correlation degree represents a correlation between a candidate document and the search term; inputting the training sample into the semantic retrieval model, to obtain n second correlation degrees output by the semantic retrieval model, wherein each second correlation degree represents a correlation between a candidate document and the search term; and training the semantic retrieval model and the ranking model jointly based on the n first correlation degrees and the n second correlation degrees.</p><p id="p-0008" num="0007">According to a second aspect of the disclosure, an electronic device is provided. The electronic device includes: at least one processor and a memory communicatively coupled to the at least one processor. The memory stores instructions executable by the at least one processor, and when the instructions are executed by the at least one processor, the method according to the first aspect of the disclosure is implemented.</p><p id="p-0009" num="0008">According to a third aspect of the disclosure, a non-transitory computer-readable storage medium having computer instructions stored thereon is provided. The computer instructions are configured to cause a computer to implement the method according to the first aspect of the disclosure.</p><p id="p-0010" num="0009">It should be understood that the content described in this section is not intended to identify key or important features of the embodiments of the disclosure, nor is it intended to limit the scope of the disclosure. Additional features of the disclosure will be easily understood based on the following description.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010">The drawings are used to better understand the solution and do not constitute a limitation to the disclosure, in which:</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of a method for training a semantic retrieval network according to an embodiment of the disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of joint training of a semantic retrieval model and a ranking model according to an embodiment of the disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of obtaining a training sample according to an embodiment of the disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is another flowchart of obtaining a training sample according to an embodiment of the disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of an apparatus for training a semantic retrieval network according to an embodiment of the disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of an example electronic device used to implement the embodiments of the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0018" num="0017">The following describes the exemplary embodiments of the disclosure with reference to the accompanying drawings, which includes various details of the embodiments of the disclosure to facilitate understanding, which shall be considered merely exemplary. Therefore, those of ordinary skill in the art should recognize that various changes and modifications can be made to the embodiments described herein without departing from the scope and spirit of the disclosure. For clarity and conciseness, descriptions of well-known functions and structures are omitted in the following description.</p><p id="p-0019" num="0018">In the technical solution of the disclosure, the acquisition, storage and application of the personal information of the user involved are in compliance with relevant laws and regulations, and do not violate public order and good customs. The involved personal information of the user is obtained, stored and applied with the user's consent.</p><p id="p-0020" num="0019">In the related art, an information retrieval system generally has a recalling (also referred to as candidate generation) stage and a ranking stage. In the recalling stage, K candidates are obtained from a large-scale candidate database based on a semantic retrieval model. In the ranking stage, a more complex model is used to resort the K candidates, and return a sorted answer list at last. The semantic retrieval model and the ranking model are interdependent. The ranking model needs to adapt to a distribution of candidates produced by the semantic retrieval model, and the semantic retrieval model needs to continuously learn from the ranking model to enhance its own ability.</p><p id="p-0021" num="0020">However, the retrieval system in the related art usually trains the semantic retrieval model and the ranking model independently. Due to the inseparable relationship between the two models, multiple iterations are required in the training process, so that the training efficiency is low, the training cost is high, and the effect after training is poor.</p><p id="p-0022" num="0021">Based on the above problems, the disclosure provides a method for training a semantic retrieval network, an apparatus for training a semantic retrieval network, an electronic device and a storage medium. In order to facilitate the understanding of the technical solution, the method for training a semantic retrieval network according to the disclosure will be introduced firstly.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of a method for training a semantic retrieval network according to an embodiment of the disclosure. It should be noted that, in the embodiments of the disclosure, the semantic retrieval network is a neural network that can be used to find information such as documents or fragments of text related to a search term in a massive amount of text based on the search term, and can be used in the information retrieval system to realize a retrieval function.</p><p id="p-0024" num="0023">In the embodiments of the disclosure, the semantic retrieval network includes a semantic retrieval model for the recalling stage and a ranking model for the ranking stage. It should be noted that the method for training a semantic retrieval network in the embodiments of the disclosure can be applied to the apparatus for training a semantic retrieval network in the embodiments of the disclosure, and the apparatus can be configured in an electronic device. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the method may include the following steps.</p><p id="p-0025" num="0024">In step <b>101</b>, a training sample including a search term and n candidate documents corresponding to the search term are obtained, where n is an integer greater than 1.</p><p id="p-0026" num="0025">In the embodiments of the disclosure, the search term in the training sample refers to a keyword used by a user for retrieval, and the candidate documents corresponding to the search term refer to the documents related to the search term. The candidate documents are related to specific application fields. For example, in a web page search engine, the candidate documents may be web pages related to the search term crawled by a search engine based on the search term. In a question and answer platform, the candidate documents may be question and answer information related to the search term in a question and answer knowledge base. In a book search engine, the candidate documents may be a book text related to the search term in a database.</p><p id="p-0027" num="0026">In addition, in the training sample, each search term corresponds to n candidate documents and the n candidate documents may include both positive and negative example candidate documents. The positive example candidate document refers to a candidate document completely related to the search term, and the negative example candidate document refers to a candidate document not completely related to the search term.</p><p id="p-0028" num="0027">For example, a retrieval log of the information retrieval system within a period of time can be obtained, keywords in the retrieval log are used as the search terms in the training sample, and the documents in a retrieval result corresponding to each keyword in the retrieval log are used as candidate documents of the keyword. In the retrieval log, according to the retrieval result of each search term, the document clicked by the user is labeled as a positive example candidate document, and the document that is not clicked by the user in the retrieval result is regarded as a negative example candidate document.</p><p id="p-0029" num="0028">For example, the obtained search term can be input into an initial semantic retrieval model, and the documents and correlation scores related to the search term are retrieved from a document library through the initial semantic retrieval model, and the retrieved related documents are ranked according to the corresponding correlation scores in a descending order. The ranked first document or the first few documents are regarded as positive example candidate documents, and the remaining documents in the first N documents are regarded as negative example candidate documents. It should be noted that the initial semantic retrieval model refers to a semantic retrieval model trained by an existing training mode.</p><p id="p-0030" num="0029">In step <b>102</b>, the training sample is input into the ranking model, to obtain n first correlation degrees output by the ranking model. Each first correlation degree represents a correlation between a candidate document and the search term.</p><p id="p-0031" num="0030">In the related art, the semantic retrieval model for the recalling stage is usually trained by means of in-batch negative sampling, while the ranking model is trained by a single-point-based training method. The in-batch negative sampling means that, for a certain group of samples in the same batch, positive samples in the remaining samples other than the group of samples in the batch are regarded as negative samples for the group of samples. The single-point-based method means that an input space sample is a feature vector composed of a single document and a search term, and an output space is a correlation degree between the single document and the search term. Due to different training methods, the two models have different training data construction methods, different input forms, and different output scoring distributions, thus joint training cannot be performed.</p><p id="p-0032" num="0031">In order to ensure the joint training of the semantic retrieval model and the ranking model, it is necessary to unify the data input form and data construction method of the semantic retrieval model and the ranking model. As an implementation, in some embodiments of the disclosure, a training target of the ranking model may be in the form of listwise.</p><p id="p-0033" num="0032">In some embodiments of the disclosure, each training sample includes a search term and n candidate documents corresponding to the search term. The training sample is input into the ranking model, to obtain n first correlation degrees output by the ranking model. It can be understood that for each training samples, the training sample is input into the ranking model, and through model calculation, the first correlation degree between each candidate document and the search term in the training sample is obtained.</p><p id="p-0034" num="0033">For example, if a training sample in the training samples is (search term A, candidate document <b>1</b>, candidate document <b>2</b>, candidate document <b>3</b>, candidate document <b>4</b>), then after inputting the training sample into the ranking model, four first correlation degrees output by the ranking model can be obtained, i.e., the first correlation degree a, the first correlation degree b, the first correlation degree c, and the first correlation degree d. The first correlation degree a is used to indicate a correlation between the candidate document <b>1</b> and the search term A, the first correlation degree b is used to indicate a correlation between the candidate document <b>2</b> and the search term A, the first correlation degree c is used to represent a correlation between the candidate document <b>3</b> and the search term A, and the first correlation degree d is used to represent a correlation between the candidate document <b>4</b> and the search term A.</p><p id="p-0035" num="0034">It should be noted that the ranking model adopted in some embodiments of the disclosure may be an untrained ranking model, or an initial ranking model that has been trained using an existing training method, which is not limited.</p><p id="p-0036" num="0035">In some embodiments of the disclosure, the ranking model may be an interactive model, or may be other models used in the ranking stage, which is not limited in the disclosure.</p><p id="p-0037" num="0036">In step <b>103</b>, the training sample is input into the semantic retrieval model, to obtain n second correlation degrees output by the semantic retrieval model are obtained. Each second correlation degree represents a correlation between a candidate document and the search term.</p><p id="p-0038" num="0037">For each training sample, the training sample is input into the semantic retrieval model, and through model calculation, the second correlation degree between each candidate document and the search term in the training sample is obtained. The corresponding relationship of the second correlation degree is consistent with the corresponding relationship of the first correlation degree in step <b>102</b>.</p><p id="p-0039" num="0038">In some embodiments of the disclosure, the semantic retrieval model may be a dual-tower retrieval model, or may be other models used in the recalling stage, which is not limited in this disclosure.</p><p id="p-0040" num="0039">In step <b>104</b>, the semantic retrieval model and the ranking model are trained jointly according to the n first correlation degrees and the n second correlation degrees.</p><p id="p-0041" num="0040">That is, a loss value can be calculated according to the first correlation degrees and the second correlation degrees, and the semantic retrieval model and the ranking model can be jointly trained according to the loss value.</p><p id="p-0042" num="0041">It can be understood that the correlation between the positive example candidate document and the search term in the training sample is greater than the correlation between the negative example candidate document and the search term. As an implementation, according to each difference between (a) the first correlation degree between the positive example candidate document and the search term, and (b) the first correlation degree between the negative example candidate document and the search term, differences between the n first correlations and differences between the n second correlation degrees, the loss value is calculated. According to the loss value, the semantic retrieval model and the ranking model are jointly trained.</p><p id="p-0043" num="0042">For example, the loss value of the ranking model is calculated according to each difference between (a) the first correlation degrees between the positive example candidate document and the search term, and (b) the first correlation degrees between the negative example candidate document and the search term. The loss value of the semantic retrieval model is calculated according to each difference between (a) the second correlation degree between the positive example candidate document and the search term, and (b) the second correlation degree between the negative example candidate document and the search term. Weighted calculation is performed on the loss value of the ranking model and the loss value of the semantic retrieval model to obtain a joint loss value. The semantic retrieval model and the ranking model are jointly trained according to the joint loss value.</p><p id="p-0044" num="0043">According to the method for training a semantic retrieval network of the disclosure, the training sample is input into the ranking model and the semantic retrieval model respectively, the first correlation degrees output by the ranking model and the second correlation degrees output by the semantic retrieval model are obtained. The semantic retrieval model and the ranking model are jointly trained according to the first correlation degrees and the second correlation degrees, which can not only improve the efficiency of model training, but also improve the effect after training and save training costs.</p><p id="p-0045" num="0044">To further introduce the implementation of the joint training process, the disclosure provides another embodiment.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of joint training of a semantic retrieval model and a ranking model according to an embodiment of the disclosure. In order to make the model better learn the difference between the positive example candidate document and the negative example candidate document, it is set that the correlation degree between the positive example candidate document and the search term output by the model is greater than the correlation degree between the negative example candidate document and the search term. In some embodiments of the disclosure, the n candidate documents corresponding to the search term in the training sample include one positive example candidate document and n&#x2212;1 negative example candidate documents. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, on the basis of the above embodiments, the implementation process of jointly training the semantic retrieval model and the ranking model according to the n first correlation degrees and the n second correlation degrees may include the following steps.</p><p id="p-0047" num="0046">In step <b>201</b>, a first loss value of the ranking model is calculated according to the first correlation degree between the search term and the positive example candidate document, and the first correlation degree between the search term and each negative example candidate document.</p><p id="p-0048" num="0047">It can be understood that, for each training sample, an expected result is that the correlation degree between the positive example candidate document and the search term output by the model is greater than the correlation degree between the negative example candidate document and the search term.</p><p id="p-0049" num="0048">In order to make sure that the first correlation degree between the positive example candidate document and the search term output by the ranking model is greater than the first correlation degree between the negative example candidate document and the search term in the joint training stage, the first loss value of the ranking model can be calculated according to the first correlation degree between the search term and the positive example candidate document and the first correlation degree between the search term and each of the negative example candidate documents.</p><p id="p-0050" num="0049">For example, the first loss value of the ranking model is calculated according to formula (1):</p><p id="p-0051" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>L</mi>      <mi>sup</mi>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <mo>-</mo>       <mfrac>        <mn>1</mn>        <mi>N</mi>       </mfrac>      </mrow>      <mo>&#x2062;</mo>      <msub>       <mi>&#x3a3;</mi>       <mrow>        <mrow>         <mi>q</mi>         <mo>&#x2208;</mo>         <mi>Q</mi>        </mrow>        <mo>,</mo>        <msup>         <mi>p</mi>         <mo>+</mo>        </msup>       </mrow>      </msub>      <mo>&#x2062;</mo>      <mi>log</mi>      <mo>&#x2062;</mo>      <mfrac>       <msup>        <mi>e</mi>        <mrow>         <msub>          <mi>s</mi>          <mrow>           <mi>c</mi>           <mo>&#x2062;</mo>           <mi>e</mi>          </mrow>         </msub>         <mo>(</mo>         <mrow>          <mi>q</mi>          <mo>,</mo>          <msup>           <mi>p</mi>           <mo>+</mo>          </msup>         </mrow>         <mo>)</mo>        </mrow>       </msup>       <mrow>        <msup>         <mi>e</mi>         <mrow>          <msub>           <mi>s</mi>           <mrow>            <mi>c</mi>            <mo>&#x2062;</mo>            <mi>e</mi>           </mrow>          </msub>          <mo>(</mo>          <mrow>           <mi>q</mi>           <mo>,</mo>           <msup>            <mi>p</mi>            <mo>+</mo>           </msup>          </mrow>          <mo>)</mo>         </mrow>        </msup>        <mo>+</mo>        <msub>         <mi>&#x3a3;</mi>         <mi>p</mi>        </msub>        <mo>-</mo>        <msup>         <mi>e</mi>         <mrow>          <msub>           <mi>s</mi>           <mrow>            <mi>c</mi>            <mo>&#x2062;</mo>            <mi>e</mi>           </mrow>          </msub>          <mo>(</mo>          <mrow>           <mi>q</mi>           <mo>,</mo>           <msup>            <mi>p</mi>            <mo>-</mo>           </msup>          </mrow>          <mo>)</mo>         </mrow>        </msup>       </mrow>      </mfrac>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0052" num="0050">L<sub>sup </sub>is the first loss value of the ranking model, N is the number of samples, s<sub>ce</sub>(q, p<sup>+</sup>) is the first correlation degree between the search term and the positive example candidate document output by the ranking model, s<sub>ce</sub>(q, p<sup>&#x2212;</sup>) is the first correlation degree between the search term and the negative example candidate document output by the ranking model, Q refers to the training sample, and q refers to the search term.</p><p id="p-0053" num="0051">In step <b>202</b>, a second loss value of the semantic retrieval model is calculated according to the n first correlation degrees and the n second correlation degrees.</p><p id="p-0054" num="0052">It can be understood that the semantic retrieval model and the ranking model are interdependent. The ranking model needs to adapt to a candidate distribution generated by the semantic retrieval model, and the semantic retrieval model needs to continuously learn from the ranking model to enhance its own capabilities. In some embodiments of the disclosure, a soft label may be used to correlate the correlation scores of the two models, to avoid wrong accumulation of labels for low-confidence data. That is, the first correlation degree output by the ranking model is used as the soft label of the second correlation degree output by the semantic retrieval model, so that the second loss value of the semantic retrieval model is calculated according to the difference of the first correlation degree and the second correlation degree between the corresponding candidate document and the search term.</p><p id="p-0055" num="0053">For example, the second loss value is obtained by performing Kullback-Leibler (KL) dispersion loss calculation according to the n first correlation degrees and the n second correlation degrees. Its implementation can include: for each training sample, normalizing the first correlation degree between each candidate document and the search term output by the ranking model, and simultaneously normalizing the second correlation degree between each candidate document and the search term output by the semantic retrieval model; and performing the KL dispersion loss calculation according to the normalized correlation degrees. The calculation method of the normalization processing can be implemented according to formula (2) and formula (3), and the KL dispersion loss calculation can be expressed by formula (4):</p><p id="p-0056" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mover accent="true">        <mi>s</mi>        <mi>&#x2dc;</mi>       </mover>       <mrow>        <mi>c</mi>        <mo>&#x2062;</mo>        <mi>e</mi>       </mrow>      </msub>      <mo>(</mo>      <mrow>       <mi>q</mi>       <mo>,</mo>       <mi>p</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mfrac>      <msup>       <mi>e</mi>       <mrow>        <msub>         <mi>s</mi>         <mrow>          <mi>c</mi>          <mo>&#x2062;</mo>          <mi>e</mi>         </mrow>        </msub>        <mo>(</mo>        <mrow>         <mi>q</mi>         <mo>,</mo>         <mi>p</mi>        </mrow>        <mo>)</mo>       </mrow>      </msup>      <mrow>       <msub>        <mi>&#x3a3;</mi>        <mrow>         <msup>          <mi>p</mi>          <mo>&#x2032;</mo>         </msup>         <mo>&#x2208;</mo>         <msub>          <mi>P</mi>          <mi>q</mi>         </msub>        </mrow>       </msub>       <mo>&#x2062;</mo>       <msup>        <mi>e</mi>        <mrow>         <msub>          <mi>s</mi>          <mrow>           <mi>c</mi>           <mo>&#x2062;</mo>           <mi>e</mi>          </mrow>         </msub>         <mo>(</mo>         <mrow>          <mi>q</mi>          <mo>,</mo>          <msup>           <mi>p</mi>           <mo>&#x2032;</mo>          </msup>         </mrow>         <mo>)</mo>        </mrow>       </msup>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mover accent="true">        <mi>s</mi>        <mi>&#x2dc;</mi>       </mover>       <mrow>        <mi>d</mi>        <mo>&#x2062;</mo>        <mi>e</mi>       </mrow>      </msub>      <mo>(</mo>      <mrow>       <mi>q</mi>       <mo>,</mo>       <mi>p</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mfrac>      <msup>       <mi>e</mi>       <mrow>        <msub>         <mi>s</mi>         <mrow>          <mi>d</mi>          <mo>&#x2062;</mo>          <mi>e</mi>         </mrow>        </msub>        <mo>(</mo>        <mrow>         <mi>q</mi>         <mo>,</mo>         <mi>p</mi>        </mrow>        <mo>)</mo>       </mrow>      </msup>      <mrow>       <msub>        <mi>&#x3a3;</mi>        <mrow>         <msup>          <mi>p</mi>          <mo>&#x2032;</mo>         </msup>         <mo>&#x2208;</mo>         <msub>          <mi>P</mi>          <mi>q</mi>         </msub>        </mrow>       </msub>       <mo>&#x2062;</mo>       <msup>        <mi>e</mi>        <mrow>         <msub>          <mi>s</mi>          <mi>de</mi>         </msub>         <mo>(</mo>         <mrow>          <mi>q</mi>          <mo>,</mo>          <msup>           <mi>p</mi>           <mo>&#x2032;</mo>          </msup>         </mrow>         <mo>)</mo>        </mrow>       </msup>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0057" num="0054">s<sub>ce</sub>(q, p) is the first correlation degree between the candidate document p and the search term q output by the ranking model, s<sub>de</sub>(q, p) is the second correlation degree between the candidate document p and the search term q output by the semantic retrieval model, {tilde over (s)}<sub>ce</sub>(q, p) is the normalized value of the first correlation degree between the candidate document p and the search term q, {tilde over (s)}<sub>de</sub>(q, p) is the normalized value of the second correlation degree between the candidate document p and the search term q , P<sub>q </sub>refers to the n candidate samples corresponding to q.</p><p id="p-0058" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>L</mi>      <mrow>       <mi>K</mi>       <mo>&#x2062;</mo>       <mi>L</mi>      </mrow>     </msub>     <mo>=</mo>     <mrow>      <msub>       <mi>&#x3a3;</mi>       <mrow>        <mrow>         <mi>q</mi>         <mo>&#x2208;</mo>         <mi>Q</mi>        </mrow>        <mo>,</mo>        <mrow>         <mi>p</mi>         <mo>&#x2208;</mo>         <msub>          <mi>P</mi>          <mi>q</mi>         </msub>        </mrow>       </mrow>      </msub>      <mo>&#x2062;</mo>      <mrow>       <mrow>        <msub>         <mover accent="true">          <mi>s</mi>          <mi>&#x2dc;</mi>         </mover>         <mrow>          <mi>d</mi>          <mo>&#x2062;</mo>          <mi>e</mi>         </mrow>        </msub>        <mo>(</mo>        <mrow>         <mi>q</mi>         <mo>,</mo>         <mi>p</mi>        </mrow>        <mo>)</mo>       </mrow>       <mo>&#xb7;</mo>       <mi>log</mi>      </mrow>      <mo>&#x2062;</mo>      <mfrac>       <mrow>        <msub>         <mover>          <mi>s</mi>          <mo>~</mo>         </mover>         <mrow>          <mi>d</mi>          <mo>&#x2062;</mo>          <mi>e</mi>         </mrow>        </msub>        <mo>(</mo>        <mrow>         <mi>q</mi>         <mo>,</mo>         <mi>p</mi>        </mrow>        <mo>)</mo>       </mrow>       <mrow>        <msub>         <mover>          <mi>s</mi>          <mo>~</mo>         </mover>         <mrow>          <mi>c</mi>          <mo>&#x2062;</mo>          <mi>e</mi>         </mrow>        </msub>        <mo>(</mo>        <mrow>         <mi>q</mi>         <mo>,</mo>         <mi>p</mi>        </mrow>        <mo>)</mo>       </mrow>      </mfrac>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0059" num="0055">{tilde over (s)}<sub>ce</sub>(q, p) is the normalized value of the first correlation degree between the candidate document p and the search term q, {tilde over (s)}<sub>de</sub>(q, p) is the normalized value of the second correlation degree between the candidate document p and the search term q, P<sub>q </sub>refers to the n candidate samples corresponding to the search term q, and Q refers to the training sample.</p><p id="p-0060" num="0056">In step <b>203</b>, the semantic retrieval model and the ranking model are trained jointly according to the first loss value and the second loss value.</p><p id="p-0061" num="0057">It can be understood that the first loss value is used to represent a difference between the first correlation degree output by the ranking model and a real sample value, and the second loss value is used to represent a difference between the second correlation degree output by the semantic retrieval model and the first correlation degree output by the ranking model. In this way, according to the first loss value and the second loss value, the semantic retrieval model and the ranking model can be jointly trained. On the one hand, supervised learning can be performed on the ranking model, so that the ability of the ranking model can be continuously improved, and on the other hand, it is also possible to further narrow the distribution between the ranking model and the semantic retrieval model. That is, not only can the semantic retrieval model learn the ability of the ranking model, but also the ranking model can be adapted to the distribution of the semantic retrieval model, thereby improving the efficiency of model training and the effect of model training.</p><p id="p-0062" num="0058">As an implementation, the joint loss value can be calculated according to the first loss value and the second loss value. The semantic retrieval model and the ranking model can be jointly trained according to the joint loss value. The joint loss value can be calculated by summing the first loss value and the second loss value, or the first loss value and the second loss value can be weighted and summed to obtain the joint loss value, and other methods may also be used to calculate the joint loss value according to the actual application, which is not limited in the disclosure.</p><p id="p-0063" num="0059">According to the method for training a semantic retrieval network of the embodiments of the disclosure, the first loss value of the ranking model is calculated according to the first correlation degree between the search term and the positive example candidate document, and the first correlation degree between the search term and each negative example candidate document. The second loss value of the semantic retrieval model is calculated according to the first correlation degrees and the second correlation degrees, so that the semantic retrieval model and the ranking model can be jointly trained according to the first loss value and the second loss value. On the one hand, the technical solution can carry out supervised learning of the ranking model, so that the ability of the ranking model can be continuously improved, and on the other hand, it can further narrow the distribution between the ranking model and the semantic retrieval model. That is, not only can the semantic retrieval model learn the ability of the ranking model, but also the ranking model can be adapted to the distribution of the semantic retrieval model, thereby improving the efficiency of model training and the effect of model training.</p><p id="p-0064" num="0060">The disclosure provides another embodiment for the acquisition of the training sample.</p><p id="p-0065" num="0061"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of obtaining a training sample according to an embodiment of the disclosure. As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the way to obtain training samples may include the following steps.</p><p id="p-0066" num="0062">In step <b>301</b>, the search term and the positive example candidate document labeled by the search term are obtained.</p><p id="p-0067" num="0063">In some embodiments of the disclosure, in order to enable the model to learn that the correlation degree between the positive example candidate document and the search term is greater than the correlation degree between the negative example candidate document and the search term, n candidate documents in each training sample can include one positive example candidate document and n&#x2212;1 negative example candidate documents. The positive example candidate documents are usually associated with the search term in the form of annotations.</p><p id="p-0068" num="0064">In the embodiment of the disclosure, the search term in the training sample refers to the keyword used by the user for retrieval, which can be obtained in the retrieval log of the information retrieval system, or in a constructed search term database, and other methods may also be used to obtain the search term according to actual application scenarios. The positive example candidate document refers to the document having the highest correlation with the search term, generally, it can be manually annotated to correspond to the search term, or it can be annotated through other methods.</p><p id="p-0069" num="0065">In step <b>302</b>, documents related to the search term are obtained from a document library according to the initial semantic retrieval model.</p><p id="p-0070" num="0066">It should be noted that, in some embodiments of the disclosure, the initial semantic retrieval model refers to a semantic retrieval model trained by an existing training method. It can be understood that the initial semantic retrieval model has a capability of retrieving related documents in the document library according to the search term, so the documents related to the search term can be obtained from the document library through the initial semantic retrieval model.</p><p id="p-0071" num="0067">For example, the search terms obtained in step <b>301</b> can be input into the initial semantic retrieval model respectively, and the related documents corresponding to each search term are output through calculation of the initial semantic retrieval model, and the output related documents are ranked according to the correlation in a descending order. In order to reduce the amount of computation, an upper limit N of the number of documents related to the search term can be set for each search term, so that the initial semantic retrieval model outputs the top N related documents related to the search term.</p><p id="p-0072" num="0068">In step <b>303</b>, in the related documents, n&#x2212;1 documents are randomly selected as negative example candidate documents corresponding to the search term, where n is an integer greater than 1.</p><p id="p-0073" num="0069">It can be understood that the negative example candidate documents are documents related to the search term, so n&#x2212;1 documents can be selected from the obtained related documents as the negative example candidate documents corresponding to the search term.</p><p id="p-0074" num="0070">In order to improve the training effect of the model, in the embodiment of the disclosure, for each search term, n&#x2212;1 documents are selected by random sampling from the related documents as the negative example candidate documents of the search term.</p><p id="p-0075" num="0071">In step <b>304</b>, the search term, and the positive and negative example candidate documents corresponding to the search term are determined as the training sample.</p><p id="p-0076" num="0072">According to the method for training a semantic retrieval network according to the embodiment of the disclosure, each training sample includes a search term, and candidate documents corresponding to the search term, and the candidate documents include one positive example candidate document and multiple negative example candidate documents, thus a unified data structure is constructed for the joint training of the semantic retrieval model and the ranking model, which provides a data basis for the joint training.</p><p id="p-0077" num="0073">Since the acquisition cost of the labeled data is high, in order to further reduce the training cost, the disclosure provides another embodiment for this problem.</p><p id="p-0078" num="0074"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is another flowchart of obtaining a training sample according to an embodiment of the disclosure. As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the mode of obtaining the training sample can include the following steps.</p><p id="p-0079" num="0075">In step <b>401</b>, the search term is obtained, and the documents related to the search term are obtained from the document library according to the initial semantic retrieval model.</p><p id="p-0080" num="0076">In the embodiment of the disclosure, the search term in the training sample refers to the keyword used by the user for retrieval, which can be obtained in the retrieval log of the information retrieval system, or in the constructed search term database, or obtained through other methods according to actual application scenes.</p><p id="p-0081" num="0077">For example, the obtained search terms can be input into the initial semantic retrieval model respectively, and through the calculation of the initial semantic retrieval model, the related documents corresponding to each search term and the correlation degree between each related document and the search term are output, and the output related documents are sorted in a descending order according to the correlation. In order to reduce the amount of calculation, for each search term, the upper limit N of the number of related documents for the search term can be set, such as N=1000, so that the first 1000 related documents related to the search term output by the initial semantic retrieval model can be obtained.</p><p id="p-0082" num="0078">In step <b>402</b>, a third correlation degree between each document and the search term is obtained according to the initial ranking model.</p><p id="p-0083" num="0079">It should be noted that, in some embodiments of the disclosure, the initial ranking model refers to a ranking model trained by an existing training method. It can be understood that the ranking model has an ability to calculate the correlation degree between each related document and the search term according to the search term, so the third correlation degree between each document and the search term can be calculated through the initial ranking model.</p><p id="p-0084" num="0080">For example, in order to reduce the computing cost, a part of related documents may be selected and input to the initial ranking model to calculate the third correlation degrees. The implementation method can be as follows: sorting the related documents according to the corresponding correlation degree with respect to the search term, where the correlation degree refers to the correlation degree output by the initial semantic retrieval model, and the larger the correlation degree between the document and the search term, the prior the document is arranged in the ranking list. According to the ranking, the first 50 documents are selected and input to the initial ranking model, and the third correlation degree between each document in the 50 documents and the search term is obtained.</p><p id="p-0085" num="0081">In step <b>403</b>, a maximum correlation degree in the third correlation degrees is determined as a target third correlation degree.</p><p id="p-0086" num="0082">In step <b>404</b>, in response to the target third correlation degree being greater than a first threshold, the document corresponding to the target third correlation degree is determined as a positive example candidate document corresponding to the search term.</p><p id="p-0087" num="0083">In order to reduce the cost of acquiring the labeled data, the embodiment of the disclosure may determine the positive example candidate document according to the third correlation degrees output by the initial ranking model. At the same time, in order to ensure a quality of the positive example candidate document, the first threshold can be preset, and only when the target third correlation degree is greater than the first threshold, the document corresponding to the target third correlation degree can be determined as the positive example candidate document corresponding to the search term.</p><p id="p-0088" num="0084">In step <b>405</b>, n&#x2212;1 documents are determined from the related documents as n&#x2212;1 negative example candidate documents corresponding to the search term, the third correlation degree between each of the n&#x2212;1 documents and the search term is less than a second threshold.</p><p id="p-0089" num="0085">It can be understood that, in order to reflect the difference between the positive example candidate document and the negative example candidate document, and improve the effect of model training, in the embodiments of the disclosure, a second threshold is determined as a standard for determining the negative example candidate documents. Generally, the second threshold is smaller than the first threshold.</p><p id="p-0090" num="0086">For example, the related documents obtained in step <b>401</b> are sorted according to the correlation degrees with the search term, and the correlation degree refers to the correlation degree output by the initial semantic retrieval model. The larger the correlation degree between the document and the search term, the prior the document is arranged in the ranking list. According to the ranking, the correlation degree between each document and the search term is compared with the second threshold from the front to the back, until the correlation degree between a document and the search term is found to be smaller than the second threshold, the document and all documents that are ranked after the file are determined as target selected documents. In the target selected documents, n&#x2212;1 documents are selected from front to back as the negative example candidate documents of the search term according to the ranking.</p><p id="p-0091" num="0087">For example, if the number of documents in the target selected documents is less than n&#x2212;1, other documents can be randomly selected (for example, the number of the target selected documents is 50, while n=151, then the remaining 100 documents can be randomly selected) from the related documents that are not input to the initial ranking model. For example, if there are 1000 related documents output by the initial semantic retrieval model, the first 50 documents are input into the initial ranking model according to the order of correlation degrees, and the other documents can be randomly selected from the remaining 950 documents.</p><p id="p-0092" num="0088">In step <b>406</b>, the search term and the positive example candidate document and the n&#x2212;1 negative example candidate documents corresponding to the search term are determined as the training sample.</p><p id="p-0093" num="0089">It can be understood that the method of obtaining the training sample in the embodiments of the disclosure can be applied to unlabeled data, so that unsupervised training for the semantic retrieval model and the ranking model can be realized, and the training cost can be reduced.</p><p id="p-0094" num="0090">In addition, in practical applications, the method of obtaining the training sample in the embodiments of the disclosure can also be combined with the method of obtaining the training sample in the above-mentioned embodiments, that is, the training sample may include both training sample obtained from labeled data and training sample obtained from unlabeled data. In this way, the training sample obtained through unlabeled data can be used as a supplement to the training sample obtained through labeled data to achieve data enhancement, and the training sample is constructed as weakly-supervised training data, thereby further improving the effect of model training.</p><p id="p-0095" num="0091">According to the method for training a semantic retrieval network of the embodiments of the disclosure, based on the initial semantic retrieval model and the initial ranking model, it is possible to obtain the training sample according to the unlabeled data. That is, unsupervised joint training for the semantic retrieval model and the ranking model can be implemented, which can further reduce the training cost and solve the current problem of difficulty in obtaining the labeled data. In addition, the training sample can also be obtained from the unlabeled data for data enhancement, and the training sample obtained from the labeled data can be mixed with the training sample obtained from the unlabeled data as the training sample for joint training. In this way, the training data can be expanded to a greater extent and the training effect of the model can be improved.</p><p id="p-0096" num="0092">In order to realize the above embodiments, the disclosure provides an apparatus for training a semantic retrieval network.</p><p id="p-0097" num="0093"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of an apparatus for training a semantic retrieval network according to an embodiment of the disclosure. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the apparatus may include: a first obtaining module <b>510</b>, a second obtaining module <b>520</b>, a third obtaining module <b>530</b> and a training module <b>540</b>.</p><p id="p-0098" num="0094">The first obtaining module <b>510</b> is configured to obtain a training sample comprising a search term and n candidate documents corresponding to the search term, where n is an integer greater than 1.</p><p id="p-0099" num="0095">The second obtaining module <b>520</b> is configured to input the training sample into the ranking model, to obtain n first correlation degrees output by the ranking model, wherein each first correlation degree represents a correlation between a candidate document and the search term.</p><p id="p-0100" num="0096">The third obtaining module <b>530</b> is configured to input the training sample into the semantic retrieval model, to obtain n second correlation degrees output by the semantic retrieval model, wherein each second correlation degree represents a correlation between a candidate document and the search term.</p><p id="p-0101" num="0097">The training module <b>540</b> is configured to train the semantic retrieval model and the ranking model jointly based on the n first correlation degrees and the n second correlation degrees.</p><p id="p-0102" num="0098">In some embodiments of the disclosure, the n candidate documents comprise a positive example candidate document and n&#x2212;1 negative example candidate documents. The training module <b>540</b> includes: a first calculating unit <b>541</b>, a second calculating unit <b>542</b> and a training unit <b>543</b>.</p><p id="p-0103" num="0099">The first calculating unit <b>541</b> is configured to calculate a first loss value of the ranking model based on the first correlation degree between the search term and the positive example candidate document, and the first correlation degree between the search term and each negative example candidate document.</p><p id="p-0104" num="0100">The second calculating unit <b>542</b> is configured to calculate a second loss value of the semantic retrieval model based on the n first correlation degrees and the n second correlation degrees.</p><p id="p-0105" num="0101">The training unit <b>543</b> is configured to train the semantic retrieval model and the ranking model jointly based on the first loss value and the second loss value.</p><p id="p-0106" num="0102">In some embodiments of the disclosure, the training unit <b>543</b> is further configured to: calculate a joint loss value based on the first loss value and the second loss value; and train the semantic retrieval model and the ranking model jointly according to the joint loss value.</p><p id="p-0107" num="0103">In some embodiments of the disclosure, the second calculating unit <b>541</b> is further configured to: obtain the second loss value by performing Kullback-Leibler (KL) dispersion loss calculation based on the n first correlation degrees and the n second correlation degrees.</p><p id="p-0108" num="0104">In some embodiments of the disclosure, the first obtaining module <b>510</b> is further configured to: obtain the search term and a positive example candidate document labeled by the search term; obtain documents related to the search term from a document library based on an initial semantic retrieval model; select n&#x2212;1 documents randomly from the documents related to the search term as n&#x2212;1 negative example candidate documents corresponding to the search term, where n is an integer greater than <b>1</b>; and determine the search term, the positive example candidate document corresponding to the search term and the n&#x2212;1 negative example candidate documents corresponding to the search term as the training sample.</p><p id="p-0109" num="0105">In some embodiments of the disclosure, the first obtaining module <b>510</b> is further configured to: obtain the search term, and obtain documents related to the search term from a document library based on an initial semantic retrieval model; obtain a third correlation degree between each of the documents related to the search term and the search term based on an initial ranking model; determine a maximum correlation degree in the third correlation degrees as a target third correlation degree; in response to the target third correlation degree being greater than a first threshold, determine the document corresponding to the target third correlation degree as a positive example candidate document corresponding to the search term; select n&#x2212;1 documents from the documents related to the search term as n&#x2212;1 negative example candidate documents corresponding to the search term, wherein the third correlation degree between each of the n&#x2212;1 documents and the search term is less than a second threshold; and determine the search term and the positive example candidate document and the n&#x2212;1 negative example candidate documents corresponding to the search term as the training sample.</p><p id="p-0110" num="0106">With the apparatus for training a semantic retrieval network according to the embodiment of the disclosure, the training sample is input into the ranking model and the semantic retrieval model respectively, the first correlation degrees output by the ranking model and the second correlation degrees output by the semantic retrieval model are obtained. The semantic retrieval model and the ranking model are jointly trained according to the first correlation degrees and the second correlation degrees, which can not only improve the efficiency of model training, but also improve the effect after training and save training costs.</p><p id="p-0111" num="0107">According to the embodiments of the disclosure, the disclosure also provides an electronic device, a readable storage medium and a computer program product.</p><p id="p-0112" num="0108"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of an example electronic device <b>600</b> used to implement the embodiments of the disclosure. Electronic devices are intended to represent various forms of digital computers, such as laptop computers, desktop computers, workbenches, personal digital assistants, servers, blade servers, mainframe computers, and other suitable computers. Electronic devices may also represent various forms of mobile devices, such as personal digital processing, cellular phones, smart phones, wearable devices, and other similar computing devices. The components shown here, their connections and relations, and their functions are merely examples, and are not intended to limit the implementation of the disclosure described and/or required herein.</p><p id="p-0113" num="0109">As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the device <b>600</b> includes a computing unit <b>601</b> performing various appropriate actions and processes based on computer programs stored in a read-only memory (ROM) <b>602</b> or computer programs loaded from the storage unit <b>608</b> to a random access memory (RAM) <b>603</b>. In the RAM <b>603</b>, various programs and data required for the operation of the device <b>900</b> are stored. The computing unit <b>601</b>, the ROM <b>602</b>, and the RAM <b>603</b> are connected to each other through a bus <b>604</b>. An input/output (I/O) interface <b>605</b> is also connected to the bus <b>604</b>.</p><p id="p-0114" num="0110">Components in the device <b>600</b> are connected to the I/O interface <b>605</b>, including: an inputting unit <b>606</b>, such as a keyboard, a mouse; an outputting unit <b>607</b>, such as various types of displays, speakers; a storage unit <b>608</b>, such as a disk, an optical disk; and a communication unit <b>609</b>, such as network cards, modems, and wireless communication transceivers. The communication unit <b>609</b> allows the device <b>600</b> to exchange information/data with other devices through a computer network such as the Internet and/or various telecommunication networks.</p><p id="p-0115" num="0111">The computing unit <b>601</b> may be various general-purpose and/or dedicated processing components with processing and computing capabilities. Some examples of computing unit <b>601</b> include, but are not limited to, a central processing unit (CPU), a graphics processing unit (GPU), various dedicated AI computing chips, various computing units that run machine learning model algorithms, and a digital signal processor (DSP), and any appropriate processor, controller and microcontroller. The computing unit <b>601</b> executes the various methods and processes described above, such as the method for training a semantic retrieval network. For example, in some embodiments, the method may be implemented as a computer software program, which is tangibly contained in a machine-readable medium, such as the storage unit <b>608</b>. In some embodiments, part or all of the computer program may be loaded and/or installed on the device <b>600</b> via the ROM <b>602</b> and/or the communication unit <b>609</b>. When the computer program is loaded on the RAM <b>603</b> and executed by the computing unit <b>601</b>, one or more steps of the method described above may be executed. Alternatively, in other embodiments, the computing unit <b>601</b> may be configured to perform the method in any other suitable manner (for example, by means of firmware).</p><p id="p-0116" num="0112">Various implementations of the systems and techniques described above may be implemented by a digital electronic circuit system, an integrated circuit system, Field Programmable Gate Arrays (FPGAs), Application Specific Integrated Circuits (ASICs), Application Specific Standard Products (ASSPs), System on Chip (SOCs), Load programmable logic devices (CPLDs), computer hardware, firmware, software, and/or a combination thereof. These various embodiments may be implemented in one or more computer programs, the one or more computer programs may be executed and/or interpreted on a programmable system including at least one programmable processor, which may be a dedicated or general programmable processor for receiving data and instructions from the storage system, at least one input device and at least one output device, and transmitting the data and instructions to the storage system, the at least one input device and the at least one output device.</p><p id="p-0117" num="0113">The program code configured to implement the method of the disclo sure may be written in any combination of one or more programming languages. These program codes may be provided to the processors or controllers of general-purpose computers, dedicated computers, or other programmable data processing devices, so that the program codes, when executed by the processors or controllers, enable the functions/operations specified in the flowchart and/or block diagram to be implemented. The program code may be executed entirely on the machine, partly executed on the machine, partly executed on the machine and partly executed on the remote machine as an independent software package, or entirely executed on the remote machine or server.</p><p id="p-0118" num="0114">In the context of the disclosure, a machine-readable medium may be a tangible medium that may contain or store a program for use by or in connection with an instruction execution system, apparatus, or device. The machine-readable medium may be a machine-readable signal medium or a machine-readable storage medium. A machine-readable medium may include, but is not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples of machine-readable storage media include electrical connections based on one or more wires, portable computer disks, hard disks, random access memories (RAM), read-only memories (ROM), electrically programmable read-only-memory (EPROM), flash memory, fiber optics, compact disc read-only memories (CD-ROM), optical storage devices, magnetic storage devices, or any suitable combination of the foregoing.</p><p id="p-0119" num="0115">In order to provide interaction with a user, the systems and techniques described herein may be implemented on a computer having a display device (e.g., a Cathode Ray Tube (CRT) or a Liquid Crystal Display (LCD) monitor for displaying information to a user); and a keyboard and pointing device (such as a mouse or trackball) through which the user can provide input to the computer. Other kinds of devices may also be used to provide interaction with the user. For example, the feedback provided to the user may be any form of sensory feedback (e.g., visual feedback, auditory feedback, or haptic feedback), and the input from the user may be received in any form (including acoustic input, voice input, or tactile input).</p><p id="p-0120" num="0116">The systems and technologies described herein can be implemented in a computing system that includes background components (for example, a data server), or a computing system that includes middleware components (for example, an application server), or a computing system that includes front-end components (for example, a user computer with a graphical user interface or a web browser, through which the user can interact with the implementation of the systems and technologies described herein), or include such background components, intermediate computing components, or any combination of front-end components. The components of the system may be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include: local area network (LAN), wide area network (WAN), and the Internet.</p><p id="p-0121" num="0117">The computer system may include a client and a server. The client and server are generally remote from each other and interacting through a communication network. The client-server relation is generated by computer programs running on the respective computers and having a client-server relation with each other. The server can be a cloud server, a server of a distributed system, or a server combined with a block-chain.</p><p id="p-0122" num="0118">It should be understood that the various forms of processes shown above can be used to reorder, add or delete steps. For example, the steps described in the disclosure could be performed in parallel, sequentially, or in a different order, as long as the desired result of the technical solution disclosed in the disclosure is achieved, which is not limited herein.</p><p id="p-0123" num="0119">The above specific embodiments do not constitute a limitation on the protection scope of the disclosure. Those skilled in the art should understand that various modifications, combinations, sub-combinations and substitutions can be made according to design requirements and other factors. Any modification, equivalent replacement and improvement made within the spirit and principle of the disclosure shall be included in the protection scope of the disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004819A1-20230105-M00001.NB"><img id="EMI-M00001" he="8.13mm" wi="76.20mm" file="US20230004819A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US20230004819A1-20230105-M00002.NB"><img id="EMI-M00002" he="16.93mm" wi="76.20mm" file="US20230004819A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230004819A1-20230105-M00003.NB"><img id="EMI-M00003" he="6.01mm" wi="76.20mm" file="US20230004819A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for training a semantic retrieval network, wherein the semantic retrieval network comprises a semantic retrieval model and a ranking model, and the method comprises:<claim-text>obtaining a training sample comprising a search term and n candidate documents corresponding to the search term, where n is an integer greater than 1;</claim-text><claim-text>inputting the training sample into the ranking model, to obtain n first correlation degrees output by the ranking model, wherein each first correlation degree represents a correlation between a candidate document and the search term;</claim-text><claim-text>inputting the training sample into the semantic retrieval model, to obtain n second correlation degrees output by the semantic retrieval model, wherein each second correlation degree represents a correlation between a candidate document and the search term; and</claim-text><claim-text>training the semantic retrieval model and the ranking model jointly based on the n first correlation degrees and the n second correlation degrees.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the n candidate documents comprise a positive example candidate document and n&#x2212;1 negative example candidate documents, and training the semantic retrieval model and the ranking model jointly based on the n first correlation degrees and the n second correlation degrees, comprises:<claim-text>calculating a first loss value of the ranking model based on the first correlation degree between the search term and the positive example candidate document, and the first correlation degree between the search term and each negative example candidate document;</claim-text><claim-text>calculating a second loss value of the semantic retrieval model based on the n first correlation degrees and the n second correlation degrees; and</claim-text><claim-text>training the semantic retrieval model and the ranking model jointly based on the first loss value and the second loss value.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein training the semantic retrieval model and the ranking model jointly based on the first loss value and the second loss value, comprises:<claim-text>calculating a joint loss value based on the first loss value and the second loss value; and</claim-text><claim-text>training the semantic retrieval model and the ranking model jointly according to the joint loss value.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein calculating the second loss value of the semantic retrieval model based on the n first correlation degrees and the n second correlation degrees, comprises:<claim-text>obtaining the second loss value by performing Kullback-Leibler (KL) dispersion loss calculation based on the n first correlation degrees and the n second correlation degrees.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the training sample, comprises:<claim-text>obtaining the search term and a positive example candidate document labeled by the search term;</claim-text><claim-text>obtaining documents related to the search term from a document library based on an initial semantic retrieval model;</claim-text><claim-text>selecting n&#x2212;1 documents randomly from the documents related to the search term as n&#x2212;1 negative example candidate documents corresponding to the search term; and</claim-text><claim-text>determining the search term, the positive example candidate document corresponding to the search term and the n&#x2212;1 negative example candidate documents corresponding to the search term as the training sample.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the training sample, comprises:<claim-text>obtaining the search term, and obtaining documents related to the search term from a document library based on an initial semantic retrieval model;</claim-text><claim-text>obtaining a third correlation degree between each of the documents related to the search term and the search term based on an initial ranking model;</claim-text><claim-text>determining a maximum correlation degree in the third correlation degrees as a target third correlation degree;</claim-text><claim-text>in response to the target third correlation degree being greater than a first threshold, determining the document corresponding to the target third correlation degree as a positive example candidate document corresponding to the search term;</claim-text><claim-text>selecting n&#x2212;1 documents from the documents related to the search term as n&#x2212;1 negative example candidate documents corresponding to the search term, wherein the third correlation degree between each of the n&#x2212;1 documents and the search term is less than a second threshold; and</claim-text><claim-text>determining the search term and the positive example candidate document and the n&#x2212;1 negative example candidate documents corresponding to the search term as the training sample.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. An electronic device, comprising:<claim-text>at least one processor; and</claim-text><claim-text>a memory communicatively coupled to the at least one processor; wherein,</claim-text><claim-text>the memory stores instructions executable by the at least one processor, when the instructions are executed by the at least one processor, the at least one processor is caused to perform the following:</claim-text><claim-text>obtaining a training sample comprising a search term and n candidate documents corresponding to the search term, where n is an integer greater than 1;</claim-text><claim-text>inputting the training sample into a ranking model of a semantic retrieval network, to obtain n first correlation degrees output by the ranking model, wherein each first correlation degree represents a correlation between a candidate document and the search term;</claim-text><claim-text>inputting the training sample into a semantic retrieval model of the semantic retrieval network, to obtain n second correlation degrees output by the semantic retrieval model, wherein each second correlation degree represents a correlation between a candidate document and the search term; and</claim-text><claim-text>training the semantic retrieval model and the ranking model jointly based on the n first correlation degrees and the n second correlation degrees.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the n candidate documents comprise a positive example candidate document and n&#x2212;1 negative example candidate documents, and training the semantic retrieval model and the ranking model jointly based on the n first correlation degrees and the n second correlation degrees, comprises:<claim-text>calculating a first loss value of the ranking model based on the first correlation degree between the search term and the positive example candidate document, and the first correlation degree between the search term and each negative example candidate document;</claim-text><claim-text>calculating a second loss value of the semantic retrieval model based on the n first correlation degrees and the n second correlation degrees; and</claim-text><claim-text>training the semantic retrieval model and the ranking model jointly based on the first loss value and the second loss value.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein training the semantic retrieval model and the ranking model jointly based on the first loss value and the second loss value, comprises:<claim-text>calculating a joint loss value based on the first loss value and the second loss value; and</claim-text><claim-text>training the semantic retrieval model and the ranking model jointly according to the joint loss value.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein calculating the second loss value of the semantic retrieval model based on the n first correlation degrees and the n second correlation degrees, comprises:<claim-text>obtaining the second loss value by performing Kullback-Leibler (KL) dispersion loss calculation based on the n first correlation degrees and the n second correlation degrees.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein obtaining the training sample, comprises:<claim-text>obtaining the search term and a positive example candidate document labeled by the search term;</claim-text><claim-text>obtaining documents related to the search term from a document library based on an initial semantic retrieval model;</claim-text><claim-text>selecting n&#x2212;1 documents randomly from the documents related to the search term as n&#x2212;1 negative example candidate documents corresponding to the search term; and</claim-text><claim-text>determining the search term, the positive example candidate document corresponding to the search term and the n&#x2212;1 negative example candidate documents corresponding to the search term as the training sample.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein obtaining the training sample, comprises:<claim-text>obtaining the search term, and obtaining documents related to the search term from a document library based on an initial semantic retrieval model;</claim-text><claim-text>obtaining a third correlation degree between each of the documents related to the search term and the search term based on an initial ranking model;</claim-text><claim-text>determining a maximum correlation degree in the third correlation degrees as a target third correlation degree;</claim-text><claim-text>in response to the target third correlation degree being greater than a first threshold, determining the document corresponding to the target third correlation degree as a positive example candidate document corresponding to the search term;</claim-text><claim-text>selecting n&#x2212;1 documents from the documents related to the search term as n&#x2212;1 negative example candidate documents corresponding to the search term, wherein the third correlation degree between each of the n&#x2212;1 documents and the search term is less than a second threshold; and</claim-text><claim-text>determining the search term and the positive example candidate document and the n&#x2212;1 negative example candidate documents corresponding to the search term as the training sample.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A non-transitory computer-readable storage medium storing computer instructions, wherein the instructions are configured to cause a computer to perform the following:<claim-text>obtaining a training sample comprising a search term and n candidate documents corresponding to the search term, where n is an integer greater than 1;</claim-text><claim-text>inputting the training sample into a ranking model of a semantic retrieval network, to obtain n first correlation degrees output by the ranking model, wherein each first correlation degree represents a correlation between a candidate document and the search term;</claim-text><claim-text>inputting the training sample into a semantic retrieval model of the semantic retrieval network, to obtain n second correlation degrees output by the semantic retrieval model, wherein each second correlation degree represents a correlation between a candidate document and the search term; and</claim-text><claim-text>training the semantic retrieval model and the ranking model jointly based on the n first correlation degrees and the n second correlation degrees.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The storage medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the n candidate documents comprise a positive example candidate document and n&#x2212;1 negative example candidate documents, and training the semantic retrieval model and the ranking model jointly based on the n first correlation degrees and the n second correlation degrees, comprises:<claim-text>calculating a first loss value of the ranking model based on the first correlation degree between the search term and the positive example candidate document, and the first correlation degree between the search term and each negative example candidate document;</claim-text><claim-text>calculating a second loss value of the semantic retrieval model based on the n first correlation degrees and the n second correlation degrees; and</claim-text><claim-text>training the semantic retrieval model and the ranking model jointly based on the first loss value and the second loss value.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The storage medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein training the semantic retrieval model and the ranking model jointly based on the first loss value and the second loss value, comprises:<claim-text>calculating a joint loss value based on the first loss value and the second loss value; and</claim-text><claim-text>training the semantic retrieval model and the ranking model jointly according to the joint loss value.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The storage medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein calculating the second loss value of the semantic retrieval model based on the n first correlation degrees and the n second correlation degrees, comprises:<claim-text>obtaining the second loss value by performing Kullback-Leibler (KL) dispersion loss calculation based on the n first correlation degrees and the n second correlation degrees.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The storage medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein obtaining the training sample, comprises:<claim-text>obtaining the search term and a positive example candidate document labeled by the search term;</claim-text><claim-text>obtaining documents related to the search term from a document library based on an initial semantic retrieval model;</claim-text><claim-text>selecting n&#x2212;1 documents randomly from the documents related to the search term as n&#x2212;1 negative example candidate documents corresponding to the search term; and</claim-text><claim-text>determining the search term, the positive example candidate document corresponding to the search term and the n&#x2212;1 negative example candidate documents corresponding to the search term as the training sample.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The storage medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein obtaining the training sample, comprises:<claim-text>obtaining the search term, and obtaining documents related to the search term from a document library based on an initial semantic retrieval model;</claim-text><claim-text>obtaining a third correlation degree between each of the documents related to the search term and the search term based on an initial ranking model;</claim-text><claim-text>determining a maximum correlation degree in the third correlation degrees as a target third correlation degree;</claim-text><claim-text>in response to the target third correlation degree being greater than a first threshold, determining the document corresponding to the target third correlation degree as a positive example candidate document corresponding to the search term;</claim-text><claim-text>selecting n&#x2212;1 documents from the documents related to the search term as n&#x2212;1 negative example candidate documents corresponding to the search term, wherein the third correlation degree between each of the n&#x2212;1 documents and the search term is less than a second threshold; and</claim-text><claim-text>determining the search term and the positive example candidate document and the n&#x2212;1 negative example candidate documents corresponding to the search term as the training sample.</claim-text></claim-text></claim></claims></us-patent-application>