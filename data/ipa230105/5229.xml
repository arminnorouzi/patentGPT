<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005230A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005230</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17552102</doc-number><date>20211215</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2219</main-group><subgroup>2004</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">EFFICIENT STORAGE, REAL-TIME RENDERING, AND DELIVERY OF COMPLEX GEOMETRIC MODELS AND TEXTURES OVER THE INTERNET</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63218121</doc-number><date>20210702</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Cylindo ApS</orgname><address><city>Copenhagen</city><country>DK</country></address></addressbook><residence><country>DK</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Fersund</last-name><first-name>Jens</first-name><address><city>Virum</city><country>DK</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>M&#xf8;ller</last-name><first-name>Andreas</first-name><address><city>Valby</city><country>DK</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for real-time compositing, rendering and delivery of complex geometric models and textures, includes storing a plurality of three-dimensional models of at least two sub-parts of a whole three-dimensional object, storing a plurality of image textures for each of the plurality of three-dimensional models, receiving instructions from a user, the instructions including a selection of at least two of the plurality of three-dimensional models, each of the at least two of the plurality of three-dimensional models being one of the at least two sub-parts of the whole three-dimensional object, and generating the whole three-dimensional object including the at least one of the plurality of image textures for each of the at least two of the plurality of three-dimensional models applied according to the instructions to the at least two of the plurality of three-dimensional models.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="109.73mm" wi="158.75mm" file="US20230005230A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="215.39mm" wi="147.66mm" orientation="landscape" file="US20230005230A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="214.97mm" wi="150.54mm" orientation="landscape" file="US20230005230A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="214.97mm" wi="161.29mm" orientation="landscape" file="US20230005230A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="226.99mm" wi="154.43mm" orientation="landscape" file="US20230005230A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="229.62mm" wi="161.71mm" orientation="landscape" file="US20230005230A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="229.62mm" wi="159.85mm" orientation="landscape" file="US20230005230A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="234.27mm" wi="157.14mm" orientation="landscape" file="US20230005230A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="239.78mm" wi="184.23mm" orientation="landscape" file="US20230005230A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="227.50mm" wi="184.57mm" orientation="landscape" file="US20230005230A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="216.32mm" wi="158.24mm" orientation="landscape" file="US20230005230A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="216.32mm" wi="177.04mm" orientation="landscape" file="US20230005230A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="216.32mm" wi="163.32mm" orientation="landscape" file="US20230005230A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="216.32mm" wi="172.21mm" orientation="landscape" file="US20230005230A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATION INFORMATION</heading><p id="p-0002" num="0001">This patent claims priority from U.S. provisional patent application No. 63/218,121 entitled &#x201c;EFFICIENT STORAGE, REAL-TIME RENDERING AND DELIVERY OF COMPLEX GEOMETRIC MODELS AND TEXTURES OVER THE INTERNET&#x201d; filed Jul. 2, 2021, the entire content of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">NOTICE OF COPYRIGHTS AND TRADE DRESS</heading><p id="p-0003" num="0002">A portion of the disclosure of this patent document contains material which is subject to copyright protection. This patent document may show and/or describe matter which is or may become trade dress of the owner. The copyright and trade dress owner has no objection to the facsimile reproduction by anyone of the patent disclosure as it appears in the Patent and Trademark Office patent files or records, but otherwise reserves all copyright and trade dress rights whatsoever.</p><heading id="h-0003" level="1">BACKGROUND</heading><heading id="h-0004" level="1">Field</heading><p id="p-0004" num="0003">This disclosure relates to storage and delivery of complex geometric models and textures over the internet.</p><heading id="h-0005" level="1">Description of the Related Art</heading><p id="p-0005" num="0004">There exist some datasets that are particularly difficult or impossible to easily share over the internet. Since the advent of the internet for public consumption in the early 1990s, storage capability and bandwidth have dramatically increased. Where downloading a single 4-megabyte MP3 file used to take minutes or hours (depending on the type of connection), it now takes seconds for most Americans and many users worldwide. Likewise, where digital video discs (DVDs) with capacities of approximately 4 gigabytes were needed to contain the voluminous data for film and television programs, now encoding techniques and increased bandwidth have enabled entire neighborhoods to concurrently stream video content from services like Netflix&#xae; and Hulu&#xae; over standard home internet services.</p><p id="p-0006" num="0005">Despite all of these advances in both storage and data transmission, there still exist some datasets that are too costly, too slow, too large, or too complex to be stored on a server and/or easily transmitted over the internet on demand. These datasets are atypical. They are not ones commonly considered by most consumers or most personal computer or mobile device users. The problems presented by these types of datasets are not often confronted, precisely because they are rare. As a result, the solutions for delivery of these types of datasets are likewise not common or even available.</p><p id="p-0007" num="0006">One example of such a dataset is high-quality images of customizable furniture. As many have experienced when shopping for virtually any item online, there tends to be between one to ten images of that item, often from various perspectives, but sometimes from only a few. Furniture, particularly high-quality and custom furniture (e.g., where one can select particular legs, fabrics, arms, etc.) is a major purchase for many. When making those purchases, many wish to see the product, or the potential product, as closely as possible to its end form. Home designers and professional decorators carefully craft an entire home or room design. Having only a few views is unacceptable. Because the furniture is custom (in whole or in part), there may literally never yet have been a created, real-world example of a design proposed by the designer.</p><p id="p-0008" num="0007">In a pre-internet world given this situation, a designer would travel to a location where swatches of fabric and example arms and designs would be available, and he or she would combine the elements in near proximity to one another and &#x201c;guess&#x201d; what the resulting appearance would be. An experienced designer could become quite good at this type of work. Some customers or homeowners are able to do this mental exercise as well, but many are unable to do this kind of imagining and have difficulty picturing how their proposed new furniture will fit into their environment.</p><p id="p-0009" num="0008">There may be literally hundreds of potential fabrics, multiple &#x201c;backs&#x201d; for a given chair or sofa, tens or hundreds of &#x201c;arms,&#x201d; various legs, and so on. Some of the more complex furniture to which the systems described herein are applied have resulted in as many as 40,000 different permutations of elements, fabrics, materials and the like for a single piece of furniture. In that context, it is literally impossible to store images for all of the different fabric texture, arm, leg, back, and the like combinations that are possible for that piece of furniture. The data capacity necessary would be astronomical. Multiply this by many different views of a particular piece (e.g. from different sides and angles) and the many various pieces of furniture in a given collection or available from a manufacturer, and the result only compounds.</p><p id="p-0010" num="0009">Furthermore, designers wish to view the furniture from any one of many perspectives, potentially even an augmented reality (AR) perspective allowing full range of movement around an object. Providing only a front view or even a perspective view or both is insufficient. The designer may wish to see it from any number of perspectives or many perspectives before making a decision on a given design. So, the resulting need of images 360 degrees around a piece in addition to all of the permutations on design and fabric result in an untenable amount of data for storage and transmission. One option is to rely upon three-dimensional rendering in real time, but that requires substantial processing power, is difficult to cause the furnishings to appear photorealistic on all but the best computing devices, and it is difficult to scale such a process while responding to potentially hundreds or thousands of web requests at once in real time. In addition, simply transmitting a three-dimensional model and associated textures is time-consuming because they are larger than typical two-dimensional images used in most web browsing. The web browsing experience then bogs down and becomes unenjoyable or impossible for a consumer. As most sellers know, any impediment to a sale (such as slow responsiveness of a website) is a bad thing.</p><p id="p-0011" num="0010">The foregoing example is made with respect to furniture because it is one example that is familiar to the patentee and in which the patentee's technology has been effective at resolving difficult problems. However, other situations exist in which complex geometry and high-quality textures meet with a need for delivery of multiple perspectives on the object. Automotive purchases, especially high-end automotive, custom clothing, architecture, and other industries can rely upon similar functionality as discussed herein to some similar, if not identical, problems.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011">The patent application file contains at least one drawing to be executed in color. Copies of this patent application publication with color drawings will be provided by the Office upon request and payment of the necessary fee.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an overview of a system for efficient storage, real-time rendering, and delivery of complex geometric models and textures over the internet.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an exemplary computing device.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a functional block diagram of a system for efficient storage, real-time rendering, and delivery of complex geometric models and textures over the internet.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example three-dimensional model of an object presented in wireframe.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an example three-dimensional model with the wireframes interconnected by triangles.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an example three-dimensional model covered in a particular material and an associated texture.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an example of the multiple components making up a given object.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is an example flow for compositing a two-dimensional image from a series of components as layers.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an example flow for compositing a three-dimensional, textured model from components as layers.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart of a process for efficient storage, real-time rendering, and delivery of complex geometric models and textures over the internet.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart of a process for generating dynamic three-dimensional models and textures.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart of a process for applying realistic lighting effects to a generated three-dimensional model.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart of a process for applying appropriate visual warp to translucent and transparent three-dimensional models.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0026" num="0025">Throughout this description, elements appearing in figures are assigned three-digit reference designators, where the most significant digit is the figure number and the two least significant digits are specific to the element. An element that is not described in conjunction with a figure may be presumed to have the same characteristics and function as a previously-described element having a reference designator with the same least significant digits.</p><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0027" num="0026">In response to these issues, the patentee has created a system for two-dimensional compositing of images wherein, for a particular design, the individual components are saved (e.g., their shape) in the form of a 2D image. The associated textures (e.g., fabric, or wood color for legs or arms) are stored in a known format. The 2D images are stored for each of 32 different points of &#x201c;rotation&#x201d; about the object. So, for example, 32 different views of a chair base (with no arms or legs, etc.) and 32 different views of each type of leg, each type of arm, each type of back, etc. These images are basically stored &#x201c;blank&#x201d; for purposes of texture, enabling application of textures after-the-fact. This reduces the need to have full images with every texture for every permutation. The images that are only parts of an eventual furniture product are categorized for their kind (e.g., legs or backs) and their relationships to other parts of the images are defined (e.g., legs attach here on the design, arms attach here, etc.). The images and textures are likewise associated with each object component (e.g., maple wood texture only goes with legs or arms, not cushions, but cushions have fabric textures that are not present one most legs, etc.).</p><p id="p-0028" num="0027">As a user browses the website and selects a piece of furniture, an arm, a leg, and an associated fabric or fabrics, the associated web call can provide that information to a content management server or high-availability web server (e.g., Akamai or Cloudflare) which can access its individual assets for each component and texture. Then, in real time, those components are intelligently overlaid on one another. However, this framework has relied upon two-dimensional models from 32 different perspectives. It is still limited, particularly for use in augmented reality use cases where an infinite number of perspectives may be necessary (e.g. every possible three-dimensional angle for viewing).</p><p id="p-0029" num="0028">In response, the inventors also have created a system that operates similarly but relies upon untextured three-dimensional models of each element (e.g., each cushion, arm, leg, back, etc.) for a given piece of furniture. The use of a three-dimensional model enables precise perspective from any position relative to a (0, 0, 0) position in a virtual world that is placed at the center of the 3D object. For purposes of this kind of system, it remains necessary to identify the locations on the 3D model to which other objects attach. So, for example, legs only attach to tables (generally) at the four corners. There are obviously exceptions to that rule, but in general that is the case. So, when legs are selected, the positions of those legs relative to that (0, 0, 0) position in an (x, y, z) virtual world are set. Thereafter, any leg of any design and texture may be attached at those positions. This is done for each aspect of each piece.</p><p id="p-0030" num="0029">An associated logic to enable customers (e.g., the furniture manufacturers) to easily format data for this system is likewise necessary. The logic may enable definitions for common types (e.g., legs, or table tops or arms, etc.). Then, the system may guide a typical non-technical user in identifying the components, and their relative position, and enable easily adding or disabling additional components in each slot, along with the associated three-dimensional models.</p><p id="p-0031" num="0030">In much the same way as the two-dimensional version, the three-dimensional models for each component have associated textures. So, for example, a cushion may have cloth or leather textures associated therewith, while wooden or metal legs may have woodgrain, painted, or stained textures.</p><p id="p-0032" num="0031">In this system, a call is made in real-time upon a request for an image to a high-availability server from a web page (or AR headset or application on a tablet PC, for example) for a particular piece of furniture, with particular legs, particular arms, and particular cushions, and having a particular fabric texture. The model is obtained, and the server applies the requested textures in real time, and provides the complete model to the end user. Thereafter, the user may move, rotate, and otherwise interact with that model and texture combination until such time as a different combination is selected.</p><p id="p-0033" num="0032">Alternatively, the model and texture may be applied on the server in real time, such that adjustments are detected (e.g., moving an AR display or moving the model) and the rendering happens on the remote server for that new positional information. Thereafter, an image, with an alpha mask applied along its edges, of the furniture is transmitted. The main benefit of remote rendering is that the transmission of the image will take very little time because it may be sent merely as a 2D image of the rendered 3D model, and the rendering can be completed quickly on specialized hardware, without regard to the particular AR or other device making the request for the object in a particular orientation.</p><p id="p-0034" num="0033">One major benefit of full three-dimensional models is that they may be &#x201c;placed&#x201d; virtually within an augmented reality view of a potential purchaser's actual home or space. When placing, they may be very specifically oriented, moved about, and adjusted, unlike the 32-view version reliant upon two-dimensional images.</p><p id="p-0035" num="0034">Description of Apparatus</p><p id="p-0036" num="0035">Referring now to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, an overview of a system <b>100</b> for efficient storage, real-time rendering, and delivery of complex geometric models and textures over the internet is shown. The system <b>100</b> includes a data server <b>120</b>, a render server <b>130</b>, a user computing device <b>140</b>, and a user mobile computing device <b>150</b> all interconnected by a network <b>110</b>.</p><p id="p-0037" num="0036">The data server <b>120</b> is a computing device (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) or a group of computing devices. The data server <b>120</b> is used to store three-dimensional models, two-dimensional images (from various perspectives, and any textures associated with the various three-dimensional models. The three-dimensional models, as will be discussed herein, are of various components of the objects to be rendered and delivered. Likewise, the data server <b>120</b> may store two-dimensional images of the various components of the objects to be delivered via the internet in the earlier incarnation of this invention involving multiple views (from multiple angles) of each texture combination. The textures are specifically designed to take up less space.</p><p id="p-0038" num="0037">The data server <b>120</b> may be self-hosted&#x2014;meaning operated by a company or entity that enables the functions and systems described herein. Alternatively, the data server <b>120</b> may be on a shared resource service such as Amazon AWS or Microsoft Azure. Even more likely, however, the data server <b>120</b> is hosted on a high-availability server service&#x2014;self-hosted or hosted by a service&#x2014;such as Cloudflare or Akamai and similar services typically reserved for ensuring web pages and other content load quickly when images and video are requested at a very large scale.</p><p id="p-0039" num="0038">The render server <b>130</b> is a computing device or a group of computing devices. The render server <b>130</b> may be in a single location, but preferably is in many locations so as to serve the maximum number of users throughout the world quickly and efficiently. The render server <b>130</b> is a server which is used to render the three-dimensional models and apply textures using one or more shaders. The render server <b>130</b> may be integrated with the data server <b>120</b> in some implementations. However, it may be preferrable for the render server <b>130</b> to incorporate one or more graphics processing units (GPUs) so as to be better equipped to render three-dimensional images with the associated textures. In general, data servers <b>120</b> are not equipped with GPUs at all. Specialized hosting services typically incorporate GPUs.</p><p id="p-0040" num="0039">In some implementations, there may not be a render server <b>130</b> at all. The textures and models may be provided directly to a requesting device which performs the render operation itself. That is not ideal, particularly for many mobile devices such as phones and tablets, which have weaker or no GPUs for rendering. This option will be discussed in more detail below.</p><p id="p-0041" num="0040">The user computing device <b>140</b> is a computing device such as a personal computer, laptop computer, desktop computer or the like. The user computing device <b>140</b> is typically a device browsing a website using web browser software or an associated application (e.g. a store or shopping application). The user computing device <b>140</b> may be a typical consumer computing device, lacking in any significant specialized capabilities. However, the user computing device <b>140</b> may include a GPU or an integrated GPU (e.g. integrated into a single chip with a CPU) that can perform some or all of the rendering operations described herein. In other cases, the user computing device <b>140</b> may include no GPU at all, and rendering must take place using a render server <b>130</b>.</p><p id="p-0042" num="0041">The user mobile computing device <b>150</b> is effectively identical to the user computing device, though its form factor may be that of a mobile device. It may, for example, be a mobile phone, a smart phone, a tablet computer, or other, similar device. One unique attribute of the user mobile computing device <b>150</b> is that its power may typically be less than the user computing device <b>140</b>, particularly in rendering three-dimensional models and textures. Though it is increasingly becoming the case that mobile devices are as powerful or more powerful than most laptops or desktop computers, for the moment most user mobile computing devices are less powerful, particularly at three-dimensional rendering.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an exemplary computing device <b>200</b>, which may be a part of the data server <b>120</b> or the render server <b>130</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the computing device <b>200</b> includes a processor <b>210</b>, memory <b>220</b>, a communications interface <b>230</b>, along with storage <b>240</b>, and an input/output interface <b>250</b>. Some of these elements may or may not be present, depending on the implementation. Further, although these elements are shown independently of one another, each may, in some cases, be integrated into another.</p><p id="p-0044" num="0043">The processor <b>210</b> may be or include one or more microprocessors, microcontrollers, digital signal processors, application specific integrated circuits (ASICs), or a systems-on-a-chip (SOCs). The memory <b>220</b> may include a combination of volatile and/or non-volatile memory including read-only memory (ROM), static, dynamic, and/or magnetoresistive random access memory (SRAM, DRM, MRAM, respectively), and nonvolatile writable memory such as flash memory.</p><p id="p-0045" num="0044">The memory <b>220</b> may store software programs and routines for execution by the processor. These stored software programs may include an operating system software. The operating system may include functions to support the input/output interface <b>250</b>, such as protocol stacks, coding/decoding, compression/decompression, and encryption/decryption. The stored software programs may include an application or &#x201c;app&#x201d; to cause the computing device to perform portions of the processes and functions described herein. The word &#x201c;memory&#x201d;, as used herein, explicitly excludes propagating waveforms and transitory signals.</p><p id="p-0046" num="0045">The communications interface <b>230</b> may include one or more wired interfaces (e.g. a universal serial bus (USB), high definition multimedia interface (HDMI)), one or more connectors for storage devices such as hard disk drives, flash drives, or proprietary storage solutions. The communications interface <b>230</b> may also include a cellular telephone network interface, a wireless local area network (LAN) interface, and/or a wireless personal area network (PAN) interface. A cellular telephone network interface may use one or more cellular data protocols. A wireless LAN interface may use the WiFi&#xae; wireless communication protocol or another wireless local area network protocol. A wireless PAN interface may use a limited-range wireless communication protocol such as Bluetooth&#xae;, Wi-Fi&#xae;, ZigBee&#xae;, or some other public or proprietary wireless personal area network protocol. The cellular telephone network interface and/or the wireless LAN interface may be used to communicate with devices external to the computing device <b>200</b>.</p><p id="p-0047" num="0046">The communications interface <b>230</b> may include radio-frequency circuits, analog circuits, digital circuits, one or more antennas, and other hardware, firmware, and software necessary for communicating with external devices. The communications interface <b>230</b> may include one or more specialized processors to perform functions such as coding/decoding, compression/decompression, and encryption/decryption as necessary for communicating with external devices using selected communications protocols. The communications interface <b>230</b> may rely on the processor <b>210</b> to perform some or all of these function in whole or in part.</p><p id="p-0048" num="0047">Storage <b>240</b> may be or include non-volatile memory such as hard disk drives, flash memory devices designed for long-term storage, writable media, and proprietary storage media, such as media designed for long-term storage of data. The word &#x201c;storage&#x201d;, as used herein, explicitly excludes propagating waveforms and transitory signals.</p><p id="p-0049" num="0048">The input/output interface <b>250</b>, may include a display and one or more input devices such as a touch screen, keypad, keyboard, stylus or other input devices. The processes and apparatus may be implemented with any computing device. A computing device as used herein refers to any device with a processor, memory and a storage device that may execute instructions including, but not limited to, personal computers, server computers, computing tablets, set top boxes, video game systems, personal video recorders, telephones, personal digital assistants (PDAs), portable computers, and laptop computers. These computing devices may run an operating system, including, for example, variations of the Linux, Microsoft Windows, Symbian, and Apple Mac operating systems.</p><p id="p-0050" num="0049">The techniques may be implemented with machine readable storage media in a storage device included with or otherwise coupled or attached to a computing device <b>200</b>. That is, the software may be stored in electronic, machine readable media. These storage media include, for example, magnetic media such as hard disks, optical media such as compact disks (CD-ROM and CD-RW) and digital versatile disks (DVD and DVD&#xb1;RW), flash memory cards, and other storage media. As used herein, a storage device is a device that allows for reading and/or writing to a storage medium. Storage devices include hard disk drives, DVD drives, flash memory devices, and others.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a functional block diagram of a system for efficient storage, real-time rendering, and delivery of complex geometric models and textures over the internet. The system includes the data server <b>320</b>, the render server <b>330</b>, the user computing device <b>340</b>, and the user mobile computing device <b>350</b>.</p><p id="p-0052" num="0051">The data server <b>320</b> includes a communications interface <b>322</b>, a models database <b>324</b> and a textures database <b>326</b>.</p><p id="p-0053" num="0052">The communications interface <b>322</b> is an interface for communicating data to and from the data server <b>320</b>. It may include hardware (e.g. networking hardware such as wireless or wired network adaptors), but it includes software. The communications interface <b>322</b> may incorporate standardized network protocols and software, but also may include specialized calls, interfaces, or APIs (application programming interfaces). At a minimum, the communications interface <b>322</b> is capable of responding to HTTP, HTTPS, or other requests for data from the models database <b>324</b> and the textures database <b>326</b> so as to receive the request and act upon it to transmit back models and textures in response.</p><p id="p-0054" num="0053">The models database <b>324</b> stores three-dimensional models for the objects. In an example familiar to the inventors, those objects are furnishings. However, the objects could be automobiles, wall hangings, clothing, or other, similar objects with different textures, paint colors, and shapes. The models database <b>324</b> may store the models as a series of sub-parts. For example, a chair may be made up of a base, a back, and three or more legs. The sub-parts may be associated with one another such that the models are designed for each sub-part to connect to another sub-part at a particular location or locations. This association of the way in which each sub-part connects with other sub-parts of a whole model may be described as a ruleset. The associated ruleset data, defining these connections between sub-parts may be stored in metadata associated with each sub-part or with an overall model as a whole (e.g. a chair or a desk). The data may preferably be a location in an (x, y, z) axis relative to an origin (0, 0, 0) location for each model.</p><p id="p-0055" num="0054">The textures database <b>326</b> stores the textures for the three-dimensional models and/or each sub-part of the three-dimensional models. Textures are a term of art in the computer graphics space which mean images applied to or &#x201c;wrapped&#x201d; around three-dimensional images. The textures for a given shape in three-dimensional computer graphics give the object its appearance as a chair, a desk, a wall, and the like. The textures may be relatively simple (e.g. a repeating leather grain so as to not have seams appear in an object applying that texture) or complex (e.g. the pattern of a woven rug). There are numerous techniques for reducing the amount of data necessary for storing and transmitting texture files, including incorporating numerous textures for a given object on a single, flat (two-dimensional) image file.</p><p id="p-0056" num="0055">So, for example, a series of textures may be stored for the legs of a chair or even for a particular type of legs of a chair. For example, a chair may have options for both wooden or metal legs. For wooden legs, the associated textures may be or represent different woods such as maple, oak, and pine, but also different stains for those various woods. For the metal legs, the associated textures may represent brushed chrome, stainless steel, aluminum, chrome, gold, etc. but, when wooden legs are selected, none of those metal leg textures may be shown.</p><p id="p-0057" num="0056">The render server <b>330</b> includes a communications interface <b>332</b>, a render distribution <b>334</b>, render software <b>336</b>, depth from field software <b>337</b>, and lighting render software <b>338</b>.</p><p id="p-0058" num="0057">The communications interface <b>332</b> operates in much the same way as communications interface <b>322</b> operates, except the communications interface <b>332</b> serves to enable the render server <b>330</b> to send and receive render requests and to provide rendered images in response.</p><p id="p-0059" num="0058">The render distribution <b>334</b> operates to allocate render requests to the render server <b>330</b> or to the user computing device <b>340</b> or user mobile computing device <b>350</b>. The decision primarily may hinge upon the capabilities of the user computing device <b>340</b> or user mobile computing device <b>350</b> to render the models and textures from the data server <b>320</b> or upon the availability or settings associated with the render server <b>330</b>. Render distribution <b>334</b> may provide a render request to the render software <b>336</b> or send it to one of the user computing devices.</p><p id="p-0060" num="0059">The render distribution <b>334</b> may also manage a load-balancing functionality to allocate the render requests (and the accompanying textures and models) to a particular instance of the render software <b>336</b> or to a particular render server <b>330</b> based upon availability or current usage of either or both.</p><p id="p-0061" num="0060">The render software <b>336</b> is software for converting three-dimensional models and textures into a combined textured model and/or two-dimensional image. Depending on the implementation employed, the render software <b>336</b> may return a fully-rendered three-dimensional model to a requesting device (e.g. the user computing device <b>340</b>) or may return a two-dimensional image of a fully-rendered three-dimensional model with textures and lighting applied from a perspective provided in a render request to the render server <b>330</b>. The render software may be or include a modified video game rendering software. Common examples of software that may be used for rendering, in whole or in part, are Unity&#xae;, the Unreal Engine&#xae;, and the CryEngine&#xae;. Others may be used or may be more appropriate in given scenarios.</p><p id="p-0062" num="0061">The depth from field software <b>337</b> is shown as distinct from the render software <b>336</b>, but in most instances, they will be integrated with one another. One example of depth from field software <b>337</b> is commonly referred to as &#x201c;ray tracing&#x201d; software. Ray tracing is a technique employed where a position (e.g. for a viewer or for a light source) may be &#x201c;ray traced&#x201d; to any point visible in a three-dimensional environment. Once the ray trace is complete (and in reality ray tracing takes place constantly and is updated in real-time as the model moves), a depth from the place from which the ray trace began is known. This depth is a &#x201c;depth from field&#x201d; that may be used as discussed herein. There are also other methods for performing depth from field operations, but ray tracing is becoming the most common.</p><p id="p-0063" num="0062">The depth from field software <b>337</b> is used in particular to perform lighting operations (e.g., place a light source here within the space, and calculate the resulting lighting and shadows) and, as discussed more fully below, to perform operations to determine which elements of a multi-part three-dimensional object are visible from a given perspective within a three-dimensional environment. In this way, multi-part models may be appropriately rendered without unnecessarily rendering &#x201c;invisible&#x201d; portions of the model and by applying appropriate shadows and lighting transformations.</p><p id="p-0064" num="0063">The lighting render software <b>338</b> is shown as distinct from the render software <b>336</b>, but in most instances will be integrated with the render software <b>336</b>. The lighting render software <b>338</b> is similar to the depth from field software <b>337</b>, but, in particular, is focused on placing one or more lights within the three-dimensional environment and adjusting the model and its applied textures accordingly. So, for example, planar objects nearby to a light source tend to reflect that light source. Colors brighten and sometimes disappear. Complex software is used to model the interaction of models and in particular textures in response to lighting within the environment and the alter the resulting render. Shadows may also be calculated and created from these light sources. The lighting render software <b>338</b> performs these functions.</p><p id="p-0065" num="0064">The user computing device <b>340</b> includes a communications interface <b>342</b>, a web browser <b>344</b>, and a renderer <b>346</b>.</p><p id="p-0066" num="0065">The communications interface <b>342</b> enables network communications with the internet generally, or other networks, but in particular with the data server <b>320</b> and the renderer server <b>330</b>.</p><p id="p-0067" num="0066">The web browser <b>344</b> is a typical web browser software which may be stand-alone or integrated into another application or the operating system itself.</p><p id="p-0068" num="0067">The renderer <b>346</b> may or may not be present on the user computing device. In addition, the renderer <b>346</b> may be a part of another application or the operating system itself. The renderer <b>346</b> can perform the same function as the render software <b>336</b>, but may operate using the models and textures transmitted to the user computing device <b>340</b> from the data server <b>320</b>, based upon the decision made about distribution by the render distribution <b>334</b>.</p><p id="p-0069" num="0068">The user mobile computing device <b>350</b> includes its own communications interface <b>352</b>, a web browser <b>354</b>, and a renderer <b>356</b>. The user mobile computing device's <b>350</b> components operate in much the same way as those of the user computing device <b>340</b>. That discussion will not be repeated here. The web browser <b>354</b> is shown as a separate component, but it may be a part of another application or the mobile device operating system itself.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example three-dimensional model <b>400</b> of an object presented in wireframe. The model is shown in wireframe without any texture being applied. This is, roughly-speaking, how three-dimensional models are stored in digital form. In most three-dimensional engines, models are stored as a series of vertices that interconnect. These vertices form a grouping of triangles. The faces of these triangles may be &#x201c;covered&#x201d; in textures from two-dimensional texture files.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an example three-dimensional model <b>500</b> with the wireframes interconnected by triangles. This object appears much more like a traditional three-dimensional model, but in an untextured form.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an example three-dimensional model <b>600</b> covered in a particular material and an associated texture. Here, the object is fully-realized. There is a leather texture to the model <b>600</b>, it has legs that appear to be made of black metal, and it has realistic lighting applied (e.g. shadows near the base darker than on the top).</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an example of the multiple components making up a given object model <b>700</b>. Because it is impossible to save thirty-two (or any other suitable number) of individual two-dimensional images for a huge number of potential model configurations when these objects are composed of multiple components, the objects themselves may be broken down into multiple, individual three-dimensional sub-parts. Here, the overall object model <b>700</b> is composed of sub-parts of a base <b>710</b>, a backing <b>712</b> and a face <b>714</b>.</p><p id="p-0074" num="0073">The base <b>710</b> shown is a four-star pole base. However, the base <b>710</b> may be four traditional chair legs or three traditional chair legs, and the materials may be chrome, steel, wood, and other options. Likewise, the backing <b>712</b> is shown to be made of cloth. However, different materials may be used, such as leather, or wood (with any number of finishes), plastic, or even metal. Also, the face <b>714</b> is shown as made of leather in this particular model. Other models for the face <b>714</b> may have cloth, or even particular types of cloth (e.g. corduroy, cotton, linen, etc.) each of which would have their own appearance that would be different from that of the leather face <b>714</b> model.</p><p id="p-0075" num="0074">The base may be specifically set forth in associated model metadata to connect at a particular location on the backing <b>712</b> model, which may in turn have a particular portion of the face <b>714</b> to which it affixes. Thereby, the entire model may be joined from a series of sub-parts. Each sub-part may be separately modeled to create a combined, unique model from each of these sub-parts. Each texture may apply to only certain of the models (e.g. the leather-appearance model only has browns and tans and reds for textures, while the cloth has other colors like blue and red and white). This model and texture separation and sub-parts is preferable, and uses less overall storage space, than storing each option for every part as an entire model.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is an example flow for compositing a two-dimensional image from a series of components as layers. This is the first example of a way to tackle the problem of voluminous data. Here, a series of thirty-two (though it could be more or fewer) perspectives are rendered of an object. To reduce the overall storage impact, from each perspective, only those portions that are visible are stored. So, the first layer <b>810</b> (the back-most layer) is merely the shadows that will be cast by the object. The object will always be visible over its shadows, because it is the one casting them. The next layer <b>812</b> is the feet layer. They are lower or below every other object, so they are the second back-most layer. The next layer is the angle front layer <b>814</b>. This is the majority of the object itself. A &#x201c;back&#x201d; layer <b>816</b> (for the back of the object) is then applied. But, in this perspective, it turns out that it is entirely non-visible, so it is basically irrelevant. The final image <b>818</b> is only the visible portions (once each layer is sequentially applied) for the model and its shadow.</p><p id="p-0077" num="0076">One of the drawbacks of this process is that the images are still merely two-dimensional images. There is no possibility of applying dynamic shadows or lighting (e.g., a lighting color or tone). Also, other depth-based tactics are difficult to employ. In addition, because there is no model, a human has to intelligently input each of the individual layers in the appropriate order for the final image <b>818</b> to appear appropriately from a given perspective. Otherwise, the shadows can appear over the object itself or other nonsensical results can occur. While this option solves the data-storage problem, a more-elegant, but more complex, three-dimensional approach would be better.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an example flow for compositing a three-dimensional, textured model from components as layers. This is an example of such a three-dimensional approach. Under this approach, any angle may be chosen, because it is comprised of fully three-dimensional models making up the sub-parts of the overall model. Here, in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a desk is shown.</p><p id="p-0079" num="0078">The first sub-parts <b>910</b> are the handles and feet for the desk. The handles will be in fixed positions, as will the feet. Then, the backing <b>912</b> is integrated. It may be fully modelled, but its interior texture need not be applied because it is never visible. Next, the front portion <b>914</b> of the model is applied showing the drawers. Then, the texture is applied to each of the integrated sub-parts of this model at <b>916</b>. Then, the associated textures are applied to the handles at feet at <b>918</b> to generate the final model.</p><p id="p-0080" num="0079">Uniquely, this model may be rotated. Its shadows may be dynamically created by the renderer once the model is complete. Object interaction is enabled whereby this model may appropriately &#x201c;sit&#x201d; in a given area, or respond to interactions with different sub-parts (e.g. swapping legs, tops, materials). This object may be placed (in augmented reality) within a real space, with current lighting even simulated by the renderer to make the object appear as closely as possible to its position in the real world. The three-dimensional model system is much more flexible and powerful, while having virtually all of the space-saving benefits of the original two-dimensional image model. Further benefits will be discussed below.</p><p id="p-0081" num="0080">Description of Processes</p><p id="p-0082" num="0081">Referring now to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, a flowchart of a process for efficient storage, real-time rendering, and delivery of complex geometric models and textures over the internet. The flow chart has both a start <b>1005</b> and an end <b>1095</b>, but the process may be cyclical in nature.</p><p id="p-0083" num="0082">Following the start <b>1005</b>, the process begins with the storage of the three-dimensional models at <b>1010</b>. Here, the models have been created for each sub-part of each object that is desired to be modeled and textured using this system. For an automobile, this may be the exterior, windows, doors, wheels, any optional spoilers or tonneau covers and any exterior badging. For furniture, it may be legs, arms, backs, seats, cushions, and may also include the physical portion of any texture having a shape (e.g. grain of leather, cloth appearance, etc.). The storage of those models at <b>1010</b> may also involve storage of associated metadata describing the way in which each of the sub-parts of a given entire object model are interlinked (e.g. legs attach here, arms attach here, etc.).</p><p id="p-0084" num="0083">Next, the textures are stored at <b>1020</b>. Here, the corresponding images making up the textures (e.g. woodgrain, leather colors, cloth colors) are stored for each model and for each sub-part of a given model of an object. Much like the way in which the parts interlink, metadata may be stored along with these textures that indicates the way in which the textures are applied, and to which components or sub-parts they apply. Woodgrain, for example, does not apply to a seat cushion, but may apply to the legs of a couch.</p><p id="p-0085" num="0084">This association of textures and models may take place at <b>1030</b> through interaction by a user with a user interface, or may be pre-programmed into the models themselves. There may be a conversion process from a model or series of models and textures to automatically generate a different format as required for the storage taking place at steps <b>1010</b> and <b>1020</b> to thereby associate those textures and models at <b>1030</b>. Once this is complete, the data server <b>120</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) is ready to respond to requests for models and textures.</p><p id="p-0086" num="0085">Next, a request for an image is received at <b>1040</b>. At this step, a web browser has requested a particular set of characteristics for a model of an object (e.g. a chair) and it has been received by, for example, a render server <b>130</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). This request may likewise include a particular orientation of the object or perspective and may likewise include a desired lighting color, tone, and position in space relative to the object to be modeled.</p><p id="p-0087" num="0086">Next, instructions are created to combine the sub-parts of the model (e.g. the model(s)) and associated textures at <b>1050</b>. These instructions inform a renderer in the way the sub-parts are interconnected, the textures to apply to each sub-part, and the positioning of the lighting and camera relative to the object.</p><p id="p-0088" num="0087">Optionally, these instructions may be sent to a remote rendering device <b>1060</b>. This could be the render server <b>130</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) or it could be sent as a part of the response back to the requesting user computing device <b>140</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). The rendering may take place right on the same device that stores the models and textures and that generated the instructions at <b>1050</b>. If the rendering is to take place remote from the server where the model(s) and texture(s) are stored, then the necessary model(s) and texture(s) may be sent along with the instructions. This also can aid in easy re-rendering (e.g. slight movements or changes to position) for the object by a remote renderer.</p><p id="p-0089" num="0088">Wherever the render is to take place, preferably a render server, the object is rendered at <b>1070</b>. Here, all of the request, model(s) and texture(s) are considered and the overall model is rendered as a two-dimensional image (e.g. an image suitable for transport to the requesting device such as a web browser or augmented reality application). The rendered object may then be transmitted to that device (if necessary) or may be displayed on the device if transmission is unnecessary.</p><p id="p-0090" num="0089">If an update is requested (e.g. the model or perspective is moved or the lighting changes or the components changes, such as new legs or back) at <b>1075</b> (&#x201c;yes&#x201d; at <b>1075</b>), then this requested change is processed much as the original request for image at <b>1040</b>.</p><p id="p-0091" num="0090">If there are no updates requested at <b>1075</b> (&#x201c;no&#x201d; at <b>1075</b>), then the process ends at <b>1095</b>.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart of a process for generating dynamic three-dimensional models and textures. The flow chart has both a start <b>1105</b> and an end <b>1195</b>, but the process may be cyclical in nature. This process begins following start <b>1105</b> with receipt of a request for an image <b>1110</b>, much as in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. The request may identify all of the sub-parts desired and the associated desired textures for a given model.</p><p id="p-0093" num="0092">Thereafter, the first step is to obtain an associated model portion at <b>1120</b>. This may be the legs of a given chair, or the arms. Then, a determination is made at <b>1125</b> whether additional model parts are needed. If yes (e.g. a chair is not arms alone), then the associated connections for the part are obtained at <b>1130</b>. This may be from metadata associated with the request itself received at <b>1110</b> or may be obtained from the model portion obtained at <b>1120</b>. Any model portions obtained are also joined at <b>1140</b> to create a combined model making up the two (or more) model portions.</p><p id="p-0094" num="0093">Again, the system determines whether there are additional model parts at <b>1125</b>. If so (&#x201c;yes&#x201d; at <b>1125</b>), then the further connections are obtained.</p><p id="p-0095" num="0094">If not, or once the model is complete (&#x201c;no&#x201d; at <b>1125</b>), then the textures requested in the request are obtained for each model part at <b>1150</b>. Here, the textures for each sub-part are obtained. These may be woodgrain for legs, textured cloth of a certain color for the arms and seat cushions, and a pattern for a backing.</p><p id="p-0096" num="0095">Next, a determination is made whether there are any changes to the model at <b>1155</b>. Here, if there are changes (&#x201c;yes&#x201d; at <b>1155</b>), then the process may begin again with obtaining the model portion and any connected portions, etc. If there are no changes (&#x201c;no&#x201d; at <b>1155</b>), then the rendering instructions are provided including the model, textures and instructions for the way in which those components are joined to one another for rendering at <b>1160</b>.</p><p id="p-0097" num="0096">As discussed above, the rendering may take place on the same device or on a user device. The model, textures and instructions are provided to the device that needs to perform the rendering.</p><p id="p-0098" num="0097">The process then ends at <b>1195</b>.</p><p id="p-0099" num="0098">Using these models, rather than a series of two-dimensional images enables secondary effects, such as depth from camera-based compositing. For a given perspective in the two-dimensional set of images, for each of the 32 perspectives, every pixel of every component (e.g., each leg, chair, arm, tabletop, etc.) must be individually identified as &#x201c;visible&#x201d; or &#x201c;not-visible&#x201d;from a given perspective. So, for example, part of a leg may be obscured by a skirt on a chair, or the chair cushion itself (given a perspective), etc. But, there may be thousands of permutations of each design, and performing this individual flagging for each component can be time-consuming.</p><p id="p-0100" num="0099">Using the three-dimensional version of this process, a depth-from-camera algorithm (e.g., ray tracing) easily can be used to determine which pixels are &#x201c;closest&#x201d; to the camera when it comes to render time (i.e., time to create the two-dimensional image that is passed over the web). In this way, the data behind the other data (i.e., the data with a greater depth-from-camera may be masked out. This enables the three-dimensional render to merely render those portions of the design (e.g., from the given perspective) that are relevant for the requested view (e.g., not showing the underneath of a chair or a part of a leg that is not visible). This speeds up render time and enables the eventual two-dimensional image of the object to be transmitted faster to a requesting user.</p><p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart of a process for applying realistic lighting effects to a generated three-dimensional model. The flow chart has both a start <b>1205</b> and an end <b>1295</b>, but the process may be cyclical in nature.</p><p id="p-0102" num="0101">This process may be called on-demand rendering of shadows and illumination. Using a similar technology to the depth from camera-based compositing, the application of textures to three-dimensional models results in &#x201c;generic-looking&#x201d; models. The models do not consider interactions with the room in which they appear, or other objects that may be placed on those models (e.g., a brightly-colored pillow or shadow cast by the arm on the base of the couch, etc.). To combat this, and to provide a more realistic application of expected light interactions, ray tracing or similar technology may be used to dynamically &#x201c;place&#x201d; light into a space in which the object appears. Then, the visible rays of light may automatically interact with the model (e.g., after its position is determined and its textures and components selected). So, for contrast objects like light couches or reflections on tables and nearby dark objects, the ray tracing will make those light interactions that human vision experiences more lifelike. Similarly, a light perspective will enable shadows to intelligently be cast in the image in a given two-dimensional render of a piece of furniture or the like. In some cases, a light source may be estimated from a real-time image captured by an augmented reality device to provide that light-source relationship information to the render pipeline. The ray tracing technology is applied at the render stage to provide a more realistic image, from the selected perspective, of the model and textures.</p><p id="p-0103" num="0102">To avoid application of ray tracing to all aspects of the model, the depth-from-camera algorithm can be used to mask out the portions of the model and texture which are not visible from the chosen perspective prior to application of the ray tracing technology. In that way, the model and textures may be significantly simplified for application of ray tracing. Basically, only visible portions of the furniture (or other object) need have ray tracing applied. The ray tracing may even start from those visible pixel portions, rather than a reflecting light from a particular source, as those pixels may be easier to identify using the depth-from-camera algorithms.</p><p id="p-0104" num="0103">Following the start <b>1205</b>, the process begins with performance of the render operation at <b>1210</b>. Here, the model is rendered according to <figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref>.</p><p id="p-0105" num="0104">Thereafter, light and perspective positions are determined. Here, a virtual &#x201c;camera&#x201d; is placed in the world according to the request for modelling received. There may be a typical neutral position for this camera, or it may be incredibly specific (e.g. for augmented reality operations). Likewise, light may be evenly sourced from above or from the front. Alternatively, lighting may be applied in a special way from an odd angle. This may be a part of the instructions or may be automatically detected by a requesting device (e.g. a user mobile computing device <b>150</b>, <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0106" num="0105">Next, the relative depth of model portions is determined at <b>1230</b>. Here, there may be multiple determinations made. First, depth from field from the position of the camera itself may be determined. In this way, the renderer can determine which elements of the model are simply not visible at all. If they are not visible, then they need not be rendered at all, and they can be masked out of further consideration. In addition, a depth from field determination may be made from the one or more light sources to generate appropriate lighting for the model from those sources (relative to the camera location).</p><p id="p-0107" num="0106">Finally, the shadow and light sources are generated and positioned relative of the model at <b>1240</b>. Here, the lighting effects can be applied intelligently using the depth from field information. Shadows can be appropriately positioned for a given room (or virtual room) based upon the camera and lighting positions. This may be particularly useful for textured objects, or unusual model shapes for certain objects. The shadows may cast appropriately by using this depth from field from multiple perspectives (e.g. light and camera).</p><p id="p-0108" num="0107">Finally, the process ends at <b>1295</b>.</p><p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart of a process for applying appropriate visual warp to translucent and transparent three-dimensional models. The flow chart has both a start <b>1305</b> and an end <b>1395</b>, but the process may be cyclical in nature.</p><p id="p-0110" num="0109">Transparent or translucent objects may incorrectly interact with real-time rendered objects. So, for example, a glass vase or drinking glass or nearby curved window may throw shadows and may bend or otherwise interact with a three-dimensional model to which a texture has been applied. At the render stage, that information may be incorporated. The render may take place as usual, but as a final pass, ray tracing may be applied to the model to detect transparent, translucent or partially transparent or translucent objects and to cause appropriate interaction of light with the selected texture and model. Depth from field is relevant here, as distant objects are just blurrier or disrupted more than objects nearby to the transparent or translucent surface. But, ray tracing enables those interactions to be &#x201c;baked into&#x201d; the image that is transmitted back to a requesting user following a real-time render of that three-dimensional model.</p><p id="p-0111" num="0110">This likewise may be done intelligently, meaning the pixels that have no transparency or translucency can be ignored for purposes of this ray tracing. As is generally known, ray tracing is a processor-intensive prospect, so it slows rendering down. To perform this function, the pixels that do not involve transparency may be detected (or pre-determined) and those may be processed normally. Then, only the transparent pixels may be processed using the ray tracing technology. This significantly cuts render time, and still enables the appropriate interaction of the light and transparency with the three-dimensional model before it is rendered and sent to a requesting user.</p><p id="p-0112" num="0111">Here, following the start <b>1305</b>, the process begins with performance of render operations at <b>1310</b>, determining perspective view information at <b>1320</b>, and detecting relative depth of model portions <b>1330</b>. These steps are similar to those described in <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0113" num="0112">However, at <b>1330</b>, transparent portions may be detected specifically. They may have an associated transparency flag or other setting identifying those pixels as such at render time. Those pixels may be purely transparent or may be refractory (e.g. curved glass).</p><p id="p-0114" num="0113">For any detected curved glass or similar transparency (colored glass could alter this as well), model warp for objects detected to be &#x201c;behind&#x201d; that glass may be applied using ray tracing or similar depth from field technology at <b>1340</b>. The tracing of light through even warped glass is one of the applications of depth from field and ray tracing technology. But, only those portions flagged as transparent or semi-transparent need be rendered this way. This is particularly useful for situations in which augmented reality may be used and an object on a table (e.g. a glass vase) is involved. The object may be realistically warped using ray tracing.</p><p id="p-0115" num="0114">The process then ends at <b>1395</b>.</p><heading id="h-0008" level="1">CLOSING COMMENTS</heading><p id="p-0116" num="0115">Throughout this description, the embodiments and examples shown should be considered as exemplars, rather than limitations on the apparatus and procedures disclosed or claimed. Although many of the examples presented herein involve specific combinations of method acts or system elements, it should be understood that those acts and those elements may be combined in other ways to accomplish the same objectives. With regard to flowcharts, additional and fewer steps may be taken, and the steps as shown may be combined or further refined to achieve the methods described herein. Acts, elements and features discussed only in connection with one embodiment are not intended to be excluded from a similar role in other embodiments.</p><p id="p-0117" num="0116">As used herein, &#x201c;plurality&#x201d; means two or more. As used herein, a &#x201c;set&#x201d; of items may include one or more of such items. As used herein, whether in the written description or the claims, the terms &#x201c;comprising&#x201d;, &#x201c;including&#x201d;, &#x201c;carrying&#x201d;, &#x201c;having&#x201d;, &#x201c;containing&#x201d;, &#x201c;involving&#x201d;, and the like are to be understood to be open-ended, i.e., to mean including but not limited to. Only the transitional phrases &#x201c;consisting of&#x201d; and &#x201c;consisting essentially of&#x201d;, respectively, are closed or semi-closed transitional phrases with respect to claims. Use of ordinal terms such as &#x201c;first&#x201d;, &#x201c;second&#x201d;, &#x201c;third&#x201d;, etc., in the claims to modify a claim element does not by itself connote any priority, precedence, or order of one claim element over another or the temporal order in which acts of a method are performed, but are used merely as labels to distinguish one claim element having a certain name from another element having a same name (but for use of the ordinal term) to distinguish the claim elements. As used herein, &#x201c;and/or&#x201d; means that the listed items are alternatives, but the alternatives also include any combination of the listed items.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for real-time compositing, rendering and delivery of complex geometric models and textures, comprising:<claim-text>first data storage for storing a plurality of three-dimensional models of at least two sub-parts of a whole three-dimensional object, the plurality of three-dimensional models combinable in a pre-determined fashion based upon a ruleset into the whole three-dimensional object;</claim-text><claim-text>second data storage for storing a plurality of image textures for each of the plurality of three-dimensional models; and</claim-text><claim-text>a computing device for:<claim-text>receiving instructions from a user, the instructions comprising a selection of at least two of the plurality of three-dimensional models, each of the at least two of the plurality of three-dimensional models being one of the at least two sub-parts of the whole three-dimensional object,</claim-text><claim-text>wherein the instructions further comprise a selection of at least one of the plurality of image textures for each of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object; and</claim-text><claim-text>generating the whole three-dimensional object comprised of the at least one of the plurality of image textures for each of the at least two of the plurality of three-dimensional models applied according to the instructions to the at least two of the plurality of three-dimensional models.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the user providing the instructions is using a second computing device remote from the computing device.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>rendering the three-dimensional object using a graphics processor to generate a two-dimensional image of the three-dimensional object from a perspective within three-dimensional space, the perspective included within the instructions; and</claim-text><claim-text>transmitting the two-dimensional image to a second computing device remote from the computing device over a network for display on the second computing device.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>transmitting the at least two of the plurality of three-dimensional models making up the whole three-dimensional object and the at least one of the plurality of image textures for each of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object over a network to a second computing device along with rendering instructions;</claim-text><claim-text>rendering the whole three-dimensional object using the second computing device to thereby generate a two-dimensional image of the whole three-dimensional object from a perspective within three-dimensional space, the perspective provided by the second computing device and updated as desired by the user of the second computing device; and</claim-text><claim-text>displaying the rendered whole three-dimensional object on a display.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>compositing the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from a perspective;</claim-text><claim-text>detecting a relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from the perspective to thereby determine which portions of the at least two of the plurality of three-dimensional models overlay other portions based upon the perspective; and</claim-text><claim-text>identifying the other portions in the instructions as not necessary to be rendered.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>compositing the at least two of the plurality of three-dimensional models making up the whole three-dimensional object;</claim-text><claim-text>detecting a relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from a perspective comprising a light source; and</claim-text><claim-text>generating shadows based upon the relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object and the perspective of the light source.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>compositing the at least two of the plurality of three-dimensional models making up the whole three-dimensional object;</claim-text><claim-text>detecting a relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from a perspective;</claim-text><claim-text>tracing light transmission through one or more transparent or translucent portions of the whole three-dimensional object; and</claim-text><claim-text>generating distortion of the whole three-dimensional object or an object behind the whole three-dimensional object, based upon the light transmission traced through the translucent portions of the whole three-dimensional object.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A non-volatile machine-readable medium storing a program having instructions which when executed by a processor will cause the processor to:<claim-text>store a plurality of three-dimensional models of at least two sub-parts of a whole three-dimensional object in a first data storage, the plurality of three-dimensional models combinable in a pre-determined fashion based upon a ruleset into the whole three-dimensional object;</claim-text><claim-text>store a plurality of image textures for each of the plurality of three-dimensional models in a second data storage;</claim-text><claim-text>receive instructions from a user, the instructions comprising a selection of at least two of the plurality of three-dimensional models, each of the at least two of the plurality of three-dimensional models being one of the at least two sub-parts of the whole three-dimensional object,</claim-text><claim-text>wherein the instructions further comprise a selection of at least one of the plurality of image textures for each of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object; and</claim-text><claim-text>generate the whole three-dimensional object comprised of the at least one of the plurality of image textures for each of the at least two of the plurality of three-dimensional models applied according to the instructions to the at least two of the plurality of three-dimensional models.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the user providing the instructions uses a computing device remote from the processor.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>rendering the whole three-dimensional object using a graphics processor to thereby generate a two-dimensional image of the whole three-dimensional object from a perspective within three-dimensional space, the perspective included within the instructions; and</claim-text><claim-text>transmitting the two-dimensional image to a second computing device remote from the computing device over a network for subsequent display on the second computing device.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>transmitting the at least two of the plurality of three-dimensional models making up the whole three-dimensional object and the at least one of the plurality of image textures for each of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object over a network to a second computing device along with rendering instructions;</claim-text><claim-text>rendering the three-dimensional object using the second computing device to thereby generate a two-dimensional image of the whole three-dimensional object from a perspective within three-dimensional space, the perspective provided by the second computing device and updated as desired by the user of the second computing device; and</claim-text><claim-text>displaying the rendered whole three-dimensional object on a display.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>compositing the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from a perspective;</claim-text><claim-text>detecting a relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from the perspective to determine which portions of the at least two of the plurality of three-dimensional models overlay other portions based upon the perspective; and</claim-text><claim-text>identifying the other portions in the instructions as not necessary to be rendered.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>compositing the at least two of the plurality of three-dimensional models making up the whole three-dimensional object;</claim-text><claim-text>detecting a relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from a perspective comprising a light source; and</claim-text><claim-text>generating shadows based upon the relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object and the perspective of the light source.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>compositing the at least two of the plurality of three-dimensional models making up the whole three-dimensional object;</claim-text><claim-text>detecting a relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from a perspective;</claim-text><claim-text>tracing light transmission through one or more transparent or translucent portions of the whole three-dimensional object; and</claim-text><claim-text>generating distortion of the whole three-dimensional object or an object behind the whole three-dimensional object, based upon the light transmission traced through the translucent portions of the whole three-dimensional object.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref> further comprising:<claim-text>the processor; and</claim-text><claim-text>a memory,</claim-text><claim-text>wherein the processor and the memory comprise circuits and software for performing the instructions on the storage medium.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A method for real-time compositing, rendering and delivery of complex geometric models and textures, comprising:<claim-text>storing a plurality of three-dimensional models of at least two sub-parts of a whole three-dimensional object in a first data storage, the plurality of three-dimensional models combinable in a pre-determined fashion based upon a ruleset into the whole three-dimensional object;</claim-text><claim-text>storing a plurality of image textures for each of the plurality of three-dimensional models in a second data storage;</claim-text><claim-text>receiving instructions from a user, the instructions comprising a selection of at least two of the plurality of three-dimensional models, each of the at least two of the plurality of three-dimensional models being one of the at least two sub-parts of the whole three-dimensional object,</claim-text><claim-text>wherein the instructions further comprise a selection of at least one of the plurality of image textures for each of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object; and</claim-text><claim-text>generating the whole three-dimensional object comprised of the at least one of the plurality of image textures for each of the at least two of the plurality of three-dimensional models applied according to the instructions to the at least two of the plurality of three-dimensional models.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>transmitting the at least two of the plurality of three-dimensional models making up the whole three-dimensional object and the at least one of the plurality of image textures for each of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object over a network to a second computing device along with rendering instructions;</claim-text><claim-text>rendering the three-dimensional object using the second computing device to thereby generate a two-dimensional image of the whole three-dimensional object from a perspective within three-dimensional space, the perspective provided by the second computing device and updated as desired by the user of the second computing device; and</claim-text><claim-text>displaying the rendered whole three-dimensional object on a display.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>compositing the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from a perspective;</claim-text><claim-text>detecting a relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from the perspective to thereby determine which portions of the at least two of the plurality of three-dimensional models overlay other portions based upon the perspective; and</claim-text><claim-text>identifying the other portions in the instructions as not necessary to be rendered.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>compositing the at least two of the plurality of three-dimensional models making up the whole three-dimensional object;</claim-text><claim-text>detecting a relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from a perspective comprising a light source; and</claim-text><claim-text>generating shadows based upon the relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object and the perspective of the light source.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the generating the whole three-dimensional object comprises:<claim-text>compositing the at least two of the plurality of three-dimensional models making up the whole three-dimensional object;</claim-text><claim-text>detecting a relative depth of the at least two of the plurality of three-dimensional models making up the whole three-dimensional object from a perspective;</claim-text><claim-text>tracing light transmission through one or more transparent or translucent portions of the whole three-dimensional object; and</claim-text><claim-text>generating distortion of the whole three-dimensional object or an object behind the whole three-dimensional object, based upon the light transmission traced through the translucent portions of the whole three-dimensional object.</claim-text></claim-text></claim></claims></us-patent-application>