<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007167A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007167</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17756581</doc-number><date>20201028</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-222440</doc-number><date>20191209</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23219</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>251</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23299</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232935</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE PROCESSING DEVICE AND IMAGE PROCESSING SYSTEM, AND IMAGE PROCESSING METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>TOKYO</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>USAMI</last-name><first-name>SHINNOSUKE</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/040443</doc-number><date>20201028</date></document-id><us-371c12-date><date>20220527</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Provided are a device and method that calculate a predicted motion vector corresponding to a type and posture of a tracked subject, and generate a camera control signal necessary for capturing an image of a tracked subject. There are included a predicted subject motion vector calculation unit that detects a tracked subject of a previously designated type from a captured image input from an imaging unit and calculates a predicted motion vector corresponding to a type and posture of the detected tracked subject, and a camera control signal generation unit that generates, on the basis of the predicted motion vector calculated by the predicted subject motion vector calculation unit, a camera control signal for capturing an image of a tracked image of the tracked subject. By using a neural network or the like, the predicted subject motion vector calculation unit executes processing of detecting a tracked subject of a type designated from the captured image by a user, and predicted motion vector calculation processing.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="104.90mm" wi="158.75mm" file="US20230007167A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="211.50mm" wi="144.19mm" orientation="landscape" file="US20230007167A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="203.96mm" wi="149.94mm" orientation="landscape" file="US20230007167A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="225.89mm" wi="144.70mm" orientation="landscape" file="US20230007167A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="230.72mm" wi="156.97mm" orientation="landscape" file="US20230007167A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="237.41mm" wi="159.17mm" orientation="landscape" file="US20230007167A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="209.72mm" wi="153.75mm" orientation="landscape" file="US20230007167A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="223.10mm" wi="152.99mm" orientation="landscape" file="US20230007167A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="206.16mm" wi="153.33mm" orientation="landscape" file="US20230007167A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="226.48mm" wi="153.92mm" orientation="landscape" file="US20230007167A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="180.42mm" wi="116.33mm" orientation="landscape" file="US20230007167A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="176.11mm" wi="131.40mm" orientation="landscape" file="US20230007167A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="226.31mm" wi="148.76mm" orientation="landscape" file="US20230007167A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="231.22mm" wi="157.31mm" orientation="landscape" file="US20230007167A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="231.48mm" wi="159.68mm" orientation="landscape" file="US20230007167A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="183.30mm" wi="148.08mm" orientation="landscape" file="US20230007167A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to an image processing device and an image processing method, and a program. More specifically, the present disclosure relates to an image processing device and an image processing system, and an image processing method that perform subject tracking and image capturing.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">When an image of a moving subject is captured by using an imaging device (camera), an automatic tracking imaging device capable of capturing a moving image while tracking a subject is used.</p><p id="p-0004" num="0003">For example, the imaging device is attached to a camera platform device in which an image capturing direction can be freely set, and the camera platform device is driven to follow a motion of the subject to capture an image.</p><p id="p-0005" num="0004">Note that the automatic tracking imaging device is described in, for example, Patent Document 1 (Japanese Patent Application Laid-Open No. 2010-154391) or the like.</p><p id="p-0006" num="0005">With many automatic tracking imaging devices, in a case where a tracked image (moving image) of a subject is captured, a user (photographer) first performs processing of selecting the subject as a tracking target.</p><p id="p-0007" num="0006">For example, before starting the image capturing with a camera, the user checks a live view image (through image) displayed on a display unit of the camera and selects the tracking target subject.</p><p id="p-0008" num="0007">Specifically, for example, the user selects the subject as the tracking target in the live view image displayed on a touch-panel type display screen, and sets a frame surrounding an image area of the selected subject by using a finger. Regarding an image within the framed area as a template image, a control unit of the automatic tracking imaging device drives a camera platform so as to follow an image area similar to the template image. Through such processing, the tracked image is captured.</p><p id="p-0009" num="0008">However, in a case where the image area of the tracking target subject cannot be accurately set, it is difficult to capture an image while following the tracking target subject.</p><p id="p-0010" num="0009">For example, in a state where the camera is unsecured, such as in a case where the user (photographer) holds the camera and tries to capture a tracked image of &#x201c;bird&#x201d; as a subject while following the &#x201c;bird&#x201d;, it is difficult for the user to draw an accurate frame.</p><p id="p-0011" num="0010">Thus, there is a problem that it is extremely difficult to draw an accurate frame surrounding a subject as a target in a situation where both the subject and a photographer are moving.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Document</heading><p id="p-0012" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0011">Patent Document 1: Japanese Patent Application Laid-Open No. 2010-154391</li></ul></p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Problems to be Solved by the Invention</heading><p id="p-0013" num="0012">The present disclosure has been made in view of the above-described problem, for example, and an object thereof is to provide an image processing device and an image processing system, and an image processing method that are capable of capturing a tracked image of a tracking target subject as a target without a user (photographer) performing processing such as a frame setting on a display screen.</p><heading id="h-0007" level="1">Solutions to Problems</heading><p id="p-0014" num="0013">A first aspect of the present disclosure is</p><p id="p-0015" num="0014">an image processing device including</p><p id="p-0016" num="0015">a predicted subject motion vector calculation unit that detects a tracked subject corresponding to a previously designated type from a captured image, and calculates a predicted motion vector corresponding to a type and posture of the detected tracked subject, and</p><p id="p-0017" num="0016">a camera control signal generation unit that generates, on the basis of the predicted motion vector, a camera control signal for tracking the tracked subject.</p><p id="p-0018" num="0017">Moreover, a second aspect of the present disclosure is</p><p id="p-0019" num="0018">an image processing system including</p><p id="p-0020" num="0019">a camera mounted on a camera platform, and a camera platform control unit that controls the camera platform,</p><p id="p-0021" num="0020">in which the camera has</p><p id="p-0022" num="0021">a predicted subject motion vector calculation unit that detects a tracked subject corresponding to a previously designated type from a captured image, and</p><p id="p-0023" num="0022">calculates a predicted motion vector corresponding to a type and posture of the detected tracked subject, and</p><p id="p-0024" num="0023">a camera control signal generation unit that generates, on the basis of the predicted motion vector, a camera control signal for tracking the tracked subject, and</p><p id="p-0025" num="0024">the camera platform control unit</p><p id="p-0026" num="0025">executes, on the basis of the camera control signal, camera platform control for capturing an image of a tracked image of the tracked subject.</p><p id="p-0027" num="0026">Moreover, a third aspect of the present disclosure is</p><p id="p-0028" num="0027">an image processing method executed in an image processing device,</p><p id="p-0029" num="0028">in which a predicted subject motion vector calculation unit executes</p><p id="p-0030" num="0029">a predicted subject motion vector calculation step of detecting a tracked subject corresponding to a previously designated type from a captured image, and</p><p id="p-0031" num="0030">calculating a predicted motion vector corresponding to a type and posture of the detected tracked subject, and</p><p id="p-0032" num="0031">a camera control signal generation unit</p><p id="p-0033" num="0032">executes, on the basis of the predicted motion vector, a camera control signal generation step of generating a camera control signal for tracking the tracked subject.</p><p id="p-0034" num="0033">Still other objects, features, and advantages of the present disclosure will become apparent from more detailed description based on embodiments of the present disclosure described below and the accompanying drawings. Note that, in the present specification, a system is a logical set configuration of a plurality of devices, and is not limited to one in which devices of respective configurations are in the same housing.</p><p id="p-0035" num="0034">According to a configuration of an embodiment of the present disclosure, there are implemented a device and method that calculate a predicted motion vector corresponding to a type and posture of a tracked subject, and generate a camera control signal necessary for capturing an image of a tracked subject.</p><p id="p-0036" num="0035">Specifically, for example, there are included a predicted subject motion vector calculation unit that detects a tracked subject of a previously designated type from a captured image input from an imaging unit and calculates a predicted motion vector corresponding to a type and posture of the detected tracked subject, and a camera control signal generation unit that generates, on the basis of the predicted motion vector calculated by the predicted subject motion vector calculation unit, a camera control signal for capturing an image of a tracked image of the tracked subject. By using a neural network or the like, the predicted subject motion vector calculation unit executes processing of detecting a tracked subject of a type designated from the captured image by a user, and predicted motion vector calculation processing.</p><p id="p-0037" num="0036">With this configuration, there are implemented a device and method that calculate a predicted motion vector corresponding to a type and posture of a tracked subject, and generate a camera control signal necessary for capturing an image of a tracked subject.</p><p id="p-0038" num="0037">Note that the effects described herein are only examples and are not limited thereto, and additional effects may also be present.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0008" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram describing a configuration example of an automatic tracking imaging device that is an example of an image processing device according to the present disclosure.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram describing tracked subject type selection processing.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram describing an automatic tracking mode setting and image capturing start processing.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram describing tracked subject type selection processing using a communication terminal.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram describing an automatic tracking mode setting and image capturing start processing using a communication terminal.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart describing a sequence of processing executed by the image processing device according to the present disclosure.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart describing a sequence of processing executed by the image processing device according to the present disclosure.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram describing a specific example of tracked subject detection processing.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram describing a specific example of tracked subject detection processing.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram describing a specific example of a predicted motion vector of a tracked subject.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram describing a specific example of a predicted motion vector of a tracked subject.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram describing a configuration example of the image processing device according to the present disclosure.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram describing a specific example of processing executed by a predicted subject motion vector generation unit <b>110</b> of the image processing device according to the present disclosure.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram describing a specific example of processing executed by a predicted subject motion vector generation unit <b>110</b> of the image processing device according to the present disclosure.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram describing a specific example of processing executed by a camera control signal generation unit of the image processing device according to the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0009" level="1">MODE FOR CARRYING OUT THE INVENTION</heading><p id="p-0054" num="0053">Hereinafter, details of an image processing device and an image processing system, and an image processing method according to the present disclosure will be described with reference to the drawings. Note that the description will be given according to the following items.</p><p id="p-0055" num="0054">1. Overview of configuration of and processing by image processing device according to present disclosure</p><p id="p-0056" num="0055">2. Sequence of processing executed by image processing device according to present disclosure</p><p id="p-0057" num="0056">3. Details of image processing device configuration and predicted subject motion vector generation processing</p><p id="p-0058" num="0057">4. Conclusion of present disclosure</p><heading id="h-0010" level="1">1. OVERVIEW OF CONFIGURATION OF AND PROCESSING BY IMAGE PROCESSING DEVICE ACCORDING TO PRESENT DISCLOSURE</heading><p id="p-0059" num="0058">First, an overview of a configuration of and processing by the image processing device according to the present disclosure will be described.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating a configuration example of an automatic tracking imaging device <b>10</b> that is an example of the image processing device according to the present disclosure.</p><p id="p-0061" num="0060">The automatic tracking imaging device <b>10</b> has a camera <b>11</b>, a camera platform <b>12</b>, and a camera platform control unit <b>13</b>.</p><p id="p-0062" num="0061">The camera <b>11</b> is mounted on the camera platform <b>12</b>. The camera <b>11</b> is configured so that an image capturing direction thereof can be freely changed on the camera platform <b>12</b>.</p><p id="p-0063" num="0062">The camera platform control unit <b>13</b> drives and controls the camera platform <b>12</b> so as to direct the image capturing direction of the camera <b>11</b> in a direction of a preset tracked subject.</p><p id="p-0064" num="0063">Note that, in order to capture an image of a tracked subject, it is necessary to detect the tracked subject from an image captured by a camera, analyze a direction of the detected tracked subject, and continuously execute processing of matching an image capturing direction of the camera with the analyzed direction.</p><p id="p-0065" num="0064">For this processing, the following processing is required, for example.</p><p id="p-0066" num="0065">(1) Processing of detecting a tracked subject from an image captured by the camera <b>11</b>,</p><p id="p-0067" num="0066">(2) Processing of calculating a camera direction for capturing an image of the detected tracked subject,</p><p id="p-0068" num="0067">(3) Processing of generating a camera platform drive control signal for driving the camera in the calculated camera direction,</p><p id="p-0069" num="0068">(4) Camera platform drive processing based on the generated camera platform drive control signal</p><p id="p-0070" num="0069">These pieces of processing (1) to (4) are executed by either the camera <b>11</b> or the camera platform control unit <b>13</b>.</p><p id="p-0071" num="0070">For example, all pieces of the above-described processing (1) to (4) may be executed in the camera platform control unit <b>13</b>, or the above-described processing (1) to (3) may be executed in the camera <b>11</b>, and only the processing (4) may be executed in the camera platform control unit <b>13</b>.</p><p id="p-0072" num="0071">In a case where all pieces of the above-described processing (1) to (4) are executed in the camera platform control unit <b>13</b>, an image captured by the camera <b>11</b> is input to the camera platform control unit <b>13</b> via a signal line <b>14</b>.</p><p id="p-0073" num="0072">The camera platform control unit <b>13</b> executes the above-described processing (1) to (4) by using the captured image.</p><p id="p-0074" num="0073">Meanwhile, in a case where the above-described processing (1) to (3) is executed in the camera <b>11</b>, and only the processing (4) is executed in the camera platform control unit <b>13</b>, a camera platform drive control signal generated by the camera <b>11</b> is input to the camera platform control unit <b>13</b> via the signal line <b>14</b>.</p><p id="p-0075" num="0074">The camera platform control unit <b>13</b> executes the above-described processing (4) by using the camera platform drive control signal generated by the camera <b>11</b>.</p><p id="p-0076" num="0075">Note that, other than the above, whether either the camera <b>11</b> or the camera platform control unit <b>13</b> executes the above-described processing (1) to (4) can be set variously.</p><p id="p-0077" num="0076">Moreover, an external device, such as an external communication terminal or an external server, which is a server on cloud for example, may execute the above-described processing (1) to (3), while communicating with the camera <b>11</b> or the camera platform control unit <b>13</b>.</p><p id="p-0078" num="0077">As described above, with many conventional automatic tracking imaging devices, in a case where a tracked image (moving image) of a subject is captured, a user (photographer) first performs processing of selecting a subject as a tracking target.</p><p id="p-0079" num="0078">For example, the user selects the subject as the tracking target in the live view image (through image) displayed on a touch-panel type camera display screen, and sets a frame surrounding an image area of the selected subject by using a finger.</p><p id="p-0080" num="0079">Note that subject selection processing is not limited to operation on a touch-panel type camera display screen, and may be performed by using a physical operation unit such as an operation button.</p><p id="p-0081" num="0080">However, for example, in a state where the camera is unsecured, such as in a case where the user (photographer) holds the camera and tries to shoot a tracked image of &#x201c;bird&#x201d; as a subject while following the &#x201c;bird&#x201d;, it is difficult for the user to draw an accurate frame, as in (1) Usage example illustrated in the upper left of <figref idref="DRAWINGS">FIG. <b>1</b></figref></p><p id="p-0082" num="0081">Thus, it is extremely difficult to draw an accurate frame surrounding a subject as a target in a situation where both the subject and a photographer are moving.</p><p id="p-0083" num="0082">The image processing device according to the present disclosure solves this problem, for example, and is capable of capturing a tracked image of the subject as a target without the user (photographer) performing processing of setting a frame surrounding a tracked subject area.</p><p id="p-0084" num="0083">Furthermore, many of conventional methods including Patent Document 1 (Japanese Patent Application Laid-Open No. 2010-154391) described above and the like are configured to predict a moving position of a subject by utilizing a plurality of pieces of frame information, and occurrence of latency time is inevitable in order to improve prediction accuracy. Therefore, there is a possibility that a high-speed subject leaves a screen before capturing the subject and starting tracking.</p><p id="p-0085" num="0084">In contrast, a method according to the present disclosure is a method that enables image capturing of a tracked subject based on a result of estimating a posture of a tracking target subject in one frame without utilizing a plurality of pieces of frame information, and enables image capturing of the tracked subject without latency time.</p><p id="p-0086" num="0085">Hereinafter, an overview of processing executed by the image processing device according to the present disclosure will be described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref> and subsequent figures.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> are diagrams describing processing steps for capturing a moving image as a tracked image of a specific subject by using the automatic tracking imaging device <b>10</b> that is illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> that is an example of the image processing device according to the present disclosure.</p><p id="p-0088" num="0087">First, as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, by using a tracked subject type selection UI <b>16</b> displayed on a camera display unit <b>15</b>, the user (photographer) selects a type of the subject to be tracked.</p><p id="p-0089" num="0088">The UI illustrated in the figure displays icons with which the following subject types can be selected.</p><p id="p-0090" num="0089">(1) Human</p><p id="p-0091" num="0090">(2) Dog</p><p id="p-0092" num="0091">(3) Car</p><p id="p-0093" num="0092">(4) Cat</p><p id="p-0094" num="0093">(5) Bird</p><p id="p-0095" num="0094">(6) Ball</p><p id="p-0096" num="0095">Note that, in addition to these subject types, there are subject types that can be selected by the user, and, by sliding the screen right and left or up and down, the user can display icons indicating other various selectable subject types.</p><p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example in which the user (photographer) selects &#x201c;(5) Bird&#x201d; as a tracked subject type in the processing in (step S<b>11</b>).</p><p id="p-0098" num="0097">The tracked subject type information selected by the user (photographer) is input to a data processing unit of the image processing device (the automatic tracking imaging device <b>10</b>).</p><p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates next processing steps.</p><p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating a state where an image is displayed on the camera display unit <b>15</b>. The display image is a current image, a so-called live view image (LV image), before video recording is started.</p><p id="p-0101" num="0100">While viewing the live view image (LV image), the user first touches, at an arbitrary timing, an automatic tracking (Auto Lock ON) mode setting icon <b>17</b> (step S<b>12</b>).</p><p id="p-0102" num="0101">The touch operation of the automatic tracking (Auto Lock ON) mode setting icon <b>17</b> is input to the data processing unit of the image processing device (automatic tracking imaging device <b>10</b>), and the automatic tracking processing is started. Note that an automatic tracking target subject is the tracked subject type=&#x201c;bird&#x201d; designated in the above (step S<b>11</b>).</p><p id="p-0103" num="0102">Note that, although an image of a bird is captured on the live view image (LV image) in the example illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, it is not essential that the image of the bird is captured on the live view image (LV image) at this time point, and the automatic tracking (Auto Lock ON) mode setting icon <b>17</b> may be touched on the screen on which the image of the bird is not captured.</p><p id="p-0104" num="0103">Next, in (step S<b>13</b>), the user (photographer) touches an image capturing (video recording) start instruction icon <b>18</b>. The touch operation is input to the data processing unit of the image processing device (automatic tracking imaging device <b>10</b>), and image capturing (video recording) is started.</p><p id="p-0105" num="0104">Note that the processing of touching the image capturing (video recording) start instruction icon <b>18</b> can be executed at an arbitrary timing desired by the user (photographer). Although the image of the bird is captured on the live view image (LV image) in the example illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, it is also possible to start video recording by performing processing of touching the image capturing (video recording) start instruction icon <b>18</b> on the screen on which the image of the bird is not captured.</p><p id="p-0106" num="0105">Image capturing (video recording) is started by processing of touching the image capturing (video recording) start instruction icon <b>18</b>.</p><p id="p-0107" num="0106">Image capturing processing is executed in the automatic tracking mode, and image capturing is executed so as to track the tracked subject type=&#x201c;bird&#x201d; designated in the above (step S<b>11</b>). That is, camera platform control for moving the image capturing direction of the camera <b>11</b> is executed so as to track the tracked subject type=&#x201c;bird&#x201d;, and image capturing is performed.</p><p id="p-0108" num="0107">As described with reference to <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>, processing executed by the user (photographer) is the following processing.</p><p id="p-0109" num="0108">(S<b>11</b>) Processing of selecting tracked subject type</p><p id="p-0110" num="0109">(S<b>12</b>) Processing of setting automatic tracking mode to ON</p><p id="p-0111" num="0110">(S<b>13</b>) Processing of image capturing (video recording) start instruction</p><p id="p-0112" num="0111">With only these simple operations, it is possible to start capturing a tracked image of a subject of a type designated (selected) by the user.</p><p id="p-0113" num="0112">That is, with the configuration according to the present disclosure, it is not necessary for the user (photographer) to perform troublesome processing of framing the tracking target subject on the screen.</p><p id="p-0114" num="0113">Furthermore, a method according to the present disclosure is a method that enables image capturing of a tracked subject based on a result of estimating a posture of a tracking target subject in one frame without utilizing a plurality of pieces of frame information, and enables image capturing of the tracked subject without latency time.</p><p id="p-0115" num="0114">Note that processing that the image processing device according to the present disclosure executes on the basis of the user operation in the above-described (S<b>11</b>) to (S<b>13</b>) will be described in detail later.</p><p id="p-0116" num="0115">Note that another specific example of the image processing device according to the present disclosure will be described with reference to <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref>.</p><p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a communication terminal <b>30</b> such as a smartphone (smartphone) owned by the user (photographer).</p><p id="p-0118" num="0117">The communication terminal <b>30</b> has a configuration capable of communicating with at least either the camera <b>11</b> or the camera platform control unit <b>13</b>.</p><p id="p-0119" num="0118">A tracked subject type selection UI <b>31</b> similar to the tracked subject type selection UI described above with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref> is displayed on the communication terminal <b>30</b>.</p><p id="p-0120" num="0119"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example in which the user (photographer) selects &#x201c;(5) Bird&#x201d; as a tracked subject type in the processing in (step S<b>11</b>).</p><p id="p-0121" num="0120">The tracked subject type information selected by the user (photographer) is transmitted from the communication terminal <b>30</b> to the camera <b>11</b> or the camera platform control unit <b>13</b>.</p><p id="p-0122" num="0121"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates next processing steps executed by using the communication terminal <b>30</b>.</p><p id="p-0123" num="0122"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a state where an image captured by the camera <b>11</b> is displayed on the communication terminal <b>30</b>. The image captured by the camera <b>11</b> is transmitted to the communication terminal <b>30</b> via a communication unit of the camera <b>11</b> and displayed.</p><p id="p-0124" num="0123">The display image is a current image, a so-called live view image (LV image), before video recording is started.</p><p id="p-0125" num="0124">While viewing the live view image (LV image) displayed on the communication terminal <b>30</b>, the user first touches, at an arbitrary timing in (step S<b>12</b>), an automatic tracking (Auto Lock ON) mode setting icon <b>32</b> displayed on the communication terminal <b>30</b>.</p><p id="p-0126" num="0125">Information of the touch operation of the automatic tracking (Auto Lock ON) mode setting icon <b>32</b> is transmitted from the communication terminal <b>30</b> to the camera <b>11</b> or to the camera platform control unit <b>13</b>, and automatic tracking processing is started in the automatic tracking imaging device <b>10</b>. Note that an automatic tracking target subject is the tracked subject type=&#x201c;bird&#x201d; designated in the above (step S<b>11</b>).</p><p id="p-0127" num="0126">Next, in (step S<b>13</b>), the user (photographer) touches an image capturing (video recording) start instruction icon <b>33</b> displayed on the communication terminal <b>30</b>. This touch operation information is transmitted from the communication terminal <b>30</b> to the camera <b>11</b> or to the camera platform control unit <b>13</b>, and input to the data processing unit of the automatic tracking imaging device <b>10</b>, and image capturing (video recording) is started.</p><p id="p-0128" num="0127">Image capturing processing is executed in the automatic tracking mode, and image capturing is executed so as to track the tracked subject type=&#x201c;bird&#x201d; designated in the above (step S<b>11</b>). That is, camera platform control for moving the image capturing direction of the camera <b>11</b> is executed so as to track the tracked subject type=&#x201c;bird&#x201d;, and image capturing is performed.</p><p id="p-0129" num="0128">Thus, the user (photographer) may execute each of the following processing by utilizing a communication terminal such as a smartphone.</p><p id="p-0130" num="0129">(S<b>11</b>) Processing of selecting tracked subject type</p><p id="p-0131" num="0130">(S<b>12</b>) Processing of setting automatic tracking mode to ON</p><p id="p-0132" num="0131">(S<b>13</b>) Processing of image capturing (video recording) start instruction</p><p id="p-0133" num="0132">With only these simple operations, it is possible to start capturing a tracked image of a subject of a type designated (selected) by the user.</p><p id="p-0134" num="0133">That is, with the configuration according to the present disclosure, it is not necessary for the user (photographer) to perform troublesome processing of framing the tracking target subject on the screen.</p><p id="p-0135" num="0134">Furthermore, a method according to the present disclosure is a method that enables image capturing of a tracked subject based on a result of estimating a posture of a tracking target subject in one frame without utilizing a plurality of pieces of frame information, and enables image capturing of the tracked subject without latency time.</p><heading id="h-0011" level="1">2. SEQUENCE OF PROCESSING EXECUTED BY IMAGE PROCESSING DEVICE ACCORDING TO PRESENT DISCLOSURE</heading><p id="p-0136" num="0135">Next, a sequence of processing executed by the image processing device according to the present disclosure will be described.</p><p id="p-0137" num="0136">The flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart describing a sequence of processing executed by the image processing device according to the present disclosure.</p><p id="p-0138" num="0137">Note that the image processing device that executes the processing according to the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a device including the communication terminal <b>30</b> and either the automatic tracking imaging device <b>10</b> described above with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>3</b></figref> or the automatic tracking imaging device <b>10</b> described above with reference to <figref idref="DRAWINGS">FIGS. <b>4</b> to <b>5</b></figref>.</p><p id="p-0139" num="0138">Hereinafter, processing in each processing step in the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> will be sequentially described.</p><p id="p-0140" num="0139">(Step S<b>101</b>)</p><p id="p-0141" num="0140">First, a tracked subject type is designated in step S<b>101</b>.</p><p id="p-0142" num="0141">This processing corresponds to the processing described above with reference to <figref idref="DRAWINGS">FIG. <b>2</b> or <b>4</b></figref>.</p><p id="p-0143" num="0142">By using display data illustrated in <figref idref="DRAWINGS">FIG. <b>2</b> or <b>4</b></figref>, that is, a tracked subject type selection UI displayed on a display unit of the camera <b>11</b> or the communication terminal <b>30</b>, the user (photographer) selects a type of the subject to be tracked.</p><p id="p-0144" num="0143">The processing examples in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>4</b></figref> described above are examples in which the user (photographer) selects &#x201c;(5) Bird&#x201d; as the tracked subject type.</p><p id="p-0145" num="0144">The tracked subject type information selected by the user (photographer) is input to a data processing unit of the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>).</p><p id="p-0146" num="0145">(Step S<b>102</b>)</p><p id="p-0147" num="0146">Next, the automatic tracking mode is set to ON in step S<b>102</b>.</p><p id="p-0148" num="0147">This processing corresponds to the processing (in step S<b>12</b>) described above with reference to <figref idref="DRAWINGS">FIG. <b>3</b> or <b>5</b></figref>.</p><p id="p-0149" num="0148">The user (photographer) touches an automatic tracking mode setting icon displayed on the display unit of the camera <b>11</b> or the communication terminal <b>30</b> to set the automatic tracking mode to ON.</p><p id="p-0150" num="0149">Information indicating that the automatic tracking mode is set to ON is input to a data processing unit of the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>).</p><p id="p-0151" num="0150">The automatic tracking target subject is the tracked subject type designated in the above (step S<b>101</b>), which is &#x201c;bird&#x201d; for example.</p><p id="p-0152" num="0151">Note that, although the live view image (LV image) is displayed on the display unit of the camera <b>11</b> or the communication terminal <b>30</b> in <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>5</b></figref> described above, the live view image (LV image) may or may not be displayed.</p><p id="p-0153" num="0152">(Step S<b>103</b>)</p><p id="p-0154" num="0153">Next, an image capturing (video recording) start instruction is issued in step S<b>103</b>.</p><p id="p-0155" num="0154">This processing corresponds to the processing (in step S<b>13</b>) described above with reference to <figref idref="DRAWINGS">FIG. <b>3</b> or <b>5</b></figref>.</p><p id="p-0156" num="0155">The user (photographer) touches an image capturing (video recording) start instruction icon as described with reference to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>5</b></figref>. This touch operation information is input to the data processing unit of the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>).</p><p id="p-0157" num="0156">Note that the image capturing (video recording) start instruction can be executed at an arbitrary timing desired by the user (photographer). Although the captured image of the &#x201c;bird&#x201d; designated as the tracked subject is displayed on the live view image (LV image) in the examples illustrated in <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>5</b></figref>, it is also possible to start video recording by issuing an image capturing (video recording) start instruction at a screen position where the bird is not captured.</p><p id="p-0158" num="0157">The processing in steps S<b>101</b> to S<b>103</b> is processing involving user operation on a UI, and these pieces of user operation information are input to the data processing unit of the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>), and image (moving image) capturing processing in the automatic tracking mode is started.</p><p id="p-0159" num="0158">The tracking target is a subject matching the subject type selected in step S<b>101</b>, which is &#x201c;bird&#x201d; for example.</p><p id="p-0160" num="0159">(Step S<b>104</b>)</p><p id="p-0161" num="0160">When the following input processing are completed in steps S<b>101</b> to S<b>103</b>, the automatic tracking imaging device <b>10</b> starts image (moving image) capturing processing in the automatic tracking mode in step S<b>104</b> and subsequent steps.</p><p id="p-0162" num="0161">(S<b>101</b>) Processing of selecting tracked subject type</p><p id="p-0163" num="0162">(S<b>102</b>) Processing of setting automatic tracking mode to ON</p><p id="p-0164" num="0163">(S<b>103</b>) Processing of image capturing (video recording) start instruction</p><p id="p-0165" num="0164">First, in step S<b>104</b>, processing of detecting the tracked subject from within the captured image is executed.</p><p id="p-0166" num="0165">Here, the subject to be detected is a subject of the type designated by the user in step S<b>101</b>, which is &#x201c;bird&#x201d; for example.</p><p id="p-0167" num="0166">The data processing unit of the image processing device executes processing of detecting a subject matching the tracked subject type designated from the captured image by the user (photographer). Note that determination of whether or not the subject matching the tracked subject type designated by the user (photographer) is in the captured image can be executed as processing of determining the subject to be present (1) or not to be present (0). Alternatively, processing of applying a degree of reliability may be performed in which a certain degree of reliability (a degree of reliability of 0 to 100 or the like, for example) of a subject matching the tracked subject type is calculated, and the subject is determined to be detected in a case where it is determined to be a predetermined reliability threshold value or more.</p><p id="p-0168" num="0167">Note that this subject identification processing is executed by performing image analysis processing of the captured image. Specifically, for example, subject identification processing using training data is executed. Details of this processing will be described later.</p><p id="p-0169" num="0168">(Step S<b>105</b>)</p><p id="p-0170" num="0169">In step S<b>105</b>, it is determined whether or not the processing of detecting the tracked subject from within the captured image in step S<b>104</b> has been successful.</p><p id="p-0171" num="0170">In a case where it is determined to be successful, the processing proceeds to step S<b>106</b>.</p><p id="p-0172" num="0171">Meanwhile, in a case where it is determined that the processing has failed, the processing returns to step S<b>104</b>, and processing of detecting the tracked subject from a continuously captured image frame is executed.</p><p id="p-0173" num="0172">As described above, the subject to be detected is a subject of the type designated by the user in step S<b>101</b>, which is &#x201c;bird&#x201d; for example.</p><p id="p-0174" num="0173">When the &#x201c;bird&#x201d; is not detected in the captured image, it is determined in step S<b>105</b> that the processing of detecting the tracked subject from within the captured image has failed, the processing returns to step S<b>104</b>, and the detection processing is performed from the next image frame.</p><p id="p-0175" num="0174">However, for example, there may be a case where a plurality of subjects of the type designated by the user, for example, a plurality of &#x201c;birds&#x201d;, is detected from one image frame.</p><p id="p-0176" num="0175">In such a case, the user presets a tracking mode for which &#x201c;bird&#x201d; is to be set as the tracking target.</p><p id="p-0177" num="0176">An example of a sequence of setting the tracking mode will be described with reference to the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0178" num="0177">The processing flow illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is tracking mode setting processing executed by input by the user (photographer), as pre-processing before starting image capturing processing in a subject tracking mode according to the flow illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0179" num="0178">The tracking mode information set according to the processing illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is stored in a storage unit in the image processing device (the automatic tracking imaging device <b>10</b>), and success determination processing of processing of detecting the tracked subject from within the captured image in steps S<b>104</b> to S<b>105</b> in the flow in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is executed according to the set tracking mode.</p><p id="p-0180" num="0179">The processing in each step of the flow illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref> will be described.</p><p id="p-0181" num="0180">(Step S<b>201</b>)</p><p id="p-0182" num="0181">First, in step S<b>201</b>, the user decides whether or not to limit the number of the tracked subjects to one, and inputs selection information. Note that the UI for inputting the selection information is displayed on the display unit of the camera <b>11</b> or the communication terminal <b>30</b> for example, and the user inputs the selection information by using the displayed UI.</p><p id="p-0183" num="0182">In a case where the user selects a setting for limiting the number of the tracked subjects to one In step S<b>201</b>, the processing proceeds to step S<b>202</b>.</p><p id="p-0184" num="0183">Meanwhile, in a case where the user selects a setting for not limiting the number of the tracked subjects to one in step S<b>201</b>, the processing proceeds to step S<b>211</b>.</p><p id="p-0185" num="0184">(Step S<b>202</b>)</p><p id="p-0186" num="0185">In a case where the user selects a setting for limiting the number of the tracked subjects to one In step S<b>201</b>, the processing proceeds to step S<b>202</b>.</p><p id="p-0187" num="0186">In this case, in step S<b>202</b>, the user decides whether or not the tracked subject is a subject close to a center of the image, and inputs the selection information. Note that the UI for inputting the selection information is also displayed on the display unit of the camera <b>11</b> or the communication terminal <b>30</b> for example, and the user inputs the selection information by using the displayed UI.</p><p id="p-0188" num="0187">In a case where the user selects, in step S<b>202</b>, a setting in which the tracked subject is a subject close to the center of the image, the processing proceeds to step S<b>212</b>.</p><p id="p-0189" num="0188">Meanwhile, in a case where the user selects, in step S<b>202</b>, a setting in which the tracked subject is not a subject close to the center of the image, the processing proceeds to step S<b>213</b>.</p><p id="p-0190" num="0189">The selection information in steps S<b>201</b> and S<b>202</b> by the user is input to the data processing unit in the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>), and, on the basis of these pieces of input information, the data processing unit determines a subject tracking mode to be executed.</p><p id="p-0191" num="0190">There are following three types of subject tracking modes, for example.</p><p id="p-0192" num="0191">(A) Multiple subjects tracking mode</p><p id="p-0193" num="0192">(B) Center subject tracking mode</p><p id="p-0194" num="0193">(C) Largest subject tracking mode</p><p id="p-0195" num="0194">According to the selection information in steps S<b>201</b> and S<b>202</b> by the user, the data processing unit in the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>) decides which of the tracking modes (A) to (C) described above is executed.</p><p id="p-0196" num="0195">In a case where determination in step S<b>201</b> is No, the processing proceeds to step S<b>211</b>, and the tracking mode to be executed is set to</p><p id="p-0197" num="0196">(A) Multiple Subjects Tracking Mode.</p><p id="p-0198" num="0197">In a case where the determination in step S<b>201</b> is Yes and determination in step S<b>202</b> is Yes, the processing proceeds to step S<b>212</b>, and the tracking mode to be executed is set to</p><p id="p-0199" num="0198">(B) Center Subject Tracking Mode.</p><p id="p-0200" num="0199">In a case where the determination in step S<b>201</b> is Yes and the determination in step S<b>202</b> is No, the processing proceeds to step S<b>213</b>, and the tracking mode to be executed is set to</p><p id="p-0201" num="0200">(C) Largest Subject Tracking Mode.</p><p id="p-0202" num="0201">Processing in steps S<b>211</b> to S<b>213</b> will be described.</p><p id="p-0203" num="0202">(Step S<b>211</b>)</p><p id="p-0204" num="0203">In a case where the determination in step S<b>201</b> is No, that is, in a case where the user selects a setting for not limiting the number of the tracked subjects to one in step S<b>201</b>, the processing proceeds to step S<b>211</b>.</p><p id="p-0205" num="0204">In this case, the data processing unit of the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>) sets the tracking mode to be executed to</p><p id="p-0206" num="0205">(A) Multiple Subjects Tracking Mode.</p><p id="p-0207" num="0206">(A) Multiple subjects tracking mode is a mode in which, in a case where a plurality of subjects corresponding to the tracked subject type designated in step S<b>101</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is detected from the captured image, an area including all the subjects corresponding to the tracked subject type is set as the tracked subject.</p><p id="p-0208" num="0207">As illustrated in the image (step S<b>211</b>) in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in a case where a plurality of subjects (&#x201c;birds&#x201d; in this example) corresponding to the tracked subject type is detected in the captured image, an area including all of these subjects is set as the tracked subject.</p><p id="p-0209" num="0208">(Step S<b>212</b>)</p><p id="p-0210" num="0209">In a case where the determination in step S<b>201</b> is Yes and the determination in step S<b>202</b> is Yes, that is, in a case where the user selects the setting for limiting the number of the tracked subjects to one in step S<b>201</b>, and the user selects the setting in which the tracked subject is a subject close to the center of the image in step S<b>202</b>, the processing proceeds to step S<b>212</b>.</p><p id="p-0211" num="0210">In this case, the data processing unit of the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>) sets the tracking mode to be executed to</p><p id="p-0212" num="0211">(B) Center Subject Tracking Mode.</p><p id="p-0213" num="0212">(B) Center subject tracking mode is a mode in which, in a case where a plurality of subjects corresponding to the tracked subject type designated in step S<b>101</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is detected from the captured image, one subject, of the plurality of detected subjects, closest to the center of the captured image is set as the tracked subject.</p><p id="p-0214" num="0213">As illustrated in the image (step S<b>212</b>) in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in a case where a plurality of subjects (&#x201c;birds&#x201d; in this example) corresponding to the tracked subject type is detected in the captured image, one subject, of these detected subjects, closest to the center of the captured image is selected and set as the tracked subject.</p><p id="p-0215" num="0214">(Step S<b>213</b>)</p><p id="p-0216" num="0215">In a case where the determination in step S<b>201</b> is Yes and the determination in step S<b>202</b> is No, that is, in a case where the user selects the setting for limiting the number of the tracked subjects to one in step S<b>201</b>, and the user does not select the setting in which the tracked subject is a subject close to the center of the image in step S<b>202</b>, the processing proceeds to step S<b>213</b>.</p><p id="p-0217" num="0216">In this case, the data processing unit of the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>) sets the tracking mode to be executed to</p><p id="p-0218" num="0217">(C) Largest Subject Tracking Mode.</p><p id="p-0219" num="0218">(C) Largest subject tracking mode is a mode in which, in a case where a plurality of subjects corresponding to the tracked subject type designated in step S<b>101</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is detected from the captured image, one subject, of the plurality of detected subjects, having a largest image area is set as the tracked subject.</p><p id="p-0220" num="0219">As illustrated in the image (step S<b>213</b>) in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in a case where a plurality of subjects (&#x201c;birds&#x201d; in this example) corresponding to the tracked subject type is detected in the captured image, one subject, of these detected subjects, having the largest image area within the captured image is selected and set as the tracked subject.</p><p id="p-0221" num="0220">Tracking mode information set according to the processing flow illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is stored in the storage unit in the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>).</p><p id="p-0222" num="0221">The above-described success determination processing of processing of detecting the tracked subject from within the captured image in steps S<b>104</b> to S<b>105</b> in the flow in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is executed according to the set tracking mode.</p><p id="p-0223" num="0222">Note that, although three types of mode classification examples, (A) Multiple subjects tracking mode, (B) Center subject tracking mode, and (C) Largest subject tracking mode, have been described in the example illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, another different mode may be settable. For example, a mode setting such as a leading subject selection mode may be selectable in which a leading car or human in a race or the like is set as a tracked subject.</p><p id="p-0224" num="0223">A specific example of the processing of detecting the tracked subject from the captured image will be described with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0225" num="0224"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram describing an example of the processing in steps S<b>104</b> to S<b>105</b> in the flow illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, that is, the processing of detecting the tracked subject from within the captured image.</p><p id="p-0226" num="0225"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a captured image on which tracked subject detection processing is executed.</p><p id="p-0227" num="0226">Note that the example illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is an example of a case where the tracked subject type is set to &#x201c;bird&#x201d;.</p><p id="p-0228" num="0227">First, (State A) illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a state where a subject corresponding to the tracked subject type=bird is not detected in a captured image. In this case, determination in step S<b>105</b> is NO, and the processing returns to step S<b>104</b> to execute processing of detecting a bird, which is the tracked subject, from a continuously captured image.</p><p id="p-0229" num="0228">(State B) in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a state where a subject corresponding to the tracked subject type=bird is detected in the captured image. In this state, the determination in step S<b>105</b> is Yes, and the processing proceeds to step S<b>106</b>.</p><p id="p-0230" num="0229"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a processing example of a case where a plurality of subjects corresponding to the tracked subject type=bird is detected from the captured image.</p><p id="p-0231" num="0230">In step S<b>104</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref> (State <b>1</b>), first, an image area of a bird, which is a tracked subject candidate, is detected from a captured image on the basis of the tracked subject type=bird designated by the user (photographer) in step S<b>101</b>.</p><p id="p-0232" num="0231">In this example, two birds are detected, and these birds are tracked subject candidates.</p><p id="p-0233" num="0232">Next, as a second half processing in step S<b>104</b>, processing of deciding the tracked subject is performed according to a preset tracking mode.</p><p id="p-0234" num="0233">Here, it is assumed that</p><p id="p-0235" num="0234">(B) Center Subject Tracking Mode</p><p id="p-0236" num="0235">described above with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref> is set.</p><p id="p-0237" num="0236">According to the setting of (B) Center subject tracking mode, the data processing unit of the image processing device determines, of the two birds detected from the captured image, a bird close to a center of the image as the tracked subject. In this manner, the processing of deciding the tracked subject is executed.</p><p id="p-0238" num="0237">Returning to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the processing in step S<b>106</b> and subsequent steps will be described.</p><p id="p-0239" num="0238">(Step S<b>106</b>)</p><p id="p-0240" num="0239">In a case where it is determined in step S<b>105</b> that the processing of detecting the tracked subject from within the captured image in step S<b>104</b> has been successful, the processing proceeds to step S<b>106</b>.</p><p id="p-0241" num="0240">In step S<b>106</b>, the image processing device executes processing of calculating a predicted motion vector corresponding to a type and posture of the tracked subject.</p><p id="p-0242" num="0241">The type of a tracked subject is a subject type such as a bird, a human, or a dog, for example designated as a tracked subject by the user. The posture is a posture of a bird in a tree, a posture of a bird flying, a posture of a human walking, a posture of a human running, or the like. Furthermore, the posture also includes a size of the tracked subject in the image, and processing is performed in consideration of the size of the tracked subject in the image when the predicted motion vector is calculated.</p><p id="p-0243" num="0242">Note that the processing of calculating the predicted motion vector corresponding to the type and posture of the tracked subject is executed in the data processing unit in the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>).</p><p id="p-0244" num="0243">Details of the predicted motion vector calculation processing will be described later.</p><p id="p-0245" num="0244">Specific examples of the predicted motion vector corresponding to the type and posture of the tracked subject will be described with reference to <figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref>.</p><p id="p-0246" num="0245"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating a specific example of a predicted motion vector corresponding to one posture of &#x201c;bird&#x201d; in a case where the type of the tracked subject is &#x201c;bird&#x201d;.</p><p id="p-0247" num="0246">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, a vector extending in an upper right direction from a center position of the bird as the tracked subject is illustrated. This vector is a predicted motion vector of the bird as the tracked subject.</p><p id="p-0248" num="0247">A direction of the predicted motion vector is set to a direction corresponding to a predicted moving direction of the bird as the tracked subject, and length of the predicted motion vector is set to a length corresponding to a predicted moving velocity (80 Km/h) of the bird as the tracked subject.</p><p id="p-0249" num="0248">A specific example of the predicted motion vector calculation processing will be described later.</p><p id="p-0250" num="0249"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating a specific example of a predicted motion vector corresponding to one posture of &#x201c;human&#x201d; in a case where the type of the tracked subject is &#x201c;human&#x201d;.</p><p id="p-0251" num="0250">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a vector extending in a left horizontal direction from a center position of the human as the tracked subject is illustrated. This vector is a predicted motion vector of the human as the tracked subject.</p><p id="p-0252" num="0251">A direction of the predicted motion vector is set to a direction corresponding to a predicted moving direction of the human as the tracked subject, and length of the predicted motion vector is set to a length corresponding to a predicted moving velocity (38 Km/h) of the human as the tracked subject.</p><p id="p-0253" num="0252">Thus, in step S<b>106</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, processing of calculating a predicted motion vector corresponding to a type and posture of the tracked subject is executed.</p><p id="p-0254" num="0253">As described above, processing of calculating the predicted motion vector corresponding to the type and posture of the tracked subject is executed in the data processing unit in the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>).</p><p id="p-0255" num="0254">Processing of calculating a predicted motion vector corresponding to a type and posture of the tracked subject can be executed as processing using training data.</p><p id="p-0256" num="0255">Alternatively, a table on which a predicted motion vector corresponding to a type and posture of the tracked subject is recorded in advance may be stored in the storage unit, and a predicted motion vector may be acquired with reference to the table.</p><p id="p-0257" num="0256">Note that, although the processing of calculating a predicted motion vector according to a type and posture of the tracked subject is executed in units of, for example, frames of images captured by the camera <b>11</b> when calculating a predicted motion vector corresponding to a new image frame, processing of calculating a new predicted motion vector may be performed with reference to a predicted motion vector already calculated on the basis of a past frame such as a previous frame, or a measured motion vector reflecting an actual motion of the tracked subject, the actual motion being obtained from a past captured image.</p><p id="p-0258" num="0257">For example, for an initial frame at a time of tracking start, a predicted motion vector is calculated on the basis of a result of analyzing the initial frame, because the initial frame does not have information corresponding to a past frame. However, for a next frame and subsequent frames, processing may be performed in which a motion predicted motion vector calculated from a latest captured image frame is compared with a measured motion vector reflecting an actual motion of the tracked subject, the actual motion being obtained from a past captured image in the past, correction processing is performed so that the difference is reduced, and a final predicted motion vector is calculated.</p><p id="p-0259" num="0258">Furthermore, processing of setting weights to the predicted motion vector and a measured vector and calculating a final predicted motion vector by weight addition may be performed. As a use case, processing may be performed in which, in a case where the subject is in an irregular motion, the measured vector is weighted more, because accuracy is degraded if the predicted motion vector is weighted.</p><p id="p-0260" num="0259">(Step S<b>107</b>)</p><p id="p-0261" num="0260">Next, in step S<b>107</b>, the image capturing direction of the camera is controlled according to the predicted motion vector corresponding to the type and posture of the tracked subject, the predicted motion vector being calculated in step S<b>106</b>. That is, it is assumed that the tracked subject moves according to the predicted motion vector corresponding to the type and posture of the tracked subject calculated in step S<b>106</b>, and a camera direction control signal for changing the image capturing direction of the camera is generated so that the image capturing direction of the camera matches the moving direction.</p><p id="p-0262" num="0261">Moreover, the camera direction is changed according to the generated camera direction control signal.</p><p id="p-0263" num="0262">Note that, on the basis of the predicted motion vector corresponding to the type and posture of the tracked subject calculated in step S<b>106</b>, the data processing unit in the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>) calculates a camera image capturing direction movement vector for matching the moving position of the tracked subject and the image capturing direction of the camera <b>11</b>.</p><p id="p-0264" num="0263">Moreover, the data processing unit in the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>) decides a drive mode (drive direction and drive amount) of the camera platform <b>12</b>, the drive mode being necessary for changing the camera image capturing direction according to the calculated camera image capturing direction movement vector.</p><p id="p-0265" num="0264">Moreover, the data processing unit in the image processing device (the automatic tracking imaging device <b>10</b> or the communication terminal <b>30</b>) generates a camera platform drive control signal for driving the camera platform <b>12</b> according to the decided drive mode (drive direction and drive amount) of the camera platform <b>12</b>, and outputs the generated camera platform drive control signal to the camera platform control unit <b>13</b>. The camera platform control unit <b>13</b> executes drive of the camera platform <b>12</b> by applying the input camera platform drive control signal.</p><p id="p-0266" num="0265">Through these processing, in a case where the tracked subject moves according to the predicted motion vector corresponding to the type and posture of the tracked subject calculated in step S<b>106</b>, the image capturing direction of the camera is controlled so that the image capturing direction of the camera matches the moving direction.</p><p id="p-0267" num="0266">As a result, image capturing of a captured image (moving image) in which the tracked subject is tracked is possible.</p><p id="p-0268" num="0267">Note that, not only the image capturing direction of the camera, but also camera setting information, such as pan, tilt, or zoom for example, for capturing an optimum image of the tracked subject, may also be calculated, and the calculated information may also be output to the camera <b>11</b> or the camera platform control unit <b>13</b> to perform control.</p><p id="p-0269" num="0268">(Step S<b>108</b>)</p><p id="p-0270" num="0269">Finally, in step S<b>108</b>, it is determined whether or not the automatic tracking mode is set to OFF.</p><p id="p-0271" num="0270">In a case where the automatic tracking mode is set to OFF, the image capturing processing in the automatic tracking mode ends.</p><p id="p-0272" num="0271">In a case where the automatic tracking mode is not set to OFF, the processing returns to step S<b>104</b>, and the processing in steps S<b>104</b> to S<b>107</b> is repeated for a new captured image frame.</p><p id="p-0273" num="0272">By this repetitive processing, the image capturing processing in the automatic tracking mode is continuously executed.</p><heading id="h-0012" level="1">3. DETAILS OF IMAGE PROCESSING DEVICE CONFIGURATION AND PREDICTED SUBJECT MOTION VECTOR GENERATION PROCESSING</heading><p id="p-0274" num="0273">Next, details of a configuration of the image processing device and predicted subject motion vector generation processing will be described.</p><p id="p-0275" num="0274"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a configuration example of an image processing device <b>100</b> according to the present disclosure.</p><p id="p-0276" num="0275">The image processing device <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref> corresponds to a device obtained by combining the communication terminal <b>30</b> with the automatic tracking imaging device <b>10</b> described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>3</b></figref> or the automatic tracking device <b>10</b> described with reference to <figref idref="DRAWINGS">FIGS. <b>4</b> to <b>5</b></figref>.</p><p id="p-0277" num="0276">Note that the configuration diagram illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a configuration diagram obtained by extracting only a part related to processing of controlling image capturing direction of the camera executed by the image processing device according to the present disclosure. General configurations of the camera, communication terminal, and camera platform control unit are omitted.</p><p id="p-0278" num="0277">As illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the image processing device <b>100</b> has an input unit <b>101</b>, an imaging unit <b>102</b>, a predicted subject motion vector generation unit <b>110</b>, a camera control signal generation unit <b>121</b>, and a camera (camera platform) drive unit <b>122</b>.</p><p id="p-0279" num="0278">The predicted subject motion vector generation unit <b>110</b> has a tracked subject identification unit <b>111</b>, subject estimation training data <b>112</b>, a tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b>, and tracked subject type &#x26; posture-corresponding predicted motion vector estimation training data <b>114</b>.</p><p id="p-0280" num="0279">The input unit <b>101</b> inputs, for example, tracked subject type information.</p><p id="p-0281" num="0280">Specifically, as described above with reference to <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>4</b></figref>, by using the tracked subject type selection UI displayed on the display unit of the camera <b>11</b> or the display unit of the communication terminal <b>30</b>, the user selects and inputs a type of the subject to be tracked.</p><p id="p-0282" num="0281">Tracked subject type designation information <b>201</b> input from the input unit <b>101</b> is input to the predicted subject motion vector generation unit <b>110</b>.</p><p id="p-0283" num="0282">The imaging unit <b>102</b> corresponds to the imaging unit of the camera <b>11</b> in the configuration described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>5</b></figref>.</p><p id="p-0284" num="0283">The imaging unit <b>102</b> acquires a captured image such as a current image before start of video recording, which is a so-called live view image (LV image), or an image for video recording after the start of the video recording.</p><p id="p-0285" num="0284">A captured image <b>202</b> acquired by the imaging unit <b>102</b> is input to the predicted subject motion vector generation unit <b>110</b>.</p><p id="p-0286" num="0285">The predicted subject motion vector generation unit <b>110</b> inputs the tracked subject type designation information <b>201</b> from the input unit <b>101</b>, inputs the captured image <b>202</b> from the imaging unit <b>102</b>, and generates and outputs a tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b> by using these pieces of input information.</p><p id="p-0287" num="0286">The tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b> is a predicted motion vector corresponding to a type of the tracked subject, for example, a type of the tracked subject such as a bird, a dog, a human, or a ball, and a posture of the tracked subject.</p><p id="p-0288" num="0287">For example, in a case where a tracked subject type is a bird, a length of predicted motion vector of the bird in a tree is substantially zero, whereas in a case where the bird is flying in the sky, the predicted motion vector is a vector having substantially the same direction as an orientation of a head of the bird and having a length corresponding to flight velocity of the bird.</p><p id="p-0289" num="0288">In the predicted subject motion vector generation unit <b>110</b>, first, the tracked subject identification unit <b>111</b> detects a tracked subject from the captured image <b>202</b> by using the tracked subject type designation information <b>201</b> input from the input unit <b>101</b> and the captured image <b>202</b> input from the imaging unit <b>102</b>. Specifically, a tracked subject matching the tracked subject type designated by the user is detected from the captured image <b>202</b>.</p><p id="p-0290" num="0289">For example, in a case where the tracked subject type designated by the user is &#x201c;bird&#x201d;, processing of detecting &#x201c;bird&#x201d; from the captured image <b>202</b>, or the like is executed.</p><p id="p-0291" num="0290">For example, in a case where the tracked subject type designated by the user is &#x201c;human&#x201d;, processing of detecting &#x201c;human&#x201d; from the captured image <b>202</b>, or the like is executed.</p><p id="p-0292" num="0291">The tracked subject identification unit <b>111</b> executes subject identification processing by using the subject estimation training data <b>112</b> when performing processing of detecting the tracked subject.</p><p id="p-0293" num="0292">The subject estimation training data <b>112</b> is data accumulated by learning processing executed in advance, and is training data capable of accumulating image characteristic information of various moving subjects such as a human, a bird, a dog, a cat, and a ball, and estimating a type of a subject from an image.</p><p id="p-0294" num="0293">From the captured image <b>202</b> by utilizing a neural network configured by using the subject estimation training data <b>112</b> for example, the tracked subject identification unit <b>111</b> detects the tracking target, for example, &#x201c;bird&#x201d;, that matches the tracked subject type designated by the user.</p><p id="p-0295" num="0294">The tracked subject identification unit <b>111</b> detects an image area matching a tracked subject type, for example, &#x201c;bird&#x201d;, designated by the user (photographer) and outputs, as a tracked subject detection information <b>203</b>, a captured image in which &#x201c;label=bird&#x201d; is set for a detected image area.</p><p id="p-0296" num="0295">The detection information of the tracked subject identification unit <b>111</b> is output, as the tracked subject detection information <b>203</b>, to the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b>.</p><p id="p-0297" num="0296">The tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> inputs the tracked subject detection information <b>203</b> from the tracked subject identification unit <b>111</b>. The tracked subject detection information <b>203</b> is a captured image in which &#x201c;label=bird&#x201d; is set for the image area that matches the tracked subject, for example, &#x201c;bird&#x201d;, designated by the user. The image includes an image of a bird in a certain posture, for example, an image of a bird flying in the sky.</p><p id="p-0298" num="0297">On the basis of the input information, the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> calculates a predicted motion vector corresponding to the type and posture of the tracked subject included in the captured image <b>202</b>.</p><p id="p-0299" num="0298">The tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> calculates a predicted motion vector corresponding to a type of the tracked subject, for example, a type of the tracked subject such as a bird, a dog, a human, or a ball, and a posture of the tracked subject, that is, the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b>.</p><p id="p-0300" num="0299">The tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> executes vector calculation processing using the tracked subject type &#x26; posture-corresponding predicted motion vector estimation training data <b>114</b> when performing processing of calculating the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b>.</p><p id="p-0301" num="0300">The tracked subject type &#x26; posture-corresponding predicted motion vector estimation training data <b>114</b> is data accumulated by learning processing executed in advance, and is capable of accumulating motion vectors corresponding to types and postures of various moving subjects such as a human, a bird, a dog, a cat, and a ball, and estimating a motion vector of a subject from a type and posture of the subject detected from an image.</p><p id="p-0302" num="0301">The tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> calculates a predicted motion vector corresponding to the type and posture of the tracked subject included in the captured image <b>202</b> by using, for example, a neural network configured by using the tracked subject type &#x26; posture-corresponding predicted motion vector estimation training data <b>114</b>.</p><p id="p-0303" num="0302">Note that the processing of calculating the predicted motion vector corresponding to the type and posture of the tracked subject by the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> may be acquired from a table generated in advance, in addition to the processing using the above-described training data.</p><p id="p-0304" num="0303">That is, a table on which a predicted motion vector corresponding to a type and posture of the tracked subject is recorded in advance may be stored in the storage unit, and a predicted motion vector may be acquired with reference to the table.</p><p id="p-0305" num="0304">The tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b> calculated by the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> is output to the camera control signal generation unit <b>121</b>.</p><p id="p-0306" num="0305">The camera control signal generation unit <b>121</b> generates a control signal for controlling the image capturing direction of the camera <b>11</b> by using the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b> calculated by the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b>.</p><p id="p-0307" num="0306">That is, a camera direction control signal for setting, according to the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b>, an image capturing direction of the camera to a position to which the tracked subject moves is generated. Note that the camera direction control signal specifically corresponds to a drive control signal for the camera platform <b>12</b> that controls the image capturing direction of the camera <b>11</b>.</p><p id="p-0308" num="0307">The camera direction control signal generated by the camera control signal generation unit <b>121</b> is input to the camera (camera platform) drive unit <b>122</b>.</p><p id="p-0309" num="0308">Note that the camera (camera platform) drive unit <b>122</b> is a component of the camera platform control unit <b>13</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0310" num="0309">The camera (camera platform) drive unit <b>122</b> drives the camera platform on the basis of the camera direction control signal generated by the camera control signal generation unit <b>121</b>.</p><p id="p-0311" num="0310">That is, the camera platform is driven so that the image capturing direction of the camera matches, according to the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b>, the position to which the tracked subject moves.</p><p id="p-0312" num="0311">With this processing, the image capturing direction of the camera moves according to movement of the tracked subject designated by the user (photographer), and a tracked image (moving image) of the tracked subject can be captured.</p><p id="p-0313" num="0312">Next, specific processing examples of processing executed by the predicted subject motion vector generation unit <b>110</b>, that is, processing of generating a type of the tracked subject, for example, a type of the tracked subject such as a bird, a dog, a human, or a ball, and the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b> that is a predicted motion vector corresponding to a posture of the tracked subject, will be described with reference to <figref idref="DRAWINGS">FIGS. <b>13</b> and <b>14</b></figref>.</p><p id="p-0314" num="0313"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates a processing example of a case where the type of the tracked subject designated by the user (photographer) is &#x201c;bird&#x201d;.</p><p id="p-0315" num="0314"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a processing example of a case where the type of the tracked subject designated by the user (photographer) is &#x201c;human&#x201d;.</p><p id="p-0316" num="0315">First, a processing example of a case where the type of the tracked subject designated by the user (photographer) is &#x201c;bird&#x201d; will be described with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0317" num="0316"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates the tracked subject identification unit <b>111</b> and the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> that are components of the predicted subject motion vector generation unit <b>110</b>. Each of them has a neural network generated by training data.</p><p id="p-0318" num="0317">The tracked subject identification unit <b>111</b> has a designated subject type detection neural network generated by the training data.</p><p id="p-0319" num="0318">Note that the designated subject type detection neural network is a neural network that inputs the tracked subject type designation information <b>201</b> from the input unit <b>101</b> and the captured image <b>202</b> from the imaging unit <b>102</b> and outputs the tracked subject detection information <b>203</b>.</p><p id="p-0320" num="0319">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the tracked subject identification unit <b>111</b> detects an image area matching the tracked subject type, &#x201c;bird&#x201d;, designated from the captured image <b>202</b> by the user (photographer) utilizing the neural network, and outputs, as the tracked subject detection information <b>203</b>, a captured image in which &#x201c;label=bird&#x201d; is set for a detected image area.</p><p id="p-0321" num="0320">The tracked subject detection information <b>203</b> generated by the tracked subject identification unit <b>111</b> is output to the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b>.</p><p id="p-0322" num="0321">The tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> has a bird posture-corresponding predicted motion vector estimation neural network generated by training data.</p><p id="p-0323" num="0322">This neural network is a neural network that estimates, with a subject type=bird, a predicted motion vector corresponding to a posture of a bird.</p><p id="p-0324" num="0323">Note that the neural network may be set, for example, in units of bird types. For example, a neural network in units of bird types such as a pigeon, a sparrow, and a swan may be applied. For example, processing of estimating a predicted motion vector corresponding to the posture of the pigeon may be performed with a subject type=pigeon.</p><p id="p-0325" num="0324">Note that the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> has a predicted motion vector estimation neural network corresponding to a subject type (human, bird, dog . . . ), and utilizes a neural network while switching the neural network according to the subject type (human, bird, dog . . . ) designated as the tracking target by the user.</p><p id="p-0326" num="0325">Note that, although the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> performs processing of identifying a tracked subject type and posture-corresponding predicted motion vector calculation processing in the present embodiment, processing of identifying a tracked subject type and posture-corresponding predicted motion vector calculation processing may be performed by individual processing units. Furthermore, these pieces of processing may be performed by different devices. For example, the type of the subject (whether or not the subject is a subject designated to be tracked) may be identified on a camera side, and motion prediction may be performed in another information processing device such as an external communication terminal or a server on a cloud side by using the information.</p><p id="p-0327" num="0326">The tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> inputs the tracked subject detection information <b>203</b> input from the tracked subject identification unit <b>111</b>, that is, a captured image in which &#x201c;label=bird&#x201d; is set for an image area that matches &#x201c;bird&#x201d;, and, by utilizing a neural network, calculates the type of the tracked subject (bird) and the predicted motion vector corresponding to a posture thereof, that is, the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b>.</p><p id="p-0328" num="0327">As illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> calculates, as the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b>, a vector having an orientation in a forward direction of the bird flying in the sky as the tracked subject and having a length corresponding to the flight velocity of the bird.</p><p id="p-0329" num="0328">The tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b> is output to the camera control signal generation unit <b>121</b>. The camera control signal generation unit <b>121</b> uses the predicted motion vector to generate a camera direction control signal for setting the image capturing direction of the camera to a position to which the tracked subject moves according to the predicted motion vector.</p><p id="p-0330" num="0329">Next, a processing example of a case where the type of the tracked subject designated by the user (photographer) is &#x201c;human&#x201d; will be described with reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>.</p><p id="p-0331" num="0330">Similarly to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, <figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates the tracked subject identification unit <b>111</b> and the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> that are components of the predicted subject motion vector generation unit <b>110</b>. Each of them has a neural network generated by training data.</p><p id="p-0332" num="0331">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the tracked subject identification unit <b>111</b> detects an image area matching the tracked subject type, &#x201c;human&#x201d;, designated from the captured image <b>202</b> by the user (photographer) utilizing the neural network, and outputs, as the tracked subject detection information <b>203</b>, a captured image in which &#x201c;label=human&#x201d; is set for a detected image area.</p><p id="p-0333" num="0332">The tracked subject detection information <b>203</b> generated by the tracked subject identification unit <b>111</b> is output to the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b>.</p><p id="p-0334" num="0333">The tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> has a &#x201c;human&#x201d; posture-corresponding predicted motion vector estimation neural network generated by training data.</p><p id="p-0335" num="0334">This neural network is a neural network that estimates a predicted motion vector corresponding to various types of humans and a posture of a human. The types of human are types such as an adult, a child, a male, and a female, and the posture of a human is, for example, a walking posture, a running posture, or the like.</p><p id="p-0336" num="0335">Note that the neural network can also generate and utilize a neural network for each specific individual as a type of human. It is possible to generate a neural network that learns motions corresponding to various postures of a certain person A and estimates predicted motion vectors corresponding to the various postures of the certain person A on the basis of the training data.</p><p id="p-0337" num="0336">In a case where the certain person A is detected from the captured image, it is possible to estimate a highly accurate predicted motion vector based on data unique to the person A by applying a neural network unique to the person A.</p><p id="p-0338" num="0337">The tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> inputs the tracked subject detection information <b>203</b> input from the tracked subject identification unit <b>111</b>, that is, a captured image in which &#x201c;label=human&#x201d; is set for an image area, which matches &#x201c;human&#x201d; and is a tracked subject selected according to a previously designated subject tracking mode, for example, a leading subject tracking mode, and, by utilizing a neural network, calculates the type of the tracked subject (human) and the predicted motion vector corresponding to a posture thereof, that is, the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b>.</p><p id="p-0339" num="0338">As illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b> calculates, as the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b>, a vector having an orientation in a forward direction of the human in a running posture as the tracked subject and having a length corresponding to the velocity of the human running.</p><p id="p-0340" num="0339">The tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b> is output to the camera control signal generation unit <b>121</b>. The camera control signal generation unit <b>121</b> uses the predicted motion vector to generate a camera direction control signal for setting the image capturing direction of the camera to a position to which the tracked subject moves according to the predicted motion vector.</p><p id="p-0341" num="0340">Next, a specific example of processing executed by the camera control signal generation unit <b>121</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0342" num="0341">As described above with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the camera control signal generation unit <b>121</b> generates a control signal for controlling the image capturing direction of the camera <b>11</b> by using the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b> calculated by the tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b>.</p><p id="p-0343" num="0342">That is, a camera direction control signal for setting, according to the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b>, an image capturing direction of the camera to a position to which the tracked subject moves is generated. Note that the camera direction control signal specifically corresponds to a drive control signal for the camera platform <b>12</b> that controls the image capturing direction of the camera <b>11</b>.</p><p id="p-0344" num="0343"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram illustrating a processing example of the camera control signal generation unit <b>121</b>. <figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates a processing example of a case where the tracked subject is a bird.</p><p id="p-0345" num="0344">A current frame (n), and a next frame (n+1) to be captured by moving the camera <b>11</b> according to movement of the tracked subject are illustrated.</p><p id="p-0346" num="0345">&#x201c;Tracked subject type &#x26; posture-corresponding predicted motion vector&#x201d; is indicated by a solid line in the forward and upward direction from the bird, which is a tracked subject detected in the current frame (n).</p><p id="p-0347" num="0346">The &#x201c;tracked subject type &#x26; posture-corresponding predicted motion vector&#x201d; illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a vector calculated on the basis of the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b> calculated by the subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b>, and corresponds to a vector indicating a movement destination of the subject within one frame between image capturing frames (n) and (n+1) of the camera <b>11</b>.</p><p id="p-0348" num="0347">In the next frame (n+1), the subject is predicted to move to a position of the bird indicated by a dotted line as illustrated in the figure, and in order to capture an image of the bird at the position of the bird indicated by the dotted line in the next frame (n+1), it is necessary to move the image capturing direction of the camera so as to capture the next frame (n+1) indicated by the dotted line frame in the figure.</p><p id="p-0349" num="0348">For this purpose, it is necessary to change the image capturing direction of the camera <b>11</b> from a center of the current frame (n) image, the center being indicated by a black circle in the figure, to a center of the next frame (n+1) image, the center being indicated by a dotted-line circle in the figure.</p><p id="p-0350" num="0349">A direction control amount is defined by the dotted-line arrow illustrated in the drawing, that is, a camera image capturing direction movement vector.</p><p id="p-0351" num="0350">Note that the processing example illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref> is an example, and the image processing device according to the present disclosure can predict a movement vector of the tracked subject and control the subject to be at a target position (for example, a center position of an image) including the information. Therefore, it is also possible to set a bird position to a position other than the position of the bird indicated by the dotted line in the frame (n+1) illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref>. Furthermore, the image capturing direction of the camera <b>11</b> can be variously set other than the setting of changing to the center of the next frame (n+1) image indicated by the dotted-line circle in <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0352" num="0351">The camera control signal generation unit <b>121</b> calculates the camera image capturing direction movement vector on the basis of the tracked subject type &#x26; posture-corresponding predicted motion vector <b>204</b> calculated by the subject type &#x26; posture-corresponding predicted motion vector calculation unit <b>113</b>, and moves the image capturing direction of the camera according to the vector. With this processing, it is possible to capture a tracked image in which a tracked subject designated by the user is captured within each captured image frame.</p><heading id="h-0013" level="1">4. CONCLUSION OF PRESENT DISCLOSURE</heading><p id="p-0353" num="0352">Hereinabove, the embodiment according to the present disclosure have been described in detail with reference to the specific embodiment. However, it is obvious that those skilled in the art may make modifications or substitutions to the embodiment without departing from the scope of the present disclosure. That is to say, the present invention has been disclosed in a form of exemplification, and should not be interpreted to be limited. In order to determine the scope of the present disclosure, the claims should be taken into consideration.</p><p id="p-0354" num="0353">Note that the technology disclosed in the present specification can have the following configurations.</p><p id="p-0355" num="0354">(1) An image processing device including</p><p id="p-0356" num="0355">a predicted subject motion vector calculation unit that detects a tracked subject corresponding to a previously designated type from a captured image, and</p><p id="p-0357" num="0356">calculates a predicted motion vector corresponding to a type and posture of the detected tracked subject, and</p><p id="p-0358" num="0357">a camera control signal generation unit that generates, on the basis of the predicted motion vector, a camera control signal for tracking the tracked subject.</p><p id="p-0359" num="0358">(2) The image processing device according to (1),</p><p id="p-0360" num="0359">in which the predicted subject motion vector calculation unit has</p><p id="p-0361" num="0360">a tracked subject identification unit that detects a tracked subject corresponding to a designated type from the captured image, and</p><p id="p-0362" num="0361">a tracked subject type &#x26; posture-corresponding predicted subject motion vector calculation unit that calculates a predicted motion vector corresponding to a type and posture of the tracked subject detected by the tracked subject identification unit.</p><p id="p-0363" num="0362">(3) The image processing device according to (1) or (2),</p><p id="p-0364" num="0363">in which the predicted subject motion vector calculation unit</p><p id="p-0365" num="0364">executes tracked subject identification processing and predicted motion vector calculation processing that use training data.</p><p id="p-0366" num="0365">(4) The image processing device according to any one of (1) to (3),</p><p id="p-0367" num="0366">in which the image processing device</p><p id="p-0368" num="0367">has a display unit that displays a user interface (UI) for causing a user to select a type of the tracked subject, and</p><p id="p-0369" num="0368">the predicted subject motion vector calculation unit</p><p id="p-0370" num="0369">detects, from the captured image, a subject of a type corresponding to the type of the tracked subject designated by the user using the UI, and calculates a predicted motion vector corresponding to a type and posture of the detected tracked subject.</p><p id="p-0371" num="0370">(5) The image processing device according to any one of (1) to (4),</p><p id="p-0372" num="0371">in which, according to automatic tracking mode start that is input by a user, the image processing device starts processing of detecting, from the captured image, a tracked subject of a previously designated type.</p><p id="p-0373" num="0372">(6) The image processing device according to any one of (1) to (5),</p><p id="p-0374" num="0373">in which the predicted subject motion vector calculation unit</p><p id="p-0375" num="0374">has a tracked subject identification unit that detects, from the captured image by using a neural network generated on the basis of training data, a tracked subject of a type designated by a user.</p><p id="p-0376" num="0375">(7) The image processing device according to any one of (1) to (6),</p><p id="p-0377" num="0376">in which the predicted subject motion vector calculation unit</p><p id="p-0378" num="0377">has a tracked subject type &#x26; posture-corresponding predicted subject motion vector calculation unit that calculates a predicted motion vector corresponding to a type and posture of the tracked subject by using a neural network generated on the basis of training data.</p><p id="p-0379" num="0378">(8) The image processing device according to any one of (1) to (7),</p><p id="p-0380" num="0379">in which, in a case where a plurality of tracked subjects of a type designated by a user is detected in the captured image,</p><p id="p-0381" num="0380">the predicted subject motion vector calculation unit decides a tracked subject according to preset subject tracking mode setting information.</p><p id="p-0382" num="0381">(9) The image processing device according to (8),</p><p id="p-0383" num="0382">in which the predicted subject motion vector calculation unit</p><p id="p-0384" num="0383">decides the tracked subject according to any one of</p><p id="p-0385" num="0384">(A) Multiple subjects tracking mode,</p><p id="p-0386" num="0385">(B) Center subject tracking mode, or</p><p id="p-0387" num="0386">(C) Largest subject tracking mode.</p><p id="p-0388" num="0387">(10) The image processing device according to any one of (1) to (9),</p><p id="p-0389" num="0388">in which the predicted subject motion vector calculation unit</p><p id="p-0390" num="0389">acquires a predicted motion vector corresponding to a type and posture of the tracked subject with reference to a table on which a predicted motion vector corresponding to a type and posture of a tracked subject is recorded.</p><p id="p-0391" num="0390">(11) The image processing device according to any one of (1) to (10),</p><p id="p-0392" num="0391">in which the predicted subject motion vector calculation unit</p><p id="p-0393" num="0392">executes calculation of a predicted motion vector in consideration of a size of the tracked subject in an image.</p><p id="p-0394" num="0393">(12) The image processing device according to any one of (1) to (11),</p><p id="p-0395" num="0394">in which the predicted subject motion vector calculation unit</p><p id="p-0396" num="0395">calculates a predicted motion vector by utilizing a measured motion vector reflecting a motion of the tracked subject, the motion being obtained from a past captured image.</p><p id="p-0397" num="0396">(13) The image processing device according to any one of (1) to (12),</p><p id="p-0398" num="0397">in which the predicted subject motion vector calculation unit</p><p id="p-0399" num="0398">compares a motion predicted motion vector calculated from a latest captured image with a measured motion vector reflecting a motion of a tracked subject, the motion obtained from a past captured image, and corrects the measured motion vector.</p><p id="p-0400" num="0399">(14) The image processing device according to any one of (1) to (13),</p><p id="p-0401" num="0400">in which, on the basis of a predicted motion vector calculated by the predicted subject motion vector calculation unit, the camera control signal generation unit generates a control signal of a camera direction for capturing an image of a tracked image of the tracked subject.</p><p id="p-0402" num="0401">(15) The image processing device according to any one of (1) to (14),</p><p id="p-0403" num="0402">in which, on the basis of a predicted motion vector calculated by the predicted subject motion vector calculation unit, the camera control signal generation unit generates a camera control signal of at least any one of pan, tilt, or zoom with a camera, for capturing an image of a tracked image of the tracked subject.</p><p id="p-0404" num="0403">(16) An image processing system including</p><p id="p-0405" num="0404">a camera mounted on a camera platform; and a camera platform control unit that controls the camera platform,</p><p id="p-0406" num="0405">in which the camera has</p><p id="p-0407" num="0406">a predicted subject motion vector calculation unit that detects a tracked subject corresponding to a previously designated type from a captured image, and</p><p id="p-0408" num="0407">calculates a predicted motion vector corresponding to a type and posture of the detected tracked subject, and</p><p id="p-0409" num="0408">a camera control signal generation unit that generates, on the basis of the predicted motion vector, a camera control signal for tracking the tracked subject, and</p><p id="p-0410" num="0409">the camera platform control unit</p><p id="p-0411" num="0410">executes, on the basis of the camera control signal, camera platform control for capturing an image of a tracked image of the tracked subject.</p><p id="p-0412" num="0411">(17) The image processing system according to (16),</p><p id="p-0413" num="0412">in which the predicted subject motion vector calculation unit of the camera has</p><p id="p-0414" num="0413">a tracked subject identification unit that detects a tracked subject corresponding to a designated type from the captured image, and</p><p id="p-0415" num="0414">a tracked subject type &#x26; posture-corresponding predicted subject motion vector calculation unit that calculates a predicted motion vector corresponding to a type and posture of the tracked subject detected by the tracked subject identification unit.</p><p id="p-0416" num="0415">(18) An image processing method executed in an image processing device,</p><p id="p-0417" num="0416">in which a predicted subject motion vector calculation unit executes</p><p id="p-0418" num="0417">a predicted subject motion vector calculation step of detecting a tracked subject corresponding to a previously designated type from a captured image, and</p><p id="p-0419" num="0418">calculating a predicted motion vector corresponding to a type and posture of the detected tracked subject, and</p><p id="p-0420" num="0419">a camera control signal generation unit</p><p id="p-0421" num="0420">executes, on the basis of the predicted motion vector, a camera control signal generation step of generating a camera control signal for tracking the tracked subject.</p><p id="p-0422" num="0421">Furthermore, the series of processing described in the specification can be executed by hardware, software, or a combined configuration of both. In a case where processing is executed by software, it is possible to install a program in which a processing sequence is recorded, on a memory in a computer incorporated in dedicated hardware and execute the program, or it is possible to install and execute the program on a general-purpose personal computer that is capable of executing various kinds of processing. For example, the program can be previously recorded on a recording medium. In addition to installation from the recording medium to the computer, the program can be received via a network such as a local area network (LAN) or the Internet and installed on a recording medium such as a built-in hard disk.</p><p id="p-0423" num="0422">Note that the various kinds of processing described in the specification may be executed not only in time series according to the description but also in parallel or individually, according to processing capability of a device that executes the processing, or as necessary. Furthermore, in the present specification, a system is a logical set configuration of a plurality of devices, and is not limited to a system in which devices of respective configurations are in the same housing.</p><heading id="h-0014" level="1">INDUSTRIAL APPLICABILITY</heading><p id="p-0424" num="0423">As described above, according to a configuration of an embodiment of the present disclosure, there are implemented a device and method that calculate a predicted motion vector corresponding to a type and posture of a tracked subject, and generate a camera control signal necessary for capturing an image of a tracked subject.</p><p id="p-0425" num="0424">Specifically, for example, there are included a predicted subject motion vector calculation unit that detects a tracked subject of a previously designated type from a captured image input from an imaging unit and calculates a predicted motion vector corresponding to a type and posture of the detected tracked subject, and a camera control signal generation unit that generates, on the basis of the predicted motion vector calculated by the predicted subject motion vector calculation unit, a camera control signal for capturing an image of a tracked image of the tracked subject. By using a neural network or the like, the predicted subject motion vector calculation unit executes processing of detecting a tracked subject of a type designated from the captured image by a user, and predicted motion vector calculation processing.</p><p id="p-0426" num="0425">With this configuration, there are implemented a device and method that calculate a predicted motion vector corresponding to a type and posture of a tracked subject, and generate a camera control signal necessary for capturing an image of a tracked subject.</p><heading id="h-0015" level="1">REFERENCE SIGNS LIST</heading><p id="p-0427" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0426"><b>10</b> Automatic tracking imaging device</li>    <li id="ul0002-0002" num="0427"><b>11</b> Camera</li>    <li id="ul0002-0003" num="0428"><b>12</b> Camera platform</li>    <li id="ul0002-0004" num="0429"><b>13</b> Camera platform control unit</li>    <li id="ul0002-0005" num="0430"><b>15</b> Camera display unit</li>    <li id="ul0002-0006" num="0431"><b>16</b> Tracked subject type selection UI</li>    <li id="ul0002-0007" num="0432"><b>17</b> Automatic tracking mode setting icon</li>    <li id="ul0002-0008" num="0433"><b>18</b> Image capturing (video recording) start instruction icon</li>    <li id="ul0002-0009" num="0434"><b>30</b> Communication terminal</li>    <li id="ul0002-0010" num="0435"><b>31</b> Tracked subject type selection UI</li>    <li id="ul0002-0011" num="0436"><b>32</b> Automatic tracking mode setting icon</li>    <li id="ul0002-0012" num="0437"><b>338</b> Image capturing (video recording) start instruction icon</li>    <li id="ul0002-0013" num="0438"><b>100</b> Image processing device</li>    <li id="ul0002-0014" num="0439"><b>101</b> Input unit</li>    <li id="ul0002-0015" num="0440"><b>102</b> Imaging unit</li>    <li id="ul0002-0016" num="0441"><b>110</b> Predicted subject motion vector generation unit</li>    <li id="ul0002-0017" num="0442"><b>111</b> Tracked subject identification unit</li>    <li id="ul0002-0018" num="0443"><b>112</b> Subject estimation training data</li>    <li id="ul0002-0019" num="0444"><b>113</b> Tracked subject type &#x26; posture-corresponding predicted motion vector calculation unit</li>    <li id="ul0002-0020" num="0445"><b>114</b> Tracked subject type &#x26; posture-corresponding predicted motion vector estimation training data</li>    <li id="ul0002-0021" num="0446"><b>121</b> Camera control signal generation unit</li>    <li id="ul0002-0022" num="0447"><b>122</b> Camera (camera platform) drive unit</li>    <li id="ul0002-0023" num="0448"><b>201</b> Tracked subject type designation information</li>    <li id="ul0002-0024" num="0449"><b>202</b> Captured image</li>    <li id="ul0002-0025" num="0450"><b>203</b> Tracked subject detection information</li>    <li id="ul0002-0026" num="0451"><b>204</b> Tracked subject type &#x26; posture-corresponding predicted motion vector</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image processing device comprising:<claim-text>a predicted subject motion vector calculation unit that detects a tracked subject corresponding to a previously designated type from a captured image, and</claim-text><claim-text>calculates a predicted motion vector corresponding to a type and posture of the detected tracked subject; and</claim-text><claim-text>a camera control signal generation unit that generates, on a basis of the predicted motion vector, a camera control signal for tracking the tracked subject.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the predicted subject motion vector calculation unit has</claim-text><claim-text>a tracked subject identification unit that detects a tracked subject corresponding to a designated type from the captured image, and</claim-text><claim-text>a tracked subject type &#x26; posture-corresponding predicted subject motion vector calculation unit that calculates a predicted motion vector corresponding to a type and posture of the tracked subject detected by the tracked subject identification unit.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the predicted subject motion vector calculation unit</claim-text><claim-text>executes tracked subject identification processing and predicted motion vector calculation processing that use training data.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the image processing device</claim-text><claim-text>has a display unit that displays a user interface (UI) for causing a user to select a type of the tracked subject, and</claim-text><claim-text>the predicted subject motion vector calculation unit</claim-text><claim-text>detects, from the captured image, a subject of a type corresponding to the type of the tracked subject designated by the user using the UI, and calculates a predicted motion vector corresponding to a type and posture of the detected tracked subject.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein, according to automatic tracking mode start that is input by a user, the image processing device starts processing of detecting, from the captured image, a tracked subject of a previously designated type.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the predicted subject motion vector calculation unit</claim-text><claim-text>has a tracked subject identification unit that detects, from the captured image by using a neural network generated on a basis of training data, a tracked subject of a type designated by a user.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the predicted subject motion vector calculation unit</claim-text><claim-text>has a tracked subject type &#x26; posture-corresponding predicted subject motion vector calculation unit that calculates a predicted motion vector corresponding to a type and posture of the tracked subject by using a neural network generated on a basis of training data.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein, in a case where a plurality of tracked subjects of a type designated by a user is detected in the captured image, the predicted subject motion vector calculation unit decides a tracked subject according to preset subject tracking mode setting information.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The image processing device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>,<claim-text>wherein the predicted subject motion vector calculation unit</claim-text><claim-text>decides the tracked subject according to any one of</claim-text><claim-text>(A) Multiple subjects tracking mode,</claim-text><claim-text>(B) Center subject tracking mode, or</claim-text><claim-text>(C) Largest subject tracking mode.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the predicted subject motion vector calculation unit</claim-text><claim-text>acquires a predicted motion vector corresponding to a type and posture of the tracked subject with reference to a table on which a predicted motion vector corresponding to a type and posture of a tracked subject is recorded.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the predicted subject motion vector calculation unit</claim-text><claim-text>executes calculation of a predicted motion vector in consideration of a size of the tracked subject in an image.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the predicted subject motion vector calculation unit</claim-text><claim-text>calculates a predicted motion vector by utilizing a measured motion vector reflecting a motion of the tracked subject, the motion being obtained from a past captured image.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the predicted subject motion vector calculation unit</claim-text><claim-text>compares a motion predicted motion vector calculated from a latest captured image with a measured motion vector reflecting a motion of a tracked subject, the motion obtained from a past captured image, and corrects the predicted motion vector.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein, on a basis of a predicted motion vector calculated by the predicted subject motion vector calculation unit, the camera control signal generation unit generates a control signal of a camera direction for capturing an image of a tracked image of the tracked subject.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein, on a basis of a predicted motion vector calculated by the predicted subject motion vector calculation unit, the camera control signal generation unit generates a camera control signal of at least any one of pan, tilt, or zoom with a camera, for capturing an image of a tracked image of the tracked subject.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. An image processing system comprising:<claim-text>a camera mounted on a camera platform; and a camera platform control unit that controls the camera platform,</claim-text><claim-text>wherein the camera has</claim-text><claim-text>a predicted subject motion vector calculation unit that detects a tracked subject corresponding to a previously designated type from a captured image, and</claim-text><claim-text>calculates a predicted motion vector corresponding to a type and posture of the detected tracked subject, and</claim-text><claim-text>a camera control signal generation unit that generates, on a basis of the predicted motion vector, a camera control signal for tracking the tracked subject, and</claim-text><claim-text>the camera platform control unit</claim-text><claim-text>executes, on a basis of the camera control signal, camera platform control for capturing an image of a tracked image of the tracked subject.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The image processing system according to <claim-ref idref="CLM-00016">claim 16</claim-ref>,<claim-text>wherein the predicted subject motion vector calculation unit of the camera has</claim-text><claim-text>a tracked subject identification unit that detects a tracked subject corresponding to a designated type from the captured image, and</claim-text><claim-text>a tracked subject type &#x26; posture-corresponding predicted subject motion vector calculation unit that calculates a predicted motion vector corresponding to a type and posture of the tracked subject detected by the tracked subject identification unit.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. An image processing method executed in an image processing device,<claim-text>wherein a predicted subject motion vector calculation unit executes</claim-text><claim-text>a predicted subject motion vector calculation step of detecting a tracked subject corresponding to a previously designated type from a captured image, and</claim-text><claim-text>calculating a predicted motion vector corresponding to a type and posture of the detected tracked subject, and</claim-text><claim-text>a camera control signal generation unit executes, on a basis of the predicted motion vector, a camera control signal generation step of generating a camera control signal for tracking the tracked subject.</claim-text></claim-text></claim></claims></us-patent-application>