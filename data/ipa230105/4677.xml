<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004678A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004678</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942417</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>62</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>17</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>6254</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>17</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR OBSCURING DATA FROM A DATA SOURCE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16843487</doc-number><date>20200408</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11443065</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17942417</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62831129</doc-number><date>20190408</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>IMMUTA, INC.</orgname><address><city>College Park</city><state>MD</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>REGENSBURGER</last-name><first-name>Joseph J.</first-name><address><city>Grove City</city><state>OH</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>BURT</last-name><first-name>Andrew D.</first-name><address><city>Washington</city><state>DC</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>HAMMEN</last-name><first-name>Barry R.</first-name><address><city>Severn</city><state>MD</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>ROSSI, III</last-name><first-name>Alfred V.</first-name><address><city>Hilliard</city><state>OH</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for obscuring data from a data source include devices and processes that may objectively measure the information loss for the data source that is caused by applying a privacy policy, and may apply a policy to the data source based on the measured information loss. The systems and methods may measure the information loss for a large data source by taking a representative sample from the data source and applying the policy to the sample in order to quantify the information loss. The quantified information loss can be iteratively used to change the policy in order to meet utility and/or privacy goals, and the system can subsequently apply the changed policy to the data source.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="152.74mm" wi="112.18mm" file="US20230004678A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="183.13mm" wi="156.63mm" file="US20230004678A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="201.17mm" wi="151.72mm" orientation="landscape" file="US20230004678A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="139.62mm" wi="150.20mm" file="US20230004678A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="181.27mm" wi="114.22mm" file="US20230004678A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="151.13mm" wi="96.18mm" file="US20230004678A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="182.03mm" wi="121.92mm" file="US20230004678A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="219.29mm" wi="147.91mm" file="US20230004678A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="226.40mm" wi="154.86mm" orientation="landscape" file="US20230004678A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="159.00mm" wi="73.91mm" orientation="landscape" file="US20230004678A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="246.80mm" wi="161.46mm" orientation="landscape" file="US20230004678A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="244.18mm" wi="146.39mm" orientation="landscape" file="US20230004678A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="223.69mm" wi="115.06mm" orientation="landscape" file="US20230004678A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="235.37mm" wi="167.81mm" orientation="landscape" file="US20230004678A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="235.37mm" wi="171.11mm" orientation="landscape" file="US20230004678A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="218.19mm" wi="164.76mm" orientation="landscape" file="US20230004678A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit and filing date of U.S. Provisional Application No. 62/831,129 filed on 8 Apr. 2019, which is hereby incorporated by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The ubiquity of data has made it possible to develop empirical mathematical models and beneficial analytics to, for example, predict a large number of event types. Events types such as shopping behavior, entertainment preferences, medical conditions, driving actions, financial risk, etc. can now be accurately predicted and/or beneficially analyzed using the expansive amount of data people generate and organizations record every day.</p><p id="p-0004" num="0003">This, however, comes at a price to personal privacy, as data that identifies an individual or that describes something about an individual that should be kept private, (which may be referred to as personal data or sensitive data), is contained in the datasets collected by organizations, and may be disseminated to other organizations or individuals.</p><p id="p-0005" num="0004">In response to data privacy concerns, personal or sensitive data can be simply removed or redacted or held back from the data stores or data sources of organizations. But depending on the data, this may render the dataset less suitable, or even unusable, for various purposes that are valuable to the organization and/or to society. In other words, removing or redacting data significantly reduces its analytic value and can reduce the analytic value of the entire dataset.</p><p id="p-0006" num="0005">Thus, it is desirable to provide systems and methods that enable an organization to balance safeguards to personal privacy with the need to provide access to the data in its data stores in a manner that reduces the reduction in analytic value, keeping the data suitable and usable for various purposes.</p><heading id="h-0003" level="1">BRIEF SUMMARY</heading><p id="p-0007" num="0006">Embodiments consistent with the present invention include systems and processes that measure information loss on a dataset when the dataset is protected by a privacy policy. The described embodiments include systems and processes that efficiently sample a large dataset, apply policies to the sample, and objectively quantify the data-loss or utility impact of those policies, which quantification may be used to select and apply specific policies to the dataset.</p><p id="p-0008" num="0007">Various embodiments include a system that can include a computer-readable data storage device containing program instructions and an associated processor that executes the instruction to perform a process or set of operations. The operations can include sampling a data source to obtain a sample of data that is statistically representative; determining ridge statistics for the sample of data; measuring or otherwise determining a first entropy, or an information baseline, of the sample of data; applying the data privacy policy to the sample of data; measuring or otherwise determining a second entropy, or an information content, of the sample of data with the policy applied; calculating an information loss value based on the difference between the first entropy and the second entropy; and displaying the information loss value. In some variants, the operations can also include applying the data privacy policy to the data source if the information loss value is within predetermined limits.</p><p id="p-0009" num="0008">Other embodiments can include a process or operations for sampling a data source to obtain a sample of data that is statistically representative; determining ridge statistics for the sample of data; receiving a data privacy policy for a dataset of the data source; determining an estimate of information loss caused by the data privacy policy using the ridge statistics; displaying the estimate of the information loss; and, optionally, applying the data privacy policy to the data source if the estimate of the information loss is within predetermined limits.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of an example of an environment <b>100</b> for implementing systems and methods in accordance with aspects of the present disclosure.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a system block diagram illustrating an example of a computing system, in accordance with aspects of the present disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram showing an example of the relationships between data sources, fingerprints, and ridges, in accordance with aspects of the present disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a flowchart of an example of a process for determining information loss and applying a policy to a data source, in accordance with aspects of the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a flowchart of an example of a process for generating a fingerprint, in accordance with aspects of the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b>C</figref> is a flowchart of an example of a process for measuring the information loss caused by a policy, in accordance with aspects of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example of descriptive statistics and other data for a basic numeric ridge, in accordance with aspects of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a representation of an example of descriptive statistics and other data for a cardinality ridge, in accordance with aspects of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example of descriptive statistics and other data for a sensitivity ridge, in accordance with aspects of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> shows the first part of an example of descriptive statistics and other data for a string ridge, in accordance with aspects of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> shows the second part of an example of descriptive statistics and other data for a string ridge, in accordance with aspects of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is block diagram showing an example of obscuration techniques, in accordance with aspects of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> shows a screen shot of an example of an information loss report, in accordance with aspects of the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> shows a screen shot of another example of an information loss report, in accordance with aspects of the present disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>10</b>C</figref> shows a screen shot of another example of quantified information-loss information, in accordance with aspects of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0025" num="0024">One way to protect privacy in a dataset or data store is to apply one more privacy policy. A privacy policy is a set of rules, techniques, and/or operations which perturb, redact, or otherwise obscure data within or from a dataset, often focused on sensitive data or personal data. Privacy policies can produce the effect of reducing the precision of data (e.g. representing date-of-birth as year-of-birth or generalizing zip code to state), removing sensitive data elements (e.g. removing social security numbers), or randomizing certain data elements, such as measurements, among other things. In various implementations described herein, a privacy policy may have numerous associated parameters, such as parameters that: specify the user(s) that can access the data from a data store, specify what portions of data can be accessed by a specified user, specify the purpose(s) for which the data can be used, and specify the amount of precision with which to show the data, among other things. Each policy, depending on its content, type, and parameter set, provides a variable amount of privacy at the tradeoff cost of loss of some analytic value, which may also be referred to as utility.</p><p id="p-0026" num="0025">The privacy-utility tradeoff is a central concern for the multitude of useful and beneficial analytics applications which operate with sensitive data. Favoring the privacy extreme, a dataset provided to an analytics application can simply be represented as randomized entries. This assures that no private information is leaked, but would have no utility for gaining insights. On the other extreme, all collected data can be utilized as is without privacy protections. The data would have maximal utility but present a significant risk of being misused, either maliciously or accidentally.</p><p id="p-0027" num="0026">How to efficiently and objectively applying one or more a privacy policy to obscure sensitive data while balancing the privacy-utility tradeoff is a technical problem that is central to establishing and sustaining a useful and beneficial analytics practice. But current conventional systems for choosing and applying privacy policies do not address this problem. Current conventional systems simply apply whatever privacy policy is selected by a user without regard to the utility cost of the policy and without regard to the amount of information loss caused by the policy. In those conventional systems, policies are applied subjectively, at the whim of the current administrator or user.</p><p id="p-0028" num="0027">Unlike convention systems where a privacy policy is subjectively chosen using human intuition and/or a user's prior experience with other databases, various embodiments of the systems and methods described herein instead perform an unconventional, unique combination of steps for measuring and/or calculating an information-loss factor or estimate that is unique for a specific privacy policy as it applies to a specific dataset (e.g., database or portion thereof). The same policy will often have different impacts on information loss for different datasets, and various embodiments described herein quantify the impact for each different dataset, which enables objective decisions regarding whether or not to apply a policy to a given dataset. Similarly, different policies will typically cause different amounts of information loss (e.g., utility) for a single dataset, and various embodiments described herein quantify those different amounts of information loss to enable objective decisions regarding selecting a policy to apply to the dataset. By evaluating and quantifying the information loss associated with a specific privacy policy and a specific dataset, and applying a policy to obscure data based on the quantified information loss, various embodiments described herein remove what were previously subjective human decisions made based on criteria other than information loss (e.g., based on intuition or past experiences with different datasets) and replaces them with rules-based, objective, computerized operations and decisions.</p><p id="p-0029" num="0028">Measuring the information-loss impact of a privacy policy on data is non-trivial and doing it efficiently is a difficult technical and practical problem. A system could, for example, measure the information-theoretic content of a data source both before and after a policy is applied to the data source, using a metric such as Shannon's Entropy to calculate the amount of information before and after. This approach is not practical or efficient in many real-life systems, however, because it requires repeatedly querying or accessing the data source to obtain before and after Shannon's Entropy measurements each time a policy and/or the policy parameters are changed. This is very slow and time consuming and it uses large amounts of computing resources. Under this approach, the performance become untenable and impractical when data volumes become large, and/or when policies are being altered frequently. And, altering policies often occurs frequently during the initial phases of an analytic project.</p><p id="p-0030" num="0029">Various embodiments of the systems and methods described herein quickly and efficiently measure the impact of a privacy policy on a dataset, including a large dataset, in part by accessing the dataset once or infrequently to build a model, or fingerprint, of the dataset, and thereafter the model is used to calculate the information loss or privacy impact or utility change caused by different policies. The fingerprint represents the dataset, and is used instead of the dataset to evaluate the information loss, etc. associated with a new policy. Various embodiments of the systems and methods described herein provide a practical solution for measuring how privacy policies impact utility and for applying one or more policy based on that impact, providing a novel and efficient solution for testing and balancing the privacy-utility tradeoff.</p><p id="p-0031" num="0030">More particularly, various embodiments described herein provide practical systems and methods for obscuring data from a data source based on the amount of information loss caused by an obscuration policy. Various embodiments may include devices and operations for specifying a policy, measuring or calculating the information loss caused by the policy on a given dataset, associating the specified policy with the dataset, and then applying the specified policy to the data source such that only obscured data is available to users. Thus, in one aspect, the embodiments described herein improve the functioning of databases by enabling the objective and automated application of a privacy policy to the databases, especially large databases. This may be achieved by partially or completely obscuring a specific portion(s) (e.g., column) of a dataset according to a privacy policy(ies) that is selected or used according to the amount of information loss the policy causes for that dataset, where the system objectively calculates the amount of information loss.</p><p id="p-0032" num="0031">In another aspect, the embodiments described herein improve the functioning of conventional data obscuring technology on the data in a dataset (e.g., a database) by providing an automatically and objectively calculated information-loss estimate for use in comparing privacy policies to each other, e.g., according to the amount of information loss for each policy.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of an example of a system or an environment <b>100</b> for implementing systems and methods in accordance with aspects of the present disclosure. The environment <b>100</b> may include a query user <b>105</b>, a client device <b>110</b>, an admin user <b>135</b>, an admin device <b>140</b>, and a computing system <b>115</b>, which includes a policy engine <b>130</b>, a virtualized database <b>123</b>, and one or more data source(s) <b>125</b>, which may be referred to in the singular herein for ease of explanation although it may comprise two or more data sources.</p><p id="p-0034" num="0033">In one usage example, the query user <b>105</b> may use the client device <b>110</b> to send a query <b>117</b> (e.g., a request for data from a data store) to the computing system <b>115</b>, which provides a result <b>119</b>, after applying a privacy policy to the requested data (e.g., which may be done by the policy engine <b>130</b>). The privacy policy typically impacts the result <b>119</b> of any query issued by the user <b>105</b>, for example, by obscuring some or all of the data in the result <b>119</b>.</p><p id="p-0035" num="0034">In a similar usage example, the admin user <b>135</b> may use the admin device <b>140</b> to send a policy <b>217</b> and associated metadata (e.g., a privacy policy and its parameters <b>219</b> and/or commands for application to the data source <b>125</b>) to the computing system <b>115</b>, which provides a response <b>139</b>, (which may include an information loss report), after calculating the effect of the policy <b>217</b> on the particular data of the data source <b>125</b>. In various implementations, the response <b>139</b> objectively quantifies the information-loss impact caused by the policy <b>217</b> with respect to the data from the data source <b>125</b>, allowing the admin user <b>135</b> and/or admin device <b>140</b> to compare and select from among multiple possible policies <b>217</b> based on their information-loss effects.</p><p id="p-0036" num="0035">In various implementations, the policy <b>217</b> may specify to the system <b>100</b> which obscuring technique or algorithm to apply to a specific portion of the data (e.g., a column) from the data source <b>125</b>, among other things. In various embodiments, the admin user <b>135</b> may create, select, and/or customize the policy <b>217</b> to specify the obscuring technique for the system to use, among other criteria. Two examples of obscuring techniques include obfuscation by hashing a column of data (e.g., hashing state name data and putting the hash value into the column when accessed by the client device <b>110</b>) and suppression by replacing a column of data with null values (e.g., removing the state name data and leaving the column blank when accessed by the client device <b>110</b>). Various obscuring techniques and algorithms are discussed in detail below, including with respect to <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0037" num="0036">In various embodiments, the client device <b>110</b> can be an input/output device or computing device that is communicatively connected (directly or indirectly) to the computing system <b>115</b> via one or more communication channels <b>120</b>. In some implementations, the client device <b>110</b> can be a terminal of the computing system <b>115</b> or a personal computing device or the like that provides a user interface. For example, the client device <b>110</b> may provide the query user <b>105</b> with a graphic user interface via which the query user <b>105</b> can send the query <b>117</b> to the computing system <b>115</b>, and receive results <b>119</b>. The communication channel <b>120</b> can comprise a wired or wireless data link and/or communication networks, such as a data bus, a local area network, a wide area network, or the Internet.</p><p id="p-0038" num="0037">As noted previously as a usage example, the query user <b>105</b> may use the client device <b>110</b> to send a query <b>117</b> (e.g., a request for data from a data store) to the computing system <b>115</b>. In various embodiments, the policy engine <b>130</b> can process the query <b>117</b> to determine whether any of the policies <b>217</b> (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>) and their associated parameters <b>219</b> are applicable to the particular query user <b>105</b>, data source <b>125</b>, and/or virtualized database <b>123</b> specified by the query <b>117</b>, and if so, the policy engine <b>130</b> can apply the appropriate privacy policy <b>217</b> to the responsive data before providing the result <b>119</b> to the client device <b>110</b>. Thus, the query user <b>105</b> can only access or see the requested data after the data has been obscured according to the privacy policy <b>217</b>, which may be applicable to some of the users of the client device <b>110</b>, but not to others, which can be specified, for example, in the parameters <b>219</b> associated with a policy <b>217</b>.</p><p id="p-0039" num="0038">In various embodiments, the admin device <b>140</b> can be an input/output device or computing device that is communicatively connected (directly or indirectly) to the computing system <b>115</b> via one or more communication channels <b>150</b>. In some implementations, the admin device <b>140</b> can be a terminal of the computing system <b>115</b> or a personal computing device or the like that provides a user interface. For example, the admin device <b>140</b> may provide the admin user <b>135</b> with a graphic user interface via which the admin user <b>135</b> can send a policy <b>217</b> to the computing system <b>115</b>, and receive a response <b>139</b>. The communication channel <b>150</b> can comprise a wired or wireless data link and/or communication networks, such as a data bus, a local area network, a wide area network, or the Internet.</p><p id="p-0040" num="0039">In various embodiments, the computing system <b>115</b> includes hardware and software that perform the processes, services, operations, and functions described herein. In some embodiments, the computing system <b>115</b> can be implemented as a server. As shown in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the computing system <b>115</b> can include or implement a virtualized database <b>123</b>, which interacts with and accesses information from the data source(s) <b>125</b>, and a policy engine <b>130</b>, which interacts with the virtualized database <b>123</b>, and also interacts with and accesses information from the data source(s) <b>125</b>. In the embodiment shown, the virtualized database <b>123</b> also interacts with the client device <b>110</b> of the query user <b>105</b>, and policy engine <b>130</b> also interacts with the admin device <b>140</b> of the admin user <b>135</b>.</p><p id="p-0041" num="0040">In various embodiments, the data source(s) <b>125</b> can be any type of information repository that is queryable. In some implementations, the data source <b>125</b> can be one or more queryable database, which may be columnated or columnar. In some implementations, the data source <b>125</b> can be or include data structures, such as PYTHON'S PANDAS DATAFRAME&#x2122; or R DATAFRAMES&#x2122;, and/or can be or include data structures for common consumer applications, such as MICROSOFT EXCEL&#x2122; worksheets or MICROSOFT ACCESS&#x2122; databases, as are known to those of skill in the art.</p><p id="p-0042" num="0041">In various implementations, the data source <b>125</b> can contain Personally Identifiable Information (PII) or other sensitive data. In one use case example, the data source <b>125</b> can be a collection of information (e.g., a dataset) maintained by a healthcare provider, and can contain medical, personal, and other sensitive information regarding patients, caregivers and insurers. In other use cases, the data source <b>125</b> can contain human resources data, business data, financial data (e.g., trade or transactional data), insurance data, etc.</p><p id="p-0043" num="0042">In various typical implementations, the data source <b>125</b> is comprised of a collection of records (e.g., rows), and each record is comprised of a series of attributes (e.g., columns), as is commonly known in the art.</p><p id="p-0044" num="0043">While the data source <b>125</b> is illustrated as being part of the computing system <b>115</b> in the example shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, it is understood that some or all of the data source <b>125</b> could be located or stored remotely from the computing system <b>115</b> and/or maintained by one or more entities different from the entity that provides, operates, or maintains the computing system <b>115</b>.</p><p id="p-0045" num="0044">In various embodiments, the virtualized database <b>123</b> can be a database management system or service that acts as a container to transparently view and query other data sources (such as multiple databases or data sources <b>125</b>) through a uniform interface that presents information from multiple sources as if they were a single source. For example, multiple databases (e.g. multiple data sources <b>125</b>) can look like a single database to the query user <b>105</b>. The virtualized database <b>123</b> can be system, service, and/or data structure that allows the query user <b>105</b> to transparently view and query multiple data sources <b>125</b> as if they were a one set of data. By doing so, the query user <b>105</b> can access information of the data source(s) <b>125</b> as a single database, without copying, duplicating, or combining the information of the data source(s) <b>125</b>. The virtualized database <b>123</b> can be implemented as a queryable database system or service, such as POSTGRESQL, AMAZON REDSHIFT&#x2122;, APACHE HIVE&#x2122;, or SQL SERVER&#x2122;, as are known to those of skill in the art.</p><p id="p-0046" num="0045">In various embodiments, the policy engine <b>130</b> can be or include software instructions, a software program, a software application, or dedicated hardware, such as an application-specific integrated circuit (ASIC) or a field-programmable gate array (FPGA), among other things. In various embodiments, the policy engine <b>130</b> can implement some, a portion of, or all of the processes, functions, and/or operations described herein, including those related to measuring, calculating, and/or reporting the information loss caused by applying a policy <b>217</b> to a data source <b>125</b> and to selecting and/or applying a specified policy to the data source <b>125</b>.</p><p id="p-0047" num="0046">Various embodiments of the system <b>100</b> may enable an admin user <b>135</b> to objectively balance utility and privacy and understand that these are two complimentary things, and can also automatically provide an objective indication of the highest available utility for a group of possible privacy policies or the highest available privacy level for a target utility. Various embodiments of the system <b>100</b> can provide an automated calculation of the information loss, which represents a tradeoff, and instigate the selection and application of an appropriate policy(ies).</p><p id="p-0048" num="0047">Other embodiments may similarly provide a system for measuring privacy built upon this framework. Such embodiments can enable a holistical understanding of the privacy-utility trade off associated with various policies and obscuring techniques, and objective selections of policies and techniques that preserve and provide the needed information from a dataset while maintaining a minimum threshold of privacy.</p><p id="p-0049" num="0048">One of ordinary skill will recognize that the components, arrangement, and implementation details of the system <b>100</b> are examples presented for conciseness and clarity of explanation. Other components, implementation details, and variations may be used, including adding, combining, or subtracting components. For example, there may be two or more data sources <b>125</b>, some or all of which are external to the computing system <b>115</b>. For another example, the admin device <b>110</b> may be removed and its functionality implemented by the computing system <b>115</b>, which may provide a user interface for the admin user <b>135</b> via an I/O device <b>233</b> as described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. For yet another example, the client device <b>110</b> and the admin device <b>140</b> may be combined into a single device with multiple, different functions for different users <b>105</b>, <b>135</b>.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a system block diagram illustrating an example of the computing system <b>115</b>, in accordance with aspects of the present disclosure. As shown in this example, the computing system <b>115</b> can include a computing subsystem <b>230</b>, an input/output (I/O) device <b>233</b>, and a storage system <b>235</b>. The I/O device <b>233</b> can be or include any device that enables an individual (e.g., an operator, a user, and/or consumer) to interact with the computing subsystem <b>230</b> and/or any device that enables the computing subsystem <b>230</b> to communicate with one or more other computing devices (e.g., client device <b>110</b>, admin device <b>140</b>) using any type of communications link (e.g., communication links <b>120</b>, <b>150</b>). The I/O device <b>233</b> can include, for example, a touchscreen display, pointer device, keyboard, etc. In various implementations, the I/O device can provide a network interface for the computing system <b>115</b> to connect with another computing device (e.g., client device <b>110</b>, admin device <b>140</b>, a data source <b>125</b>).</p><p id="p-0051" num="0050">The storage system <b>235</b> can be or include a computer-readable, non-volatile, hardware data storage device that stores information and/or program instructions. For example, the storage system <b>235</b> can be one or more solid state memories and/or hard disk drives. In accordance with aspects of the present disclosure, the storage system <b>235</b> can store or include the policy engine <b>130</b>, the policy <b>217</b>, policy parameters <b>219</b>, a fingerprint cache <b>221</b>, and the virtualized database <b>123</b>, which can be the same or similar to that previously described. One of ordinary skill will recognize that the storage system <b>235</b> can store multiple instances of these items, such as multiple policies <b>217</b> and their associated policy parameters <b>219</b>. One of ordinary skill will further recognize that depicting the policy <b>217</b> and the policy parameters <b>219</b> as part of the policy engine <b>130</b> is a design choice, and other implementations may organize the policy <b>217</b> and the policy parameters <b>219</b> separate from the policy engine <b>130</b>.</p><p id="p-0052" num="0051">In various embodiments, the computing subsystem <b>230</b> can include one or more processors <b>239</b> (e.g., a microprocessor, a microchip, or an application-specific integrated circuit), one or more memory devices <b>241</b> (e.g., random access memory (RAM) and read only memory (ROM)), one or more I/O interfaces <b>243</b>, and one or more network interfaces <b>245</b>. The memory device <b>241</b> can be a local, high-speed memory (e.g., RAM and a cache memory) employed during execution of program instructions by the processor <b>239</b>. Additionally, the computing subsystem <b>230</b> includes at least one communication channel <b>247</b> (e.g., a data bus) by which it communicates internally and with the I/O device <b>233</b> and the storage system <b>235</b>. In the example shown, the processor <b>239</b> executes computer program instructions (e.g., an operating system and/or application programs, e.g., an implementation of the policy engine <b>130</b>), which can be stored in the memory device <b>241</b> and/or the storage system <b>235</b>. For example, the processor <b>239</b> can execute the computer program instructions of the policy engine <b>130</b>, which functions to, among other things described herein, process queries (e.g., query <b>117</b>) and respond by producing differentially private (e.g., obscured according to a privacy policy <b>217</b>) query results (e.g., result <b>119</b>).</p><p id="p-0053" num="0052">It is noted that the computing subsystem <b>230</b> is only representative of various possible equivalent computing devices that can perform the processes, functions, and operations described herein. To this extent, in embodiments, the functionality provided by the computing subsystem <b>230</b> can be provided by any combination of general purpose hardware, and/or specific purpose hardware (e.g., ASIC, FPGA), and/or computer program instructions. In each embodiment, the program instructions and hardware can be created using standard programming and engineering techniques, respectively.</p><p id="p-0054" num="0053">The fingerprint cache <b>221</b> stores at least one fingerprint, which is a series of measurements and artifacts about the data source <b>125</b>. A fingerprint represents the data in a data source <b>125</b> in various ways. In various implementations, the fingerprint cache <b>221</b> may store multiple fingerprints: there may be one fingerprint for each of several different data sources <b>125</b>, and/or there may be two or more fingerprints for a single data source <b>125</b>.</p><p id="p-0055" num="0054">In various implementations, a fingerprint is a collection of data that is derived and/or measured from a data source <b>125</b>, and which describes or represents the data source <b>125</b>. The fingerprint can be used to assess the impact of a privacy policy <b>217</b> and its associated parameters <b>219</b> on the information and/or utility of the data source <b>125</b>. The measurements included in a fingerprint can comprise, but are not limited to, distributional statistics such as counts, averages, variances, distributional moments, common factors, frequencies of occurrence of these factors, typical string structures, etc., where these measurements, statistics, and calculations may have been made by the policy engine <b>130</b> on the data in the data source <b>125</b>. The artifacts included in a fingerprint can comprise, but are not limited to, data structures such as bloom filters, raw observations, probabilistic sketches, etc. As used herein, each element (e.g., each measurement or artifact) of the fingerprint may be referred to as a &#x201c;ridge&#x201d;.</p><p id="p-0056" num="0055">One of ordinary skill will recognize that the components, arrangement, and implementation details of the computing system <b>115</b> are examples presented for conciseness and clarity of explanation. Other components, implementation details, and variations may be used, including adding, combining, or subtracting components and functions.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an example of the relationships between a data source <b>125</b>, a fingerprint <b>310</b>, and a set of ridges <b>315</b>-<b>335</b>. As shown in this example, the basic numeric ridge <b>315</b>, the cardinality ridge <b>320</b>, the PG (Postgres) stats ridge <b>325</b>, the string ridge <b>330</b>, and the sensitivity ridge <b>335</b> make up the fingerprint <b>310</b> that describes and represents the data source <b>125</b>. In various implementations, the ridges may include numeric statistics, descriptive statistics, timestamp statistics, lists of the most frequently occurring values, other frequency metrics, etc. that are used in measuring, estimating, or calculating the information loss caused by a policy <b>217</b>, e.g., when applied to a specified dataset (e.g., to a column) of the data source <b>125</b>.</p><p id="p-0058" num="0057">For example, the basic numeric ridge <b>315</b> can include (e.g. store) a series of descriptive calculation results, artifacts, statistics, etc. about any numeric data within the data source <b>125</b>, such as the numeric data that is within a column of data from a data source <b>125</b>. For example, the system <b>115</b> executing the policy engine <b>130</b> may measure and/or calculate several descriptive statistics from a column of data (e.g., a dataset from the data source <b>125</b>) that contains numbers. In various implementations, the statistics may include, e.g., the mean, the median, the mode, etc. of the column, which can be stored as part of the basic numeric ridge <b>315</b>. Similarly, the system <b>115</b> and policy engine <b>130</b> may calculate or create a histogram for the column's dataset, which can be stored as part of the basic numeric ridge <b>315</b>.</p><p id="p-0059" num="0058">The measures, metrics, and statics stored in the basic numeric ridge <b>315</b> are used by the system <b>115</b> for calculating the information loss caused by a given privacy policy <b>217</b> when applied to the numeric dataset. For example, consider the example where the dataset (e.g., a column of data in a spreadsheet) contains numeric annual income data. If the policy <b>217</b> for the income dataset calls for generalizing (e.g., rounding off) the data to the nearest $10,000, then there is a significant amount of information loss, particularly compared to, for example, rounding off the annual income data to the nearest $100. Moreover, the amount of information lost depends on the characteristics of the raw, un-obscured data that is in the dataset. For example, if the raw data all falls in the range of $0-$15,000 and the policy <b>217</b> for the income dataset calls for generalizing the data to the nearest $10,000, then almost all of the information in the dataset will be lost after applying the rounding policy; whereas if the raw data all falls in the range of $150,000-$500,000 and the policy <b>217</b> for the income dataset calls for generalizing the data to the nearest $10,000, then very little of the information in the dataset will be lost after applying the policy.</p><p id="p-0060" num="0059">For another example, the cardinality ridge <b>320</b> can include (e.g. store) a series of descriptive calculation results, artifacts, statistics, etc. about any set-type data within the data source <b>125</b>, for example, string data that is within a column of data from a data source <b>125</b>. For instance, the system <b>115</b> executing the policy engine <b>130</b> may measure and/or calculate several descriptive statistics from a column of data (e.g., from the data source <b>125</b>) that contains a finite set of character or numeric values, such as a set of character strings that were chosen from a finite list of strings, or a set of ZIP code numbers.</p><p id="p-0061" num="0060">The measures, metrics, and statics stored in the cardinality ridge <b>320</b> are used by the system <b>115</b> for calculating the information loss caused by a given privacy policy <b>217</b> when applied the data from which the cardinality ridge <b>320</b> was generated. For example, consider the example where the dataset (e.g., a column of data in a spreadsheet) contains U.S. state names, which could be any string from a set of fifty different state-name strings, where the cardinality ridge <b>320</b> may include, among other things, a representation (e.g., a percentage) of the number of times each of the fifty names occurs in the dataset.</p><p id="p-0062" num="0061">The PG stats ridge <b>325</b> can include (e.g. store) a series of descriptive calculation results, artifacts, statistics, etc. about internal processes used by a Postgres implementation to service a query, which is not useful within an information loss context, but is included for completeness.</p><p id="p-0063" num="0062">The string ridge <b>330</b> can include (e.g. store) a series of descriptive calculation results, artifacts, statistics, etc. about a dataset (e.g., column) that consists of character strings. An example of such a dataset is the above-mentioned dataset that contains U.S. state names, which could be any string from a set of fifty different state-name strings.</p><p id="p-0064" num="0063">The sensitivity ridge <b>335</b> can include (e.g. store) a series of descriptive calculation results, artifacts, statistics, etc. about a dataset (e.g., column) that contains information which is automatically classified by the system as sensitive information, for example, social security numbers, credit card numbers, birth dates, and the like. In various embodiments, the system <b>115</b> can use a predefined dictionary of common patterns for sensitive information (e.g., numeric data in format xxx-xx-xxxx, which is a format commonly used for social security numbers) to identify datasets or columns that contain sensitive information within a data source <b>125</b>.</p><p id="p-0065" num="0064">Several of the ridges will be described in greater detail below with respect to <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>8</b></figref>.</p><p id="p-0066" num="0065">It should be noted that although <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows only one each of ridges <b>315</b>-<b>335</b> for the sake of clarity and ease of explanation, in various implementations, a fingerprint <b>310</b> may include multiple instances of each of the ridges <b>315</b>-<b>335</b>, according to how may columns of corresponding data are contained in the data source <b>125</b>. For example, if the data source <b>125</b> contains 10 columns of numeric data, then the fingerprint <b>310</b> may have 10 different instances of the basic numeric ridge <b>315</b>, one for each numeric-data column/dataset.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIGS. <b>4</b>A-C</figref> show examples of processes for generating a fingerprint and measuring the impact (e.g., information loss) of policies upon the underlying data.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is block diagram of an example of a method or process <b>400</b> for objectively determining information loss and applying a privacy policy to a fingerprint, according to an implementation of the invention. In some embodiments, all or part of the process <b>400</b> may be implemented by the computing system <b>115</b>, for example by executing the policy engine <b>130</b>.</p><p id="p-0069" num="0068">In the example shown, at block <b>405</b>, the computing system <b>115</b> receives a privacy policy <b>217</b> and its associated parameters <b>219</b>, e.g., from the admin device <b>140</b> associated with the admin user <b>135</b>.</p><p id="p-0070" num="0069">In some use cases, the admin user <b>135</b>, or an application running on the admin device <b>140</b>, may submit or select a policy <b>217</b> for the purpose of determining how much information loss would be caused by applying the policy <b>217</b> to a specific data source <b>125</b>. The policy <b>217</b> may then be either applied to the data source <b>125</b> at the end of the process <b>400</b>, or not, based on the amount of information loss calculated by the system <b>115</b>. If not, then the admin user <b>135</b>/admin device <b>140</b> may send or select another, different policy <b>217</b>, to repeat the process <b>400</b> in order to find out its associated information loss and apply it to the data source <b>125</b> if the information loss versus utility balance is satisfactory, e.g., above a predetermined threshold.</p><p id="p-0071" num="0070">In various implementations, the policy <b>217</b> may include or specify one or more different types and/or instances of data obscuration, which are applied on a column by column basis to specified columns of the data source <b>125</b>. In such implementations, the admin user <b>135</b> and/or admin device <b>140</b> may define or specify in the policy what is obscured per column, which affects what can be seen or accessed by the query user <b>105</b> or an application executing on a client device <b>110</b>.</p><p id="p-0072" num="0071">In some embodiments, the admin user <b>135</b> can select a policy <b>217</b> from a predetermined group of policies or may otherwise define the policy <b>217</b> to provide to the system at block <b>405</b>.</p><p id="p-0073" num="0072">The system <b>115</b> can record the time of policy receipt at block <b>409</b>. In various embodiments, the recorded time and policy information may be used to form a log or record of the activities of the admin user <b>135</b>, which may be useful if the admin user <b>135</b> wishes to recreate a policy or the like that the admin user <b>135</b> had created or used in the past.</p><p id="p-0074" num="0073">At block <b>417</b>, the system <b>115</b> does a check to determine whether a fingerprint (e.g., fingerprint <b>310</b>) of the data source <b>125</b> exists. In various embodiments, this may be done, for example, by determining whether there is a fingerprint file <b>310</b> for the data source <b>125</b> in the fingerprint cache <b>221</b>.</p><p id="p-0075" num="0074">In some implementations, the admin user <b>135</b> may specify to the system <b>115</b> which data source(s) <b>125</b> to use from among a plurality of data sources which with the system <b>115</b> functions; while in other implementations, the system <b>115</b> may be dedicated to one specific data source <b>125</b>.</p><p id="p-0076" num="0075">In some implementations, after determining that a fingerprint <b>310</b> representing the data source <b>125</b> exists, an additional check (not shown) may be executed in conjunction with block <b>417</b> to determine whether the existing fingerprint <b>310</b> is recent enough to use. For example, the system <b>115</b> may determine whether or not the existing fingerprint file <b>310</b> was created after a predetermined time in the past, such as one day ago, two days ago, five days ago, seven days ago, 14 days ago, 30 days ago, 60 days ago, 90 days ago, one year ago, or the like. This predetermined time may be based on how often and/or how much the data in a data source <b>125</b> is changed or is updated; e.g., the more often the data changes, the shorter the predetermined time-in-the past threshold.</p><p id="p-0077" num="0076">If a fingerprint does not exit (or optionally if it is older than the time-in-the past threshold (e.g., more than 14 days old)), (<b>317</b> No), then a new fingerprint <b>310</b> is generated from the data source <b>125</b>, for example, using the process for fingerprint generation as shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>.</p><p id="p-0078" num="0077">If the fingerprint <b>310</b> exits (and optionally if it is not older than the time-in-the past threshold (e.g., not more than 14 days old)) (<b>317</b>, yes), then the process <b>400</b> proceeds to block <b>425</b>.</p><p id="p-0079" num="0078">At block <b>425</b>, the system <b>115</b> determines the information loss using the policy <b>217</b> that was received at block <b>405</b> and the fingerprint <b>310</b> of the data source <b>125</b>. In some implementations, the information loss produced by the policy <b>217</b> can be measured, calculated, or evaluated for a single column of the data source <b>125</b> that is represented by the fingerprint <b>310</b>. While in other implementations, the information loss produced by the policy <b>217</b> can be measured, calculated, or evaluated for multiple columns, e.g., on a column by column basis, and the fingerprint <b>310</b> may have different ridges for different columns of the data source <b>125</b>. In some implementations, the system <b>115</b> may weight some columns (e.g., columns with extremely sensitive information, such as SSN) more heavily than others in quantifying the information loss. A detailed example of a process for determining the information loss is shown in <figref idref="DRAWINGS">FIG. <b>4</b>C</figref>.</p><p id="p-0080" num="0079">In various implementations of block <b>425</b>, the system <b>115</b> calculates a measurement of the disparity between a dataset (e.g., column of data) with and without the policy <b>217</b> applied; or in other words, an estimate of the amount of obscuration of the data that would be induced by the application of the policy <b>217</b>. In various implementations, the system <b>115</b> can produce this information-loss estimate in the form of a number on a scale from zero to one. For example, an information-loss estimate of 1.00 means the policy <b>217</b> completely obliterated all information in the dataset; while an information-loss estimate of 0.00 means the policy <b>217</b> made no change to the original dataset. Similarly, an information-loss estimate of 0.05 means the policy <b>217</b> had very little impact on the dataset, e.g., it obscured very little of the data and/or changed or hid the data in a minor way; while an information-loss estimate of 0.95 means the policy <b>217</b> had a large impact on the data, e.g., it obscured much of the data and/or changed or hid most of the data.</p><p id="p-0081" num="0080">In various embodiments, the system <b>115</b> determines information loss by comparing the obscured data that is visible or accessible to a query user <b>105</b>/client device <b>110</b> after the policy <b>217</b> is applied, to the un-obscured raw data and quantifying the difference. This may be calculated, in some implementations, as a reduction in resolution caused by the obscuration policy. As another example, in some implementations, the system <b>115</b> may calculate the percentage of information that is redacted or suppressed by the policy <b>217</b>. For instance, for a dataset (e.g., column) that contains nine digit social security numbers, if a policy <b>217</b> removes or suppresses the first three digits of the dataset (i.e., a query user <b>105</b> can see only the last six digits of the social security numbers), then the system <b>115</b> may calculate an information loss value of 0.33 (i.e., 3/9) for the policy <b>217</b> as applied to this dataset; similarly, if a policy <b>217</b> removes or suppresses the first five digits of the dataset (i.e., a query user <b>105</b> can see only the last four digits of the social security numbers), then the system <b>115</b> may calculate an information loss value of 0.56 (i.e., 5/9) for the policy <b>217</b> and this dataset. In another example, for a dataset (e.g., column) that contains state names, if a policy <b>217</b> removes or suppresses the rows of the data source <b>125</b> that contain Wyoming as the state name (i.e., a query user <b>105</b> can see only the rows from states other than Wyoming), then the system <b>115</b> may count or otherwise measure the number of occurrences of &#x201c;Wyoming&#x201d; in the &#x201c;State Name&#x201d; column and divide that by the total number of rows in order to calculate an information loss value. In this example, if the number of occurrences of Wyoming is 578 and the total number of rows in the dataset is 32,753, then the system <b>115</b> may calculate an information loss value of 0.02 for the policy <b>217</b> of suppressing the Wyoming rows in this dataset.</p><p id="p-0082" num="0081">In various implementations, the system <b>115</b> can additionally or alternatively calculate and generate other information-loss metrics and information besides the above-described information-loss estimate number on a scale from zero to one. For example, in the case of a rounding policy, the system <b>115</b> can generate one or more histograms of the rounded and/or unrounded dataset and/or various statistical metrics representing the rounded and/or unrounded dataset.</p><p id="p-0083" num="0082">In some of the embodiments described above, the information-loss estimate number represents the information loss for one column of data after application of a policy to that column. Additionally or alternatively, the system <b>115</b> can calculate or determine an information-loss estimate number for, or that takes into account, other part(s) of the data source <b>125</b> in addition to the column to which the policy <b>217</b> was applied, where these other part(s) of the data source <b>125</b> are secondarily affected by the application of the policy. This is particularly applicable for policies that remove rows of data based on the values in a specific column(s), because the removed rows will skew or affect the makeup of the data in all, or most of, the other rows to various degrees.</p><p id="p-0084" num="0083">For example, consider the case of a policy <b>217</b> that suppresses or removes rows from a data source <b>125</b> based on a specific value in a specific column, such as removing the rows that have the value &#x201c;Wisconsin&#x201d; in a &#x201c;State Name&#x201d; column. Further consider that the data source <b>125</b> also has another column for &#x201c;Occupation,&#x201d; which includes character strings for the occupation &#x201c;dairy farmer.&#x201d; Because there are a large number of dairy farmers in Wisconsin compared to most other states (i.e., non-uniformities in the population), removing the &#x201c;Wisconsin&#x201d; rows has a significant secondary effect on the content and makeup of the &#x201c;Occupation&#x201d; column dataset, causing a disproportionate amount of information loss or disparity in the amount of dairy farmers in the Occupation column; i.e., after the policy is applied, the percentage of dairy farmers across all occupations is significantly lower than in the raw dataset. Various embodiments consistent with the invention take this into account when determining information loss, for example, by calculating an information-loss estimate for some or all columns if a policy <b>127</b> specifies removal of rows of data.</p><p id="p-0085" num="0084">At block <b>431</b>, the system <b>115</b> reports the calculated, quantified information loss to the admin user <b>135</b> and/or the admin device <b>140</b>, for example, in an information-loss report <b>139</b>.</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> shows an example of how information-loss information is presented to the user for categorical data. In this figure, the cross-hatched &#x201c;new value&#x201d; bars show the frequency of a value after a data policy is applied. The dotted &#x201c;baseline confidence interval&#x201d; regions show the frequency of a value without any policy. And, the heavy vertical lines in the baseline confidence interval regions show the baseline value for the attribute (e.g., the data from the &#x201c;Supplier State&#x201d; column). In the example of information-loss information shown in <figref idref="DRAWINGS">FIG. <b>10</b>A</figref>, as shown at the bottom, records from the states of WASHINGTON, NEW MEXICO, TEXAS, and NEW YORK are removed from the dataset, perturbing the relative frequencies of the remaining items away from their baseline values in several instances, although all except DELAWARE are within their baseline confidence interval regions. <figref idref="DRAWINGS">FIG. <b>10</b>B</figref> shows an example of an information-loss report that was generated for numeric data. In this example, a generalization policy is applied to the &#x201c;Gross Profit&#x201d; column on a table, slightly perturbing the descriptive statistics of the underlying data away from their baseline values. <figref idref="DRAWINGS">FIG. <b>10</b>C</figref> shows an example of information-loss information in the form of a summary of the differences, over all columns, produced by the applied policy. The severity value for each row gives a statistical quantification (e.g., on a scale of 0.000 to 1.000) of how different each column's distribution is from its baseline in the presence of the new policy.</p><p id="p-0087" num="0086">Referring again to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, in some implementations, the admin user <b>135</b> and/or an application running on the admin device <b>140</b> may evaluate the reported information-loss information to determine whether or not the policy <b>217</b> from block <b>405</b> provides sufficient privacy (e.g., a high enough amount of information loss to protect sensitive data) and/or sufficient utility (e.g., a low enough amount of information loss for the data in the result <b>119</b> to be useful for a specific purpose). And, by considering the system's objectively calculated information-loss information, the admin user <b>135</b> can get a qualitative feel for how much the data is impacted by a policy and/or by a change from one policy to another.</p><p id="p-0088" num="0087">In some such implementations, the admin user <b>135</b> and/or the admin device <b>140</b> can, for example, compare the information-loss estimate number (e.g., ranging from 0.00 to 1.00 as described above) to a target information-loss number, plus or minus some tolerance(s), such as +0.05 and &#x2212;0.02. And, if the calculated information-loss estimate number does not match the target information-loss number plus or minus the tolerances, then the admin user <b>135</b> and/or the admin device <b>140</b> may iteratively provide a new or adjusted policy <b>217</b> at block <b>405</b> for the process <b>400</b> to evaluate, until the target information-loss number is reached. Otherwise, when the calculated information-loss estimate number matches the target information-loss number plus or minus the tolerances, then the process <b>400</b> can proceed to block <b>435</b>.</p><p id="p-0089" num="0088">At block <b>435</b>, the system <b>115</b> applies the policy <b>217</b> to the data source <b>125</b>. In various implementations, this may include storing the policy <b>217</b> and its associated parameters <b>219</b> in the storage system <b>235</b>, and subsequently processing each query <b>117</b> to determine whether the stored policy <b>217</b> and its associated parameters <b>219</b> are applicable to the particular query user <b>105</b>, data source <b>125</b>, and/or virtualized database <b>123</b> specified by the query <b>117</b>. If a received query <b>217</b> is one that the privacy policy <b>217</b> covers, then the system <b>115</b> applies the obscuring techniques specified by the policy <b>217</b> to the query-responsive data before the result <b>119</b> is provided to the client device <b>110</b>. Thus, the query user <b>105</b> can only access or see the requested data after the data has been obscured according to the applicable privacy policy <b>217</b>.</p><p id="p-0090" num="0089">As noted previously, in some embodiments, the functionality of the admin device <b>140</b> can be implemented by the computing system <b>115</b>, and a software application or the like may take the place of the admin user <b>135</b>, such that the operations <b>431</b> and <b>435</b> can be automatically done based on the determined information-loss estimate matching a predetermined target information-loss number, plus or minus a tolerance(s). In such embodiments, the system <b>115</b> can automatically trigger or control the application of a policy that is objectively selected to meet a specific information loss requirement, without run-time input from the admin user <b>135</b>.</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a block diagram showing an example of a fingerprint generation process <b>450</b>. In some embodiments, all or part of the process <b>450</b> may be implemented by the computing system <b>115</b>, for example by executing the policy engine <b>130</b>. As noted with regard to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a fingerprint <b>310</b> can be implemented as a set collection of ridges, each of which may be a file or data structure containing measurements, statistics, artifacts, or the like that are measured or derived from a data source <b>125</b>, e.g., from a column or other dataset in the data source <b>125</b>.</p><p id="p-0092" num="0091">In various embodiments, the process <b>450</b> will generate one or more of the ridges <b>315</b>, <b>320</b>, <b>330</b>, <b>335</b> for a dataset (e.g., a column) of the data source <b>125</b>, depending on the type of data in the dataset. For example, if a column contains numerical data, such as in a column that records &#x201c;age in years&#x201d; or &#x201c;annual income,&#x201d; then the system <b>115</b> will generate a numeric ridge <b>315</b> for that dataset. For another example, if the column contains string data, such as the string-type attributes &#x201c;yes&#x201d; or &#x201c;no&#x201d; in a column that records the yes/no answer to &#x201c;U.S. Citizen?&#x201d; or &#x201c;Employed?&#x201d;, then the system <b>115</b> will generate a cardinality ridge <b>315</b> for that dataset.</p><p id="p-0093" num="0092">In some embodiments, the system <b>115</b> may interact with the admin user <b>135</b> to provide a framework that enables the user <b>135</b> to develop and/or plug in new metrics/ridges that they have created.</p><p id="p-0094" num="0093">In the example shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the process <b>450</b> starts at block <b>442</b> by determining a sample size, which may be different for different ridges. For example, in the case of descriptive statistic ridges, the system <b>115</b> may determine the sample size based on or controlled by the minimum sample size required for the sampling error to begin to show asymptotic behavior. Other standard statistical techniques instead of asymptotic behavior can also be used to calculate a sample size that is, for example, statistically significant. In some implementations, the sample size may be predefined based on the table size or number of rows, and determining the sample size at <b>442</b> may simply involve looking it up. For example, when the table or column has a size 1-1000 (e.g. having 1-1000 rows), the sample size=the table size; when the table or column has a size 1001-5000, the sample size=1000; when the table or column has a size 5001-10000, the sample size=2000; etc.</p><p id="p-0095" num="0094">For another example, in the case of sketching approaches, (e.g. count-min sketch), the system <b>115</b> may determine the sample size based on linear table scans of the data source <b>125</b>.</p><p id="p-0096" num="0095">Once the sample size is determined, at block <b>445</b> the system <b>115</b> samples the data source <b>125</b> in accordance with the determined sample size to create a representative sample of, e.g., the data in a column of the data source <b>125</b>. In various implementations, the system <b>115</b> may employ randomized sampling, stratified sampling, hypercube sampling, linear table scan, or other appropriate sampling regimes.</p><p id="p-0097" num="0096">Using this representative sample, at block <b>439</b> the system <b>115</b> determines the ridge statistics by performing the procedures, algorithms, calculations, and/or operations that produce the contents of a ridge, which contents are different for each of the different ridge types <b>315</b>-<b>335</b>. Ridge statistics are any descriptive statistics that can be measured on a random sample of data. These statistics can include, but are not limited to, averages, variances, categorical counts, histograms, covariances, etc.</p><p id="p-0098" num="0097">For example, in the case of a cardinality ridge <b>315</b> (see <figref idref="DRAWINGS">FIG. <b>6</b></figref> for additional details), the system <b>115</b>: employs a standard statistical algorithm to estimate the number of unobserved attributes or values that are missing from the representative sample; determines the cardinality of the representative sample by counting the actual number of different attributes in the sample; employs a standard statistical algorithm to extrapolate the cardinality of the raw dataset (e.g., of the data source <b>125</b>) based on the number of unobserved attributes and the cardinality of the representative sample; calculates the number of distinct attributes as a percentage; calculates the percentage of the number of unobserved attributes in the sample size; stores the size of the representative sample; calculates the percentage of the size of the representative sample over the size of the entire dataset; and calculates the percentage of appearance or occurrence of the most common attributes&#x2014;for example the percentage of occurrence of the top 20 most common attributes.</p><p id="p-0099" num="0098">At block <b>453</b>, the system <b>115</b> stores the ridge statics, for example in a file or data structure for one of the ridge types <b>315</b>-<b>335</b>, several of which may be associated to make a fingerprint <b>310</b>. In various embodiments, the ridge statistics/files may be stored in the fingerprint cache <b>221</b>. The stored fingerprint <b>310</b> (i.e., set of ridges) is used to measure and estimate the impact of a privacy policy <b>217</b> on the underlying data source <b>125</b>, e.g., as described above with respect to block <b>425</b> and elsewhere herein.</p><p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. <b>4</b>C</figref> is a block diagram showing an example of a process <b>460</b> for determining information loss for a policy <b>217</b> using the fingerprint <b>310</b> of a data source <b>125</b>. This example uses precomputed ranges, which may be stored as part of the fingerprint <b>310</b>. Thus, in different implementations, blocks <b>463</b>-<b>469</b> of the process <b>460</b> can be performed as part of a fingerprinting process, such as added on as part of the process <b>450</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, and the results stored in a ridge of the fingerprint <b>310</b> for use by the process <b>460</b> when needed.</p><p id="p-0101" num="0100">In the example shown, at block <b>463</b>, the system <b>115</b> measures the entropy of the representative sample (see block <b>445</b> of <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>) from the data source <b>125</b>, with no policies applied, to get a baseline measurement. In some embodiments, the entropy can be measured using an information theory measurement such as Shannon's entropy. Entropy is closely related to information loss, and entropy may be thought of as a measure of the amount of information in a dataset; thus it is analogous to the converse of information loss. That is, a decrease in entropy represents an increase in information loss.</p><p id="p-0102" num="0101">In measuring the impact of a privacy policy and its obscuration technique(s) on a dataset, it is noted that entropy will be maximized without any policies applied to the dataset. Furthermore, as the set x<sub>Support </sub>becomes more discrete, entropy will decrease monotonically.</p><p id="p-0103" num="0102">At block <b>465</b> the process <b>460</b> identifies any high-leverage generalization ranges in the representative sample, where a high-leverage generalization range is a portion or series of data points where a rapid change in entropy is identified. One example would be at or above the average interstitial distance between an ordered list of observations. These high leverage points or ranges indicate where the entropy curve is best sampled to reduce the error in interpolating the curve, (for example to calculate an information-loss value at block <b>473</b>).</p><p id="p-0104" num="0103">In various implementations, the system <b>115</b> can identify the high-leverage generalization range(s) by applying a series of generalization values (e.g., rounding values, such as round to the nearest $1000, $2000, $3000, $4000, $5000, etc.) to the representative sample and measuring the entropy throughout the series until the entropy nears zero, which will generate a curve. The system <b>115</b> can then look for inflexion point(s) or an inflexion range(s) on that curve where the entropy changes significantly (e.g., the curve becomes steeper), which is the high leverage generalization range(s).</p><p id="p-0105" num="0104">Once a high leverage generalization range is identified, at block <b>467</b>, the system <b>115</b> can create a new series of generalization values (i.e., the high leverage generalization points) that span the high leverage generalization range (e.g., in $100 increments to continue the previous example) and apply them to the representative sample in the high leverage generalization range, and then at block <b>469</b>, calculate or measure the entropy for each of the points to obtain a dense number of entropy data points in the high leverage range. The group of entropy measurements can form the basis of a piecewise interpolant (e.g., a piecewise polynomial curve), which is stored by the system <b>115</b>, (e.g., as part of a fingerprint <b>310</b>). Thus, the piecewise interpolant has, rather than uniform sampling throughout, a denser sampling in the high-leverage range(s) where the entropy curve is steep, which significantly decreases the error from interpolation using the curve to generate an entropy estimate, and indirectly, an information-loss estimate.</p><p id="p-0106" num="0105">At block <b>471</b>, the system <b>115</b> receives a generalization range or value (e.g., a value to which to round a column of data), for example from the policy <b>217</b> that was received at block <b>405</b> of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>.</p><p id="p-0107" num="0106">At block <b>473</b>, the system <b>115</b> uses the piecewise interpolant from block <b>469</b> (which may be part of the fingerprint <b>310</b>) to approximate or estimate the entropy associated with the received generalization range or value from the policy <b>217</b>, and by extension to estimate the information loss introduced by the policy <b>217</b>. For example, if the dataset contains annual salary amounts, and the policy <b>217</b> specifies rounding to the nearest $3,500, the system looks up the piecewise interpolant for $3500 and gets or calculates the corresponding entropy estimate.</p><p id="p-0108" num="0107">In various implementations, as noted, the calculated entropy from block <b>473</b> corresponds to, represents, or is an indirect measurement of the information loss. At block <b>475</b>, the system <b>115</b> converts the entropy value into an information-loss estimate value.</p><p id="p-0109" num="0108">Similarly, for obscuration types other than generalization, the system <b>115</b> can analyze the dataset and/or representative sample without and with the policy's obscuration applied, and the information content can be measured. The information content can be quantified using an information theoretic metric, such as Shannon's Entropy, a signal to noise ratio, or the like.</p><p id="p-0110" num="0109">Shannon's Entropy is defined as the expectation value of the log of the probability of some observed outcome:</p><p id="p-0111" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mi>H</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mi>x</mi>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mo>-</mo>   <mrow>    <munder>     <mo>&#x2211;</mo>     <mi>i</mi>    </munder>    <mtext> </mtext>    <mrow>     <msub>      <mi>p</mi>      <mi>i</mi>     </msub>     <mo>&#x2062;</mo>     <mi>log</mi>     <mo>&#x2062;</mo>     <msub>      <mi>p</mi>      <mi>i</mi>     </msub>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0112" num="0110">In this expression, p<sub>i </sub>is the probability of observing the i<sup>th </sup>value in a dataset. In this expression, H(x) is bounded between 0 and log(K), where K is the number of distinct values in a dataset. In various implementations, information loss can be measured as the change in H(x) introduced by the perturbation on an attribute. For example with a suppress technique or policy <b>987</b> (see <figref idref="DRAWINGS">FIG. <b>9</b></figref>), H(x) would be zero, making the information loss identically H(x) without policies applied. In the case of a generalize technique or policy <b>383</b>, the values are mapped from a space of K values into a space of K&#x2032; possible values, where K&#x2032;&#x3c;K. An obfuscate technique or policy <b>385</b>, will typically, but not necessarily, maintain counting statistics, keeping H(x) constant and introducing no information loss.</p><p id="p-0113" num="0111">A randomize technique or policy <b>389</b> will either maintain or increase a diversity of values into a dataset, and, in all cases, produce a more uniform distribution of values, leading to an increase in entropy. Under a Shannon's entropy construction, a randomize technique will create a deceptive increase in information content. In this scenario, various embodiments of the system <b>115</b> can more appropriately represent information loss as the change in the signal to noise ratio of the data. In general the Signal to Noise Ratio (SNR) is unknown in the data prior to policy application. However obscurations can either reduce signal or increase noise. As such the change in SNR can be modeled as follows:</p><p id="p-0114" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <msub>   <mi>SNR</mi>   <mi>policy</mi>  </msub>  <mo>=</mo>  <mrow>   <msub>    <mi>SNR</mi>    <mn>0</mn>   </msub>   <mo>&#x2a2f;</mo>   <mrow>    <mo>(</mo>    <mrow>     <mn>1</mn>     <mo>-</mo>     <mfrac>      <mi>&#x1d4b6;</mi>      <mi>&#x3b2;</mi>     </mfrac>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0115" num="0112">Where a denotes a decrease in signal and &#x3b2; denotes an increase in noise. By convention a &#x2265;1 and &#x3b2;&#x2264;1. In this way SNR<sub>Policy</sub>&#x2264;SNR<sub>0</sub>, where SNR<sub>Policy </sub>is the SNR after the obscuration, and SNR<sub>0 </sub>is the raw SNR. In the case of a randomize technique or policy <b>389</b>, this amounts to quantifying the scale of noise added by the randomization process. Since randomization can be seen as convolving the true signal with some obscuring function, various embodiments can estimate the noise as an additive noise and quantify it using either information content of the noise or some other information theoretic measure.</p><p id="p-0116" num="0113">One of ordinary skill will recognize that the processes <b>400</b>, <b>450</b>, and <b>460</b> of <figref idref="DRAWINGS">FIGS. <b>4</b>A-C</figref> are presented for conciseness and clarity of explanation, and that blocks and operations may be added to, deleted from, reordered, performed in parallel, or modified within process <b>400</b> without departing from the principles of the invention. For example, in the process <b>400</b>, blocks may be added to compare the information-loss estimate to a target information-loss number and either loop back to the top to try a different policy or automatically apply the policy. Other variations are possible within the scope of the invention.</p><p id="p-0117" num="0114"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example of the descriptive statistics, attributes, characteristics, calculated values, etc., of a basic numeric ridge <b>315</b>, which in various embodiments can be generated by the system <b>115</b> from a dataset (e.g., a column of date from the data source <b>125</b>) and stored in a file, a data structure, or the like, for example in the fingerprint cache <b>221</b>.</p><p id="p-0118" num="0115">In the example shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the system <b>115</b> has calculated a set of values and saved them in the data structure shown. In this example, IQR is the inner quartile range, which is a measure of statistical dispersion (e.g., the width of the distribution of the dataset). Q05, Q25, Q75, and Q95 are the fifth, 25<sup>th</sup>, 75<sup>th</sup>, and 95th quantiles of the dataset. Mean is the well-known statistical average value of the dataset. Median is the well-known statistical median value of the dataset. Keratosis, skewness and variance are the well-known standard descriptive statistics for this dataset. Bin counts, bin size, and bin lower limits represent the measurements for a histogram for the dataset. Blurred min and blurred max represent the approximate min and max of the dataset, but with some random factor added so as to avoid disclosing actual raw data from the dataset in the fingerprint <b>210</b>. Sample size the size of the dataset, here, 10,490 samples or rows. Percent nulls is the number of nulls in the dataset, expressed as a percentage of the total dataset. And, mean absolute deviation is that well-known standard descriptive statistic for this dataset.</p><p id="p-0119" num="0116">As noted previously, in various embodiments, the system <b>115</b> will generate a numeric ridge, e.g., as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, only if the dataset contains numeric data. Thus, if the dataset consisted of character strings, for example, last names, then the system <b>115</b> will not generate a numeric ridge <b>315</b> for that data.</p><p id="p-0120" num="0117"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example of the frequency statistics, attributes, values, etc., of a cardinality ridge <b>320</b>, which in various embodiments can be generated by the system <b>115</b> from a dataset (e.g., a column of date from the data source <b>125</b>) and stored in a file, a data structure, or the like, for example in the fingerprint cache <b>221</b>. In <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the data from the cardinality ridge <b>320</b> is represented in a tabular format for ease of explanation and clarity.</p><p id="p-0121" num="0118">In the example shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the raw dataset contained 10,490 samples or rows, and each sample contained a character string having a value or attribute of either &#x201c;Y&#x201d; (for yes) or &#x201c;N&#x201d; (for no). The system <b>115</b> analyzed the entire dataset to generate the fingerprint, and thus the size of the representative sample is the same as the size of the raw data set.</p><p id="p-0122" num="0119">In the example shown, the top row <b>601</b> contains the number of unobserved attributes or values that are in the raw dataset but not in the representative sample used to generate this ridge, (i.e., that are unobserved in the representative sample), as calculated by the system <b>115</b>. In various embodiments, the system <b>115</b> may perform standard statistical analysis to estimate the number of unobserved unique attributes, and then use that estimate to extrapolate the true cardinality of the dataset (row <b>602</b>). In the example shown, because the sample size is equal to the data set size, there are zero unobserved attributes, as shown in the column labelled &#x201c;Value&#x201d;.</p><p id="p-0123" num="0120">The second row <b>602</b> contains the extrapolated cardinality of the dataset, as calculated by the system <b>115</b>. In various embodiments, this can be calculated by adding the number of unobserved attributes (row <b>601</b>) plus the cardinality of the samples (row <b>603</b>).</p><p id="p-0124" num="0121">The third row <b>603</b> contains the cardinality of the attributes in the representative sample (i.e., the number of distinct attributes in the sample), which is counted by the system <b>115</b>. In this example, there are only two distinct attributes in the samples: &#x201c;Y&#x201d; and &#x201c;N&#x201d;, and therefore the cardinality of the samples is 2. It is also noted that in this example, the extrapolated cardinality of the raw dataset is the same as the actual cardinality of the samples because the system <b>115</b> sampled the entire dataset.</p><p id="p-0125" num="0122">The fourth row <b>604</b> contains the percentage of distinct attributes. Percent distinct is measured as the ratio between the extrapolated cardinality (row <b>602</b>) and the total population. The total population is estimated by the sample size (row <b>606</b>) divided by the sample fraction (row <b>607</b>).</p><p id="p-0126" num="0123">The fifth row <b>605</b> contains the percentage of null, which is calculated by the system <b>115</b> by dividing the number of unobserved attributes (row <b>601</b>) by the total population.</p><p id="p-0127" num="0124">The sixth row <b>606</b> contains the size of the sample taken from the raw data source <b>125</b>, which is counted by the system <b>115</b>.</p><p id="p-0128" num="0125">The seventh row <b>607</b> contains the sample fraction, which the system <b>115</b> calculates by dividing the sample size (row <b>606</b>) by the size of the data source from which the sample is drawn. In this example, the sample size is equal to the data source size (e.g., all of the data in the data source was used in the sample), and thus the sample fraction is 1.0 or 100%.</p><p id="p-0129" num="0126">The eighth row <b>608</b> contains the frequency of occurrence of the most commonly occurring attribute in the data set, which the system <b>115</b> calculates by dividing the number of occurrences by the total sample size (row <b>606</b>). In this example, the attribute &#x201c;N&#x201d; is the most commonly occurring attribute, and it appeared in 99.3232% of the rows in the sample.</p><p id="p-0130" num="0127">The ninth row <b>609</b> contains the frequency of occurrence of the second most commonly occurring attribute in the data set, which the system <b>115</b> calculates by dividing the number of occurrences for that attribute by the total sample size (row <b>606</b>). In this example, the attribute &#x201c;Y&#x201d; is the second most commonly occurring attribute, and it appeared in 0.6768% of the rows in the sample.</p><p id="p-0131" num="0128">In other examples of cardinality ridge data that have more than two attribute values, there may be a larger number of most common attribute rows similar to the eighth and ninth rows (<b>608</b>, <b>609</b>) of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. For example, the system <b>115</b> may determine and calculate the frequency of occurrence for the top five, 10, 20, 30, etc. most commonly occurring attributes, or for the number of attributes that together make up 60% of sample size, or the like. For example, if the dataset contained U.S. state names and the names appeared in the sample approximately in proportion to each state's population, then the four most common attribute rows starting at 608 might be California 11.9%, Texas 8.0%, Florida 6.8%, and New York 6.2%.</p><p id="p-0132" num="0129">As noted previously, in various embodiments, the system <b>115</b> will generate a cardinality ridge <b>320</b>, e.g., as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, only if the dataset contains a finite, repeating set of attributes, such as gender, occupation, city name, state name, and the like. If the dataset contains numeric attributes or a set of unique, non-repeating attributes, whether string or numeric, such as social security numbers or bank account numbers, then the system <b>115</b> will not generate a cardinality ridge <b>320</b> for that data.</p><p id="p-0133" num="0130"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example of the frequency statistics, attributes, values, etc., of a sensitivity ridge <b>335</b>, which in various embodiments can be generated by the system <b>115</b> from a data source <b>125</b> and stored in a file, a data structure, or the like, for example in the fingerprint cache <b>221</b>.</p><p id="p-0134" num="0131">In various implementations, to generate the sensitivity ridge <b>335</b>, the system <b>115</b> can, for example, assess, analyze, or calculate the contents of the data source <b>125</b> against predetermined descriptors that characterize various sensitive data types, including but not limited to SOCIAL SECURITY NUMBER, CREDIT CARD NUMBER, PASSPORT NUMBER, GENDER, and ADDRESS. Sensitive data types can be modeled using a combination of text patterns (for example the pattern for social security number would be 9 digits with hyphens between the third and fourth digit and fifth and sixth digits), checksums (for example credit card numbers), and list of known values (for example United States ZIP codes or genders). These patterns, dictionaries, and checksums are used to detect the presence of potentially sensitive data within a data source. <figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example of the reported results for a data source. Each attribute (e.g., column) within a data source will have an entry in the sensitivity ridge <b>335</b>, with each entry being a list of sensitive data types (if any) that match the pattern(s) in the data. In this example the &#x201c;website,&#x201d; &#x201c;gender,&#x201d; &#x201c;passport,&#x201d; and &#x201c;date_time_of_purchase&#x201d; attributes are consistent with the sensitive data types &#x201c;URL,&#x201d; &#x201c;GENDER,&#x201d; &#x201c;FRANCE_PASSPORT,&#x201d; and &#x201c;DATE&#x201d; and &#x201c;TIME,&#x201d; respectively; while &#x201c;purchase_amount&#x201d; and &#x201c;purchase_number&#x201d; do not match any sensitive data types. The admin user <b>135</b> or an application running on the admin device <b>140</b> can use the information in the sensitivity ridge <b>335</b> to identify which attributes in a data source <b>125</b> are most sensitive and, for example, apply more restrictive policies (e.g., policies that cause more information loss) to their datasets, for example, based on a set of rules.</p><p id="p-0135" num="0132"><figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref> together show an example of the frequency statistics, attributes, values, etc., of a string ridge <b>330</b>, which in various embodiments can be generated by the system <b>115</b> from a data source <b>125</b> (e.g., from the columns in a data source <b>125</b>, where each column has its own entry or data structure) and stored in a file, a data structure, or the like, for example in the fingerprint cache <b>221</b>.</p><p id="p-0136" num="0133">In various implementations, to generate the string ridge <b>330</b>, the system <b>115</b> can, for example, assess, analyze, or calculate the format and characteristics of strings attributes (e.g., columns) within the dataset. Similar to the cardinality ridge <b>320</b>, this string ridge <b>330</b> contains the number of missing elements seen in each string attribute, expressed as a percentage, using the field &#x201c;Percent NULL.&#x201d; In the example shown, the fields: Average String Length, Maximum String Length, and Minimum String Length, represent the average, maximum, and minimum number of characters in each string, respectively. Average White Space, Average Upper Case Characters, and Average Lower Case Characters represent the average number of character in each string that are spaces, upper case ([A-Z]), or lower case ([a-z]), respectively.</p><p id="p-0137" num="0134">Each string is further decomposed into a regular expression. This is done by breaking each string down into a series of contiguous character types: upper case ([A-Z]), lower case ([a-z]), digits ([\d]), white space ([\s]), and everything character type ([{circumflex over (&#x2003;)}A-Za-zMdMs]). This decomposition is assembled into a regular expression that matches the observed string. The frequency of each unique regular expression is counted.</p><p id="p-0138" num="0135">The field Leading Regular Expressions represents the most frequently occurring distinct regular expression. The field Leading Regular Expression Frequencies is the number of times each leading regular expression occurs. In various embodiments, the system <b>115</b> may use the results of the string ridge <b>330</b> to discover consistent patterns in the text, which may be used to augment the patterns within the sensitivity ridge <b>335</b> and discover unforeseen sensitive attributes, for example, provider-specific attributes such as medical plan numbers or medical record numbers.</p><p id="p-0139" num="0136"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an example of several obscurations, which may be, or be included in, a policy <b>217</b>. The obscurations shown in this non-limiting example include five different processes, algorithms, or techniques that can be performed on a dataset: generalize <b>983</b>, obfuscate <b>985</b>, suppress <b>987</b>, randomize <b>989</b>, and redact <b>991</b>. In various embodiments of the system <b>100</b>, an admin user <b>135</b> can create, select, and/or modify a policy <b>217</b> to include one or more of the obscuration techniques <b>983</b>-<b>991</b>. As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the system <b>115</b> may apply one or more of the obscuration techniques <b>983</b>-<b>991</b> to the raw data <b>981</b>, e.g., the data in the data source <b>125</b> and/or the virtualized database <b>123</b>, and produce obscured data <b>993</b>. The obscured data <b>993</b> may be provided to a query user <b>105</b> in response to a query <b>117</b>, and/or the obscured data <b>993</b> may be used by the system <b>115</b> to calculate the amount of information loss caused by a policy <b>217</b> that includes one or more of the obscuration techniques <b>983</b>-<b>991</b>.</p><p id="p-0140" num="0137">In various implementations, generalize <b>983</b> can be a process which reduces the precision of any datum. The implementations can take several forms, including but not limited to rounding off numeric attributes to arbitrary precision, bucketing numeric values into predefined buckets, bucketing date and/or time data to predefined buckets, or generalizing categorical data into less discriminating value. An example of rounding would be rounding a dollar value such as $37,450.45 to the nearest thousand ($37,000.00). An example of bucketing may be converting age values into predefined buckets for the age brackets of 00-14, 14-18, 18-65, and 65 and over. In this example scheme, an individual of age 35 would be simply bucketed into the 18-65 bucket, providing increased anonymity on their true age. Bucketing date or time data could take the form of representing the date, Apr. 1, 2019 as simply a month-year bucket (April 2019), a year bucket (2019), or some other defined date or time precision. Generalizing categorical attributes can take the form of rolling up some category into a category which is a superset of the existing value. An example would be representing the city, Ottumwa, Iowa as either the county (Wapello County, Iowa), the state (Iowa), or the country (United States).</p><p id="p-0141" num="0138">In various implementations, obfuscate <b>985</b> can be a process for replacing attributes from a data source <b>125</b> with some obscured value. The implementations could involve tokenization, hashing, encrypting, or otherwise replacing the sensitive data. For example the value or attribute &#x201c;Emma Smith&#x201d; could be represented using a unique hash of the string, such as &#x201c;1c272047233576d77a9b9a1acfdf741c.&#x201d; This obscures the original value (Emma Smith), while retaining a 1:1 mapping with the original value. As a result the obscured data still retains information content. Other implementations of obfuscation mechanisms include obscuring the surname, and exposing only &#x201c;Emma,&#x201d; tokenizing using a reversible process with a code book or dictionary, and encrypting the data.</p><p id="p-0142" num="0139">In various implementations, suppress <b>987</b> can include removing or replacing the value of an attribute from the data. For example, replacing all names with a placeholder such as &#x201c;Jane Doe&#x201d; or an empty string. This process severely reduces the information content of an attribute in a dataset <b>125</b>.</p><p id="p-0143" num="0140">In various implementations, randomize <b>989</b> can replace an attribute at random according to some non-zero probability. This type of obscuring technique will provide plausible uncertainty regarding the value of the attribute in any single record, while preserving some of the information content of the underlying data. Randomization mechanisms can include a randomized response, local differential privacy, or other stochastic perturbative process.</p><p id="p-0144" num="0141">In various implementations, redact <b>991</b> can be the conditional removal of a record or set of records from the dataset. This assures that certain records are not included in specific analyses or query results, in accordance with the right to be forgotten, residency requirements, or usage restrictions.</p><p id="p-0145" num="0142">As noted previously, each of the obscuration techniques <b>983</b> through <b>991</b> can be applied, e.g., via a policy <b>217</b>, either independently or in conjunction to create the obscured dataset <b>993</b>.</p><p id="p-0146" num="0143">Other embodiments of the invention will be apparent to those skilled in the art from consideration of the specification and practice of the invention disclosed herein. It is intended that the specification and examples be considered as exemplary only, with a true scope and spirit of the invention being indicated by the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004678A1-20230105-M00001.NB"><img id="EMI-M00001" he="6.01mm" wi="76.20mm" file="US20230004678A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004678A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.25mm" wi="76.20mm" file="US20230004678A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-001-3" num="001-3"><claim-text><b>1</b>.-<b>3</b>. (canceled)</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. A computer-implemented method for measuring an impact of a data privacy policy, the method comprising:<claim-text>sampling a data source to obtain a sample of data that is statistically representative;</claim-text><claim-text>determining ridge statistics for the sample of data;</claim-text><claim-text>measuring a first entropy of the sample of data;</claim-text><claim-text>applying the data privacy policy to the sample of data;</claim-text><claim-text>measuring a second entropy of the sample of data with the policy applied;</claim-text><claim-text>calculating an information loss value based on the difference between the first entropy and the second entropy; and</claim-text><claim-text>presenting the information loss value.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:<claim-text>applying the data privacy policy to the data source based on the information loss value.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. A computer-implemented method for measuring information loss, the method comprising:<claim-text>sampling a data source to obtain a sample of data that is statistically representative;</claim-text><claim-text>determining ridge statistics for the sample of data;</claim-text><claim-text>receiving a data privacy policy for a dataset of the data source;</claim-text><claim-text>determining an estimate of information loss caused by the data privacy policy using the ridge statistics; and</claim-text><claim-text>presenting the estimate of the information loss.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>applying the data privacy policy to the data source based on the estimate of the information loss.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein determining ridge statistics for the sample of data comprises creating numeric ridge statistics for the sample of data.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein determining ridge statistics for the sample of data comprises creating cardinality ridge statistics for the sample of data.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein determining ridge statistics for the sample of data comprises creating string ridge statistics for the sample of data.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein determining ridge statistics for the sample of data comprises creating sensitivity ridge statistics for the sample of data.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A system for measuring information loss for a data source, the system comprising:<claim-text>a computer-readable data storage device containing program instructions; and</claim-text><claim-text>a processor, operably connected to the computer-readable data storage device, that executes the program instructions to perform operations comprising:</claim-text><claim-text>determining one or more ridge statistics for the data source;</claim-text><claim-text>receiving a data privacy policy for the data source;</claim-text><claim-text>determining an estimate of information loss caused by the data privacy policy using the one or more ridge statistics; and</claim-text><claim-text>presenting the estimate of the information loss.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the operations further comprise sampling the data source to obtain a sample of data that is statistically representative.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein determining one or more ridge statistics for the data source comprises determining one or more ridge statistics for the sample of data.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the operations further comprise applying the data privacy policy to the data source.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein determining one or more ridge statistics for the data source comprises at least one of: creating numeric ridge statistics for the data source, creating cardinality ridge statistics for the data source, creating string ridge statistics for the data source, or creating sensitivity ridge statistics for the data source.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein determining one or more ridge statistics for the data source comprises obtaining one or more ridge statistics that were previously stored.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A computer-implemented method measuring information loss for a data source, the method comprising:<claim-text>determining one or more ridge statistics for the data source;</claim-text><claim-text>receiving a data privacy policy for the data source;</claim-text><claim-text>determining an estimate of information loss caused by the data privacy policy using the one or more ridge statistics; and</claim-text><claim-text>presenting the estimate of the information loss.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising sampling the data source to obtain a sample of data that is statistically representative.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein determining one or more ridge statistics for the data source comprises determining one or more ridge statistics for the sample of data.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising applying the data privacy policy to the data source.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein determining one or more ridge statistics for the data source comprises at least one of: creating numeric ridge statistics for the data source, creating cardinality ridge statistics for the data source, creating string ridge statistics for the data source, or creating sensitivity ridge statistics for the data source.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein determining one or more ridge statistics for the data source comprises obtaining one or more ridge statistics that were previously stored.</claim-text></claim></claims></us-patent-application>