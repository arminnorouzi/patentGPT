<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230003549A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230003549</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17856690</doc-number><date>20220701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>C</subclass><main-group>25</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>C</subclass><main-group>3</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>C</subclass><main-group>25</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>C</subclass><main-group>3</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">CALIBRATION OF SENSOR POSITION OFFSETS BASED ON ROTATION AND TRANSLATION VECTORS FOR MATCHED TRAJECTORIES</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63259233</doc-number><date>20210701</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Summer Robotics, Inc.</orgname><address><city>Campbell</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Paden</last-name><first-name>Brian Alexander</first-name><address><city>Scotts Valley</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Embodiments are directed to calibrating multi-view triangulation systems that perceive surfaces and objects based on reflections of one or more scanned laser beams that are continuously sensed by two or more sensors. In addition to sampling and triangulating points from a spline formed by an unbroken line trajectory of a laser beam, the calibration system samples and triangulates a corresponding velocity vector. Iterative reduction is performed on velocity vectors instead of points or splines. The velocity vector includes directions and magnitudes along a trajectory of a scanning laser beam which are used to determine the actual velocities. Translation and rotation vectors are based on the velocity vectors for matching trajectories determined for two or more sensors having offset physical positions, which are used to calibrate sensor offset errors associated with the matching trajectories provided to a modeling engine.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="107.78mm" wi="158.75mm" file="US20230003549A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="224.79mm" wi="161.46mm" file="US20230003549A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="240.62mm" wi="165.86mm" file="US20230003549A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="242.23mm" wi="165.86mm" file="US20230003549A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="214.21mm" wi="169.59mm" orientation="landscape" file="US20230003549A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="200.07mm" wi="124.04mm" orientation="landscape" file="US20230003549A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="206.42mm" wi="164.00mm" orientation="landscape" file="US20230003549A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="236.05mm" wi="156.29mm" file="US20230003549A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="236.05mm" wi="158.07mm" file="US20230003549A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="235.97mm" wi="166.79mm" file="US20230003549A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="236.05mm" wi="170.94mm" file="US20230003549A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="236.05mm" wi="171.96mm" file="US20230003549A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="236.39mm" wi="165.35mm" file="US20230003549A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="236.05mm" wi="121.84mm" file="US20230003549A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="240.62mm" wi="172.89mm" file="US20230003549A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="234.95mm" wi="162.90mm" file="US20230003549A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="219.54mm" wi="165.95mm" file="US20230003549A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a Utility Patent application based on previously filed U.S. Provisional Patent Application U.S. Ser. No. 63/259,233 filed on Jul. 1, 2021, the benefit of the filing date of which is hereby claimed under 35 U.S.C. &#xa7; 119(e) and the contents of which is further incorporated in entirety by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present invention relates generally to machine sensing or machine vision systems, and more particularly, but not exclusively, to real time calibration systems for light sensors of multi-view triangulation systems to improve accuracy and efficiency.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">The accuracy of a multi-view triangulation system generally requires knowledge of the relative geometric position of two or more light sensors, such as cameras and/or event sensor cameras. Also, accurate calibration of these sensors during assembly and between operations is essential to achieving accurate three-dimensional (3D) position estimations. Also, elaborate 3D rigging of an assembly of sensors and extensive calibration takes time and substantial resources. Rigidity (structural stiffness) of the frame on which the sensors are mounted is important. Unfortunately, structural deformations due to temperature variations or flexing due to external forces or vibrations of the assembly will rapidly degrade the accuracy of 3D triangulating systems.</p><p id="p-0005" num="0004">The rigidity requirements of mounting structures essentially limit the scale and range accuracy of these systems. For example, in an I-phone, the active near infrared spectroscopy (NIR) illumination dot projector of the facing camera (the selfie camera that unlocks the phone after a biometric scan of the user's face) is mounted on a specially designed rigid subassembly which holds this projector to within an exceedingly precise tolerance with respect to the 1 mega pixel NIR sensor mounted approx. 15 mm away. The physical constraints of the phone would make a base offset of more than 15 mm impractical. Since the range accuracy of this 3D triangulation system directly scales with this base offset, the size of this base offset limits the range and accuracy of the system. Even bulkier systems outfitted with especially stiff titanium torsion bars are limited in practice to a base offset of about 30 cm, limiting the accurate 3D triangulation to about 20&#xd7; that baseline, or about 6 meters.</p><p id="p-0006" num="0005">Typically, stereo 3D triangulation systems require extensive calibration target structures that are used to carefully calibrate such systems after they are constructed at their manufacturing location, or in a laboratory, or at specialized service and maintenance areas. Also, mobile 2D triangulation systems deployed in harsh environments such as on roads or mounted on drones for surveying in mines and construction sites tend to require frequent re-calibration. Consequently, this non-operational down time greatly diminishes their efficacy and reliability.</p><p id="p-0007" num="0006">It would be desirable to provide an efficient real-time calibration system for multi-view triangulation systems that can run in parallel with and continuously during operation without needing external calibration targets, that can continuously adjust the depth estimation algorithms for the actual current relative light sensor positions and orientations, i.e. their extrinsic parameters, and detect and track and correct for even minute changes of these extrinsic parameters. In other words, there is a need for a real-time calibration system that is fast enough to keep up with the various changes to the operation of multi-view triangulation systems. Further, such a novel real time calibration system and method, would be both instantaneous and accurate, enable flexible mounting of light sensors, and thereby enable far wider deployment of highly accurate multi-view triangulation systems, with substantially larger base offsets and triangulation accuracy, e.g., over much larger ranges of 30 to 40 meters such as when mounted on vehicles. Thus, it is with respect to these considerations and others that the present invention has been made.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007">Non-limiting and non-exhaustive embodiments of the present innovations are described with reference to the following drawings. In the drawings, like reference numerals refer to like parts throughout the various figures unless otherwise specified. For a better understanding of the described innovations, reference will be made to the following Detailed Description of Various Embodiments, which is to be read in association with the accompanying drawings, wherein:</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a system environment in which various embodiments may be implemented;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a schematic embodiment of a client computer;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a schematic embodiment of a network computer;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a logical architecture of a system for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a logical schematic of a system for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a logical representation of sensors and sensor output information for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> illustrates logical representations of scanning paths for two sensors that are offset from one another, wherein a Rotation vector R and translation vector t describe the relative position between the two sensors in accordance with one or more of the various embodiments;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates logical representations of scanning paths for two sensors that are offset from one another, wherein a rotation vector R and translation vector t describe the relative position between the two sensors and matching of scan splines and sample points in accordance with one or more of the various embodiments;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b>C</figref> illustrates logical representations of scanning paths for two sensors that are offset from one another, wherein the rotation vector R and translation vector t are estimated, matches are triangulated, and reprojected back into a two dimensional (2D) image plane in accordance with one or more of the various embodiments;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b>D</figref> illustrates logical representations of scanning paths for two sensors that are offset from one another, wherein the rotation vector R and translation vector t employing an exemplary variational bundle adjustment technique for reducing the reprojection error for the R and t.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b>E</figref> illustrates logical representations of scanning paths for matching trajectories along with rotation and translation vectors for two different sensors in accordance with one or more of the various embodiments;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a logical schematic of a system for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a logical schematic of a system for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an overview flowchart of a process for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates a flowchart of a process for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments; and</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a flowchart of a process for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF VARIOUS EMBODIMENTS</heading><p id="p-0025" num="0024">Various embodiments now will be described more fully hereinafter with reference to the accompanying drawings, which form a part hereof, and which show, by way of illustration, specific exemplary embodiments by which the invention may be practiced. The embodiments may, however, be embodied in many different forms and should not be construed as limited to the embodiments set forth herein; rather, these embodiments are provided so that this disclosure will be thorough and complete, and will fully convey the scope of the embodiments to those skilled in the art. Among other things, the various embodiments may be methods, systems, media or devices. Accordingly, the various embodiments may take the form of an entirely hardware embodiment, an entirely software embodiment or an embodiment combining software and hardware aspects. The following detailed description is, therefore, not to be taken in a limiting sense.</p><p id="p-0026" num="0025">Throughout the specification and claims, the following terms take the meanings explicitly associated herein, unless the context clearly dictates otherwise. The phrase &#x201c;in one embodiment&#x201d; as used herein does not necessarily refer to the same embodiment, though it may. Furthermore, the phrase &#x201c;in another embodiment&#x201d; as used herein does not necessarily refer to a different embodiment, although it may. Thus, as described below, various embodiments may be readily combined, without departing from the scope or spirit of the invention.</p><p id="p-0027" num="0026">In addition, as used herein, the term &#x201c;or&#x201d; is an inclusive &#x201c;or&#x201d; operator, and is equivalent to the term &#x201c;and/or,&#x201d; unless the context clearly dictates otherwise. The term &#x201c;based on&#x201d; is not exclusive and allows for being based on additional factors not described, unless the context clearly dictates otherwise. In addition, throughout the specification, the meaning of &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; include plural references. The meaning of &#x201c;in&#x201d; includes &#x201c;in&#x201d; and &#x201c;on.&#x201d;</p><p id="p-0028" num="0027">For example embodiments, the following terms are also used herein according to the corresponding meaning, unless the context clearly dictates otherwise.</p><p id="p-0029" num="0028">As used herein the term, &#x201c;engine&#x201d; refers to logic embodied in hardware or software instructions, which can be written in a programming language, such as C, C++, Objective-C, COBOL, Java&#x2122;, PHP, Perl, JavaScript, Ruby, VBScript, Microsoft .NET&#x2122; languages such as C#, or the like. An engine may be compiled into executable programs or written in interpreted programming languages. Software engines may be callable from other engines or from themselves. Engines described herein refer to one or more logical modules that can be merged with other engines or applications, or can be divided into sub-engines. The engines can be stored in non-transitory computer-readable medium or computer storage device and be stored on and executed by one or more general purpose computers, thus creating a special purpose computer configured to provide the engine.</p><p id="p-0030" num="0029">As used herein the term &#x201c;scanning signal generator&#x201d; refers to a system or device that may produce a beam that may be scanned/directed to project into an environment. For example, scanning signal generators may be fast laser-based scanning devices based on dual axis microelectromechanical systems (MEMS) that are arranged to scan a laser in a defined area of interest. The characteristics of scanning signal generator may vary depending on the application or service environment. Scanning signal generator are not strictly limited to lasers or laser MEMS, other type of beam signal generators may be employed depending on the circumstances. Critical selection criteria for scanning signal generator characteristics may include beam width, beam dispersion, beam energy, wavelength(s), phase, or the like. Scanning signal generator may be selected such that they enable sufficiently precise energy reflections from scanned surfaces or scanned objects in the scanning environment of interest. The scanning signal generators may be designed to scan up to frequencies of 10s of kHz. The scanning signal generators may be controlled in a closed loop fashion with one or more processor that may provide feedback about objects in the environment and instructs the scanning signal generator to modify its amplitudes, frequencies, phase, or the like.</p><p id="p-0031" num="0030">As used herein the term &#x201c;sensor&#x201d; refers to a device or system that can detect reflected energy from scanning signal generator. Sensors may be considered to comprise an array of detector cells that are responsive to energy reflected from scanning signal generators. Sensors may provide outputs that indicate which detector cells are triggered and the time they are triggered. Sensors may be considered to generate a sensor output that reports the cell location and time of detection for individual cell rather than being limited reporting the state or status of every cell. For example, sensors may include event sensor cameras, SPAD arrays, SiPM arrays, or the like.</p><p id="p-0032" num="0031">As used herein the terms &#x201c;trajectory,&#x201d; &#x201c;surface trajectory&#x201d; refers to one or more data structures that store or represent parametric representations of curve segments that may correspond to surfaces sensed by one or more sensors. Trajectories may include one or more attributes/elements that correspond to constants or coefficients of segments of one-dimensional analytical curves in three-dimensional space. Trajectories for a surface may be determined based on fitting or associating one or more sensor events to known analytical curves. Sensor events that are inconsistent with the analytical curves may be considered noise or otherwise excluded from trajectories.</p><p id="p-0033" num="0032">As used herein the term &#x201c;configuration information&#x201d; refers to information that may include rule-based policies, pattern matching, scripts (e.g., computer readable instructions), or the like, that may be provided from various sources, including, configuration files, databases, user input, built-in defaults, plug-ins, extensions, or the like, or combination thereof.</p><p id="p-0034" num="0033">The following briefly describes embodiments of the invention in order to provide a basic understanding of some aspects of the invention. This brief description is not intended as an extensive overview. It is not intended to identify key or critical elements, or to delineate or otherwise narrow the scope. Its purpose is merely to present some concepts in a simplified form as a prelude to the more detailed description that is presented later.</p><p id="p-0035" num="0034">Historically, active laser scanned 3D triangulation systems used a laser to illuminate at least one point at one time forming a scan line, a 3D time sequential trajectory across a remote (yet unknown) 3D surface (a remote manifold). For contiguous surfaces or surface elements the continuously lit up laser would thus create an unbroken line trajectory, i.e., splines, across sections of the 3D manifold during certain determined time intervals. Also, two or more cameras would be used to observe a 3D spline by back projecting these 3D trajectory splines onto their respective sensor planes as 2D time trajectories. Further, for every time t the laser beam points at a 3D surface point P; each sensor has a matching 2D image location P&#x2032;. Traditionally, the calibration required for a successful triangulation required that the relative rotation and translation in six degrees of freedom (6 DoF) between the two or more cameras was known quantitatively.</p><p id="p-0036" num="0035">Previously, extrinsic based calibration methods would guess the rotation vector R and the translation vector t, that in combination describe shifted perspective from one sensor view to any other sensor view, and to triangulate the matches, then reproject them back into the image plane. If they were off, then a mismatch would be identified. Further, a variational bundle adjustment technique could be used dialing in the R and t with the objective to minimize this mismatch. In general, the more matches, the better.</p><p id="p-0037" num="0036">Briefly stated, various embodiments are directed to a calibration system for multi-view triangulation systems that perceive surfaces and objects based on reflections of one or more scanned laser beams that are continuously sensed by two or more sensors. In addition to sampling and triangulating points from a spline formed by an unbroken line trajectory of a laser beam, the calibration system samples and triangulates a corresponding velocity vector. This vector adds to the information available per unit of time and is complementary to position measurements.</p><p id="p-0038" num="0037">In one or more embodiments, iterative reduction is performed on velocity vectors instead of points or splines. The velocity vector includes directions and magnitudes along a trajectory of a scanning laser beam which are used to determine the actual velocities.</p><p id="p-0039" num="0038">In one or more embodiments, at least two types of velocity vectors may be provided. A first type is a two-dimensional (2D) vector that is observed along a sensor plane that is traced out by a scanning laser beam. The second type is a 3D vector that is observed along surface contours that are traced out by the scanning laser beam. Also, for both vector types, they represent a unique velocity vector associated with every position along an observed trajectory segment of the scanned laser beam.</p><p id="p-0040" num="0039">In one or more embodiments, the 2D sensor plane (pixel row and column space) of sensors, reprojected spot velocities and directions are observed and measured in terms of motion across column and row boundaries, i.e., the rate at which x and y increment and/or decrement separately for each sensor. This motion is measured against time units, e.g., microseconds or nanoseconds, which yields a precise measure of instantaneous flying spot progression (movement). In this way, the velocity vector for each spline detected by each sensor is instantaneously available and may be continuously estimated in real time without delay.</p><p id="p-0041" num="0040">In one or more embodiments, distant reflections of the laser beam having little disparity provide enough of a tangent signal to enable calibration of a rotation matrix in an out of a sensor plane direction. Further, velocity vectors along the trajectories are scanned by the laser beam to yield rotational information quickly, which is useful for distant observations.</p><p id="p-0042" num="0041">In one or more embodiments, extrinsic information may be determined from velocity vector information. For example, 2D velocity vectors observed in one sensor may be, by a trial and error method, rotated and translated to match the 2D velocity vectors observed along the matching trajectory observed in another sensor. Iterative adjustment of the rotation and translation vectors, R and t, and taking care to minimize the observed mismatch is employed to provide &#x201c;the best fit&#x201d; values for R and t, i.e. resulting in an optimal extrinsic calibration.</p><p id="p-0043" num="0042">As shown, <figref idref="DRAWINGS">FIG. <b>7</b>E</figref> illustrates an overview <b>218</b> of the rotation and translation values as vectors <b>711</b> along the matching trajectories <b>706</b><i>a </i>and <b>706</b><i>b </i>for two different sensors <b>702</b> and <b>704</b>. Also, in one or more embodiments, the R rotation vector may be represented in radians for each of the x, y, z axes. Also, the t translation vector may be represented in magnitudes for each of the x, y, z axes relative to the optical center of each sensor.</p><p id="p-0044" num="0043">In one or more of the various embodiments, one or more trajectories may be generated based on a continuous stream of sensor events such that each trajectory may be a parametric representation of a one-dimensional curve segment in a three-dimensional space.</p><p id="p-0045" num="0044">In one or more of the various embodiments, the one or more trajectories may be employed to determine the one or more surfaces.</p><p id="p-0046" num="0045">In one or more of the various embodiments, the one or more trajectories may be provided to a modeling engine to execute one or more actions based on the one or more trajectories and the one or more surfaces.</p><p id="p-0047" num="0046">In one or more of the various embodiments, in response to one or more changes to the one or more surfaces, further actions may be performed, including: updating the one or more trajectories based on the continuous stream of sensor events; executing one or more additional actions based on the one or more updated trajectories and the one or more changed surfaces; or the like.</p><p id="p-0048" num="0047">In one or more of the various embodiments, the one or more changes to the one or more surfaces may include one or more of a position change, an orientation change, a motion change, a deformation of the one or more surfaces, or the like.</p><p id="p-0049" num="0048">In one or more of the various embodiments, the continuous stream of sensor events may be provided based on one or more sensors such that each sensor event includes one or more of a timestamp, time of flight, velocities or location values.</p><p id="p-0050" num="0049">In one or more of the various embodiments, one or more shapes that correspond to the one or more surfaces may be determined based on one or more characteristics of the one or more surfaces and the one or more trajectories.</p><p id="p-0051" num="0050">In one or more of the various embodiments, each trajectory may further include a parametric representation of a B-spline.</p><p id="p-0052" num="0051">In one or more of the various embodiments, the one or more trajectories may be employed to continuously determine one or more changes to one or more of a position of the one or more surfaces, an orientation of the one or more surfaces, a deformation of the one or more surfaces, a motion of the one or more surfaces, or the like.</p><p id="p-0053" num="0052">In one or more of the various embodiments, the modeling engine may be arranged to perform further actions including, determining one or more objects based on a portion of the one or more trajectories that may be associated with a portion of the one or more surfaces.</p><p id="p-0054" num="0053">In one or more of the various embodiments, the modeling engine may be arranged to perform further actions including, determining one or more features of the one or more objects based on the one or more trajectories such that the one or more features include one or more of a position, an orientation, a motion or a deformation of the one or more objects.</p><heading id="h-0006" level="2">Illustrated Operating Environment</heading><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows components of one embodiment of an environment in which embodiments of the invention may be practiced. Not all of the components may be required to practice the invention, and variations in the arrangement and type of the components may be made without departing from the spirit or scope of the invention. As shown, system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> includes local area networks (LANs)/wide area networks (WANs)-(network) <b>110</b>, wireless network <b>108</b>, client computers <b>102</b>-<b>105</b>, application server computer <b>116</b>, sensing systems <b>118</b>, or the like.</p><p id="p-0056" num="0055">At least one embodiment of client computers <b>102</b>-<b>105</b> is described in more detail below in conjunction with <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In one embodiment, at least some of client computers <b>102</b>-<b>105</b> may operate over one or more wired or wireless networks, such as networks <b>108</b>, or <b>110</b>. Generally, client computers <b>102</b>-<b>105</b> may include virtually any computer capable of communicating over a network to send and receive information, perform various online activities, offline actions, or the like. In one embodiment, one or more of client computers <b>102</b>-<b>105</b> may be configured to operate within a business or other entity to perform a variety of services for the business or other entity. For example, client computers <b>102</b>-<b>105</b> may be configured to operate as a web server, firewall, client application, media player, mobile telephone, game console, desktop computer, or the like. However, client computers <b>102</b>-<b>105</b> are not constrained to these services and may also be employed, for example, as for end-user computing in other embodiments. It should be recognized that more or less client computers (as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) may be included within a system such as described herein, and embodiments are therefore not constrained by the number or type of client computers employed.</p><p id="p-0057" num="0056">Computers that may operate as client computer <b>102</b> may include computers that typically connect using a wired or wireless communications medium such as personal computers, multiprocessor systems, microprocessor-based or programmable electronic devices, network PCs, or the like. In some embodiments, client computers <b>102</b>-<b>105</b> may include virtually any portable computer capable of connecting to another computer and receiving information such as, laptop computer <b>103</b>, mobile computer <b>104</b>, tablet computers <b>105</b>, or the like. However, portable computers are not so limited and may also include other portable computers such as cellular telephones, display pagers, radio frequency (RF) devices, infrared (IR) devices, Personal Digital Assistants (PDAs), handheld computers, wearable computers, integrated devices combining one or more of the preceding computers, or the like. As such, client computers <b>102</b>-<b>105</b> typically range widely in terms of capabilities and features. Moreover, client computers <b>102</b>-<b>105</b> may access various computing applications, including a browser, or other web-based application.</p><p id="p-0058" num="0057">A web-enabled client computer may include a browser application that is configured to send requests and receive responses over the web. The browser application may be configured to receive and display graphics, text, multimedia, and the like, employing virtually any web-based language. In one embodiment, the browser application is enabled to employ JavaScript, HyperText Markup Language (HTML), eXtensible Markup Language (XML), JavaScript Object Notation (JSON), Cascading Style Sheets (CS S), or the like, or combination thereof, to display and send a message. In one embodiment, a user of the client computer may employ the browser application to perform various activities over a network (online). However, another application may also be used to perform various online activities.</p><p id="p-0059" num="0058">Client computers <b>102</b>-<b>105</b> also may include at least one other client application that is configured to receive or send content between another computer. The client application may include a capability to send or receive content, or the like. The client application may further provide information that identifies itself, including a type, capability, name, and the like. In one embodiment, client computers <b>102</b>-<b>105</b> may uniquely identify themselves through any of a variety of mechanisms, including an Internet Protocol (IP) address, a phone number, Mobile Identification Number (MIN), an electronic serial number (ESN), a client certificate, or other device identifier. Such information may be provided in one or more network packets, or the like, sent between other client computers, application server computer <b>116</b>, sensing systems <b>118</b>, or other computers.</p><p id="p-0060" num="0059">Client computers <b>102</b>-<b>105</b> may further be configured to include a client application that enables an end-user to log into an end-user account that may be managed by another computer, such as application server computer <b>116</b>, sensing systems <b>118</b>, or the like. Such an end-user account, in one non-limiting example, may be configured to enable the end-user to manage one or more online activities, including in one non-limiting example, project management, software development, system administration, configuration management, search activities, social networking activities, browse various websites, communicate with other users, or the like. Also, client computers may be arranged to enable users to display reports, interactive user-interfaces, or results provided by sensing systems <b>118</b>.</p><p id="p-0061" num="0060">Wireless network <b>108</b> is configured to couple client computers <b>103</b>-<b>105</b> and its components with network <b>110</b>. Wireless network <b>108</b> may include any of a variety of wireless sub-networks that may further overlay stand-alone ad-hoc networks, and the like, to provide an infrastructure-oriented connection for client computers <b>103</b>-<b>105</b>. Such sub-networks may include mesh networks, Wireless LAN (WLAN) networks, cellular networks, and the like. In one embodiment, the system may include more than one wireless network.</p><p id="p-0062" num="0061">Wireless network <b>108</b> may further include an autonomous system of terminals, gateways, routers, and the like connected by wireless radio links, and the like. These connectors may be configured to move freely and randomly and organize themselves arbitrarily, such that the topology of wireless network <b>108</b> may change rapidly.</p><p id="p-0063" num="0062">Wireless network <b>108</b> may further employ a plurality of access technologies including 2nd (2G), 3rd (3G), 4th (4G) 5th (5G) generation radio access for cellular systems, WLAN, Wireless Router (WR) mesh, and the like. Access technologies such as 2G, 3G, 4G, 5G, and future access networks may enable wide area coverage for mobile computers, such as client computers <b>103</b>-<b>105</b> with various degrees of mobility. In one non-limiting example, wireless network <b>108</b> may enable a radio connection through a radio network access such as Global System for Mobil communication (GSM), General Packet Radio Services (GPRS), Enhanced Data GSM Environment (EDGE), code division multiple access (CDMA), time division multiple access (TDMA), Wideband Code Division Multiple Access (WCDMA), High Speed Downlink Packet Access (HSDPA), Long Term Evolution (LTE), and the like. In essence, wireless network <b>108</b> may include virtually any wireless communication mechanism by which information may travel between client computers <b>103</b>-<b>105</b> and another computer, network, a cloud-based network, a cloud instance, or the like.</p><p id="p-0064" num="0063">Network <b>110</b> is configured to couple network computers with other computers, including, application server computer <b>116</b>, sensing systems <b>118</b>, client computers <b>102</b>, and client computers <b>103</b>-<b>105</b> through wireless network <b>108</b>, or the like. Network <b>110</b> is enabled to employ any form of computer readable media for communicating information from one electronic device to another. Also, network <b>110</b> can include the Internet in addition to local area networks (LANs), wide area networks (WANs), direct connections, such as through a universal serial bus (USB) port, Ethernet port, other forms of computer-readable media, or any combination thereof. On an interconnected set of LANs, including those based on differing architectures and protocols, a router acts as a link between LANs, enabling messages to be sent from one to another. In addition, communication links within LANs typically include twisted wire pair or coaxial cable, while communication links between networks may utilize analog telephone lines, full or fractional dedicated digital lines including T1, T2, T3, and T4, or other carrier mechanisms including, for example, E-carriers, Integrated Services Digital Networks (ISDNs), Digital Subscriber Lines (DSLs), wireless links including satellite links, or other communications links known to those skilled in the art. Moreover, communication links may further employ any of a variety of digital signaling technologies, including without limit, for example, DS-0, DS-1, DS-2, DS-3, DS-4, OC-3, OC-12, OC-48, or the like. Furthermore, remote computers and other related electronic devices could be remotely connected to either LANs or WANs via a modem and temporary telephone link. In one embodiment, network <b>110</b> may be configured to transport information of an Internet Protocol (IP).</p><p id="p-0065" num="0064">Additionally, communication media typically embodies computer readable instructions, data structures, program modules, or other transport mechanism and includes any information non-transitory delivery media or transitory delivery media. By way of example, communication media includes wired media such as twisted pair, coaxial cable, fiber optics, wave guides, and other wired media and wireless media such as acoustic, RF, infrared, and other wireless media.</p><p id="p-0066" num="0065">Also, one embodiment of application server computer <b>116</b> or sensing systems <b>118</b> are described in more detail below in conjunction with <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Although <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates application server computer <b>116</b> and sensing systems <b>118</b> each as a single computer, the innovations or embodiments are not so limited. For example, one or more functions of application server computer <b>116</b>, sensing systems <b>118</b>, or the like, may be distributed across one or more distinct network computers. Moreover, in one or more embodiments, sensing systems <b>118</b> may be implemented using a plurality of network computers. Further, in one or more of the various embodiments, application server computer <b>116</b>, sensing systems <b>118</b>, or the like, may be implemented using one or more cloud instances in one or more cloud networks. Accordingly, these innovations and embodiments are not to be construed as being limited to a single environment, and other configurations, and other architectures are also envisaged.</p><heading id="h-0007" level="2">Illustrative Client Computer</heading><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows one embodiment of client computer <b>200</b> that may include many more or less components than those shown. Client computer <b>200</b> may represent, for example, one or more embodiment of mobile computers or client computers shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0068" num="0067">Client computer <b>200</b> may include processor <b>202</b> in communication with memory <b>204</b> via bus <b>228</b>. Client computer <b>200</b> may also include power supply <b>230</b>, network interface <b>232</b>, audio interface <b>256</b>, display <b>250</b>, keypad <b>252</b>, illuminator <b>254</b>, video interface <b>242</b>, input/output interface <b>238</b>, haptic interface <b>264</b>, global positioning systems (GPS) receiver <b>258</b>, open air gesture interface <b>260</b>, temperature interface <b>262</b>, camera(s) <b>240</b>, projector <b>246</b>, pointing device interface <b>266</b>, processor-readable stationary storage device <b>234</b>, and processor-readable removable storage device <b>236</b>. Client computer <b>200</b> may optionally communicate with a base station (not shown), or directly with another computer. And in one embodiment, although not shown, a gyroscope may be employed within client computer <b>200</b> to measuring or maintaining an orientation of client computer <b>200</b>.</p><p id="p-0069" num="0068">Power supply <b>230</b> may provide power to client computer <b>200</b>. A rechargeable or non-rechargeable battery may be used to provide power. The power may also be provided by an external power source, such as an AC adapter or a powered docking cradle that supplements or recharges the battery.</p><p id="p-0070" num="0069">Network interface <b>232</b> includes circuitry for coupling client computer <b>200</b> to one or more networks, and is constructed for use with one or more communication protocols and technologies including, but not limited to, protocols and technologies that implement any portion of the OSI model for mobile communication (GSM), CDMA, time division multiple access (TDMA), UDP, TCP/IP, SMS, MMS, GPRS, WAP, UWB, WiMax, SIP/RTP, GPRS, EDGE, WCDMA, LTE, UMTS, OFDM, CDMA2000, EV-DO, HSDPA, or any of a variety of other wireless communication protocols. Network interface <b>232</b> is sometimes known as a transceiver, transceiving device, or network interface card (MC).</p><p id="p-0071" num="0070">Audio interface <b>256</b> may be arranged to produce and receive audio signals such as the sound of a human voice. For example, audio interface <b>256</b> may be coupled to a speaker and microphone (not shown) to enable telecommunication with others or generate an audio acknowledgement for some action. A microphone in audio interface <b>256</b> can also be used for input to or control of client computer <b>200</b>, e.g., using voice recognition, detecting touch based on sound, and the like.</p><p id="p-0072" num="0071">Display <b>250</b> may be a liquid crystal display (LCD), gas plasma, electronic ink, light emitting diode (LED), Organic LED (OLED) or any other type of light reflective or light transmissive display that can be used with a computer. Display <b>250</b> may also include a touch interface <b>244</b> arranged to receive input from an object such as a stylus or a digit from a human hand, and may use resistive, capacitive, surface acoustic wave (SAW), infrared, radar, or other technologies to sense touch or gestures.</p><p id="p-0073" num="0072">Projector <b>246</b> may be a remote handheld projector or an integrated projector that is capable of projecting an image on a remote wall or any other reflective object such as a remote screen.</p><p id="p-0074" num="0073">Video interface <b>242</b> may be arranged to capture video images, such as a still photo, a video segment, an infrared video, or the like. For example, video interface <b>242</b> may be coupled to a digital video camera, a web-camera, or the like. Video interface <b>242</b> may comprise a lens, an image sensor, and other electronics. Image sensors may include a complementary metal-oxide-semiconductor (CMOS) integrated circuit, charge-coupled device (CCD), or any other integrated circuit for sensing light.</p><p id="p-0075" num="0074">Keypad <b>252</b> may comprise any input device arranged to receive input from a user. For example, keypad <b>252</b> may include a push button numeric dial, or a keyboard. Keypad <b>252</b> may also include command buttons that are associated with selecting and sending images.</p><p id="p-0076" num="0075">Illuminator <b>254</b> may provide a status indication or provide light. Illuminator <b>254</b> may remain active for specific periods of time or in response to event messages. For example, when illuminator <b>254</b> is active, it may backlight the buttons on keypad <b>252</b> and stay on while the client computer is powered. Also, illuminator <b>254</b> may backlight these buttons in various patterns when particular actions are performed, such as dialing another client computer. Illuminator <b>254</b> may also cause light sources positioned within a transparent or translucent case of the client computer to illuminate in response to actions.</p><p id="p-0077" num="0076">Further, client computer <b>200</b> may also comprise hardware security module (HSM) <b>268</b> for providing additional tamper resistant safeguards for generating, storing or using security/cryptographic information such as, keys, digital certificates, passwords, passphrases, two-factor authentication information, or the like. In some embodiments, hardware security module may be employed to support one or more standard public key infrastructures (PKI), and may be employed to generate, manage, or store keys pairs, or the like. In some embodiments, HSM <b>268</b> may be a stand-alone computer, in other cases, HSM <b>268</b> may be arranged as a hardware card that may be added to a client computer.</p><p id="p-0078" num="0077">Client computer <b>200</b> may also comprise input/output interface <b>238</b> for communicating with external peripheral devices or other computers such as other client computers and network computers. The peripheral devices may include an audio headset, virtual reality headsets, display screen glasses, remote speaker system, remote speaker and microphone system, and the like. Input/output interface <b>238</b> can utilize one or more technologies, such as Universal Serial Bus (USB), Infrared, WiFi, WiMax, Bluetooth&#x2122;, and the like.</p><p id="p-0079" num="0078">Input/output interface <b>238</b> may also include one or more sensors for determining geolocation information (e.g., GPS), monitoring electrical power conditions (e.g., voltage sensors, current sensors, frequency sensors, and so on), monitoring weather (e.g., thermostats, barometers, anemometers, humidity detectors, precipitation scales, or the like), or the like. Sensors may be one or more hardware sensors that collect or measure data that is external to client computer <b>200</b>.</p><p id="p-0080" num="0079">Haptic interface <b>264</b> may be arranged to provide tactile feedback to a user of the client computer. For example, the haptic interface <b>264</b> may be employed to vibrate client computer <b>200</b> in a particular way when another user of a computer is calling. Temperature interface <b>262</b> may be used to provide a temperature measurement input or a temperature changing output to a user of client computer <b>200</b>. Open air gesture interface <b>260</b> may sense physical gestures of a user of client computer <b>200</b>, for example, by using single or stereo video cameras, radar, a gyroscopic sensor inside a computer held or worn by the user, or the like. Camera <b>240</b> may be used to track physical eye movements of a user of client computer <b>200</b>.</p><p id="p-0081" num="0080">GPS transceiver <b>258</b> can determine the physical coordinates of client computer <b>200</b> on the surface of the Earth, which typically outputs a location as latitude and longitude values. GPS transceiver <b>258</b> can also employ other geo-positioning mechanisms, including, but not limited to, triangulation, assisted GPS (AGPS), Enhanced Observed Time Difference (E-OTD), Cell Identifier (CI), Service Area Identifier (SAI), Enhanced Timing Advance (ETA), Base Station Subsystem (BSS), or the like, to further determine the physical location of client computer <b>200</b> on the surface of the Earth. It is understood that under different conditions, GPS transceiver <b>258</b> can determine a physical location for client computer <b>200</b>. In one or more embodiment, however, client computer <b>200</b> may, through other components, provide other information that may be employed to determine a physical location of the client computer, including for example, a Media Access Control (MAC) address, IP address, and the like.</p><p id="p-0082" num="0081">In at least one of the various embodiments, applications, such as, operating system <b>206</b>, other client apps <b>224</b>, web browser <b>226</b>, or the like, may be arranged to employ geo-location information to select one or more localization features, such as, time zones, languages, currencies, calendar formatting, or the like. Localization features may be used in, file systems, user-interfaces, reports, as well as internal processes or databases. In at least one of the various embodiments, geo-location information used for selecting localization information may be provided by GPS <b>258</b>. Also, in some embodiments, geolocation information may include information provided using one or more geolocation protocols over the networks, such as, wireless network <b>108</b> or network <b>111</b>.</p><p id="p-0083" num="0082">Human interface components can be peripheral devices that are physically separate from client computer <b>200</b>, allowing for remote input or output to client computer <b>200</b>. For example, information routed as described here through human interface components such as display <b>250</b> or keyboard <b>252</b> can instead be routed through network interface <b>232</b> to appropriate human interface components located remotely. Examples of human interface peripheral components that may be remote include, but are not limited to, audio devices, pointing devices, keypads, displays, cameras, projectors, and the like. These peripheral components may communicate over a Pico Network such as Bluetooth&#x2122;, Zigbee&#x2122; and the like. One non-limiting example of a client computer with such peripheral human interface components is a wearable computer, which might include a remote pico projector along with one or more cameras that remotely communicate with a separately located client computer to sense a user's gestures toward portions of an image projected by the pico projector onto a reflected surface such as a wall or the user's hand.</p><p id="p-0084" num="0083">A client computer may include web browser application <b>226</b> that is configured to receive and to send web pages, web-based messages, graphics, text, multimedia, and the like. The client computer's browser application may employ virtually any programming language, including a wireless application protocol messages (WAP), and the like. In one or more embodiment, the browser application is enabled to employ Handheld Device Markup Language (HDML), Wireless Markup Language (WML), WMLScript, JavaScript, Standard Generalized Markup Language (SGML), HyperText Markup Language (HTML), eXtensible Markup Language (XML), HTML5, and the like.</p><p id="p-0085" num="0084">Memory <b>204</b> may include RAM, ROM, or other types of memory. Memory <b>204</b> illustrates an example of computer-readable storage media (devices) for storage of information such as computer-readable instructions, data structures, program modules or other data. Memory <b>204</b> may store BIOS <b>208</b> for controlling low-level operation of client computer <b>200</b>. The memory may also store operating system <b>206</b> for controlling the operation of client computer <b>200</b>. It will be appreciated that this component may include a general-purpose operating system such as a version of UNIX, or Linux&#xae;, or a specialized client computer communication operating system such as Windows Phone&#x2122;, or the Symbian&#xae; operating system. The operating system may include, or interface with a Java virtual machine module that enables control of hardware components or operating system operations via Java application programs.</p><p id="p-0086" num="0085">Memory <b>204</b> may further include one or more data storage <b>210</b>, which can be utilized by client computer <b>200</b> to store, among other things, applications <b>220</b> or other data. For example, data storage <b>210</b> may also be employed to store information that describes various capabilities of client computer <b>200</b>. The information may then be provided to another device or computer based on any of a variety of methods, including being sent as part of a header during a communication, sent upon request, or the like. Data storage <b>210</b> may also be employed to store social networking information including address books, buddy lists, aliases, user profile information, or the like. Data storage <b>210</b> may further include program code, data, algorithms, and the like, for use by a processor, such as processor <b>202</b> to execute and perform actions. In one embodiment, at least some of data storage <b>210</b> might also be stored on another component of client computer <b>200</b>, including, but not limited to, non-transitory processor-readable removable storage device <b>236</b>, processor-readable stationary storage device <b>234</b>, or even external to the client computer.</p><p id="p-0087" num="0086">Applications <b>220</b> may include computer executable instructions which, when executed by client computer <b>200</b>, transmit, receive, or otherwise process instructions and data. Applications <b>220</b> may include, for example, other client applications <b>224</b>, web browser <b>226</b>, or the like. Client computers may be arranged to exchange communications, such as, queries, searches, messages, notification messages, event messages, sensor events, alerts, performance metrics, log data, API calls, or the like, combination thereof, with application servers or network monitoring computers.</p><p id="p-0088" num="0087">Other examples of application programs include calendars, search programs, email client applications, IM applications, SMS applications, Voice Over Internet Protocol (VOIP) applications, contact managers, task managers, transcoders, database programs, word processing programs, security applications, spreadsheet programs, games, search programs, and so forth.</p><p id="p-0089" num="0088">Additionally, in one or more embodiments (not shown in the figures), client computer <b>200</b> may include an embedded logic hardware device instead of a CPU, such as, an Application Specific Integrated Circuit (ASIC), Field Programmable Gate Array (FPGA), Programmable Array Logic (PAL), or the like, or combination thereof. The embedded logic hardware device may directly execute its embedded logic to perform actions. Also, in one or more embodiments (not shown in the figures), client computer <b>200</b> may include one or more hardware microcontrollers instead of CPUs. In one or more embodiment, the one or more microcontrollers may directly execute their own embedded logic to perform actions and access its own internal memory and its own external Input and Output Interfaces (e.g., hardware pins or wireless transceivers) to perform actions, such as System On a Chip (SOC), or the like.</p><heading id="h-0008" level="2">Illustrative Network Computer</heading><p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows one embodiment of network computer <b>300</b> that may be included in a system implementing one or more of the various embodiments. Network computer <b>300</b> may include many more or less components than those shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. However, the components shown are sufficient to disclose an illustrative embodiment for practicing these innovations. Network computer <b>300</b> may represent, for example, one embodiment of at least one of application server computer <b>116</b>, or sensing systems <b>118</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0091" num="0090">Network computers, such as, network computer <b>300</b> may include a processor <b>302</b> that may be in communication with a memory <b>304</b> via a bus <b>328</b>. In some embodiments, processor <b>302</b> may be comprised of one or more hardware processors, or one or more processor cores. In some cases, one or more of the one or more processors may be specialized processors designed to perform one or more specialized actions, such as, those described herein. Network computer <b>300</b> also includes a power supply <b>330</b>, network interface <b>332</b>, audio interface <b>356</b>, display <b>350</b>, keyboard <b>352</b>, input/output interface <b>338</b>, processor-readable stationary storage device <b>334</b>, and processor-readable removable storage device <b>336</b>. Power supply <b>330</b> provides power to network computer <b>300</b>.</p><p id="p-0092" num="0091">Network interface <b>332</b> includes circuitry for coupling network computer <b>300</b> to one or more networks, and is constructed for use with one or more communication protocols and technologies including, but not limited to, protocols and technologies that implement any portion of the Open Systems Interconnection model (OSI model), global system for mobile communication (GSM), code division multiple access (CDMA), time division multiple access (TDMA), user datagram protocol (UDP), transmission control protocol/Internet protocol (TCP/IP), Short Message Service (SMS), Multimedia Messaging Service (MMS), general packet radio service (GPRS), WAP, ultra-wide band (UWB), IEEE 802.16 Worldwide Interoperability for Microwave Access (WiMax), Session Initiation Protocol/Real-time Transport Protocol (SIP/RTP), or any of a variety of other wired and wireless communication protocols. Network interface <b>332</b> is sometimes known as a transceiver, transceiving device, or network interface card (NIC). Network computer <b>300</b> may optionally communicate with a base station (not shown), or directly with another computer.</p><p id="p-0093" num="0092">Audio interface <b>356</b> is arranged to produce and receive audio signals such as the sound of a human voice. For example, audio interface <b>356</b> may be coupled to a speaker and microphone (not shown) to enable telecommunication with others or generate an audio acknowledgement for some action. A microphone in audio interface <b>356</b> can also be used for input to or control of network computer <b>300</b>, for example, using voice recognition.</p><p id="p-0094" num="0093">Display <b>350</b> may be a liquid crystal display (LCD), gas plasma, electronic ink, light emitting diode (LED), Organic LED (OLED) or any other type of light reflective or light transmissive display that can be used with a computer. In some embodiments, display <b>350</b> may be a handheld projector or pico projector capable of projecting an image on a wall or other object.</p><p id="p-0095" num="0094">Network computer <b>300</b> may also comprise input/output interface <b>338</b> for communicating with external devices or computers not shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Input/output interface <b>338</b> can utilize one or more wired or wireless communication technologies, such as USB&#x2122;, Firewire&#x2122;, WiFi, WiMax, Thunderbolt&#x2122;, Infrared, Bluetooth&#x2122;, Zigbee&#x2122;, serial port, parallel port, and the like.</p><p id="p-0096" num="0095">Also, input/output interface <b>338</b> may also include one or more sensors for determining geolocation information (e.g., GPS), monitoring electrical power conditions (e.g., voltage sensors, current sensors, frequency sensors, and so on), monitoring weather (e.g., thermostats, barometers, anemometers, humidity detectors, precipitation scales, or the like), or the like. Sensors may be one or more hardware sensors that collect or measure data that is external to network computer <b>300</b>. Human interface components can be physically separate from network computer <b>300</b>, allowing for remote input or output to network computer <b>300</b>. For example, information routed as described here through human interface components such as display <b>350</b> or keyboard <b>352</b> can instead be routed through the network interface <b>332</b> to appropriate human interface components located elsewhere on the network. Human interface components include any component that allows the computer to take input from, or send output to, a human user of a computer. Accordingly, pointing devices such as mice, styluses, track balls, or the like, may communicate through pointing device interface <b>358</b> to receive user input.</p><p id="p-0097" num="0096">GPS transceiver <b>340</b> can determine the physical coordinates of network computer <b>300</b> on the surface of the Earth, which typically outputs a location as latitude and longitude values. GPS transceiver <b>340</b> can also employ other geo-positioning mechanisms, including, but not limited to, triangulation, assisted GPS (AGPS), Enhanced Observed Time Difference (E-OTD), Cell Identifier (CI), Service Area Identifier (SAI), Enhanced Timing Advance (ETA), Base Station Subsystem (BSS), or the like, to further determine the physical location of network computer <b>300</b> on the surface of the Earth. It is understood that under different conditions, GPS transceiver <b>340</b> can determine a physical location for network computer <b>300</b>. In one or more embodiments, however, network computer <b>300</b> may, through other components, provide other information that may be employed to determine a physical location of the client computer, including for example, a Media Access Control (MAC) address, IP address, and the like.</p><p id="p-0098" num="0097">In at least one of the various embodiments, applications, such as, operating system <b>306</b>, sensing engine <b>322</b>, modeling and calibration engine <b>324</b>, web services <b>329</b>, or the like, may be arranged to employ geo-location information to select one or more localization features, such as, time zones, languages, currencies, currency formatting, calendar formatting, or the like. Localization features may be used in file systems, user-interfaces, reports, as well as internal processes or databases. In at least one of the various embodiments, geo-location information used for selecting localization information may be provided by GPS <b>340</b>. Also, in some embodiments, geolocation information may include information provided using one or more geolocation protocols over the networks, such as, wireless network <b>108</b> or network <b>111</b>.</p><p id="p-0099" num="0098">Memory <b>304</b> may include Random Access Memory (RAM), Read-Only Memory (ROM), or other types of memory. Memory <b>304</b> illustrates an example of computer-readable storage media (devices) for storage of information such as computer-readable instructions, data structures, program modules or other data. Memory <b>304</b> stores a basic input/output system (BIOS) <b>308</b> for controlling low-level operation of network computer <b>300</b>. The memory also stores an operating system <b>306</b> for controlling the operation of network computer <b>300</b>. It will be appreciated that this component may include a general-purpose operating system such as a version of UNIX&#xae;, or Linux&#xae;, or a specialized operating system such as Microsoft Corporation's Windows&#xae; operating system, or the Apple Corporation's macOS&#xae; operating system. The operating system may include, or interface with one or more virtual machine modules, such as, a Java virtual machine module that enables control of hardware components or operating system operations via Java application programs. Likewise, other runtime environments may be included.</p><p id="p-0100" num="0099">Memory <b>304</b> may further include one or more data storage <b>310</b>, which can be utilized by network computer <b>300</b> to store, among other things, applications <b>320</b> or other data. For example, data storage <b>310</b> may also be employed to store information that describes various capabilities of network computer <b>300</b>. The information may then be provided to another device or computer based on any of a variety of methods, including being sent as part of a header during a communication, sent upon request, or the like. Data storage <b>310</b> may also be employed to store social networking information including address books, buddy lists, aliases, user profile information, or the like. Data storage <b>310</b> may further include program code, data, algorithms, and the like, for use by a processor, such as processor <b>302</b> to execute and perform actions such as those actions described below. In one embodiment, at least some of data storage <b>310</b> might also be stored on another component of network computer <b>300</b>, including, but not limited to, non-transitory media inside processor-readable removable storage device <b>336</b>, processor-readable stationary storage device <b>334</b>, or any other computer-readable storage device within network computer <b>300</b>, or even external to network computer <b>300</b>. Data storage <b>310</b> may include, for example, evaluation models <b>314</b>, or the like.</p><p id="p-0101" num="0100">Applications <b>320</b> may include computer executable instructions which, when executed by network computer <b>300</b>, transmit, receive, or otherwise process messages (e.g., SMS, Multimedia Messaging Service (MMS), Instant Message (IM), email, or other messages), audio, video, and enable telecommunication with another user of another mobile computer. Other examples of application programs include calendars, search programs, email client applications, IM applications, SMS applications, Voice Over Internet Protocol (VOIP) applications, contact managers, task managers, transcoders, database programs, word processing programs, security applications, spreadsheet programs, games, search programs, and so forth. Applications <b>320</b> may include sensing engine <b>322</b>, modeling and calibration engine <b>324</b>, web services <b>329</b>, or the like, which may be arranged to perform actions for embodiments described below. In one or more of the various embodiments, one or more of the applications may be implemented as modules or components of another application. Further, in one or more of the various embodiments, applications may be implemented as operating system extensions, modules, plugins, or the like.</p><p id="p-0102" num="0101">Furthermore, in one or more of the various embodiments, sensing engine <b>322</b>, modeling and calibration engine <b>324</b>, web services <b>329</b>, or the like, may be operative in a cloud-based computing environment. In one or more of the various embodiments, these applications, and others, which comprise the management platform may be executing within virtual machines or virtual servers that may be managed in a cloud-based based computing environment. In one or more of the various embodiments, in this context the applications may flow from one physical network computer within the cloud-based environment to another depending on performance and scaling considerations automatically managed by the cloud computing environment. Likewise, in one or more of the various embodiments, virtual machines or virtual servers dedicated to sensing engine <b>322</b>, modeling and calibration engine <b>324</b>, web services <b>329</b>, or the like, may be provisioned and de-commissioned automatically.</p><p id="p-0103" num="0102">Also, in one or more of the various embodiments, sensing engine <b>322</b>, modeling and calibration engine <b>324</b>, web services <b>329</b>, or the like, may be located in virtual servers running in a cloud-based computing environment rather than being tied to one or more specific physical network computers.</p><p id="p-0104" num="0103">Further, network computer <b>300</b> may also comprise hardware security module (HSM) <b>360</b> for providing additional tamper resistant safeguards for generating, storing or using security/cryptographic information such as, keys, digital certificates, passwords, passphrases, two-factor authentication information, or the like. In some embodiments, hardware security module may employ to support one or more standard public key infrastructures (PKI), and may be employed to generate, manage, or store keys pairs, or the like. In some embodiments, HSM <b>360</b> may be a stand-alone network computer, in other cases, HSM <b>360</b> may be arranged as a hardware card that may be installed in a network computer.</p><p id="p-0105" num="0104">Additionally, in one or more embodiments (not shown in the figures), network computer <b>300</b> may include an embedded logic hardware device instead of a CPU, such as, an Application Specific Integrated Circuit (ASIC), Field Programmable Gate Array (FPGA), Programmable Array Logic (PAL), or the like, or combination thereof. The embedded logic hardware device may directly execute its embedded logic to perform actions. Also, in one or more embodiments (not shown in the figures), the network computer may include one or more hardware microcontrollers instead of a CPU. In one or more embodiment, the one or more microcontrollers may directly execute their own embedded logic to perform actions and access their own internal memory and their own external Input and Output Interfaces (e.g., hardware pins or wireless transceivers) to perform actions, such as System On a Chip (SOC), or the like.</p><heading id="h-0009" level="2">Illustrative Logical System Architecture</heading><p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a logical architecture of system <b>400</b> for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments.</p><p id="p-0107" num="0106">In this example, for some embodiments, sensing systems, such as system <b>400</b> may include one or more servers, such as sensing server <b>402</b>. In some embodiments, sensing servers may be arranged to include: one or more sensing engines, such as, sensing engine <b>404</b>; and one or more modeling and calibration engines, such as, modeling and calibration engine <b>406</b>.</p><p id="p-0108" num="0107">Also, in some embodiments, sensing systems may include one or more signal generators that may at least generate sensor information based on where the energy from the signal generator reflects from a surface. In this example, for some embodiments, signal generator <b>408</b> may be considered to be a laser scanning system. Further, in some embodiments, sensing systems may include one or more sensors that may receive the reflected signal energy. In this example, for some embodiments, the sensors may be considered sensors that may be arranged to generate sensor information that corresponds to the reflected signal energy. In this example, sensors, such as, sensor <b>410</b>, sensor <b>412</b>, sensor <b>414</b> may be considered to be CCDs, or the like, that provide two-dimensional (2D) sensor information based on the CCD cells that detect the reflected signal energy.</p><p id="p-0109" num="0108">Accordingly, in some embodiments, the 2D sensor information from each sensor may be provided to a sensing engine, such as, sensing engine <b>404</b>. In some embodiments, sensing engines may be arranged to synthesize the 2D points provided by the sensors into 3D points based on triangulation, or the like.</p><p id="p-0110" num="0109">Further, in some embodiments, sensing engines may be arranged to employ direct the signal generator (e.g., scanning laser <b>408</b>) to follow a specific pattern based on one or more path-functions. Thus, in some embodiments, signal generators may scan the subject area using a known and precise path that may be defined or described using one or more function that correspond to the curve/path of the scanning.</p><p id="p-0111" num="0110">Accordingly, in some embodiments, sensing engines may be arranged to synthesize information about the objects or surfaces scanned by the signal generate based on the 3D sensor information provided by the sensors and the known scanning curve pattern.</p><p id="p-0112" num="0111">In some embodiments, scanning signal generator <b>408</b> may implemented using one or more fast laser scanning devices, such as a dual-axis MEMS mirror that scans a laser beam. In some embodiments, the wavelength of the laser may be in a broad range from the UV into the IR. In some embodiments, scanning signal generators may be designed to scan up to frequencies of 10s of kHz. In some embodiments, scanning signal generators may be controlled in a closed loop fashion using one or more processors that may provide feedback about the objects in the environment and instruct the scanning signal generator to adapt one or more of amplitude, frequency, phase, or the like. In some cases, for some embodiments, scanning signal generator may be arranged to periodically switch on and off, such as, at points if the scanner may be slowing before changing direction or reversing direction.</p><p id="p-0113" num="0112">In some embodiments, system <b>400</b> may include two or more sensors, such as, sensor <b>410</b>, sensor <b>412</b>, sensor <b>414</b>, or the like. In some embodiments, sensors may comprise arrays of pixels or cells that are responsive to reflected signal energy. In some embodiment, sensors may be arranged such that some or all of the sensors share a portion of their fields of view with one another and with the scanning signal generator. Further, in some embodiments, the relative position and poses of each sensor may be known. Also, in some embodiments, each sensor employs synchronized clock. For example, in some embodiments, sensors may be time synchronized by using a clock of one sensor as the master clock or by using an external source that periodically sends a synchronizing signal to the sensors. Alternatively, in some embodiments, sensors may be arranged to provide sensor events to sensing engines independently or asynchronously of each other.</p><p id="p-0114" num="0113">Accordingly, as a beam from the scanning signal generator beam scans across the scene, the sensors receive the reflected signal energy (e.g., photons/light from lasers) and trigger events in their cells/pixels based on observing physical reflections in the scene. Accordingly, in some embodiments, each event (e.g., sensor event) in a sensor may be determined based on cell location and a timestamp based on where and when the reflected energy is detected in each sensor. Thus, in some embodiments, each sensor reports each sensor event independently as it is detected rather than collecting information/signal from the entire sensor array before providing the sensor event. This behavior may be considered distinguishable from many conventional pixel arrays or CCDs which may &#x2018;raster scan&#x2019; the entire array of cells before outputting signal data. In contrast, sensors, such as, sensor <b>410</b>, sensor <b>412</b>, sensor <b>414</b>, or the like, may immediately and continuously report signals (if any) from individual cells. Accordingly, the cells in an individual sensor do not share a collective exposure time rather each cell reports its own detection events. Accordingly, in some embodiments, sensors, such as, sensor <b>410</b>, sensor <b>412</b>, sensor <b>414</b>, or the like, may be based on Event Sensor cameras, SPAD, SiPM arrays, or the like.</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a logical schematic of system <b>500</b> for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments. In some embodiments, sensing engines, such as, sensing engine <b>502</b> may be arranged to be provided sensor outputs that represent sensor information, such as, location, timing. As described above, in some embodiments, signal generators, such as, scanning lasers may scan an area of interest such that reflections of the energy may be collected by sensors. Accordingly, in some embodiments, information from each sensor may be provided to sensing engine <b>502</b>.</p><p id="p-0116" num="0115">Also, in some embodiments, sensing engine <b>502</b> may be provided a scanning path that corresponds the scanning path of the scanning signal generator. Accordingly, in some embodiments, sensing engine <b>502</b> may employ the scanning path to determine the path that the scanning signal generator traverses to scan the area of interest.</p><p id="p-0117" num="0116">Accordingly, in some embodiments, sensing engine <b>502</b> may be arranged to generate sensor events correspond to a surface location in three-dimensions based on the sensor output. For example, if there may be three sensors, the sensing engine may employ triangulation to compute the location in the area of interest where the scanning signal energy was reflected. One of ordinary skill in the art will appreciate that triangulation or other similar techniques may be applied to determine the scanned location if the position of the sensors is known.</p><p id="p-0118" num="0117">In some embodiments, scanning signal generators (e.g., fast scanning laser) may be configured to execute a precision scanning pattern. Accordingly, in some embodiments, sensing engine <b>502</b> may be provided the particular scanning path function. Also, in some embodiments, sensing engine <b>502</b> may be arranged to determine the particular scanning path based on configuration information to account for local circumstances of local requirements.</p><p id="p-0119" num="0118">In one or more of the various embodiments, sensing engines, such as, sensing engine <b>502</b> may generate a sequence of surface trajectories that may be based the scan path and the sensor information synthesized from the sensor output <b>504</b>.</p><p id="p-0120" num="0119"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a logical representation of sensors and sensor output information for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments.</p><p id="p-0121" num="0120">In one or more of the various embodiments, sensing engines may be provided sensor output from various sensors. In some embodiments, the particular sensor characteristics may vary depending on the particular application that perceiving objects based on sensing surfaces and sensing surface motion may be directed towards. In this example, for some embodiments, sensor <b>602</b>A may be considered to represent a generic sensor that can emit signals that correspond to the precise location on the sensor where reflected energy from the scanning signal generator may be detected. For example, sensor <b>602</b>A may be considered an array of detector cells that reports the cell location that has detected energy reflected from the scanning signal generator. In this example, horizontal location <b>604</b> and vertical location <b>606</b> may be considered to represent a location corresponding to the location in sensor <b>602</b> where reflected signal energy has been detected.</p><p id="p-0122" num="0121">In one or more of the various embodiments, sensing engines may be arranged to receive sensor information for one or more detection events from one or more sensors. Accordingly, in some embodiments, sensing engines may be arranged to determine additional information about the source of the reflect energy (beam location on scanned surface) based on triangulation or other methods. In some embodiments, if sensing engines employs triangulation or other methods to locate the location of the signal beam in the scanning environment, the combined sensor information may be considered a single sensor event comprising a horizontal (x) location, vertical location (y) and time component (t). Also, in some embodiments, sensor event may include other information, such as, time-of-flight information depending on the type or capability of the sensors.</p><p id="p-0123" num="0122">Further, as described above, the scanning signal generator (e.g., scanning laser) may be configured to traverse a precise path/curve (e.g., scanning path). Accordingly, in some embodiments, the pattern or sequence of cells in the sensors that detect reflected energy will follow a path/curve that is related to the path/curve of the scanning signal generator. Accordingly, in some embodiments, if the signal generator scans a particular path/curve a related path/curve of activated cells in the sensors may be detected. Thus, in this example, for some embodiments, path <b>608</b> may represent a sequence of cells in sensor <b>602</b>B that have detected reflected energy from the scanning signal generator.</p><p id="p-0124" num="0123">In one or more of the various embodiments, sensing engines may be arranged to fit sensor events to the scanning path curve. Accordingly, in one or more of the various embodiments, sensing engines may be arranged to predict where sensor events should occur based on the scanning path curve to determine information about the location or orientation of scanned surfaces or objects. Thus, in some embodiments, if sensing engines receive sensor events that are unassociated with the known scanning path curve, sensing engines may be arranged to perform various actions, such as, closing the current trajectory and beginning a new trajectory, discarding the sensor event as noise, or the like.</p><p id="p-0125" num="0124">In one or more of the various embodiments, scanning path curves may be configured in advance within the limits or constraints of the scanning signal generator and the sensors. For example, a scanning signal generator may be configured or directed to scan the scanning environment using a various curves including Lissajous curves, 2D lines, or the like. In some cases, scanning path curves may be considered piece-wise function in that they change direction or shape at different parts of the scan. For example, a 2D line scan path may be configured to change direction if the edge of the scanning environment (e.g., field-of-view) is approached.</p><p id="p-0126" num="0125">One of ordinary skill in the art will appreciate that if an unobstructed surface is scanned, the scanning frequency, scanning path, and sensor response frequency may determine if the sensor detection path appears as a continuous path. Thus, the operational requirements of the scanning signal generator, sensor precision, sensor response frequency, or the like, may vary depending on application of the system. For example, if the scanning environment may be relatively low featured and static, the sensors may have a lower response time because the scanned environment is not changing very fast. Also, for example, if the scanning environment is dynamic or includes more features of interest, the sensors may require increased responsiveness or precision to accurately capture the paths of the reflected signal energy. Further, in some embodiments, the characteristics of the scanning signal generator may vary depending on the scanning environment. For example, if lasers are used for the scanning signal generator, the energy level, wavelength, phase, beam width, or the like, may be tuned to suit the environment.</p><p id="p-0127" num="0126">In one or more of the various embodiments, sensing engines may be provided sensor output as a continuous stream of sensor events or sensor information that identifies the cell location in the sensor cell-array and a timestamp that corresponds to when the detection event occurred.</p><p id="p-0128" num="0127">In this example, for some embodiments, data structure <b>610</b> may be considered a data structure for representing instantaneous sensor events based on sensor output provided to a sensing engine. In this example, column <b>612</b> represents the horizontal position of the location in the scanning environment; column <b>614</b> represent a vertical position in the scanning environment; column <b>616</b> represents the time of the event; and column <b>617</b> represents an instantaneous velocity vector based on the vertical position, horizontal position, and time for the sensor event. Accordingly, in some embodiments, sensing engines may be arranged to determine which (if any) sensor events should be associated with a trajectory. In some embodiments, sensing engines may be arranged to associated sensor events with existing trajectories or create new trajectories. In some embodiments, if the sensor events fit an expected/predicted curve as determined based on the scanning path curve, sensing engines may be arranged to associate the sensor events with an existing trajectory or create a new trajectory. Further, in some cases, for some embodiments, sensing engines may be arranged to determine one or more sensor event as noise if their location deviates from a predicted path beyond a defined threshold value. Also, in some embodiments, sensing engines may be arranged to determine a velocity vector for one or more of a sensor event or a trajectory.</p><p id="p-0129" num="0128">In one or more of the various embodiments, sensing engines may be arranged to determine sensor events for each individual sensor rather being limited to provide sensor events computed based on outputs from multiple sensors. For example, in some embodiments, sensing engines may be arranged to provide a data structure similar to data structure <b>610</b> to collect sensor events for individual sensors.</p><p id="p-0130" num="0129">In some embodiments, sensing engines may be arranged to generate a sequence of trajectories that correspond to the reflected energy paths detected by the sensors. In some embodiments, sensing engines may be arranged to employ one or more data structures, such as, calculated data structure <b>618</b> to represent a trajectory that are determined based on the information captured by the sensors. In this example, data structure <b>610</b> may be table-like structure that includes columns, such as, column <b>620</b> for storing a first x-position, column <b>622</b> for storing a second x-position, column <b>624</b> for storing a first y-position, column <b>626</b> for storing a second y-position, column <b>628</b> for storing the beginning time of a trajectory, column <b>630</b> for storing an end time of a trajectory, of the like. Also, column <b>631</b> includes a calculated velocity for each set of points on a spline.</p><p id="p-0131" num="0130">In this example, row <b>632</b> represents information for a first trajectory and row <b>634</b> represents information for another trajectory. As described herein, sensing engines may be arranged to employ one or more rules or heuristics to determine if one trajectory ends and another begins. In some embodiments, such heuristics may include observing the occurrence sensor events that are geometrically close or temporally close. Note, the particular components or elements of a trajectory may vary depending on the parametric representation of the analytical curve or the type of analytical curve associated with the scanning path and the shape or orientation of the scanned surfaces. Accordingly, one of ordinary skill in the art will appreciate that different types of analytical curves or curve representations may result in more or fewer parameters for each trajectory. Thus, in some embodiments, sensing engines may be arranged to determine the specific parameters for trajectories based on rules, templates, libraries, or the like, provided via configuration information to account for local circumstances or local requirements</p><p id="p-0132" num="0131">In one or more of the various embodiments, trajectories may be represented using curve parameters rather than a collection of individual points or pixels. Accordingly, in some embodiments, sensing engines may be arranged to employ one or more numerical methods to continuously fit sequences of sensor events to scanning path curves. Also, another data structure (not shown) that is based on calculated data structure <b>618</b> may be generated that uses one or more numerical methods to fit and smooth out the calculated spline with sampled points that include velocity vectors at the sampled points.</p><p id="p-0133" num="0132">Further, in some embodiments, sensing engines may be arranged to employ one or more smoothing methods to improve the accuracy of trajectories or trajectory fitting. For example, in some embodiments, the scanning curve may be comprised of sensor events triggered by a scanning laser that may not one cell wide because in some cases reflected energy may splash to neighboring cells or land on the border of two or more cells. Accordingly, in some embodiments, to better estimate the real position of the reflected signal beam as it traverses the sensor plane, sensing engines may be arranged to perform an online smoothing estimate, e.g., using a smoothing Kalman filter to predict where the scanning beam point should have been in fractional units of detector cell position and fractional units of the fundamental timestamp of the sensor. Also, in some embodiments, sensing engines may be arranged to employ a batch-based optimization routine such as weighted least squares to fit a smooth curve to continuous segments of the scanning trajectory, which may correspond to when the scanning signal generator beam was scanning over a continuous surface.</p><p id="p-0134" num="0133">Also, in some embodiments, the scanning path may be employed to determine if trajectories begin or end. For example, if the scanning path reaches an edge of a scanning area and changes direction, in some cases, a current trajectory may be terminated while a new trajectory may be started to begin capturing information based on the new direction of the scan. Also, in some embodiments, objects or other features that occlude or obstruct scanning energy or reflected scanning energy may result in breaks in the sensor output that introduce gaps or other discontinuities that may trigger a trajectory to be closed and another trajectory to be opened subsequent to the break or gap. Further, in some embodiments, sensing engines may be configured to have a maximum length of trajectories such that a trajectory may be closed if it has collected enough sensor events or enough time has elapsed from the start of the trajectory.</p><p id="p-0135" num="0134">Also, in some embodiments, sensing engines may be arranged to determine trajectories for individual sensor. Accordingly, in some embodiments, sensing engines may be arranged to provide data structures similar to data structure <b>618</b> for each sensor.</p><p id="p-0136" num="0135">Also, <figref idref="DRAWINGS">FIGS. <b>7</b>A, <b>7</b>B, <b>7</b>C, and <b>7</b>D</figref> illustrate a canonical approach to find a series of points along the 3D trajectories over some time interval and use a sufficient amount of these discrete sequentially observed spatial points (sampled from the splines) to mathematically derive the &#x201c;extrinsics&#x201d;, i.e., the six degrees of freedom for these sensors (time trajectory &#x3e;&#x3e;sequential points &#x3e;&#x3e;extrinsics). Further, this series of points may be represented as refined non-integer (real number) estimations of real time values associated with successive positions along the spline for the 3D time trajectory.</p><p id="p-0137" num="0136"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> shows an overview <b>700</b> of two sensors <b>702</b> and <b>704</b> that are physically offset from one another. Also, the rotation vector R and translation vector t together describe the relative offset positions between two or more sensors. Further, each sensor position itself has six degrees of freedom: e.g. a position of its optical center (e.g cartesian coordinates x,y,z) and its pointing direction (e.g. elevation, azimuth and yaw).</p><p id="p-0138" num="0137"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> shows an overview <b>712</b> of the matching of scan splines and sample points <b>706</b><i>a </i>and <b>706</b><i>b </i>at some time interval to get matched points. Further, <figref idref="DRAWINGS">FIG. <b>7</b>C</figref> illustrates an overview <b>714</b> of the results of estimating the R and t, triangulating the matches, and then reprojecting back into the 2D image plane <b>709</b>. If the reprojection is off, there will be a mismatch <b>708</b>, i.e., the reprojection error. Also, <figref idref="DRAWINGS">FIG. <b>7</b>D</figref> shows an exemplary variational bundle adjustment technique for reducing the reprojection error for the R and t. Ideally, extrinsic calibration is achieved when the reprojection error is minimized to zero.</p><p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a logical schematic of system <b>800</b> for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments. As described above, in some embodiments, sensing engines may be arranged to generate trajectory information based on sensor output collected based on reflected signal energy.</p><p id="p-0140" num="0139">Accordingly, in some embodiments, modeling and calibration engines, such as, modeling and calibration engine <b>802</b> may be arranged to receive matching trajectory information, such as, trajectory information <b>804</b>A an <b>804</b>B from two sensors offset in position from one another (not shown) and translation and rotation vectors to calibrate position offset errors for the two sensors. In this example, for brevity and clarity trajectory information <b>804</b>A and <b>804</b>B is illustrated using drawings that may be representative of the trajectory information. However, in some embodiments, a sensing engine may provide the modeling and calibration engines parameterized trajectory information provided via one or more data structures, such as, data structures <b>610</b> and <b>618</b> shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0141" num="0140">Accordingly, modeling and calibration engine <b>802</b> may be arranged to employ evaluation models <b>1006</b> to evaluate matching trajectory information <b>804</b>A and <b>804</b>B and translation and rotation vectors associated with the physically offset positions of the two sensors. In some embodiments, evaluation models may be configured to include one or more heuristics, rules, conditions, machine learning classifiers, or the like, that may be employed to evaluate the scanned environment based on trajectory information <b>804</b>A and <b>804</b>B. In some embodiments, evaluation models may be conventionally trained or tuned to recognize or perceive various objects, shapes, actions, activities, relationships between or among objects, or the like. However, in some embodiments, input data used for training or tuning evaluation models may be in the form of numerical representations of trajectories. In one or more embodiments, a stream of updating parametric representation of analytical curve segments, video frame captures, pixel based edge detection, pixel based motion detection, pixel based color/brightness gradients, or the like may be provided in place of information derived from point clouds.</p><p id="p-0142" num="0141">In some embodiments, if trajectory information <b>804</b>A and <b>804</b>B may be evaluated, modeling and calibration engines may be arranged to generate or update one or more scene reports that provide information about the evaluated scenes. In some embodiments, scene reports may comprise conventional reports, interactive reports, graphical dashboards, charts, plots, or the like. Also, in some embodiments, scene reports may comprise one or more data structures that include information that may represent various scene features that may further be provided to one or more machine vision applications, such as, machine vision application <b>810</b> that may automatically interpret the scene reports.</p><p id="p-0143" num="0142">One of ordinary skill in the art will appreciate that many machine vision or machine perception applications may employ the innovation described herein. For example, in one or more of the various embodiments, the combination of high-speed surface scanning and low latency, high throughput sensors, arranged in a configuration that provides a direct depth measurement, may enable sensing engines to scan the entire field of view in one millisecond, measuring surfaces with a radial precision on the scale of a human nose at 30 meters or the individual threads on an M3 screw at 0.5 meters. Also, in some embodiments, sensing engines may be arranged to progressively scans the environment. Accordingly, in some embodiments, sensing engines may be arranged to provide increasing radial precision measurements of the surface as more observations are collected.</p><p id="p-0144" num="0143">Also, in some embodiments, because the trajectories for surface representation may be built up progressively on sub-millisecond scales, sensing systems may be enabled to track motions by observing how the apparent distance of the surface in a very recently scanned region has changed over time periods that may be short enough to be considered continuous time updates. Accordingly, in some embodiments, sensing engines may be arranged to provide a full six degrees (6D) representation of reality comprised of 2D surfaces with orientations moving through 3D space over time. This native 6D representation may be advantageous because it may provide for a more expressive and accurate basis for various application specific perception algorithms to operate on.</p><p id="p-0145" num="0144">Also, in some embodiments, machine learning based recognition algorithms may use shape and motion primitives (e.g., trajectories) as their input instead of 2D color contrast or 3D point arrays. Representing scanning environment using trajectories distinctly and accurately identify shapes and features as they exist in the scanned environment. In contrast, some conventional machine vision systems may rely on guesses or statistical approximations about whether there may an object in a location or a phantom due to novel contrasts, or whether points in space are part of the same surface or how the surface is moving.</p><p id="p-0146" num="0145">Further, in some embodiments, sensing engines may require fewer data values to represent surface or object features using parameterized trajectories than conventional representation using 2D pixel or 3D point clouds. Also, in some embodiments, representing surfaces/objects using trajectories may be advantageous because they may be natively invariant under many transforms such as rotations, translations and lighting changes.</p><p id="p-0147" num="0146">Accordingly, in some embodiments, the amount of data collected to train deep learning recognition algorithms may be orders of magnitude less than the less expressive representations such as 2D images or 3D point clouds. As an example, 2D color techniques employed to recognize a pedestrian may require the collection of data with pedestrians in all possible poses and positions with respect to the system, along with all possible lighting conditions and ambient background textures. In contrast, in some embodiments, sensing engines may be arranged to identify the surface shape of pedestrians in a variety of poses to not just localize and recognize them, but also describe very important properties such as their orientation and relative motion of body parts, all done within several milliseconds of first seeing the pedestrian.</p><p id="p-0148" num="0147">Also, in some embodiments, sensing systems as described herein may be applied to other application domains beyond autonomous mobility. For example, in some embodiments, sensing engines and modeling and calibration engines may be employed by a robot picking fruit in a field to search through dense foliage for a ripe fruit, identifying the surface characteristics in fine detail, while at the same time calculating the optimal grasp location in sub-millisecond updates while the robot hand quickly reaches in to grab the berry, far faster than human pickers or conventional picking machines.</p><p id="p-0149" num="0148"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a logical schematic of system <b>900</b> for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments. As described above, in some embodiments, scanning signal generators may scan for surfaces in scanning environments. In some cases, conditions of the scanning environment or characteristics of the scanned surfaces may result in one or more spurious sensor events (e.g., noise) generated by one or more sensors. For example, sensor view <b>902</b> represents a portion of sensor events that may be generated during a scan.</p><p id="p-0150" num="0149">In conventional machine vision applications, one or more 2D filters may be applied to a captured video image, point clusters, or the like, to attempt to separate noise events from the signals of interest. In some cases, conventional 2D image-based filters may be disadvantageous because they may employ one or more filters (e.g., weighted moving averaging, Gaussian filters, or the like) that may rely on statistical evaluation of pixel color/weight, pixel color/weight gradients, pixel distribution/clustering, or the like. Accordingly, in some cases, conventional 2D image filtering may be inherently fuzzy and highly dependent on application/environmental assumptions. Also, in some cases, conventional noise detection/noise reduction methods may erroneously miss some noise events while at the same time misclassifying one or more scene events as noise.</p><p id="p-0151" num="0150">In contrast, in some embodiments, sensing engines may be arranged to associate sensor events into trajectories based on precise heuristics, such as, nearness in time and location that may be used to fit sensor events to analytical curves that may be predicted based on the scanning path. Because scanning paths are defined in advance, sensing engines may be arranged to predict which sensor events should be included in the same trajectory.</p><p id="p-0152" num="0151">Further, in some embodiments, if surface or object features create gaps or breaks in trajectories, sensing engines may be arranged to close the current trajectory and start a new trajectory as soon as one may be recognized.</p><p id="p-0153" num="0152">Also, in some embodiments, sensing engines may be arranged to determine trajectories directly from sensor events having the form (x, y, t) rather than employing fuzzy pattern matching or pattern recognition methods. Thus, in some embodiments, sensing engines may be arranged to accurately compute distance, direction, or the like, rather than relying fuzzy machine vision methods to distinguish noise from sensor events that should be in the same trajectory.</p><heading id="h-0010" level="2">Generalized Operations</heading><p id="p-0154" num="0153"><figref idref="DRAWINGS">FIGS. <b>10</b>-<b>12</b></figref> represent generalized operations for perceiving objects based on sensing surfaces and sensing surface motion and calibrating the positions of sensors that are offset from one another in accordance with one or more of the various embodiments. In one or more of the various embodiments, processes <b>1000</b>, <b>1100</b> and <b>1200</b> described in conjunction with <figref idref="DRAWINGS">FIGS. <b>10</b>-<b>12</b></figref> may be implemented by or executed by one or more processors on a single network computer (or network monitoring computer), such as network computer <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In other embodiments, these processes, or portions thereof, may be implemented by or executed on a plurality of network computers, such as network computer <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In yet other embodiments, these processes, or portions thereof, may be implemented by or executed on one or more virtualized computers, such as, those in a cloud-based environment. However, embodiments are not so limited and various combinations of network computers, client computers, or the like may be utilized. Further, in one or more of the various embodiments, the processes described in conjunction with <figref idref="DRAWINGS">FIGS. <b>10</b>-<b>12</b></figref> may perform actions for perceiving objects based on sensing surfaces and sensing surface motion in accordance with at least one of the various embodiments or architectures such as those described herein. Further, in one or more of the various embodiments, some or all of the actions performed by processes <b>1000</b>, <b>1100</b>, and <b>1200</b> may be executed in part by sensing engine <b>322</b>, or modeling and calibration engine <b>324</b> running on one or more processors of one or more network computers.</p><p id="p-0155" num="0154"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an overview flowchart of process <b>1000</b> for perceiving objects based on sensing surfaces and sensing surface motion and calibrating trajectories for sensors offset from one another in accordance with one or more of the various embodiments. After a start flowchart block, at flowchart block <b>1002</b>, in one or more of the various embodiments, one or more scanning signal generators, two or more sensors, or the like. Also, in some embodiments, a specific scanning path may be provided to direct the beam or signal from the scanning signal generator to traverse a specified curve or path through a scanning environment. At block <b>1004</b>, in one or more of the various embodiments, sensing engines may be arranged to employ the scanning signal generator to scan a signal beam through the environment of interest to collect signal reflections of the signal at the sensors. At block <b>1006</b>, in one or more of the various embodiments, sensing engines may be arranged to provide matching scene trajectories based on the sensor output information from the two or more sensors. Also, at block <b>1008</b>, rotation and translation vectors representing position offset errors between the two or more sensors are determined. The rotation and translation vectors are based on determined velocities for the matching scene trajectories from the two or more sensors. The rotation and translation vectors enable calibration of the two or more sensors having physical positions that are offset from each other.</p><p id="p-0156" num="0155">At block <b>1010</b>, in one or more of the various embodiments, sensing engines may be arranged to provide one or more matching scene trajectories with the rotation and translation vectors to one or more modeling engines. The one or more modeling engines may employ the rotation and translation vectors to calibrate the offset errors for the different positions of the two or more sensors that provide the matching scene trajectories.</p><p id="p-0157" num="0156">At block <b>1012</b>, in one or more of the various embodiments, one or more modeling engines may be arranged to evaluate the scene in the scanned environment based on the calibration of the two or more sensors and the matching scene trajectories associated with the two calibrated sensors. As described herein, modeling engines may be arranged to employ various evaluation models that may be tuned or trained to identify one or more shapes, objects, object activity, or the like, based on trajectories. At decision block <b>1014</b>, in one or more of the various embodiments, if the scanning may be finished, control may be returned to a calling process; otherwise, control may loop back to block <b>1004</b>.</p><p id="p-0158" num="0157"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates a flowchart of process <b>1100</b> for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments. After a start block <b>1102</b>, in one or more of the various embodiments, two or more sensors may capture signal reflections of one or more light beams from one or more surfaces or objects.</p><p id="p-0159" num="0158">At block <b>1104</b>, in one or more of the various embodiments, sensor event information e.g., location and time information, based on sensor output from the two or more sensors may be provided to sensing engines. Further, the sensing engines may be arranged to determine one or more sensor events based on the scanning signal source location and two or more sensor locations. Also, the sensing engines are employed to determine the 2D coordinates, and time for one or more trajectories.</p><p id="p-0160" num="0159">At block <b>1106</b>, the sensing engine may be employed to determine matching trajectories for the two or more sensors based on their associated sensor event information. Also, matching velocities are determined for the matching trajectories.</p><p id="p-0161" num="0160">At block <b>1108</b>, in one or more embodiments, the sensing engines may be employed to determine rotation and translation vectors based on the matching velocities.</p><p id="p-0162" num="0161">At block <b>1110</b>, in one or more of the various embodiments, modeling engines may be arranged to calibrate any offset errors for the two or more sensors due to their physically offset positions</p><p id="p-0163" num="0162">At block <b>1112</b>, the modeling engines employ the calibration of the two or more sensors and the matching trajectories to evaluate the scene. Next, in one or more of the various embodiments, control may be returned to a calling process.</p><p id="p-0164" num="0163"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a flowchart of process <b>1202</b> for perceiving objects based on sensing surfaces and sensing surface motion in accordance with one or more of the various embodiments. After a start block <b>1202</b>, in one or more of the various embodiments, sensing engines may be arranged to a collect a sensor event based on the output of one or more sensors. At decision block <b>1204</b>, in one or more of the various embodiments, if the sensor event may be included in a trajectory, control may flow to block <b>1208</b>; otherwise, control may flow block <b>1206</b>. At block <b>1206</b>, in one or more of the various embodiments, sensing engines may be arranged to exclude or discard the sensor event as noise. Next, in one or more of the various embodiments, control may be returned to a calling process. At block <b>1208</b>, in one or more of the various embodiments, sensing engines may be arranged to associate the sensor event with trajectory. At block <b>1210</b>, one or more functions may be applied to a trajectory to smooth and improve fit to a determined curve segment. Also, sampled points may be used with the one or more functions. Next, in one or more of the various embodiments, control may be returned to a calling process.</p><p id="p-0165" num="0164">It will be understood that each block in each flowchart illustration, and combinations of blocks in each flowchart illustration, can be implemented by computer program instructions. These program instructions may be provided to a processor to produce a machine, such that the instructions, which execute on the processor, create means for implementing the actions specified in each flowchart block or blocks. The computer program instructions may be executed by a processor to cause a series of operational steps to be performed by the processor to produce a computer-implemented process such that the instructions, which execute on the processor, provide steps for implementing the actions specified in each flowchart block or blocks. The computer program instructions may also cause at least some of the operational steps shown in the blocks of each flowchart to be performed in parallel. Moreover, some of the steps may also be performed across more than one processor, such as might arise in a multi-processor computer system. In addition, one or more blocks or combinations of blocks in each flowchart illustration may also be performed concurrently with other blocks or combinations of blocks, or even in a different sequence than illustrated without departing from the scope or spirit of the invention.</p><p id="p-0166" num="0165">Accordingly, each block in each flowchart illustration supports combinations of means for performing the specified actions, combinations of steps for performing the specified actions and program instruction means for performing the specified actions. It will also be understood that each block in each flowchart illustration, and combinations of blocks in each flowchart illustration, can be implemented by special purpose hardware based systems, which perform the specified actions or steps, or combinations of special purpose hardware and computer instructions. The foregoing example should not be construed as limiting or exhaustive, but rather, an illustrative use case to show an implementation of at least one of the various embodiments of the invention.</p><p id="p-0167" num="0166">Further, in one or more embodiments (not shown in the figures), the logic in the illustrative flowcharts may be executed using an embedded logic hardware device instead of a CPU, such as, an Application Specific Integrated Circuit (ASIC), Field Programmable Gate Array (FPGA), Programmable Array Logic (PAL), or the like, or combination thereof. The embedded logic hardware device may directly execute its embedded logic to perform actions. In one or more embodiments, a microcontroller may be arranged to directly execute its own embedded logic to perform actions and access its own internal memory and its own external Input and Output Interfaces (e.g., hardware pins or wireless transceivers) to perform actions, such as System On a Chip (SOC), or the like.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed as new and desired to be protected by Letters Patent of the United States is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for perceiving surfaces using one or more processors that are configured to execute instructions, wherein the executed instructions enable actions, comprising:<claim-text>employing two or more sensors to separately monitor a continuous stream of sensor events based on one or more scanned light beam reflections from one or more surfaces;</claim-text><claim-text>employing one or more portions of the monitored sensor events to generate one or more trajectories for each sensor;</claim-text><claim-text>determining a set of velocity vectors associated with a trajectory for one of the two or more sensors that matches another set of determined velocity vectors associated with another trajectory;</claim-text><claim-text>generating a translation vector and a rotation vector associated with the determined velocity vectors associated with the matching trajectories, wherein physical positions of the two or more sensors are offset from each other;</claim-text><claim-text>calibrating the two or more sensors by decreasing their one or more offset errors based on the translation vector and the rotation vector; and</claim-text><claim-text>providing the matching trajectories, the translation vector, and the rotation vector to a modeling engine, wherein the modeling engine employs the matching trajectories to calculate a position of a surface in three dimensional space.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each sensor event further comprises information that includes one or more of a timestamp, a column location, a row location, and a velocity.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each trajectory is a parametric representation of a one-dimensional curve segment in a three-dimensional space.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>iteratively updating the one or more trajectories based on the continuous stream of sensor events; and</claim-text><claim-text>recalibrating the one or more updated matching trajectories, wherein the one or more recalibrated trajectories are provided to the modeling engine.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining one or more objects based on a portion of the one or more trajectories that are associated with a portion of the one or more surfaces.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>providing a representation of the rotation vector in radians for each of an x, y and z axis; and</claim-text><claim-text>providing a representation of the translation vector in magnitudes for each of an x, y, and z axis.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A system for perceiving surfaces:<claim-text>a network computer, comprising:<claim-text>a memory that stores at least instructions; and</claim-text><claim-text>one or more processors configured to execute instructions, wherein the execution of the instructions enables performance of actions, including:<claim-text>employing two or more sensors to separately monitor a continuous stream of sensor events based on one or more scanned light beam reflections from one or more surfaces;</claim-text><claim-text>employing one or more portions of the monitored sensor events to generate one or more trajectories for each sensor;</claim-text><claim-text>determining a set of velocity vectors associated with a trajectory for one of the two or more sensors that matches another set of determined velocity vectors associated with another trajectory;</claim-text><claim-text>generating a translation vector and a rotation vector associated with the determined velocity vectors associated with the matching trajectories, wherein physical positions of the two or more sensors are offset from each other;</claim-text><claim-text>calibrating the two or more sensors by decreasing their one or more offset errors based on the translation vector and the rotation vector; and</claim-text><claim-text>providing the matching trajectories, the translation vector, and the rotation vector to a modeling engine, wherein the modeling engine employs the matching trajectories to calculate a position of a surface in a three dimensional (3D) space; and</claim-text></claim-text></claim-text><claim-text>one or more client computers, comprising:<claim-text>a memory that stores at least instructions; and</claim-text><claim-text>one or more processors configured to execute instructions, wherein the executed instructions enable the performance of actions, including:<claim-text>providing one or more portions of the sensor events.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein each sensor event further comprises information that includes one or more of a timestamp, a column location, a row location, and a velocity;</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein each trajectory is a parametric representation of a one-dimensional curve segment in a three-dimensional space.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>iteratively updating the one or more trajectories based on the continuous stream of sensor events; and</claim-text><claim-text>recalibrating the one or more updated matching trajectories, wherein the one or more recalibrated trajectories are provided to the modeling engine.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>determining one or more objects based on a portion of the one or more trajectories that are associated with a portion of the one or more surfaces.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>providing a representation of the rotation vector in radians for each of an x, y and z axis; and</claim-text><claim-text>providing a representation of the translation vector in magnitudes for each of an x, y, and z axis.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A processor readable non-transitory storage media that includes instructions for perceiving surfaces, wherein execution of the instructions by one or more processors on one or more network computers enables performance of actions, comprising:<claim-text>employing two or more sensors to separately monitor a continuous stream of sensor events based on one or more scanned light beam reflections from one or more surfaces;</claim-text><claim-text>employing one or more portions of the monitored sensor events to generate one or more trajectories for each sensor;</claim-text><claim-text>determining a set of velocity vectors associated with a trajectory for one of the two or more sensors that matches another set of determined velocity vectors associated with another trajectory;</claim-text><claim-text>generating a translation vector and a rotation vector associated with the determined velocity vectors associated with the matching trajectories, wherein physical positions of the two or more sensors are offset from each other;</claim-text><claim-text>calibrating the two or more sensors by decreasing their one or more offset errors based on the translation vector and the rotation vector; and</claim-text><claim-text>providing the matching trajectories, the translation vector, and the rotation vector to a modeling engine, wherein the modeling engine employs the matching trajectories, translation vector and the rotation vector to calculate a position of a surface in a three dimensional (3D) space.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The processor readable non-transitory storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein each sensor event further comprises information that includes one or more of a timestamp, a column location, a row location, and a velocity;</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The processor readable non-transitory storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein each trajectory is a parametric representation of a one-dimensional curve segment in a three-dimensional space.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The processor readable non-transitory storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>iteratively updating the one or more trajectories based on the continuous stream of sensor events; and</claim-text><claim-text>recalibrating the one or more updated matching trajectories, wherein the one or more recalibrated trajectories are provided to the modeling engine.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The processor readable non-transitory storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>determining one or more objects based on a portion of the one or more trajectories that are associated with a portion of the one or more surfaces.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The processor readable non-transitory storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>providing a representation of the rotation vector in radians for each of an x, y and z axis; and</claim-text><claim-text>providing a representation of the translation vector in magnitudes for each of an x, y, and z axis.</claim-text></claim-text></claim></claims></us-patent-application>