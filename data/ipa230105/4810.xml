<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004811A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004811</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17791967</doc-number><date>20200207</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>082</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><invention-title id="d2e43">LEARNING PROCESSING DEVICE AND LEARNING PROCESSING METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Hitachi High-Tech Corporation</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ISHIKAWA</last-name><first-name>Masayoshi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>OUCHI</last-name><first-name>Masanori</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SHINDO</last-name><first-name>Hiroyuki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>TOYODA</last-name><first-name>Yasutaka</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>SHINODA</last-name><first-name>Shinichi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/004878</doc-number><date>20200207</date></document-id><us-371c12-date><date>20220711</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A learning processing device and method achieves learning of a lightweight model that is completed in a short amount of time. The learning processing device obtains a new, second learning model from an existing first learning model. An input unit acquires a first learning model generated in advance by learning a first learning data set, and an unpruned neural network (hereinafter, NN). An important parameter identification unit uses the first learning model and the NN to initialize a NN to be learned, and uses a second learning data set and the initialized NN to identify a degree of importance of parameters in a recognition process of the initialized NN. A new model generation unit carries out a pruning process for deleting parameters which are not important from the initialized NN, thereby generating a second NN; and a learning unit uses the second learning data set to learn the second NN.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="113.79mm" wi="157.65mm" file="US20230004811A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="243.25mm" wi="139.45mm" file="US20230004811A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="199.73mm" wi="166.20mm" file="US20230004811A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="236.05mm" wi="97.20mm" orientation="landscape" file="US20230004811A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="246.46mm" wi="159.68mm" file="US20230004811A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="212.68mm" wi="158.75mm" file="US20230004811A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="189.23mm" wi="148.17mm" file="US20230004811A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="245.79mm" wi="103.72mm" orientation="landscape" file="US20230004811A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="133.60mm" wi="109.90mm" file="US20230004811A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="168.74mm" wi="169.67mm" file="US20230004811A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="127.51mm" wi="60.28mm" file="US20230004811A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="231.65mm" wi="104.65mm" file="US20230004811A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="249.09mm" wi="161.21mm" orientation="landscape" file="US20230004811A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="236.81mm" wi="145.97mm" orientation="landscape" file="US20230004811A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to a learning processing device and a learning processing method using a neural network.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">In the field of signal processing including image processing, natural language processing, and speech recognition, there may be a case where recognition processing is performed using a learning processing device that uses a multilayer neural network. In recent years, a neural network has a large number of parameters to improve recognition performance and hence, the neural network requires an enormous amount of arithmetic operation. However, data including a large amount of images or the like is to be processed in recognition processing that is performed after learning. Accordingly, it is desirable to perform processing with a small number of parameters and a small amount of arithmetic operation. In the present invention, a neural network that can be processed with a small number of parameters and a small amount of arithmetic operation is referred to as a lightweight model.</p><p id="p-0004" num="0003">As a technique for reducing the number of parameters and an amount of arithmetic operation of a multilayer neural network after learning, there has been known pruning. The pruning is a technique where unnecessary parameters and unnecessary arithmetic operation in the neural network after learning are identified, and are deleted. With such pruning, the number of parameters and an amount of arithmetic operation required for recognition processing of the neural network can be reduced.</p><p id="p-0005" num="0004">For example, in PTL 1, studies have been made on a system where units in a neural network after learning are randomly deleted, a cost function is evaluated after relearning, and a structure having the best cost function is outputted as an optimal structure of the neural network.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0006" num="0005">PTL 1: Japanese Patent Application Laid-Open No. 2015-11510</p><heading id="h-0005" level="1">SUMMARY OF INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0007" num="0006">In general, the weight reduction of a neural network by pruning requires a long learning period. This is because parameters have to be deleted little by little in order to minimize an adverse influence on the recognition accuracy in performing the reduction of weight of the neural network. When a large number of parameters are deleted at a time, the recognition accuracy is largely deteriorated. Accordingly, a lightweight model with small deterioration of recognition accuracy is usually learned where the deletion of a small number of parameters and the relearning are repeated a plurality of times.</p><p id="p-0008" num="0007">Therefore, the learning of the lightweight neural network requires to be performed the number of times that is several times as large as the number of times of the learning of the normal neural network. Accordingly, the learning of the lightweight neural network requires a long learning period. In particular, in a case where the number of types of recognition processing to be performed is large or in a case where models of a neural network used in an environment where the processing is performed is to be switched, it is necessary to prepare a plurality of lightweight models. Accordingly, a learning period becomes long and this becomes a factor that causes a delay in development and updating of a recognition processing function.</p><p id="p-0009" num="0008">In view of the above, there has been a demand for a configuration that enables the learning of a lightweight model to be finished within a short period of time. PTL <b>1</b>, however, has not studied shortening of a period for learning a lightweight model.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0010" num="0009">In view of such circumstances, according to the present invention, there is provided a learning processing device for obtaining a new second learning model from an existing first learning model, the learning processing device including:</p><p id="p-0011" num="0010">an input unit that acquires a first learning model generated in advance by learning a first learning data set, and an unpruned neural network; an important parameter identification unit that initializes the neural network that is an object to be learned by using the first learning model and the neural network, and identifies degrees of importance of parameters in recognition processing of the initialized neural network by using a second learning data set and the initialized neural network; a new model generating unit that generates a second neural network by performing pruning processing for pruning unimportant parameters from the initialized neural network using the degrees of importance of the parameters; a learning unit that learns the second neural network by using the second learning data set; and an output unit that outputs the second neural network after learning as a second learning model.</p><p id="p-0012" num="0011">Further, according to the present invention, there is provided a learning processing method for obtaining a new second learning model from an existing first learning model, the learning processing method including: acquiring a first learning model generated in advance by learning a first learning data set, and an unpruned neural network; initializing the neural network that is an object to be learned using the first learning model and the neural network; identifying degrees of importance of parameters in recognition processing of the initialized neural network by using a second learning data set and the initialized neural network; generating a second neural network by performing pruning processing for pruning unimportant parameters from the initialized neural network using the degrees of importance of the parameters; learning the second neural network using the second learning data set; and setting the second neural network after learning as a second learning model.</p><p id="p-0013" num="0012">Further, in the present invention, there is provided a learning processing method for obtaining a new second learning model from an existing first learning model, the learning processing method including: initializing an unpruned neural network that is an object to be learned by using the unpruned neural network and the first learning model; obtaining a degrees of importance of a parameter in recognition processing of the initialized neural network by using a second learning data set and the initialized neural network; performing pruning processing according to the degrees of importance; and obtaining a second learning model by learning of the neural network after pruning by using the second learning data set.</p><heading id="h-0008" level="1">Advantageous Effects of Invention</heading><p id="p-0014" num="0013">According to the present invention, even in a case where the number of types of recognition processing to be executed is large or even in a case where a model of a neural network to be used is switched depending on an environment where the processing is performed, it is possible to shorten the time required for learning a lightweight model, and it is also possible to provide a rapid recognition processing function.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating an example of a use mode of a neural network that is assumed in a learning processing device of the present invention.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of a use mode of a neural network that is assumed in a learning processing device of the present invention.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an outline of pruning by a conventional method.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an outline of processing of the learning processing device according to the present invention.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a comparison between an effect acquired by the present invention and an effect acquired by the conventional method.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating a functional configurational example of the learning processing device according to the present invention.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating an example of a processing flow of a learning processing method according to the present invention.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a table illustrating determination material information in a table format when an operator performs degree-of-importance evaluation on a plurality of candidates for a pre-trained model <b>302</b>.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating a functional configurational example of a learning processing device according to an embodiment 2 of the present invention.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating an example of a processing flow of a learning processing method according to the embodiment 2 of the present invention.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating an outline of partial reinitialization processing.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating a functional constitutional example of a learning processing device according to an embodiment 3 of the present invention.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart illustrating an example of a processing flow of a learning processing method according to the embodiment 2 of the present invention.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating an outline of pruning using a masking layer.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram illustrating a method of applying a mask layer to a neural network having a complicated network structure.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a table illustrating a situation in which convolution layers <b>1402</b>-<i>a </i>to <b>1402</b>-<i>d </i>share four channels in a table format.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram illustrating a configurational example of a monitor screen suitable for the learning processing device according to the present invention.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram illustrating a configurational example of a screen for performing a degree-of-importance evaluation with respect to a plurality of candidates for a pre-trained model.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0033" num="0032">Hereinafter, embodiments of the present invention will be described with reference to the drawings.</p><heading id="h-0011" level="1">Embodiment 1</heading><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>2</b></figref> are diagrams illustrating examples of a use mode of a neural network that is assumed in a learning processing device of the present invention. In these cases, a case is exemplified where a learning processing device <b>100</b> performs image processing. However, processing to which the learning processing device <b>100</b> is applicable includes natural language processing, speech recognition, and other signal processing.</p><p id="p-0035" num="0034">In the case illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in the learning processing device <b>100</b>, an input image <b>101</b> is inputted to a plurality of image processing units <b>102</b> (<b>102</b>-<b>1</b>, <b>102</b>-<b>2</b>, <b>102</b>-<b>3</b>), and each image processing unit <b>102</b> performs recognition processing on the input image <b>101</b>.</p><p id="p-0036" num="0035">The processing in the image processing unit <b>102</b> is image processing such as image classification, object detection, semantic segmentation, and captioning, and the image processing units <b>102</b>-<b>1</b>, <b>102</b>-<b>2</b>, <b>102</b>-<b>3</b> perform respectively different recognition processing. For example, in an example of an image inspection of a product, the image processing unit <b>102</b>-<b>1</b> performs image processing <b>1</b> for classifying the quality of the product into a non-defective product and a defective product with respect to a given image. The image processing unit <b>102</b>-<b>2</b> performs image processing <b>2</b> for detecting a foreign substance mixed in the product. The image processing unit <b>102</b>-<b>3</b> performs image processing <b>3</b> for performing segmentation by which the shape of the product is recognized. As a result, the processing result units <b>103</b> (<b>103</b>-<b>1</b>, <b>103</b>-<b>2</b>, <b>103</b>-<b>3</b>) acquire the processing results with respect to respective parts of the product.</p><p id="p-0037" num="0036">The image processing units <b>102</b>-<b>1</b>, <b>102</b>-<b>2</b>, <b>102</b>-<b>3</b> are each a learning model including a neural network having a configuration specialized for each recognition processing. As described above, in a case where a plurality of image processing are applied to one input image <b>101</b> and different neural networks are used for respective image processing or the like, it is necessary to learn a lightweight model with respect to the plurality of neural networks.</p><p id="p-0038" num="0037">In the learning processing device <b>100</b> according to the present invention having the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, it is assumed that an image processing unit <b>102</b>-<b>4</b> (lightweight model) that obtains an image processing result <b>4</b> added when a new inspection item occurs, and a processing result unit <b>103</b>-<b>4</b> are newly added.</p><p id="p-0039" num="0038">In addition, there is a case where it is necessary to learn a plurality of lightweight models even in the same recognition processing. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a case where respective learning models in the image processing units <b>102</b>-<b>1</b>, <b>102</b>-<b>2</b>, <b>102</b>-<b>3</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref> are set to learning models that are optimized in response to, for example, a change in environment in image measurement. Image processing units <b>102</b>&#x2032; (<b>102</b>-<b>1</b>, <b>102</b>&#x2032;-<b>2</b>, <b>102</b>&#x2032;-<b>3</b>) each have the configuration that uses learning models (environment models) optimized in response to an environmental change. In the present invention, it is assumed that the environment model of FIG. <b>2</b> is newly added based on the configuration of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0040" num="0039">For example, a learning model for executing processing in a case where an image is acquired via a lens having a different magnification differs from a learning model for executing processing using a lens having a normal magnification. In this case, it is necessary to construct a new neural network specialized for an image obtained via the lens having different magnification. Accordingly, it is necessary to newly obtain the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0041" num="0040">As described above, the environmentally different models <b>102</b>&#x2032; (<b>102</b>-<b>1</b>, <b>102</b>&#x2032;-<b>2</b>, <b>102</b>&#x2032;-<b>3</b>) illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> execute the same image processing as the learning models <b>102</b> (<b>102</b>-<b>1</b>, <b>102</b>-<b>2</b>, <b>102</b>-<b>3</b>) illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. However, <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates the example where the model to be performed changes in response to the environment related to the input image <b>101</b>. In this embodiment, the environment may be a place or a situation where the input image <b>101</b> is acquired, or a place or a situation where the recognition processing is performed. For example, in the case of outdoor image data, the model to be performed is changed depending on a situation such as daytime or nighttime. In the case of an image inspection system, the model to be performed is changed for respective types of objects to be inspected.</p><p id="p-0042" num="0041">The processing result units <b>103</b>&#x2032; (<b>103</b>&#x2032;-<b>1</b>, <b>103</b>&#x2032;-<b>2</b>, <b>103</b>&#x2032;-<b>3</b>) obtain outputs of execution from the environmentally different model <b>102</b>&#x2032; (<b>102</b>-<b>1</b>, <b>102</b>&#x2032;-<b>2</b>, <b>102</b>&#x2032;-<b>3</b>). The neural network to be performed in <figref idref="DRAWINGS">FIG. <b>2</b></figref> differs from the neural network to be performed in <figref idref="DRAWINGS">FIG. <b>1</b></figref> and hence, processing results <b>1</b>&#x2032;, <b>2</b>&#x2032;, and <b>3</b>&#x2032; output processing results different from the processing results acquired in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0043" num="0042">Even when the environmentally different models <b>102</b>&#x2032; (<b>102</b>-<b>1</b>, <b>102</b>&#x2032;-<b>2</b>, <b>102</b>&#x2032;-<b>3</b>) are used in this manner, it is necessary to learn a plurality of lightweight models. In particular, in the case of an image inspection system, the number of combinations of the types of objects that are objects to be inspected and the number of combinations of the number of types of inspection processes are infinite. Accordingly, the number of combinations of the environmentally different models and the image processing becomes enormous. Therefore, the efficient acquisition of lightweight models is indispensable.</p><p id="p-0044" num="0043">In the present embodiment, three examples are described with respect to both the image processing and the environmentally different models. However, one or more examples are sufficient.</p><p id="p-0045" num="0044">As a method for learning and preparing a lightweight model, a pruning method is known. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a view illustrating an outline of pruning by a conventional method. By performing the pruning, it is possible to reduce the weight of the learning model.</p><p id="p-0046" num="0045">Normally, in performing the pruning, first, an unpruned neural network <b>204</b>-<b>0</b> is learned by learning processing <b>201</b>-<b>0</b> using a learning data set <b>301</b>. Then, the pruning is performed in pruning processing <b>202</b>-<b>1</b>. That is, unnecessary parameters in the unpruned neural network <b>204</b>-<b>0</b> after learning are identified, the unnecessary parameters are deleted, and a neural network <b>204</b>-<b>1</b> after pruning is outputted. In this embodiment, as parameters that are identified as unnecessary parameters by pruning, parameters having small absolute values or the like are selected.</p><p id="p-0047" num="0046">There is a case where the recognition accuracy of the neural network <b>204</b>-<b>1</b> from which the parameters have been deleted by pruning is lower than the recognition accuracy of the neural network <b>204</b>-<b>0</b> to which pruning is not applied. In consideration of such a case, the parameters in the neural network <b>204</b> -<b>1</b> that are not pruned after the pruning are updated by relearning processing <b>203</b> -<b>1</b>. As a result, the lowered recognition accuracy is recovered to a value close to the recognition accuracy of the neural network <b>204</b>-<b>0</b> to which pruning is not applied. Then, in a case where an amount of pruning is insufficient or in a case where a plurality of pruning processing are set, a neural network <b>204</b>-<b>2</b> that is further pruned is obtained by performing a pruning processing <b>202</b>-<b>2</b> and a relearning processing <b>203</b>-<b>2</b>. The pruning processing and the relearning processing may be repeatedly applied twice or more. In the relearning processing <b>203</b>-<b>1</b> and <b>203</b>-<b>2</b>, at the time of learning, learning is performed using the learning data set <b>301</b> having the same content as the learning data set used in the first learning processing <b>201</b>-<b>0</b>.</p><p id="p-0048" num="0047">As described above, in the pruning by the conventional method, unnecessary parameters are identified after the learning, and the relearning is repeatedly performed after the unnecessary parameters are deleted. This is because it is necessary to gradually delete the parameters so as not to adversely influence the recognition performance at the time of weight reduction by pruning. In the processing performed in accordance with these processes, the repetitive learning processing must be performed a plurality of times. Accordingly, the time required for learning the lightweight model becomes long. Accordingly, in particular, in a case where it is necessary to prepare a plurality of lightweight models as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a preparation period becomes long. This causes a delay in offering a recognition function.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an outline of processing of a learning processing device according to the present invention. In this processing, a second model (network) suitable for another processing is generated from a first model (network) prepared in advance. This relationship is expressed as follows, for example. That is, by using one model (network) of the image processing units <b>102</b> (<b>102</b>-<b>1</b>, <b>102</b>-<b>2</b>, <b>102</b>-<b>3</b>) in <figref idref="DRAWINGS">FIG. <b>1</b></figref> as a first model, image processing unit <b>102</b>-<b>4</b> is newly generated as a second model. Alternatively, by using one model (network) of the image processing unit <b>102</b> (<b>102</b>-<b>1</b>, <b>102</b>-<b>2</b>, <b>102</b>-<b>3</b>) as a first model, an environment model <b>102</b>&#x2032; illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is newly generated as a second model.</p><p id="p-0050" num="0049">Accordingly, the first model based on which the new model is generated is a pre-trained model <b>302</b> prepared in advance, and the pre-trained model <b>302</b> is generated using a first learning data set <b>301</b>A that is an object to be learned specialized for the pre-trained model <b>302</b>. In this drawing, reference numeral <b>303</b> denotes an unpruned neural network that is provided by an operator.</p><p id="p-0051" num="0050">To an important parameter identification unit <b>304</b>, a second learning data set <b>301</b>B is applied. The second learning data set <b>301</b>B is an object to be learned specialized for a second model to be newly generated with respect to the pre-trained model <b>302</b>, which is the first model. With such processing, important parameters in the second learning data set <b>301</b>B are identified by the pre-trained model <b>302</b>, and only the important parameters are extracted. In this processing, identifying the important parameter means identifying parameters in the second model that are important for the recognition of the second learning data set <b>301</b>B.</p><p id="p-0052" num="0051">Then, in a new model generating unit <b>306</b>, a neural network <b>305</b> is generated where only the parameters in the second model that are important for recognition of the second learning data set <b>301</b>B are extracted from an unpruned neural network <b>303</b>. By learning the neural network <b>305</b> by learning processing <b>307</b>, it is possible to acquire a lightweight model in a short period of time. In this learning processing <b>307</b>, the second learning data set <b>301</b>B is used.</p><p id="p-0053" num="0052">A method where pruning is not applied, that is, a method of performing learning using a model learned by a different data set (learning data set <b>301</b>A) as an initial value is referred to as transfer learning. In the present invention, by applying the pruning at the time of such transfer, the learning processing <b>201</b>-<b>0</b> that has been necessary conventionally can be made unnecessary. Accordingly, the number of times of learning can be reduced and hence, a period for learning the lightweight model can be shortened.</p><p id="p-0054" num="0053">In the present invention, the processing is basically performed in accordance with the processes as described above. However, there are some points to be devised for realizing the processing.</p><p id="p-0055" num="0054">A first point to be devised or improved is that it has been difficult to apply pruning at the time of transfer learning in conventional methods. In general, in the learning processing <b>201</b>-<b>0</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, regularization that conforms with the pruning processing <b>202</b> is applied so as to reduce lowering of accuracy when the pruning processing <b>202</b> is performed. For example, in a case where the parameters having small absolute values are deleted in the pruning processing <b>202</b>, the regularization that is referred to as Weight decay in which absolute values of parameters in a neural network become small is applied in the learning processing <b>201</b>-<b>0</b>. A loss function of a neural network that includes Weight decay is expressed by Formula (1).</p><p id="p-0056" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Formula (1)]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0057" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>L=Lr+&#x3bb;&#x2225;&#x3b8;&#x2225;</i><sup>2 </sup>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0058" num="0055">In the Formula (1), a first term on the right side that forms a loss function L of the neural network is a loss function Lr that is defined for each learning of recognition processing, and a second term is the term of Weight decay. &#x3bb; is a coefficient of Weight decay, and &#x3b8; is a parameter in the neural network. In order to delete more parameters, it is necessary to set a relatively large value as the coefficient &#x3bb; of Weight decay.</p><p id="p-0059" num="0056">However, when the coefficient &#x3bb; of Weight decay is made large, many parameters take minute values. As a result, the recognition performance can be exhibited only with a learned data set, and the learning method is not suitable for transfer learning. In view of the above, it is desirable that the pre-trained model <b>302</b> be not a model in which strong Weiget decay is applied to the specific learning data set <b>302</b> but be a neural network in which many parameters have non-zero values.</p><p id="p-0060" num="0057">In such a case, the important parameter identification unit <b>304</b> cannot identify the important parameters based on absolute values of the parameters.</p><p id="p-0061" num="0058">In terms of improvement of this point, the following technique is preferably adopted in the embodiment of the present invention.</p><p id="p-0062" num="0059">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in general, in the pre-trained model <b>302</b> learned by the learning data set <b>301</b>A that differs from the learning data set <b>301</b>B, parameters that are effective for recognition of the learning data set <b>301</b>B and parameters that are unnecessary for the recognition of the learning data set <b>301</b>B exist. The important parameter identification unit <b>304</b> extracts parameters that are effective for recognition of the learning data set <b>301</b>B by analyzing the Hessian. The Hessian is a quadratic derivative with respect to parameters of the loss function L. Accordingly, for example, the influence exerted on the loss function when each parameter is deleted can be calculated by a product of a square of the value of each parameter and a diagonal component of the Hessian.</p><p id="p-0063" num="0060">The calculation Formula is expressed by a Formula (2). A subscript q indicates a value related to the qth parameter. H represents the Hessian, and a subscript qq represents a value in the qth row and the qth column. It is possible to extract only the parameters effective for recognition of the learning data set <b>301</b>B from the pre-trained model <b>302</b> by deleting the parameters until this value becomes a desired pruning amount from a small value.</p><p id="p-0064" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Formula (2)]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0065" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>dL=&#x3b8;<sub>q</sub><sup>2</sup>H<sub>qq </sub>&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0066" num="0061">As described above, in the present invention, the influence exerted on the behavior of the neural network when a certain parameter is deleted is evaluated, and this evaluation is set as a degree of importance of each parameter. The higher the degree of importance of the parameter, the lower the recognition performance of the neural network becomes when the parameter is deleted. Accordingly, by performing pruning from the parameters having the low degrees of importance, it is possible to suppress lowering of recognition performance of the neural network, and many parameters can be deleted.</p><p id="p-0067" num="0062">The second point to be devised or improved in association with this point is as follows. That is, there may be a case where, in general, a multilayer neural network has a huge number of parameters and hence, it is difficult to directly calculate the Hessian.</p><p id="p-0068" num="0063">In such a case, the influence exerted when each parameter is deleted may be calculated by the Fisher information matrix that is a second moment of a first derivative of the loss function L or Kronecker-Factored Approximated Curvature that is an approximation of the Fisher information matrix. Such influence may be calculated using a statistical quantity of a first derivative of the loss function L or a statistical quantity of a product of the first derivative and the parameter. In this case, for example, an absolute value of a product of an expected value of a gradient and a value of a parameter can be calculated as the influence exerted on the loss function when each parameter is deleted.</p><p id="p-0069" num="0064">In the case of a convolutional neural network, the degrees of importance obtained in terms of a unit of parameter may be collected for each dimension that is referred to as a channel or a filter and may be used as the degree of importance for each channel or filter.</p><p id="p-0070" num="0065">The degrees of importance of respective parameters may be evaluated using the relaxed Bernoulli distribution described in an embodiment 4.</p><p id="p-0071" num="0066"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a comparison between an effect acquired by the present invention and an effect acquired by a conventional method. In both a learning curve <b>401</b> according to the present invention and a learning curve <b>402</b> according to the conventional method, the number of times that parameters are updated at the time of learning is taken on an axis of abscissas, and the recognition performance is taken on an axis of ordinates.</p><p id="p-0072" num="0067">According to the present invention, as illustrated on a left side of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, as indicated by a learning curve <b>401</b>, only one learning curve is drawn in order to identify only important parameters in the learning data set <b>301</b>B from the pre-trained model <b>302</b> and to directly learn the lightweight model <b>306</b>.</p><p id="p-0073" num="0068">On the other hand, in the conventional method, the learning processing <b>201</b>-<b>0</b> and the relearning processing <b>203</b> that are performed one or more times are required and hence, as indicated by the learning curve <b>402</b>, the learning curve is drawn two or more times. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a broken line in the vicinity of the center of the learning curve <b>402</b> indicates the influence exerted on the lowering of accuracy by pruning. As described above, according to the present invention, a lightweight model can be acquired by learning performed one time and hence, the time required for acquiring one lightweight model can be significantly shortened.</p><p id="p-0074" num="0069"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a functional configuration example of the learning processing device of the present invention, and <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example of processing flow of the learning processing method of the present invention. In these drawings, the configuration and the flow until the lightweight model is learned using the learning data set <b>301</b> and the pre-trained model <b>302</b> are illustrated.</p><p id="p-0075" num="0070">First, the functional configuration example diagram of the learning processing device of the present invention illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> describes processing contents in an arithmetic operation unit when the processing of the present invention is realized by a computer as a main processing function. In this embodiment, the description is made with respect to a case where the pre-trained model <b>302</b> learned by the learning data set <b>301</b>A and the unpruned neural network <b>303</b> are used.</p><p id="p-0076" num="0071">In the important parameter identification unit <b>304</b>, first, the neural network <b>303</b> that is an object to be learned is initialized by using the pre-trained model <b>302</b> and the unpruned neural network <b>303</b>. By this initialization, the parameters in the pre-trained model <b>302</b> are reflected in the neural network <b>303</b> in a non-mowed form. Accordingly, it is safe to say that the neural network <b>303</b> after initialization is equivalent to the pre-trained model <b>302</b>. In the important parameter identification unit <b>304</b>, as next processing, parameters that are important in the recognition processing of the neural network <b>303</b> after initialization are identified using the neural network after initialization and the second learning data set <b>301</b>B that is an object to be learned identified for the second model <b>305</b> to be newly generated.</p><p id="p-0077" num="0072">A pruning unit <b>306</b> is a processing unit that performs processing corresponds to the processing performed by a new model generating unit <b>306</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In the pruning unit <b>306</b>, a neural network <b>305</b> in a pruned form after only the parameters of the second model that are important for the recognition of the second learning data set <b>301</b>B are extracted is generated from the unpruned neural network <b>303</b>.</p><p id="p-0078" num="0073">In the learning processing unit <b>307</b>, the neural network <b>305</b> is learned using the second learning data set <b>301</b>B.</p><p id="p-0079" num="0074">In a model evaluation unit <b>503</b>, the arithmetic accuracy of the neural network <b>305</b> (second model) is evaluated using an evaluation data set <b>504</b>. The neural network <b>305</b> that exhibits performance that satisfies the criteria of the evaluation data set <b>504</b> is eventually set as the second model (metric model).</p><p id="p-0080" num="0075">In the example of processing flow of the learning processing method according to the present invention illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, first, the learning flow is started in step S<b>601</b>. This process is started when an operator performs learning after items necessary for learning a lightweight model are set by the operator or the like.</p><p id="p-0081" num="0076">Step S<b>602</b> corresponds to a part of the process performed by the important parameter identification unit <b>304</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In this process, a neural network that is an object to be learned is initialized using the unpruned network structure <b>303</b> and the pre-trained model <b>302</b> that are inputted by the operator. In this learning processing method, the unpruned network structure <b>303</b> is a network structure provided by an operator to perform image processing that is a target for learning the lightweight model. Also in this learning processing method, the initialization of the neural network to be learned is performed so as to determine initial values of parameters in the given network structure <b>303</b>. A part of or the whole neural network to be learned is initialized by copying the parameters of the pre-trained model <b>302</b>. In the neural network to be learned, values of parameters to which the parameters of the pre-trained model <b>302</b> have not been copied are determined using random numbers or constants set by an operator.</p><p id="p-0082" num="0077">Such initialization of the neural network using the parameters of the pre-trained model <b>302</b> is generally referred to as transfer learning. The initialized network structure <b>303</b> has an unpruned neural network configuration, and the network structure <b>303</b> reflects parameters of the pre-trained model <b>302</b>. Accordingly, it is safe to say that the initialized network structure <b>303</b> is a structure obtained by reflecting the pre-trained model <b>302</b> in a lightweight form in the network structure of an unpruned network original form.</p><p id="p-0083" num="0078">In this learning processing method, which parameters of the neural network to be learned are copied from the pre-trained model <b>302</b> is determined by the operator. The operation of copying the parameter from the pre-trained model <b>302</b> with respect to only a part of the neural network to be learned is performed when an image processing function of an object to be learned and an image processing function of the pre-trained model differ from each other or the like. For example, when the image processing function of the object to be learned is an object detection function and the image processing function of the pre-trained model <b>302</b> is an image classification function, parameters are copied to a neural network for object detection only with respect to a part of a neural network that is referred to as a feature extractor that extracts an image feature from an input image. Even when both image processing functions are the same image classification function, parameters are copied only with respect to a part of a neural network that is referred to as a feature extractor, and a subsequent layer for predicting a classification result may be initialized by other methods.</p><p id="p-0084" num="0079">Step S<b>603</b> corresponds to a part of the important parameter identification unit <b>304</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In this learning processing method, the degree of importance of each parameter in the neural network initialized in step S<b>602</b> is evaluated using the neural network initialized in step S<b>602</b> and the learning data set <b>301</b>B. In this step, the evaluation of the degrees of importance is performed as described in the description with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0085" num="0080">Step S<b>604</b> corresponds to the pruning unit (new model generating unit in <figref idref="DRAWINGS">FIG. <b>4</b></figref>) <b>306</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In this step, parameters having low degrees of importance are deleted from the neural network initialized in step S<b>602</b> in accordance with the degrees of importance of the respective parameters evaluated in step S<b>603</b>. In this step, a method of determining parameters to be deleted is selected by an operator. As such a method, a method of deleting parameters other than some percentages of parameters belonging to an upper percentage group having high degrees of importance, and a method of deleting the parameters having the degrees of importance at levels equal to or lower than a certain threshold.</p><p id="p-0086" num="0081">Step S<b>605</b> corresponds to the learning unit <b>307</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In this step S<b>605</b>, the parameters of the neural network that has become lightweight and is obtained in step S<b>604</b> are learned using the learning data set <b>301</b>B. This step is performed by a stochastic gradient descent method or the like that is usually used for learning of a neural network.</p><p id="p-0087" num="0082">Step S<b>606</b> corresponds to the model evaluation unit <b>503</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In this step, the performance of the neural network learned in step S<b>605</b> is evaluated using the evaluation data set <b>504</b>. The performance evaluation of the neural network is performed so as to evaluate the degree of performance that the obtained lightweight model <b>505</b> can achieve in image processing of a recognition target.</p><p id="p-0088" num="0083">In step S<b>607</b>, the neural network learned in step s<b>605</b> is outputted as the lightweight model <b>505</b>. At this stage of processing, it is preferable to output the lightweight model <b>505</b> together with the recognition performance evaluated in step S<b>606</b> and the time during which the recognition processing is performed.</p><p id="p-0089" num="0084">In step S<b>608</b>, the learning flow ends.</p><p id="p-0090" num="0085">As described above, in the present invention, it is possible to generate a lightweight model at the time of initialization by identifying parameters important for the recognition of the learning data set <b>301</b>B and the evaluation data set <b>504</b> from the pre-trained model <b>302</b> and by transferring only the important parameters. Accordingly, it is possible to acquire the lightweight model <b>505</b> having high recognition accuracy by performing learning one time.</p><heading id="h-0012" level="1">Embodiment 2</heading><p id="p-0091" num="0086">In a case where a plurality of pre-trained models exist, it is necessary to determine the utilization of which pre-trained model is appropriate in generating a desired lightweight model. An embodiment 2 relates to preparing materials for making such determination in advance, and providing such materials as references in an actual operation.</p><p id="p-0092" num="0087">According to the embodiment 1 of the present invention, it is possible to evaluate how many important parameters for the learning data set <b>301</b>B and the evaluation data set <b>504</b> the pre-trained model <b>302</b> holds even when learning is not performed for a long time. In a case where a plurality of learning models are prepared, a plurality of model candidates for a model used as the pre-trained model <b>302</b> exist. In general, the performance of a neural network to be learned changes depending on a model used as the pre-trained model <b>302</b>. Accordingly, it is important to select an appropriate pre-trained model <b>302</b> in order to learn a lightweight model in a short period of time.</p><p id="p-0093" num="0088">However, in the conventional method, it is not possible to determine whether each pre-trained model is good or bad unless the neural network is actually learned by the learning data set <b>301</b>B and the learning unit <b>306</b>, and the recognition performance is evaluated by the evaluation data set <b>504</b> and the model evaluation unit <b>503</b>.</p><p id="p-0094" num="0089">Accordingly, in a case where a plurality of candidates for the pre-trained model <b>302</b> exist, it is necessary to perform learning and evaluation on all candidates in order to select the best model as the pre-trained model <b>302</b>. In such a case, the time required for learning the lightweight model <b>305</b> becomes enormous. Accordingly, the selection of one appropriate model from among a plurality of candidates for the pre-trained model <b>302</b> without performing learning becomes important in learning a highly accurate lightweight model in a short period of time.</p><p id="p-0095" num="0090">In this embodiment, an unpruned non-lightweight model becomes an index for the recognition performance that is reachable by a neural network that is an object to be learned. Accordingly, it is necessary to learn and hold the non-lightweight model in terms of model management.</p><p id="p-0096" num="0091">In the embodiment 2 of the present invention, a degree-of-importance evaluation table <b>700</b> illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is generated in advance using the important parameter identification unit <b>304</b> that is a partial function of the learning processing device <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Then, the degree-of-importance evaluation table <b>700</b> that is prepared in advance is provided as the reference for selecting an appropriate pre-trained model in an actual operation. The configuration of the device and a processing flow for preparing the degree-of-importance evaluation table <b>700</b> in advance will be described later with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref> and <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0097" num="0092"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a table illustrating an example of a degree-of-importance evaluation table with respect to a plurality of candidates for the pre-trained model <b>302</b>. When the setting related to the learning data set <b>301</b>B, the network structure <b>303</b>, and the pruning is selected by an operator, this table is provided to the operator as determination material information for extracting pre-training candidates capable of performing transfer learning. The specific determination material information is information on the network structure of the plurality of pre-trained models, and information on the sum of degrees of importance of respective pre-trained models prepared for respective data sets used for learning the pre-trained models and the sum of degrees of importance after pruning.</p><p id="p-0098" num="0093">In the degree-of-importance evaluation table <b>700</b> illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the type of pre-trained model <b>701</b> is a network type of each pre-trained model. For example, a case is considered where the recognition processing of an object to be learned is object detection. In this case, when an image classifier is used as the pre-trained model <b>302</b>, only a part of a neural network that is referred to as a feature extractor is transferred. When the same object detector is used as the pre-trained model <b>302</b>, a part of a neural network for object detection is also transferred in addition to the feature extractor. Accordingly, even when the network structure <b>303</b> is provided, a plurality of types of pre-trained models exist. Even in the case where only the feature extractor is transferred, a plurality of types of pre-trained models may exist depending on the number of layers, the number of parameters of the respective layers, and the like. Accordingly, hereinafter, the type <b>701</b> of the pre-trained model is clarified.</p><p id="p-0099" num="0094">The pre-training data set <b>702</b> indicates a data set used for learning pre-trained model (corresponding to the learning data set <b>301</b>A illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>). When the transfer learning is performed, in order to increase the accuracy of the neural network, it is important that the feature learned by a transfer source and the feature of the data set <b>301</b>A that is an object to be learned match with each other.</p><p id="p-0100" num="0095">The sum of degrees of importance <b>703</b> is a sum of the degrees of importance of the respective pre-trained models <b>302</b>. The sum of degrees of importance <b>703</b> is the sum of the degrees of importance of the respective parameters evaluated by the learning data set <b>301</b>A and the important parameter identification unit <b>304</b>. The number of parameters varies depending on the pre-trained model <b>302</b>. Accordingly, when the simple sum is acquired, the model having the large number of parameters is likely to have the larger sum of degrees of importance. In consideration of such a case, it is possible to use a degree-of-importance average that is obtained by dividing a degree-of-importance sum of the pre-trained model by the number of parameters of the respective pre-trained models.</p><p id="p-0101" num="0096">The post-pruning sum of degrees of importance <b>704</b> is a sum of degrees of importance of the parameters remaining after pruning is performed using setting related to pruning that is given by an operator. In the post-pruning sum of degrees of importance <b>703</b>, in the same manner as the sum of degrees of importance, a degree-of-importance average may be used.</p><p id="p-0102" num="0097">The degree of importance is a value for evaluating an adverse influence exerted on the recognition performance when a certain parameter is deleted. Accordingly, it may be considered that the larger sum of degrees of importance <b>703</b>, the larger number of parameters important for the recognition processing of the object to be learned are included.</p><p id="p-0103" num="0098">Accordingly, by selecting a model having a large sum of degrees of importance <b>703</b> as the pre-trained model <b>302</b>, it becomes easy to learn a highly accurate model. Furthermore, at the time of learning a lightweight model, a plurality of parameters are deleted by pruning. Therefore, by evaluating a post-pruning sum of degrees of importance <b>704</b>, it is possible to evaluate how much the parameters remaining after pruning are important for the recognition of the learning data set <b>301</b>B. Accordingly, when a model having the large post-pruning sum of degrees of importance <b>704</b> learns the lightweight model <b>505</b>, a lightweight and highly accurate model can be learned. An operator can learn a lightweight and highly accurate model by checking such a table and by selecting a model in which both the sum of degrees of importance <b>703</b> and the post-pruning sum of degrees of importance <b>704</b> are large.</p><p id="p-0104" num="0099"><figref idref="DRAWINGS">FIG. <b>9</b></figref> and <figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrate an apparatus configuration and an example of processing flow for preparing the degree-of-importance evaluation table <b>700</b> with respect to the plurality of candidates for the pre-trained model <b>302</b> illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0105" num="0100">First, a diagram of a functional configurational example of the learning processing device according to the embodiment <b>2</b> of the present invention is illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The learning processing device functional illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref> is obtained by adding the respective functions of a learned model storage unit <b>801</b>, a pre-trained model candidate extraction unit <b>802</b>, the pre-trained model selection unit <b>803</b>, and a degree-of-importance evaluation result storage unit <b>804</b> to the learning processing device <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Among them, the learned model storage unit <b>801</b> and the degree-of-importance evaluation result storage unit <b>804</b> are storage units such as a database of a computer. The learned model storage unit <b>801</b> stores, for example, information of the models (networks) <b>102</b>-<b>1</b>, <b>102</b>-<b>2</b>, <b>102</b>-<b>3</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> as a pre-trained model, and the degree-of-importance evaluation result storage unit <b>804</b> stores an evaluation result related to the important parameters that are extracted by the important parameter identification unit (pruning unit) <b>304</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The degree-of-importance evaluation data described in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is also eventually accumulated and stored in the degree-of-importance evaluation result storage unit <b>804</b>. The pre-trained model candidate extraction unit <b>802</b> and the pre-trained model selection unit <b>803</b> express processing and determination contents by an operator.</p><p id="p-0106" num="0101">In a flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the following processes are sequentially performed using hardware resources, software resources, and the like illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. First, in first step S<b>901</b> illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, a degree-of-importance evaluation flow with respect to a plurality of candidates for the pre-trained model <b>302</b> is started. Such process is started at a timing when the operator instructs performance.</p><p id="p-0107" num="0102">In step S<b>902</b> (corresponding to the processing in the pre-trained model candidate extraction unit <b>802</b>), the candidates for the pre-trained model <b>302</b> are extracted from the learning model storage unit <b>801</b> using the network structure <b>303</b> given by the operator. The learning model storage unit <b>801</b> stores a model ID set for each model, a model name, a model network structure, a data set <b>301</b>A used for learning, and reached recognition performance in association with each other. In step S<b>902</b> (corresponding to the processing in the pre-trained model candidate extraction unit <b>802</b>), a model including a network structure that can be transferred to the given network structure <b>303</b> is extracted from the learned model storage unit <b>801</b>, and the model is outputted to the pre-trained model selection unit <b>803</b> as a pre-trained model candidate.</p><p id="p-0108" num="0103">In step S<b>903</b> (corresponding to the processing in the pre-trained model selection unit <b>803</b>), one model on which the degree-of-importance evaluation is not performed is selected from among the pre-trained model candidates extracted by the pre-trained model candidate extraction unit <b>802</b>, and the model is set as the pre-trained model <b>302</b>.</p><p id="p-0109" num="0104">In step S<b>904</b> (corresponding to processing in the important parameter identification unit <b>304</b>), a neural network to be learned is initialized using the network structure <b>303</b> given by the operator and the pre-trained model <b>302</b>. This processing is performed by a method similar to the corresponding method used in step S<b>602</b> illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0110" num="0105">In step S<b>905</b> (corresponding to the processing in the important parameter identification unit <b>304</b>), the degrees of importance of respective parameters in the neural network initialized in step S<b>904</b> are evaluated using the learning data set <b>301</b>B. This processing is performed by a method similar to the corresponding method used in step S<b>603</b> illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0111" num="0106">In step S<b>906</b> (corresponding to the processing in the important parameter identification unit <b>304</b>), it is checked whether the number of times that the evaluations in steps S<b>904</b> and S<b>905</b> are performed has reached the number of times set by the operator. When the number of times that the evaluations in steps S<b>904</b> and S<b>905</b> are performed has reached the number of times set by the operator, the process advances to step S<b>907</b>. When the number of times that the evaluations in steps S<b>904</b> and S<b>905</b> are performed has not reached the number of times set by the operator, the processing returns to step S<b>904</b> and repeats the processing until the set number of times is reached. The number of times set in the processing may be any number of times as long as the number of times is one or more. The random numbers are used at the time of initialization of the neural network performed in step S<b>904</b>. This processing may be performed to suppress the influence of the random numbers exerted by performing the evaluation a plurality of times.</p><p id="p-0112" num="0107">For example, in a case where a model that is pre-trained by image classification is transferred when a neural network for object detection is learned, in general, parameters are copied only with respect to a part of a feature extractor, and initialization is performed with random numbers with respect to other parts. The first-order or second-order gradient information of the feature extractor used at the time of performing the degree-of-importance evaluation is influenced by the initialization of another neural network at a stage subsequent to the feature extractor. Accordingly, in such a case, in order to correctly evaluate the degrees of importance of respective parameters included in the feature extractor, the initialization may be performed using a random number a plurality of times, and the degree-of-importance evaluation may be performed a plurality of times. In an actual process, it is desirable that the degrees of importance calculated in step S<b>907</b> from the degrees of importance evaluated a plurality of times using the statistical information be used as the degrees of importance of the respective pre-trained models.</p><p id="p-0113" num="0108">In step S<b>907</b> (corresponding to the processing in the important parameter identification unit <b>304</b>), the degrees of importance of the respective parameters are calculated from the degrees of importance evaluated in steps S<b>904</b>, s<b>905</b>, and S<b>906</b>. In this processing, it is possible to use the results of the time at which the degree-of importance sum is the largest. It is also possible to use the average values in the respective evaluations. The use of the order at which the degree-of-importance sum is largest corresponds to the use of the evaluation value of the time when the best initial value is obtained. In this case, the initial values of the respective parameters may be stored and may be used in actual learning. Furthermore, in a case where an average of degrees of importance at respective times is used as the degree of importance, the case corresponds to a case where parameters that are minimally influenced by the random numbers are used with emphasis.</p><p id="p-0114" num="0109">In step S<b>908</b> (corresponding to the processing in the important parameter identification unit <b>304</b>), the evaluation conditions that are used and the degrees of importance that are evaluated in step S<b>907</b> are stored in the degree-of-importance evaluation result storage unit <b>804</b> in association with each other. The evaluation conditions used in this processing relate to information related to the pre-trained model <b>302</b>, the number of times that the evaluation of the degrees of importance is performed, the method that is used for evaluating the degrees of importance in step S<b>907</b>, and the like. The degree-of-importance evaluation result storage unit <b>804</b> stores information on the learning data set <b>301</b>B used in respective evaluations and information related to the evaluated degrees of importance, a model ID, a name, and a network structure of a model used as a pre-trained model, a data set used for pre-training, and recognition performance at the time of pre-training. In this processing, with respect to the information related to the degrees of importance, all of the degrees of importance related to the respective parameters may be stored, or the statistical information may be stored. By performing the processing in step S<b>908</b>, data on the sum of degrees of importance <b>703</b> and the post-pruning sum of degrees of importance <b>704</b> with respect to a focused pre-trained model candidate is added to the degree-of-importance evaluation table <b>700</b> illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0115" num="0110">In step S<b>909</b> (corresponding to the processing of the important parameter identification unit <b>304</b>), it is checked whether the evaluation is completed with respect to all candidates for the pre-trained model extracted in step S<b>902</b>. When the evaluation is completed, the processing advances to step S<b>910</b>. When the evaluation is not completed, the processing is repeated from step S<b>903</b> until the evaluation is completed.</p><p id="p-0116" num="0111">In step S<b>910</b>, the degree-of-importance evaluation flow ends. After the processing for all necessary advance preparation models is completed, the data illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is prepared in the degree-of-importance evaluation result storage unit <b>804</b>.</p><p id="p-0117" num="0112">As has been described above, according to the present invention, by identifying only important parameters from the pre-trained model <b>302</b> at the time of performing transfer learning and by performing the learning, the lightweight model <b>306</b> can be acquired by one-time learning and hence, learning of the lightweight model <b>306</b> can be performed in a short period of time.</p><p id="p-0118" num="0113">Furthermore, as illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, <figref idref="DRAWINGS">FIG. <b>9</b></figref>, and <figref idref="DRAWINGS">FIG. <b>10</b></figref>, by only evaluating the degrees of importance with respect to the pre-trained model <b>302</b>, it is possible to evaluate whether each pre-trained model is good or bad without performing actual learning and hence, it is possible to shorten the learning of the lightweight model <b>305</b>.</p><heading id="h-0013" level="1">Embodiment 3</heading><p id="p-0119" num="0114">An embodiment 3 corresponds to a case where the pre-trained model <b>302</b> has not learned features important for recognition processing that is an object to be learned at the time of performing transfer learning.</p><p id="p-0120" num="0115">In the embodiment 1 of the present invention, the time required for learning the lightweight model <b>505</b> is shortened by transferring only important parameters in the recognition processing of the object to be learned at the time of performing transfer learning. However, at the time of performing transfer learning, there may be a case where the pre-trained model <b>302</b> has not learned features that are important for recognition processing of an object to be learned. In such a case, it is difficult to learn the lightweight model <b>505</b> with high accuracy only by transferring important parameters from the pre-trained model <b>302</b>.</p><p id="p-0121" num="0116"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating partial reinitialization processing introduced to solve such a problem. <figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates a flow of processing corresponding to the flow of processing illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The flow of processing illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> is characterized in that the processing consisting of partial reinitialization processing <b>1001</b> and pruning processing <b>1003</b> is newly added to the processing illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0122" num="0117">Such a series of pieces of processing is performed on the following premise. As has been described heretofore, the important parameter identification unit <b>304</b> identifies the important parameters and hence, the network <b>305</b> at that point of time is assumed. However, at the time of performing transfer learning, the pre-trained model <b>302</b> has not learned features important for the recognition processing of the object to be learned. Accordingly, the network <b>305</b> is insufficient. In the above-described processing, the network <b>305</b> being insufficient means that the network is excessively simplified, or necessary routes are not formed in the network.</p><p id="p-0123" num="0118">In consideration of the above-mentioned drawback, in the partial reinitialization processing <b>1001</b>, after the important parameter identification unit <b>304</b> identifies the important parameters, only the parameters to be pruned are reinitialized. As a result, when the partial reinitialization processing <b>1001</b> is applied to the neural network <b>305</b> where only the parameters having the high degrees of importance are left, the neural network <b>305</b> is reinitialized to the neural network <b>1002</b> where neurons are indicated by a broken line. This reinitialization is performed in a mode where pruning can be easily performed. That is, the reinitialization is performed such that, after learning is performed by the model generation processing unit <b>306</b>, pruning can be performed in the pruning processing <b>1003</b> with least influence on accuracy. The reinitialization that enables easy pruning can be performed such that the initialization is performed while setting the respective parameters to zero. Alternatively, random numbers that can take small absolute values may be used.</p><p id="p-0124" num="0119"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating a functional configuration example of the learning processing device of the present invention including the partial reinitialization illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart illustrating a processing flow example of the learning processing method of the present invention including the partial reinitialization illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. The functional configuration example illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref> is the configuration obtained by adding the partial reinitialization processing to the functional configuration example of the present invention described in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The processing flow example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref> is the processing flow example obtained by adding partial reinitialization processing to the example of processing flow of the present invention illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. Accordingly, only the differences between them will be described.</p><p id="p-0125" num="0120">In the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, in step S<b>1201</b>, the partial reinitialization unit <b>1101</b> reinitializes the parameters that are obtained in step S<b>604</b> by making the pruning unit <b>502</b> delete the parameters having low degrees of importance from the neural network initialized in step S<b>602</b> corresponding to the degrees of importance that are evaluated in step S<b>603</b>. As described above, the reinitialization is performed with zero or random numbers having small absolute values such that pruning can be performed easily.</p><p id="p-0126" num="0121">In step S<b>1202</b>, the pruning unit <b>502</b> deletes unnecessary parameters from the neural network that is learned in step S<b>605</b>. The reinitialization processing performed in step S<b>1201</b> is performed such that pruning can be performed easily. Accordingly, the influence exerted on the accuracy can be made small.</p><p id="p-0127" num="0122">By combining the partial reinitialization processing with the learning processing in this manner, in a case where the pre-trained model <b>302</b> has parameters important for recognition of the learning data set <b>301</b>B, the transferred parameter is learned, and in a case where the pre-trained model <b>302</b> does not have the parameters important for recognition of the learning data set <b>301</b>B, the neurons that are partially reinitialized are learned. In a case where all the features important for recognition of the learning data set <b>301</b>B are learned by the pre-trained model <b>302</b>, the neurons that are partially reinitialized can be easily pruned. The neural network may be learned again in step S<b>605</b> after the pruning in step S<b>1202</b>.</p><heading id="h-0014" level="1">Embodiment 4</heading><p id="p-0128" num="0123">In the embodiment 3, the description has been made with respect to the case where the parameters are re-initialized with small values in the partial reinitialization processing <b>1001</b>. However, the initialization may be performed substantially in the same manner as the initialization in normal learning, and a region where pruning is performed may be obtained by a method different from a method in normal learning. In an embodiment 4 of the present invention, pruning using a mask layer is described.</p><p id="p-0129" num="0124"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating a concept of pruning using a mask layer. <figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a configuration where a mask layer <b>1304</b> is applied to a convolutional neural network often used in image processing. A plurality of convolution layers <b>1302</b> are applied in a normal convolutional neural network. However, <figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a configuration when attention is paid to one convolution layer <b>1302</b>. In a normal convolutional neural network, an output <b>1306</b> is obtained by applying a convolution layer <b>1302</b>, a normalization layer <b>1303</b>, and an activation layer <b>1305</b> to an input <b>1301</b>.</p><p id="p-0130" num="0125">When the processing and parameters in the convolution layer <b>1302</b> are denoted as f and w, the processing and parameters of the normalization layer <b>1303</b> as g and &#x3b8;, the processing of the activation layer <b>1305</b> as a, the input as x, and the output as y, a series of pieces of processing in the convolution layer <b>1302</b> is expressed by a following Formula (3).</p><p id="p-0131" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Formula (3)]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0132" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>y=a(g(f(x; w); &#x3b8;)) &#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0133" num="0126">In the Formula (3), each of the input x and the output y is a third-order tensor where an image feature has dimensions of a width, a height, and a feature channel. In general, in the case of speeding up the convolution layer <b>1302</b>, it is effective to delete an arithmetic operation at a unit of feature channel. For example, in a case where the original convolution layer <b>1302</b> has one hundred twenty-eight channels, ten channels that do not exert influence on recognition performance are deleted, and one hundred and eighteen channels are outputted.</p><p id="p-0134" num="0127">In the embodiment <b>4</b> of the present invention, such deletion of the channels is performed using the mask layer <b>1304</b>. When the processing and a parameter of the mask layer <b>1304</b> are denoted as m and &#x3bd;, a series of pieces of processing of the convolution layer <b>1302</b> that is expressed by Formula (3) is converted into Formula (4).</p><p id="p-0135" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Formula (4)]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0136" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>y=a(m(g(f(x; w); &#x3b8;); &#x3bd;)) &#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0137" num="0128">In order to delete the channels at a unit of feature channel of the convolution layer <b>1302</b>, the mask layer <b>1304</b> learns the parameter &#x3bd;. The parameter &#x3bd; is a vector having the same dimension as the feature channel that takes a value of 0 or 1. In the processing m of the mask layer <b>1304</b>, an input feature map is outputted as it is in a portion where the parameter &#x3bd; is 1, and the input feature map is outputted with values of the entire width and the entire height set to in a portion where the parameter &#x3bd; is 0. By enabling learning of such parameter &#x3bd;, the calculation of the portion where the parameter &#x3bd; is 0 can be deleted after learning without exerting influence on the recognition performance. That is, by setting a hidden variable obtained when the convolution layer <b>1302</b> and the normalization layer <b>1303</b> are applied as an input as h, and by setting a hidden variable obtained by applying a mask layer to the hidden variable h as h&#x2032;, respective components of h&#x2032; are given by Formula (5).</p><p id="p-0138" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Formula (5)]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0139" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>h&#x2032;=m(g(f(x; w); &#x3b8;); &#x3bd;)=m(h; &#x3bd;) &#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0140" num="0129">In Formula (5), the hidden variable h&#x2032; to which the mask layer <b>1304</b> is applied can be expressed by the hidden variable h to which the convolution layer processing <b>1302</b> and the normalization layer <b>1303</b> are applied as an input. Accordingly, the hidden variable h&#x2032; can be expressed by Formula (6) where a width i, a height j, and a position k of a feature channel of the hidden variable that is a third-order tensor are variable.</p><p id="p-0141" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Formula (6)]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0142" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>h&#x2032;<sub>ijk</sub>=v<sub>k</sub>h<sub>ijk </sub>&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0143" num="0130">Here, the parameter &#x3bd; takes a value of 0 or 1 and hence, the parameter &#x3bd; can be expressed by Formula (7).</p><p id="p-0144" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Formula (7)]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0145" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3bd;={0, 1}&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0146" num="0131">However, the discrete parameter &#x3bd; in Formula (7) that takes a value of 0 or 1 cannot be learned by a stochastic gradient method usually used for learning of a neural network. This is because a gradient does not propagate in the discrete parameter &#x3bd;. Accordingly, it is difficult to learn which feature channel of the parameter &#x3bd; should be made effective.</p><p id="p-0147" num="0132">To solve this problem, in the embodiment 4 according to the present invention, the parameter &#x3bd; is further sampled from a relaxed Bernoulli distribution during learning. The relaxed Bernoulli distribution is obtained by relaxing the Bernoulli distribution, which is a discrete distribution, to a continuous distribution, and can be learned by a stochastic gradient descent method. The relaxed Bernoulli distribution is also known as Gumbel-softmax. When the relaxed Bernoulli distribution is used, continuous values having values that fall within a range from 0 to 1 such as 0.1 and 0.5 are sampled as the parameter &#x3bd;. The mask layer <b>1304</b> calculates and outputs products of the sampled parameter &#x3bd; and the entire channels corresponding to inputted feature maps. As a result, with respect to the channels that are unnecessary for the recognition during learning, the parameter &#x3bd; takes a value that is gradually decreased by a stochastic gradient descent method. On the other hand, with respect to the channels that are important for the recognition during learning, the parameter &#x3bd; takes a value that is gradually increased by a stochastic gradient descent method. The mask layer <b>1304</b> learns a logit of the relaxed Bernoulli distribution for sampling the parameter &#x3bd;. This logit is a parameter similar to a logit of the Bernoulli distribution. When the logit is a large value, the probability that 1 is generated is high in the Bernoulli distribution. On the other hand, the probability that a value close to 1 is generated is high in the relaxed Bernoulli distribution. At the time of performing deduction after learning is finished, by using only a feature channel where the logit of the relaxed Bernoulli distribution is larger than a predetermined value, recognition processing can be performed with a small amount of arithmetic operation without decreasing recognition accuracy. That is, the output of the mask layer <b>1304</b> is expressed by Formula (8).</p><p id="p-0148" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Formula (8)]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0149" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>h&#x2032;<sub>ijk</sub>=v&#x2032;<sub>k</sub>h<sub>ijk </sub>&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0150" num="0133">Here, &#x3bd;&#x2032; is a value sampled from the relaxed Bernoulli distribution RB having a logit <b>1</b> and a temperature t as parameters as expressed by Formula (9), and is a continuous variable between 0 and 1. In this way, the logit <b>1</b> can be learned by a stochastic gradient descent method. Accordingly, a mask can be learned at a unit of feature channel for performing pruning.</p><p id="p-0151" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Formula (9)]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0152" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3bd;&#x2032;&#x2dc;RB(1, t) &#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0153" num="0134">By performing learning such that the number of effective channels is equal to or less than a predetermined value in the relaxed Bernoulli distribution, the learning can be performed such that the logit becomes large only with respect to important feature channels, and the logit becomes small with respect to parameters that do not contribute to the recognition. As a result, at the end of learning, a state is brought about where learning is performed with fewer feature channels. Accordingly, a situation substantially equal to the post pruning relearning illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is obtained. Therefore, by using only feature channels having large logits without performing relearning, it is possible to perform pruning without decreasing recognition accuracy. Here, the learning where the number of effective channels becomes equal to or less than a predetermined value is a method where learning is performed such that logits belonging to an upper percentage group of the relaxed Bernoulli distribution that the entire neural network has become large, logits of the remaining channels become small. For example, learning is performed such that Kullback-Leibler divergence between the relaxed Bernoulli distribution and the Bernoulli distribution where generation probability of 1 is set to a high value is minimized in feature channels corresponding to the logits belonging to the upper percentage group of the relaxed Bernoulli distribution. That is, a term related to the logit is added to a loss function at the time of learning as expressed by Formula (10).</p><p id="p-0154" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Formula (10)]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0155" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>L=Lr+&#x3bb;&#x2225;&#x3b8;&#x2225;</i><sup>2</sup>+&#x3bb;<sub>l</sub><i>KL</i>(<i>B</i>(<i>l</i>)&#x2225;<i>B</i>(<i>y</i><sub>l</sub>)) &#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0156" num="0135">Here, the third term of Formula (10) is an amount of Kullback Leibler divergence between a Bernoulli distribution B (l) having the same logits as the relaxed Bernoulli distribution RB (l, t) and B (yl) where yl in which components corresponding to the logits belonging to an upper percentage group take large values are set as the logits.</p><p id="p-0157" num="0136">The pruning processing performed by the mask layer <b>1304</b> may be applied singly without being combined with the important parameter identification unit.</p><p id="p-0158" num="0137">The learning processing according to the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> may be performed using the values of the logits of the relaxed Bernoulli distribution obtained by adding the mask layer as the degrees of importance that are evaluated by the important parameter identification unit <b>304</b>. Also in this case, it is considered that the larger the values of the logits that feature channels have, the larger the influence exerted on the recognition performance of the neural network when the channels are deleted at a unit of channel becomes.</p><p id="p-0159" num="0138">In a case where the mask layer is applied to the configuration described in the embodiment 3, when the partial reinitialization unit <b>1101</b> performs reinitialization, weighting related to the neural network is initialized using an initialization method substantially equal to an initialization method used in a normal method, and the logits of the mask layer are initialized with values smaller than normal values. As a result, with respect to the reinitialized parameters, the initialization can be performed in a mode where pruning can be performed more easily than other networks.</p><heading id="h-0015" level="1">Embodiment 5</heading><p id="p-0160" num="0139">In the embodiment 5, the description is made with respect to a method of applying a mask layer to a neural network having a complicated network structure.</p><p id="p-0161" num="0140"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram illustrating the method of applying a mask layer to a neural network having a complicated network structure.</p><p id="p-0162" num="0141">In a neural network having a structure referred to as &#x201c;Residual Shortcut&#x201d; or &#x201c;Residual connection&#x201d;, a plurality of convolution layers share the same feature map as illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref>. For example, convolution layers <b>1402</b>-<i>a </i>and <b>1402</b>-<i>b </i>are applied to an input <b>1401</b> so as to form third-order tensors having the same dimension, and a sum of outputs of the convolution layers <b>1402</b>-<i>a </i>and <b>1402</b>-<i>b </i>is obtained. A convolution layer <b>1402</b>-<i>c </i>is applied to the sum of the outputs of these two convolution layers, and the sum of the convolution layer <b>1402</b>-<i>c </i>itself and two preceding convolution layers is calculated. Furthermore, the convolution layer <b>1402</b>-<i>d </i>is applied, and the sum of the convolution layer <b>1402</b>-d itself and three preceding convolution layers is calculated and a calculated value becomes an output <b>1403</b>. In this embodiment, layers other than convolution layers, that is, a normalization layer, a mask layer, and an activation layer are omitted for the sake of simplicity. However, these layers may be applied in association with each convolution layer. In a case where the plurality of convolution layers <b>1402</b>-<i>a </i>to <b>1402</b>-<i>d </i>share the same feature map in this manner, there is a case where pruning cannot be efficiently performed.</p><p id="p-0163" num="0142">On the other hand, <figref idref="DRAWINGS">FIG. <b>16</b></figref> is a table illustrating a relationship between the degrees of effectiveness in the recognition of four channels shared by four layers consisting of convolution layers <b>1402</b>-<i>a </i>to <b>1402</b>-<i>d </i>shared in the neural network having the complicated network structure illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref> and the channels that can actually be pruned.</p><p id="p-0164" num="0143">A first channel number <b>1404</b> on a first row of the table indicates the number of four feature channels of the respective convolution layers <b>1402</b>-<i>a </i>to <b>1402</b>-<i>d. </i>The valid feature channels <b>1405</b>-<i>a </i>to <b>1405</b>-<i>d </i>indicate feature channels that are made valid in the respective convolution layers <b>1402</b>-<i>a </i>to <b>1402</b>-<i>d</i>. Here, the feature channels that are made valid are feature channels in which the logit of the mask layer <b>1304</b> is equal to or larger than a threshold. In the valid feature channels <b>1405</b>-<i>a </i>to <b>1405</b>-<i>d, </i>when the logits of the corresponding channel numbers are equal to or larger than the threshold, 1 is described in the corresponding cells as the valid feature channel, and when the corresponding logits are not the valid feature channels, 0 is described in the corresponding cells. For example, in the valid feature channel <b>1405</b>-<i>a, </i>the first and fourth elements of the logit of the mask layer <b>1304</b> corresponding to the channel numbers 1 and 4 are equal to or larger than the threshold. Accordingly, the valid feature channel <b>1405</b>-<i>a </i>is a feature channel important for the recognition processing.</p><p id="p-0165" num="0144">A logical sum <b>1406</b> is a logical sum of the valid feature channels <b>1405</b>-<i>a </i>to <b>1405</b>-<i>d </i>of the convolution layers that share the feature map. Only the channel number <b>3</b> is valid in none of the convolution layers <b>1402</b>-<i>a </i>to <b>1402</b>-<i>d </i>and hence, 0 is described in corresponding cells. Other channel numbers are valid feature channels in any of the convolution layers <b>1402</b>-<i>a </i>to <b>1402</b>-<i>d </i>and hence, 1 is described in corresponding cells. With respect to the above-mentioned case, cases exist where any one of feature channels is not valid in any one of the convolution layers. Accordingly, an arithmetic operation processing cannot be deleted only with respect to the channel that is valid in none of the shared convolution layers <b>1402</b>-<i>a </i>to <b>1402</b>-<i>d </i>such as the channel number 3. Accordingly, it is difficult to delete many arithmetic operations.</p><p id="p-0166" num="0145">In a conventional pruning method, parameters to be pruned are evaluated based on values of parameters of a convolution layer and the degrees of importance of the respective parameters. Accordingly, it has been difficult to match the feature channels to be pruned among a plurality of convolution layers sharing a feature map.</p><p id="p-0167" num="0146">The mask layer <b>1403</b> can determine a feature channel that can efficiently perform pruning even with respect to a layer having a complicated network. This is performed by allowing logits of relaxed Bernoulli distributions of the mask layers <b>1304</b> accompanying the convolution layers sharing the feature map to have the same value. This processing is performed, for example, by using the same parameter with respect to the logits of the mask layers accompanying the convolution layers <b>1402</b>-<i>a </i>to <b>1402</b>-<i>d </i>or by performing learning so as to reduce an amount of Kullback-Leibler divergence between the logits. Besides the above methods, an amount of Kullback-Leibler divergence with a statistical amount such as an average or a maximum value of a plurality of logits may be reduced, or an amount of Kullback-Leibler divergence with Bernoulli distribution in which a logical sum of valid feature channels indicated by a plurality of logits is regarded as the probability that the parameter &#x3bd; takes 1, such as the logical sum <b>1406</b> may be reduced.</p><heading id="h-0016" level="1">Embodiment 6</heading><p id="p-0168" num="0147">In the embodiment 6, the description is made with respect to a configurational example of a monitor screen suitable for the learning processing device according to the present invention. <figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram illustrating a configuration of a screen for performing learning of a lightweight model. The screen configurational example illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref> includes various setting units, execution units, and result display units. The various setting units include setting factors <b>1501</b> to <b>1507</b>.</p><p id="p-0169" num="0148">Among the setting units, the learning data set setting unit <b>1501</b> is a region where setting related to the learning data set <b>301</b>B is performed. Here, data is read by designating a storage region that is stored in a predetermined format. The evaluation data set setting unit <b>1502</b> is a region where setting related to the evaluation data set <b>504</b> is performed. Here, the setting substantially equal to the setting performed in the learning data set setting unit <b>1501</b> is performed. The learning condition setting unit <b>1503</b> is a region where conditions when the learning unit (new model generating unit) <b>306</b> learns a neural network are set. The conditions relate to, for example, the number of times that parameters are updated at the time of learning, a learning rate schedule, a coefficient of weight decay, and coefficients of various loss functions.</p><p id="p-0170" num="0149">The network structure setting unit <b>1504</b> is a region where setting relating to the network structure <b>303</b> to be learned is performed. Here, an operator selects the appropriate network structure <b>303</b> in response to the recognition processing of an object to be learned. The pre-trained model setting unit <b>1505</b> is a region where the pre-trained model <b>302</b>A is set. Here, the operator selects or designates one pre-trained model.</p><p id="p-0171" num="0150">The pruning parameter setting unit <b>1506</b> is a region where parameters related to pruning are set. In the case of performing the pruning using the mask layer <b>1304</b>, an initial value of a logit of a relaxed Bernoulli distribution, an initial value of the mask probability for determining the logit, coefficients of a loss function related to the mask layer <b>1304</b>, and the like are set. In general, in learning a neural network, at an initial stage, primitive image features such as edges and curves are learned, and complex features important for improving recognition performance are gradually learned. Accordingly, in a case where a loss function related to the mask layer is strongly set from the beginning of learning or in a case where many feature channels are set to be not valid from the beginning of learning, there may be a case where it is difficult to perform learning. Accordingly, at an initial stage of learning, coefficients of a loss function of a mask may be set to extremely small values. Alternatively, it may be possible to gradually relax a mask target value that determines the number of logits belonging to an upper percentage group of the mask layer <b>1304</b> that are allowed to be valid as the learning progresses. For example, the learning may be performed such that a coefficient of a mask loss function is set to 0 in the initial 30% of the learning. Alternatively, the learning may be performed such that only 10% of the feature channels of the entire network is valid in the initial 50% of the learning, the feature channel of up to 20% is valid until the progress of the learning reaches 75%, and the feature channel of 30% is valid until the end of the learning comes.</p><p id="p-0172" num="0151">The important parameter identifying condition setting unit <b>1507</b> is a region where setting related to the processing conditions of the important parameter identification unit <b>304</b> is performed. Here, for example, the conditions including the following conditions are set. That is, the conditions are: a matrix product of a Hessian and parameters, an element product of a diagonal component of the Hessian and parameter vectors, an absolute value of an element product of gradient information and a parameter vector as an evaluation function of the degrees of importance; the use of K-FAC as a substitute for the Hessian; and the use of a mask layer. Further, a ratio of parameters to be transferred, the presence or absence of execution of partial reinitialization, and the execution method are set.</p><p id="p-0173" num="0152">Next, the execution unit is formed of: a degree-of-importance evaluation execution unit <b>1508</b>; and a learning result display unit <b>1510</b>. Here, when the degree-of-importance evaluation execution unit <b>1508</b> is selected by an operator, the important parameter identification unit <b>304</b> is operated and performs the evaluation of the degrees of importance of the respective parameters of the set pre-trained model <b>302</b>. When a lightweight model creation execution unit <b>1509</b> is selected by an operator, the learning unit <b>306</b> is operated and starts the learning flow from step S<b>604</b> in accordance with the set conditions.</p><p id="p-0174" num="0153">The learning result display unit <b>1510</b> is a region where information related to the learning result is displayed. Here, the learning result display unit <b>1510</b> displays, for example, changes in the recognition performance and the number of effective feature channels with respect to the achieved recognition performance, the time required for deduction of the obtained lightweight model, and the learning time. At a point of time that the degree-of-importance evaluation execution unit <b>1508</b> is selected and the evaluation is completed, a histogram of the degrees of importance may be displayed.</p><p id="p-0175" num="0154">The operator adjusts the respective parameters until the desired recognition performance and a desired deduction speed are acquired using this information.</p><p id="p-0176" num="0155"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram illustrating a configuration of a screen for performing the degree-of-importance evaluation with respect to a plurality of candidates for a pre-trained model. Also in this case, the screen is formed of a setting unit, an execution unit, and a result display unit.</p><p id="p-0177" num="0156">The operator determines a pre-trained model to be used for learning the lightweight model <b>505</b> illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref> using this screen. A learning data set setting unit <b>1501</b>, a network structure setting unit <b>1504</b>, and an important parameter identifying condition setting unit <b>1507</b> are substantially equal to the corresponding units illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>.</p><p id="p-0178" num="0157">An evaluation pre-trained model setting unit <b>1601</b> sets the pre-trained model whose degree of importance is evaluated. In <figref idref="DRAWINGS">FIG. <b>8</b></figref> and <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the description has been made with respect to an example where all candidates for the pre-trained model that are extracted by the pre-trained model candidate extraction unit <b>802</b> are evaluated. However, in order to shorten the evaluation time, the operator may select the model to which the degree-of-importance evaluation is performed by the evaluation pre-trained model setting unit <b>1601</b>. Here, the pre-trained model that the pre-trained model candidate extraction unit <b>802</b> extracts may be provided to the user.</p><p id="p-0179" num="0158">When a degree-of-importance evaluation collective execution unit <b>1602</b> is selected by the operator, the processing is started from step <b>5903</b> of the degree-of-importance evaluation flow illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0180" num="0159">A pre-trained model evaluation result display unit <b>1603</b> displays evaluation results of the respective pre-trained models. Here, the description has been made with respect to the example where the table illustrated in FIG. and the histogram of the degrees of importance of the selected pre-trained models are displayed.</p><p id="p-0181" num="0160">According to the present invention that adopts the above-described screen configuration, it is possible to efficiently learn the lightweight model <b>505</b>.</p><p id="p-0182" num="0161">The present invention is not limited to the above-described embodiments, and includes various modifications of these embodiments. For example, the above-described embodiments have been described in detail for facilitating the understanding of the present invention. However, the embodiments are not necessarily limited to the learning processing device that includes all configurations described above. A part of the configuration of one embodiment can be replaced with the configuration of another embodiment, and the configuration of another embodiment can be added to the configuration of one embodiment. Further, with respect to parts of the configurations of the respective embodiments, the addition, the deletion and the replacement of other configurations can be made.</p><heading id="h-0017" level="1">REFERENCE SIGNS LIST</heading><p id="p-0183" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0162"><b>101</b> input image</li>    <li id="ul0001-0002" num="0163"><b>102</b> (<b>102</b>-<b>1</b>, <b>102</b>-<b>2</b>, <b>102</b>-<b>3</b>, <b>102</b>-<b>4</b>) image processing unit (learning model)</li>    <li id="ul0001-0003" num="0164"><b>103</b> (<b>103</b>-<b>1</b>, <b>103</b>-<b>2</b>, <b>103</b>-<b>3</b>, <b>103</b>-<b>4</b>) processing result unit</li>    <li id="ul0001-0004" num="0165"><b>102</b>&#x2032; (<b>102</b>-<b>1</b>, <b>102</b>&#x2032;-<b>2</b>, <b>102</b>&#x2032;-<b>3</b>) Image processing unit (environmentally different model)</li>    <li id="ul0001-0005" num="0166"><b>103</b>&#x2032; (<b>103</b>&#x2032;-<b>1</b>, <b>103</b>&#x2032;-<b>2</b>, <b>103</b>&#x2032;-<b>3</b>) processing result unit</li>    <li id="ul0001-0006" num="0167"><b>201</b> learning processing in conventional pruning</li>    <li id="ul0001-0007" num="0168"><b>202</b> pruning processing in conventional pruning</li>    <li id="ul0001-0008" num="0169"><b>203</b> relearning processing in conventional pruning</li>    <li id="ul0001-0009" num="0170"><b>204</b> configurational example of neural network by conventional pruning</li>    <li id="ul0001-0010" num="0171"><b>301</b> (<b>301</b>A, <b>301</b>B) learning data set</li>    <li id="ul0001-0011" num="0172"><b>302</b> pre-trained model</li>    <li id="ul0001-0012" num="0173"><b>303</b> unpruned neural network</li>    <li id="ul0001-0013" num="0174"><b>304</b> important parameter identification unit</li>    <li id="ul0001-0014" num="0175"><b>305</b> neural network formed using extracted important parameters</li>    <li id="ul0001-0015" num="0176"><b>306</b> new model generating unit</li>    <li id="ul0001-0016" num="0177"><b>307</b> learning processing</li>    <li id="ul0001-0017" num="0178"><b>401</b>, <b>402</b> learning curve</li>    <li id="ul0001-0018" num="0179"><b>503</b> model evaluation unit</li>    <li id="ul0001-0019" num="0180"><b>504</b> evaluation data set</li>    <li id="ul0001-0020" num="0181"><b>700</b> degree-of-importance evaluation table</li>    <li id="ul0001-0021" num="0182"><b>701</b> type of pre-trained model</li>    <li id="ul0001-0022" num="0183"><b>702</b> pre-trained data set</li>    <li id="ul0001-0023" num="0184"><b>703</b> sum of degrees of importance</li>    <li id="ul0001-0024" num="0185"><b>704</b> post-pruning sum of degrees of importance</li>    <li id="ul0001-0025" num="0186"><b>801</b> learned model storage unit</li>    <li id="ul0001-0026" num="0187"><b>802</b> pre-trained model candidate extraction unit</li>    <li id="ul0001-0027" num="0188"><b>803</b> pre-trained model selection unit</li>    <li id="ul0001-0028" num="0189"><b>804</b> degree-of-importance evaluation result storage unit</li>    <li id="ul0001-0029" num="0190"><b>1001</b> partial reinitialization processing unit</li>    <li id="ul0001-0030" num="0191"><b>1002</b> partially reinitialized neural network</li>    <li id="ul0001-0031" num="0192"><b>1003</b> pruning processing unit</li>    <li id="ul0001-0032" num="0193"><b>1004</b> pruned neural network</li>    <li id="ul0001-0033" num="0194"><b>1301</b> inputting to convolution layer</li>    <li id="ul0001-0034" num="0195"><b>1302</b> convolution layer</li>    <li id="ul0001-0035" num="0196"><b>1303</b> normalization layer</li>    <li id="ul0001-0036" num="0197"><b>1304</b> mask layer</li>    <li id="ul0001-0037" num="0198"><b>1305</b> activation layer</li>    <li id="ul0001-0038" num="0199"><b>1306</b> outputting of a series of convolution layers</li>    <li id="ul0001-0039" num="0200"><b>1401</b> inputting to neural network having complicated network structure</li>    <li id="ul0001-0040" num="0201"><b>1402</b>-<i>a </i>to <i>d </i>plurality of convolution layers in neural network having complicated network structure</li>    <li id="ul0001-0041" num="0202"><b>1403</b> outputting to neural network having complicated network structure</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A learning processing device for obtaining a new second learning model from an existing first learning model, the learning processing device comprising:<claim-text>an input unit configured to acquire a first learning model generated in advance by learning a first learning data set, and an unpruned neural network;</claim-text><claim-text>an important parameter identification unit configured to initialize the neural network that is an object to be learned using the first learning model and the neural network, and to identify degrees of importance of parameters in recognition processing of the initialized neural network by using a second learning data set and the initialized neural network;</claim-text><claim-text>a new model generating unit that configured to generate a second neural network by performing pruning processing for deleting unimportant parameters from the initialized neural network using the degrees of importance of the parameters;</claim-text><claim-text>a learning unit configured to learn the second neural network using the second learning data set; and</claim-text><claim-text>an output unit configured to output the second neural network after learning as a second learning model.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The learning processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a display unit configured to obtain the degree of importance of the parameter obtained by the important parameter identification unit for each of the plurality of the first learning models, to store the degree of importance together with the type of the first learning model, and to provide the degree of importance as degree-of-importance information.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The learning processing device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the degrees of importance include a sum of degrees of importance and a post-pruning sum of degrees of importance.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The learning processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a partial reinitialization unit configured to partially reinitialize the parameter to be pruned when the first learning model has not been able to learn a feature important for recognition processing of an object to be learned with respect to a degree of importance given by the important parameter identification unit, wherein processing of the new model generating unit is performed after the partial reinitialization.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The learning processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the neural network is a convolutional neural network, and</claim-text><claim-text>whether or not a feature channel of an output of a convolution layer is used for recognition in a part or all of the convolution layers included in the convolutional neural network is learned by relaxed Bernoulli distribution.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The learning processing device according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>an amount of the feature channel used for recognition during learning of the relaxed Bernoulli distribution is gradually increased during the learning.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The learning processing device according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>an evaluation of the degree of importance is obtained by an amount obtained from first-order or second-order differential information related to a parameter of the neural network of a loss function and the parameter of the neural network, or a parameter of the relaxed Bernoulli distribution.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The learning processing device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>in initializing a portion of the network structure that is not initialized by the parameter of the first learning model, the portion is initialized so that the parameter is easily pruned.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A learning processing method for obtaining a new second learning model from an existing first learning model, the learning processing method comprising:<claim-text>acquiring a first learning model generated in advance by learning a first learning data set, and an unpruned neural network;</claim-text><claim-text>initializing the neural network that is an object to be learned using the first learning model and the neural network;</claim-text><claim-text>identifying degrees of importance of parameters in recognition processing of the initialized neural network by using a second learning data set and the initialized neural network;</claim-text><claim-text>generating a second neural network by performing pruning processing for deleting unimportant parameters from the initialized neural network using the degrees of importance of the parameters; learning the second neural network using the second learning data set;</claim-text><claim-text>and setting the second neural network after learning as a second learning model.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A learning processing method for obtaining a new second learning model from an existing first learning model, the learning processing method comprising:<claim-text>initializing an unpruned neural network that is an object to be learned using the unpruned neural network and the first learning model;</claim-text><claim-text>obtaining a degree of importance of a parameter in recognition processing of the initialized neural network by using a second learning data set and the initialized neural network;</claim-text><claim-text>performing pruning processing corresponding to the degree of importance; and</claim-text><claim-text>obtaining a second learning model by learning of the neural network after performing pruning using the second learning data set.</claim-text></claim-text></claim></claims></us-patent-application>