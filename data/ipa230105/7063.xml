<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007064A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007064</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364375</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>29</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0481</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0484</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>65</main-group><subgroup>4015</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04812</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04845</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">TECHNIQUES FOR EFFICIENT COMMUNICATION DURING A VIDEO COLLABORATION SESSION</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>Dropbox, Inc.</orgname><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Yang</last-name><first-name>Siya</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Rogers</last-name><first-name>Alan</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Wagner</last-name><first-name>Daniel</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Ma</last-name><first-name>Irene</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Stakelon</last-name><first-name>Jason</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques are disclosed for allowing remote participation in collaborative video review based on joint state data, for a video collaboration session, maintained by a video collaboration service. A user at a participant client device may provide one or more annotations, such as a drawing annotation, for the video data via a client application. The client application transmits computer-readable instructions for re-creating the drawing annotation to the service, which distributes the drawing instructions to the other participant client devices. Using the drawing instructions, the client applications at the client devices are configured to re-create the drawing annotation on the associated video frame displayed at the client devices. The joint state data communicated to client devices for a given session may include co-presence data that efficiently increases communication among the participants in a session. Co-presence data received by a participant client device may be depicted in a GUI of the client application.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="234.95mm" wi="155.53mm" file="US20230007064A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="200.49mm" wi="151.21mm" orientation="landscape" file="US20230007064A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="255.69mm" wi="164.51mm" orientation="landscape" file="US20230007064A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="251.54mm" wi="165.69mm" file="US20230007064A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="255.69mm" wi="164.17mm" orientation="landscape" file="US20230007064A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="255.69mm" wi="164.76mm" orientation="landscape" file="US20230007064A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="251.97mm" wi="166.29mm" file="US20230007064A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="255.69mm" wi="169.59mm" orientation="landscape" file="US20230007064A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="255.69mm" wi="165.10mm" orientation="landscape" file="US20230007064A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="252.31mm" wi="155.62mm" orientation="landscape" file="US20230007064A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="224.37mm" wi="146.39mm" orientation="landscape" file="US20230007064A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="238.34mm" wi="165.35mm" orientation="landscape" file="US20230007064A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is related to U.S. application Ser. No. ______ (Attorney Docket No. 60332-0380), titled &#x201c;Techniques for Avoiding Conflicting User Actions During a Video Collaboration Session&#x201d;, and filed Jun. 30, 2021, the entire contents of which is hereby incorporated by reference as if fully set forth herein.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">This disclosure relates to collaborative viewing of streaming video data, and more specifically, to a video collaboration service facilitating communication among client devices participating in a video collaboration session via client interfaces displayed at the client devices.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Video production generally requires multiple collaborators for any given video project, including, e.g., directors, producers, cast, crew, lighting specialists, artists, computer animators, etc. It can be very helpful for multiple collaborators to collectively view and comment on the same video at the same time.</p><p id="p-0005" num="0004">One way to accomplish collaborative video review is to gather all of the collaborators together in the same location to view and discuss the video in-person. However, given the current global nature of video production, not all teams are comprised of members that have easy access to the same geographical location. Furthermore, it can be challenging to record the particulars of such an in-person meeting for future review, including all comment information from the collaborators and the portions of the video to which the comments pertain.</p><p id="p-0006" num="0005">Another way to accomplish collaborative video review, which does not require the collaborators to be in the same location, is via networked computing systems that provide video collaboration services. Such video collaboration services allow users to collectively view video data and comment on the video data using client interfaces running on individual client devices. Some video collaboration services allow users to share video data based on screen sharing, where one of the collaborators plays the video on their client device and shares the screen with the other collaborators via the video collaboration service. Other video collaboration services stream the video data to the various client devices that are part of the collaboration.</p><p id="p-0007" num="0006">There are different ways that video collaboration services allow collaborators to share information about the subject video data. For example, some video collaboration services allow collaborating client devices to share a communication stream, which may be recorded, and may also allow collaborators to record comments on video data.</p><p id="p-0008" num="0007">Because sharing video/audio data requires significant network bandwidth, it can be challenging to communicate additional data among collaborating client devices without negatively impacting the quality of the shared video/audio data. For example, image data for drawing annotations generated for video data can be large and costly to transmit. Systems and methods disclosed herein address this and other issues.</p><p id="p-0009" num="0008">The approaches described in the above section are approaches that could be pursued, but not necessarily approaches that have been previously conceived or pursued. Therefore, unless otherwise indicated, it should not be assumed that any of the approaches described in this section qualify as prior art, or are well-understood, routine, or conventional, merely by virtue of their inclusion in this section.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">In the drawings:</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an example network arrangement for efficient communication among client devices during a video collaboration session.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an example graphical user interface for a client application of a video collaboration service.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a flowchart for disseminating information for drawing annotations to video collaboration session participants.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an example graphical user interface with an example drawing annotation on a video data frame.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts an example graphical user interface with example co-presence indicators.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a flowchart for detecting and enforcing a freeze condition for a current video timestamp in joint state data maintained for a video collaboration session.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>7</b>A-B</figref> depict example graphical user interfaces with example drawing annotations and example visual video freeze indicators.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts an example content management system.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example computing device with which techniques described herein may be implemented.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts example visual annotations for video frames.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">In the following description, for the purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the techniques described herein. It will be apparent, however, that the techniques described herein may be practiced without these specific details. In other instances, well-known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the described techniques.</p><heading id="h-0006" level="1">1. General Overview</heading><p id="p-0022" num="0021">Techniques are disclosed for allowing users at different client devices to participate in collaborative video review based on joint state data, for a video collaboration session, maintained by a video collaboration service. Specifically, the video collaboration service maintains joint state data for a video collaboration session, and provides at least a portion of the joint state data to each client device that participates in the session. The video collaboration service facilitates efficient communication among client devices participating in a video collaboration session by sending incremental updates of the joint state data for the session to the client devices.</p><p id="p-0023" num="0022">The joint state data for a video collaboration session may identify video data for the session, a current frame number or current video timestamp identifying a synchronously-viewed (or current) frame for video data being jointly viewed for the session, and one or more types of annotation provided by client devices participating in the session, including information for one or more of: general text comments, frame-specific text comments, frame-specific drawing annotations, and multi-frame drawing annotations.</p><p id="p-0024" num="0023">Annotations on video data for a session created by users that are participating in the session may be provided to the video collaboration service via client applications running at the participant client devices. Specifically, a client application running at a client device causes video data to be displayed at a display device, and a user at the client device may provide one or more annotations for the video data via the client application. For example, a user uses drawing tools of a client application at a particular client device to create a drawing annotation on a current frame displayed by the client application. The client application transmits computer-readable instructions for re-creating the drawing annotation to the video collaboration service, which distributes the drawing instructions to the other client devices participating in the video collaboration session. According to some embodiments, the client devices store drawing instructions to local versions of the joint state data. Using the drawing instructions, the client applications at the client devices are configured to re-create the drawing annotation on the associated video frame displayed at the client devices.</p><p id="p-0025" num="0024">The joint state data communicated to client devices for a given session may further include co-presence data that increases the efficiency of communication among the participants in a session thereby reducing the cognitive burden on the participants and improving the human-computer interface for collaborative video review systems. Some or all of this co-presence data may also be persisted to the session data recorded for the session. Co-presence data may include one or more of: cursor positions for participant client devices, identification of &#x201c;invisible&#x201d; actions performed at participant client devices (such as mouse-overs of graphical user interface (GUI) controls), information regarding actions that affect the joint state of the session (such as activation of video playback controls), etc. Co-presence data received by a participant client device may be depicted in a GUI of the client application in many ways, including as displayed text, as an icon associated with a GUI control, as a visual indication associated with a GUI control, as a user identifier displayed in connection with a GUI control, as a user identifier displayed in connection with a video frame, etc.</p><p id="p-0026" num="0025">Information from the joint state data maintained for a given video collaboration session, including some or all of the participant annotations (including drawing instructions) received during the session may be persisted to session data that includes information from the joint state data for the session. The persisted session data may be used to efficiently display the participant annotations with the video data after the session is complete, e.g., during a review of the session. Session data may also include events timeline data for the session, with stored participant annotations being associated with timestamps of the events timeline data.</p><p id="p-0027" num="0026">The participant annotations stored in session data may be reviewed, in connection with the associated video data, with or without respect to a stored events timeline. During review, drawing instructions included in the session data are used to re-create (re-play) drawing annotations created during the session.</p><p id="p-0028" num="0027">The timing of drawing instruction execution can be implemented in various ways. According to some embodiments, all of drawing instructions associated, in the session data, with a given video frame are automatically executed upon loading the video frame. According to some embodiments, drawing instructions associated with a given video frame are executed in an order in which the instructions were originally received during the session as indicated by the events timeline data stored in the session data. For example, during a session review, each instruction is performed, in order, with a fixed amount of delay time between performing each instruction. According to another embodiment, drawing instructions associated with a given video frame are executed at relative times indicated by the events timeline data stored in the session data. Thus, the timing of drawing annotations performed during the session review mirrors the timing of drawing annotations performed during the original session. This embodiment facilitates review of the session data in connection with video or audio data generated during the session.</p><p id="p-0029" num="0028">Furthermore, additional drawing annotations on video data associated with a completed session may be generated outside of the session. Such additional drawing annotations may be displayed with the session drawing annotations without requiring any additional processing of the session data. For example, drawing instructions for additional drawing annotations may be added to the persisted session data, or may be stored in auxiliary session data that may be either (a) combined with the original session data upon review of the session, or (b) reviewed separately from the original session data. If drawing instructions for additional drawings are included in stored session data for a previous session, the additional drawings may be visually tagged as out-of-session drawings.</p><p id="p-0030" num="0029">Because client devices store participant annotations in local joint state data stores, the client devices are able to display received participant annotations without using network bandwidth (other than receiving the initial joint state data updates, from the video collaboration service, indicating participant annotations not originating from the client device). Furthermore, because the drawing annotations are re-created (as needed) based on lightweight drawing instructions, rather than using bulky image data, communication of drawing annotations among participant client devices is relatively inexpensive. Also, the drawing instructions may be persisted for future display using much less storage space than would be required to store image data depicting drawing annotations. Further, adding drawing annotations after completion of a session is inexpensive because, like the drawing annotations created during the session, the additional drawing annotations are re-created from drawing instructions that are very inexpensive to store and to transmit across the network.</p><heading id="h-0007" level="1">2. Example Client Application</heading><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an example network arrangement <b>100</b> for efficient communication among client devices during a video collaboration session. According to some embodiments, a video collaboration service (VCS) <b>112</b> running on an example server device <b>110</b> of network arrangement <b>100</b> receives a request to set up a video collaboration session based on one or more videos, i.e., the video data for the session. In response to the request, VCS <b>112</b> establishes a video collaboration session based on joint state data that identifies the video data for the session. Users may join the established session via client applications <b>132</b>A-N on client devices <b>130</b>A-N. (For ease of explanation, when the general functionality of a client device is being discussed, an individual client device is referred to as client device <b>130</b>, an individual client application is referred to as client application <b>132</b>, and an individual local joint state data store (described in further detail below) is referred to as joint state data <b>134</b>.) According to some embodiments, client application <b>132</b> is a stand-alone application executing on client device <b>130</b>. According to another embodiment, client application <b>132</b> runs via a browser at the client device <b>130</b>.</p><p id="p-0032" num="0031">According to some embodiments, client application <b>132</b> is implemented by a client object that wraps a websocket connection and keeps track of joint state data <b>134</b>. For playback, client application <b>132</b> keeps a local copy joint state data <b>134</b>, which is initialized as undefined. When a client application <b>132</b> joins a particular session, the client application receives a video playback message from VCS <b>112</b> and updates the local copy of joint state data <b>134</b>, as described in further detail below.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an example GUI <b>200</b>, for client application <b>132</b>, displayed at a display device for client device <b>130</b>. GUI <b>200</b> is used to illustrate embodiments described herein, but embodiments are not limited thereto. Example GUI <b>200</b> includes a video viewing area <b>210</b>, a participant area <b>220</b>, a commenting area <b>230</b>, a posted comments area <b>240</b>, and a visual annotations area <b>250</b>.</p><p id="p-0034" num="0033">2.1. Video Viewing Area</p><p id="p-0035" num="0034">According to some embodiments, VCS <b>112</b> causes video data for the session to be streamed, via network <b>120</b>, to each of participating client devices <b>130</b>A-E. Each client device <b>130</b> displays the video via client application <b>132</b>. For example, video viewing area <b>210</b> displays the video data being streamed in connection with the video collaboration session. Video viewing area <b>210</b> further includes video playback controls <b>212</b>, which comprise a play/pause control, a rewind-single-frame control, a fast-forward-single-frame control, a rewind 10 seconds control, and a fast-forward 10 seconds control. Video viewing area <b>210</b> also includes a progress timestamp <b>214</b> that displays the current timestamp of the viewed video data, and also the length of the video data. Video viewing area <b>210</b> further includes a progress bar <b>216</b> with a current timestamp control <b>216</b>A, which may be moved (e.g., by clicking on and dragging control <b>216</b>A) along progress bar <b>216</b> to cause the timestamp of the displayed portion of the current video data to correspond to the position of current timestamp control <b>216</b>A along progress bar <b>216</b>.</p><p id="p-0036" num="0035">2.2. Participant Area</p><p id="p-0037" num="0036">Participant area <b>220</b> of GUI <b>200</b> includes an indication <b>222</b> of how many client devices are participating in the video collaboration session (e.g., five client devices <b>130</b>A-E). Participant area <b>220</b> further includes a toggle control <b>224</b> that controls whether client application <b>132</b> is in synchronous mode (as described in further detail below). To illustrate, when toggle control <b>224</b> is toggled to the &#x201c;on&#x201d; position, client application <b>132</b> is in synchronous mode, and when toggle control <b>224</b> is toggled to the &#x201c;off&#x201d; position, client application <b>132</b> is in asynchronous mode.</p><p id="p-0038" num="0037">Participant area <b>220</b> further includes user identifier tokens <b>226</b> that identify the users participating in the video collaboration session, e.g., via client devices <b>130</b>A-E. User identifier tokens <b>226</b> may include a user identifier or may show pictures or icons associated with the identified users. In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, users A-E have authenticated with VCS <b>112</b> via client applications <b>132</b>A-E on client devices <b>130</b>A-E, and are identified by tokens <b>226</b> as participating in the video collaboration session. A user may participate in a video collaboration session as an unauthenticated guest, in which case a token <b>226</b> representing the unauthenticated user does not reflect a user identifier.</p><p id="p-0039" num="0038">2.3. Commenting Area</p><p id="p-0040" num="0039">Commenting area <b>230</b> comprises a comment input text box <b>232</b> into which a user may type a comment annotation on the video data. Commenting area <b>230</b> further comprises a post control <b>234</b> that, when activated, causes any text in comment input text box <b>232</b>, with a current timestamp of the video being displayed in area <b>210</b> and an identifier of the user (if available), to be saved to local joint state data <b>134</b> and to be sent to VCS <b>112</b> as an annotation on the video data. VCS <b>112</b> includes the posted comment annotation in joint state data maintained by VCS <b>112</b>, and causes local versions of joint state data <b>134</b> to be updated to reflect the posted comment annotation.</p><p id="p-0041" num="0040">2.4. Posted Comments Area and Visual Annotations Area</p><p id="p-0042" num="0041">Posted comments area <b>240</b> displays comment annotations that are included in joint state data <b>134</b> with the video timestamps associated with the comment annotations. Similarly, visual annotations area <b>250</b> displays visual annotations for which instructions are included in joint state data <b>134</b> with the video timestamps associated with the visual annotations. Annotations may further be represented with user identifiers and/or an indication of the type of annotation (such as &#x201c;text&#x201d;, &#x201c;freehand drawing&#x201d;, &#x201c;sticker&#x201d;, &#x201c;shape&#x201d;, &#x201c;emoji&#x201d;, etc.). According to some embodiments, the type of annotation or other information included in posted comments area <b>240</b> and/or visual annotations area <b>250</b> for a particular annotation may be based on metadata for the annotation provided by a user (e.g., a title, a type, or a portion of comment text, etc.).</p><p id="p-0043" num="0042">Thus, when VCS <b>112</b> causes joint state data <b>134</b> to be updated with additional comment annotations and drawing annotations from other participant devices, these annotations are automatically reflected in posted comments area <b>240</b> and visual annotations area <b>250</b>. According to some embodiments, posted comments area <b>240</b> and visual annotations area <b>250</b> continue to be updated based on joint state data <b>134</b> while client application <b>132</b> is in either of synchronous mode or asynchronous mode. According to some embodiments, representations of annotations included in posted comments area <b>240</b> and visual annotations area <b>250</b> are video playback controls, activation of which jumps video playback to the timestamp associated with the activated annotation representation.</p><p id="p-0044" num="0043">According to some embodiments, annotation representations displayed in posted comments area <b>240</b> and visual annotations area <b>250</b> are hyperlinked such that clicking on (or otherwise activating a control associated with) an annotation representation causes VCS <b>112</b> to receive a video playback instruction to move playback to the video frame associated with the annotation representation. According to some embodiments, the video playback instruction further causes playback to automatically pause on the annotated frame. Furthermore, progress bar <b>216</b> may be associated with visual identifiers of annotated frames. Each of these visual identifiers may comprise a control, activation of which causes VCS <b>112</b> to receive a similar video playback instruction to skip to the associated annotated frame and pause playback.</p><heading id="h-0008" level="1">3. Disseminating Annotations and Co-Presence Data</heading><p id="p-0045" num="0044">As indicated above, VCS <b>112</b> allows video collaboration session participants to create drawing annotations for the video data of the session, e.g., displayed in viewing area <b>210</b>, and to share the drawing annotation with the other participants of the session. A drawing annotation may be displayed to the other session participants in real time, which allows the other participants to view the drawing as it is being created, or may be transmitted based on an explicit instruction from a user. <figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a flowchart <b>300</b> for disseminating information for drawing annotations to client applications for video collaboration session participants, according to embodiments described herein.</p><p id="p-0046" num="0045">3.1. Creating and Joining a Video Collaboration Session</p><p id="p-0047" num="0046">Specifically, at step <b>302</b>, video data is streamed to a plurality of client devices, the video data representing a plurality of video frames. For example, VCS <b>112</b> establishes a particular video collaboration session by initializing joint state data for the session with an identifier of particular video data and, e.g., a current video timestamp of 00:00:00. Client applications <b>132</b>A-E of client devices <b>130</b>A-E join the particular session by downloading the joint state data for the session from VCS <b>112</b>. After client applications <b>132</b>A-E join the session, VCS <b>112</b> causes the particular video data for the session to be streamed, via network <b>120</b>, to each of participating client devices <b>130</b>A-E. Each client device <b>130</b> displays the video via client application <b>132</b>, e.g., in viewing area <b>210</b> of GUI <b>200</b>.</p><p id="p-0048" num="0047">For example, VCS <b>112</b> receives a request to create a video collaboration session for the particular video data. In response to receiving the request, VCS <b>112</b> creates a new session, as described above, and provides a Uniform Resource Locator (URL) that is associated with the new session as a response to the request. Activation of the URL retrieves a webpage from VCS <b>112</b> via network <b>120</b>. Loading the webpage into a browser application running on client device <b>130</b> causes execution of client application <b>132</b>, which downloads the joint state data <b>134</b> for the session associated with the URL and causes display of GUI <b>200</b> as a user interface to client application <b>132</b>. In this example, the provided URL is activated at each of client devices <b>130</b>A-E to retrieve and load the webpage into a browser application running at each client device. Thus, each of client devices <b>130</b>A-E stores a local version of the joint state data <b>134</b>A-E for the established session.</p><p id="p-0049" num="0048">According to some embodiments, joint state data <b>134</b> and/or session data (e.g., session data <b>142</b>, <b>144</b> described in detail below) are managed by a content management system. In such embodiments, the content management system facilitates communication of joint state data <b>134</b> among client devices <b>130</b>, as described in further detail below. VCS <b>112</b> may be implemented as part of the content management system, or may interface with the content management system for purposes of managing joint state data <b>134</b> and/or session data.</p><p id="p-0050" num="0049">3.2. Video Playback Messages</p><p id="p-0051" num="0050">Video playback messages are used to synchronize video playback among client applications that are part of a given session. For example, a message comprising a video playback instruction includes a current frame (e.g., the frame of the video data at which the playback instruction was initiated), and a playback rate (e.g., a positive rate for forward movement and a negative rate for backward movement, such as 1.0 for playing at normal speed, 10.0 for fast forward, 0.0 for paused, and &#x2212;10.0 for rewind). According to some embodiments, such a message further includes a timestamp, which represents a current date/time recorded at the source client device that marks a time at which the action was initiated. Such a timestamp may be used to allow receiving client applications <b>132</b> to &#x2018;skip ahead&#x2019; to account for how long it takes for the message to go be communicated among client devices.</p><p id="p-0052" num="0051">Client application <b>132</b> sends a video playback message to VCS <b>112</b> whenever the play state locally changes at the client application, e.g., while the client application is in synchronous mode, as described in further detail below. Specifically, when a user takes an action that triggers a video playback event, such as play, pause, seek, rate change, etc., client application <b>132</b> calculates the current frame and playback rate, and updates the local joint state data <b>134</b>. According to some embodiments, if the current frame and playback rate values have changed, or if the video has been playing for longer than a single frame duration (in case the instruction is to seek back to the starting position after the video has played), then client application <b>132</b> sends a video playback message to VCS <b>112</b> reflecting the video playback instruction.</p><p id="p-0053" num="0052">Upon receipt of such a video playback message from a client application <b>132</b>, VCS <b>112</b> sends corresponding video playback messages to all other client applications <b>132</b>. According to some embodiments, VCS <b>112</b> also echoes back a video playback message to the client application <b>132</b> that originally sent the message.</p><p id="p-0054" num="0053">According to some embodiments, when sending a video playback message for an action locally taken at a client application, the client application keeps a count of pending changes that have been sent. According to some embodiments, when receiving a video playback message from VCS <b>112</b> that originated at another client application, client application <b>132</b> ignores received video playback messages when the local count of pending changes is &#x3e;0. However, if the video playback message originated from the local client application (i.e., an echo), client application <b>132</b> decrements the pending changes count. If the pending changes count is already zero, then client application <b>132</b> updates joint state data <b>134</b> and initiates the onPlayback protocol. It is safe to ignore remote video playback messages after a client application has originated one or more video playback messages, as VCS <b>112</b> is guaranteed to send out messages in the order the message are received. Thus, if a client application <b>132</b> receives any remotely-originated messages before a locally-originated message is echoed back, the received remotely-originated messages are old and out of date.</p><p id="p-0055" num="0054">Further, a video playback message may be sent by VCS <b>112</b> to a client application <b>132</b> when the client application has newly joined a session, i.e., to inform the newly-joined client application of the current video playback action for the session. To facilitate these initial video playback instructions, video playback instructions received from client applications <b>132</b> may be persisted, e.g., to session data <b>142</b>, which acts as a data store from which VCS <b>112</b> may retrieve initial video playback instructions, as needed.</p><p id="p-0056" num="0055">According to some embodiments, when client application <b>132</b> receives an initial video playback message, client application <b>132</b> updates joint state data <b>134</b> to reflect the information in the initial message and initiates an onPlayback protocol described in further detail below.</p><p id="p-0057" num="0056">3.3. OnPlayback Protocol</p><p id="p-0058" num="0057">Client application <b>132</b> implements an onPlayback protocol by which client application <b>132</b> that updates a remotePlaybackState shared state variable in joint state data <b>134</b>. Client application <b>132</b> uses this state to control playback of the video data. Thus, whenever client application <b>132</b> sees the remotePlaybackState change, client application <b>132</b> performs the following:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0058">Disables any player event handlers (so that remote changes do not trigger another broadcast).</li>        <li id="ul0002-0002" num="0059">Calculates the correct player time using the frame number and frame rate of the video.</li>        <li id="ul0002-0003" num="0060">If the new state indicates a playing mode for the video data, client application <b>132</b> sets the currentTime of the local player, and then plays the video.</li>        <li id="ul0002-0004" num="0061">If the new state indicates a paused mode for the video data, client application <b>132</b> pauses the local player and then sets the currentTime.</li>        <li id="ul0002-0005" num="0062">Client application <b>132</b> then reenables the player event handlers.</li>    </ul>    </li></ul></p><p id="p-0059" num="0063">3.4. Synchronous Mode and Asynchronous Mode</p><p id="p-0060" num="0064">According to some embodiments, client application <b>132</b> may be in synchronous mode or asynchronous mode, e.g., based on user preferences or based on a mode assigned by the session master. When client application <b>132</b> is in synchronous mode, the application controls video playback synchronously with the session data, i.e., based on joint state data <b>134</b>, which synchronizes video playback for the client applications in the video collaboration session operating in synchronous mode. When client application <b>132</b> is in asynchronous mode, the application controls video playback based on activation of the controls in GUI <b>200</b> without respect to joint state data <b>134</b>, which allows control of the video playback independent of the video collaboration session.</p><p id="p-0061" num="0065">Thus, in general, when client application <b>132</b> is in synchronous mode, activation of video playback controls, such as playback controls <b>212</b> and current timestamp control <b>216</b>A, causes client application <b>132</b> to send video playback instructions to VCS <b>112</b>, which (in general) causes local joint state data <b>134</b> stored at all participant client devices to be updated based on the video playback instructions. Video playback instructions are instructions that affect playback of the video data, such as instructions resulting from activation of controls <b>212</b> including play, rewind, fast forward, pause, and skip to a particular video timestamp. There are some instances (such as when there is an active freeze condition described in further detail below, or when a client application <b>132</b> does not have permission to control joint video playback) where activation of playback controls when in synchronous mode does not cause VCS <b>112</b> to update the joint state data. When client application <b>132</b> is in synchronous mode, the application performs playback of the streaming video data in viewing area <b>210</b> based on joint state data <b>134</b>, e.g., client application <b>132</b> causes the streamed video to be displayed at the current video timestamp indicated in joint state data <b>134</b>. Thus, all participant client devices operating in synchronous mode view the same portions of the video data at the same time, even while the video data is being individually streamed to the participant client devices <b>130</b>A-E.</p><p id="p-0062" num="0066">According to some embodiments, multiple different video files are associated with a session, and the joint state data maintained for the session identifies all of the different video files. In this embodiment, the joint state data further identifies a particular one of the different video files as the current video file for the session. A client application that is in synchronous view mode displays the video identified as the current video file for the session.</p><p id="p-0063" num="0067">In asynchronous mode, client application <b>132</b> does not control video playback based on joint state data <b>134</b>, but instead controls video playback based on activation of the controls in GUI <b>200</b> of the client application. According to some embodiments, any annotations created while client application <b>132</b> is in asynchronous mode are transmitted to VCS <b>112</b> according to embodiments described in further detail below. However, drawing annotations created while client application <b>132</b> is in asynchronous mode are not displayed in real-time at the other participant client devices (as described in further detail below). Furthermore, in asynchronous mode, client application <b>132</b> continues to update joint state data <b>134</b> based on actions performed at participating client devices that are communicated to VCS <b>112</b>. According to some embodiments, GUI <b>200</b> includes a control that allows a user to switch between synchronous mode and asynchronous mode. When a user activates a control to enter synchronous mode, video playback at the client device of the user is controlled by the joint state, as described herein. When a user activates a control to enter asynchronous mode, client application <b>132</b> continues with any current video playback actions from the synchronous state (e.g., play, pause, rewind, fast forward, etc.). However, while a client application <b>132</b> is in asynchronous mode, any video playback instructions generated at that client affects only video playback for the client, as described herein.</p><p id="p-0064" num="0068">According to some embodiments, VCS <b>112</b> records, in the joint state data for the session, playback instructions received from client applications that are in synchronous mode. These recorded playback instructions may be associated with timestamps of an events timeline, which facilitates review of the session data, as described in further detail below.</p><p id="p-0065" num="0069">3.5. Input Cursor Co-Presence</p><p id="p-0066" num="0070">According to some embodiments, VCS <b>112</b> displays, via GUI <b>200</b> at participant client devices <b>130</b>A-E, location indicators of input cursors controlled at the other participant client devices that are found within an active co-presence area of GUI <b>200</b>, which includes one or more of areas <b>210</b>-<b>250</b>. According to some embodiments, the active area of GUI <b>200</b> includes at least area <b>210</b>.</p><p id="p-0067" num="0071">According to some embodiments, in addition to displaying a location indicator of an input cursor controlled by a particular client device, VCS <b>112</b> also causes client applications <b>132</b> to display one or more auxiliary location indicators for one or more additional location-based activities being performed at the particular client device. For example, in addition to an input cursor, a user at a particular client device <b>130</b> provides information for an auxiliary location input, such as a stylus location or touch screen activation location, within the active area of GUI <b>200</b>. In this example, VCS <b>112</b> causes the other client devices <b>130</b> to display both a first location indicator of the input cursor of the particular client device <b>130</b>, and also a second location indicator of the auxiliary location input from the particular client device <b>130</b>. The second location indicator may have one or more common visual elements to indicate that the first and second location indicators originate from the same client device. Any number of location indicators may originate from a particular client device <b>130</b>. In such embodiments, cursor data further includes data for the one or more auxiliary location inputs.</p><p id="p-0068" num="0072">Returning to the discussion of flowchart <b>300</b>, at step <b>304</b>, from each of one or more client devices of the plurality of client devices, real-time cursor data is received, the real-time cursor data indicating a position of an input cursor controlled at said each client device within a graphical user interface, the graphical user interface comprising a viewing area of the video data. For example, VCS <b>112</b> receives, in real time from one or more of client devices <b>130</b>A-E, cursor data that indicates a position, within the active area of GUI <b>200</b>, of an input cursor controlled at each client device. According to some embodiments, cursor data further comprises one or more of pressure, velocity, angle, tool type, etc., which may be used to adjust the depiction of the cursor. Further, such cursor metadata may be used to inform a visual representation of an action being performed via the cursor, such as a drawing annotation being created. For example, the pressure associated with a cursor of a user creating a freehand drawing is used to select a line weight for the drawing, where light pressure generates a thin line and heavier pressure generates a thicker line.</p><p id="p-0069" num="0073">According to some embodiments, cursor data comprises X-Y coordinates indicating the position of the cursor within the active area. For example, when multiple areas of GUI <b>200</b> are included in the active area, a particular cursor position datum includes an area identifier, and X-Y coordinates relative to the indicated area. As another example, when the active area of GUI <b>200</b> is amenable to a single system of X-Y coordinates, a particular cursor position datum includes X-Y coordinates without an area identifier. As yet another example, a particular cursor position datum comprises cursor movement data indicating a direction and distance, which can be used to calculate a current position of the cursor based on previously-known positions.</p><p id="p-0070" num="0074">For example, point <b>260</b> in area <b>210</b> is considered to be at the X-Y coordinate &#x201c;0, 0&#x201d;, and point <b>262</b> in area <b>210</b> is considered to be at the X-Y coordinate &#x201c;1, 1&#x201d;. In this example, the position of an input cursor that is located within area <b>210</b> is determined relative to points <b>260</b> and <b>262</b>.</p><p id="p-0071" num="0075">To illustrate, a user A at client device <b>130</b>A, who has authenticated with client application <b>132</b>A, moves an input cursor into an active area (e.g., viewing area <b>210</b>) of GUI <b>200</b>. Accordingly, client application <b>132</b>A sends, to VCS <b>112</b>, cursor data that identifies the current position of the input cursor as &#x201c;0.26, 0.53&#x201d;, i.e., relative to points <b>260</b> and <b>262</b> of area <b>210</b>. Similarly, VCS <b>112</b> receives cursor data from client application <b>132</b>D, which has authenticated a user D, identifying a current cursor position of &#x201c;0.83, 0.28&#x201d;, and VCS <b>112</b> receives further cursor data from client application <b>132</b>E, which has authenticated a user E, identifying a current cursor position of &#x201c;0.79, 0.75&#x201d;.</p><p id="p-0072" num="0076">At step <b>306</b> of flowchart <b>300</b>, real-time cursor data received from the other client devices of the plurality of client devices is transmitted, in real time, to each client device of the plurality of client devices, said each client device being configured to display, within the graphical user interface displayed by said each client device, visual representations of the input cursors controlled at the other client devices at respective locations indicated in the real-time cursor data. For example, VCS <b>112</b> transmits, in real-time to each client device <b>130</b>A-E, cursor data not received from the client device along with information identifying a source of each cursor position datum (e.g., a user identifier, a cursor identifier, a client device identifier, etc.). Real-time data is data received within a specified (relatively short) time of creation. According to some embodiments, cursor data is real-time data when the data is received by VCS <b>112</b> and disseminated to the other participant client devices within a threshold amount of time, e.g., 100 milliseconds.</p><p id="p-0073" num="0077">Based on the received cursor data, each client device <b>130</b> displays representations of input cursors being controlled at the other participant client devices. For example, GUI <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> is displayed at client device <b>130</b>B. Client application <b>132</b>B receives cursor data from VCS <b>112</b> and, according to some embodiments, updates joint state data <b>134</b>B to reflect the cursor data. Based on the cursor data, client application <b>132</b>B displays input cursor representations <b>218</b>A, <b>218</b>D, and <b>218</b>E, which are location indicators showing the locations indicated in the received cursor data.</p><p id="p-0074" num="0078">In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, representations of input cursors for users B and C (authenticated at client applications <b>132</b>B and <b>132</b>C, respectively) are not displayed in GUI <b>200</b> at any of the participant client devices <b>130</b>A-E (other than the device at which the input cursor is controlled), either because the client application <b>132</b> is in stealth mode, or because the input cursor is not within the active area of GUI <b>200</b>.</p><p id="p-0075" num="0079">According to some embodiments, the input cursor controlled by a particular user may not be displayed by other client applications participating in the same session because the client application <b>132</b> is in a stealth mode whereby the input cursor is not displayed by the other participants in the video collaboration session even when within the active area of GUI <b>200</b>. Stealth mode may be selected by the user, e.g., in an options menu of GUI <b>200</b>, or may be imposed on the user by a master of the session. For example, cursor data may not be sent to VCS <b>112</b> by client application <b>132</b> while in stealth mode, or coordinate data may be received at VCS <b>112</b> from a client application <b>132</b> in stealth mode and, based on the application being in stealth mode, VCS <b>112</b> does not disseminate the cursor data to the other participant client applications (as described in further detail below).</p><p id="p-0076" num="0080">According to some embodiments, one or more visual aspects of a representation of a cursor displayed in GUI <b>200</b> identifies the source of the cursor data. For example, each cursor representation may be displayed with information identifying the source of the cursor, such as an avatar or other picture, a username, user initials, etc. As another example, each cursor may be depicted at least partly with a particular color that is unique among the displayed cursor representations, where the color may be on the whole cursor representation or on part of the cursor, or the color may be associated with data identifying a source of the cursor. For example, the identifying color may be behind text associated with the cursor, may be the color of some or all text associated with the cursor, may outline text associated with the cursor, may outline a picture associated with the cursor, etc. The color may be assigned by a session master, chosen by the user, or may be automatically assigned by VCS <b>112</b>. According to some embodiments, the user may change their assigned color via a menu in GUI <b>200</b>, or by middle- or right-clicking within GUI <b>200</b>, or by using a keyboard input (such as a dedicated function button or a combination of buttons used to change cursor color). According to some embodiments, the user may select a color from the colors not used to identify other participants, or may be automatically assigned a different color that is not already used to identify another participant.</p><p id="p-0077" num="0081">According to some embodiments, the active area of GUI <b>200</b> includes areas other than area <b>210</b>, such as one or both of posted comments area <b>240</b> and/or visual annotations area <b>250</b>. In this example, VCS <b>112</b> causes location indicators to be displayed for input cursors and/or other location-based activities within these other areas that are part of the active area for GUI <b>200</b>. Furthermore, VCS <b>112</b> may cause activities performed within one or more areas other than area <b>210</b> to be propagated among client devices <b>130</b>. For example, VCS <b>112</b> may cause joint state data <b>134</b> to reflect actions within these areas (such as expand/collapse of lists, activation of controls, highlighting of a particular annotation representation, etc.) which is used to perform these actions in GUI <b>200</b> displayed at other client devices <b>130</b>.</p><p id="p-0078" num="0082">3.6. Creating Drawing Annotations</p><p id="p-0079" num="0083">According to some embodiments, VCS <b>112</b> allows users to create drawing annotations on frames of the video data being streamed, and includes information for the drawing annotations in joint state data recorded for the session. For example, when the video data is paused on a particular frame, a user may click-and-drag across area <b>210</b>, which causes a line to be drawn on the displayed frame where the user drags the cursor.</p><p id="p-0080" num="0084">Other drawing tools may also be available to the user. For example, <figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts GUI <b>200</b> with example visual annotations for video frames, according to some embodiments. Specifically, <figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts stickers <b>1002</b>, emojis <b>1004</b>, and shapes <b>1010</b> that have been applied to a current frame displayed in GUI <b>200</b>. <figref idref="DRAWINGS">FIG. <b>10</b></figref> further depicts an emoji/sticker/shape control bar <b>1006</b> with controls for applying emojis, shapes, and/or stickers to the current frame. Shapes may be any geometric shape, and may be any size, color, or line weight, and may be filled in any way. Emojis, stickers, and/or shapes may be generally associated with a given frame, or may be associated with a particular location on the frame, as described herein for drawing annotations. The example visual annotations herein may be managed (e.g., with respect to communication among client devices, tracking, recording in joint session data <b>134</b> and/or session data <b>142</b>/<b>144</b>, freeze conditions, etc.) as described herein for drawing annotations.</p><p id="p-0081" num="0085">Furthermore, progress bar <b>216</b> comprises emoji/sticker/shape indicators <b>1008</b> marking timestamps at which emojis, shapes, and/or stickers have been applied to frames of the video data. Indicators <b>1008</b> may also mark timestamps of one or more other types of annotations, such as comments and/or drawing annotations. According to some embodiments, the visual depiction of each of indicators <b>1008</b> reflects one or more characteristics of the represented annotation. In the example of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, each indicator <b>1008</b> is an example emoji/sticker/shape applied to the frame. The emoji/sticker/shape used as the visual representation of an indicator <b>1008</b> may be selected in any way, e.g., based on the emoji with the highest number of applications to the frame, the emoji/sticker/shape that was first applied to the frame, etc. Distinct visual representation aspects for indicators <b>1008</b> may correspond to the different types of annotation data. Furthermore, the visual representation of an indicator may include an identifier of a user that created the represented annotation, or of a set of users that have generated annotations on the frame. According to some embodiments, selection of one of indicators <b>1008</b> causes generation of a video control command that causes the video data to jump to the timestamp marked by the selected indicator.</p><p id="p-0082" num="0086">According to some embodiments, a look and feel of a drawing annotation is unique to the user that created the annotation. For example, a color of the drawing may correspond to a color with which the input cursor, for the user, is displayed within GUI <b>200</b>. As another example, one or more other aspects of the drawing may be unique to the user creating the drawing, such as line style (solid, dashes, dots, dash patterns, dot patterns, double lines, etc.), line weight, etc. As yet another example, a drawing annotation may be displayed in visual association with information identifying the source of the annotation, such as a username, a picture, a symbol, initials of the user, etc.</p><p id="p-0083" num="0087">When operating in synchronous mode, a user may only create a drawing annotation when the video is paused according to joint state data <b>134</b>. However, when operating in asynchronous mode, a user may independently pause the video data and create a drawing annotation on the current video frame for the individual client application. According to some embodiments, whether operating in synchronous or asynchronous mode, information for the drawing annotations is transmitted to VCS <b>112</b> and stored in local joint state data <b>134</b> such that the drawing annotations may be disseminated to the other client devices, as described in further detail below.</p><p id="p-0084" num="0088">3.7. Sharing Drawing Annotations</p><p id="p-0085" num="0089">According to some embodiments, drawing annotations performed on a particular video frame are shared with other participant client devices. Drawing annotation sharing may be performed explicitly, or may be done in real-time, i.e., as the user creates the annotation. For example, as a drawing annotation on a video frame is being created by a user at a particular client device <b>130</b>A, one or more drawing instructions for re-creating the drawing annotation are provided to VCS <b>112</b> from client application <b>132</b>A. VCS <b>112</b> provides the drawing instructions to the other participant client devices <b>130</b>B-E. Client application <b>132</b> is configured to draw the drawing annotation on the video frame based on the drawing instructions upon receiving the instructions. Thus, when instructions are disseminated by VCS <b>112</b> in real-time, all participants to the session are able to view the drawing being created. Furthermore, transmission of the drawing instructions requires very little bandwidth, which conserves resources of network arrangement <b>100</b>.</p><p id="p-0086" num="0090">Returning to the discussion of flowchart <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, at step <b>308</b>, one or more drawing instructions are received from a particular client device of the plurality of client devices, where the one or more drawing instructions are for recreating a particular drawing annotation performed, at the particular client device, on a particular frame of the plurality of video frames. For example, VCS <b>112</b> receives, from client application <b>132</b>A, a plurality of drawing instructions for recreating a drawing annotation created by user A via GUI <b>200</b> displayed at client device <b>130</b>A.</p><p id="p-0087" num="0091">To illustrate, <figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts GUI <b>200</b> with an example drawing annotation <b>402</b> on a current video frame being displayed in video viewing area <b>210</b>. While drawing annotation <b>402</b> is being created, client application <b>132</b>A generates a plurality of drawing instructions to recreate the drawing annotation. To illustrate in the context of a freeform line, when client application <b>132</b>A detects a click-and-drag event across area <b>210</b> signaling line creation, client application <b>132</b>A periodically records the coordinates of the input cursor location until there is a button-release event signaling the end of a current line creation. Client application <b>132</b>A creates a drawing instruction based on each recorded coordinate and sends each drawing instruction to VCS <b>112</b> in real-time.</p><p id="p-0088" num="0092">For example, VCS <b>112</b> receives, from client application <b>132</b>A at a first time during the depicted session, the example drawing instruction &#x201c;start_drawing(user_A, 0.36, 0.60)&#x201d;, where user_A identifies the source of the drawing instructions such as an identifier of user A at client device <b>130</b>A or an identifier of client device <b>130</b>A. This instruction is created by client application <b>132</b>A in response to detecting the initiation of a drawing at the indicated coordinates. Accordingly, this example drawing instruction indicates that the user at client device <b>130</b>A has started drawing annotation <b>402</b> on the current frame of the video data displayed in area <b>210</b>, where the starting location <b>402</b>A of drawing annotation <b>402</b> is at the coordinates 0.36, 0.60.</p><p id="p-0089" num="0093">At a second time during the session (later than the first time), VCS <b>112</b> receives, from client application <b>132</b>A, a second example drawing instruction &#x201c;drawing_segment(user_A, 0.45, 0.54)&#x201d;. According to some embodiments, this instruction is created by client application <b>132</b>A based on a periodic identification of the location of the input cursor while drawing annotation <b>402</b> is being created. Periodically creating drawing instructions based on the current state of the drawing annotation allows for the other participants in the session to view the drawing as it is being created. The example drawing instruction communicates that a line should be drawn between the last-sent coordinates (location <b>402</b>A) and coordinates 0.45, 0.54 (location <b>402</b>B). This second example instruction is a non-terminal or intermediate drawing instruction, which provides additional information about the drawing without signaling drawing completion.</p><p id="p-0090" num="0094">At a third time during the session (later than the second time), VCS <b>112</b> receives, from client application <b>132</b>A, a third example drawing instruction &#x201c;end_drawing(user_A, 0.43, 0.29)&#x201d;. This drawing instruction is a terminating drawing instruction indicating completion of the current drawing. Client application <b>132</b>A creates this terminating instruction in response to detecting termination of drawing annotation <b>402</b> when input cursor representation <b>218</b>A is at location <b>402</b>C (coordinates 0.43, 0.29), e.g., based on the user releasing the click-and-drag action at location <b>402</b>C. Further, this example instruction communicates that a line should be drawn between the last-sent coordinates (location <b>402</b>B) and location <b>402</b>C.</p><p id="p-0091" num="0095">According to some embodiments, a user creating a particular annotation, such as drawing annotation <b>402</b>, may associate metadata with the annotation. To illustrate, <figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts a metadata control <b>408</b> visually associated with drawing annotation <b>402</b>. Activation of metadata control <b>408</b> causes GUI <b>200</b> to include a form with one or more text input fields that allows input of metadata for the annotation, which may include a title, a type, an importance level, a text comment, a tag (such as a hashtag), a user identifier, a workstream identifier, a team identifier, etc. This metadata is associated with the visual annotation, and is disseminated with the information for the visual annotation. According to an embodiment, metadata associated with an annotation may be depicted with representations of the associated annotation, e.g., in area <b>210</b>, area <b>240</b>, area <b>250</b>, etc.</p><p id="p-0092" num="0096">At step <b>310</b> of flowchart <b>300</b>, the one or more drawing instructions are transmitted to each client device of the plurality of client devices other than the particular client device, said each client device being configured to draw, based on the one or more drawing instructions, the particular drawing annotation on the particular frame displayed at said each client device. For example, VCS <b>112</b> transmits the drawing instructions received from client application <b>132</b>A to each of the other participant client devices <b>130</b>B-E via network <b>120</b>. According to some embodiments, VCS <b>112</b> further communicates a frame identifier of the video frame on which the drawing was created.</p><p id="p-0093" num="0097">Each client device <b>130</b> is configured to execute the drawing instructions on the indicated video frame. If the indicated video frame is currently displayed by client application <b>132</b> (e.g., client application <b>132</b>A is in synchronous mode when the drawing annotation is created and the receiving client application <b>132</b> is also in synchronous mode), the receiving client application executes the drawing instructions in real-time, i.e., within a threshold amount of time of receiving the drawing instructions. According to some embodiments, the receiving client application identifies a look and feel for the drawing annotation, created using the received drawing instructions, based on the source identifier. According to another embodiment, VCS <b>112</b> sends information for the look and feel of the drawing annotation to the receiving client application, e.g., as a part of the drawing instruction. To illustrate, VCS <b>112</b> includes, in each drawing instruction, a color code indicating a color for the drawing annotation created based on the drawing instruction.</p><p id="p-0094" num="0098">According to some embodiments, drawing instructions may be received, at VCS <b>112</b>, from multiple participant client devices during the same timeframe. For example, after receiving the first example drawing instruction and before receiving the second example drawing instruction from client application <b>132</b>A, VCS <b>112</b> receives a drawing instruction from client application <b>132</b>E. VCS <b>112</b> disseminates the drawing instruction from client application <b>132</b>E in real-time as described above in connection with the example drawing instructions from client application <b>132</b>A. Client application <b>132</b> is configured to create drawings from different sources concurrently as drawing instructions from the multiple sources are received from VCS <b>112</b>.</p><p id="p-0095" num="0099">3.8. Explicit Sharing of Drawing Annotations</p><p id="p-0096" num="0100">According to some embodiments, client application <b>132</b> may automatically send drawing instructions to VCS <b>112</b> as they are identified, e.g., in real time, or client application <b>132</b> may send drawing instructions in response to detecting an explicit instruction to share the drawing instructions. Specifically, according to some embodiments, client application <b>132</b> provides an option to create drawing annotations without automatically sharing the drawing with other session client applications, e.g., drawing suppression mode. While client application <b>132</b> is in drawing suppression mode, drawing annotations created at the application may be created and stored only to the local joint state data <b>134</b> (e.g., with a &#x201c;local&#x201d; tag to indicate that the drawing was not shared), and/or may be deleted without sharing the annotation.</p><p id="p-0097" num="0101">Furthermore, a drawing annotation created while client application <b>132</b> is in drawing suppression mode may be modified until the user is satisfied, where drawing instructions are sent to VCS <b>112</b> for dissemination in response to an explicit share command at the client application. When a drawing annotation is explicitly shared, client application <b>132</b> either (a) sends, to VCS <b>112</b>, drawing instructions generated during the drawing annotation creation process, including instructions for portions of the drawing that are not included in the final drawing annotation, or (b) generates a set of drawing instructions that reflect the final drawing annotation and sends the generated set of drawing instructions to VCS <b>112</b>.</p><p id="p-0098" num="0102">For example, as depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, while client application <b>132</b>A is in synchronous mode and in drawing suppression mode, drawing annotation <b>402</b> is created on a current video frame displayed in GUI <b>200</b> of client application <b>132</b>A, e.g., while the users are discussing other aspects of the project or frame. When user A is prepared to share drawing annotation <b>402</b>, either because the drawing annotation is fully prepared or the discussion is ready for the drawing, user A uses a menu of GUI <b>200</b> or a dedicated function key/combination of keys to send drawing instructions for the annotation to VCS <b>112</b>.</p><p id="p-0099" num="0103">VCS <b>112</b> receives the instructions for drawing annotation <b>402</b>, as a set, from client application <b>132</b>A and, within a threshold amount of time of receiving the set of instructions, automatically sends the set of instructions to each of client devices <b>130</b>B-E. Client application <b>132</b> at each participant client device that displays the indicated video frame automatically uses the set of drawing instructions to create drawing annotation <b>402</b> on the video frame, which allows the participants to view drawing annotation <b>402</b> in real-time with respect to the explicit instruction from user A to share the annotation.</p><heading id="h-0009" level="1">4. Video Playback with Drawing Annotations</heading><p id="p-0100" num="0104">According to some embodiments, each client application <b>132</b> stores received drawing instructions in local joint state data <b>134</b>. Client application <b>132</b> re-creates drawing annotations for a displayed video frame using the drawing instructions associated with video frame in local joint state data <b>134</b>. During the session, playback of the video data may include display of drawing annotations or exclude drawing annotations. Based on user preferences, VCS <b>112</b> may cause video playback to skip all annotations, or to show one or more drawing annotations according to a selected display mode, as described in further detail below.</p><p id="p-0101" num="0105">According to some embodiments, users may identify one or more annotated video frames on which to display annotations during playback, and selection data is disseminated to joint state data <b>134</b> maintained by participant client devices. Such selection information may also be persisted to session data <b>142</b> for later review.</p><p id="p-0102" num="0106">For example, drawing annotation area <b>450</b> in GUI <b>200</b> depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref> includes annotation inclusion controls <b>404</b>A and <b>404</b>B, which, when set to the on position (such as with annotation inclusion control <b>404</b>B), causes the drawing annotations in the associated video frame to be displayed during playback. In example area <b>450</b>, annotation inclusion control <b>404</b>A is set to the off position, and playback of the video data will exclude drawing annotations on the associated video frame at time 00:00:10:06. However, annotation inclusion control <b>404</b>B is set to the on position, and playback of the video data will exclude drawing annotations on the associated video frame at time 00:00:18:11 (i.e., the currently-viewed video frame).</p><p id="p-0103" num="0107">According to the embodiment shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, because control <b>404</b>B is set to the on position, an additional display mode control <b>406</b> is made available for the associated video frame. In this example, display mode control <b>406</b> is a drop-down box with a (hidden) list of options, among which &#x201c;PAUSE&#x201d; has been selected. Available display modes may include options to: display annotations on the video frame during normal speed playback; slow down on the indicated frame with drawing annotations (including slowing down on zero or more frames before and/or after the annotated frame); or pause on the associated frame with drawing annotations (e.g., the selected &#x201c;PAUSE&#x201d; option).</p><p id="p-0104" num="0108">For example, during video playback based on the example drawing annotation display options selected in drawing annotation area <b>450</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, client application <b>132</b> displays the video frame at timestamp 00:00:10:06 without displaying any drawing annotations on the frame. Further, when playback reaches the video frame at timestamp 00:00:18:11, client applications <b>132</b> pauses playback and re-creates drawing annotation <b>402</b> in area <b>210</b> based on the drawing instructions stored in joint state data <b>134</b> associated with the video frame.</p><heading id="h-0010" level="1">5. Joint State Data Persisted as Session Data</heading><p id="p-0105" num="0109">According to some embodiments, VCS <b>112</b> persists joint session data generated during a particular session for future review, e.g., as session data <b>142</b> in data store <b>140</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For example, joint session data generated by VCS <b>112</b> is persisted based on a retention policy, which may be established by a session master described in further detail below. The retention policy indicates what types of data from the generated joint session data is persisted to session data <b>142</b>. To illustrate, a retention policy may indicate one or more types of data to persist, such as: an events timeline, drawing annotation data, comment annotation data, annotation data from one or more sources, co-presence data (such as cursor data), video playback instructions, etc. If a retention policy indicates that an events timeline should be persisted, instructions recorded in session data <b>142</b> are associated with timestamps indicating the original timing of the instructions during the recorded session.</p><p id="p-0106" num="0110">5.1. Reviewing Session Data</p><p id="p-0107" num="0111">Based on session data <b>142</b>, the recorded video collaboration session may be reviewed after the session is complete. For example, after completion of a session that was persisted in session data <b>142</b>, VCS <b>112</b> receives a request to review session data <b>142</b> from a client device <b>130</b>F. Accordingly, VCS <b>112</b> provides session data <b>142</b> to client device <b>130</b>F, which stores the session data as joint state data <b>134</b>F. A client application <b>132</b>F running on client device <b>130</b>F uses joint state data <b>134</b>F to review the session. Client application <b>132</b>F may provide controls for session review, such as controls for which drawing annotations to display, what display mode to use for displaying the annotations, whether and how to use an events timeline, etc.</p><p id="p-0108" num="0112">Client application <b>132</b>F may use a stored events timeline to cause instructions (including drawing instructions, and/or playback instructions) to be performed in an order in which the instructions were received, or to be performed with timing that mirrors the original timing during the session. For example, events timeline data may be used to cause drawing annotations to be re-created during session review based on the order of received drawing instructions. The timing of drawing instruction execution during session review may comprise a fixed amount of delay between drawing instruction execution, or may mirror the timing of the original instructions as indicated by associated events timeline timestamps.</p><p id="p-0109" num="0113">As a further example, client application <b>132</b>F may cause playback instructions in joint state data <b>134</b>F to be executed based on a stored events timeline, which facilitates review of the session in connection with audio and/or video generated by participant users during the session. According to some embodiments, when playback instructions are executed based on a stored events timeline, representations of input cursors from one or more of the session participants may also be displayed, i.e., based on cursor data associated with events timeline timestamps stored in joint state data <b>134</b>F. According to some embodiments, GUI <b>200</b> includes a control to show/hide input cursor representations during session review. These controls may allow selective display of cursor representations associated with particular users.</p><p id="p-0110" num="0114">5.2. Adding Annotations after Session Completion</p><p id="p-0111" num="0115">According to some embodiments, the video annotations recorded for video data in session data <b>142</b> may be adjusted. Such adjustments may include adding an annotation to the session data, either (a) during session review, or (b) during a separate session. For example, during session review at client application <b>132</b>F, a user creates a comment annotation or drawing annotation for a particular video frame. Accordingly, client application <b>132</b>F sends information for the annotation to VCS <b>112</b>. The annotation may be associated with an &#x201c;out-of-session&#x201d; tag or with an identifier of a user F authenticated with client application <b>132</b>F. In future reviews of session data <b>142</b>, the out-of-session annotation is available, e.g., in posted comments area <b>240</b> or visual annotations area <b>250</b>, and may be associated with a visual out-of-session identifier.</p><p id="p-0112" num="0116">New annotations may be stored in session data <b>142</b> or in second session data <b>144</b> in data store <b>140</b>, which may be configured by VCS <b>112</b> as a separate session that is reviewable independent of session data <b>142</b> or may be configured as auxiliary data to session data <b>142</b> that is reviewable in connection with session data <b>142</b>. According to some embodiments, session data <b>142</b> is modified to include a reference to session data <b>144</b>.</p><p id="p-0113" num="0117">To illustrate review of session data with out-of-context annotations, after creation of session data <b>144</b> with one or more out-of-session annotations, VCS <b>112</b> receives a request to review session data <b>142</b> from a client device <b>130</b>G. Accordingly, based on a reference to session data <b>144</b> in session data <b>142</b>, VCS <b>112</b> provides session data <b>142</b> and session data <b>144</b> to client device <b>130</b>G, which stores the session data as joint state data <b>134</b>G. A client application <b>132</b>G running on client device <b>130</b>G uses joint state data <b>134</b>G to review the session with the out-of-session annotations stored in joint state data <b>134</b>G. For example, a particular frame of video data is associated with both drawing annotations from the original session and out-of-session drawing annotations. Upon displaying the particular frame in area <b>210</b>, client application <b>132</b>G causes both the drawing annotations from the original session and the out-of-session drawing annotations to be re-created in area <b>210</b>, e.g., with visual &#x201c;out-of-session&#x201d; indicators associated with the out-of-session drawing annotations.</p><heading id="h-0011" level="1">6. Additional Co-Presence Context</heading><p id="p-0114" num="0118">It can be useful, during a discussion of video data, to know that a particular user is about to perform, or has performed, an action that affects the joint state of a session such as video playback, or annotation creation. Collaborative video sessions can become disorganized when multiple people try to express different ideas at the same time. Information about the actions of session participants can help the other participants to have context for their own actions, which facilitates communication and smoother transitions during a collaboration session. These benefits allow a collaboration session to take less time and computing resources to complete. Further, in-person meetings provide context clues to people's behavior that are not available in an online setting. Session context information provided throughout a video collaboration session can further help the session participants to feel more connected, and help the participants to work together more easily.</p><p id="p-0115" num="0119">Thus, according to some embodiments, VCS <b>112</b> receives and disseminates co-presence information for context events occurring during a video collaboration session, where context events are actions performed by session participants such as one or more of: interaction with a video playback control, interaction with an annotation control, joining or leaving a session, entering or exiting a particular mode (i.e., stealth mode, drawing suppression mode, synchronous mode, asynchronous mode), etc. Interaction with a control comprises any kind of interaction including activating the control, rolling over the control, clicking on a control, typing text into a text box control, requesting to view a set of controls (such as drawing controls), etc. Upon receiving co-presence information describing a particular context event, client application <b>132</b> is configured to display, in GUI <b>200</b>, a visual co-presence notification with information for the particular context event. According to some embodiments, the co-presence information is also stored in joint state data maintained for the session.</p><p id="p-0116" num="0120">To illustrate, VCS <b>112</b> receives, from a particular client device of a plurality of client devices participating in a video collaboration session, user interaction information reflecting a user interaction with a graphical user interface displayed by the particular client device. For example, user B at client device <b>130</b>B rolls over or activates one of controls <b>212</b> in GUI <b>200</b> displayed at the client device. Client application <b>132</b>B at client device <b>130</b>B sends co-presence information to VCS <b>112</b> that describes this context event.</p><p id="p-0117" num="0121">According to some embodiments, in response to receiving the user interaction information, VCS <b>112</b> transmits, to each client device of the plurality of client devices other than the particular client device, the user interaction information, said each client device being configured to display, in real-time, a visual representation of the user interaction information. For example, VCS <b>112</b> sends the received co-presence information with an identifier of the source of the co-presence information to client devices <b>130</b>A and <b>130</b>C-E.</p><p id="p-0118" num="0122">Client application <b>132</b> is configured to display a visual indication of the context event reflected in the co-presence information. To illustrate, in response to receiving the co-presence information from VCS <b>112</b>, client device <b>130</b>A displays a visual indication, in GUI <b>200</b>, describing the context event. For example, <figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts GUI <b>200</b> with a context message <b>502</b> that comprises a visual indication of the context event described in the received co-presence information.</p><p id="p-0119" num="0123">As another example, the visual indication of the context event is associated with a control, in GUI <b>200</b>, that corresponds to the context event. The visual indication associated with a control in GUI <b>200</b> may comprise any change in the look and feel of the control, such as one or more of: a color change, a size change, a line weight change, an outline color change, animation (such as blinking), visual association with an icon, visual association with a user identifier, etc. To illustrate, <figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a highlight <b>504</b> is applied to the play control, which client application <b>132</b> displays in response to receiving co-presence information describing the context event of a user (user B) rolling over the play control. As another example, in response to receiving the co-presence information describing the context event of a user (user B) rolling over the play control, a user identifier <b>506</b> is visually associated with the play control. While the play control is the subject of this example, any control in GUI <b>200</b> may be associated with a visual context event indication.</p><p id="p-0120" num="0124">According to some embodiments, a user may suppress visual context event indications, e.g., by suppressing all visual indications, suppressing visual indications based on a type of context event, suppressing visual indications based on a user that initiated the context event, etc.</p><p id="p-0121" num="0125">According to some embodiments, VCS <b>112</b> includes the co-presence information in session data <b>142</b> recorded for the video collaboration session. The co-presence information recorded in session data <b>142</b> may be associated with event timeline timestamps, such that review of the session data may include visual context event indications displayed during the original session.</p><heading id="h-0012" level="1">7. Video Progression Freeze Condition</heading><p id="p-0122" num="0126">When a first user is performing a frame-specific interaction with client application <b>132</b>, such as creating a drawing annotation on a particular frame of video data, it can be very disruptive for another user to cause VCS <b>112</b> to move the video away from the particular frame. To address this issue, VCS <b>112</b> freezes a current video timestamp indicated in the joint state data for a session in response to detecting a freeze condition that, according to some embodiments, is based on real-time drawing updates being received by VCS <b>112</b>.</p><p id="p-0123" num="0127"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a flowchart <b>600</b> for detecting and enforcing a freeze condition for a current video timestamp in joint state data maintained for a video collaboration session. Specifically, at step <b>602</b> of flowchart <b>600</b>, video data is concurrently streamed to a plurality of client devices based, at least in part, on joint state data that comprises a current frame value identifying a currently-viewed frame of a plurality of video frames of the video data. For example, VCS <b>112</b> receives requests for client devices <b>130</b>A-E to join a particular video collaboration session established by VCS <b>112</b> (as described in detail above). VCS <b>112</b> provides each of client devices <b>130</b>A-E with joint state data <b>134</b> for the requested session, which indicates video data for the session, and a current video timestamp that identifies a synchronously-viewed (or current) frame for the video data. VCS <b>112</b> causes the video data to be streamed to each of client devices <b>130</b>A-E, which, when in synchronous mode, display the streamed video at the current video timestamp indicated in joint state data <b>134</b>.</p><p id="p-0124" num="0128">At step <b>604</b> of flowchart <b>600</b>, a plurality of real-time drawing data updates for a drawing annotation performed on a particular frame of the plurality of video frames at a particular client device, of the plurality of client devices, are received from the particular client device, the plurality of real-time drawing data updates being received in real time over a particular timespan. For example, as depicted in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, VCS <b>112</b> receives a plurality of drawing instructions for re-creating a drawing annotation <b>702</b> created at client application <b>132</b>A running on client device <b>130</b>A.</p><p id="p-0125" num="0129">To illustrate, VCS <b>112</b> receives, at a first time during the depicted session, the example drawing instruction &#x201c;start_drawing(user_A, 0.36, 0.60)&#x201d;, where user_A is an identifier the source of the drawing instruction, and coordinates 0.36, 0.60 identify location <b>702</b>A. At a second time during the session (later than the first time), VCS <b>112</b> receives a second example drawing instruction &#x201c;drawing_segment(user_A, 0.45, 0.54)&#x201d;, where coordinates 0.45, 0.54 identify location <b>702</b>B. At a third time during the session (later than the second time), VCS <b>112</b> receives a third example drawing instruction &#x201c;drawing_segment(user_A, 0.43, 0.29)&#x201d;, where coordinates 0.43, 0.29 identify location <b>702</b>C. Note that none of the received drawing instructions are terminating drawing instructions.</p><p id="p-0126" num="0130">At step <b>606</b> of flowchart <b>600</b>, during the particular timespan, each real-time drawing data update of the plurality of real-time drawing data updates is transmitted to each client device of the plurality of client devices other than the particular client device, said each client device being configured to display, during the particular timespan and based on each real-time drawing data update of the plurality of real-time drawing data updates, an updated version of the drawing annotation on the particular frame displayed at said each client device. For example, in real-time with respect to the first time during the session, VCS <b>112</b> sends the first example drawing instruction (&#x201c;start_drawing(user_A, 0.36, 0.60)&#x201d;) to each of client devices <b>130</b>B-E, which are the participant client devices participating in the session other than the source client device of the instruction. Similarly, in real-time with respect to the second time during the session, VCS <b>112</b> sends the second example drawing instruction (&#x201c;drawing_segment(user_A, 0.45, 0.54)&#x201d;) to each of client devices <b>130</b>B-E. Further, in real-time with respect to the third time during the session, VCS <b>112</b> sends the third example drawing instruction (&#x201c;drawing_segment(user_A, 0.43, 0.29)&#x201d;) to each of client devices <b>130</b>B-E. Using these received instructions, the client application <b>132</b> is able to re-create drawing annotation <b>702</b> being created at client application <b>132</b>A.</p><p id="p-0127" num="0131">According to some embodiments, these drawing instructions received at VCS <b>112</b> are also stored (in explicit association with an identifier of the frame on which the drawing annotation was created) to the joint state data for the session. Thus, any participant client application that joins the session (and receives the joint state data for the session) after the time that a stored drawing instruction is received will receive the persisted drawing instruction to perform on the indicated frame.</p><p id="p-0128" num="0132">At step <b>608</b> of flowchart <b>600</b>, a freeze condition is detected based on the plurality of real-time drawing updates. A freeze condition is an occurrence that causes VCS <b>112</b> to freeze the current frame of the joint state data. A freeze condition is considered &#x201c;active&#x201d; until the freeze condition is &#x201c;lifted&#x201d;, or the freeze condition is no longer applicable. Freezing the current frame of the joint state data comprises maintaining a current value of the current frame number or current video timestamp of the joint state data, which causes the session participants that are in synchronous mode to continue viewing the current frame indicated in the joint state data, even if synchronous video playback instructions are received to the contrary. Freezing the current frame of the joint state data allows participants in the session, that are operating in synchronous mode, to complete frame-specific actions before the synchronized current frame is moved away from the frame to which the frame-specific actions apply.</p><p id="p-0129" num="0133">According to some embodiments, detection of a freeze condition may be implicit, indicated by VCS <b>112</b> detecting that a synchronized frame-specific action is being performed or is likely imminent, such as receiving co-presence data indicating a context event describing an interaction with annotation tools (e.g., drawing tools or comment tools in area <b>230</b>, etc.) within a threshold amount of time of the current time by a client application that is in synchronous mode. As another example implicit detection of a freeze condition, VCS <b>112</b> receives, from a client application that is in synchronous mode, drawing instructions for a drawing annotation on the current video frame (a) within a threshold amount of time of the current time, or (b) without receiving a terminating drawing instruction indicating completion of the current drawing, etc. Furthermore, detection of a freeze condition may be explicit, such as receiving a request from a client application to impose a freeze condition.</p><p id="p-0130" num="0134">To illustrate step <b>608</b>, VCS <b>112</b> detects a freeze condition based on having received one or more drawing instructions, from client application <b>132</b>A in synchronous mode, for a current frame indicated in the joint state data without having received a terminating drawing instruction. To illustrate in the context of drawing annotation <b>702</b>, the last drawing instruction received by VCS <b>112</b> was an intermediate-type drawing instruction (&#x201c;drawing_segment(user_A, 0.43, 0.29)&#x201d;). Thus, VCS <b>112</b> detects an active freeze condition for the current frame of the joint state data.</p><p id="p-0131" num="0135">To illustrate an explicitly-detected freeze condition, <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> depict GUI <b>200</b> displayed on client device <b>130</b>C that includes a freeze control <b>704</b>A. A user C, at client device <b>130</b>C, activates freeze control <b>704</b>A, based on which VCS <b>112</b> detects a freeze condition precipitated by client device <b>130</b>C. This freeze condition may concurrently exist with other detected freeze conditions (e.g., with the implicit freeze condition precipitated by client device <b>130</b>A based on drawing instructions for drawing annotation <b>702</b>).</p><p id="p-0132" num="0136">At step <b>610</b> of flowchart <b>600</b>, based on the detected freeze condition, the current frame value, in the joint state data, is frozen at the particular frame. For example, in response to detecting the active freeze condition, VCS <b>112</b> freezes the current video timestamp or current frame value in the joint state data for the current session. According to some embodiments, freezing the current frame value, in the joint state data, at the particular frame comprises causing video playback controls, displayed at one or more client devices of the plurality of client devices, to be disabled.</p><p id="p-0133" num="0137">According to another embodiment, freezing the current frame value, in the joint state data, at the particular frame comprises ignoring video playback instructions from participant client devices, participating in the session, until the freeze condition is lifted. In this embodiment, VCS <b>112</b> receives, from a second client device of the plurality of client devices other than a client device that precipitated the freeze condition (e.g., client device <b>130</b>A), a video playback instruction to change the current frame value in the joint state data. After receiving the video playback instruction, and based, at least in part, on the detected freeze condition, VCS <b>112</b> maintains the current frame value, in the joint state data, identifying the particular current frame being synchronously displayed from the video data.</p><p id="p-0134" num="0138">7.1. Lifting the Video Progression Freeze Condition</p><p id="p-0135" num="0139">In order to unfreeze the current frame of the joint state data, no active freeze conditions may be in effect. According to some embodiments, the freeze condition may be lifted implicitly or explicitly. For example, VCS <b>112</b> may detect an implicit lift of a previously-detected freeze condition based on determining that the threshold amount of time since detecting the freeze condition has lapsed, or based on receiving a terminating drawing instruction from the client device whose drawing instructions precipitated the freeze condition.</p><p id="p-0136" num="0140">For example, as depicted in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, VCS <b>112</b> receives a further drawing instruction for a drawing annotation <b>702</b>. Specifically, in this example, VCS <b>112</b> receives, at a fourth time during the depicted session (after the third time referenced in the example drawing instructions above), a fourth example drawing instruction &#x201c;end_drawing(user_A, 0.46, 0.22)&#x201d; from client device <b>130</b>A. This drawing instruction is a terminating instruction, e.g., caused by user A disengaging the drawing mechanism of GUI <b>200</b> that ends the third drawing segment of drawing annotation <b>702</b> at location <b>702</b>D (coordinates 0.46, 0.22). Based on receiving the terminating drawing instruction from client device <b>130</b>A, VCS <b>112</b> detects an implicit lift to the previously-detected freeze condition precipitated by drawing instructions from the same client device.</p><p id="p-0137" num="0141">Multiple freeze conditions may be active during the same time period. For example, drawing instructions are received from multiple client devices during the same time period, and VCS <b>112</b> detects a distinct freeze condition for each of the multiple client devices based on the drawing instructions from each client device. When multiple freeze conditions are simultaneously active, VCS <b>112</b> maintains a freeze on the current frame of the joint state data until all of the freeze conditions are lifted.</p><p id="p-0138" num="0142">Furthermore, VCS <b>112</b> may detect an explicit lift of a previously-detected freeze condition, e.g., based on receiving an instruction to lift the freeze condition. More specifically, an explicitly-requested freeze condition requested by a particular client device may be lifted by an instruction to unfreeze the frame from the same client device. For example, a particular freeze condition is precipitated by an explicit freeze request from client application <b>132</b>C. As depicted in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, because activation of freeze control <b>704</b>A (of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>) of client application <b>132</b>C precipitated a currently-active freeze condition, GUI <b>200</b> of client application <b>132</b>C displays an unfreeze control <b>704</b>B. Unfreeze control <b>704</b>B may be activated, e.g., by user C at client application <b>132</b>C, to explicitly lift the freeze condition precipitated by that client device.</p><p id="p-0139" num="0143">7.2. Visual Video Freeze Indication</p><p id="p-0140" num="0144">According to some embodiments, based on an active freeze condition being in effect, VCS <b>112</b> causes a visual video freeze indication to be displayed by one or more client devices of the plurality of client devices. A visual video freeze indication may be associated with one or more of the video playback controls in client application <b>132</b> in synchronous mode, and may comprise a look and feel for the controls that is distinct from a non-freeze look and feel. For example, based on one or more active freeze conditions, VCS <b>112</b> causes video playback controls <b>712</b> (depicted in <figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>7</b>B</figref>) to be displayed with example video freeze indications, in that the video playback controls have a different look and feel from example controls <b>212</b> displayed without visual video freeze indications.</p><p id="p-0141" num="0145">As another example, a visual video freeze indication may identify the source(s) of one or more active freeze conditions. To illustrate, <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> includes a minimized freeze information control <b>706</b>A. According to some embodiments, minimized freeze information control <b>706</b>A is a visual video freeze indication that is displayed in GUI <b>200</b> at participant client devices in synchronous mode when there is an active freeze condition. When activated, minimized freeze information control <b>706</b>A expands to maximized freeze information control <b>706</b>B, which includes information regarding users or client devices that precipitated the one or more active freeze conditions. In this example, user A is currently drawing, and user C has explicitly requested a video freeze. Activation of maximized freeze information control <b>706</b>B returns the control to minimized freeze information control <b>706</b>A.</p><p id="p-0142" num="0146">According to another embodiment, a visual video freeze indication may not be visually associated with video playback controls. For example, in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, context message <b>708</b> indicates that user C has activated freeze control <b>704</b>A. In this example, context message <b>708</b> is displayed by client application <b>132</b>C, which precipitated the freeze condition that is the topic of the visual video freeze indication. However, according to some embodiments, such a context message is displayed at client applications that did not precipitate the freeze condition that is the topic of the visual video freeze indication.</p><heading id="h-0013" level="1">8. Session Master</heading><p id="p-0143" num="0147">One or more session masters may perform administrative functions for a session, such as setting session options, mediating freeze conditions, and controlling mode options for session participants, and/or controlling modes in which client applications function. According to some embodiments, one or more of client devices <b>130</b>A-E or one or more user identifiers are identified, in the joint state data of an established session, as session masters. For example, the master of a particular session is the client device from which the request to initiate the session originated, or the master of a particular session is identified based on a user identifier included in the request to initiate the session, etc.</p><p id="p-0144" num="0148">A session master of a particular session may grant or revoke, to other participants in the session, permission to perform actions in a session such as adding annotations, controlling video playback, etc. Furthermore, a session master may determine which participant annotations are persisted to session data <b>142</b> on, e.g., based on user identifier, a role of the user, etc. Specifically, in addition to the session master role, participants in a session may be assigned roles such as &#x201c;principal collaborator&#x201d;, &#x201c;secondary collaborator&#x201d;, &#x201c;observer&#x201d;, etc.</p><p id="p-0145" num="0149">Furthermore, a session master may determine whether a particular participant may operate in a particular mode such as stealth mode, asynchronous mode, or drawing suppression mode. Furthermore, the session master may determine what kind of co-presence data is communicated and/or represented by client applications participating in the session, whether a user may opt out of co-presence notifications, and what kind of co-presence data may be persisted in the session data <b>142</b>.</p><p id="p-0146" num="0150">Furthermore, the session master may control the look and feel of drawing annotations, such as assigning particular colors to identify particular users for input cursor representations and/or for drawing annotations, whether users may select colors for effect rather than identification, whether a user identifier is displayed in connection with a drawing annotation, what drawing tools are available for the session, etc.</p><p id="p-0147" num="0151">According to some embodiments, permission from a session master is required for a session participant to perform one or more actions, such as actions that affect video playback, add an annotation to the video data, or enter or exit a particular mode (i.e., stealth mode, drawing suppression mode, synchronous mode, asynchronous mode). For example, VCS <b>112</b> receives a notification of an action that requires session master permission, and in response, requests permission to follow the instruction from the session master. VCS <b>112</b> performs the action (and performs any other tasks for the action, such as storing an instruction in session data, or sending co-presence information describing the action, etc.) in response to receiving permission from the session master. If permission is denied, VCS <b>112</b> does not perform the action and notifies the requesting client application <b>132</b> that permission was denied, where client application <b>132</b> is configured to display this information in GUI <b>200</b>.</p><p id="p-0148" num="0152">Furthermore, a master of the session may cause one or more freeze conditions to be lifted based on an explicit lift of the freeze conditions or on an implicit lift of the freeze conditions. For example, a master of the session may have access to a list of active freeze conditions associated with options to explicitly lift any of the active freeze conditions. As another example, a master of the session may have, in their GUI <b>200</b>, a master unfreeze control that lifts all active freeze conditions. As yet another example, a master of the session may implicitly instruct that all active freeze conditions be lifted by activating one or more of video playback controls <b>212</b>.</p><heading id="h-0014" level="1">9. Computing System Implementation</heading><p id="p-0149" num="0153"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram that depicts an example network arrangement <b>100</b> for efficient communication among client devices during a video collaboration session. Network arrangement <b>100</b> includes a server device <b>110</b> and client devices <b>130</b>A-N communicatively coupled via a network <b>120</b>. While embodiments are described herein in connection with example network arrangement <b>100</b>, they are not limited thereto. For example, embodiments may be implemented by a peer-to-peer system with the functionality of VCS <b>112</b> described herein being implemented at the peer level.</p><p id="p-0150" num="0154">Client devices <b>130</b>A-N may be implemented by any type of computing device that is capable of communicating with server device <b>110</b> over network <b>120</b>. Example implementations of client devices <b>130</b>A-N include, without limitation, workstations, personal computers, laptop computers, personal digital assistants (PDAs), tablet computers, cellular telephony devices such as smart phones, and any other type of computing device. In network arrangement <b>100</b>, each of client devices <b>130</b>A-N (also referred to generally as client device <b>130</b>) is configured with a client application <b>132</b>. Each of client applications <b>132</b>A-N may be implemented in any number of ways, including as a stand-alone application running on client device <b>130</b>, as a plugin to a browser running at client device <b>130</b>, as a webpage loaded into a browser running at client device <b>130</b>, etc. Client devices <b>130</b>A-N may be configured with other mechanisms, processes, and functionalities, depending upon a particular implementation.</p><p id="p-0151" num="0155">Network <b>120</b> may be implemented with any type of medium and/or mechanism that facilitates the exchange of information between client devices <b>130</b>A-N and server device <b>110</b>. Furthermore, network <b>120</b> may facilitate use of any type of communications protocol, and may be secured or unsecured, depending upon the requirements of a particular embodiment.</p><p id="p-0152" num="0156">Server device <b>110</b> may be implemented by any type of computing device that is capable of communicating with client devices <b>130</b>A-N over network <b>120</b>. In network arrangement <b>100</b>, server device <b>110</b> is configured with VCS <b>112</b>. Any of the functionality attributed to VCS <b>112</b> herein may be performed by another entity running on server device <b>110</b>, or by another entity on client device <b>130</b> or on other devices that are communicatively coupled to network <b>120</b> (not depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>), according to some embodiments. Server device <b>110</b> may be configured with other mechanisms, hardware, processes, and functionalities, depending upon a particular implementation.</p><p id="p-0153" num="0157">Server device <b>110</b> is communicatively coupled to a data store <b>140</b>. Data store <b>140</b> maintains information for session data <b>142</b> and session data <b>144</b>. Data store <b>140</b> may reside (in whole or in part) in any type of storage, including volatile and non-volatile storage (e.g., random access memory (RAM), a removable or disk drive, main memory, etc.), and may be implemented by one or more databases. The storage on which data store <b>140</b> resides may be external or internal to server device <b>110</b>.</p><heading id="h-0015" level="1">10. Example Content Management System</heading><p id="p-0154" num="0158">With respect to implementing various embodiments of the disclosed technology, an example system configuration <b>800</b> is shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, wherein electronic devices communicate via a network for purposes of exchanging content and other data. The system can be configured for use on a wide area network such as that illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. However, the present principles are applicable to a wide variety of network configurations that facilitate the intercommunication of electronic devices. For example, each of the components of system <b>800</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref> can be implemented in a localized or distributed fashion in a network.</p><p id="p-0155" num="0159">In system <b>800</b>, an end-user can interact with content management system <b>806</b> (e.g., an online synchronized content management system) through client devices <b>130</b>A-N (collectively &#x201c;<b>130</b>&#x201d;) connected to network <b>120</b> by direct and/or indirect communication. Content management system <b>806</b> can include a single computing device (e.g., a server) or multiple computing devices (e.g., multiple servers) that are configured to perform the functions and/or operations necessary to provide the services described herein.</p><p id="p-0156" num="0160">Content management system <b>806</b> can support connections from a variety of different client devices, such as: desktop computers; mobile computers; mobile communications devices, e.g., mobile phones, smart phones, tablets; smart televisions; set-top boxes; and/or any other network enabled computing devices.</p><p id="p-0157" num="0161">Client devices <b>130</b> can be of varying type, capabilities, operating systems, etc. Furthermore, content management system <b>806</b> can concurrently accept connections from and interact with multiple client devices <b>130</b>.</p><p id="p-0158" num="0162">An end-user can interact with content management system <b>806</b> via a client-side application installed on client device <b>130</b>X, such as client application <b>132</b>. In some embodiments, the client-side application can include a content management system specific component. For example, the component can be a stand-alone application, one or more application plug-ins, and/or a browser extension. However, the end-user can also interact with content management system <b>806</b> via a third-party application, such as a web browser, that resides on client device <b>130</b>X and is configured to communicate with content management system <b>806</b>. In either case, the client-side application can present a user interface (UI) for the end-user to interact with content management system <b>806</b>. For example, the end-user can interact with the content management system <b>806</b> via a client-side application integrated with the file system or via a webpage displayed using a web browser application. Furthermore, the client-side application can interact with content management system <b>806</b> implicitly, as described above in connection with joint state data <b>134</b>.</p><p id="p-0159" num="0163">Content management system <b>806</b> can enable an application or end-user to store content items, as well as perform a variety of content management tasks, such as retrieve, modify, browse, and/or share the content items. Furthermore, content management system <b>806</b> can enable an end-user or application to access the content from multiple client devices <b>130</b>. For example, client device <b>130</b>X can upload content, as described above, to content management system <b>806</b> via network <b>120</b>. Later, the same client device <b>130</b>X or some other client device <b>130</b>Y can retrieve the content from content management system <b>806</b>. For example, the retrieved content may be an update to joint state data <b>134</b>A.</p><p id="p-0160" num="0164">To facilitate the various content management services, an end-user can create an account with content management system <b>806</b>. User account database <b>850</b> can maintain the account information. User account database <b>850</b> can store profile information for registered users. In some cases, the only personal information in the user profile can be a username and/or email address. However, content management system <b>806</b> can also be configured to accept additional user information such as birthday, address, billing information, etc.</p><p id="p-0161" num="0165">User account database <b>850</b> can include account management information, such as account type (e.g., free or paid), usage information, (e.g. file edit history), maximum storage space authorized, storage space used, content storage locations, security settings, personal configuration settings, content sharing data, etc. Account management module <b>824</b> can be configured to update and/or obtain user account details in user account database <b>850</b>. The account management module <b>824</b> can be configured to interact with any number of other modules in content management system <b>806</b>.</p><p id="p-0162" num="0166">An account can be used to store content items, such as digital data, documents, text files, audio files, video files, etc., from one or more client devices <b>130</b> authorized on the account. The content items can also include collections for grouping content items together with different behaviors, such as folders, playlists, albums, etc. For example, an account can include a public folder that is accessible to any user. The public folder can be assigned a web-accessible address. A link to the web-accessible address can be used to access the contents of the public folder. In another example, an account can include: a photos collection that is intended for photos and that provides specific attributes and actions tailored for photos; an audio collection that provides the ability to play back audio files and perform other audio related actions; or other special purpose collection. An account can also include shared collections or group collections that are linked with and available to multiple user accounts. The permissions for multiple users may be different for a shared collection.</p><p id="p-0163" num="0167">The content items can be stored in content storage <b>860</b>. Content storage <b>860</b> can be a storage device, multiple storage devices, or a server. Alternatively, content storage <b>860</b> can be a cloud storage provider or network storage accessible via one or more communications networks. Content management system <b>806</b> can hide the complexity and details from client devices <b>130</b> so that client devices <b>130</b> do not need to know exactly where or how the content items are being stored by content management system <b>806</b>. In some embodiments, content management system <b>806</b> can store the content items in the same collection hierarchy as they appear on client device <b>130</b>X. However, content management system <b>806</b> can store the content items in its own order, arrangement, or hierarchy. Content management system <b>806</b> can store the content items in a network accessible storage (NAS) device, in a redundant array of independent disks (RAID), etc. Content storage <b>860</b> can store content items using one or more partition types, such as FAT, FAT32, NTFS, EXT2, EXT3, EXT4, HFS/HFS+, BTRFS, and so forth.</p><p id="p-0164" num="0168">Content storage <b>860</b> can also store metadata describing content items, content item types, and the relationship of content items to various accounts, collections, or groups. The metadata for a content item can be stored as part of the content item or can be stored separately. In one variation, each content item stored in content storage <b>860</b> can be assigned a system-wide unique identifier.</p><p id="p-0165" num="0169">Content storage <b>860</b> can decrease the amount of storage space required by identifying duplicate content items or duplicate segments of content items. Instead of storing multiple copies, content storage <b>860</b> can store a single copy and then use a pointer or other mechanism to link the duplicates to the single copy. Similarly, content storage <b>860</b> can store content items more efficiently, as well as provide the ability to undo operations, by using a content item version control that tracks changes to content items, different versions of content items (including diverging version trees), and a change history. The change history can include a set of changes that, when applied to the original content item version, produce the changed content item version.</p><p id="p-0166" num="0170">Content management system <b>806</b> can be configured to support automatic synchronization of content items from one or more client devices <b>130</b>. The synchronization can be platform agnostic. That is, the content items can be synchronized across multiple client devices <b>130</b> of varying type, capabilities, operating systems, etc. For example, client device <b>130</b>X can include client software, which synchronizes, via a synchronization module <b>832</b> at content management system <b>806</b>, content in client device <b>130</b>X's file system with the content in an associated user account. In some cases, the client software can synchronize any changes to content in a designated collection and its sub-collections, such as new, deleted, modified, copied, or moved content items or collections. The client software can be a separate software application, can integrate with an existing content management application in the operating system, or some combination thereof. In one example of client software that integrates with an existing content management application, an end-user can manipulate content items directly in a local collection, while a background process monitors the local collection for changes and synchronizes those changes to content management system <b>806</b>. Conversely, the background process can identify content items that have been updated at content management system <b>806</b> and synchronize those changes to the local collection. The client software can provide notifications of synchronization operations, and can provide indications of content statuses directly within the content management application. Sometimes client device <b>130</b>X may not have a network connection available. In this scenario, the client software can monitor the linked collection for content item changes and queue those changes for later synchronization to content management system <b>806</b> when a network connection is available. Similarly, an end-user can manually start, stop, pause, or resume synchronization with content management system <b>806</b>.</p><p id="p-0167" num="0171">An end-user can view or manipulate content via a web interface generated and served by user interface module <b>822</b>. For example, the end-user can navigate in a web browser to a web address provided by content management system <b>806</b>. Changes or updates to content in the content storage <b>860</b> made through the web interface, such as uploading a new version of a content item, can be propagated back to other client devices <b>130</b> associated with the end-user's account. For example, multiple client devices <b>130</b>, each with their own client software, can be associated with a single account and content items in the account can be synchronized between each of the multiple client devices <b>130</b>.</p><p id="p-0168" num="0172">Content management system <b>806</b> can include a communications interface <b>820</b> for interfacing with various client devices <b>130</b>, and can interact with other content and/or service providers <b>809</b>-<b>1</b>, <b>809</b>-<b>2</b>, . . . , <b>809</b>-N (collectively &#x201c;<b>1009</b>&#x201d;) via an Application Program Interface (API). Certain software applications can access content storage <b>860</b> via an API on behalf of an end-user. For example, a software package, such as an app running on a smartphone or tablet computing device, can programmatically make calls directly to content management system <b>806</b>, when an end-user provides credentials, to read, write, create, delete, share, or otherwise manipulate content. Similarly, the API can allow users to access all or part of content storage <b>860</b> through a web site.</p><p id="p-0169" num="0173">Content management system <b>806</b> can also include authenticator module <b>826</b>, which can verify user credentials, security tokens, API calls, specific client devices, and so forth, to ensure only authorized clients and users can access content items. Further, content management system <b>806</b> can include analytics module <b>834</b> module that can track and report on aggregate file operations, user actions, network usage, total storage space used, as well as other technology, usage, or business metrics. A privacy and/or security policy can prevent unauthorized access to user data stored with content management system <b>806</b>.</p><p id="p-0170" num="0174">Content management system <b>806</b> can include sharing module <b>830</b> for managing sharing content publicly or privately. Sharing content publicly can include making the content item accessible from any computing device in network communication with content management system <b>806</b>. Sharing content privately can include linking a content item in content storage <b>860</b> with two or more user accounts so that each user account has access to the content item. The sharing can be performed in a platform agnostic manner. That is, the content can be shared across multiple client devices <b>130</b> of varying type, capabilities, operating systems, etc. The content can also be shared across varying types of user accounts.</p><p id="p-0171" num="0175">In some embodiments, content management system <b>806</b> can be configured to maintain a content directory identifying the location of each content item in content storage <b>860</b>. The content directory can include a unique content entry for each content item stored in the content storage.</p><p id="p-0172" num="0176">A content entry can include a content path that can be used to identify the location of the content item in a content management system. For example, the content path can include the name of the content item and a folder hierarchy associated with the content item. For example, the content path can include a folder or path of folders in which the content item is placed as well as the name of the content item. Content management system <b>806</b> can use the content path to present the content items in the appropriate folder hierarchy.</p><p id="p-0173" num="0177">A content entry can also include a content pointer that identifies the location of the content item in content storage <b>860</b>. For example, the content pointer can include the exact storage address of the content item in memory. In some embodiments, the content pointer can point to multiple locations, each of which contains a portion of the content item.</p><p id="p-0174" num="0178">In addition to a content path and content pointer, a content entry can also include a user account identifier that identifies the user account that has access to the content item. In some embodiments, multiple user account identifiers can be associated with a single content entry indicating that the content item has shared access by the multiple user accounts.</p><p id="p-0175" num="0179">To share a content item privately, sharing module <b>830</b> can be configured to add a user account identifier to the content entry associated with the content item, thus granting the added user account access to the content item. Sharing module <b>830</b> can also be configured to remove user account identifiers from a content entry to restrict a user account's access to the content item.</p><p id="p-0176" num="0180">To share content publicly, sharing module <b>830</b> can be configured to generate a custom network address, such as a uniform resource locator (URL), which allows any web browser to access the content in content management system <b>806</b> without any authentication. To accomplish this, sharing module <b>830</b> can be configured to include content identification data in the generated URL, which can later be used to properly identify and return the requested content item. For example, sharing module <b>830</b> can be configured to include the user account identifier and the content path in the generated URL. Upon selection of the URL, the content identification data included in the URL can be transmitted to content management system <b>806</b> which can use the received content identification data to identify the appropriate content entry and return the content item associated with the content entry.</p><p id="p-0177" num="0181">In addition to generating the URL, sharing module <b>830</b> can also be configured to record that a URL to the content item has been created. In some embodiments, the content entry associated with a content item can include a URL flag indicating whether a URL to the content item has been created. For example, the URL flag can be a Boolean value initially set to 0 or false to indicate that a URL to the content item has not been created. Sharing module <b>830</b> can be configured to change the value of the flag to 1 or true after generating a URL to the content item.</p><p id="p-0178" num="0182">In some embodiments, sharing module <b>830</b> can also be configured to deactivate a generated URL. For example, each content entry can also include a URL active flag indicating whether the content should be returned in response to a request from the generated URL. For example, sharing module <b>830</b> can be configured to only return a content item requested by a generated link if the URL active flag is set to 1 or true. Thus, access to a content item for which a URL has been generated can be easily restricted by changing the value of the URL active flag. This allows an end-user to restrict access to the shared content item without having to move the content item or delete the generated URL. Likewise, sharing module <b>830</b> can reactivate the URL by again changing the value of the URL active flag to 1 or true. An end-user can thus easily restore access to the content item without the need to generate a new URL.</p><p id="p-0179" num="0183">While content management system <b>806</b> is presented with specific components, it should be understood by one skilled in the art, that the architectural configuration of system <b>806</b> is simply one possible configuration and that other configurations with more or fewer components are possible.</p><heading id="h-0016" level="1">11. Hardware Overview</heading><p id="p-0180" num="0184">A process, service, or application comprises a combination of the software and allocation of resources from a machine node. Specifically, a process, service, or application is a combination of integrated software components and an allocation of computational resources, such as memory, a machine node (i.e., a computing device and/or memory accessible to the computing device), and/or sub-processes on the machine node for executing the integrated software components on a processor, the combination of the software and computational resources being dedicated to performing a particular function, e.g., on behalf of one or more clients.</p><p id="p-0181" num="0185">One or more of the functions attributed to any process described herein, may be performed by any other logical entity that may or may not be depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, according to one or more embodiments. In some embodiments, each of the techniques and/or functionality described herein is performed automatically and may be implemented using one or more computer programs, other software elements, and/or digital logic in any of a general-purpose computer or a special-purpose computer, while performing data retrieval, transformation, and storage operations that involve interacting with and transforming the physical state of memory of the computer.</p><p id="p-0182" num="0186">According to one embodiment, the techniques described herein are implemented by one or more special-purpose computing devices. The special-purpose computing devices may be hard-wired to perform the techniques, or may include digital electronic devices such as one or more application-specific integrated circuits (ASICs) or field programmable gate arrays (FPGAs) that are persistently programmed to perform the techniques, or may include one or more general purpose hardware processors programmed to perform the techniques pursuant to program instructions in firmware, memory, other storage, or a combination. Such special-purpose computing devices may also combine custom hard-wired logic, ASICs, or FPGAs with custom programming to accomplish the techniques. The special-purpose computing devices may be desktop computer systems, portable computer systems, handheld devices, networking devices or any other device that incorporates hard-wired and/or program logic to implement the techniques.</p><p id="p-0183" num="0187">For example, <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram that illustrates a computer system <b>900</b> upon which some embodiments of the invention may be implemented. Computer system <b>900</b> includes a bus <b>902</b> or other communication mechanism for communicating information, and a hardware processor <b>904</b> coupled with bus <b>902</b> for processing information. Hardware processor <b>904</b> may be, for example, a general-purpose microprocessor.</p><p id="p-0184" num="0188">Computer system <b>900</b> also includes a main memory <b>906</b>, such as a random access memory (RAM) or other dynamic storage device, coupled to bus <b>902</b> for storing information and instructions to be executed by processor <b>904</b>. Main memory <b>906</b> also may be used for storing temporary variables or other intermediate information during execution of instructions to be executed by processor <b>904</b>. Such instructions, when stored in non-transitory storage media accessible to processor <b>904</b>, render computer system <b>900</b> into a special-purpose machine that is customized to perform the operations specified in the instructions.</p><p id="p-0185" num="0189">Computer system <b>900</b> further includes a read only memory (ROM) <b>908</b> or other static storage device coupled to bus <b>902</b> for storing static information and instructions for processor <b>904</b>. A storage device <b>910</b>, such as a magnetic disk, optical disk, or solid-state drive is provided and coupled to bus <b>902</b> for storing information and instructions.</p><p id="p-0186" num="0190">Computer system <b>900</b> may be coupled via bus <b>902</b> to a display <b>912</b>, such as a cathode ray tube (CRT), for displaying information to a computer user. An input device <b>914</b>, including alphanumeric and other keys, is coupled to bus <b>902</b> for communicating information and command selections to processor <b>904</b>. Another type of user input device is cursor control <b>916</b>, such as a mouse, a trackball, or cursor direction keys for communicating direction information and command selections to processor <b>904</b> and for controlling cursor movement on display <b>912</b>. This input device typically has two degrees of freedom in two axes, a first axis (e.g., x) and a second axis (e.g., y), that allows the device to specify positions in a plane.</p><p id="p-0187" num="0191">Computer system <b>900</b> may implement the techniques described herein using customized hard-wired logic, one or more ASICs or FPGAs, firmware and/or program logic which in combination with the computer system causes or programs computer system <b>900</b> to be a special-purpose machine. According to one embodiment, the techniques herein are performed by computer system <b>900</b> in response to processor <b>904</b> executing one or more sequences of one or more instructions contained in main memory <b>906</b>. Such instructions may be read into main memory <b>906</b> from another storage medium, such as storage device <b>910</b>. Execution of the sequences of instructions contained in main memory <b>906</b> causes processor <b>904</b> to perform the process steps described herein. In alternative embodiments, hard-wired circuitry may be used in place of or in combination with software instructions.</p><p id="p-0188" num="0192">The term &#x201c;storage media&#x201d; as used herein refers to any non-transitory media that store data and/or instructions that cause a machine to operate in a specific fashion. Such storage media may comprise non-volatile media and/or volatile media. Non-volatile media includes, for example, optical disks, magnetic disks, or solid-state drives, such as storage device <b>910</b>. Volatile media includes dynamic memory, such as main memory <b>906</b>. Common forms of storage media include, for example, a floppy disk, a flexible disk, hard disk, solid-state drive, magnetic tape, or any other magnetic data storage medium, a CD-ROM, any other optical data storage medium, any physical medium with patterns of holes, a RAM, a PROM, and EPROM, a FLASH-EPROM, NVRAM, any other memory chip or cartridge.</p><p id="p-0189" num="0193">Storage media is distinct from but may be used in conjunction with transmission media. Transmission media participates in transferring information between storage media. For example, transmission media includes coaxial cables, copper wire and fiber optics, including the wires that comprise bus <b>902</b>. Transmission media can also take the form of acoustic or light waves, such as those generated during radio-wave and infra-red data communications.</p><p id="p-0190" num="0194">Various forms of media may be involved in carrying one or more sequences of one or more instructions to processor <b>904</b> for execution. For example, the instructions may initially be carried on a magnetic disk or solid-state drive of a remote computer. The remote computer can load the instructions into its dynamic memory and send the instructions over a telephone line using a modem. A modem local to computer system <b>900</b> can receive the data on the telephone line and use an infra-red transmitter to convert the data to an infra-red signal. An infra-red detector can receive the data carried in the infra-red signal and appropriate circuitry can place the data on bus <b>902</b>. Bus <b>902</b> carries the data to main memory <b>906</b>, from which processor <b>904</b> retrieves and executes the instructions. The instructions received by main memory <b>906</b> may optionally be stored on storage device <b>910</b> either before or after execution by processor <b>904</b>.</p><p id="p-0191" num="0195">Computer system <b>900</b> also includes a communication interface <b>918</b> coupled to bus <b>902</b>. Communication interface <b>918</b> provides a two-way data communication coupling to a network link <b>920</b> that is connected to a local network <b>922</b>. For example, communication interface <b>918</b> may be an integrated services digital network (ISDN) card, cable modem, satellite modem, or a modem to provide a data communication connection to a corresponding type of telephone line. As another example, communication interface <b>918</b> may be a local area network (LAN) card to provide a data communication connection to a compatible LAN. Wireless links may also be implemented. In any such implementation, communication interface <b>918</b> sends and receives electrical, electromagnetic, or optical signals that carry digital data streams representing various types of information.</p><p id="p-0192" num="0196">Network link <b>920</b> typically provides data communication through one or more networks to other data devices. For example, network link <b>920</b> may provide a connection through local network <b>922</b> to a host computer <b>924</b> or to data equipment operated by an Internet Service Provider (ISP) <b>926</b>. ISP <b>926</b> in turn provides data communication services through the worldwide packet data communication network now commonly referred to as the &#x201c;Internet&#x201d; <b>928</b>. Local network <b>922</b> and Internet <b>928</b> both use electrical, electromagnetic, or optical signals that carry digital data streams. The signals through the various networks and the signals on network link <b>920</b> and through communication interface <b>918</b>, which carry the digital data to and from computer system <b>900</b>, are example forms of transmission media.</p><p id="p-0193" num="0197">Computer system <b>900</b> can send messages and receive data, including program code, through the network(s), network link <b>920</b> and communication interface <b>918</b>. In the Internet example, a server <b>930</b> might transmit a requested code for an application program through Internet <b>928</b>, ISP <b>926</b>, local network <b>922</b> and communication interface <b>918</b>.</p><p id="p-0194" num="0198">The received code may be executed by processor <b>904</b> as it is received, and/or stored in storage device <b>910</b>, or other non-volatile storage for later execution.</p><heading id="h-0017" level="1">12. Conclusion</heading><p id="p-0195" num="0199">Reference herein to &#x201c;some embodiments&#x201d; or &#x201c;some embodiments&#x201d; means that a particular feature, structure, or characteristic is described in connection with and can be included in at least one embodiment of the invention. The appearances of the phrase &#x201c;in some embodiments&#x201d; in various places in the specification are not necessarily all referring to the same embodiment or embodiments, nor are separate or alternative embodiments mutually exclusive of other embodiments.</p><p id="p-0196" num="0200">Although some of various drawings illustrate a number of logical stages in a particular order, stages that are not order dependent may be reordered and other stages may be combined or broken out. While some reordering or other groupings are specifically mentioned, the ordering and groupings presented herein are not an exhaustive list of alternatives.</p><p id="p-0197" num="0201">In the foregoing detailed description and in the appended claims, although the terms first, second, etc. are, in some instances, used herein to describe various elements, these elements should not be limited by these terms. These terms are only used to distinguish one element from another. For example, a first computing device could be termed a second computing device, and, similarly, a second computing device could be termed a first computing device. The first computing device and the second computing device are both computing devices, but they are not the same computing device.</p><p id="p-0198" num="0202">As used in the foregoing detailed description and in the appended claims, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. As used in the foregoing detailed description and in the appended claims, the term &#x201c;and/or&#x201d; refers to and encompasses any and all possible combinations of one or more of the associated listed items.</p><p id="p-0199" num="0203">As used in the foregoing detailed description in the appended claims, the terms &#x201c;based on,&#x201d; &#x201c;according to,&#x201d; &#x201c;includes,&#x201d; &#x201c;including,&#x201d; &#x201c;comprises,&#x201d; and/or &#x201c;comprising,&#x201d; specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.</p><p id="p-0200" num="0204">As used in the foregoing detailed description and in the appended claims, the term &#x201c;if&#x201d; is, optionally, construed to mean &#x201c;when&#x201d; or &#x201c;upon&#x201d; or &#x201c;in response to determining&#x201d; or &#x201c;in response to detecting&#x201d; or &#x201c;in accordance with a determination that,&#x201d; depending on the context. Similarly, the phrase &#x201c;if it is determined&#x201d; or &#x201c;if [a stated condition or event] is detected&#x201d; is, optionally, construed to mean &#x201c;upon determining&#x201d; or &#x201c;in response to determining&#x201d; or &#x201c;upon detecting [the stated condition or event]&#x201d; or &#x201c;in response to detecting [the stated condition or event]&#x201d; or &#x201c;in accordance with a determination that [a stated condition or event] is detected,&#x201d; depending on the context.</p><p id="p-0201" num="0205">The foregoing detailed description, for purpose of explanation, has been described with reference to specific implementations. However, the illustrative discussions above are not intended to be exhaustive or to limit the scope of the claims to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The implementations were chosen in order to best explain the principles underlying the claims and their practical applications, to thereby enable others skilled in the art to best use the implementations with various modifications as are suited to the particular uses contemplated.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>streaming digital video data to a plurality of client devices, the video data representing a plurality of video frames;</claim-text><claim-text>from each of one or more client devices of the plurality of client devices, receiving real-time cursor data, the real-time cursor data indicating a position of an input cursor controlled at said each client device within a graphical user interface, the graphical user interface comprising a viewing area of the video data;</claim-text><claim-text>transmitting, in real time to each client device of the plurality of client devices, real-time cursor data received from the other client devices of the plurality of client devices, said each client device being configured to display, within the graphical user interface displayed by said each client device, visual representations of the input cursors controlled at the other client devices at respective locations indicated in the real-time cursor data;</claim-text><claim-text>receiving, from a particular client device of the plurality of client devices, one or more drawing instructions for recreating a particular drawing annotation performed, at the particular client device, on a particular frame of the plurality of video frames;</claim-text><claim-text>transmitting, to each client device of the plurality of client devices other than the particular client device, the one or more drawing instructions, said each client device being configured to draw, based on the one or more drawing instructions, the particular drawing annotation on the particular frame displayed at said each client device;</claim-text><claim-text>wherein the method is performed by one or more computing devices.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving, from a second client device of the plurality of client devices, second one or more drawing instructions for recreating a second drawing annotation performed, at the second client device, on the particular frame; and</claim-text><claim-text>transmitting, to each client device of the plurality of client devices other than the second client device, the second one or more drawing instructions, said each client device being configured to draw, based on the second one or more drawing instructions, the second drawing annotation on the particular frame displayed at said each client device.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising, after said transmitting the one or more drawing instructions:<claim-text>receiving second one or more drawing instructions, from the particular client device, for recreating the particular drawing annotation;</claim-text><claim-text>responsive to receiving the second one or more drawing instructions, transmitting, in real time to each client device of the plurality of client devices other than the particular client device, the second one or more drawing instructions.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said receiving, from the particular client device of the plurality of client devices, the one or more drawing instructions is based on a user command, to transmit the one or more drawing instructions, received at the particular client device.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>storing, in session data associated with the video data, a plurality of drawing instructions, for one or more drawing annotations, received while streaming the video data to the plurality of client devices;</claim-text><claim-text>wherein the plurality of drawing instructions comprises the one or more drawing instructions;</claim-text><claim-text>receiving a request to review the session data from a particular client device;</claim-text><claim-text>providing the session data to the particular client as a response to the request, the particular client being configured to draw, based on the plurality of drawing instructions in the session data, the one or more drawing annotations on one or more associated video frames displayed at the particular client device.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:<claim-text>storing, in the session data, a plurality of timestamps associated with the respective plurality of drawing instructions, the plurality of timestamps reflecting timing of receipt of the respective drawing instructions of the plurality of drawing instructions;</claim-text><claim-text>wherein the particular client is configured to draw the one or more drawing annotations timed according to the plurality of timestamps.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>storing, in the session data, one or more video playback instructions received from the plurality of client devices with one or more respective timestamps reflecting timing of receipt of the respective video playback instructions of the one or more video playback instructions;</claim-text><claim-text>wherein the particular client is further configured to perform the one or more video playback instructions timed according to the one or more respective timestamps.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>storing, in the session data, the real-time cursor data received from each client device of the one or more client devices;</claim-text><claim-text>wherein each cursor position datum in the session data is associated with a timestamp of receipt of said each cursor position datum;</claim-text><claim-text>wherein the particular client is further configured to display one or more input cursor representations based on the real-time cursor data in the session data.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>said streaming video data to the plurality of client devices is performed in connection with a first streaming session;</claim-text><claim-text>the method further comprises:<claim-text>storing, in first session data for the first streaming session of the video data, the one or more drawing instructions associated with the particular frame;</claim-text><claim-text>storing, in second session data for a second streaming session of the video data, second one or more drawing instructions associated with the particular frame, the second one or more drawing instructions comprising second one or more drawing instructions for recreating a second drawing annotation performed on the particular frame by a second client device;</claim-text><claim-text>after storing the first session data and the second session data:<claim-text>in connection with streaming the video data, causing display of the particular frame at a third client device;</claim-text><claim-text>transmitting, to the third client device, the one or more drawing instructions and the second one or more drawing instructions, said third client device being configured to draw, based on the one or more drawing instructions and the second one or more drawing instructions, the particular drawing annotation and the second drawing annotation on the particular frame displayed at the third client device.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the visual representations of the input cursors displayed at the plurality of client devices are visually associated, respectively, with information identifying respective users associated with the input cursors.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the visual representation of each input cursor, displayed at the plurality of client devices, has a distinct color; and</claim-text><claim-text>the particular drawing annotation displayed at each client device, of the plurality of client devices, is the color of a particular input cursor being controlled at the particular client device.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. One or more non-transitory computer-readable media storing one or more sequences of instructions that, when executed by one or more processors, cause:<claim-text>streaming video data to a plurality of client devices, the video data representing a plurality of video frames;</claim-text><claim-text>from each of one or more client devices of the plurality of client devices, receiving real-time cursor data, the real-time cursor data indicating a position of an input cursor controlled at said each client device within a graphical user interface, the graphical user interface comprising a viewing area of the video data;</claim-text><claim-text>transmitting, in real time to each client device of the plurality of client devices, real-time cursor data received from the other client devices of the plurality of client devices, said each client device being configured to display, within the graphical user interface displayed by said each client device, visual representations of the input cursors controlled at the other client devices at respective locations indicated in the real-time cursor data;</claim-text><claim-text>receiving, from a particular client device of the plurality of client devices, user interaction information reflecting a user interaction with the graphical user interface displayed by the particular client device; and</claim-text><claim-text>in response to receiving the user interaction information, transmitting, to each client device of the plurality of client devices other than the particular client device, the user interaction information, said each client device being configured to display, in real-time, a visual representation of the user interaction information.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The one or more non-transitory computer-readable media of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein:<claim-text>the user interaction information identifies a particular control in the graphical user interface displayed by the particular client device;</claim-text><claim-text>for each client device of the plurality of client devices other than the particular client device, the visual representation of the user interaction information comprises a visual highlight of a corresponding graphical interface control.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The one or more non-transitory computer-readable media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the user interaction information indicates that an input cursor at the particular client device rolled over the particular control.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The one or more non-transitory computer-readable media of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein:<claim-text>the user interaction information identifies a particular action performed at the particular client device;</claim-text><claim-text>for each client device of the plurality of client devices other than the particular client device, the visual representation of the user interaction information comprises text data that indicates the particular action and a user identifier associated with the particular client device.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The one or more non-transitory computer-readable media of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein:<claim-text>the user interaction with the graphical user interface displayed by the particular client device comprises an instruction to:<claim-text>affect video playback,</claim-text><claim-text>add an annotation to the video data,</claim-text><claim-text>enter/exit stealth mode,</claim-text><claim-text>enter/exit asynchronous mode, or</claim-text><claim-text>enter/exit drawing suppression mode;</claim-text></claim-text><claim-text>wherein the one or more sequences of instructions further comprise instructions which, when executed by one or more processors, cause, in response to receiving the user interaction information:<claim-text>requesting permission to follow the instruction from a session master;</claim-text><claim-text>receiving, from the session master, permission to follow the instruction;</claim-text><claim-text>wherein said transmitting the user interaction information is performed in response to receiving permission to follow the instruction.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A computing system comprising:<claim-text>one or more processors;</claim-text><claim-text>one or more non-transitory computer-readable media; and</claim-text><claim-text>one or more sequences of instructions stored in the one or more non-transitory computer-readable media which, when executed by the one or more processors, cause:<claim-text>streaming video data to a plurality of client devices, the video data representing a plurality of video frames;</claim-text><claim-text>receiving, from a particular client device of the plurality of client devices, user interaction information reflecting a user interaction with a graphical user interface displayed by the particular client device; and</claim-text><claim-text>in response to receiving the user interaction information, transmitting, to each client device of the plurality of client devices other than the particular client device, the user interaction information, said each client device being configured to display, in real-time, a visual representation of the user interaction information;</claim-text><claim-text>receiving, from a second client device of the plurality of client devices, one or more drawing instructions for a particular frame of the plurality of video frames, the one or more drawing instructions comprising one or more drawing instructions for recreating a particular drawing annotation performed on the particular frame in the graphical user interface displayed at the second client device;</claim-text><claim-text>transmitting, to each client device of the plurality of client devices other than the second client device, the one or more drawing instructions, said each client device being configured to draw, based on the one or more drawing instructions, the particular drawing annotation on the particular frame in the graphical user interface displayed at said each client device.</claim-text></claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computing system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the user interaction information identifies a particular control in the graphical user interface displayed by the particular client device;</claim-text><claim-text>for each client device of the plurality of client devices other than the particular client device, the visual representation of the user interaction information comprises a visual highlight of a graphical interface control.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computing system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the user interaction information identifies a particular action performed at the particular client device;</claim-text><claim-text>for each client device of the plurality of client devices other than the particular client device, the visual representation of the user interaction information comprises text data that indicates the particular action and a user identifier associated with the particular client device.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computing system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the one or more sequences of instructions further comprise instructions which, when executed by the one or more processors, cause:<claim-text>receiving, from a second client device of the plurality of client devices, second one or more drawing instructions for recreating a second drawing annotation performed, at the second client device, on the particular frame; and</claim-text><claim-text>transmitting, to each client device of the plurality of client devices other than the second client device, the second one or more drawing instructions, said each client device being configured to draw, based on the second one or more drawing instructions, the second drawing annotation on the particular frame displayed at said each client device.</claim-text></claim-text></claim></claims></us-patent-application>