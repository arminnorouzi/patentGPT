<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005206A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005206</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943668</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>006</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>147</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>1895</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHOD AND SYSTEM FOR REPRESENTING AVATAR FOLLOWING MOTION OF USER IN VIRTUAL SPACE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/KR2020/003887</doc-number><date>20200320</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17943668</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>LINE Plus Corporation</orgname><address><city>Seongnam-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>YOO</last-name><first-name>Geum Yong</first-name><address><city>Seongnam-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>KWON</last-name><first-name>Snow</first-name><address><city>Seongnam-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>HA</last-name><first-name>Hunkwang</first-name><address><city>Seongnam-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>KWON</last-name><first-name>Ohik</first-name><address><city>Seongnam-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>GWOCK</last-name><first-name>Jungnam</first-name><address><city>Seongnam-si</city><country>KR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>LINE Plus Corporation</orgname><role>03</role><address><city>Seongnam-si</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A non-transitory computer-readable recording medium storing instructions that, when executed by a processor, cause the processor to set a communication session in which a plurality of users participate through a server, generate data for a virtual space, share motion data related to motions of the plurality of users through the communication session, generate a video in which avatars following the motions of the plurality of users are represented in the virtual space, based on the motion data, and share the generated video with the plurality of users through the communication session.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="132.33mm" wi="92.96mm" file="US20230005206A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="115.49mm" wi="125.81mm" file="US20230005206A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="119.38mm" wi="109.90mm" file="US20230005206A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="93.39mm" wi="151.89mm" file="US20230005206A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="103.80mm" wi="151.89mm" file="US20230005206A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="110.57mm" wi="151.89mm" file="US20230005206A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="82.47mm" wi="151.89mm" file="US20230005206A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="125.14mm" wi="151.89mm" file="US20230005206A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="104.22mm" wi="94.06mm" file="US20230005206A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="146.13mm" wi="88.14mm" file="US20230005206A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="147.15mm" wi="95.00mm" file="US20230005206A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="138.68mm" wi="124.88mm" file="US20230005206A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="152.65mm" wi="125.05mm" file="US20230005206A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This is a bypass continuation of International Application No. PCT/KR2020/003887, filed Mar. 20, 2020, in the Korean Intellectual Property Receiving Office, the contents of which are incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">1. Field</heading><p id="p-0003" num="0002">The present disclosure relates generally to a method and system for representing an avatar following a motion of a user in a virtual space.</p><heading id="h-0004" level="1">2. Description of Related Art</heading><p id="p-0004" num="0003">An avatar refers to a character that represents an individual online and is gaining attention as an expression tool of a user for providing a realistic virtual environment through constant interaction with others in the virtual world. Such an avatar is being used in various fields, such as an advertisement, film production, game design, and teleconference.</p><p id="p-0005" num="0004">However, the related arts only provide an avatar that simply performs a motion selected by a user from among preset motions (e.g., movements and/or facial expressions of the avatar) on a service in which a plurality of participants are present and do not express avatars that follow motions of participants in real time in the service.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0006" num="0005">Provided is an avatar representation method and system that may represent avatars of participants following motions of the participants including an owner on a virtual space and may share the virtual space with the participants in real time.</p><p id="p-0007" num="0006">According to an aspect of the disclosure, a non-transitory computer-readable recording medium may store instructions that, when executed by a processor, cause the processor to set a communication session in which a plurality of users participate through a server, generate data for a virtual space, share motion data related to motions of the plurality of users through the communication session, generate a video in which avatars following the motions of the plurality of users are represented in the virtual space, based on the motion data, and share the generated video with the plurality of users through the communication session.</p><p id="p-0008" num="0007">The instructions, when executed, may cause the processor to generate the data for the virtual space by capturing an image input through a camera, and the instructions, when executed, may cause the processor to generate the video by generating the video by representing the avatars following the motions of the plurality of users on the captured image.</p><p id="p-0009" num="0008">The instructions, when executed, may cause the processor to share of the motion data related to the motions of the plurality of users by receiving the motion data in real time through the communication session based on a real-time transmission protocol, and the instructions, when executed, may cause the processor to share the generated video with the plurality of users by transmitting the generated video to terminals of the plurality of users in real time through the communication session based on the real-time transmission protocol.</p><p id="p-0010" num="0009">The server may be configured to route data transmitted between terminals of the plurality of users through the communication session.</p><p id="p-0011" num="0010">The instructions, when executed, may further cause the processor to share voices of the plurality of users through the communication session or another communication session set separate from the communication session.</p><p id="p-0012" num="0011">The motion data may include data related to at least one of poses and facial expressions of the plurality of users.</p><p id="p-0013" num="0012">A pose of each of the avatars may be configured to include a plurality of bones, and the motion data may include an index of each of the plurality of bones, rotation information of each of the plurality of bones in a three-dimensional (3D) space, position information of each of the plurality of bones in the virtual space, and information on a current tracking state of each of the plurality of bones.</p><p id="p-0014" num="0013">The motion data may include coefficient values calculated for a plurality of points predefined for a face of a person based on a face blendshape scheme.</p><p id="p-0015" num="0014">According to an aspect of the disclosure, an avatar representation method may include setting a communication session in which a plurality of users participate through a server, generating data for a virtual space, sharing motion data related to motions of the plurality of users through the communication session, generating a video in which avatars following the motions of the plurality of users are represented in the virtual space, based on the motion data, and sharing the generated video with the plurality of users through the communication session.</p><p id="p-0016" num="0015">According to an aspect of the disclosure, an avatar representation method may include setting a communication session in which a plurality of users participate through a server, receiving data for a virtual space from a terminal of a user among the plurality of users that is an owner of the virtual space, receiving motion data related to motions of the plurality of users from terminals of the plurality of users through the communication session, generating a video in which avatars following the motions of the plurality of users are represented in the virtual space, based on the motion data, and transmitting the generated video to each of the terminals of the plurality of users through the communication session.</p><p id="p-0017" num="0016">The receiving of the data for the virtual space may include receiving, as the data for the virtual space, an image captured through a camera of the terminal of the user that is the owner of the virtual space, and the generating of the video may include generating the video by representing the avatars following the motions of the plurality of users on the received image.</p><p id="p-0018" num="0017">The receiving of the motion data may include receiving the motion data from the terminals of the plurality of users in real time through the communication session based on a real-time transmission protocol, and the transmitting of the generated video may include transmitting the video generated to the terminals of the plurality of users in real time through the communication session based on the real-time transmission protocol.</p><p id="p-0019" num="0018">The method may include routing data transmission between the terminals of the plurality of users through the communication session.</p><p id="p-0020" num="0019">The method may include mixing voices received from the plurality of users through the communication session or another communication session set separate from the communication session, and providing the mixed voice to the plurality of users.</p><p id="p-0021" num="0020">The motion data may include data related to at least one of poses and facial expressions of the plurality of users.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0022" num="0021">The above and other objects, features and advantages of the present disclosure will become more apparent to those of ordinary skill in the art by describing in detail exemplary embodiments thereof with reference to the accompanying drawings, in which:</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating an example of a network environment according to an example embodiment;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of a computer device according to an example embodiment;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. <b>3</b>, <b>4</b>, <b>5</b> and <b>6</b></figref> are diagrams of examples of an avatar representation method according to an example embodiment;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram of an example of an avatar representation method according to an example embodiment;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram of an example of a bone structure of an avatar according to an example embodiment;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram of an example of selecting participants according to an example embodiment;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram of an example of displaying a mixed video according to an example embodiment;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating an example of an avatar representation method of a client according to an example embodiment; and</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart illustrating an example of an avatar representation method of a server according to an example embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0032" num="0031">Example embodiments are described in greater detail below with reference to the accompanying drawings.</p><p id="p-0033" num="0032">In the following description, like drawing reference numerals are used for like elements, even in different drawings. The matters defined in the description, such as detailed construction and elements, are provided to assist in a comprehensive understanding of the example embodiments. However, it is apparent that the example embodiments can be practiced without those specifically defined matters. Also, well-known functions or constructions are not described in detail since they would obscure the description with unnecessary detail.</p><p id="p-0034" num="0033">One or more example embodiments will be described in detail with reference to the accompanying drawings. Example embodiments, however, may be embodied in various different forms, and should not be construed as being limited to only the illustrated embodiments. Rather, the illustrated embodiments are provided as examples so that this disclosure will be thorough and complete, and will fully convey the concepts of this disclosure to those skilled in the art. Accordingly, known processes, elements, and techniques, may not be described with respect to some example embodiments. Unless otherwise noted, like reference characters denote like elements throughout the attached drawings and written description, and thus descriptions will not be repeated.</p><p id="p-0035" num="0034">Although the terms &#x201c;first,&#x201d; &#x201c;second,&#x201d; &#x201c;third,&#x201d; etc., may be used herein to describe various elements, components, regions, layers, and/or sections, these elements, components, regions, layers, and/or sections, should not be limited by these terms. These terms are only used to distinguish one element, component, region, layer, or section, from another region, layer, or section. Thus, a first element, component, region, layer, or section, discussed below may be termed a second element, component, region, layer, or section, without departing from the scope of this disclosure.</p><p id="p-0036" num="0035">Spatially relative terms, such as &#x201c;beneath,&#x201d; &#x201c;below,&#x201d; &#x201c;lower,&#x201d; &#x201c;under,&#x201d; &#x201c;above,&#x201d; &#x201c;upper,&#x201d; and the like, may be used herein for ease of description to describe one element or feature's relationship to another element(s) or feature(s) as illustrated in the figures. It will be understood that the spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. For example, if the device in the figures is turned over, elements described as &#x201c;below,&#x201d; &#x201c;beneath,&#x201d; or &#x201c;under,&#x201d; other elements or features would then be oriented &#x201c;above&#x201d; the other elements or features. Thus, the example terms &#x201c;below&#x201d; and &#x201c;under&#x201d; may encompass both an orientation of above and below. The device may be otherwise oriented (rotated 90 degrees or at other orientations) and the spatially relative descriptors used herein interpreted accordingly. In addition, when an element is referred to as being &#x201c;between&#x201d; two elements, the element may be the only element between the two elements, or one or more other intervening elements may be present.</p><p id="p-0037" num="0036">As used herein, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the,&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms &#x201c;comprises&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups, thereof. As used herein, the term &#x201c;and/or&#x201d; includes any and all combinations of one or more of the associated listed products. Expressions such as &#x201c;at least one of,&#x201d; when preceding a list of elements, modify the entire list of elements and do not modify the individual elements of the list. For example, the expression, &#x201c;at least one of a, b, and c,&#x201d; should be understood as including only a, only b, only c, both a and b, both a and c, both b and c, all of a, b, and c, or any variations of the aforementioned examples. Also, the term &#x201c;exemplary&#x201d; is intended to refer to an example or illustration.</p><p id="p-0038" num="0037">When an element is referred to as being &#x201c;on,&#x201d; &#x201c;connected to,&#x201d; &#x201c;coupled to,&#x201d; or &#x201c;adjacent to,&#x201d; another element, the element may be directly on, connected to, coupled to, or adjacent to, the other element, or one or more other intervening elements may be present. In contrast, when an element is referred to as being &#x201c;directly on,&#x201d; &#x201c;directly connected to,&#x201d; &#x201c;directly coupled to,&#x201d; or &#x201c;immediately adjacent to,&#x201d; another element there are no intervening elements present.</p><p id="p-0039" num="0038">Unless otherwise defined, all terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which example embodiments belong. Terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and/or this disclosure, and should not be interpreted in an idealized or overly formal sense unless expressly so defined herein.</p><p id="p-0040" num="0039">Example embodiments may be described with reference to acts and symbolic representations of operations (e.g., in the form of flow charts, flow diagrams, data flow diagrams, structure diagrams, block diagrams, etc.) that may be implemented in conjunction with units and/or devices discussed in more detail below. Although discussed in a particular manner, a function or operation specified in a specific block may be performed differently from the flow specified in a flowchart, flow diagram, etc. For example, functions or operations illustrated as being performed serially in two consecutive blocks may actually be performed simultaneously, or in some cases be performed in reverse order.</p><p id="p-0041" num="0040">It should be noted that these figures are intended to illustrate the general characteristics of methods and/or structure utilized in certain example embodiments and to supplement the written description provided below. These drawings are not, however, to scale and may not precisely reflect the precise structural or performance characteristics of any given embodiment, and should not be interpreted as defining or limiting the range of values or properties encompassed by example embodiments.</p><p id="p-0042" num="0041">Units and/or devices according to one or more example embodiments may be implemented using hardware and/or a combination of hardware and software. For example, hardware devices may be implemented using processing circuitry such as, but not limited to, a processor, central processing unit (CPU), a controller, an arithmetic logic unit (ALU), a digital signal processor, a microcomputer, a field programmable gate array (FPGA), a system-on-chip (SoC), a programmable logic unit, a microprocessor, or any other device capable of responding to and executing instructions in a defined manner.</p><p id="p-0043" num="0042">Software may include a computer program, program code, instructions, or some combination thereof, for independently or collectively instructing or configuring a hardware device to operate as desired. The computer program and/or program code may include program or computer-readable instructions, software components, software modules, data files, data structures, and/or the like, capable of being implemented by one or more hardware devices, such as one or more of the hardware devices mentioned above. Examples of program code include both machine code produced by a compiler and higher level program code that is executed using an interpreter.</p><p id="p-0044" num="0043">For example, when a hardware device is a computer processing device (e.g., a processor), CPU, a controller, an ALU, a digital signal processor, a microcomputer, a microprocessor, etc., the computer processing device may be configured to carry out program code by performing arithmetical, logical, and input/output operations, according to the program code. Once the program code is loaded into a computer processing device, the computer processing device may be programmed to perform the program code, thereby transforming the computer processing device into a special purpose computer processing device. In a more specific example, when the program code is loaded into a processor, the processor becomes programmed to perform the program code and operations corresponding thereto, thereby transforming the processor into a special purpose processor.</p><p id="p-0045" num="0044">Software and/or data may be embodied permanently or temporarily in any type of machine, component, physical or virtual equipment, or computer record medium or device, capable of providing instructions or data to, or being interpreted by, a hardware device. The software also may be distributed over network coupled computer systems so that the software is stored and executed in a distributed fashion. In particular, for example, software and data may be stored by one or more computer readable record mediums, including the tangible or non-transitory computer-readable storage media discussed herein.</p><p id="p-0046" num="0045">According to one or more example embodiments, computer processing devices may be described as including various functional units that perform various operations and/or functions to increase the clarity of the description. However, computer processing devices are not intended to be limited to these functional units. For example, in one or more example embodiments, the various operations and/or functions of the functional units may be performed by other ones of the functional units. Further, the computer processing devices may perform the operations and/or functions of the various functional units without sub-dividing the operations and/or functions of the computer processing units into these various functional units.</p><p id="p-0047" num="0046">Units and/or devices according to one or more example embodiments may also include one or more storage devices. The one or more storage devices may be tangible or non-transitory computer-readable storage media, such as random access memory (RAM), read only memory (ROM), a permanent mass storage device (such as a disk drive, solid state (e.g., NAND flash) device, and/or any other like data storage mechanism capable of storing and recording data. The one or more storage devices may be configured to store computer programs, program code, instructions, or some combination thereof, for one or more operating systems and/or for implementing the example embodiments described herein. The computer programs, program code, instructions, or some combination thereof, may also be loaded from a separate computer readable record medium into the one or more storage devices and/or one or more computer processing devices using a drive mechanism. Such separate computer readable record medium may include a universal serial bus (USB) flash drive, a memory stick, a Blu-ray/digital versatile disc (DVD)/compact disc (CD)-ROM drive, a memory card, and/or other like computer readable storage media. The computer programs, program code, instructions, or some combination thereof, may be loaded into the one or more storage devices and/or the one or more computer processing devices from a remote data storage device via a network interface, rather than via a local computer readable record medium. Additionally, the computer programs, program code, instructions, or some combination thereof, may be loaded into the one or more storage devices and/or the one or more processors from a remote computing system that is configured to forward and/or distribute the computer programs, program code, instructions, or some combination thereof, over a network. The remote computing system may forward and/or distribute the computer programs, program code, instructions, or some combination thereof, via a wired interface, an air interface, and/or any other like medium.</p><p id="p-0048" num="0047">The one or more hardware devices, the one or more storage devices, and/or the computer programs, program code, instructions, or some combination thereof, may be specially designed and constructed for the purposes of the example embodiments, or they may be known devices that are altered and/or modified for the purposes of example embodiments.</p><p id="p-0049" num="0048">A hardware device, such as a computer processing device, may run an operating system (OS) and one or more software applications that run on the OS. The computer processing device also may access, store, manipulate, process, and create data in response to execution of the software. For simplicity, one or more example embodiments may be exemplified as one computer processing device. However, one skilled in the art will appreciate that a hardware device may include multiple processing elements and multiple types of processing elements. For example, a hardware device may include multiple processors or a processor and a controller. In addition, other processing configurations are possible, such as parallel processors.</p><p id="p-0050" num="0049">Although described with reference to specific examples and drawings, modifications, additions and substitutions of example embodiments may be variously made according to the description by those of ordinary skill in the art. For example, the described techniques may be performed in an order different with that of the methods described, and/or components such as the described system, architecture, devices, circuit, and the like, may be connected or combined to be different from the above-described methods, or results may be appropriately achieved by other components or equivalents.</p><p id="p-0051" num="0050">Hereinafter, example embodiments will be described with reference to the accompanying drawings.</p><p id="p-0052" num="0051">An avatar representation system according to the example embodiments may include a computer device that implements at least one client and a computer device that implements at least one server, and an avatar representation method according to the example embodiments may be performed through at least one computer device included in the avatar representation system. Here, a computer program according to an example embodiment may be installed and executed on the computer device. The computer device may perform the avatar representation method according to the example embodiments under control of the executed computer program. The computer program may be stored in a non-transitory computer-readable record medium to computer-implement the avatar representation method in conjunction with the computer program. <b>100521</b> <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of a network environment according to an example embodiment. Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the network environment may include a plurality of electronic devices <b>110</b>, <b>120</b>, <b>130</b>, and <b>140</b>, a plurality of servers <b>150</b> and <b>160</b>, and a network <b>170</b>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is provided as an example only. A number of electronic devices or a number of servers is not limited thereto. Also, the network environment of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is provided as an example only among environments applicable to the example embodiments. The environments applicable to the example embodiments are not limited to the network environment of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0053" num="0052">Each of the plurality of electronic devices <b>110</b>, <b>120</b>, <b>130</b>, and <b>140</b> may be a fixed terminal or a mobile terminal that is configured as a computer device. For example, the plurality of electronic devices <b>110</b>, <b>120</b>, <b>130</b>, and <b>140</b> may be a smartphone, a mobile phone, a navigation device, a computer, a laptop computer, a digital broadcasting terminal, a personal digital assistant (PDA), a portable multimedia player (PMP), a tablet personal computer (PC), and the like. For example, although <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a shape of a smartphone as an example of the electronic device <b>110</b>, the electronic device <b>110</b> used herein may refer to one of various types of physical computer devices capable of communicating with other electronic devices <b>120</b>, <b>130</b>, and <b>140</b>, and/or the servers <b>150</b> and <b>160</b> over the network <b>170</b> in a wireless or wired communication manner.</p><p id="p-0054" num="0053">The communication scheme is not limited and may include a near field wireless communication scheme between devices as well as a communication scheme using a communication network (e.g., a mobile communication network, wired Internet, wireless Internet, a broadcasting network, etc.) includable in the network <b>170</b>. For example, the network <b>170</b> may include at least one of network topologies that include a personal area network (PAN), a local area network (LAN), a campus area network (CAN), a metropolitan area network (MAN), a wide area network (WAN), a broadband network (BBN), and the Internet. Also, the network <b>170</b> may include at least one of network topologies that include a bus network, a star network, a ring network, a mesh network, a star-bus network, a tree or hierarchical network, and the like. However, they are provided as examples only.</p><p id="p-0055" num="0054">Each of the servers <b>150</b> and <b>160</b> may be configured as a computer device or a plurality of computer devices that provides an instruction, a code, a file, content, a service, etc., through communication with the plurality of electronic devices <b>110</b>, <b>120</b>, <b>130</b>, and <b>140</b> over the network <b>170</b>. For example, the server <b>150</b> may be a system that provides a service to the plurality of electronic devices <b>110</b>, <b>120</b>, <b>130</b>, and <b>140</b> connected over the network <b>170</b>. For example, the service may include an instant messaging service, a game service, a group call service (or a voice conference service), a messaging service, a mail service, a social network service, a map service, a translation service, a financial service, a payment service, a search service, a content providing service, and the like.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating an example of a computer device according to an example embodiment. Each of the plurality of electronic devices <b>110</b>, <b>120</b>, <b>130</b>, and <b>140</b> or each of the servers <b>150</b> and <b>160</b> may be implemented by the computer device <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0057" num="0056">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the computer device <b>200</b> may include a memory <b>210</b>, a processor <b>220</b>, a communication interface <b>230</b>, and an input/output (I/O) interface <b>240</b>. The memory <b>210</b> may include a permanent mass storage device, such as a RAM, a ROM, and a disk drive, as a non-transitory computer-readable record medium. The permanent mass storage device, such as ROM and a disk drive, may be included in the computer device <b>200</b> as a permanent storage device separate from the memory <b>210</b>. Also, an OS and at least one program code may be stored in the memory <b>210</b>. Such software components may be loaded to the memory <b>210</b> from another non-transitory computer-readable record medium separate from the memory <b>210</b>. The other non-transitory computer-readable record medium may include a non-transitory computer-readable record medium, for example, a floppy drive, a disk, a tape, a DVD/CD-ROM drive, a memory card, etc. According to other example embodiments, software components may be loaded to the memory <b>210</b> through the communication interface <b>230</b>, instead of the non-transitory computer-readable record medium. For example, the software may be loaded to the memory <b>210</b> of the computer device <b>200</b> based on a computer program installed by files received over the network <b>170</b>.</p><p id="p-0058" num="0057">The processor <b>220</b> may be configured to process instructions of a computer program by performing basic arithmetic operations, logic operations, and I/O operations. The computer-readable instructions may be provided from the memory <b>210</b> or the communication interface <b>230</b> to the processor <b>220</b>. For example, the processor <b>220</b> may be configured to execute received instructions in response to the program code stored in the storage device, such as the memory <b>210</b>.</p><p id="p-0059" num="0058">The communication interface <b>230</b> may provide a function for communication between the computer device <b>200</b> and another device, for example, the aforementioned storage devices. For example, the processor <b>220</b> of the computer device <b>200</b> may forward a request or an instruction created based on a program code stored in the storage device such as the memory <b>210</b>, data, and a file, to other apparatuses over the network <b>170</b> under control of the communication interface <b>230</b>. Inversely, a signal, an instruction, data, a file, etc., from another apparatus may be received at the computer device <b>200</b> through the communication interface <b>230</b> of the computer device <b>200</b>. For example, a signal, an instruction, data, etc., received through the communication interface <b>230</b> may be forwarded to the processor <b>220</b> or the memory <b>210</b>, and a file, etc., may be stored in a storage medium, for example, the permanent storage device, further includable in the computer device <b>200</b>.</p><p id="p-0060" num="0059">The I/O interface <b>240</b> may be a device used for interfacing with an I/O device <b>250</b>. For example, an input device may include a device, such as a microphone, a keyboard, a mouse, etc., and an output device may include a device, such as a display, a speaker, etc. As another example, the I/O interface <b>240</b> may be a device for interfacing with an apparatus in which an input function and an output function are integrated into a single function, such as a touchscreen. At least one of the I/O device <b>250</b> may be configured as a single apparatus with the computer device <b>200</b>. For example, a touchscreen, a microphone, a speaker, etc., of a smartphone, may be included in the computer device <b>200</b>.</p><p id="p-0061" num="0060">According to other example embodiments, the computer device <b>200</b> may include a number of components greater than or less than a number of components shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. For example, the computer device <b>200</b> may include at least a portion of the I/O device <b>250</b>, or may further include other components, for example, a transceiver, a database (DB).</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIGS. <b>3</b>, <b>4</b>, <b>5</b> and <b>6</b></figref> are diagrams of examples of an avatar representation method according to at least one example embodiment. <figref idref="DRAWINGS">FIGS. <b>3</b> to <b>6</b></figref> illustrate an owner <b>310</b>, a second user <b>320</b>, a third user <b>330</b>, an avatar API server (AAS) <b>340</b>, and an avatar media server (AMS) <b>350</b>. The AAS <b>340</b> and the AMS <b>350</b> may be included in a single server, or may be distributed in a plurality of servers.</p><p id="p-0063" num="0062">Here, each of the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> may be a terminal as a physical device substantially used by a corresponding user to use a service. This terminal may be implemented in a form of the computer device <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. For example, the owner <b>310</b> may be implemented in a form of the computer device <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and may perform an operation for performing the avatar representation method by way of the processor <b>220</b> included in the computer device <b>200</b> under control of an application installed and running on the computer device <b>200</b>, to use a specific service. The owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> that provide the specific service through this application may be clients of the specific service.</p><p id="p-0064" num="0063">Also, each of the AAS <b>340</b> and the AMS <b>350</b> may be a software module that is implemented in individual physical devices or implemented in a single physical device. The physical device in which the AAS <b>340</b> and/or the AMS <b>350</b> are implemented may also be implemented in a form of the computer device <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The AAS <b>340</b> and the AMS <b>350</b> may be at least a portion of a server system for providing the specific service.</p><p id="p-0065" num="0064">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a preparation process <b>360</b> may include a room generation process <b>361</b>, a channel generation process <b>362</b>, a friend invitation process <b>363</b>, and invitation processes <b>364</b> and <b>365</b>.</p><p id="p-0066" num="0065">In the room generation process <b>361</b>, the owner <b>310</b> may request the AAS <b>340</b> to generate a room. For example, the room may represent a chatroom for conducting a conversation between participants based on text, audio, and/or video.</p><p id="p-0067" num="0066">In the channel generation process <b>362</b>, the AAS <b>340</b> may request the AMS <b>350</b> to generate a media channel in response to a room generation request from the owner <b>310</b>. If the room is a logical channel for the participants, the media channel may refer to an actual channel through which participant data is delivered. Here, the generated media channel may be maintained for the following voice communication process <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> and screen sharing process <b>600</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0068" num="0067">In the friend invitation process <b>363</b>, the owner <b>310</b> may request the AAS <b>340</b> to invite friends to the generated room. Here, the friends may represent other users having a personal relationship with the owner <b>310</b> in the corresponding service. The example embodiment describes an example in which the owner <b>310</b> invites the second user <b>320</b> and the third user <b>330</b>. For example, the owner <b>310</b> may request the AAS <b>340</b> to invite desired friends by selecting friends that the owner <b>310</b> desires to invite from a list of friends.</p><p id="p-0069" num="0068">In the invitation processes <b>364</b> and <b>365</b>, the AAS <b>340</b> may invite the second user <b>320</b> and the third user <b>330</b> selected as the friends of the owner <b>310</b> to the room in response to the request from the owner <b>310</b>.</p><p id="p-0070" num="0069">As described above, the preparation process <b>360</b> may be an example of a process of setting a communication session between participants of a service using the avatar representation method according to example embodiments. Although the example embodiment of <figref idref="DRAWINGS">FIG. <b>3</b></figref> describes an example of setting a chatroom, the communication session is not limited to the chatroom. Also, although three participants in the communication session are present in the preparation process <b>360</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, it will be easily understood that a number of participants in the communication session may be variously set based on a number of friends invited by the owner <b>310</b>. The number of participants may be variously set by the owner <b>310</b> within a limited number of persons set in the service.</p><p id="p-0071" num="0070">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the voice communication process <b>400</b> may include voice transmission processes <b>410</b>, <b>420</b>, and <b>430</b> and voice reception processes <b>440</b>, <b>450</b>, and <b>460</b>. This voice communication process <b>400</b> may be optionally used to enable a voice conversation between the participants. That is, the voice communication process <b>400</b> may be omitted from a service that does not provide the voice conversation between the participants.</p><p id="p-0072" num="0071">In the voice transmission processes <b>410</b>, <b>420</b>, and <b>430</b>, the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> may transmit their own voice to the AMS <b>350</b>. Here, voice transmission may be premised on a case in which the voices are recognized from the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b>. For example, unless the voice of the second user <b>320</b> is recognized, the voice transmission process <b>420</b> from the second user <b>320</b> the AMS <b>350</b> may be omitted.</p><p id="p-0073" num="0072">In the voice reception processes <b>440</b>, <b>450</b>, and <b>460</b>, each of the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> may receive the mixed voice from the AMS <b>350</b>. Here, the mixed voice may refer to an audio in which remaining voices excluding his or her own voice are mixed. For example, it is assumed that voices are simultaneously transmitted from the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> to the AMS <b>350</b>. In this case, the AMS <b>350</b> may transmit, to the third user <b>330</b>, an audio in which the voices of the owner <b>310</b> and the second user <b>320</b> are mixed, may transmit, to the second user <b>320</b>, an audio in which the voices of the owner <b>310</b> and the third user <b>330</b> are mixed, and may transmit, to the owner <b>310</b>, an audio in which the voices of the second user <b>320</b> and the third user <b>330</b> are mixed. As another example, it is assumed that the voices are simultaneously transmitted from the owner <b>310</b> and the third user <b>330</b> to the AMS <b>350</b>. In this case, the AMS <b>350</b> may transmit, to the second user <b>320</b>, an audio in which the voices of the owner <b>310</b> and the third user <b>330</b> are mixed, and may transmit, to the third user <b>330</b>, an audio in which the voice of the owner <b>310</b> is included, and may transmit, to the owner <b>310</b>, an audio in which the voice of the third user <b>330</b> is included. As another example, when only the voice of the owner <b>310</b> is transmitted to the AMS <b>350</b>, the AMS <b>350</b> may transmit an audio in which the voice of the owner <b>310</b> is included to each of the second user <b>320</b> and the third user <b>330</b>.</p><p id="p-0074" num="0073">As described above, the voice communication process <b>400</b> may be optionally used to enable the voice conversation between the participants. Also, the following avatar sharing process <b>500</b> and screen sharing process <b>600</b> may be performed in parallel with the voice communication process <b>400</b>.</p><p id="p-0075" num="0074">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the avatar sharing process <b>500</b> may include motion data transmission processes <b>510</b> and <b>520</b>, a motion data reception process <b>530</b>, and a video generation process <b>540</b>.</p><p id="p-0076" num="0075">In the motion data transmission processes <b>510</b> and <b>520</b>, the second user <b>320</b> and the third user <b>330</b> may transmit their motion data to the AAS <b>340</b>. The motion data may be acquired from an image captured through a camera in each of the second user <b>320</b> and the third user <b>330</b>. The motion data may include data related to at least one of a pose and a facial expression of a corresponding user. As another example embodiment, the motion data may include data of a motion selected by the corresponding user from among a plurality of preset motions. As still another example embodiment, the motion data may be extracted from an image or a video prestored in a terminal of a corresponding user or prestored on the web.</p><p id="p-0077" num="0076">In the motion data reception process <b>530</b>, the owner <b>310</b> may receive the motion data of the second user <b>320</b> and the third user <b>330</b> from the AAS <b>340</b>. That is, the motion data from the second user <b>320</b> and the third user <b>330</b> may be delivered to the owner <b>310</b> through the AAS <b>340</b>.</p><p id="p-0078" num="0077">In the video generation process <b>540</b>, the owner <b>310</b> may represent avatars of the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> following the motions of the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> in the virtual space of the owner <b>310</b>, based on the motion data of the second user <b>320</b> and the third user <b>330</b> and the motion data of the owner <b>310</b> and may generate a video for the virtual space in which such avatars are represented. Here, the virtual space of the owner <b>310</b> may include, for example, an augmented reality (AR) space in an image captured through the camera of the virtual space. That is, in the AR space captured through the camera by the owner <b>310</b>, avatars of the second user <b>320</b> and the third user <b>330</b> as well as the avatar of the owner <b>310</b> may be displayed and motions of the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> may be applied to the avatars in real time. In another example embodiment, the virtual space of the owner <b>310</b> may be a virtual space selected by the owner <b>310</b> from among pre-generated virtual spaces. In still another example embodiment, the virtual space of the owner <b>310</b> may be extracted from an image or a video prestored in the terminal of the owner <b>310</b> or prestored on the web.</p><p id="p-0079" num="0078">Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the screen sharing process <b>600</b> may include a video transmission process <b>610</b> and video reception processes <b>620</b> and <b>630</b>.</p><p id="p-0080" num="0079">In the video transmission process <b>610</b>, the owner <b>310</b> may transmit, to the AMS <b>350</b>, a video in which avatars of participants are displayed on a virtual space of the owner <b>310</b>. Here, the mixed video may correspond to the video generated in the video generation process <b>540</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0081" num="0080">In the video reception processes <b>620</b> and <b>630</b>, the second user <b>320</b> and the third user <b>330</b> may receive the mixed video from the AMS <b>350</b>. That is, the video in which the avatars of the participants in the room are displayed on the virtual space of the owner <b>310</b> and, at the same time, motions of the participants are applied to the corresponding avatars in real time may be shared between the participants of the room in real time. To this end, in the voice communication process <b>400</b>, the avatar sharing process <b>500</b>, and the screen sharing process <b>600</b>, communication between the participants and the AMS <b>350</b> may be performed using a real-time transmission protocol. For example, the voice communication process <b>400</b> may be conducted using a real-time transport protocol (RTP) and the avatar sharing process <b>500</b> and the screen sharing process <b>600</b> may be performed using a real-time streaming protocol (RTSP).</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram of an example of an avatar representation method according to at least one example embodiment. The avatar representation method of <figref idref="DRAWINGS">FIG. <b>7</b></figref> may include the preparation process <b>360</b> and the voice communication process <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, and may also include a screen sharing process <b>700</b> in which the avatar sharing process <b>500</b> and the screen sharing process <b>600</b> are combined. <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates only the screen sharing process <b>700</b>.</p><p id="p-0083" num="0082">The screen sharing process <b>700</b> may include a video transmission process <b>710</b>, motion data transmission processes <b>720</b>, <b>730</b>, and <b>740</b>, a video generation process <b>750</b>, and video reception processes <b>760</b>, <b>770</b>, and <b>780</b>.</p><p id="p-0084" num="0083">In the video transmission process <b>710</b>, the owner <b>310</b> may transmit the video to the AMS <b>350</b>. Here, the transmitted video may be a video that represents a virtual space of the owner <b>310</b>. For example, when the virtual space of the owner <b>310</b> is a video captured through a camera included in a terminal of the owner <b>310</b>, the corresponding video may be transmitted to the AMS <b>350</b>.</p><p id="p-0085" num="0084">In motion data transmission processes <b>720</b>, <b>730</b>, and <b>740</b>, each of the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> may transmit the motion data to the AMS <b>350</b>. As described above, the motion data may include data related to at least one of a pose and a facial expression of a corresponding user. As another example embodiment, the motion data may include data of a motion selected by the corresponding user from among a plurality of preset motions. As still another example embodiment, the motion data may be extracted from an image or a video prestored in the terminal of the corresponding user or prestored on the web.</p><p id="p-0086" num="0085">In the video generation process <b>750</b>, the AMS <b>350</b> may generate a mixed video by mixing avatars that follow motions of the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> based on the motion data of each of the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> received by the AMS <b>350</b> in the motion data transmission processes <b>720</b>, <b>730</b>, and <b>740</b> in the virtual space of the owner <b>310</b> received by the AMS <b>350</b> through the video transmission process <b>710</b>.</p><p id="p-0087" num="0086">In the video reception processes <b>760</b>, <b>770</b>, and <b>780</b>, the owner <b>310</b>, the second user <b>320</b>, and the third user <b>330</b> may receive, from the AMS <b>350</b>, the mixed video that is generated in the video generation process <b>750</b>. Therefore, avatars of the participants in the room may be displayed on the virtual space of the owner <b>310</b> and the video in which the avatars follow motions of the corresponding participants may be shared between the participants in real time.</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram of an example of a bone structure of an avatar according to an example embodiment. Table 1 shows an example of a data structure for expressing a pose as motion data. From perspective of a single frame of video in which an avatar is represented, only a pose of the avatar may be represented in the corresponding frame and a motion of the avatar may be implemented by poses of the avatar connected through connection such frames.</p><p id="p-0089" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="77pt" align="left"/><colspec colname="2" colwidth="140pt" align="left"/><thead><row><entry namest="1" nameend="2" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry>Key</entry><entry>Description</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>Bone index</entry><entry>A number of a bone that constitutes an avatar</entry></row><row><entry>Rotation information</entry><entry>Quaternions x, y, z, and w that represent</entry></row><row><entry>(qx, qy, qz, qw)</entry><entry>rotation information in a 3D space</entry></row><row><entry>Position information</entry><entry>Position vectors x, y, and z that represent</entry></row><row><entry>(tx, ty, tz)</entry><entry>position information in a virtual space</entry></row><row><entry>Tracking state</entry><entry>A current tracking state of a bone (having</entry></row><row><entry/><entry>paused, stopped, and tracking values)</entry></row><row><entry>Number of bones</entry><entry>A number of bones that constitute an avatar</entry></row><row><entry>fps</entry><entry>A number of pieces of motion data to be</entry></row><row><entry/><entry>transmitted per second</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0090" num="0088">As described above, a pose of an avatar may be configured by including a plurality of bones, and the motion data may include an index of each of the plurality of bones, rotation information of each of the plurality of bones in a 3D space, position information of each of the plurality of bones in the virtual space, and information on a current tracking state of each of the plurality of bones.</p><p id="p-0091" num="0089">For example, with the assumption that motion data is transmitted at 10 fps, the motion data may be transmitted ten times per second. Here, each piece of motion data may include a bone index, rotation information of each bone, position information of each bone, and information on a tracking state of each bone. In the case of an avatar that includes 11 bones as in the example embodiment of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the motion data transmitted once may include 11 bone index, 11 pieces of rotation information, 11 pieces of position information, and 11 tracking states.</p><p id="p-0092" num="0090">Also, as described above, the motion data may further include data related to facial expressions of avatars as well as poses of users. To this end, the motion data may include coefficient values calculated for a plurality of points predefined for a face of a person based on a face blendshape scheme. For example, 52 facial points may be defined as the plurality of points and each of the coefficient values may be calculated to have a value between 0.0 and 1.0. For example, for a point &#x201c;eye,&#x201d; a value of 0.0 may correspond to a state in which eyes are closed and a value of 1.0 may correspond to a state in which the eyes are open as wide as possible. For motion data related to this facial expression, a number of transmissions may be determined based on set fps.</p><p id="p-0093" num="0091"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram of an example of selecting participants according to at least one example embodiment. An avatar selection screen <b>900</b> may represent an example of an interface screen displayed on a display of the terminal of the owner <b>310</b> such that the owner <b>310</b> may select participants (avatars of the participants) to be invited to a room. An application installed and running on the terminal of the owner <b>310</b> may provide a list of friends of the owner <b>310</b>, and friends selected by the owner <b>310</b> from the list of friends may be selected as participants to be invited to the room.</p><p id="p-0094" num="0092"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram of an example of displaying a mixed video according to at least one example embodiment. A video display screen <b>1000</b> may be an example of a video sharing screen displayed on a terminal display of the owner <b>310</b> or terminal displays of other participants. For example, the video display screen <b>1000</b> represents an example in which avatars <b>1020</b> of three participants including the owner <b>310</b> are represented on a virtual space <b>1010</b> acquired through a video captured through a camera included in the terminal of the owner <b>310</b>. The example displayed on the video display screen <b>1000</b> may be a single frame of the corresponding video. When a plurality of frames is sequentially displayed according to the aforementioned avatar representation method, it may be easily understood that motions of the participants may be applied to the avatars in real time.</p><p id="p-0095" num="0093"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating an example of an avatar representation method of a client according to an example embodiment. The avatar representation method according to the example embodiment may be performed by the computer device <b>200</b> that implements a client device. Here, the client device may be an entity that uses a service from a server under control of a client program installed on the client device. Also, the client program may correspond to an application for the aforementioned service. Here, the processor <b>220</b> of the computer device <b>200</b> may be implemented to execute a control instruction according to a code of at least one computer program or a code of an OS included in the memory <b>210</b>. Here, the processor <b>220</b> may control the computer device <b>200</b> to perform operations <b>1110</b> to <b>1160</b> included in the method of <figref idref="DRAWINGS">FIG. <b>11</b></figref> in response to the control instruction provided from the code stored in the computer device <b>200</b>.</p><p id="p-0096" num="0094">In operation <b>1110</b>, the computer device <b>200</b> may set a communication session in which terminals of a plurality of users participate through a server. An example of setting the communication session is described above through the preparation process <b>360</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Here, data transmitted between the terminals of the plurality of users may be routed at the server through the communication session.</p><p id="p-0097" num="0095">In operation <b>1120</b>, the computer device <b>200</b> may share voices of the plurality of users through the communication session or another communication session set separate from the communication session. For example, an example of sharing the voices of the plurality of users is described above through the voice communication process <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Operation <b>1120</b> may be performed after operation <b>1110</b> and may be performed in parallel with the following operations <b>1130</b> to <b>1160</b>. Depending on example embodiments, operation <b>1120</b> may be omitted.</p><p id="p-0098" num="0096">In operation <b>1130</b>, the computer device <b>200</b> may generate data for a virtual space. For example, the computer device <b>200</b> may generate data for the virtual space by capturing an image input through a camera included in the computer device <b>200</b>. As another example, the computer device <b>200</b> may generate data for the virtual space by selecting a specific virtual space from among pre-generated virtual spaces. As still another example, the computer device <b>200</b> may extract data for the virtual space from an image or a video prestored in a local storage of the computer device <b>200</b> or prestored on the web.</p><p id="p-0099" num="0097">In operation <b>1140</b>, the computer device <b>200</b> may share motion data related to motions of the plurality of users through the communication session. An example of sharing the motion data is described above through the avatar sharing process <b>500</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>. For example, the motion data may include data related to at least one of poses and facial expressions of the plurality of users. In detail, for example, a pose of each of the avatars may be configured by including a plurality of bones. In this case, the motion data may include an index of each of the plurality of bones, rotation information of each of the plurality of bones in a three-dimensional (3D) space, position information of each of the plurality of bones in the virtual space, and information on a current tracking state of each of the plurality of bones. As another example, the motion data may include coefficient values calculated for a plurality of points predefined for a face of a person based on a face blendshape scheme.</p><p id="p-0100" num="0098">In operation <b>1150</b>, the computer device <b>200</b> may generate a video in which avatars following the motions of the plurality of users are represented in the virtual space, based on the motion data. An example of generating the video in which the avatars are represented in the virtual space is described above through the avatar sharing process <b>500</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>. For example, the computer device <b>200</b> may generate the video by representing the avatars that follow motions of the plurality of users on an image captured through the camera.</p><p id="p-0101" num="0099">In operation <b>1160</b>, the computer device <b>200</b> may share the generated video with the plurality of users through the communication session. An example of sharing the generated video is described above through the screen sharing process <b>600</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. For example, in operation <b>1140</b>, the computer device <b>200</b> may receive the motion data in real time through the communication session using a real-time transmission protocol. In this case, in operation <b>1160</b>, the computer device <b>200</b> may transmit the video generated based on the motion data to the terminals of the plurality of users in real time through the communication session using the real-time transmission protocol. Through this, the participants of the communication session may share the virtual space in which the avatars to which the motions of the participants of the communication session are applied in real time are represented.</p><p id="p-0102" num="0100"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart illustrating an example of an avatar representation method of a server according to an example embodiment. The avatar representation method according to the example embodiment may be performed by the computer device <b>200</b> that implements the server. Here, the server may be an entity that provides a service to a plurality of client devices each in which a client program is installed. For example, the server may include the aforementioned AAS <b>340</b> and AMS <b>350</b>. Also, the client program may correspond to an application for the aforementioned service. Here, the processor <b>220</b> of the computer device <b>200</b> may be implemented to execute a control instruction according to a code of at least one computer program or a code of an OS included in the memory <b>210</b>. Here, the processor <b>220</b> may control the computer device <b>200</b> to perform operations <b>1210</b> to <b>1260</b> included in the method of <figref idref="DRAWINGS">FIG. <b>12</b></figref> according to the control instruction provided from the code stored in the computer device <b>200</b>.</p><p id="p-0103" num="0101">In operation <b>1210</b>, the computer device <b>200</b> may set a communication session in which terminals of a plurality of users participate. An example of setting the communication session is described above through the preparation process <b>360</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. To this end, the computer device <b>200</b> may route a data transmission between the terminals of the plurality of users through the communication session.</p><p id="p-0104" num="0102">In operation <b>1220</b>, the computer device <b>200</b> may mix voices received from the plurality of users through the communication session or another communication session set separate from the communication session and may provide the mixed voice to the plurality of users. For example, an example of mixing, by the AMS <b>350</b>, and providing the voices of the plurality of users is described above through the voice communication process <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Operation <b>1220</b> may be performed after operation <b>1210</b>, and may be performed in parallel with the following operations <b>1230</b> to <b>1260</b>. Depending on example embodiments, operation <b>1220</b> may be omitted.</p><p id="p-0105" num="0103">In operation <b>1230</b>, the computer device <b>200</b> may receive data for a virtual space from a terminal of a user that is an owner of the virtual space among the plurality of users. For example, the computer device <b>200</b> may receive an image captured through a camera included in the terminal of the user that is the owner of the virtual space, as data for the virtual space. As described above, the data for the virtual space may be generated through the existing image or video instead of using the image captured through the camera.</p><p id="p-0106" num="0104">In operation <b>1240</b>, the computer device <b>200</b> may receive motion data related to motions of the plurality of users from the terminals of the plurality of users through the communication session. For example, the motion data may include data related to at least one of poses and facial expressions of the plurality of users. In detail, for example, a pose of each avatar may be configured by including a plurality of bones. In this case, the motion data may include an index of each of the plurality of bones, rotation information of each of the plurality of bones in a 3D space, position information of each of the plurality of bones in the virtual space, and information on a current tracking state of each of the plurality of bones. As another example, the motion data may include coefficient values calculated for a plurality of points predefined for a face of a person based on a face blendshape scheme.</p><p id="p-0107" num="0105">In operation <b>1250</b>, the computer device <b>200</b> may generate a video in which avatars following the motions of the plurality of users are represented in the virtual space, based on the motion data. For example, the computer device <b>200</b> may generate the video by representing the avatars that follow the motions of the plurality of users on the received image.</p><p id="p-0108" num="0106">In operation <b>1260</b>, the computer device <b>200</b> may transmit the generated video to each of the terminals of the plurality of users through the communication session. An example of receiving, by the AMS <b>350</b>, the data for the virtual space and the motion data of the users and generating and transmitting the video is described above through the screen sharing process <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0109" num="0107">Here, in operation <b>1240</b>, the computer device <b>200</b> may receive the motion data from the terminals of the plurality of users in real time through the communication session using a real-time transmission protocol. In operation <b>1260</b>, the computer device <b>200</b> may transmit the video generated based on the motion data to the terminals of the plurality of users in real time through the communication session using the real-time transmission protocol. Through this, the virtual space in which the avatars to which the motions of the participants of the communication session are applied in real time are represented may be shared between the participants in real time.</p><p id="p-0110" num="0108">As described above, according to some example embodiments, it is possible to represent avatars of participants following motions of the participants including an owner on a virtual space of the owner and to share the virtual space with the participants in real time.</p><p id="p-0111" num="0109">The systems and/or apparatuses described herein may be implemented using hardware components, software components, and/or a combination thereof. For example, the apparatuses and components described herein may be implemented using one or more general-purpose or special purpose computers, for example, a processor, a controller, an ALU, a digital signal processor, a microcomputer, an FPGA, a programmable logic unit (PLU), a microprocessor, or any other device capable of responding to and executing instructions in a defined manner. A processing device may run an OS and one or more software applications that run on the OS. The processing device also may access, store, manipulate, process, and create data in response to execution of the software. For simplicity, the description of a processing device is used as singular; however, one skilled in the art will appreciate that a processing device may include multiple processing elements and/or multiple types of processing elements. For example, a processing device may include multiple processors or a processor and a controller. In addition, different processing configurations are possible, such as parallel processors.</p><p id="p-0112" num="0110">The software may include a computer program, a piece of code, an instruction, or some combinations thereof, for independently or collectively instructing or configuring the processing device to operate as desired. Software and/or data may be embodied permanently or temporarily in any type of machine, component, physical equipment, virtual equipment, a computer storage medium or device, or in a propagated signal wave capable of providing instructions or data to or being interpreted by the processing device. The software also may be distributed over network coupled computer systems so that the software is stored and executed in a distributed fashion. In particular, the software and data may be stored by one or more computer readable record mediums.</p><p id="p-0113" num="0111">The methods according to the above-described example embodiments may be configured in a form of program instructions performed through various computer devices and recorded in non-transitory computer-readable media. The media may also include, alone or in combination with the program instructions, data files, data structures, and the like. The media may continuously store computer-executable programs or may temporarily store the same for execution or download. Also, the media may be various types of recording devices or storage devices in a form in which one or a plurality of hardware components are combined. Without being limited to media directly connected to a computer system, the media may be distributed over the network. Examples of the media include magnetic media such as hard disks, floppy disks, and magnetic tapes; optical media such as CD-ROM and DVDs; magneto-optical media such as floptical disks; and hardware devices that are specially configured to store and perform program instructions, such as ROM, RAM, flash memory, and the like. Examples of other media may include record media and storage media managed by an app store that distributes applications or a site, a server, and the like that supplies and distributes other various types of software.</p><p id="p-0114" num="0112">The foregoing embodiments are merely examples and are not to be construed as limiting. The present disclosure be readily applied to other types of apparatuses. Also, the description of the exemplary embodiments is intended to be illustrative, and not to limit the scope of the claims, and many alternatives, modifications, and variations will be apparent to those skilled in the art.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A non-transitory computer-readable recording medium storing instructions that, when executed by a processor, cause the processor to:<claim-text>set a communication session in which a plurality of users participate through a server;</claim-text><claim-text>generate data for a virtual space;</claim-text><claim-text>share motion data related to motions of the plurality of users through the communication session;</claim-text><claim-text>generate a video in which avatars following the motions of the plurality of users are represented in the virtual space, based on the motion data; and</claim-text><claim-text>share the generated video with the plurality of users through the communication session.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The non-transitory computer-readable recording medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the instructions, when executed, cause the processor to generate the data for the virtual space by capturing an image input through a camera, and<claim-text>wherein the instructions, when executed, cause the processor to generate the video by generating the video by representing the avatars following the motions of the plurality of users on the captured image.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The non-transitory computer-readable recording medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the instructions, when executed, cause the processor to share of the motion data related to the motions of the plurality of users by receiving the motion data in real time through the communication session based on a real-time transmission protocol, and<claim-text>wherein the instructions, when executed, cause the processor to share the generated video with the plurality of users by transmitting the generated video to terminals of the plurality of users in real time through the communication session based on the real-time transmission protocol.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The non-transitory computer-readable recording medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the server is configured to route data transmitted between terminals of the plurality of users through the communication session.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The non-transitory computer-readable recording medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the instructions, when executed, further cause the processor to:<claim-text>share voices of the plurality of users through the communication session or another communication session set separate from the communication session.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The non-transitory computer-readable recording medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the motion data comprises data related to at least one of poses and facial expressions of the plurality of users.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The non-transitory computer-readable recording medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a pose of each of the avatars is configured to include a plurality of bones, and<claim-text>wherein the motion data comprises an index of each of the plurality of bones, rotation information of each of the plurality of bones in a three-dimensional (3D) space, position information of each of the plurality of bones in the virtual space, and information on a current tracking state of each of the plurality of bones.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The non-transitory computer-readable recording medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the motion data comprises coefficient values calculated for a plurality of points predefined for a face of a person based on a face blendshape scheme.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An avatar representation method comprising:<claim-text>setting a communication session in which a plurality of users participate through a server;</claim-text><claim-text>generating data for a virtual space;</claim-text><claim-text>sharing motion data related to motions of the plurality of users through the communication session;</claim-text><claim-text>generating a video in which avatars following the motions of the plurality of users are represented in the virtual space, based on the motion data; and</claim-text><claim-text>sharing the generated video with the plurality of users through the communication session.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An avatar representation method comprising:<claim-text>setting a communication session in which a plurality of users participate through a server;</claim-text><claim-text>receiving data for a virtual space from a terminal of a user among the plurality of users that is an owner of the virtual space;</claim-text><claim-text>receiving motion data related to motions of the plurality of users from terminals of the plurality of users through the communication session;</claim-text><claim-text>generating a video in which avatars following the motions of the plurality of users are represented in the virtual space, based on the motion data; and</claim-text><claim-text>transmitting the generated video to each of the terminals of the plurality of users through the communication session.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The avatar representation method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the receiving of the data for the virtual space comprises receiving, as the data for the virtual space, an image captured through a camera of the terminal of the user that is the owner of the virtual space, and<claim-text>wherein the generating of the video comprises generating the video by representing the avatars following the motions of the plurality of users on the received image.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The avatar representation method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the receiving of the motion data comprises receiving the motion data from the terminals of the plurality of users in real time through the communication session based on a real-time transmission protocol, and<claim-text>wherein the transmitting of the generated video comprises transmitting the video generated to the terminals of the plurality of users in real time through the communication session based on the real-time transmission protocol.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The avatar representation method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:<claim-text>routing data transmission between the terminals of the plurality of users through the communication session.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The avatar representation method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:<claim-text>mixing voices received from the plurality of users through the communication session or another communication session set separate from the communication session, and</claim-text><claim-text>providing the mixed voice to the plurality of users.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The avatar representation method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the motion data comprises data related to at least one of poses and facial expressions of the plurality of users.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A server comprising:<claim-text>at least one memory storing instructions; and</claim-text><claim-text>at least one processor configured to execute the instructions to:<claim-text>set a communication session in which a plurality of users participate;</claim-text><claim-text>generate data for a virtual space;</claim-text><claim-text>share motion data related to motions of the plurality of users through the communication session;</claim-text><claim-text>generate a video in which avatars following the motions of the plurality of users are represented in the virtual space, based on the motion data; and</claim-text><claim-text>share the generated video with the plurality of users through the communication session.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The server of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the at least one processor is configured to execute the instructions to generate the data for the virtual space by capturing an image input through a camera, and<claim-text>wherein the at least one processor is configured to execute the instructions to generate the video by generating the video by representing the avatars following the motions of the plurality of users on the captured image.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The server of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the at least one processor is configured to execute the instructions to share of the motion data related to the motions of the plurality of users by receiving the motion data in real time through the communication session based on a real-time transmission protocol, and<claim-text>wherein the at least one processor is configured to execute the instructions to share the generated video with the plurality of users by transmitting the generated video to terminals of the plurality of users in real time through the communication session based on the real-time transmission protocol.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The server of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the at least one processor is configured to execute the instructions to route data transmitted between terminals of the plurality of users through the communication session.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The server of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the at least one processor is further configured to execute the instructions to share voices of the plurality of users through the communication session or another communication session set separate from the communication session.</claim-text></claim></claims></us-patent-application>