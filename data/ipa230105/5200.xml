<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005201A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005201</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17360493</doc-number><date>20210628</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>51</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>205</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00342</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>51</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>088</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30196</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">HARMONY-AWARE HUMAN MOTION SYNTHESIS WITH MUSIC</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>TCL Research America Inc.</orgname><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WU</last-name><first-name>Xinyi</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Haohong</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method and device for harmony-aware audio-driven motion synthesis are provided. The method includes determining a plurality of testing meter units according to an input audio, each testing meter unit corresponding to an input audio sequence of the input audio, obtaining an auditory input corresponding to each testing meter unit, obtaining an initial pose of each testing meter unit as a visual input based on a visual motion sequence synthesized for a previous testing meter unit, and automatically generating a harmony-aware motion sequence corresponding to the input audio using a generator of a generative adversarial network (GAN) model. The GAN model is trained by incorporating a hybrid loss function. The hybrid loss function includes a multi-space pose loss, a harmony loss, and a GAN loss. The harmony loss is determined according to beat consistencies of audio-visual beat pairs.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="35.81mm" wi="158.75mm" file="US20230005201A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="195.75mm" wi="135.13mm" file="US20230005201A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="163.75mm" wi="135.13mm" file="US20230005201A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="189.74mm" wi="165.02mm" file="US20230005201A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="196.68mm" wi="167.47mm" file="US20230005201A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="181.19mm" wi="167.72mm" file="US20230005201A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="195.75mm" wi="166.79mm" file="US20230005201A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="194.06mm" wi="167.89mm" file="US20230005201A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="184.15mm" wi="161.29mm" file="US20230005201A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="211.92mm" wi="95.76mm" file="US20230005201A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="187.45mm" wi="142.49mm" file="US20230005201A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="198.46mm" wi="107.44mm" file="US20230005201A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="216.66mm" wi="114.05mm" file="US20230005201A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="101.68mm" wi="158.75mm" file="US20230005201A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD OF THE DISCLOSURE</heading><p id="p-0002" num="0001">The present disclosure relates to the field of image processing technologies and, more particularly, relates to a method and device for harmony-aware audio-driven motion synthesis.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Machine-based generation is widely used in the tasks of producing music videos, speech editing, and animation synthesis, where harmony represents the consistent perception of rhythms, emotions, or visual appearances in the output subjectively.</p><p id="p-0004" num="0003">As a typical problem in audio-visual cross-domain generation, the task of audio-driven motion synthesis gains much attention in character animation, video generation and choreograph. The traditional methods tackle the audio-to-visual generation by retrieving visual clips that share the feature-level similarity with the given music. Different from regular motion synthesis, when conditioned with music, people are found to be sensitive to the inharmonious synthesized motions, which damages the qualitative evaluation heavily. Harmony is considered as one of the most important factors that highly influence the quality assessment of cross-domain results. However, the feeling of harmony relies on perceptual judgement. This may be challenging to enhance the audio-visual harmony in audio-driven motion synthesis tasks.</p><p id="p-0005" num="0004">The disclosed method and system are directed to solve one or more problems set forth above and other problems.</p><heading id="h-0003" level="1">BRIEF SUMMARY OF THE DISCLOSURE</heading><p id="p-0006" num="0005">One aspect of the present disclosure provides a method for harmony-aware audio-driven motion synthesis applied to a computing device. The method includes determining a plurality of testing meter units according to an input audio, each testing meter unit corresponding to an input audio sequence of the input audio, obtaining an auditory input corresponding to each testing meter unit, obtaining an initial pose of each testing meter unit as a visual input based on a visual motion sequence synthesized for a previous testing meter unit, and automatically generating a harmony-aware motion sequence corresponding to the input audio using a generator of a generative adversarial network (GAN) model. The GAN model is trained by incorporating a hybrid loss function. The hybrid loss function includes a multi-space pose loss, a harmony loss, and a GAN loss. The harmony loss is determined according to beat consistencies of audio-visual beat pairs.</p><p id="p-0007" num="0006">Another aspect of the present disclosure provides a device for harmony-aware audio-driven motion synthesis, including a memory and a processor coupled to the memory. The processor is configured to perform a plurality of operations including determining a plurality of testing meter units according to an input audio, each testing meter unit corresponding to an input audio sequence of the input audio, obtaining an auditory input corresponding to each testing meter unit, obtaining an initial pose of each testing meter unit as a visual input based on a visual motion sequence synthesized for a previous testing meter unit, and automatically generating a harmony-aware motion sequence corresponding to the input audio using a generator of a generative adversarial network (GAN) model. The GAN model is trained by incorporating a hybrid loss function. The hybrid loss function includes a multi-space pose loss, a harmony loss, and a GAN loss. The harmony loss is determined according to beat consistencies of audio-visual beat pairs.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007">The following drawings are merely examples for illustrative purposes according to various disclosed embodiments and are not intended to limit the scope of the present disclosure.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an exemplary computing system according to some embodiments of the present disclosure.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an exemplary harmony-aware audio-driven motion synthesis process <b>200</b> according to some embodiments of the present disclosure.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrate exemplary GAN model training process <b>300</b> according to some embodiments of the present disclosure</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrate an extraction of meter unions based on obtained audio beats.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> illustrates an exemplary framework of the training process according to some embodiments of the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates an exemplary framework of the testing phase according to some embodiments of the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> illustrates an exemplary framework of the generator according to some embodiments of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b>D</figref> illustrates an exemplary framework of the cross-domain discriminator according to some embodiments of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b>E</figref> illustrates an exemplary framework of the spatial-temporal discriminator according to some embodiments of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrate an example of perceptual asynchronization between beats.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> illustrates an example comparing the frame-based visual beat detection using optical flow in prior art (left) and the skeleton-based beat extraction using motion standard derivation in prior art (right) for visual beat detection in human videos.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates the video frames and corresponding extracted skeleton poses from the test case using the model in prior art.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is an illustration of harmony distortion between onset-based audio beats and SD-driven visual beats, the beat extraction in the audio (left) and visual (right) signals.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates the asynchronization between audio-visual beats under perceptual judgements.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a relationship between joint velocity sum and evolution of human movements.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> is an illustration of the improved beat-wise synchronization based on the visual beat extraction mechanism, the beat extraction in the audio (left) and visual (right) signals.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrates the synchronization between audio-visual beats considering joint velocity.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates a percentage of how many inharmonious videos have been accurately picked up according to some embodiments of the present disclosure.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates sample results of average FID between the generated motion sequences by models in prior art and HarmoGAN model and the human ground truth according to some embodiments of the present disclosure.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates a qualitative example from the dance dataset according to some embodiments of the present disclosure.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> illustrates performance of audio-visual harmony tested on the self-created testing dataset with the ground truth from the real dancer and results for the evaluation mechanism from HarmoGAN model and its variant according to some embodiments of the present disclosure.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>14</b>B</figref> illustrates performance of audio-visual harmony tested on the self-created testing dataset with the ground truth from the real dancer and results for the hit rate of audio beats in the music sequences from HarmoGAN model and its variant according to some embodiments of the present disclosure.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>15</b>A</figref> illustrates performance of audio-visual harmony for the models of prior art and HarmoGAN model and its variant tested on the Ballroom dataset for the evaluation mechanism according to some embodiments of the present disclosure.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>15</b>B</figref> illustrates performance of audio-visual harmony for the models of prior art and HarmoGAN model and its variant tested on the Ballroom dataset for the hit rate of audio beats in the music sequences according to some embodiments of the present disclosure.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates the result of the participants agrees more with the perceptual harmony based on the 30 video pairs created by putting the dance videos from three models side by side according to some embodiments of the present disclosure.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates an example for qualitative evaluation, where the generated motion sequences are presented with the tracked audio beats to demonstrate the audio-visual harmony based on different models according to some embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0035" num="0034">Reference will now be made in detail to exemplary embodiments of the invention, which are illustrated in the accompanying drawings. Hereinafter, embodiments consistent with the disclosure will be described with reference to the drawings. Wherever possible, the same reference numbers will be used throughout the drawings to refer to the same or like parts. It is apparent that the described embodiments are some but not all of the embodiments of the present invention. Based on the disclosed embodiments, persons of ordinary skill in the art may derive other embodiments consistent with the present disclosure, all of which are within the scope of the present invention.</p><p id="p-0036" num="0035">Harmony is an essential part of artistic creation. Movie directors tend to produce appealing scenes with songs that enhance emotional expression. When musicians arrange different voice parts in a chorus, they are supposed to consider whether the combination sounds harmonious. Artists pursue harmony in their works to create the senses of beauty and comfort. Since professional skills and techniques are required to complete such creative works, to save financial cost and labor, automatic generation is gradually applied to imitate the human creation process by exploiting computational models. Similar to human work, the machine-based generation needs to obey the rule of harmony in order to produce high-quality results that satisfy human aesthetics.</p><p id="p-0037" num="0036">Handling harmony in those generative tasks means the models should put effort into controlling the consistency between multiple signals, which is shown as the alignment of features explicitly for observation or implicitly in the latent spaces. The synchronization for different signal pairs may differ in their relevance so that in the high-related pairs, correlated features are easier to be captured and aligned. In human perception, over 90 percent of sense derives from the stimulus of visual or auditory signals and they interrelate and interact with each other during brain processing.</p><p id="p-0038" num="0037">The present disclosure provides a method and device for harmony-aware audio-driven motion synthesis. The disclosed method and/or device can be applied in any proper occasions where human motion synthesis with music is desired. The disclosed harmony-aware audio-driven motion synthesis process is implemented based on a beat-oriented generative adversarial network (GAN) model with harmony-aware hybrid loss function, i.e., HarmoGAN model, which utilizes audio sequences or extracted auditory features to generate the visual motion sequences. The addition of harmony evaluation mechanism in the disclosed GAN model is verified to quantify the harmony between audio and visual sequences by analyzing beat consistency.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an exemplary computing system/device capable of implementing the disclosed harmony-aware audio-driven motion synthesis method according to some embodiments of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, computing system <b>100</b> may include a processor <b>102</b> and a storage medium <b>104</b>. According to certain embodiments, the computing system <b>100</b> may further include a display <b>106</b>, a communication module <b>108</b>, additional peripheral devices <b>112</b>, and one or more bus <b>114</b> to couple the devices together. Certain devices may be omitted and other devices may be included.</p><p id="p-0040" num="0039">Processor <b>102</b> may include any appropriate processor(s). In certain embodiments, processor <b>102</b> may include multiple cores for multi-thread or parallel processing, and/or graphics processing unit (GPU). Processor <b>102</b> may execute sequences of computer program instructions to perform various processes, such as an audio-visual harmony evaluation and harmony-aware audio-driven motion synthesis program, a GAN model training program, etc. Storage medium <b>104</b> may be a non-transitory computer-readable storage medium, and may include memory modules, such as ROM, RAM, flash memory modules, and erasable and rewritable memory, and mass storages, such as CD-ROM, U-disk, and hard disk, etc. Storage medium <b>104</b> may store computer programs for implementing various processes, when executed by processor <b>102</b>. Storage medium <b>104</b> may also include one or more databases for storing certain data such as video data, training data set, testing video data set, data of trained GAN model, and certain operations can be performed on the stored data, such as database searching and data retrieving.</p><p id="p-0041" num="0040">The communication module <b>108</b> may include network devices for establishing connections through a network. Display <b>106</b> may include any appropriate type of computer display device or electronic device display (e.g., CRT or LCD based devices, touch screens). Peripherals <b>112</b> may include additional I/O devices, such as a keyboard, a mouse, and so on.</p><p id="p-0042" num="0041">In operation, the processor <b>102</b> may be configured to execute instructions stored on the storage medium <b>104</b> and perform various operations related to a harmony-aware audio-driven motion synthesis method as detailed in the following descriptions.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an exemplary harmony-aware audio-driven motion synthesis process <b>200</b> according to some embodiments of the present disclosure. The process <b>200</b> may be implemented by a harmony-aware audio-driven motion synthesis device which can be any suitable computing device/server having one or more processors and one or more memories, such as computing system <b>100</b> (e.g., processor <b>102</b>).</p><p id="p-0044" num="0043">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, harmony-aware audio-driven motion synthesis method consistent with embodiments of the present disclosure includes following processes.</p><p id="p-0045" num="0044">At S<b>202</b>, a plurality of testing meter units are determined according to an input audio, each testing meter unit corresponding to an input audio sequence of the input audio.</p><p id="p-0046" num="0045">At S<b>204</b>, an auditory input corresponding to each testing meter unit is obtained.</p><p id="p-0047" num="0046">At S<b>206</b>, an initial pose of each testing meter unit is obtained as a visual input based on a visual motion sequence synthesized for a previous testing meter unit; and</p><p id="p-0048" num="0047">At S<b>208</b>, a harmony-aware motion sequence corresponding to the input audio is automatically generated using a generator of a GAN model. The GAN model is trained by incorporating a hybrid loss function. The hybrid loss function includes a multi-space pose loss, a harmony loss, and a GAN loss. The harmony loss is determined according to beat consistencies of audio-visual beat pairs.</p><p id="p-0049" num="0048">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, training the GAN model includes following processes.</p><p id="p-0050" num="0049">At S<b>302</b>, audio beats and audio beat strengths are obtained from a training sample audio. Each audio beat corresponds to one audio beat strength.</p><p id="p-0051" num="0050">Spectrogram analysis is widely used to obtain the audio beats B<sub>a</sub>(t) in audio processing. The spectrogram of given audio sequences A(t) can be obtained by the time-windowed Fast Fourier Transform (FFT). With the estimation of the amplitude of the spectrogram, the beats are extracted by looking for distinct amplitude changes in the time domain, which could be described as:</p><p id="p-0052" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>&#x210a;</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>t</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mi>Amp</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>F</mi>       <mo>&#x2062;</mo>       <mi>F</mi>       <mo>&#x2062;</mo>       <mrow>        <mi>T</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mi>A</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mi>t</mi>         <mo>)</mo>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mpadded width="0em" lspace="0em" depth="-0.1ex" height="0.1ex">  <mtable>   <mtr>    <mtd>     <mrow>      <mrow>       <msub>        <mi>B</mi>        <mi>a</mi>       </msub>       <mo>(</mo>       <mi>t</mi>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mrow>       <mo>{</mo>       <mtable>        <mtr>         <mtd>          <mn>1</mn>         </mtd>         <mtd>          <mrow>           <mrow>            <mrow>             <mrow>              <mi>if</mi>              <mo>&#x2062;</mo>              <mtext>   </mtext>              <mrow>               <mi>&#x210a;</mi>               <mo>&#x2061;</mo>               <mo>(</mo>               <mi>t</mi>               <mo>)</mo>              </mrow>             </mrow>             <mo>-</mo>             <mrow>              <mi>&#x210a;</mi>              <mo>&#x2061;</mo>              <mo>(</mo>              <msup>               <mi>t</mi>               <mo>&#x2032;</mo>              </msup>              <mo>)</mo>             </mrow>            </mrow>            <mo>&#x3e;</mo>            <msub>             <mi>c</mi>             <mn>1</mn>            </msub>           </mrow>           <mo>,</mo>           <mrow>            <mo>&#x2200;</mo>            <mrow>             <msup>              <mi>t</mi>              <mo>&#x2032;</mo>             </msup>             <mo>&#x2208;</mo>             <mrow>              <msub>               <mover>                <mi>U</mi>                <mo>.</mo>               </mover>               <mi>a</mi>              </msub>              <mo>(</mo>              <mrow>               <mi>t</mi>               <mo>,</mo>               <msub>                <mi>t</mi>                <mn>0</mn>               </msub>              </mrow>              <mo>)</mo>             </mrow>            </mrow>           </mrow>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mn>0</mn>         </mtd>         <mtd>          <mi>otherwise</mi>         </mtd>        </mtr>       </mtable>      </mrow>     </mrow>    </mtd>    <mtd>     <mrow>      <mo>(</mo>      <mn>2</mn>      <mo>)</mo>     </mrow>    </mtd>   </mtr>  </mtable> </mpadded></math></maths></p><p id="p-0053" num="0051">Amp denotes the function or model that estimates the amplitude of the spectrogram. The positive threshold c<sub>1 </sub>is set to determine the existence of beats at time t which satisfies B<sub>a</sub>(t)=1 compared with any other t&#x2032; in its punctured neighborhood {dot over (U)}<sub>a </sub>determined by a pre-defined radius t<sub>0</sub>. In the mainstream methodologies, amplitude estimation is conducted by deriving the onset strengths from the obtained spectrogram. The audio beats B<sub>a </sub>(t)=1 is then determined by the occurrence of the peak in each onset envelope.</p><p id="p-0054" num="0052">In some embodiments, to obtain the audio beats, the mainstream approach making use of the onset strength is exploited. All the audio beats can be practically processed by methods in the open-source package LibROSA, which provides the implementations of the onset-driven beat detection for audio signals. In some embodiments, the audio beat strengths are per-computed to estimate the tempo based on the auto-correlation inside onset envelope by the analysis of Mel spectrogram. Referring to Equations. (1) and (2), the audio beat B<sub>a</sub>(t)=1 can be explained by the case where there is a peak in the onset envelope at consistent t with the obtained tempo. To assemble the valid beats in B<sub>a</sub>(t), the position-based beats {p<sub>a</sub>(b)|b=1,2, . . . , N} are formed to collect all the positions in time t of occurred N beats that satisfy B<sub>a</sub>(t)=1.</p><p id="p-0055" num="0053">Simultaneously, the corresponding strengths of beats p<sub>a</sub>(b) are thus represented with the peak values as s<sub>a</sub>(b).</p><p id="p-0056" num="0054">At S<b>304</b>, a plurality of training meter units are determined according to the audio beats and the audio beat strengths. Each training meter unit corresponds to a sample audio sequence of the training sample audio and a temporal index based on a time record of the training meter unit.</p><p id="p-0057" num="0055"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates the extraction of meter unions based on the obtained audio beats. As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, given the audio sequences A(t), the audio beats p<sub>a</sub>(b) and their strengths s<sub>a</sub>(b) can be obtained. Whether the beat is strong or weak m<sub>e</sub>(b) can be determined by comparing the strength with its previous beat as:</p><p id="p-0058" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>m</mi>       <mi>e</mi>      </msub>      <mo>(</mo>      <mi>b</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mtable>       <mtr>        <mtd>         <mn>1</mn>        </mtd>        <mtd>         <mrow>          <mrow>           <mi>if</mi>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mrow>            <msub>             <mi>s</mi>             <mi>a</mi>            </msub>            <mo>(</mo>            <mi>b</mi>            <mo>)</mo>           </mrow>          </mrow>          <mo>&#x3e;</mo>          <mrow>           <msub>            <mi>s</mi>            <mi>a</mi>           </msub>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mi>b</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>)</mo>           </mrow>          </mrow>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mn>0</mn>        </mtd>        <mtd>         <mrow>          <mi>otherwise</mi>          <mo>,</mo>         </mrow>        </mtd>       </mtr>      </mtable>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0059" num="0056">m<sub>e</sub>(b)=1 means that the beat is strong and 0 is week. For example, with a quarter note, strong-weak beat combinations are mapped into 3 meter unit types, which are 4/4 , 5/4, and 6/4 in 4 categories totally. Several beats in the previous meter unit are added in a current meter unit for the transitions between meter units to form the meter units into the unified beat length to describe the flow of musical rhythms. In the example shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, each meter unit includes 7 beats (unified beat length) and one of the four music meter types listed in the mapping table. If the music meter type has 4 beats, 3 previous beats are added to the current meter unit to fill up to the 7 beats, such as Unit <b>2</b> and Unit <b>3</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Similarly, if the strong-weak beat combination has 5 beats, 2 previous beats are added to the current meter unit to fill up to the 7 beats, such as Unit <b>1</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The first strong-weak beat combination is discarded since there is not previous beat to fill up to the unified length. The last meter unit ends with the last strong-weak beat combination. In this way, the meter units can be extracted based on recognized strong-weak beat combinations of the audio beats. In some embodiments, testing meter units may be obtained from a testing audio input (e.g., S<b>202</b>) in a similar manner as the training meter units being extracted from a training sample audio (e.g., S<b>304</b>).</p><p id="p-0060" num="0057">Meter units are used as a basic unit of audio and visual pairs in the audio-driven video synthesis process. For each meter unit MU, the start time and end time are recorded as MU(t), t &#x2208; [t<sub>start</sub>, t<sub>end</sub>]. The disclosed process can find, for a meter unit corresponding to an audio sequence A(t), a motion sequence V(t) that matches the audio sequence with desirable harmony by using a generator of a trained GAN model, and repeat the similar process for all meter units. According to the time records, when training the GAN model, known audio-visual matched videos training samples having desirable harmony are obtained and corresponding audio and motion sequences A(t) and V(t) can be extracted as the ground-truth pairs, including the audio beats p<sub>a</sub>(b) and their beat strengths s<sub>a</sub>(b).</p><p id="p-0061" num="0058">Meanwhile, the temporal indexes TI(t) is formed in binary to denote the separation between the current meter unit and the previous meter unit, where TI(t) is set to 1 if the time belongs to the priors from the previous meter unit. Using the example shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the first three beats of meter unit <b>2</b> are marked as belonging to the priors from the previous meter unit (TI(t)=1), and the remaining four beats of meter unit <b>2</b> are marked as belonging to the current meter unit (TI(t)=0).</p><p id="p-0062" num="0059">In some embodiments, training the GAN model also includes segmenting audio-visual clips based on the plurality of training meter units temporally and inputting the segmented audio-visual clips for the training. The initial pose of each training meter unit is obtained from a corresponding audio-visual clip that contains the sample audio sequence. In other words, if the human motion sequences are harmonious with the given auditory rhythms, the human motion sequences can show regular recurring movement units related to the audio beats. It can be assumed that the correlation exists between such movement units and the obtained audio beats. Thus, the audio-visual clips are segmented based on the defined meter units temporally as input for training to strengthen the learning of beat-driven cross-domain unit mapping in a deep model consistent with the embodiments of the present disclosure, which can indirectly benefit the audio-visual harmony for the generation.</p><p id="p-0063" num="0060">At S<b>306</b>, features of the sample audio sequence of each training meter unit are extracted as a sample auditory input.</p><p id="p-0064" num="0061">In some embodiments, features of the input audio sequence A(t) of the current meter unit are extracted as an auditory input A<sub>f</sub>(t). For example, for A(t), the features of Mel Frequency Cepstral Coefficients (MFCCs) are extracted as the auditory input A<sub>f</sub>(t). In some embodiments, auditory input corresponding to a testing meter unit (e.g., S<b>204</b>) may be obtained in a similar manner as obtaining training sample auditory input (e.g., S<b>306</b>).</p><p id="p-0065" num="0062">At S<b>308</b>, a sample initial pose of each training meter unit is obtained as a sample visual input based on the temporal index and a training sample visual motion sequence.</p><p id="p-0066" num="0063">In some embodiments, for training, the initial poses V<sub>f</sub>(t) can be obtained based on TI(t) and V(t) by:</p><p id="p-0067" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>V</mi>       <mi>f</mi>      </msub>      <mo>(</mo>      <mi>t</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mtable>       <mtr>        <mtd>         <mrow>          <mi>V</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>t</mi>          <mo>)</mo>         </mrow>        </mtd>        <mtd>         <mrow>          <mrow>           <mi>if</mi>           <mo>&#x2062;</mo>           <mtext>&#x205f;  </mtext>           <mrow>            <mi>TI</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mi>t</mi>            <mo>)</mo>           </mrow>          </mrow>          <mo>=</mo>          <mn>1</mn>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mfrac>          <mrow>           <msub>            <mo>&#x2211;</mo>            <mi>t</mi>           </msub>           <mrow>            <mrow>             <mi>V</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mi>t</mi>             <mo>)</mo>            </mrow>            <mo>&#xd7;</mo>            <mrow>             <mi>TI</mi>             <mo>(</mo>             <mi>t</mi>             <mo>)</mo>            </mrow>           </mrow>          </mrow>          <mrow>           <msub>            <mo>&#x2211;</mo>            <mi>t</mi>           </msub>           <mrow>            <mi>TI</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mi>t</mi>            <mo>)</mo>           </mrow>          </mrow>         </mfrac>        </mtd>        <mtd>         <mrow>          <mrow>           <mi>othe</mi>           <mo>&#x2062;</mo>           <mi>r</mi>           <mo>&#x2062;</mo>           <mi>w</mi>           <mo>&#x2062;</mo>           <mi>i</mi>           <mo>&#x2062;</mo>           <mi>s</mi>           <mo>&#x2062;</mo>           <mi>e</mi>          </mrow>          <mo>,</mo>         </mrow>        </mtd>       </mtr>      </mtable>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0068" num="0064">That is, the visual features V<sub>f</sub>(t) keep the movements from the previous meter unit as priors and use the mean pose for one previous meter unit as initialization for the current meter unit. Using the example shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the visual features or initial poses corresponding to the first three beats of meter unit <b>3</b> are obtained from motion sequence corresponding to the last three beats of meter unit <b>2</b>, and the pose of each of the remaining four beats of meter unit <b>3</b> is a mean pose of the poses corresponding to the last three beats of meter unit <b>2</b>. This allows the network to enhance the temporal consistency in the synthesis of human motion.</p><p id="p-0069" num="0065">The auditory input (i.e., the audio features extracted at S<b>304</b>) and the visual input (i.e., the initial poses obtained at S<b>306</b>) can be inputted into a generator of a GAN model. The structure of the generator G can be summarized as G (A<sub>f</sub>(t), V<sub>f</sub>(t))=V&#x2032;(t), where V&#x2032;(t) denotes the generated motion sequence for a meter unit. <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> demonstrates the overview of the testing phase. Based on the generator structure, as shown in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, in the testing phase, V<sub>f</sub>(t) are processed from the previously synthesized motions, which contributes to the generation of consistent human motions recurrently with meter units for audio clips in random duration. In some embodiments, the initial pose of the first meter unit may be pre-assigned and input into the generator.</p><p id="p-0070" num="0066">At S<b>310</b>, the GAN model is trained using the sample auditory input and the sample visual input of each training meter unit by incorporating a hybrid loss function, to obtain a trained GAN model. The hybrid loss function includes a multi-space pose loss, a harmony loss, and a GAN loss. The harmony loss is determined according to beat consistencies of audio-visual beat pairs corresponding to a training meter unit. Each audio beat in the audio-visual beat pairs is from the sample auditory input of the training meter unit. Each visual beat in the audio-visual beat pairs is from an estimated visual motion sequence corresponding to the training meter unit generated during a process of training the GAN model.</p><p id="p-0071" num="0067">Since GANs shown their outstanding power in visual generation tasks, GANs are also popular to be used in cross-domain generation. <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> demonstrates the overview of the whole training process. In some embodiments, for GAN model training, the GAN model includes a generator, and a cross-domain discriminator and a spatial-temporal discriminator for jointly supervising the generator. In some embodiments, to supervise the reality of produced human motions, apart from a cross-domain discriminator, which is widely used to control the validity of feature conversion in different domains, a spatial-temporal pose discriminator is utilized to judge realistic movements both spatially and temporally. In addition to the GAN losses provided by discriminators, the multi-space pose loss and the beat-driven harmony loss based on the attentional harmony mechanism are incorporated as the regularization for the generator. Such harmony-aware hybrid loss functions can guide the generator to output human-like motion sequences that are harmonious with the given music by constraining the consistency between the pre-computed audio beats and visual beats extracted from the generated motions.</p><p id="p-0072" num="0068">In the tasks of audio-driven motion synthesis, the network is fed with the input of audio sequences or extracted auditory features to generate the visual motion sequences. Due to the difficulty of cross-domain synthesis, it is always a problem to encourage effective feature transformation in the architecture. To solve this problem, the encoder-decoder structure is considered to handle the translation between sequences to sequences. Taken into consideration the chronological order in the input and output sequences, recurrent structures are introduced into the architecture of encoder and decoder to obtain features considering temporal correlations. The Gated Recurrent Units (GRUs), as a typical structure of recurrent neural network (RNN), can outperform the common Long Short Term Memory (LSTM) structure in sequence learning for its fewer parameters and reduced computation. In some embodiments, as shown in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, the generator G includes a GRU-based audio encoder and pose decoder.</p><p id="p-0073" num="0069">In addition, differently from analyzing only the audio features outputted from the encoder in the decoding of poses, the initial pose features are concatenated with the audio features to enhance cross-domain learning for the decoder. The skip connections are also applied to intentionally add the audio-visual features into the future layers.</p><p id="p-0074" num="0070">Because the generator G is aimed of producing human motions in 3D poses, it is more difficult to accurately estimate the additional depth dimension compared to synthesizing 2D motions. In some embodiments, the 2D poses are estimated first, and then a depth lifting branch is constructed to produce the 3D poses based on the 2D estimation. Taking advantage of the similarity between the 2D and 3D poses, the depth can be efficiently generated.</p><p id="p-0075" num="0071">In the music-to-motion synthesis, not only the consistency of content style between the generated human movements and target audio sequences is needed to be supervised, but also the reality of synthesized human motions. Thus, a cross-domain discriminator D<sub>cd </sub>and a spatial-temporal discriminator D<sub>st </sub>are built to guide the network to learn the global content consistency between the audio-visual pairs and the targeted pose flow in the spatial and temporal domain, respectively.</p><p id="p-0076" num="0072">In some embodiments, as shown in <figref idref="DRAWINGS">FIG. <b>5</b>D</figref>, in the cross-domain discriminator D<sub>cd</sub>, for any audio-visual pair (A(t), V(t)), a two-branch classification network is leveraged to judge the global style consistency. After the extraction of the audio and visual features separately, the audio and visual features are concatenated together and classify the similarity based on obtained audio-visual features. The cross-domain discriminator D<sub>cd </sub>can improve the reasonable cross-domain translation for the generator G.</p><p id="p-0077" num="0073">In some embodiments, for penalizing the unrealistic produced motions, such as distorted human poses and unnatural transition between movements, the spatial-temporal discriminator D<sub>st </sub>is constructed by applying a temporal progressing network. As shown in <figref idref="DRAWINGS">FIG. <b>5</b>E</figref>, the input motion sequences V(t) are segmented evenly into 5 parts based on the time duration. By repeating the feature extraction and the concatenation of obtained spatial features progressively in chronological order, the spatial-temporal discriminator D<sub>st </sub>can lead the generator G to understand the spatial-temporal relationship of human motions in the ground truth data.</p><p id="p-0078" num="0074">Harmony plays an important role in the evaluation of generated cross-modal results. Since the sense of vision and hearing are highly related and affect each other in brain processing, harmony is especially concerned in the tasks of audio-to-visual or visual-to-audio generation. Taking the example of audio-driven human motion synthesis, in the quality assessment the audio-visual harmony is emphasized that the synthesized movements should be rhythmic and harmonious with the music. In other words, the rhythms in the audio and visual sequences are required to be consistent temporally in order to satisfy the perceptual harmony. Since the feelings of rhythm rely on subjective human perception, given the audio sequences A(t) and visual sequences V(t) as functions of time t, it is an important topic to approximate the perceptual judgement of harmony into quantitative measurements as:</p><p id="p-0079" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>h=H</i>(<i>A</i>(<i>t</i>), <i>V</i>(<i>t</i>)) &#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0080" num="0075">H denotes the algorithm that analyzes the harmony between the cross-domain signals and h is a scalar representing the quantified judgement of harmony.</p><p id="p-0081" num="0076">Referring to the rules that detect audio beats, the visual beats B<sub>v</sub>(t) are similarly extracted based on the analysis of motion trend between visual sequences V(t) and V(t-t<sub>0</sub>). When there is a drastic change in the motion trend occurs, the time t is considered as the occurrence of a beat, which could be depicted as:</p><p id="p-0082" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>d</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>t</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mi>M</mi>      <mo>&#x2062;</mo>      <mrow>       <mi>T</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mrow>         <mi>V</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mi>t</mi>         <mo>)</mo>        </mrow>        <mo>,</mo>        <mrow>         <mi>V</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>t</mi>          <mo>-</mo>          <msub>           <mi>t</mi>           <mn>1</mn>          </msub>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00004-2" num="00004.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>B</mi>       <mi>v</mi>      </msub>      <mo>(</mo>      <mi>t</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mtable>       <mtr>        <mtd>         <mn>1</mn>        </mtd>        <mtd>         <mrow>          <mrow>           <mrow>            <mrow>             <mi>if</mi>             <mo>&#x2062;</mo>             <mtext>&#x205f; </mtext>             <mrow>              <mi>d</mi>              <mo>&#x2062;</mo>              <mrow>               <mo>(</mo>               <mi>t</mi>               <mo>)</mo>              </mrow>             </mrow>            </mrow>            <mo>-</mo>            <mrow>             <mi>d</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mrow>              <mi>t</mi>              <mo>'</mo>             </mrow>             <mo>)</mo>            </mrow>           </mrow>           <mo>&#x3e;</mo>           <msub>            <mi>c</mi>            <mn>2</mn>           </msub>          </mrow>          <mo>,</mo>          <mtext>&#x205f;</mtext>          <mrow>           <mo>&#x2200;</mo>           <mrow>            <msup>             <mi>t</mi>             <mo>&#x2032;</mo>            </msup>            <mo>&#x2208;</mo>            <mrow>             <msub>              <mover>               <mi>U</mi>               <mo>.</mo>              </mover>              <mi>v</mi>             </msub>             <mo>(</mo>             <mrow>              <mi>t</mi>              <mo>,</mo>              <msub>               <mi>t</mi>               <mn>2</mn>              </msub>             </mrow>             <mo>)</mo>            </mrow>           </mrow>          </mrow>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mn>0</mn>        </mtd>        <mtd>         <mrow>          <mi>othe</mi>          <mo>&#x2062;</mo>          <mi>r</mi>          <mo>&#x2062;</mo>          <mi>w</mi>          <mo>&#x2062;</mo>          <mi>i</mi>          <mo>&#x2062;</mo>          <mi>s</mi>          <mo>&#x2062;</mo>          <mi>e</mi>         </mrow>        </mtd>       </mtr>      </mtable>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0083" num="0077">MT denotes the function or model that estimates the motion trend during t<sub>1 </sub>(a pre-defined constant value) and c<sub>2 </sub>is a positive value that controls the threshold to obtain the beat at time t where B<sub>v</sub>(t)=1 stands in comparison with any other t&#x2032; in its punctured neighborhood {dot over (U)}<sub>v </sub>radiused by pre-defined constant t<sub>2</sub>. To process the general pixel-based visual signals, the use of optical flow can capture the motion trend in the moving events. With the quantification of optical flow, the visual beats can be obtained by deriving the local maximums that denote the obvious changes in movements. When focusing on only the human motion in such pixel-based signals, the skeleton-driven method can be used to specify the motion trend for the pure skeleton-based motions extracted from visual signals. Thus, the estimation of motion trend can be converted to analyzing the directions of body movements by joint-based standard derivation. The visual beats B<sub>v</sub>=1 thus are defined as the distinct directional changes in the motion sequences.</p><p id="p-0084" num="0078">Based on the observed audio and visual beats, a common assumption is derived to tackle rhythmic consistency that the appearance of every audio beat is supposed to synchronize with that of the visual beat and vice versa. Following such assumption, the existing strategies evaluate the quantified audio-visual harmony h by performing the alignment based on the extracted beats, which extend the Eq. (5) as:</p><p id="p-0085" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>h=L</i>(<i>f</i><sub>a</sub>(<i>A</i>(<i>t</i>)), <i>f</i><sub>v</sub>(<i>V</i>(<i>t</i>)))=<i>L</i>(<i>B</i><sub>a</sub>(<i>t</i>), B<sub>v</sub>)) &#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0086" num="0079">f<sub>a </sub>and f<sub>v </sub>denote functions for beat detection in the audio and visual signals, respectively. L represents the alignment algorithm. In some embodiments, the algorithm L can formulate the alignment problem as analyzing the distances between the synchronized beat pairs by warping, cross-entropy, or F-score, which are effective to align the cross-domain objects.</p><p id="p-0087" num="0080">In some embodiments, taking human video as an example, given human and their movements as the attention points, the harmony are mainly considered as the alignment between foreground human motion in the visual frames and the associated background music. To better analyze the foreground human motions, the skeletons of human are extracted to represent the human motions in videos. Hence, harmony is evaluated between the audio signals and skeleton-based human motions.</p><p id="p-0088" num="0081">To visualize the subjective evaluation of harmony by objective expressions, based on human reaction time, the tolerance fields neighbored with audio beats are set to represent the perceptual judgement of audio-visual harmony in terms of the synchronization between beats. <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example of unsynchronized audio-visual beats, where the processing of beats for audio and visual signals separately does not reach a satisfactory alignment.</p><p id="p-0089" num="0082">In the visual case, optical flow is often used to extract beats for general frame-based visual signals. However, when fed with human video, this approach does not function effectively compared to the skeleton-based approach. One reason is that optical flow treats the motion of each pixel almost equally where much higher weights is put on the foreground human. As shown in <figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref>, with the distinct occurrences of human movement changes in the video frames perceptually, accordingly the visual beats should be obtained by the beat extraction methods. However, disturbed by possible moving events in the background, the optical flow method has difficulty in detecting beats for foreground human motions while the skeleton-based approach outperforms it significantly in obtaining visual beats that are more consistent with human perception. The joint-wise standard deviation (SD) based visual beat detection is used to represent the skeleton-based approach, where the visual beats are detected by estimating the directional changes in the body movements.</p><p id="p-0090" num="0083">In some embodiments, the mainstream onset-based audio beat detection is combined with the SD-driven visual beat detection in the beat alignment experiment conducted on the dance dataset. Since the human reaction time is around 0.25 seconds, the radius of tolerance field for each audio beat is set to 6 frames, with the total duration of 0.24 seconds under 25 fps, to evaluate the audio-visual alignment results. However, the outcome is not very satisfactory. As shown in <figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref>, the harmony distortion is quite high due to omission and redundancy between beat pairs. It reveals that, though the SD-driven visual beat detection can basically harmonize with the subjective perception, when referred with onset-based audio beats, such cross-domain audio-visual beat pairs are not consistent with each other. To cooperate with onset-based audio beats, a novel beat extraction mechanism consistent with the embodiments of the present disclosure considering velocity of joints in neighboring frames is provided to determine weights to detect visual beats that can satisfy better beat consistency in the audio-visual alignment.</p><p id="p-0091" num="0084">In some embodiments, training the GAN model further includes detecting visual beats of the estimated visual motion sequence by considering a difference between joint velocity sums in neighboring frames of the estimated visual motion sequence.</p><p id="p-0092" num="0085">Given the skeleton-oriented human motion sequences v<sub>s</sub>(t, j) with j joints at frame t obtained from V(t), the joint velocity sum J<sub>v</sub>(t) is derived by calculating the frame difference as:</p><p id="p-0093" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>J</mi>       <mi>v</mi>      </msub>      <mo>(</mo>      <mi>t</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>j</mi>       </munderover>       <mrow>        <msub>         <mi>v</mi>         <mi>s</mi>        </msub>        <mo>(</mo>        <mrow>         <mi>t</mi>         <mo>,</mo>         <mi>i</mi>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>-</mo>      <mrow>       <msub>        <mi>v</mi>        <mi>s</mi>       </msub>       <mo>(</mo>       <mrow>        <mrow>         <mi>t</mi>         <mo>-</mo>         <mn>1</mn>        </mrow>        <mo>,</mo>        <mi>i</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>9</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0094" num="0086">i denotes the i<sup>th </sup>joint. In some embodiments, the diversity and frequency can be regularized based on analyzing the joint sum.</p><p id="p-0095" num="0087">To define the motion beats that are well-aligned with audio beats, the evolution of indivisible movement units (e.g., hand lift) is mainly focused on for the analysis of visual beats in the whole motion sequences.</p><p id="p-0096" num="0088"><figref idref="DRAWINGS">FIG. <b>9</b></figref> demonstrates the correlation between the velocity graph and the real human movements. When the change of joint velocity sum is approaching zero, it is usually related to the complement of a single movement unit. Thus, the motion beats can be defined as the peaks or valleys in the velocity graph, where the acceleration equals zero. Eqs. (6) and (7) are then reformed as:</p><p id="p-0097" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mover accent="true">       <mi>d</mi>       <mi>&#x2dc;</mi>      </mover>      <mo>(</mo>      <mi>t</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mi>sign</mi>      <mo>&#x2062;</mo>      <mtext>  </mtext>      <mrow>       <mo>(</mo>       <mrow>        <mrow>         <msub>          <mi>J</mi>          <mi>v</mi>         </msub>         <mo>(</mo>         <mi>t</mi>         <mo>)</mo>        </mrow>        <mo>-</mo>        <mrow>         <msub>          <mi>J</mi>          <mi>v</mi>         </msub>         <mo>(</mo>         <mrow>          <mi>t</mi>          <mo>-</mo>          <mn>1</mn>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>10</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00006-2" num="00006.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mover>       <msub>        <mi>B</mi>        <mi>v</mi>       </msub>       <mo>&#x223c;</mo>      </mover>      <mo>(</mo>      <mi>t</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mtable>       <mtr>        <mtd>         <mn>1</mn>        </mtd>        <mtd>         <mrow>          <mrow>           <mi>if</mi>           <mo>&#x2062;</mo>           <mtext>&#x205f; </mtext>           <mrow>            <mover accent="true">             <mi>d</mi>             <mi>&#x2dc;</mi>            </mover>            <mo>(</mo>            <mi>t</mi>            <mo>)</mo>           </mrow>           <mo>&#xd7;</mo>           <mrow>            <mover accent="true">             <mi>d</mi>             <mi>&#x2dc;</mi>            </mover>            <mo>(</mo>            <mrow>             <mi>t</mi>             <mo>+</mo>             <mn>1</mn>            </mrow>            <mo>)</mo>           </mrow>          </mrow>          <mo>&#x3c;</mo>          <mn>0</mn>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mn>0</mn>        </mtd>        <mtd>         <mi>otherwise</mi>        </mtd>       </mtr>      </mtable>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>11</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0098" num="0089">Similar with the audio case, the position-based visual beats are then formulated as {p<sub>v</sub>(b)|b=1,2, . . . , M} for M valid beats satisfying {tilde over (B)}<sub>v</sub>(t)=1 and their strengths are assigned due to the corresponding J<sub>v</sub>(t) as s<sub>v</sub>(b). <figref idref="DRAWINGS">FIGS. <b>10</b>A and <b>10</b>B</figref> demonstrate that when tested with the same conditions shown in <figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref>, such obtained visual beats are basically synchronized with the occurrence of onset-based audio beats. By comparing <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> and <figref idref="DRAWINGS">FIG. <b>10</b>B</figref>, it can be observed that the novel beat extraction mechanism consistent with the embodiments of the present disclosure outperforms the existing skeleton-based approach using motion SD by reducing the omission and redundancy for the beat-wise synchronization.</p><p id="p-0099" num="0090">In addition to the synchronization between beats, another factor that highly influences the perception of audio-visual harmony is an attention mechanism consistent with the embodiments of the present disclosure. Since human attention is drawn for things that are more &#x201c;attractive&#x201d;, on the contrary, some other things may be overlooked unconsciously in the perception. Thus, to assess the audio-visual harmony close to the real human perception, the attention mechanism is needed to be introduced in the evaluation framework.</p><p id="p-0100" num="0091">The attention mechanism reveals that unsalient objects are neglected in human perception without any awareness, which influence both vision and hearing systems. When it comes to the subjective perception of rhythmic harmony, the phenomenon of inattentional blindness and deafness may also affect the judgement based on the saliency distribution in the audio and visual rhythms. In order to approximate the perceptual measurement of harmony, an attention-based evaluation framework consistent with embodiments of the present disclosure is provided to highlight the importance of salient beats, which extends Eq. (8) as:</p><p id="p-0101" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>h=L</i>(<i>W</i><sub>a</sub>(<i>p</i><sub>a</sub>(<i>b</i>)), <i>W</i><sub>v</sub>(<i>p</i><sub>v</sub>(<i>b</i>))) &#x2003;&#x2003;(12)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0102" num="0092">W<sub>a </sub>and W<sub>v </sub>denote the attentional weighting masks derived from the audio and visual beat saliency, respectively.</p><p id="p-0103" num="0093">In some embodiments, the beat consistencies of the audio-visual beat pairs corresponding to a training meter unit is determined by assigning a weight to each audio beat and a weight to each visual beat based on beat saliency. Because salient beats favor the perception of harmony, a weight is assigned to each beat based on its beat saliency to enhance the corresponding attentional impact in the evaluation. The beat saliency is represented by the beat strengths s<sub>a</sub>(b) and s<sub>v</sub>(b) and adaptive weighting masks that are constructed by considering the global SD for the strengths.</p><p id="p-0104" num="0094">In the analysis of auditory saliency, the attentional mask is built as:</p><p id="p-0105" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i><sub>a</sub>=sign(<i>s</i><sub>a</sub>(<i>b</i>)&#x2212;SD(<i>s</i><sub>a</sub>(<i>b</i>))&#xd7;&#x3bb;<sub>1</sub>) &#x2003;&#x2003;(13)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0106" num="0095">&#x3bb;<sub>1 </sub>denotes a constant scale factor to adjust the audio saliency threshold.</p><p id="p-0107" num="0096">Differently from processing the mask for audio beats, in the visual case, the motion beats are extracted from not only the peaks but also the valleys of the joint velocity sum, which means that the direct comparison with SD is not applicable for analyzing the visual saliency. Therefore, the peak-to-valley difference is utilized to define the visual saliency strength for detecting the appearances of high-impact visual beats, which is shown as:</p><p id="p-0108" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i>(<i>b</i>)=|s<sub>v</sub>(<i>b</i>)&#x2212;<i>s</i><sub>v</sub>(<i>b&#x2212;</i>1)|<i>b=</i>2<i>, . . . , M </i>&#x2003;&#x2003;(14)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0109" num="0097">R(b) denotes the peak-to-valley difference for each beat.</p><p id="p-0110" num="0098">The visual saliency mask W<sub>v </sub>is then defined by utilizing the global SD as:</p><p id="p-0111" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i><sub>v</sub>=sign(<i>s</i><sub>v</sub>(<i>b</i>)&#x2212;SD(<i>R</i>(<i>b</i>))&#xd7;&#x3bb;<sub>2</sub>) &#x2003;&#x2003;(15)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0112" num="0099">&#x3bc;<sub>2 </sub>denotes a constant scale factor to adjust the visual saliency threshold.</p><p id="p-0113" num="0100">In some embodiments, the beat consistencies of the audio-visual beat pairs corresponding to a training meter unit is determined further by obtaining, among the audio-visual beat pairs of the training meter unit, attentional beats according to the weights of the audio beats and the weights of the visual beats, the attentional beats including one or more attentional audio beats and one or more attentional visual beats, and obtaining the beat strength for each of the attentional beats. By applying the weighting masks W<sub>a </sub>and W<sub>v </sub>on the beats p<sub>a</sub>(b) and p<sub>v</sub>(b), respectively, the attentional beats p&#x2032;<sub>a</sub>(b) and p&#x2032;<sub>v</sub>(b) are obtained by extracting the positive results from W<sub>a</sub>(p<sub>a</sub>(b)) and W<sub>v</sub>(p<sub>v</sub>(b)). The corresponding beat strengths for the attentional beats are similarly defined as s&#x2032;<sub>a</sub>(b) and s&#x2032;<sub>v</sub>(b).</p><p id="p-0114" num="0101">The harmonious feeling in audio-visual human perception can be described as fuzzy measurement, which derives from the way that the brain of human being recognizes sensory signals. In some embodiments, the existing warping method is used to handle the beat alignment, which directly adjust the strength curve for visual beats to fit that of audio beats by applying compensations. In some embodiments, the contrastive difference is constructed by calculating cross-entropy distance between the auditory amplitude and motion labels. Because the brain of human being has limitations for recognizing the signals in precise amplitude, such strength-based fine mappings between audio-visual beats are not consistent with the real perception.</p><p id="p-0115" num="0102">In some embodiments, the beat consistencies of the audio-visual beat pairs corresponding to a training meter unit is determined further by constructing hitting scores by counting labels in an audio and visual domain to represent aligned attentional beats in the sample auditory input and the estimated visual motion sequence, one label representing that one attentional audio beat is aligned with a corresponding attentional visual beat according to a human reaction time delay. That is, inspired by the binary labels given to present whether the audio beats and the visual beats are synchronized in the time domain (e.g., <figref idref="DRAWINGS">FIG. <b>6</b></figref>), a hitting score is constructed by counting the &#x201c;good&#x201d; labels in the audio and visual domain to represent whether the beats are aligned in the whole sequences fuzzily. To balance the audio-visual perception, the F-score method is performed to fuse the cross-domain scores for the final judgement.</p><p id="p-0116" num="0103">Beginning with the selected high-saliency N audio beats p&#x2032;<sub>a</sub>(b) and M visual beats p&#x2032;<sub>v</sub>(b), the Eq. (12) is reformed by using the F-score measurement as:</p><p id="p-0117" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>h=L</i>(<i>p&#x2032;</i><sub>a</sub>(<i>b</i>), <i>p&#x2032;</i><sub>v</sub>(<i>b</i>))=<i>F</i><sub>s</sub>(<i>E</i>(<i>p&#x2032;</i><sub>a</sub>(<i>b</i>)), <i>E</i>(<i>p&#x2032;</i><sub>v</sub>(<i>b</i>))) &#x2003;&#x2003;(16)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0118" num="0104">E denotes the algorithm obtaining the hitting score in both audio and visual domain. F<sub>s </sub>represents the F-score measurement.</p><p id="p-0119" num="0105">With the observation that there is a delay between visual perception and brain-processed recognition, the assumption can be made that the beat can be considered to be hit as long as the time interval between the beat and the nearest cross-domain beat is less than the human-reaction delay. In this way, a fuzzy interval-based judgement can be made for measuring the alignment, instead of depending on precise strength-based mappings. As the synchronized beats appear in audio-visual pairs, the audio beats can be seen as anchors in the analysis of hitting. To obtain the interval, the position matrix Z(b<sub>a</sub>, b<sub>v</sub>) is built by repeating the M visual beats p&#x2032;<sub>v</sub>(b<sub>v</sub>) for N times as:</p><p id="p-0120" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x2200;<i>b</i><sub>a</sub><i>, Z</i>(<i>b</i><sub>a</sub><i>, b</i><sub>v</sub>)=<i>Z</i>(<i>b</i><sub>v</sub>)=<i>p&#x2032;</i><sub>v</sub>(<i>b</i><sub>v</sub>) &#x2003;&#x2003;(17)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0121" num="0106">b<sub>a</sub>=1,2, . . . , N and b<sub>v</sub>=1,2, . . . , M.</p><p id="p-0122" num="0107">The column-wise audio-visual interval D (b<sub>a</sub>, b<sub>v</sub>) based on Z(b<sub>a</sub>, b<sub>v</sub>) is computed by subtracting p&#x2032;<sub>a</sub>(b<sub>a</sub>) absolutely:</p><p id="p-0123" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>D</i>(<i>b</i><sub>a</sub><i>, b</i><sub>v</sub>)=|<i>Z</i>(<i>b</i><sub>a</sub><i>, b</i><sub>v</sub>)&#x2212;<i>p&#x2032;</i><sub>a</sub>(<i>b</i><sub>a</sub>)|&#x2003;&#x2003;(18)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0124" num="0108">Then the judgement of whether the audio beat Hp(b<sub>a</sub>) is hit can be obtained by comparing its minimum audio-visual interval T(b<sub>a</sub>) row-wisely with the pre-defined reacting delay as:</p><p id="p-0125" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>T</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <msub>       <mi>b</mi>       <mi>a</mi>      </msub>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mi>min</mi>      <mo>&#x2062;</mo>      <mtext>  </mtext>      <mrow>       <mo>(</mo>       <mrow>        <mi>D</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <msub>          <mi>b</mi>          <mi>a</mi>         </msub>         <mo>,</mo>         <msub>          <mi>b</mi>          <mi>v</mi>         </msub>        </mrow>        <mo>)</mo>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>19</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00007-2" num="00007.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>Hp</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <msub>       <mi>b</mi>       <mi>a</mi>      </msub>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mtable>       <mtr>        <mtd>         <mn>1</mn>        </mtd>        <mtd>         <mrow>          <mrow>           <mi>if</mi>           <mo>&#x2062;</mo>           <mtext>  </mtext>           <mrow>            <mi>T</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <msub>             <mi>b</mi>             <mi>a</mi>            </msub>            <mo>)</mo>           </mrow>          </mrow>          <mo>&#x2264;</mo>          <msub>           <mi>T</mi>           <mi>delay</mi>          </msub>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mn>0</mn>        </mtd>        <mtd>         <mrow>          <mi>otherwise</mi>          <mo>,</mo>         </mrow>        </mtd>       </mtr>      </mtable>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>20</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0126" num="0109">T<sub>delay </sub>is a constant frame time and Hp(b<sub>a</sub>)=1 denotes that there exist a synchronized audio-visual beat pair.</p><p id="p-0127" num="0110">Finally, the hitting score h<sub>s </sub>can be derived by performing the weighted sum of all the hitting points as:</p><p id="p-0128" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>h</mi>      <mi>s</mi>     </msub>     <mo>=</mo>     <mrow>      <munderover>       <mo>&#x2211;</mo>       <mrow>        <msub>         <mi>b</mi>         <mi>a</mi>        </msub>        <mo>=</mo>        <mn>1</mn>       </mrow>       <mi>N</mi>      </munderover>      <mrow>       <mi>H</mi>       <mo>&#x2062;</mo>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <msub>         <mi>b</mi>         <mi>a</mi>        </msub>        <mo>)</mo>       </mrow>       <mo>&#xd7;</mo>       <mrow>        <msubsup>         <mi>s</mi>         <mi>a</mi>         <mo>&#x2032;</mo>        </msubsup>        <mo>(</mo>        <msub>         <mi>b</mi>         <mi>a</mi>        </msub>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>21</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0129" num="0111">Considering the normalization for the total numbers of audio beats and motion beats, the hitting score for audio harmony can be formed as</p><p id="p-0130" num="0000"><maths id="MATH-US-00009" num="00009"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>h</mi>    <mi>a</mi>   </msub>   <mo>=</mo>   <mfrac>    <msub>     <mi>h</mi>     <mi>s</mi>    </msub>    <mi>N</mi>   </mfrac>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0131" num="0000">and the hitting score for visual harmony can be formed as.</p><p id="p-0132" num="0000"><maths id="MATH-US-00010" num="00010"><math overflow="scroll"> <mrow>  <msub>   <mi>h</mi>   <mi>v</mi>  </msub>  <mo>=</mo>  <mrow>   <mfrac>    <msub>     <mi>h</mi>     <mi>s</mi>    </msub>    <mi>M</mi>   </mfrac>   <mo>.</mo>  </mrow> </mrow></math></maths></p><p id="p-0133" num="0000">However, the correlation between h<sub>a </sub>and h<sub>v </sub>differs from source to source. For instance, given a specific input audio-visual sequences, the obtained h<sub>a </sub>may be higher than h<sub>v </sub>but the contrary observation can be obtained for another input sequences.</p><p id="p-0134" num="0112">In some embodiments, the beat consistencies of the audio-visual beat pairs corresponding to a training meter unit is determined further by determining the beat consistencies using the hitting scores. That is, to balance between the audio-visual scores, in some embodiments, the final audio-visual harmony h is obtained by performing the harmonic mean, which reforms the Eq. (16) as:</p><p id="p-0135" num="0000"><maths id="MATH-US-00011" num="00011"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>h</mi>     <mo>=</mo>     <mrow>      <mrow>       <msub>        <mi>F</mi>        <mi>S</mi>       </msub>       <mo>(</mo>       <mrow>        <msub>         <mi>h</mi>         <mi>a</mi>        </msub>        <mo>,</mo>        <msub>         <mi>h</mi>         <mi>v</mi>        </msub>       </mrow>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mfrac>       <mrow>        <mrow>         <mo>(</mo>         <mrow>          <mn>1</mn>          <mo>+</mo>          <msup>           <mi>&#x3b2;</mi>           <mn>2</mn>          </msup>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#x2062;</mo>        <msub>         <mi>h</mi>         <mi>v</mi>        </msub>        <mo>&#x2062;</mo>        <msub>         <mi>h</mi>         <mi>a</mi>        </msub>       </mrow>       <mrow>        <mrow>         <msup>          <mi>&#x3b2;</mi>          <mn>2</mn>         </msup>         <mo>&#x2062;</mo>         <msub>          <mi>h</mi>          <mi>v</mi>         </msub>        </mrow>        <mo>+</mo>        <msub>         <mi>h</mi>         <mi>a</mi>        </msub>       </mrow>      </mfrac>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>22</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0136" num="0113">&#x3b2; is a pre-defined constant. Therefore, Eq. (22) can be transformed into the function of h<sub>s </sub>as:</p><p id="p-0137" num="0000"><maths id="MATH-US-00012" num="00012"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>h</mi>     <mo>=</mo>     <mfrac>      <mrow>       <mrow>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>+</mo>         <msup>          <mi>&#x3b2;</mi>          <mn>2</mn>         </msup>        </mrow>        <mo>)</mo>       </mrow>       <mo>&#x2062;</mo>       <msub>        <mi>h</mi>        <mi>s</mi>       </msub>      </mrow>      <mrow>       <mrow>        <mi>N</mi>        <mo>&#x2062;</mo>        <msup>         <mi>&#x3b2;</mi>         <mn>2</mn>        </msup>       </mrow>       <mo>+</mo>       <mi>M</mi>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>23</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0138" num="0114">Implied by Equations. (22) and (23), the quantification of audio-visual harmony in the evaluation can be suggested as:</p><p id="p-0139" num="0115">The harmony evaluation in the present disclosure is applied according to the following Lemma (1): Given an audio clip with N obtained attentional audio beats and a visual clip with M visual beats, the quantified audio-visual harmony can be uniquely determined by h<sub>s</sub>.</p><p id="p-0140" num="0116">The hybrid loss function includes a multi-space pose loss, a harmony loss, and a GAN loss. The multi-space pose loss is employed to regularize the realism of the estimated human movements. In some embodiments, the multi-space pose loss includes one or more of Kullback-Leibler (KL) loss, Charbonnier-based mean squared error (MSE) loss, and Charbonnier-based VGG loss.</p><p id="p-0141" num="0117">For distribution space, the Kullback-Leibler (KL) loss <img id="CUSTOM-CHARACTER-00001" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>kl </sub>function is applied based on the intermediate results of 2D poses in the generation process, shown as:</p><p id="p-0142" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00002" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>kl</sub>=KL(<img id="CUSTOM-CHARACTER-00003" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/>(<i>V</i><sub>2d</sub>(<i>t</i>))&#x2225;<img id="CUSTOM-CHARACTER-00004" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/>(<i>V</i><sub>2d</sub>&#x2032;(<i>t</i>)) &#x2003;&#x2003;(24)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0143" num="0118"><img id="CUSTOM-CHARACTER-00005" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>kl </sub>denotes the Kullback-Leibler (KL) loss. <img id="CUSTOM-CHARACTER-00006" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/> denotes the operation that transforms the ground-truth 2D motion sequences V<sub>2d</sub>(t) and the intermediate output V<sub>2d</sub>&#x2032;(t) to the probability distribution.</p><p id="p-0144" num="0119">In the pixel space, a Charbonnier-based MSE loss <img id="CUSTOM-CHARACTER-00007" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>mse </sub>is established to constrain the generation of the 3D poses as:</p><p id="p-0145" num="0000"><maths id="MATH-US-00013" num="00013"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>&#x2112;</mi>      <mrow>       <mi>m</mi>       <mo>&#x2062;</mo>       <mi>s</mi>       <mo>&#x2062;</mo>       <mi>e</mi>      </mrow>     </msub>     <mo>=</mo>     <mrow>      <munder>       <mo>&#x2211;</mo>       <mi>t</mi>      </munder>      <msqrt>       <mrow>        <mrow>         <mrow>          <msub>           <mi>W</mi>           <mi>tp</mi>          </msub>          <mo>(</mo>          <mi>t</mi>          <mo>)</mo>         </mrow>         <mo>&#x2062;</mo>         <msup>          <mrow>           <mo>(</mo>           <mrow>            <mrow>             <mi>V</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mi>t</mi>             <mo>)</mo>            </mrow>            <mo>-</mo>            <mrow>             <mi>G</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mrow>              <mrow>               <msub>                <mi>A</mi>                <mi>f</mi>               </msub>               <mo>(</mo>               <mi>t</mi>               <mo>)</mo>              </mrow>              <mo>,</mo>              <mrow>               <msub>                <mi>V</mi>                <mi>f</mi>               </msub>               <mo>(</mo>               <mi>t</mi>               <mo>)</mo>              </mrow>             </mrow>             <mo>)</mo>            </mrow>           </mrow>           <mo>)</mo>          </mrow>          <mn>2</mn>         </msup>        </mrow>        <mo>+</mo>        <msup>         <mi>&#x3f5;</mi>         <mn>2</mn>        </msup>       </mrow>      </msqrt>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>25</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0146" num="0120">&#x3f5; is a positive constant close to zero to soothe the gradient vanishing in training. A weight mask</p><p id="p-0147" num="0000"><maths id="MATH-US-00014" num="00014"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>W</mi>    <mi>tp</mi>   </msub>   <mo>(</mo>   <mi>t</mi>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mfrac>   <mrow>    <mo>(</mo>    <mrow>     <mn>2</mn>     <mo>-</mo>     <mrow>      <mi>TI</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>t</mi>      <mo>)</mo>     </mrow>    </mrow>    <mo>)</mo>   </mrow>   <mn>2</mn>  </mfrac> </mrow></math></maths></p><p id="p-0148" num="0000">is applied based on the temporal index TI(t) to guide the network focus more on the generation of motions for the current meter.</p><p id="p-0149" num="0121">VGG networks are widely used to generate visual features consistent with human perception, the Charbonnier-based VGG loss <img id="CUSTOM-CHARACTER-00008" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>feat </sub>is also performed to regularize the produced human motion in the deep feature space by:</p><p id="p-0150" num="0000"><maths id="MATH-US-00015" num="00015"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>&#x2112;</mi>      <mi>feat</mi>     </msub>     <mo>=</mo>     <mrow>      <munder>       <mo>&#x2211;</mo>       <mi>t</mi>      </munder>      <msqrt>       <mrow>        <msup>         <mrow>          <mo>(</mo>          <mrow>           <mrow>            <mi>V</mi>            <mo>&#x2062;</mo>            <mi>G</mi>            <mo>&#x2062;</mo>            <mrow>             <mi>G</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mrow>              <mi>V</mi>              <mo>&#x2061;</mo>              <mo>(</mo>              <mi>t</mi>              <mo>)</mo>             </mrow>             <mo>)</mo>            </mrow>           </mrow>           <mo>-</mo>           <mrow>            <mi>V</mi>            <mo>&#x2062;</mo>            <mi>G</mi>            <mo>&#x2062;</mo>            <mrow>             <mi>G</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mrow>              <mi>G</mi>              <mo>&#x2061;</mo>              <mo>(</mo>              <mrow>               <mrow>                <msub>                 <mi>A</mi>                 <mi>f</mi>                </msub>                <mo>(</mo>                <mi>t</mi>                <mo>)</mo>               </mrow>               <mo>,</mo>               <mrow>                <msub>                 <mi>V</mi>                 <mi>f</mi>                </msub>                <mo>(</mo>                <mi>t</mi>                <mo>)</mo>               </mrow>              </mrow>              <mo>)</mo>             </mrow>             <mo>)</mo>            </mrow>           </mrow>          </mrow>          <mo>)</mo>         </mrow>         <mn>2</mn>        </msup>        <mo>+</mo>        <msup>         <mi>&#x3f5;</mi>         <mn>2</mn>        </msup>       </mrow>      </msqrt>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>26</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0151" num="0122">In some embodiments, the feature-space pose loss, such as the Kullback-Leibler (KL) loss, the Charbonnier-based MSE loss, or the Charbonnier-based VGG loss, is assumed to be capable to capture the deep features for the motion flow and regularize the flow in the synthesized motions to be consistent with the ground truth.</p><p id="p-0152" num="0123">According to the Lemma (1), the harmony between the audio and human motion sequences can be determined by evaluating the audio-visual beat consistency, which is uniquely dependent on the hitting score h<sub>s</sub>. Thus, the harmony loss is created by formulating the function:</p><p id="p-0153" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00009" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>harmo</sub><i>=E</i>(<i>p&#x2032;</i><sub>a</sub>(<i>b</i>), <i>s&#x2032;</i><sub>a</sub>(<i>b</i>), VB(<i>G</i>(<i>A</i><sub>f</sub>(<i>t</i>), <i>V</i><sub>f</sub>(<i>t</i>))))+<i>&#x221a;{square root over (|M&#x2212;N|)}</i>&#x2003;&#x2003;(27)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0154" num="0124">VB denotes the extraction of attentional visual beats with the corresponding beat strengths based on the estimated human motion sequences from the generator. Such results are then sent to the algorithm E to calculate the hitting score with the pre-computed p&#x2032;<sub>a</sub>(b) and s&#x2032;<sub>a</sub>(b). Apart from minimizing the negative hitting score, the over-frequent visual beats are penalized by adding a L1 distance comparing the number of visual beats M with N audio beats.</p><p id="p-0155" num="0125">GANs can learn to generate outputs based on the distribution of the given data during the adversarial training by solving the min-max problem:</p><p id="p-0156" num="0000"><maths id="MATH-US-00016" num="00016"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <munder>       <mi>min</mi>       <mi>&#x3b8;</mi>      </munder>      <mtext>  </mtext>      <munder>       <mi>max</mi>       <mi>&#x3d5;</mi>      </munder>      <mtext>  </mtext>      <mrow>       <msub>        <mi>&#x2112;</mi>        <mi>adv</mi>       </msub>       <mo>(</mo>       <mrow>        <mi>&#x3d5;</mi>        <mo>,</mo>        <mi>&#x3b8;</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>     <mo>=</mo>     <mrow>      <mrow>       <msub>        <mi>&#x1d53c;</mi>        <mi>x</mi>       </msub>       <mo>[</mo>       <mrow>        <mi>log</mi>        <mo>&#x2062;</mo>        <mtext>  </mtext>        <mrow>         <msub>          <mi>D</mi>          <mi>&#x3d5;</mi>         </msub>         <mo>(</mo>         <mi>x</mi>         <mo>)</mo>        </mrow>       </mrow>       <mo>]</mo>      </mrow>      <mo>+</mo>      <mrow>       <msub>        <mi>&#x1d53c;</mi>        <mi>Y</mi>       </msub>       <mo>[</mo>       <mrow>        <mi>log</mi>        <mo>&#x2062;</mo>        <mtext>  </mtext>        <mrow>         <mo>(</mo>         <mrow>          <mn>1</mn>          <mo>-</mo>          <mrow>           <msub>            <mi>D</mi>            <mi>&#x3d5;</mi>           </msub>           <mo>(</mo>           <mrow>            <msub>             <mi>G</mi>             <mi>&#x3b8;</mi>            </msub>            <mo>(</mo>            <mi>Y</mi>            <mo>)</mo>           </mrow>           <mo>)</mo>          </mrow>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>]</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>28</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0157" num="0126">&#x3d5; and &#x3b8; denote the parameters for the discriminator and generator, respectively. x represents the ground truth data while Y is the input to the generator.</p><p id="p-0158" num="0127">In some embodiments, training the GAN model further includes minimizing the harmony loss, the multi-space pose loss, and the GAN loss from the generator, and maximizing values of loss functions of the cross-domain discriminator and the spatial temporal discriminator to distinguish between a real training sample and a fake training sample. Thus, the cross-domain discriminator and the spatial-temporal pose discriminator try to distinguish between real and fake through maximizing the loss:</p><p id="p-0159" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00010" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>dcd </sub>=<img id="CUSTOM-CHARACTER-00011" he="3.89mm" wi="2.79mm" file="US20230005201A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/>[log(1&#x2212;<i>D</i><sub>cd</sub>(<i>A</i>(<i>t</i>), <i>G</i>(<i>A</i><sub>f</sub>(<i>t</i>), <i>V</i><sub>f</sub>(<i>t</i>))))]+<img id="CUSTOM-CHARACTER-00012" he="3.89mm" wi="2.79mm" file="US20230005201A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/>[log <i>D</i><sub>cd</sub>(<i>A</i>(<i>t</i>), <i>V</i>(<i>t</i>))]&#x2003;&#x2003;(29)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0160" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00013" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>dst </sub>=<img id="CUSTOM-CHARACTER-00014" he="3.89mm" wi="2.79mm" file="US20230005201A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/>[log D<sub>st</sub>(<i>V</i>(<i>t</i>))]+<img id="CUSTOM-CHARACTER-00015" he="3.89mm" wi="2.79mm" file="US20230005201A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/>[log(1&#x2212;<i>D</i><sub>st</sub>(<i>G</i>(<i>A</i><sub>f</sub>(<i>t</i>), <i>V</i><sub>f</sub>(<i>t</i>))))]&#x2003;&#x2003;(30)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0161" num="0128">On the contrary, the generator attempts to fool the discriminators by minimizing the function:</p><p id="p-0162" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00016" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>gan</sub>=<img id="CUSTOM-CHARACTER-00017" he="3.89mm" wi="2.79mm" file="US20230005201A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/>[&#x2212;log D<sub>cd</sub>(<i>A</i>(<i>t</i>), <i>G</i>(<i>A</i><sub>f</sub>(<i>t</i>), <i>V</i><sub>f</sub>(<i>t</i>)))]+<img id="CUSTOM-CHARACTER-00018" he="3.89mm" wi="2.79mm" file="US20230005201A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/>[&#x2212;log <i>D</i><sub>st</sub>(<i>G</i>(<i>A</i><sub>f</sub>(<i>t</i>), <i>V</i><sub>f</sub>(<i>t</i>)))) ]&#x2003;&#x2003;(31)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0163" num="0129">In summary, combining all the loss functions above, the final loss function for the generator can be formulated as:</p><p id="p-0164" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00019" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>total</sub>=&#x3bb;<sub>kl</sub><img id="CUSTOM-CHARACTER-00020" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>kl</sub>+&#x3bb;<sub>mse</sub><img id="CUSTOM-CHARACTER-00021" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>mse</sub>+&#x3bb;<sub>feat</sub><img id="CUSTOM-CHARACTER-00022" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>feat</sub>+&#x3bb;<sub>harmo</sub><img id="CUSTOM-CHARACTER-00023" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>harmo</sub>+&#x3bb;<sub>gan</sub><img id="CUSTOM-CHARACTER-00024" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>gan </sub>&#x2003;&#x2003;(32)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0165" num="0130">The &#x3bb;s denote the corresponding weight for each loss component.</p><p id="p-0166" num="0131">In some embodiments, referring back to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, S<b>202</b> may include determining a plurality of testing meter units according to the audio beats and the audio beat strengths, each testing meter unit corresponding to an audio sequence of the input audio and a temporal index based on a time record of the testing meter unit. S<b>204</b> may include: extracting features of the audio sequence of each testing meter unit as an auditory input. S<b>206</b> may include: obtaining an initial pose of each testing meter unit as a visual input based on the temporal index and a visual motion sequence synthesized for a previous testing meter unit.</p><p id="p-0167" num="0132">In some embodiments, obtaining the initial pose of each testing meter unit includes keeping the generated motion sequence from a previous testing meter unit right before a current testing meter unit in the initial pose of a current meter unit, and using a mean pose of the generated harmony-aware motion sequence from the previous testing meter unit as initialization for the current testing meter unit.</p><p id="p-0168" num="0133">In some embodiments, in an implementation example, the dance dataset released by Tang et al. in &#x201c;Dance with melody: An LSTM-autoencoder approach to music-oriented dance synthesis,&#x201d; proceedings of the 26<sup>th </sup>ACM international conference on Multimedia, 2018 (hereinafter, [Tang et al., 2018]), is utilized to train the HarmoGAN model, which includes 61 sequences of dancing videos performed by the professional dancer totaling 94 minutes and 907,200 frames in 25 fps. It provides the 3D human body keypoints with 21 joints collected from wearable devices and the corresponding audio tracks. The dance dataset contains four typical types of dance: cha-cha, rumba, tango, and waltz. To save the memory cost, all videos are resampled at 15 fps to create a sample dataset. 2014 clips of concatenated audio-visual input features are obtained with the corresponding target poses from the whole dance dataset, where 214 of them are selected randomly as the self-created testing data and the rest are used for model training. All the functions that handle the extraction of musical features can be found in the Librosa package of McFee et al. in &#x201c;Librosa: Audio and music signal analysis in python,&#x201d; proceedings of the 14<sup>th </sup>python in science conference, 2015 (hereinafter, [McFee et al., 2015]).</p><p id="p-0169" num="0134">To evaluate the harmony between the audio and synthesized audio-driven motion sequences, the HarmoGAN model is tested based on the ballroom music dataset by Gouyon et al. in &#x201c;An experimental comparison of audio tempo induction algorithms,&#x201d; IEEE Transactions on Audio, Speech, and Language Processing, 2006 (hereinafter, [Gouyon et al., 2006]). It extracts 698 background music clips each of 30 seconds from the online dance videos. It contains music for 7 types of dance: cha-cha, jive, quickstep, rumba, samba, tango, and waltz. In each dance category, 6 audio sequences are randomly picked to form the testing dataset. The beat-based harmony mechanism is employed to quantify the audio-visual harmony with the use of the Librosa package of [McFee et al., 2015] to obtain information of auditory beats.</p><p id="p-0170" num="0135">HarmoGAN is implemented in PyTorch. The generator is first pretrained to prepare a reasonable initialization for the following GAN training. The pretraining ends at 225 epochs with the use of <img id="CUSTOM-CHARACTER-00025" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>pretrain</sub>=0.14<img id="CUSTOM-CHARACTER-00026" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/>+<img id="CUSTOM-CHARACTER-00027" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>mse</sub>. The Adam optimizer proposed by Kingma and Ba in &#x201c;Adam: A method for stochastic optimization,&#x201d; arXir preprint arXir: 1412.6980 (2014) (hereinafter, [Kingma and Ba, 2014]), is utilized with batch size of 10. The initial learning rate is set to 0.001 and gets decreased every 50 epochs by multiplying with the factors in the order of [0.5,0.2,0.2,0.5]. Initialized with the pretrained model, GAN training is started with both the generator and discriminator networks. The weights of loss components in the hybrid loss function for our generator are set as follows: &#x3bb;<sub>kl</sub>=0.0001, &#x3bb;<sub>mse</sub>=&#x3bb;<sub>feat</sub>=&#x3bb;<sub>gan</sub>=0.001, &#x3bb;<sub>harmo</sub>=1. The weight decay is set to 0.001 for the discriminators and 0.0001 for the generator. The learning rates for all the networks are initialized at 0.0001 and divided by 2 and 5 alternatively every 5 epochs. The optimizer and batch size are kept the same as in pretraining. After 45 epochs of adversarial training, the convergence is achieved to obtain the final HarmoGAN. It only takes 53 minutes to finish the whole training process based on the NVIDIA TITAN V GPU, which is fairly efficient.</p><p id="p-0171" num="0136">For the harmony evaluation mechanism, the constant factors &#x3bb;<sub>1 </sub>and &#x3bb;<sub>2 </sub>in Eqs. (13) and (15) are set to 0.1 and 1, respectively, to obtain the attentional saliency. Meanwhile, the reaction delay is defined as 0.25 seconds, shown as T<sub>delay</sub>=3.75 frames in Eq. (20) under 15 fps. When evaluating the quantified audio-visual harmony, the &#x3b2; of F-score in Eq. (22) is set as 2 to focus more on the hit rate of audio beats.</p><p id="p-0172" num="0137">To confirm the assumption that the occurrence of inharmony in the audio-visual objects can be observed by human perception, a user study is conducted to test whether the participants are sensitive to the inharmonious audio-visual clips. 20 dance videos are collected, which consist of 10 harmonious contents from the ground truth in the dance dataset released by [Tang et al., 2018], and 10 inharmonious clips created by permuting the audio or visual sequences. The invited 10 participants are required to watch the whole 20 videos and provide the perceptual harmony evaluation by picking up all the sequences that are considered as inharmony.</p><p id="p-0173" num="0138"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates the results of the user study, where 78% of the inharmonious videos have been accurately selected by the participants. Due to lack of background knowledge for professional dancing, participants have difficulty in distinguishing all the inharmonious clips from the harmonious ones. Overall, it can conclude that the audio-visual harmony affects human perception and in most cases the occurrence of inharmony can be correctly observed and judged by perception.</p><p id="p-0174" num="0139">Before analyzing the performance of harmonization for the model, at first the HarmoGAN model is supposed to show reasonable ability to synthesize natural motion flows based on human skeletons. To evaluate the motion generation, the HarmoGAN model is tested on the self-created testing dataset obtained from the dance dataset released by [Tang et al., 2018], which can provide ground-truth dance movements performed by a real human dancer. The Fr&#xe9;chet Inception Distance (FID) metric proposed by Heusel et al. in &#x201c;Gans trained by a two time-scale update rule converge to a local nash equilibrium,&#x201d; arXir preprint arXir: 1706.08500 (2017) (hereinafter, [Heusel et al., 2015]), is utilized to measure the perceptual distance between the estimated motion sequences and the human ground truth. As there exists no standard for extracting features in pose sequences, the VGG network proposed by Simonyan and Zisserman in &#x201c;Very deep convolutional networks for large-scale image recognition,&#x201d; arXir preprint arXir: 1409.1556 (2014) (hereinafter, [Simonyan and Zisserman, 2014]), is employed to obtain pose features for measuring FID. The average results are shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>. Compared with the references from the dance generation models proposed by Lee et al. in &#x201c;Dancing to music,&#x201d; Advances in Neural Information Processing Systems, 2019 (hereinafter, [Lee et al., 2019]), and Ren et al. in &#x201c;Self-supervised Dance Video Synthesis Conditioned on Music,&#x201d; proceedings of the 28<sup>th </sup>ACM International Conference on Multimedia, 2020 (hereinafter, [Ren et al., 2020]), shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, it can be implied that the HarmoGAN model is competitive with those state-of-the-art models that can learn to synthesize human motions sharing high feature-space similarity with the real human movements in the training dataset.</p><p id="p-0175" num="0140">Meanwhile, in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, an example of motion sequence pairs is presented for qualitative evaluation. The sequences of human movements synthesized by the HarmoGAN model show similar motion flows compared with the human ground truth.</p><p id="p-0176" num="0141">To analyze the enhancement of audio-visual harmony after introducing the harmony loss into the network training, an ablation study is conducted to evaluate the performance of the HarmoGAN model with its variant without the use of <img id="CUSTOM-CHARACTER-00028" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>harmo </sub>on the self-created testing dataset, which contains relevant initial poses for the generation of motion sequences. Given the pre-computed audio beats from the music sequences, the harmony can be assessed by analyzing the audio-visual beat consistency based on the estimated human movements.</p><p id="p-0177" num="0142">Apart from the quantified harmony derived from the harmony evaluation mechanism shown in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, the hit rate, which is popular in demonstrating harmonization, is also calculated to evaluate the performance. In <figref idref="DRAWINGS">FIG. <b>14</b>B</figref> the hit rate for music beats is presented by computing the percentages of music beats that have been hit by the visual beat within the duration of the reaction delay. The human dancer basically hits half the music beats, which is a reasonable result considering the limited accuracy for data acquisition when obtaining motion sequences. Based on <figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref>, it is obvious that with the incorporation of <img id="CUSTOM-CHARACTER-00029" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>harmo </sub>the performance of audio-visual harmony boosted compared to the variant without <img id="CUSTOM-CHARACTER-00030" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>harmo </sub>and surpasses the real dancer significantly. At the same time, it implies that <img id="CUSTOM-CHARACTER-00031" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>harmo </sub>requires the &#x201c;privilege&#x201d; to edit the motion sequences for harmonization, even though different from the ground truth, which can explain the relatively weak performance shown for the baseline variant without the use of <img id="CUSTOM-CHARACTER-00032" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>harmo </sub>as it is not designed to strictly simulate the ground truth. In all, the test on the self-created dataset demonstrates the outstanding performance of the HarmoGAN model, which can achieve improved audio-visual harmony between the given music sequences and the generated motions under the assistance of the harmony loss.</p><p id="p-0178" num="0143">To further assess the ability of harmonization in the HarmoGAN model, in addition to the variant, the HarmoGAN model is compared against the other two powerful GAN-based state-of-the-art models proposed by [Lee et al., 2019] and [Ren et al., 2020], for audio-driven motion synthesis. For a fair comparison, all models are tested on the Ballroom dataset by [Gouyon et al., 2006], which is a public music dataset only providing background music for various dance types. The 42 clips of 6-second audio tracks are randomly collected from the Ballroom dataset as the testing dataset. Without any given ground-truth human movement, motion sequences in the training dataset are selected as the initial poses to generate the dance sequences.</p><p id="p-0179" num="0144">In <figref idref="DRAWINGS">FIGS. <b>15</b>A and <b>15</b>B</figref>, the results of quantified harmony and the hit rate of music beats are demonstrated by computing the average results of 7 types of dance music. It shows that the HarmoGAN model outperforms the other models distinctly in both metrics.</p><p id="p-0180" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Quantified harmony for 7 types of dance music</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="8"><colspec colname="1" colwidth="49pt" align="left"/><colspec colname="2" colwidth="28pt" align="center"/><colspec colname="3" colwidth="28pt" align="center"/><colspec colname="4" colwidth="35pt" align="center"/><colspec colname="5" colwidth="28pt" align="center"/><colspec colname="6" colwidth="28pt" align="center"/><colspec colname="7" colwidth="28pt" align="center"/><colspec colname="8" colwidth="35pt" align="center"/><tbody valign="top"><row><entry>Model name</entry><entry>Cha-cha</entry><entry>Jive</entry><entry>Quickstep</entry><entry>Rumba</entry><entry>Samba</entry><entry>Tango</entry><entry>Waltz</entry></row><row><entry namest="1" nameend="8" align="center" rowsep="1"/></row><row><entry>Lee et al. 2019</entry><entry>0.3509</entry><entry>0.3359</entry><entry>0.2773</entry><entry>0.2862</entry><entry>0.2805</entry><entry>0.2657</entry><entry>0.2704</entry></row><row><entry>Ren et al. 2020</entry><entry>0.2759</entry><entry>0.3321</entry><entry>0.1625</entry><entry>0.2154</entry><entry>0.2761</entry><entry>0.2671</entry><entry>0.3122</entry></row><row><entry>HarmoGAN</entry><entry>0.1983</entry><entry>0.2337</entry><entry>0.1511</entry><entry>0.2104</entry><entry>0.2012</entry><entry>0.1929</entry><entry>0.2076</entry></row><row><entry>w/o &#x2009;<img id="CUSTOM-CHARACTER-00033" he="2.46mm" wi="7.79mm" file="US20230005201A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> </entry><entry/><entry/><entry/><entry/><entry/><entry/><entry/></row><row><entry>HarmoGAN</entry><entry>0.4097</entry><entry>0.3995</entry><entry>0.3199</entry><entry>0.3495</entry><entry>0.3468</entry><entry>0.2948</entry><entry>0.3455</entry></row><row><entry namest="1" nameend="8" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0181" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Audio beat hit rate for 7 types of dance music</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="8"><colspec colname="1" colwidth="49pt" align="left"/><colspec colname="2" colwidth="28pt" align="center"/><colspec colname="3" colwidth="28pt" align="center"/><colspec colname="4" colwidth="35pt" align="center"/><colspec colname="5" colwidth="28pt" align="center"/><colspec colname="6" colwidth="28pt" align="center"/><colspec colname="7" colwidth="28pt" align="center"/><colspec colname="8" colwidth="35pt" align="center"/><tbody valign="top"><row><entry>Model name</entry><entry>Cha-cha</entry><entry>Jive</entry><entry>Quickstep</entry><entry>Rumba</entry><entry>Samba</entry><entry>Tango</entry><entry>Waltz</entry></row><row><entry namest="1" nameend="8" align="center" rowsep="1"/></row><row><entry>Lee et al. 2019</entry><entry>58.10%</entry><entry>59.92%</entry><entry>53.03%</entry><entry>64.91%</entry><entry>64.93%</entry><entry>58.46%</entry><entry>54.30%</entry></row><row><entry>Ren et al. 2020</entry><entry>28.13%</entry><entry>51.03%</entry><entry>33.33%</entry><entry>41.53%</entry><entry>49.07%</entry><entry>42.84%</entry><entry>52.59%</entry></row><row><entry>HarmoGAN</entry><entry>22.70%</entry><entry>23.97%</entry><entry>18.18%</entry><entry>25.33%</entry><entry>29.70%</entry><entry>21.48%</entry><entry>26.36%</entry></row><row><entry>w/o &#x2009;<img id="CUSTOM-CHARACTER-00034" he="2.79mm" wi="8.47mm" file="US20230005201A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/> </entry><entry/><entry/><entry/><entry/><entry/><entry/><entry/></row><row><entry>HarmoGAN</entry><entry>74.60%</entry><entry>75.65%</entry><entry>71.22%</entry><entry>77.01%</entry><entry>83.69%</entry><entry>74.98%</entry><entry>72.90%</entry></row><row><entry namest="1" nameend="8" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0182" num="0145">The detailed evaluation results for each dance type are shown in Table 1 and 2. Compared with the baseline HarmoGAN model without the use of <img id="CUSTOM-CHARACTER-00035" he="3.56mm" wi="2.46mm" file="US20230005201A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>harmo</sub>, the assistance of spatial-temporal GCN proposed by [Ren et al., 2020] may intrinsically benefit the harmonization by regularizing the hierarchical representations of skeletons in the generation of motion sequences. However, such improvement lacks robustness and is highly affected by the bias in the training dataset. The post-processing beat warper proposed by [Lee et al., 2019] can relatively lift the performance evenly but is still limited. In comparison with the other models, the HarmoGAN model can directly produce distinct and robust improvement for the audio-visual harmony that is independent of the dance types.</p><p id="p-0183" num="0146">In some embodiments, the HarmoGAN model can be performed to generate the visual sequences based on video frames. In some embodiments, a multi-stage or end-to-end system can be built to perform the audio-visual harmonization based on video frames.</p><p id="p-0184" num="0147">In addition, the cost of the tested models is analyzed based on the number of model parameters and training pairs. The number of parameters for the generator in the HarmoGAN model is closer to that of [Ren et al., 2020] and half of that of [Lee et al., 2019], while [Lee et al., 2019], require a 10-times larger training dataset for obtaining the final model. Thus, considering the results of harmony evaluation for each model, it reveals that the HarmoGAN model can improve the performance efficiently without increasing too much the cost in both the training and testing phase.</p><p id="p-0185" num="0148">To evaluate the audio-visual harmony qualitatively, the dance videos are synthesized by combining the audio sequences and the generated motions from the tested models. Then the user study is conducted to compare the perceptual harmony for the synthesized videos. 12 unprofessional participants are invited to watch the video pairs from the different 3 models. The unprofessional participants are asked to vote which is better in terms of the audio-visual harmony blindly.</p><p id="p-0186" num="0149">As shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, it can be concluded that the HarmoGAN model performs best with respect to the perceptual harmony, which is consistent with the results based on the quantitative metrics. The assumption that the harmony evaluation mechanism can accurately reflect the perceptual audio-visual harmony to some degree can also be verified.</p><p id="p-0187" num="0150">As an example of qualitative evaluation, as shown in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, the generated motion sequences from the tested models are demonstrated based on the Ballroom dataset. Given the tracked audio beats, in the movements produced by [Ren et al., 2020], the distinct visual beats can hardly be perceived from the slight body swings, let alone the audio-visual consistency. When it comes to the visual results estimated from [Lee et al., 2019], reasonable visual beats can be perceived with the observation of changes between movements. However, such changes are relatively even in the whole sequences, which may suffer from the inattentional blindness and result in the perceptual inconsistency between audio-visual beats due to misjudgments. By comparison, the HarmoGAN model with the regularization of the harmony loss can produce distinct changes in motion close to the occurrences of the music beats to provide visual beats that can draw enough attention to favor the harmony evaluation based on the human perception.</p><p id="p-0188" num="0151">Other embodiments of the disclosure will be apparent to those skilled in the art from consideration of the specification and practice of the invention disclosed herein. It is intended that the specification and examples be considered as exemplary only, with a true scope and spirit of the invention being indicated by the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230005201A1-20230105-M00001.NB"><img id="EMI-M00001" he="10.58mm" wi="76.20mm" file="US20230005201A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005201A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US20230005201A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005201A1-20230105-M00003.NB"><img id="EMI-M00003" he="12.36mm" wi="76.20mm" file="US20230005201A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004 MATH-US-00004-2" nb-file="US20230005201A1-20230105-M00004.NB"><img id="EMI-M00004" he="10.58mm" wi="76.20mm" file="US20230005201A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230005201A1-20230105-M00005.NB"><img id="EMI-M00005" he="8.13mm" wi="76.20mm" file="US20230005201A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006 MATH-US-00006-2" nb-file="US20230005201A1-20230105-M00006.NB"><img id="EMI-M00006" he="10.92mm" wi="76.20mm" file="US20230005201A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007 MATH-US-00007-2" nb-file="US20230005201A1-20230105-M00007.NB"><img id="EMI-M00007" he="10.24mm" wi="76.20mm" file="US20230005201A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230005201A1-20230105-M00008.NB"><img id="EMI-M00008" he="8.47mm" wi="76.20mm" file="US20230005201A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00009" nb-file="US20230005201A1-20230105-M00009.NB"><img id="EMI-M00009" he="5.67mm" wi="76.20mm" file="US20230005201A1-20230105-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00010" nb-file="US20230005201A1-20230105-M00010.NB"><img id="EMI-M00010" he="5.67mm" wi="76.20mm" file="US20230005201A1-20230105-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00011" nb-file="US20230005201A1-20230105-M00011.NB"><img id="EMI-M00011" he="7.03mm" wi="76.20mm" file="US20230005201A1-20230105-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00012" nb-file="US20230005201A1-20230105-M00012.NB"><img id="EMI-M00012" he="7.03mm" wi="76.20mm" file="US20230005201A1-20230105-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00013" nb-file="US20230005201A1-20230105-M00013.NB"><img id="EMI-M00013" he="6.35mm" wi="76.20mm" file="US20230005201A1-20230105-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00014" nb-file="US20230005201A1-20230105-M00014.NB"><img id="EMI-M00014" he="5.25mm" wi="76.20mm" file="US20230005201A1-20230105-M00014.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00015" nb-file="US20230005201A1-20230105-M00015.NB"><img id="EMI-M00015" he="6.35mm" wi="76.20mm" file="US20230005201A1-20230105-M00015.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00016" nb-file="US20230005201A1-20230105-M00016.NB"><img id="EMI-M00016" he="4.57mm" wi="76.20mm" file="US20230005201A1-20230105-M00016.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for harmony-aware audio-driven motion synthesis, applied to a computing device, comprising:<claim-text>determining a plurality of testing meter units according to an input audio, each testing meter unit corresponding to an input audio sequence of the input audio;</claim-text><claim-text>obtaining an auditory input corresponding to each testing meter unit;</claim-text><claim-text>obtaining an initial pose of each testing meter unit as a visual input based on a visual motion sequence synthesized for a previous testing meter unit; and</claim-text><claim-text>automatically generating a harmony-aware motion sequence corresponding to the input audio using a generator of a generative adversarial network (GAN) model, the GAN model being trained by incorporating a hybrid loss function, the hybrid loss function including a multi-space pose loss, a harmony loss, and a GAN loss, the harmony loss being determined according to beat consistencies of audio-visual beat pairs.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>training a GAN model to obtain the trained GAN model includes:<claim-text>obtaining audio beats and audio beat strengths from a training sample audio, each audio beat corresponding to one audio beat strength;</claim-text><claim-text>determining a plurality of training meter units according to the audio beats and audio beat strengths, each training meter unit corresponding to a sample audio sequence of the training sample audio and a temporal index based on a time record of the training meter unit;</claim-text><claim-text>extracting features of the sample audio sequence of each training meter unit as a sample auditory input; and</claim-text><claim-text>obtaining a sample initial pose of each training meter unit as a sample visual input based on the temporal index and a training sample visual motion sequence; and</claim-text><claim-text>training the GAN model using the sample auditory input and the sample visual input of each training meter unit by incorporating the hybrid loss function, to obtain the trained GAN model, the harmony loss being determined according to beat consistencies of audio-visual beat pairs corresponding to each training meter unit, each audio beat in the audio-visual beat pairs being from the sample auditory input of the training meter unit, each visual beat in the audio-visual beat pairs being from an estimated visual motion sequence corresponding to the training meter unit generated during a process of training the GAN model.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein training the GAN model further includes:<claim-text>detecting visual beats of the estimated visual motion sequence by considering a difference between joint velocity sums in neighboring frames of the estimated visual motion sequence.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the beat consistencies of the audio-visual beat pairs corresponding to each training meter unit is determined by:<claim-text>assigning a weight to each audio beat and a weight to each visual beat based on beat saliency;</claim-text><claim-text>obtaining, among the audio-visual beat pairs of the training meter unit, attentional beats according to the weights of the audio beats and the weights of the visual beats, the attentional beats including one or more attentional audio beats and one or more attentional visual beats;</claim-text><claim-text>obtaining the beat strength for each of the attentional beats; and</claim-text><claim-text>constructing hitting scores by counting labels in an audio and visual domain to represent aligned attentional beats in the sample auditory input and the estimated visual motion sequence, one label representing that one attentional audio beat is aligned with a corresponding attentional visual beat according to a human reaction time delay; and</claim-text><claim-text>determining the beat consistencies using the hitting scores.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, training the GAN model further includes:<claim-text>segmenting audio-visual clips based on the plurality of training meter units temporally; and</claim-text><claim-text>inputting the segmented audio-visual clips for the training,</claim-text><claim-text>wherein the initial pose of each training meter unit is obtained from a corresponding audio-visual clip that contains the sample audio sequence.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein:<claim-text>the GAN model further includes a cross-domain discriminator and a spatial-temporal discriminator that jointly supervise the generator; and</claim-text><claim-text>training the GAN model further includes:<claim-text>minimizing the harmony loss, the multi-space pose loss, and the GAN loss from the generator; and</claim-text><claim-text>maximizing values of loss functions of the cross-domain discriminator and the spatial temporal discriminator to distinguish between a real training sample and a fake training sample.</claim-text></claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the multi-space pose loss includes one or more of Kullback-Leibler (KL) loss, Charbonnier-based MSE loss, and Charbonnier-based VGG loss.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the generator includes a GRU-based audio encoder and pose decoder;</claim-text><claim-text>the pose decoder is configured to:<claim-text>estimate 2D poses according to a visual motion sequence corresponding to a meter unit; and</claim-text><claim-text>construct a depth lifting branch to produce 3D poses based on the estimated 2D poses.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>determining the plurality of testing meter units according to the input audio includes:<claim-text>determining the plurality of testing meter units according to audio beats and audio beat strengths of the input audio, each testing meter unit corresponding to an audio sequence of the input audio and a temporal index based on a time record of the testing meter unit;</claim-text></claim-text><claim-text>obtaining the auditory input corresponding to each testing meter unit includes:<claim-text>extracting features of the audio sequence of each testing meter unit as the auditory input; and</claim-text></claim-text><claim-text>obtaining the initial pose of each testing meter unit as the visual input based on the visual motion sequence synthesized for the previous testing meter unit includes:<claim-text>obtaining the initial pose of each testing meter unit as the visual input based on the temporal index and the visual motion sequence synthesized for the previous testing meter unit.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein obtaining the initial pose of each testing meter unit comprises:<claim-text>keeping the generated harmony-aware visual motion sequence from a previous testing meter unit right before a current testing meter unit in the initial pose of a current testing meter unit; and</claim-text><claim-text>using a mean pose of the generated harmony-aware motion sequence from the previous testing meter unit as initialization for the current testing meter unit.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A device for harmony-aware audio-driven motion synthesis, comprising:<claim-text>a memory; and</claim-text><claim-text>a processor coupled to the memory and configured to perform a plurality of operations comprising:<claim-text>determining a plurality of testing meter units according to an input audio, each testing meter unit corresponding to an input audio sequence of the input audio;</claim-text><claim-text>obtaining an auditory input corresponding to each testing meter unit;</claim-text><claim-text>obtaining an initial pose of each testing meter unit as a visual input based on a visual motion sequence synthesized for a previous testing meter unit; and</claim-text><claim-text>automatically generating a harmony-aware motion sequence corresponding to the input audio using a generator of a generative adversarial network (GAN) model, the GAN model being trained by incorporating a hybrid loss function, the hybrid loss function including a multi-space pose loss, a harmony loss, and a GAN loss, the harmony loss being determined according to beat consistencies of audio-visual beat pairs.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the plurality of operations performed by the processor further comprises:<claim-text>training the GAN model, including:<claim-text>obtaining audio beats and audio beat strengths from a training sample audio, each audio beat corresponding to one audio beat strength;</claim-text><claim-text>determining a plurality of training meter units according to the audio beats and audio beat strengths, each training meter unit corresponding to a sample audio sequence of the training sample audio and a temporal index based on a time record of the training meter unit;</claim-text><claim-text>extracting features of the sample audio sequence of each training meter unit as a sample auditory input; and</claim-text><claim-text>obtaining a sample initial pose of each training meter unit as a sample visual input based on the temporal index and a training sample visual motion sequence; and</claim-text><claim-text>training the GAN model using the sample auditory input and the sample visual input of each training meter unit by incorporating the hybrid loss function, to obtain the trained GAN model, the harmony loss being determined according to beat consistencies of audio-visual beat pairs corresponding to each training meter unit, each audio beat in the audio-visual beat pairs being from the sample auditory input of the training meter unit, each visual beat in the audio-visual beat pairs being from an estimated visual motion sequence corresponding to the training meter unit generated during a process of training the GAN model.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein training the GAN model further includes:<claim-text>detecting visual beats of the estimated visual motion sequence by considering a difference between joint velocity sums in neighboring frames of the estimated visual motion sequence.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the beat consistencies of the audio-visual beat pairs corresponding to each training meter unit is determined by:<claim-text>assigning a weight to each audio beat and a weight to each visual beat based on beat saliency;</claim-text><claim-text>obtaining, among the audio-visual beat pairs of the training meter unit, attentional beats according to the weights of the audio beats and the weights of the visual beats, the attentional beats including one or more attentional audio beats and one or more attentional visual beats;</claim-text><claim-text>obtaining the beat strength for each of the attentional beats; and</claim-text><claim-text>constructing hitting scores by counting labels in an audio and visual domain to represent aligned attentional beats in the sample auditory input and the estimated visual motion sequence, one label representing that one attentional audio beat is aligned with a corresponding attentional visual beat according to a human reaction time delay; and</claim-text><claim-text>determining the beat consistencies using the hitting scores.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, training the GAN model further includes:<claim-text>segmenting audio-visual clips based on the plurality of training meter units temporally; and</claim-text><claim-text>inputting the segmented audio-visual clips for the training,</claim-text><claim-text>wherein the initial pose of each training meter unit is obtained from a corresponding audio-visual clip that contains the sample audio sequence.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein:<claim-text>the GAN model further includes a cross-domain discriminator and a spatial-temporal discriminator that jointly supervise the generator; and</claim-text><claim-text>training the GAN model further includes:<claim-text>minimizing the harmony loss, the multi-space pose loss, and the GAN loss from the generator; and</claim-text><claim-text>maximizing values of loss functions of the cross-domain discriminator and the spatial temporal discriminator to distinguish between a real training sample and a fake training sample.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the multi-space pose loss includes one or more of Kullback-Leibler (KL) loss, Charbonnier-based MSE loss, and Charbonnier-based VGG loss.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein:<claim-text>the generator includes a GRU-based audio encoder and pose decoder;</claim-text><claim-text>the pose decoder is configured to:<claim-text>estimate 2D poses according to a visual motion sequence corresponding to a meter unit; and</claim-text><claim-text>construct a depth lifting branch to produce 3D poses based on the estimated 2D poses.</claim-text></claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein:<claim-text>determining the plurality of testing meter units according to the input audio includes:<claim-text>determining the plurality of testing meter units according to audio beats and audio beat strengths of the input audio, each testing meter unit corresponding to an audio sequence of the input audio and a temporal index based on a time record of the testing meter unit;</claim-text></claim-text><claim-text>obtaining the auditory input corresponding to each testing meter unit includes:<claim-text>extracting features of the audio sequence of each testing meter unit as the auditory input; and</claim-text></claim-text><claim-text>obtaining the initial pose of each testing meter unit as the visual input based on the visual motion sequence synthesized for the previous testing meter unit includes:<claim-text>obtaining the initial pose of each testing meter unit as the visual input based on the temporal index and the visual motion sequence synthesized for the previous testing meter unit.</claim-text></claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The device according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein obtaining the initial pose of each testing meter unit comprises:<claim-text>keeping the generated harmony-aware visual motion sequence from a previous testing meter unit right before a current testing meter unit in the initial pose of a current testing meter unit; and</claim-text><claim-text>using a mean pose of the generated harmony-aware motion sequence from the previous testing meter unit as initialization for the current testing meter unit.</claim-text></claim-text></claim></claims></us-patent-application>