<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005095A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005095</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943047</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>1</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>13</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>15</main-group><subgroup>78</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>1</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>1</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>13</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>15</main-group><subgroup>7825</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>1</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYNCHRONIZED DATA CHAINING USING ON-CHIP CACHE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16609084</doc-number><date>20191028</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11443402</doc-number></document-id></parent-grant-document><parent-pct-document><document-id><country>WO</country><doc-number>PCT/US2018/063778</doc-number><date>20181204</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17943047</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62594264</doc-number><date>20171204</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Google LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Dodge</last-name><first-name>Benjamin</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Redgrave</last-name><first-name>Jason Rupert</first-name><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Ma</last-name><first-name>Xiaoyu</first-name><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for generating, by an image sensor of a computing device, frame data comprising sub-frames of image pixel data. A first resource of the system-on-chip provides the frame data to a second resource of the system-on-chip. The frame data is provided to the second resource using a first data path included in the system-on-chip. The first resource provides a token to the second resource using a second data path included in the system-on-chip. A processor of the system-on-chip, uses the token to synchronize production of sub-frames of image pixel data provided by the first resource to the second resource and to synchronize consumption of the sub-frames of image pixel data received by the second resource from the elastic memory buffer.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="149.52mm" wi="124.29mm" file="US20230005095A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="229.45mm" wi="170.18mm" orientation="landscape" file="US20230005095A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="226.91mm" wi="153.92mm" orientation="landscape" file="US20230005095A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="200.15mm" wi="126.32mm" file="US20230005095A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="228.85mm" wi="172.13mm" orientation="landscape" file="US20230005095A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation application of, and claims priority to, U.S. application Ser. No. 16/609,084, filed on Oct. 28, 2019, which is a national stage application under 35 U.S.C. &#xa7; 371 and which claims priority to International Application No. PCT/US2018/063778, filed on Dec. 4, 2018, which claims the benefit under 35 U.S.C. &#xa7; 119(e) of U.S. Provisional Application No. 62/594,264, filed on Dec. 4, 2017. The disclosures of the prior applications are considered part of and are incorporated by reference in the disclosure of this application.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">This specification relates to computing processes for a system-on-chip.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Computing devices can include at least one system-on-chip component having a memory system and a communication network. For example, some mobile computing devices may include a cache, a direct memory access unit (DMA), static/dynamic random access memory (SRAM/DRAM), or combinations of each. Computing devices may perform data processing functions by using DMAs to execute memory write and memory read operations. For example, computing devices may process image pixel data by using a DMA to access DRAM resources. The computing device can access the data and use an on-chip communication network to perform actions related to rendering an image for display at a computing device in response to touch input from a user.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">This application is directed toward a computing scheme where producer and consumer models of a user device are configured to more efficiently use a system level cache to perform sub-frame data sharing. The computing scheme involves using tokens and data flow characteristics of device resources to address latency and power consumption challenges that can arise when the user device processes various types of data (e.g., image data generated by an image sensor). A system level cache is used as a buffer that stores sub-frames of data that are exchanged between resources of the device, such as a producer resource and a consumer resource. For example, as data is exchanged, tokens are also passed to synchronize and manage how sub-frames of data move between a data producer and a data consumer.</p><p id="p-0006" num="0005">One aspect of the subject matter described in this specification can be embodied in a computer-implemented method. The method includes, generating, by an image sensor of a computing device, frame data comprising sub-frames of image pixel data and providing, by a first resource of the system-on-chip, the frame data to at least one second resource of the system-on-chip, wherein the frame data is provided to the at least one second resource using an elastic memory buffer and a first data path included in the system-on-chip. The method further includes providing, by the first resource of the system-on-chip, a token to the at least one second resource of the system-on-chip, wherein the token is provided to the at least one second resource using a second data path included in the system-on-chip. The method also includes, using, by a processor of the system-on-chip, the token provided using the second data path to: i) synchronize production of sub-frames of image pixel data provided by the first resource to the at least one second resource; and ii) synchronize consumption of the sub-frames of image pixel data received by the at least one second resource from the elastic memory buffer.</p><p id="p-0007" num="0006">These and other implementations can each optionally include one or more of the following features. For example, in some implementations, the method further includes: providing, by the at least one second resource of the system-on-chip and using the second data path, a return token to the first resource, wherein the return token corresponds to the token provided by the first resource, and the return token is used along with the token to synchronize production of the sub-frames of image pixel data.</p><p id="p-0008" num="0007">In some implementations, the first resource is a producer resource that uses a producer direct memory access unit to: i) provide the frame data using the first data path; ii) provide the token using the second data path; and iii) receive the return token using the second data path. In some implementations, the at least one second resource is a consumer resource that uses a consumer direct memory access unit to: i) consume the frame data provided using the first path; ii) receive the token provided using the second path; and iii) provide the return token using the second data path.</p><p id="p-0009" num="0008">In some implementations, the processor allocates a variable amount of memory included in the elastic memory buffer, the memory being used for transfers of sub-frame data between multiple respective producer-consumer pairs; and the processor adjusts an initial amount of allocated memory based on a quantity of producer-consumer pairs that actively transfer sub-frame data, and wherein the sub-frame data has a data size that is less than a threshold data size.</p><p id="p-0010" num="0009">In some implementations, the first resource provides the frame data to the at least one second resource via the first data path based on a sub-frame data consumption rate of the at least one second resource, wherein the sub-frame data consumption rate is computed by the processor at least by using the return token.</p><p id="p-0011" num="0010">In some implementations, the method further includes: i) determining, by the processor and using the return token, a production rate by which the first resource is required to produce frame data to synchronize production of the sub-frames of image pixel data; and ii) providing, by the first resource, the frame data based on the determined production rate.</p><p id="p-0012" num="0011">In some implementations, the method further includes: i) determining, by the processor and using the token, a consumption rate by which the at least one second resource is required to consume sub-frames of data to synchronize consumption of the sub-frames of image pixel data; and ii) consuming, by the at least one second resource, the sub-frames of data based on the determined consumption rate.</p><p id="p-0013" num="0012">In some implementations, the first path and the second path are part of the same data communications network included in the system-on-chip. In other implementations, the second path is formed at the system-on-chip using one of multiple virtual channels included in the system-on-chip.</p><p id="p-0014" num="0013">Other implementations of this and other aspects include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices. A computing system of one or more computers or hardware circuits can be so configured by virtue of software, firmware, hardware, or a combination of them installed on the system that in operation cause the system to perform the actions. One or more computer programs can be so configured by virtue of having instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.</p><p id="p-0015" num="0014">The subject matter described in this specification can be implemented in particular implementations and can result in one or more of the following advantages. The described subject matter includes a computing scheme where producer and consumer models are configured to more efficiently use a system level cache for sub-frame data sharing, relative to conventional computing schemes used by an electronic device. The computing scheme described in this document leverages data flow characteristics of memory resources in a computing device to address latency and power consumption challenges that arise during certain data processing and memory access operations.</p><p id="p-0016" num="0015">For example, the computing scheme includes coupling a producer and consumer at a finer grain than full data frames. Based on this coupling, a footprint of data is minimized to enable elastic buffering of sub-frame data in a system level cache. Customized caching features allow for the cache to be used as a scratch elastic buffer without requiring writing back data that is already consumed to DRAM or data that is no-longer-needed data to DRAM. The described subject matter can be implemented to realize multiple computing efficiencies over current/conventional on-chip systems. For example, the computing scheme of this document enables reduced latency to user input, increased on chip system throughput, reduced processor utilization, and improved power consumption.</p><p id="p-0017" num="0016">The details of one or more implementations of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a computing system for performing data chaining using on-chip memory resources.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows example visuals of sub-frame data chaining.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of an example process for performing data chaining using on-chip memory resources.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of a computing system that can be used in connection with computer-implemented methods described in this specification.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0022" num="0021">Like reference numbers and designations in the various drawings indicate like elements.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a block diagram of a computing system <b>100</b> for performing data chaining using on-chip memory resources. System <b>100</b> includes a user device <b>102</b> and computing resources that form a system on-chip (SoC) <b>104</b> (&#x201c;SoC <b>104</b>&#x201d;). In some implementations, SoC <b>104</b> is located in user device <b>104</b>. SoC <b>104</b> generally includes an image signal processor (ISP) <b>106</b> (&#x201c;ISP <b>106</b>&#x201d;), a processing block (PB) <b>108</b> (&#x201c;PB <b>108</b>&#x201d;), a fabric <b>128</b>, a system level cache (SLC)<b>130</b> (&#x201c;SLC <b>130</b>&#x201d;), and a memory controller <b>132</b>. ISP <b>106</b> includes a first direct memory access unit <b>110</b> (&#x201c;DMA <b>110</b>&#x201d;), an image sensor <b>114</b>, a frontend data pipeline <b>116</b>, and a backend data pipeline <b>118</b>. PB <b>108</b> includes a second direct memory access unit <b>112</b> (&#x201c;DMA <b>112</b>&#x201d;) and an image processor unit (IPU) <b>120</b> (&#x201c;IPU <b>120</b>&#x201d;).</p><p id="p-0024" num="0023">Although described herein with reference to image sensors and image pixel data, computing processes described in this specification can be applied to the processing of various other types of data. For example, a user device can leverage the described computing scheme to synchronize data produced and consumed between respective resources of the device, e.g., audio data between an audio sensor and a digital signal processor, to realize improvements in data processing latency and device power consumption.</p><p id="p-0025" num="0024">Sensor <b>114</b> generates image pixel data that is transferred and/or stored within system <b>100</b> using various memory and data processing resources of SoC <b>104</b>. For example, pixel data moves, or flows, within SoC <b>104</b> based on control signals generated by each of DMA <b>110</b> and DMA <b>112</b>. To facilitate data transfers, DMA <b>110</b> and <b>112</b> execute master control logic for managing and executing memory read operations and memory write operations within SoC <b>104</b>. For example, each of DMA <b>110</b> and <b>112</b> can function as a DMA master that generates control signals for managing the production and consumption of pixel data.</p><p id="p-0026" num="0025">In some implementations, ISP <b>106</b> is a producer of image pixel data and DMA <b>110</b> is a DMA master that manages flow control requirements for data produced by ISP <b>106</b>, while PB <b>108</b> is a consumer of image pixel data and DMA <b>112</b> is a DMA master that manages flow control requirements for data consumed by PB <b>108</b>. In alternative implementations, ISP <b>106</b> may be a consumer of image pixel data and DMA <b>110</b> is a DMA master that manages flow control requirements for data consumed by ISP <b>106</b>, while PB <b>108</b> may be a producer of image pixel data and DMA <b>112</b> is a DMA master that manages flow control requirements for data produced by PB <b>108</b>. Fabric <b>128</b> interacts at least with DMA <b>110</b> and DMA <b>112</b> to move image pixel data within system <b>100</b>.</p><p id="p-0027" num="0026">DMA <b>110</b> provides image pixel data to frontend pipeline <b>116</b>. Image pixel data is received and processed at pipeline <b>116</b> before being transferred to backend pipeline <b>118</b> where additional pixel data processing operations can occur. DMA <b>110</b> causes processed pixel data to be moved or transferred to various other memory and data processing resources of SoC <b>104</b>. Pixel data processed using pipelines <b>116</b> and <b>118</b> can represent frame data produced by ISP <b>106</b> for consumption by PB <b>108</b>. In some implementations, frame data produced by ISP <b>106</b> can form items of digital or media content, such as video stream content or digital image/photo content. For example, PB <b>108</b> receives or consumes frame data and uses IPU <b>120</b> to generate representations of media content that can be displayed at user device <b>102</b>.</p><p id="p-0028" num="0027">In one implementation, connecting two DMA masters, e.g., DMA <b>110</b>, <b>112</b>, in a producer and consumer data flow model can involve double buffering the frame data to fully utilize the producer and consumer resources and meet use case requirements of a computing system. However, in some multimedia use cases, double buffering can introduce frame delays between production and consumption and increase processor latency. In some instances, when DMA masters are connected to user-controlled inputs (e.g., touch-sensitive devices, digital cameras, etc.) compound frame delays can occur and degrade user experience. In addition, double buffering typically requires write operations and read operations of large buffers to DRAM resources of a system and there is often a direct system power cost that is associated with these DRAM transfers.</p><p id="p-0029" num="0028">Further, to meet the DRAM bandwidth needs of a given use case, a system may have to run at a higher processor operating point (e.g., frequency and voltage) thereby reducing the system's power efficiency. In some implementations, double buffering may present data flow control challenges. For example, when a producer-consumer pair exchanges data across a frame, system <b>100</b> must ensure that the frame has been produced completely before a consumer receives a signal indicating that consumption of the frame data can occur.</p><p id="p-0030" num="0029">To address power and latency challenges with frame data transfers, a computing scheme/process is described where producer and consumer models are configured to efficiently use SLC <b>130</b> for sub-frame data sharing. SLC <b>130</b> can be memory embedded at, or integrated in, a microprocessor and for storing information accessed by SoC <b>104</b>. The microprocessor can be an example processing unit as described in this document, e.g., a central processing unit (CPU) or graphics processing unit (GPU). In some implementations, SLC <b>130</b> is a CPU or GPU cache used by a processing unit of a system <b>100</b> (or SoC <b>104</b>) to reduce an amount of time and/or energy that may be required to access data from a main memory of system <b>100</b>.</p><p id="p-0031" num="0030">The computing scheme leverages data flow characteristics of memory resources of user device <b>102</b> to address latency and power consumption challenges that can arise when processing frame data. The computing scheme includes coupling a producer and consumer pair (e.g., ISP <b>106</b> and PB <b>108</b>) at a finer grain than full data frames. Based on this coupling, a footprint of transfer frame data is minimized to enable elastic buffering of the data in a SLC <b>130</b>. Customized caching features allow for SLC <b>130</b> to be used as a scratch elastic buffer without requiring writing back consumed or no-longer-needed (dirty) data to DRAM.</p><p id="p-0032" num="0031">Using the described techniques to couple producer and consumer pairs for sub-frame data transfers results in reduced processor latency as well as reductions in data flow footprint and transient storage needs. In some implementations, the computing scheme can be implemented to keep end-to-end latency between a producer and a consumer smaller than full frame latency. A net effect of computing process is that bandwidth to DRAM can be reduced and any user-visible frame latency can be also reduced. The described flow control scheme can involve implementing certain modifications to traditional producer-consumer DMAs to maximize energy savings and reduced latency for a sub-frame data sharing model.</p><p id="p-0033" num="0032">Referring again to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, producer and consumer DMAs <b>110</b>, <b>112</b> are configured to issue their respective read and write operations to SLC <b>130</b> and other buffering resources included in the memory system of SoC <b>104</b>. For example, each of DMAs <b>110</b>, <b>112</b> can issue read/write operations through traditional mechanisms and interfaces, such as fabric <b>128</b> and memory controller <b>132</b>. A flow control mechanism can be added between the producer and consumer DMA masters for use in synchronizing frame data exchanges and transfers between a producer-consumer pair. The flow control mechanism ensures that the consumer does not race ahead of the producer or consume frame data faster than a rate at which the producer produces the data. Likewise, the flow control mechanism can also ensure that the producer does not race ahead of the consumer or produce frame data faster than a rate at which the consumer consumes the data.</p><p id="p-0034" num="0033">As a producer DMA, e.g., DMA <b>110</b>, writes image pixel data to the memory system of SoC <b>104</b>, the producer DMA uses flow control tokens to ensure that certain sub-frames of pixel data are available and/or globally visible. For example, a processor device of system <b>100</b> can execute programmed instructions for a specific flow control computing scheme. The computing scheme is used to control the flow of sub-frame data within SoC <b>104</b> and to control the distribution of synchronization tokens to regulate and manage the flow of sub-frame data within SoC <b>104</b>. For example, the control scheme can cause the producer DMA to generate control signals for emitting or passing synchronization tokens received by a consumer, e.g., PB <b>108</b>. Alternatively, the control scheme can cause a consumer DMA, e.g., DMA <b>112</b>, to generate control signals for emitting or passing synchronization tokens to a producer, e.g., ISP <b>106</b>.</p><p id="p-0035" num="0034">Synchronization tokens provided by a producer to a consumer can signify completed bytes or indicate that production of sub-frame data is complete. Likewise, synchronization tokens provided by a consumer to a producer can signify completed processing of bytes or indicate that consumption of sub-frame data is complete. In some implementations, a consumer DMA only proceeds to issue memory system requests (e.g., for reading data from a memory resource) when the consumer DMA detects that it has a sufficient token(s) to initiate the request. In other implementations, when a consumer completes work on a portion of frame data, the consumer can release one or more tokens back to the producer that may have provided the tokens. Hence, this token based flow control mechanism enabled by the control scheme is configured for use by both the producer and the consumer when required.</p><p id="p-0036" num="0035">Synchronization tokens are transferred, moved, or otherwise passed throughout SoC <b>104</b> using a token switch network <b>122</b>. Network <b>122</b> provides an interconnected data communications path for transferring synchronization tokens between computing resources of system <b>100</b>. In some implementations, synchronization tokens are exchanged between a producer and a consumer using a token switch network <b>122</b> that is configured as a separate network or data path of SoC <b>104</b>. For example, frame data, e.g., full-frame image pixel data or sub-frame image pixel data, can be transferred between resources of SoC <b>104</b> using a first data communications network, while synchronization tokens can be transferred between resources of SoC <b>104</b> using a second data communications network.</p><p id="p-0037" num="0036">In some implementations, the second data communications network is different than, or separate from, the first data communications network. In other implementations, the second data communications network is the same as, or interconnected with, the first data communications network. As shown at <figref idref="DRAWINGS">FIG. <b>1</b></figref>, data communication line <b>124</b> can represent producer data being transferred within SoC <b>104</b>, data communication line <b>126</b> can represent consumer data being transferred within SoC <b>104</b>, and data communication line <b>127</b> can represent synchronization tokens being transferred within SoC <b>104</b>. In some implementations, communication lines <b>124</b> and <b>126</b> are associated with the first data communications network, while communication line <b>127</b> is associated with the second, different data communications network.</p><p id="p-0038" num="0037">Fabric <b>128</b> can be used to pass or move frame data and synchronization tokens within SoC <b>104</b>. In general, fabric <b>128</b> is configured to support multiple simultaneous data connections between independent producer-consumer pairs of SoC <b>104</b> as well as between multiple different computing resources of SoC <b>104</b>. In some implementations, fabric <b>128</b> is configured to have guaranteed forward progress across all resource connections that exist within SoC <b>104</b>. In addition, to meet certain system latency requirements, fabric <b>128</b> can be also configured to have a bounded latency that is less than a threshold bound latency of the system <b>100</b> (e.g., less than approximately <b>500</b> nanoseconds (ns)).</p><p id="p-0039" num="0038">As noted above, producer and consumer models use SLC <b>130</b> for sub-frame data sharing. Customized caching features of system <b>100</b> allow for SLC <b>130</b> to be used as a scratch elastic buffer without requiring extraneous write and read operations involving DRAM. In some implementations, to minimize overall latency and therefore reduce the frame data footprint of the elastic buffer in SLC <b>130</b>, a computing scheme is configured to ensure a producer and consumer pair follow the same address walk order. Moreover, a detected differences in address walk order is identified or made coherent by the respective DMA(s) of the producer and the consumer. For example, if a producer is tiled in row-major order and the consumer is line-based also in row-major, the producer releases tokens after operations for a row of tiles has finished. In some implementations, tokens are released and consumed between producer-consumer pairs in quantized chunks to meet the respective processing needs of the producer and the consumer. The released tokens are provided to the consumer as a flow control mechanism to trigger or cause the consumer to begin obtaining and consuming the available data stored in the buffer.</p><p id="p-0040" num="0039">In other implementations, to maximize DRAM bandwidth savings, SLC <b>130</b> is configured to support one or more mechanisms to minimize, or entirely avoid, both missing a cache on data-chained reads and writing data-chained data back to DRAM. The control scheme can be configured to reduce interference between use cases in which data flows using DRAM and use cases in which sub-frame data-chaining occurs through SLC <b>130</b>. For example, potentially high bandwidth data-chained flows can be configured to occur using one or more separate virtual channels of SoC <b>104</b>. For example, SoC <b>104</b> can include multiple separate virtual channels (VCs) that are configured so high bandwidth data-chained flows cannot block, or be blocked, by other VCs that are making use of DRAM based data transfers. In some implementations, at least one VC can be used exclusively for sub-frame data chaining as well as for low-latency central processing unit (CPU) requests. In other implementations, SoC <b>104</b> can have a bus topology such that these different request sources can effectively be on separate physical channels when data chaining operations achieves the expected high hit rates.</p><p id="p-0041" num="0040">In some implementations, <figref idref="DRAWINGS">FIG. <b>1</b></figref> represents an operating scenario in which sensor frame data is flowing to PB <b>108</b>, in real-time (RT) so as to represent real-time sensor frame data. In this operating scenario, system <b>100</b> supplies or provisions real-time bandwidth requirements measured over time windows on the order of microseconds (&#x3bc;s) (e.g., approximately 10 &#x3bc;s). Likewise, system <b>100</b> can provision sufficient bandwidth to meet the average data frame needs of these real time (or non-real-time) use cases on the order of milliseconds (ms) (e.g., 10 ms). In some implementations, the same bandwidth is delivered over windows on the order of approximately 100 &#x3bc;s. For typical line times and bytes-per-pixel, 100 us of real-time data can have a footprint that ranges from approximately 100 kilobytes (KB) to 200 KB (e.g., 100 &#x3bc;s*5,000 pixels per line*1.5 Bpp/5 &#x3bc;s per line). When SLC <b>130</b> can accommodate this data footprint, the real-time frame data stream from sensor <b>114</b> may be terminated at SLC <b>130</b>/DRAM without requiring the consumer to also consume the frame data in real-time.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows example visuals of a data flow diagram <b>200</b> that are associated with sub-frame data chaining. As shown, a data frame <b>202</b> can include at least two sub-frame blocks <b>204</b>, <b>206</b> that are chained for data transfer within SoC <b>104</b> using SLC <b>130</b>. Data flow diagram <b>200</b> shows a three hop data flow process where the first two sub-frame hops (involving blocks <b>204</b> and <b>206</b>) chain data from ISP <b>106</b> to PB <b>108</b>, and from PB <b>108</b> to ISP <b>106</b>, through SLC <b>130</b>. In some implementations, the last frame data hop from ISP <b>106</b> to display block <b>208</b> can be a traditional frame-based data flow.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of an example process <b>300</b> for performing data chaining using on-chip memory resources. The process <b>300</b> includes a sensor of device <b>102</b> generating frame data that includes sub-frames of data (<b>302</b>). For example, image sensor <b>114</b> of the ISP <b>106</b> generates frame data that includes sub-frames of image pixel data for processing at system <b>100</b>. The sub-frames of image pixel data are transferred, processed, and/or stored using the computing resources of system <b>100</b>. In some implementations, the sub-frames of image pixel data are processed to render, at user device <b>102</b>, graphical image representations of media content, such as video stream content or digital image/photo content.</p><p id="p-0044" num="0043">A first resource of SoC <b>104</b> provides the frame data to a second resource of the system-on-chip (<b>304</b>). For example, ISP <b>106</b> can be a first resource of system <b>100</b> that provides the frame data to a second resource of system <b>100</b>, while PB <b>108</b> can be a second resource of system <b>100</b> that receives the frame data from a first resource such as ISP <b>106</b>. In some implementations, other components, or even combinations of components, in system <b>100</b> can represent a first resource or a second resource. For example, sensor <b>114</b>, SLC <b>130</b>, or IPU <b>120</b> can each represent a respective first resource or second resource that can provide or receive frame data.</p><p id="p-0045" num="0044">In some examples, the frame data is provided to the second resource using an elastic memory buffer and a first data path included in the system-on-chip. For example, ISP <b>106</b> represents a first resource (e.g., a producer resource) that uses a producer DMA, such as DMA <b>110</b>, to provide the frame data using the first data path and an elastic memory buffer that corresponds to SLC <b>130</b>. The first data path can correspond to communication line <b>124</b>, communication line <b>126</b>, or a combination of both. Memory of SLC <b>130</b> is used for transfers of sub-frame data between multiple respective producer-consumer pairs of system <b>100</b>. To facilitate the data transfers in a more efficient manner, an example processor of SoC <b>104</b> can vary an amount of allocated memory included in the SLC <b>130</b> that is used to move or transfer the frame data.</p><p id="p-0046" num="0045">For example, a certain amount of memory may be initially allocated. Referencing the amount, the processor can adjust this initial amount of allocated memory based on a quantity of producer-consumer pairs that actively transfer sub-frame data at the SoC <b>104</b>. The sub-frame data can have a data size that is less than a threshold data size, where the threshold data size is determined based on a memory capacity of the allocated memory or an overall memory capacity of the SLC <b>130</b>.</p><p id="p-0047" num="0046">The first resource provides a first token to the second resource using a second data path included in the system-on-chip (<b>306</b>). ISP <b>106</b> (e.g., first/producer resource) can provide a first token to PB <b>108</b> (e.g., second/consumer resource) using a data transfer network <b>122</b> that corresponds to the second data path of SoC <b>104</b>. Further, a second resource of SoC <b>104</b> uses the second data path to provide a return token to the first resource. The return token is related to the first token provided by the first resource. As described herein, the return token is used along with the first token to synchronize production and consumption of the sub-frames of data to render image content at a display of a user device or to output audio using a speaker of the user device.</p><p id="p-0048" num="0047">In some implementations, as frame data is exchanged between the first and second resources, one or more tokens are also passed between the first and second resources to synchronize and manage the transfer of data sub-frames between resources that are producers of data and other resources that are consumers of data.</p><p id="p-0049" num="0048">A processor of the system-on-chip, uses the first token to synchronize production of the sub-frames of image pixel data provided by the first resource to the second resource and to synchronize consumption of the sub-frames of image pixel data received by the second resource from the elastic memory buffer (<b>308</b>). For example, as a producer DMA <b>110</b> writes image pixel data to memory of SoC <b>104</b>, the producer DMA uses flow control tokens to ensure certain sub-frames of pixel data are available and/or globally visible for processing or consumption by other resources of SoC <b>104</b>. The other resources include at least SLC <b>130</b> and consumer DMA <b>112</b>. In some cases, the first token represents a synchronization token provided by a producer to a consumer to signify completed bytes or indicate that production of sub-frame data is complete. Likewise, the return token represents a synchronization token provided by a consumer to a producer to signify completed processing of bytes or to indicate that consumption of sub-frame data is complete.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of computing devices <b>400</b>, <b>450</b> in which the systems and methods described in this document may be advantageously implemented. Computing device <b>400</b> is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. Computing device <b>450</b> is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, smart watches, head-worn devices, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations described and/or claimed in this document.</p><p id="p-0051" num="0050">Computing device <b>400</b> includes a processor <b>402</b>, memory <b>404</b>, a storage device <b>406</b>, a high-speed interface <b>408</b> connecting to memory <b>404</b> and high-speed expansion ports <b>410</b>, and a low speed interface <b>412</b> connecting to low speed bus <b>414</b> and storage device <b>406</b>. Each of the components <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b>, <b>410</b>, and <b>412</b>, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor <b>402</b> can process instructions for execution within the computing device <b>400</b>, including instructions stored in the memory <b>404</b> or on the storage device <b>406</b> to display graphical information for a GUI on an external input/output device, such as display <b>416</b> coupled to high speed interface <b>408</b>. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices <b>400</b> may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).</p><p id="p-0052" num="0051">The memory <b>404</b> stores information within the computing device <b>400</b>. In one implementation, the memory <b>404</b> is a computer-readable medium. In one implementation, the memory <b>404</b> is a volatile memory unit or units. In another implementation, the memory <b>404</b> is a non-volatile memory unit or units.</p><p id="p-0053" num="0052">The storage device <b>406</b> is capable of providing mass storage for the computing device <b>400</b>. In one implementation, the storage device <b>406</b> is a computer-readable medium. In various different implementations, the storage device <b>406</b> may be a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory <b>404</b>, the storage device <b>406</b>, or memory on processor <b>402</b>.</p><p id="p-0054" num="0053">The high-speed controller <b>408</b> manages bandwidth-intensive operations for the computing device <b>400</b>, while the low speed controller <b>412</b> manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In one implementation, the high-speed controller <b>408</b> is coupled to memory <b>404</b>, display <b>416</b> (e.g., through a graphics processor or accelerator), and to high-speed expansion ports <b>410</b>, which may accept various expansion cards (not shown). In the implementation, low-speed controller <b>412</b> is coupled to storage device <b>406</b> and low-speed expansion port <b>414</b>. The low-speed expansion port, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.</p><p id="p-0055" num="0054">The computing device <b>400</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server <b>420</b>, or multiple times in a group of such servers. It may also be implemented as part of a rack server system <b>424</b>. In addition, it may be implemented in a personal computer such as a laptop computer <b>422</b>. Alternatively, components from computing device <b>400</b> may be combined with other components in a mobile device (not shown), such as device <b>450</b>. Each of such devices may contain one or more of computing device <b>400</b>, <b>450</b>, and an entire system may be made up of multiple computing devices <b>400</b>, <b>450</b> communicating with each other.</p><p id="p-0056" num="0055">Computing device <b>450</b> includes a processor <b>452</b>, memory <b>464</b>, an input/output device such as a display <b>454</b>, a communication interface <b>466</b>, and a transceiver <b>468</b>, among other components. The device <b>450</b> may also be provided with a storage device, such as a microdrive or other device, to provide additional storage. Each of the components <b>450</b>, <b>452</b>, <b>464</b>, <b>454</b>, <b>466</b>, and <b>468</b>, are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.</p><p id="p-0057" num="0056">The processor <b>452</b> can process instructions for execution within the computing device <b>450</b>, including instructions stored in the memory <b>464</b>. The processor may also include separate analog and digital processors. The processor may provide, for example, for coordination of the other components of the device <b>450</b>, such as control of user interfaces, applications run by device <b>450</b>, and wireless communication by device <b>450</b>.</p><p id="p-0058" num="0057">Processor <b>452</b> may communicate with a user through control interface <b>458</b> and display interface <b>456</b> coupled to a display <b>454</b>. The display <b>454</b> may be, for example, a TFT LCD display or an OLED display, or other appropriate display technology. The display interface <b>456</b> may comprise appropriate circuitry for driving the display <b>454</b> to present graphical and other information to a user. The control interface <b>458</b> may receive commands from a user and convert them for submission to the processor <b>452</b>. In addition, an external interface <b>462</b> may be provided in communication with processor <b>452</b>, so as to enable near area communication of device <b>450</b> with other devices. External interface <b>462</b> may provide, for example, for wired communication (e.g., via a docking procedure) or for wireless communication (e.g., via Bluetooth or other such technologies).</p><p id="p-0059" num="0058">The memory <b>464</b> stores information within the computing device <b>450</b>. In one implementation, the memory <b>464</b> is a computer-readable medium. In one implementation, the memory <b>464</b> is a volatile memory unit or units. In another implementation, the memory <b>464</b> is a non-volatile memory unit or units. Expansion memory <b>474</b> may also be provided and connected to device <b>450</b> through expansion interface <b>472</b>, which may include, for example, a SIMM card interface. Such expansion memory <b>474</b> may provide extra storage space for device <b>450</b>, or may also store applications or other information for device <b>450</b>. Specifically, expansion memory <b>474</b> may include instructions to carry out or supplement the processes described above, and may include secure information also. Thus, for example, expansion memory <b>474</b> may be provided as a security module for device <b>450</b>, and may be programmed with instructions that permit secure use of device <b>450</b>. In addition, secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.</p><p id="p-0060" num="0059">The memory may include for example, flash memory and/or MRAM memory, as discussed below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory <b>464</b>, expansion memory <b>474</b>, or memory on processor <b>452</b>.</p><p id="p-0061" num="0060">Device <b>450</b> may communicate wirelessly through communication interface <b>466</b>, which may include digital signal processing circuitry where necessary. Communication interface <b>466</b> may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA<b>2000</b>, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver <b>468</b>. In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, GPS receiver module <b>470</b> may provide additional wireless data to device <b>450</b>, which may be used as appropriate by applications running on device <b>450</b>.</p><p id="p-0062" num="0061">Device <b>450</b> may also communicate audibly using audio codec <b>460</b>, which may receive spoken information from a user and convert it to usable digital information. Audio codec <b>460</b> may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device <b>450</b>. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device <b>450</b>. The computing device <b>450</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone <b>480</b>. It may also be implemented as part of a smartphone <b>482</b>, personal digital assistant, or other similar mobile device.</p><p id="p-0063" num="0062">Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs, computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.</p><p id="p-0064" num="0063">These computer programs, also known as programs, software, software applications or code, include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms &#x201c;machine-readable medium&#x201d; &#x201c;computer-readable medium&#x201d; refers to any computer program product, apparatus and/or device, e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term &#x201c;machine-readable signal&#x201d; refers to any signal used to provide machine instructions and/or data to a programmable processor.</p><p id="p-0065" num="0064">To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.</p><p id="p-0066" num="0065">As discussed above, systems and techniques described herein can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component such as an application server, or that includes a front-end component such as a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here, or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication such as, a communication network. Examples of communication networks include a local area network (&#x201c;LAN&#x201d;), a wide area network (&#x201c;WAN&#x201d;), and the Internet.</p><p id="p-0067" num="0066">The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. Further to the descriptions above, a user may be provided with controls allowing the user to make an election as to both if and when systems, programs or features described herein may enable collection of user information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location), and if the user is sent content or communications from a server.</p><p id="p-0068" num="0067">A number of embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the invention. For example, various forms of the flows shown above may be used, with steps re-ordered, added, or removed. Accordingly, other embodiments are within the scope of the following claims. While this specification contains many specific implementation details, these should not be construed as limitations on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment.</p><p id="p-0069" num="0068">Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.</p><p id="p-0070" num="0069">Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.</p><p id="p-0071" num="0070">Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-01-20" num="01-20"><claim-text><b>1</b>-<b>20</b>. (canceled)</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A system-on-a-chip comprising:<claim-text>a pair of processing devices including a first processing device and a second processing device;</claim-text><claim-text>a system-level cache configured to cache memory requests to a memory by the processing devices;</claim-text><claim-text>a token switch network that is configured to carry synchronization data between the processing devices to control data sharing through the system-level cache by the processing devices,</claim-text><claim-text>wherein the first processing device is configured to store data in the system-level cache, provide a token through the token switch network to the second processing device indicating availability of the data stored in the system-level cache, and</claim-text><claim-text>wherein the second processing device is configured to receive the token through the token-switch network and, in response, read the data from the system-level cache.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the second processing device is configured to read, from the system-level cache, the data generated by the first processing device without accessing the data in the memory.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the token switch network is a different communications network than a main communications network that carries data between the processing devices and the memory.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the first processing device is an image signal processor, and wherein the second processing device is a processing device that is configured to process data generated by the image signal processor.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the first processing device is configured to generate audio data, and wherein the second processing device is configured to process the audio data generated by the first processing device.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the first processing device and the second processing device are configured to use a same address walk order.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the second processing device is configured to provide, to the first processing device using the token switch network, a token indicating that the data generated by the first processing device has been processed by the second processing device.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the data generated by the first processing device and read by the second processing device from the system-level cache comprises sub-frame data that is smaller than an image frame of a camera communicatively coupled to the system-on-a-chip.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. A method comprising:<claim-text>generating, by a first processing device of a system-on-a-chip, data to be processed by a second processing device of the system-on-a-chip;</claim-text><claim-text>storing, by the first processing device, the generated data in a system-level cache of the system-on-a-chip, wherein the system-level cache is configured to cache memory requests to a memory by the first processing device and the second processing device;</claim-text><claim-text>providing, by the first processing device to the second processing device using a token switch network of the system-on-a-chip, a token indicating the availability of the data stored in the system-level cache;</claim-text><claim-text>receiving, by the second processing device, the token from the first processing device; and</claim-text><claim-text>in response, reading, by the second processing device, the data from the system-level cache generated by the first processing device.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein reading the data by the second processing device comprises reading the data from the system-level cache without accessing the memory.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the token switch network is a different communications network than a main communications network that carries data between the processing devices and the memory.</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the first processing device is an image signal processor, and wherein the second processing device is a processing device that is configured to process data generated by the image signal processor.</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the first processing device generates audio data, and wherein the second processing device is configured to process the audio data generated by the first processing device.</claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the first processing device and the second processing device are configured to use a same address walk order.</claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, further comprising providing, by the second processing device using the token switch network to the first processing device, a token indicating that the data generated by the first processing device has been processed by the second processing device.</claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the data generated by the first processing device and read by the second processing device from the system-level cache comprises sub-frame data that is smaller than an image frame of a camera communicatively coupled to the system-on-a-chip.</claim-text></claim></claims></us-patent-application>