<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005166A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005166</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942125</doc-number><date>20220910</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>B</subclass><main-group>11</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2200</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2200</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHOD AND DEVICE FOR MEASURING PHYSICAL OBJECTS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17323906</doc-number><date>20210518</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11475582</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17942125</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>63040605</doc-number><date>20200618</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Apple Inc.</orgname><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Germer</last-name><first-name>Austin Caleb</first-name><address><city>Novato</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Sparacino</last-name><first-name>Vincent Paul</first-name><address><city>San Bruno</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Bolton</last-name><first-name>Adam James</first-name><address><city>Bend</city><state>OR</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Rodriguez</last-name><first-name>Tomas Alvarez</first-name><address><city>San Rafael</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Bullock</last-name><first-name>Ryan Steven</first-name><address><city>Pleasanton</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Smallwood</last-name><first-name>Lori Lenore</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The method performed at an electronic device including one or more processors, a non-transitory memory, and a depth sensor includes: obtaining a task associated with a physical object within a physical environment; obtaining a task associated with a physical object within a physical environment; obtaining depth information, via the depth sensor, associated with the physical environment; determining one or more measurements for the physical object based at least in part on the depth information; generating a graphical overlay for the task based at least in part on the task associated with the physical object and the one or more measurements for the physical object; and causing presentation of the graphical overlay relative to a representation of the physical object, wherein the representation is obtained using sensor readings of the physical object.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="231.90mm" wi="150.71mm" file="US20230005166A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="246.46mm" wi="163.58mm" file="US20230005166A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="229.19mm" wi="154.18mm" file="US20230005166A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="214.21mm" wi="164.00mm" orientation="landscape" file="US20230005166A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="246.38mm" wi="152.74mm" file="US20230005166A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="246.38mm" wi="123.78mm" file="US20230005166A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="240.54mm" wi="154.35mm" file="US20230005166A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="240.54mm" wi="156.21mm" file="US20230005166A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="211.24mm" wi="156.21mm" file="US20230005166A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="211.67mm" wi="156.21mm" file="US20230005166A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="240.54mm" wi="142.16mm" file="US20230005166A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="240.54mm" wi="154.35mm" file="US20230005166A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="240.54mm" wi="154.35mm" file="US20230005166A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="240.54mm" wi="153.42mm" file="US20230005166A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="240.54mm" wi="151.55mm" file="US20230005166A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="211.24mm" wi="156.21mm" file="US20230005166A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="211.24mm" wi="156.21mm" file="US20230005166A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="240.54mm" wi="154.35mm" file="US20230005166A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="240.54mm" wi="156.21mm" file="US20230005166A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="211.24mm" wi="156.21mm" file="US20230005166A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="211.24mm" wi="156.21mm" file="US20230005166A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="215.14mm" wi="121.75mm" file="US20230005166A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/323,906, filed on May 18, 2021, which claims priority to U.S. Provisional Patent App. No. 63/040,605, filed on Jun. 18, 2020, which are hereby incorporated by reference herein in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><heading id="h-0003" level="1">Background</heading><p id="p-0003" num="0002">In some instances, eyeballing volume and mass measurements for cooking or home improvement projects can be futile at best. Furthermore, estimating volume and mass measurements with a single camera is likewise a difficult task.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0004" num="0003">So that the present disclosure can be understood by those of ordinary skill in the art, a more detailed description may be had by reference to aspects of some illustrative implementations, some of which are shown in the accompanying drawings.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an example operating architecture in accordance with some implementations.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an example controller in accordance with some implementations.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of an example electronic device in accordance with some implementations.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> show a block diagram of an example image processing architecture in accordance with some implementations.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>E</figref> illustrate a sequence of instances of a first measurement scenario in accordance with some implementations.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>J</figref> illustrate a sequence of instances of a second measurement scenario in accordance with some implementations.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart representation of a method of measuring physical objects to accomplish an associated task in accordance with some implementations.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0012" num="0011">In accordance with common practice the various features illustrated in the drawings may not be drawn to scale. Accordingly, the dimensions of the various features may be arbitrarily expanded or reduced for clarity. In addition, some of the drawings may not depict all of the components of a given system, method, or device. Finally, like reference numerals may be used to denote like features throughout the specification and figures.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0013" num="0012">Various implementations disclosed herein include devices, systems, and methods for measuring physical objects to accomplish an associated task. According to some implementations, the method is performed at an electronic device including one or more processors, non-transitory memory, and a depth sensor. The method includes: obtaining a task associated with a physical object within a physical environment; obtaining depth information, via the depth sensor, associated with the physical environment; determining one or more measurements for the physical object based at least in part on the depth information; obtaining a graphical overlay based at least in part on the task and the one or more measurements for the physical object; and causing presentation of the graphical overlay adjacent to a representation of the physical object, wherein the representation is obtained using sensor readings of the physical object.</p><p id="p-0014" num="0013">In accordance with some implementations, a device includes one or more processors, a non-transitory memory, and one or more programs; the one or more programs are stored in the non-transitory memory and configured to be executed by the one or more processors and the one or more programs include instructions for performing or causing performance of any of the methods described herein. In accordance with some implementations, a non-transitory computer readable storage medium has stored therein instructions, which, when executed by one or more processors of a device, cause the device to perform or cause performance of any of the methods described herein. In accordance with some implementations, a device includes: one or more processors, a non-transitory memory, and means for performing or causing performance of any of the methods described herein.</p><p id="p-0015" num="0014">In accordance with some implementations, a computing system includes one or more processors, non-transitory memory, an interface for communicating with a display device and one or more input devices, and one or more programs; the one or more programs are stored in the non-transitory memory and configured to be executed by the one or more processors and the one or more programs include instructions for performing or causing performance of the operations of any of the methods described herein. In accordance with some implementations, a non-transitory computer readable storage medium has stored therein instructions which when executed by one or more processors of a computing system with an interface for communicating with a display device and one or more input devices, cause the computing system to perform or cause performance of the operations of any of the methods described herein. In accordance with some implementations, a computing system includes one or more processors, non-transitory memory, an interface for communicating with a display device and one or more input devices, and means for performing or causing performance of the operations of any of the methods described herein.</p><heading id="h-0006" level="1">DESCRIPTION</heading><p id="p-0016" num="0015">Numerous details are described in order to provide a thorough understanding of the example implementations shown in the drawings. However, the drawings merely show some example aspects of the present disclosure and are therefore not to be considered limiting. Those of ordinary skill in the art will appreciate that other effective aspects and/or variants do not include all of the specific details described herein. Moreover, well-known systems, methods, components, devices, and circuits have not been described in exhaustive detail so as not to obscure more pertinent aspects of the example implementations described herein.</p><p id="p-0017" num="0016">A physical environment refers to a physical world that people can sense and/or interact with without aid of electronic devices. The physical environment may include physical features such as a physical surface or a physical object. For example, the physical environment corresponds to a physical park that includes physical trees, physical buildings, and physical people. People can directly sense and/or interact with the physical environment such as through sight, touch, hearing, taste, and smell. In contrast, an extended reality (XR) environment refers to a wholly or partially simulated environment that people sense and/or interact with via an electronic device. For example, the XR environment may include augmented reality (AR) content, mixed reality (MR) content, virtual reality (VR) content, and/or the like. With an XR system, a subset of a person's physical motions, or representations thereof, are tracked, and, in response, one or more characteristics of one or more virtual objects simulated in the XR environment are adjusted in a manner that comports with at least one law of physics. As one example, the XR system may detect head movement and, in response, adjust graphical content and an acoustic field presented to the person in a manner similar to how such views and sounds would change in a physical environment. As another example, the XR system may detect movement of the electronic device presenting the XR environment (e.g., a mobile phone, a tablet, a laptop, or the like) and, in response, adjust graphical content and an acoustic field presented to the person in a manner similar to how such views and sounds would change in a physical environment. In some situations (e.g., for accessibility reasons), the XR system may adjust characteristic(s) of graphical content in the XR environment in response to representations of physical motions (e.g., vocal commands).</p><p id="p-0018" num="0017">There are many different types of electronic systems that enable a person to sense and/or interact with various XR environments. Examples include head mountable systems, projection-based systems, heads-up displays (HUDs), vehicle windshields having integrated display capability, windows having integrated display capability, displays formed as lenses designed to be placed on a person's eyes (e.g., similar to contact lenses), headphones/earphones, speaker arrays, input systems (e.g., wearable or handheld controllers with or without haptic feedback), smartphones, tablets, and desktop/laptop computers. A head mountable system may have one or more speaker(s) and an integrated opaque display. Alternatively, ahead mountable system may be configured to accept an external opaque display (e.g., a smartphone). The head mountable system may incorporate one or more imaging sensors to capture images or video of the physical environment, and/or one or more microphones to capture audio of the physical environment. Rather than an opaque display, a head mountable system may have a transparent or translucent display. The transparent or translucent display may have a medium through which light representative of images is directed to a person's eyes. The display may utilize digital light projection, OLEDs, LEDs, &#x3bc;LEDs, liquid crystal on silicon, laser scanning light source, or any combination of these technologies. The medium may be an optical waveguide, a hologram medium, an optical combiner, an optical reflector, or any combination thereof. In some implementations, the transparent or translucent display may be configured to become opaque selectively. Projection-based systems may employ retinal projection technology that projects graphical images onto a person's retina. Projection systems also may be configured to project virtual objects into the physical environment, for example, as a hologram or on a physical surface.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an example operating architecture <b>100</b> in accordance with some implementations. While pertinent features are shown, those of ordinary skill in the art will appreciate from the present disclosure that various other features have not been illustrated for the sake of brevity and so as not to obscure more pertinent aspects of the example implementations disclosed herein. To that end, as a non-limiting example, the operating architecture <b>100</b> includes an optional controller <b>110</b> and an electronic device <b>120</b> (e.g., a tablet, mobile phone, laptop, near-eye system, wearable computing device, or the like).</p><p id="p-0020" num="0019">In some implementations, the controller <b>110</b> is configured to manage and coordinate an XR experience (sometimes also referred to herein as a &#x201c;XR environment&#x201d; or a &#x201c;virtual environment&#x201d; or a &#x201c;graphical environment&#x201d;) for a user <b>150</b> and optionally other users. In some implementations, the controller <b>110</b> includes a suitable combination of software, firmware, and/or hardware. The controller <b>110</b> is described in greater detail below with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In some implementations, the controller <b>110</b> is a computing device that is local or remote relative to the physical environment <b>105</b>. For example, the controller <b>110</b> is a local server located within the physical environment <b>105</b>. In another example, the controller <b>110</b> is a remote server located outside of the physical environment <b>105</b> (e.g., a cloud server, central server, etc.). In some implementations, the controller <b>110</b> is communicatively coupled with the electronic device <b>120</b> via one or more wired or wireless communication channels <b>144</b> (e.g., BLUETOOTH, IEEE 802.11x, IEEE 802.16x, IEEE 802.3x, etc.). In some implementations, the functions of the controller <b>110</b> are provided by the electronic device <b>120</b>. As such, in some implementations, the components of the controller <b>110</b> are integrated into the electronic device <b>120</b>.</p><p id="p-0021" num="0020">In some implementations, the electronic device <b>120</b> is configured to present audio and/or video (A/V) content to the user <b>150</b>. In some implementations, the electronic device <b>120</b> is configured to present a user interface (UI) and/or an XR environment <b>128</b> to the user <b>150</b>. In some implementations, the electronic device <b>120</b> includes a suitable combination of software, firmware, and/or hardware. The electronic device <b>120</b> is described in greater detail below with respect to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0022" num="0021">According to some implementations, the electronic device <b>120</b> presents an XR experience to the user <b>150</b> while the user <b>150</b> is physically present within a physical environment <b>105</b> that includes a table <b>107</b> within the field-of-view (FOV) <b>111</b> of the electronic device <b>120</b>. As such, in some implementations, the user <b>150</b> holds the electronic device <b>120</b> in his/her hand(s). In some implementations, while presenting the XR experience, the electronic device <b>120</b> is configured to present XR content (sometimes also referred to herein as &#x201c;graphical content&#x201d; or &#x201c;virtual content&#x201d;), including an XR cylinder <b>109</b>, and to enable video pass-through of the physical environment <b>105</b> (e.g., including the table <b>107</b> or a representation thereof) on a display <b>122</b>. For example, the XR environment <b>128</b>, including the XR cylinder <b>109</b>, is volumetric or three-dimensional (3D).</p><p id="p-0023" num="0022">In one example, the XR cylinder <b>109</b> corresponds to head/display-locked content such that the XR cylinder <b>109</b> remains displayed at the same location on the display <b>122</b> as the FOV <b>111</b> changes due to translational and/or rotational movement of the electronic device <b>120</b>. As another example, the XR cylinder <b>109</b> corresponds to world/object-locked content such that the XR cylinder <b>109</b> remains displayed at its origin location as the FOV <b>111</b> changes due to translational and/or rotational movement of the electronic device <b>120</b>. As such, in this example, if the FOV <b>111</b> does not include the origin location, the XR environment <b>128</b> will not include the XR cylinder <b>109</b>. For example, the electronic device <b>120</b> corresponds to a near-eye system, mobile phone, tablet, laptop, wearable computing device, or the like.</p><p id="p-0024" num="0023">In some implementations, the display <b>122</b> corresponds to an additive display that enables optical see-through of the physical environment <b>105</b> including the table <b>107</b>. For example, the display <b>122</b> corresponds to a transparent lens, and the electronic device <b>120</b> corresponds to a pair of glasses worn by the user <b>150</b>. As such, in some implementations, the electronic device <b>120</b> presents a user interface by projecting the XR content (e.g., the XR cylinder <b>109</b>) onto the additive display, which is, in turn, overlaid on the physical environment <b>105</b> from the perspective of the user <b>150</b>. In some implementations, the electronic device <b>120</b> presents the user interface by displaying the XR content (e.g., the XR cylinder <b>109</b>) on the additive display, which is, in turn, overlaid on the physical environment <b>105</b> from the perspective of the user <b>150</b>.</p><p id="p-0025" num="0024">In some implementations, the user <b>150</b> wears the electronic device <b>120</b> such as a near-eye system. As such, the electronic device <b>120</b> includes one or more displays provided to display the XR content (e.g., a single display or one for each eye). For example, the electronic device <b>120</b> encloses the FOV of the user <b>150</b>. In such implementations, the electronic device <b>120</b> presents the XR environment <b>128</b> by displaying data corresponding to the XR environment <b>128</b> on the one or more displays or by projecting data corresponding to the XR environment <b>128</b> onto the retinas of the user <b>150</b>.</p><p id="p-0026" num="0025">In some implementations, the electronic device <b>120</b> includes an integrated display (e.g., a built-in display) that displays the XR environment <b>128</b>. In some implementations, the electronic device <b>120</b> includes a head-mountable enclosure. In various implementations, the head-mountable enclosure includes an attachment region to which another device with a display can be attached. For example, in some implementations, the electronic device <b>120</b> can be attached to the head-mountable enclosure. In various implementations, the head-mountable enclosure is shaped to form a receptacle for receiving another device that includes a display (e.g., the electronic device <b>120</b>). For example, in some implementations, the electronic device <b>120</b> slides/snaps into or otherwise attaches to the head-mountable enclosure. In some implementations, the display of the device attached to the head-mountable enclosure presents (e.g., displays) the XR environment <b>128</b>. In some implementations, the electronic device <b>120</b> is replaced with an XR chamber, enclosure, or room configured to present XR content in which the user <b>150</b> does not wear the electronic device <b>120</b>.</p><p id="p-0027" num="0026">In some implementations, the controller <b>110</b> and/or the electronic device <b>120</b> cause an XR representation of the user <b>150</b> to move within the XR environment <b>128</b> based on movement information (e.g., body pose data, eye tracking data, hand/limb/finger/extremity tracking data, etc.) from the electronic device <b>120</b> and/or optional remote input devices within the physical environment <b>105</b>. In some implementations, the optional remote input devices correspond to fixed or movable sensory equipment within the physical environment <b>105</b> (e.g., image sensors, depth sensors, infrared (IR) sensors, event cameras, microphones, etc.). In some implementations, each of the remote input devices is configured to collect/capture input data and provide the input data to the controller <b>110</b> and/or the electronic device <b>120</b> while the user <b>150</b> is physically within the physical environment <b>105</b>. In some implementations, the remote input devices include microphones, and the input data includes audio data associated with the user <b>150</b> (e.g., speech samples). In some implementations, the remote input devices include image sensors (e.g., cameras), and the input data includes images of the user <b>150</b>. In some implementations, the input data characterizes body poses of the user <b>150</b> at different times. In some implementations, the input data characterizes head poses of the user <b>150</b> at different times. In some implementations, the input data characterizes hand tracking information associated with the hands of the user <b>150</b> at different times. In some implementations, the input data characterizes the velocity and/or acceleration of body parts of the user <b>150</b> such as his/her hands. In some implementations, the input data indicates joint positions and/or joint orientations of the user <b>150</b>. In some implementations, the remote input devices include feedback devices such as speakers, lights, or the like.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an example of the controller <b>110</b> in accordance with some implementations. While certain specific features are illustrated, those skilled in the art will appreciate from the present disclosure that various other features have not been illustrated for the sake of brevity, and so as not to obscure more pertinent aspects of the implementations disclosed herein. To that end, as a non-limiting example, in some implementations, the controller <b>110</b> includes one or more processing units <b>202</b> (e.g., microprocessors, application-specific integrated-circuits (ASICs), field-programmable gate arrays (FPGAs), graphics processing units (GPUs), central processing units (CPUs), processing cores, and/or the like), one or more input/output (I/O) devices <b>206</b>, one or more communication interfaces <b>208</b> (e.g., universal serial bus (USB), IEEE 802.3x, IEEE 802.11x, IEEE 802.16x, global system for mobile communications (GSM), code division multiple access (CDMA), time division multiple access (TDMA), global positioning system (GPS), infrared (IR), BLUETOOTH, ZIGBEE, and/or the like type interface), one or more programming (e.g., I/O) interfaces <b>210</b>, a memory <b>220</b>, and one or more communication buses <b>204</b> for interconnecting these and various other components.</p><p id="p-0029" num="0028">In some implementations, the one or more communication buses <b>204</b> include circuitry that interconnects and controls communications between system components. In some implementations, the one or more I/O devices <b>206</b> include at least one of a keyboard, a mouse, a touchpad, a touch-screen, a joystick, one or more microphones, one or more speakers, one or more image sensors, one or more displays, and/or the like.</p><p id="p-0030" num="0029">The memory <b>220</b> includes high-speed random-access memory, such as dynamic random-access memory (DRAM), static random-access memory (SRAM), double-data-rate random-access memory (DDR RAM), or other random-access solid-state memory devices. In some implementations, the memory <b>220</b> includes non-volatile memory, such as one or more magnetic disk storage devices, optical disk storage devices, flash memory devices, or other non-volatile solid-state storage devices. The memory <b>220</b> optionally includes one or more storage devices remotely located from the one or more processing units <b>202</b>. The memory <b>220</b> comprises a non-transitory computer readable storage medium. In some implementations, the memory <b>220</b> or the non-transitory computer readable storage medium of the memory <b>220</b> stores the following programs, modules and data structures, or a subset thereof including an optional operating system <b>230</b>, a data processing architecture <b>400</b>, and a rendering engine <b>460</b>.</p><p id="p-0031" num="0030">The operating system <b>230</b> includes procedures for handling various basic system services and for performing hardware dependent tasks.</p><p id="p-0032" num="0031">In some implementations, the data processing architecture <b>400</b> is configured to process user information and images of a physical environment in order to measure physical objects within the physical environment to accomplish an associated task. To that end, in some implementations, the data processing architecture <b>400</b> includes a data obtainer <b>242</b>, a mapper and locator engine <b>244</b>, a context analysis engine <b>430</b>, a natural language processor (NLP) <b>432</b>, an instructions engine <b>434</b>, an image pre-processing engine <b>436</b>, a scene analysis engine <b>438</b>, an object volume determiner <b>442</b>, a current fill volume determiner <b>444</b>, a prompt/interrupt handler <b>446</b>, and a data transmitter <b>246</b>.</p><p id="p-0033" num="0032">In some implementations, the data obtainer <b>242</b> is configured to obtain data (e.g., captured image frames of the physical environment <b>105</b>, presentation data, input data, user interaction data, camera pose tracking information, eye tracking information, head/body pose tracking information, hand/limb tracking information, sensor data, location data, etc.) from at least one of the I/O devices <b>206</b> of the controller <b>110</b>, the electronic device <b>120</b>, and the optional remote input devices. To that end, in various implementations, the data obtainer <b>242</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0034" num="0033">In some implementations, the mapper and locator engine <b>244</b> is configured to map the physical environment <b>105</b> and to track the position/location of at least the electronic device <b>120</b> with respect to the physical environment <b>105</b>. To that end, in various implementations, the mapper and locator engine <b>244</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0035" num="0034">In some implementations, the context analysis engine <b>430</b> is configured to obtain (e.g., receive, retrieve, or determine/generate) a contextual information vector based on position/rotation/movement information, a gaze direction, body/head/hand/limb pose information, user input information, and/or the like based on data collected from the localization and mapping engine <b>244</b>, an eye tracking engine, a body/head pose tracking engine, a hand/limb tracking engine, a camera pose tracking engine, and/or the like. To that end, in various implementations, the context analysis engine <b>430</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0036" num="0035">In some implementations, the NLP <b>432</b> is configured to parse speech data from the user <b>150</b> and optionally convert the speech data to text. To that end, in various implementations, the NLP <b>432</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0037" num="0036">In some implementations, the instructions engine <b>434</b> is configured to obtain (e.g., receive, retrieve, or determine/generate) an instruction or a set of instructions that the user <b>150</b> intends to carry out (e.g., filling a cup with X fluid ounces of water, or measuring ingredients in order to follow a cookie recipe). In some implementations, the instruction or set of instructions corresponds to one or more tasks such as measuring out X fluid ounces of water, baking a cake and measuring out ingredients. In some implementations, the instruction or set of instructions is determined based on the speech data from the user <b>150</b>. In some implementations, the instruction or set of instructions is determined by parsing a set of text instructions provided by the user <b>150</b> (e.g., manually typing out a recipe or procuring a recipe from a local or remote electronic source). In some implementations, the instruction or set of instructions is determined by performing text/character recognition on a physical set of instructions provided by the user (e.g., a physical recipe recipe) and/or the like. To that end, in various implementations, the instructions engine <b>434</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0038" num="0037">In some implementations, the image pre-processing engine <b>436</b> is configured to obtain (e.g., receive, retrieve, or capture) an image stream of an environment in order to generate a processed image stream. In some implementations, the image stream corresponds to a sequence of sporadic images, a live video feed, and/or the like. In some implementations, the environment corresponds to a physical environment, a partially XR environment, a fully XR environment, or the like. In some implementations, the image pre-processing engine <b>436</b> is also configured to perform one or more pre-processing operations on the image stream such as warping, noise reduction, white balance, color correction, gamma correction, sharpening, and/or the like. To that end, in various implementations, the image pre-processing engine <b>436</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0039" num="0038">In some implementations, the scene analysis engine <b>438</b> is configured to perform one or more scene analysis operations on the processed image stream of the environment in order to generate semantic scene information such as labels for objects within the environment or the like. In some implementations, the one or more scene analysis operations includes text/character recognition, object recognition, instance segmentation, semantic segmentation, dimensional analysis, and/or the like. To that end, in various implementations, the scene analysis engine <b>438</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0040" num="0039">In some implementations, the object volume determiner <b>442</b> is configured to determine one or more estimated dimensions (or measurements) for an object in the environment based on the processed image stream, the semantic scene information, and depth information. In some implementations, the one or more estimated dimensions (or measurements) correspond to an estimated available volume of the object when the object corresponds to a vessel (e.g., a fillable bowl, cup, mug, dish, etc.), an estimated (unfilled) volume of the object (e.g., the volume of a closed spherical object), an estimated surface area of the object, dimensions of the object (e.g., length, width, and depth), an estimated mass of the object, and/or the like. In some implementations, the object corresponds to a physical object, a partially XR object, a fully XR object, or the like. To that end, in various implementations, the available volume determiner <b>444</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0041" num="0040">In some implementations, the optional current fill volume determiner <b>444</b> is configured to determine an estimated current fill volume of the object when the object corresponds to a vessel (e.g., a fillable bowl, cup, mug, dish, or the like with a substance therein such as a liquid or solid) based on the processed image stream, the semantic scene information, the depth information, the one or more estimated dimensions of the object, and a measurement library <b>440</b>. In some implementations, the current fill volume determiner <b>444</b> may also consider environmental information, such as a current temperature, humidity, barometric pressure, elevation, G-force, or the like, for a more accurate mass measurement. In some implementations, the measurement library <b>440</b> includes a plurality of average or typical mass-per-volume values for various liquids, solids, semi-solids, and/or the like such as water, oil, flour, sugar, seeds, chocolate chips, and/or the like (e.g., 1 mL of water weighs 1 g). To that end, in various implementations, the current fill volume determiner <b>444</b> includes instructions and/or logic therefor, and heuristics and metadata therefor. One of ordinary skill in the art will appreciate that the optional current fill volume determiner <b>444</b> may be implemented or utilized when the object corresponds to a fillable vessel but may not be implemented or utilized when the object corresponds to a solid/unfillable object.</p><p id="p-0042" num="0041">In some implementations, the prompt/interrupt handler <b>446</b> is configured to obtain (e.g., receive, retrieve, or determine/generate) audio/visual feedback based on the one or more estimated dimensions of the object and the estimated current fill volume of the object in order to complete/satisfy the set of instructions. To that end, in various implementations, the prompt/interrupt handler <b>446</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0043" num="0042">In some implementations, the data transmitter <b>246</b> is configured to transmit data (e.g., presentation data such as rendered image frames associated with the XR environment, location data, etc.) to at least the electronic device <b>120</b>. To that end, in various implementations, the data transmitter <b>246</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0044" num="0043">Although the data obtainer <b>242</b>, the mapper and locator engine <b>244</b>, the context analysis engine <b>430</b>, the NLP <b>432</b>, the instructions engine <b>434</b>, the image pre-processing engine <b>436</b>, the scene analysis engine <b>438</b>, the object volume determiner <b>442</b>, the current fill volume determiner <b>444</b>, prompt/interrupt handler <b>446</b>, and the data transmitter <b>246</b> are shown as residing on a single device (e.g., the controller <b>110</b>), it should be understood that in other implementations, any combination of the data obtainer <b>242</b>, the mapper and locator engine <b>244</b>, the context analysis engine <b>430</b>, the NLP <b>432</b>, the instructions engine <b>434</b>, the image pre-processing engine <b>436</b>, the scene analysis engine <b>438</b>, the object volume determiner <b>442</b>, the current fill volume determiner <b>444</b>, the prompt/interrupt handler <b>446</b>, and the data transmitter <b>246</b> may be located in separate computing devices.</p><p id="p-0045" num="0044">In some implementations, the rendering engine <b>460</b> is configured to render, present, and modify a virtual/XR environment. To that end, in various implementations, the rendering engine <b>460</b> includes instructions and/or logic therefor, and heuristics and metadata therefor. In some implementations, the rendering engine <b>460</b> includes a renderer <b>462</b>, a compositor <b>464</b>, and a pose determiner <b>466</b>.</p><p id="p-0046" num="0045">In some implementations, the renderer <b>462</b> is configured to render virtual/XR content from the virtual content library <b>461</b> according to a current camera pose relative thereto. To that end, in various implementations, the renderer <b>462</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0047" num="0046">In some implementations, the virtual content library <b>461</b> includes a plurality of virtual/XR objects, items, scenery, and/or the like. In some implementations, the virtual content library <b>461</b> is stored locally and/or remotely. In some implementations, the virtual content library <b>461</b> is pre-populated or manually authored by the user <b>150</b>.</p><p id="p-0048" num="0047">In some implementations, the compositor <b>464</b> is configured to composite the rendered virtual/XR content with image(s) of the physical environment. In some implementations, the compositor <b>464</b> obtains (e.g., receives, retrieves, determines/generates, or otherwise accesses) depth information (e.g., a point cloud, mesh, or the like) associated with the scene (e.g., the physical environment <b>105</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) to maintain z-order between the rendered virtual/XR content and physical objects in the physical environment. To that end, in various implementations, the compositor <b>464</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0049" num="0048">In some implementations, the pose determiner <b>466</b> is configured to determine a current camera pose of the electronic device <b>120</b> and/or the user <b>150</b> relative to the virtual/XR content. To that end, in various implementations, the pose determiner <b>466</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0050" num="0049">Although the renderer <b>462</b>, the compositor <b>464</b>, and the pose determiner <b>466</b> are shown as residing on a single device (e.g., the controller <b>110</b>), it should be understood that in other implementations, any combination of the renderer <b>462</b>, the compositor <b>464</b>, and the pose determiner <b>466</b> may be located in separate computing devices.</p><p id="p-0051" num="0050">In some implementations, the functions and/or components of the controller <b>110</b> are combined with or provided by the electronic device <b>120</b> shown below in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Moreover, <figref idref="DRAWINGS">FIG. <b>2</b></figref> is intended more as a functional description of the various features which be present in a particular implementation as opposed to a structural schematic of the implementations described herein. As recognized by those of ordinary skill in the art, items shown separately could be combined and some items could be separated. For example, some functional modules shown separately in <figref idref="DRAWINGS">FIG. <b>2</b></figref> could be implemented in a single module and the various functions of single functional blocks could be implemented by one or more functional blocks in various implementations. The actual number of modules and the division of particular functions and how features are allocated among them will vary from one implementation to another and, in some implementations, depends in part on the particular combination of hardware, software, and/or firmware chosen for a particular implementation.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of an example of the electronic device <b>120</b> (e.g., a mobile phone, tablet, laptop, near-eye system, wearable computing device, or the like) in accordance with some implementations. While certain specific features are illustrated, those skilled in the art will appreciate from the present disclosure that various other features have not been illustrated for the sake of brevity, and so as not to obscure more pertinent aspects of the implementations disclosed herein. To that end, as a non-limiting example, in some implementations, the electronic device <b>120</b> includes one or more processing units <b>302</b> (e.g., microprocessors, ASICs, FPGAs, GPUs, CPUs, processing cores, and/or the like), one or more input/output (I/O) devices and sensors <b>306</b>, one or more communication interfaces <b>308</b> (e.g., USB, IEEE 802.3x, IEEE 802.11x, IEEE 802.16x, GSM, CDMA, TDMA, GPS, IR, BLUETOOTH, ZIGBEE, and/or the like type interface), one or more programming (e.g., I/O) interfaces <b>310</b>, one or more displays <b>312</b>, an image capture device <b>370</b> (e.g., one or more optional interior- and/or exterior-facing image sensors), a memory <b>320</b>, and one or more communication buses <b>304</b> for interconnecting these and various other components.</p><p id="p-0053" num="0052">In some implementations, the one or more communication buses <b>304</b> include circuitry that interconnects and controls communications between system components. In some implementations, the one or more I/O devices and sensors <b>306</b> include at least one of an inertial measurement unit (IMU), an accelerometer, a gyroscope, a magnetometer, a thermometer, one or more physiological sensors (e.g., blood pressure monitor, heart rate monitor, blood oxygen sensor, blood glucose sensor, etc.), one or more microphones, one or more speakers, a haptics engine, a heating and/or cooling unit, a skin shear engine, one or more depth sensors (e.g., structured light, time-of-flight, LiDAR, or the like), a localization and mapping engine, an eye tracking engine, a body/head pose tracking engine, a hand/limb tracking engine, a camera pose tracking engine, and/or the like.</p><p id="p-0054" num="0053">In some implementations, the one or more displays <b>312</b> are configured to present the XR environment to the user. In some implementations, the one or more displays <b>312</b> are also configured to present flat video content to the user (e.g., a 2-dimensional or &#x201c;flat&#x201d; AVI, FLV, WMV, MOV, MP4, or the like file associated with a TV episode or a movie, or live video pass-through of the physical environment <b>105</b>). In some implementations, the one or more displays <b>312</b> correspond to touchscreen displays. In some implementations, the one or more displays <b>312</b> correspond to holographic, digital light processing (DLP), liquid-crystal display (LCD), liquid-crystal on silicon (LCoS), organic light-emitting field-effect transitory (OLET), organic light-emitting diode (OLED), surface-conduction electron-emitter display (SED), field-emission display (FED), quantum-dot light-emitting diode (QD-LED), micro-electro-mechanical system (MEMS), and/or the like display types. In some implementations, the one or more displays <b>312</b> correspond to diffractive, reflective, polarized, holographic, etc. waveguide displays. For example, the electronic device <b>120</b> includes a single display. In another example, the electronic device <b>120</b> includes a display for each eye of the user. In some implementations, the one or more displays <b>312</b> are capable of presenting AR and VR content. In some implementations, the one or more displays <b>312</b> are capable of presenting AR or VR content.</p><p id="p-0055" num="0054">In some implementations, the image capture device <b>370</b> correspond to one or more RGB cameras (e.g., with a complementary metal-oxide-semiconductor (CMOS) image sensor or a charge-coupled device (CCD) image sensor), IR image sensors, event-based cameras, and/or the like. In some implementations, the image capture device <b>370</b> includes a lens assembly, a photodiode, and a front-end architecture.</p><p id="p-0056" num="0055">The memory <b>320</b> includes high-speed random-access memory, such as DRAM, SRAM, DDR RAM, or other random-access solid-state memory devices. In some implementations, the memory <b>320</b> includes non-volatile memory, such as one or more magnetic disk storage devices, optical disk storage devices, flash memory devices, or other non-volatile solid-state storage devices. The memory <b>320</b> optionally includes one or more storage devices remotely located from the one or more processing units <b>302</b>. The memory <b>320</b> comprises a non-transitory computer readable storage medium. In some implementations, the memory <b>320</b> or the non-transitory computer readable storage medium of the memory <b>320</b> stores the following programs, modules and data structures, or a subset thereof including an optional operating system <b>330</b> and an XR presentation engine <b>340</b>.</p><p id="p-0057" num="0056">The operating system <b>330</b> includes procedures for handling various basic system services and for performing hardware dependent tasks. In some implementations, the XR presentation engine <b>340</b> is configured to present XR content to the user via the one or more displays <b>312</b>. To that end, in various implementations, the XR presentation engine <b>340</b> includes a data obtainer <b>342</b>, a presenter <b>344</b>, an interaction handler <b>346</b>, and a data transmitter <b>350</b>.</p><p id="p-0058" num="0057">In some implementations, the data obtainer <b>342</b> is configured to obtain data (e.g., presentation data such as rendered image frames associated with the XR environment, input data, user interaction data, head tracking information, camera pose tracking information, eye tracking information, sensor data, location data, etc.) from at least one of the I/O devices and sensors <b>306</b> of the electronic device <b>120</b>, the controller <b>110</b>, and the remote input devices. To that end, in various implementations, the data obtainer <b>342</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0059" num="0058">In some implementations, the presenter <b>344</b> is configured to present and update XR content (e.g., the rendered image frames associated with the XR environment) via the one or more displays <b>312</b>. To that end, in various implementations, the presenter <b>344</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0060" num="0059">In some implementations, the interaction handler <b>346</b> is configured to detect user interactions with the presented XR content. To that end, in various implementations, the interaction handler <b>346</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0061" num="0060">In some implementations, the data transmitter <b>350</b> is configured to transmit data (e.g., presentation data, location data, user interaction data, head tracking information, camera pose tracking information, eye tracking information, etc.) to at least the controller <b>110</b>. To that end, in various implementations, the data transmitter <b>350</b> includes instructions and/or logic therefor, and heuristics and metadata therefor.</p><p id="p-0062" num="0061">Although the data obtainer <b>342</b>, the presenter <b>344</b>, the interaction handler <b>346</b>, and the data transmitter <b>350</b> are shown as residing on a single device (e.g., the electronic device <b>120</b>), it should be understood that in other implementations, any combination of the data obtainer <b>342</b>, the presenter <b>344</b>, the interaction handler <b>346</b>, and the data transmitter <b>350</b> may be located in separate computing devices.</p><p id="p-0063" num="0062">Moreover, <figref idref="DRAWINGS">FIG. <b>3</b></figref> is intended more as a functional description of the various features which be present in a particular implementation as opposed to a structural schematic of the implementations described herein. As recognized by those of ordinary skill in the art, items shown separately could be combined and some items could be separated. For example, some functional modules shown separately in <figref idref="DRAWINGS">FIG. <b>3</b></figref> could be implemented in a single module and the various functions of single functional blocks could be implemented by one or more functional blocks in various implementations. The actual number of modules and the division of particular functions and how features are allocated among them will vary from one implementation to another and, in some implementations, depends in part on the particular combination of hardware, software, and/or firmware chosen for a particular implementation.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> show a block diagram of an example data processing architecture <b>400</b> in accordance with some implementations. While certain specific features are illustrated, those skilled in the art will appreciate from the present disclosure that various other features have not been illustrated for the sake of brevity, and so as not to obscure more pertinent aspects of the implementations disclosed herein.</p><p id="p-0065" num="0064">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, in some implementations, the image capture device <b>370</b> captures one or more images of the physical environment <b>105</b> (or, alternatively, a partially or fully XR environment). In some implementations, the image pre-processing engine <b>436</b> performs one or more pre-processing operations on the images from the image capture device <b>370</b>, such as warping, noise reduction, white balance, color correction, gamma correction, sharpening, and/or the like, in order to provide a processed image stream <b>412</b> of the physical environment <b>105</b>. In some implementations, the scene analysis engine <b>438</b> performs one or more scene analysis operations on the processed image stream <b>412</b> of the physical environment <b>105</b> in order to generate semantic scene information <b>414</b> such as labels for objects within the physical environment <b>105</b> or the like.</p><p id="p-0066" num="0065">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the context analysis engine <b>430</b> obtains (e.g., receives, retrieves, or determines/generates) a contextual information vector <b>404</b> based on user information, including position/rotation/movement information <b>402</b>A, a gaze direction <b>402</b>B, body/head/hand/limb pose information <b>402</b>C, user input information <b>402</b>D, and/or the like based on data collected from a localization and mapping engine, an eye tracking engine, a body/head pose tracking engine, a hand/limb tracking engine, a camera pose tracking engine, and/or the like. In some implementations, the NLP <b>432</b> obtains (e.g., receives or retrieves) speech data <b>402</b>E from the user <b>150</b>. In some implementations, the NLP <b>432</b> parses the speech data <b>402</b>E from the user <b>150</b> and, optionally, converts the speech data <b>402</b>E to a text representation <b>406</b> thereof. In some implementations, the user information, which includes the position/rotation/movement information <b>402</b>A, the gaze direction <b>402</b>B, the body/head/hand/limb pose information <b>402</b>C, the user input information <b>402</b>D, and the speech data <b>402</b>E, may be subject to an optional privacy subsystem <b>428</b> prior to ingestion of the user information by the context analysis engine <b>430</b> and the NLP <b>432</b>.</p><p id="p-0067" num="0066">To this end, in various implementations, the data processing architecture <b>400</b> includes the optional privacy subsystem <b>428</b> with one or more privacy filters associated with user information and/or identifying information (e.g., at least some portions of the position/rotation/movement information <b>402</b>A, the gaze direction <b>402</b>B, the body/head/hand/limb pose information <b>402</b>C, the user input information <b>402</b>D, and the speech data <b>402</b>E). In some implementations, the privacy subsystem <b>428</b> selectively prevents and/or limits the data processing architecture <b>400</b> or portions thereof from obtaining and/or transmitting the user information. To this end, the privacy subsystem <b>428</b> receives user preferences and/or selections from the user in response to prompting the user for the same. In some implementations, the privacy subsystem <b>428</b> prevents the data processing architecture <b>400</b> from obtaining and/or transmitting the user information unless and until the privacy subsystem <b>428</b> obtains informed consent from the user. In some implementations, the privacy subsystem <b>428</b> anonymizes (e.g., scrambles or obscures) certain types of user information. For example, the privacy subsystem <b>428</b> receives user inputs designating which types of user information the privacy subsystem <b>428</b> anonymizes. As another example, the privacy subsystem <b>428</b> anonymizes certain types of user information likely to include sensitive and/or identifying information, independent of user designation (e.g., automatically).</p><p id="p-0068" num="0067">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the instructions engine <b>434</b> obtains (e.g., receives, retrieves, or determines/infers/generates) an instruction or a set of instructions <b>408</b> that the user <b>150</b> intends to carry out (e.g., filling a cup with X fluid ounces of water, or measuring ingredients in order to follow a cookie recipe) based at least in part on the contextual information vector <b>404</b> and the text representations <b>406</b> of the speech data <b>402</b>E. As one example, the instructions engine <b>434</b> determines the set of instructions <b>408</b> based on the text representation <b>406</b> of the speech data <b>402</b>E from the user <b>150</b>. As another example, the instructions engine <b>434</b> determines the set of instructions <b>408</b> by parsing a set of text instructions provided by the user <b>150</b> (e.g., a manually typed out a recipe, or a text recipe procured from a local or remote electronic source). As yet another example, the instructions engine <b>434</b> determines the set of instructions <b>408</b> by performing text/character recognition on a physical set of instructions provided by the user (e.g., a physical recipe card). One of ordinary skill in the art will appreciate that the instructions engine <b>434</b> may determine the set of instructions <b>408</b> in myriad ways and from myriad input modalities. <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>E</figref> illustrate a sequence of example instances of a first measurement scenario according to a first set of instructions (e.g., a verbal user request to fill a cup with X fluid ounces of water) in accordance with some implementations. Similarly, <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>J</figref> illustrate a sequence of example instances of a second measurement scenario according to a second set of instructions (e.g., a physical index card with a chocolate chip cookie recipe thereon) in accordance with some implementations.</p><p id="p-0069" num="0068">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the object volume determiner <b>442</b> determines one or more estimated dimensions (or measurements) <b>443</b> of a physical object within the physical environment <b>105</b> based on the processed image stream <b>412</b>, the semantic scene information <b>414</b>, and depth information <b>452</b>. In some implementations, the depth information <b>452</b> corresponds to a (depth) mesh, point cloud, or the like of the physical environment <b>105</b>. In some implementations, the depth information <b>452</b> corresponds to a (depth) mesh, point cloud, or the like of a portion of the physical environment <b>105</b> such as one or more physical objects that the user intends on interacting with. For example, the data processing architecture <b>400</b> determines user intent based on the contextual information vector <b>404</b> (e.g., the gaze direction <b>202</b>B). In some implementations, the depth information <b>452</b> is collected by a depth sensor using techniques known in the art such as structured light, time-of-flight, LiDAR, or the like.</p><p id="p-0070" num="0069">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the one or more estimated dimensions (or measurements) <b>443</b> of the physical object corresponds to an estimated available volume of the physical object when the physical object corresponds to a vessel (e.g., a fillable bowl, cup, mug, dish, etc.), an estimated (unfilled) volume of the physical object (e.g., the volume of a closed spherical object, or another solid non-concave object), an estimated surface area of the physical object, dimensions of the physical object (e.g., length, width, and depth), an estimated mass of the physical object, and/or the like.</p><p id="p-0071" num="0070">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the current fill volume determiner <b>444</b> determines an estimated current fill volume <b>445</b> of the physical object when the physical object corresponds to a vessel (e.g., a fillable bowl, cup, mug, dish, or the like with a substance therein such as a liquid or solid) based on the processed image stream <b>412</b>, the semantic scene information <b>414</b>, the depth information <b>452</b>, the one or more estimated dimensions <b>443</b> of the physical object, and a measurement library <b>440</b>. In some implementations, the current fill volume determiner <b>444</b> may also consider environmental information, such as a current temperature, humidity, barometric pressure, elevation, G-force, or the like, for more accurate measurements (e.g., mass measurement). In some implementations, the measurement library <b>440</b> includes a plurality of average or typical mass-per-volume values for various liquids, solids, semi-solids, and/or the like such as water, oil, flour, sugar, seeds, chocolate chips, and/or the like (e.g., 1 mL of water weighs 1 g).</p><p id="p-0072" num="0071">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the prompt/interrupt handler <b>446</b> obtains (e.g., receives, retrieves, or determines/generates) audio/visual feedback <b>447</b> based on the one or more estimated dimensions <b>443</b> of a physical object and the estimated current fill volume <b>445</b> of the physical object in order to complete/satisfy the set of instructions <b>408</b>. <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>E</figref> illustrate a sequence of example instances of a first measurement scenario accompanied by first feedback (e.g., an XR overlay <b>522</b> indicating a fill line for water in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>) in accordance with some implementations. Similarly, <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>J</figref> illustrate a sequence of example instances of a second measurement scenario accompanied by second feedback (e.g., an XR overlay <b>644</b> for measuring out a correct amount of chocolate chips in <figref idref="DRAWINGS">FIG. <b>6</b>E</figref>) in accordance with some implementations. One of ordinary skill in the will appreciate that the term &#x201c;XR overlay&#x201d; may also be replaced with &#x201c;graphical overlay&#x201d; in various implementations.</p><p id="p-0073" num="0072">As one example, the prompt/interrupt handler <b>446</b> provides audible feedback when the estimated current fill volume <b>445</b> of the physical object satisfies or does not satisfy the one or more instructions <b>408</b> (e.g., &#x201c;the mixing bowl now contains 15 g of sugar according to the recipe&#x201d;, or &#x201c;the mixing bowl is still 5 g short of the amount of sugar indicated by the recipe&#x201d;). As another example, the prompt/interrupt handler <b>446</b> provides an XR overlay indicating a fill line in order to fill a vessel with X fluid ounces of water. As yet another example, the prompt/interrupt handler <b>446</b> provides an XR overlay indicating a cutting/apportionment line in order to measure out 1 cup of butter relative to a stick of butter.</p><p id="p-0074" num="0073">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the renderer <b>462</b> renders virtual/XR content <b>463</b> from the virtual content library <b>461</b> relative to a current camera pose from the pose determiner <b>466</b>. In some implementations, the virtual/XR content <b>463</b> may include one or more XR overlays indicating the one or more estimated dimensions <b>443</b> of the physical object, the estimated current fill volume <b>445</b> of the physical object, the set of instructions <b>408</b>, and/or the audio/visual feedback <b>447</b>.</p><p id="p-0075" num="0074">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the compositor <b>464</b> composites the rendered virtual/XR content <b>463</b> with the processed image stream <b>412</b> based at least in part on the depth information <b>452</b> (e.g., to maintain z-order) to generate a rendered frame <b>465</b> of the XR environment. In turn, the display <b>480</b> displays the rendered frame <b>465</b> of the XR environment to the user <b>150</b>. In some implementations, the compositor <b>464</b> obtains (e.g., receives, retrieves, determines/generates, or otherwise accesses) the depth information <b>452</b> (e.g., a point cloud, mesh, or the like) associated with the scene (e.g., the physical environment <b>105</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, or a portion thereof) to maintain z-order between the rendered virtual/XR content and physical objects in the physical environment.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>E</figref> illustrate a sequence of instances <b>500</b>, <b>510</b>, <b>520</b>, <b>530</b>, and <b>540</b> of a first measurement scenario in accordance with some implementations. While certain specific features are illustrated, those skilled in the art will appreciate from the present disclosure that various other features have not been illustrated for the sake of brevity, and so as not to obscure more pertinent aspects of the implementations disclosed herein.</p><p id="p-0077" num="0076">As shown in <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>E</figref>, the first measurement scenario includes a physical environment <b>505</b> and an XR environment <b>128</b> displayed on the display <b>122</b> of the electronic device <b>120</b>. The electronic device <b>120</b> presents the XR environment <b>128</b> to the user <b>150</b> while the user <b>150</b> is physically present within the physical environment <b>505</b> (e.g., a home kitchen) that includes a pitcher <b>502</b> on a countertop <b>504</b> within the FOV <b>111</b> of an exterior-facing image sensor of the electronic device <b>120</b>. As such, in some implementations, the user <b>150</b> holds the electronic device <b>120</b> in his/her hand(s) similar to the operating environment <b>100</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0078" num="0077">In other words, in some implementations, the electronic device <b>120</b> is configured to present XR content and to enable optical see-through or video pass-through of at least a portion of the physical environment <b>505</b> on the display <b>122</b>. For example, the electronic device <b>120</b> corresponds to a mobile phone, tablet, laptop, near-eye system, wearable computing device, or the like.</p><p id="p-0079" num="0078">As shown in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, during the instance <b>500</b> (e.g., associated with time T<sub>1</sub>) of the first measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof detects a user speech input <b>506</b> (e.g., &#x201c;I would like to measure out 16 fluid ounces in the pitcher.&#x201d;) via one or more microphones. In <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref>, the FOV <b>111</b> of an exterior-facing image sensor of the electronic device <b>120</b> corresponds to a perspective view of the pitcher <b>502</b> on the countertop <b>504</b>. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the NLP <b>432</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) processes the user speech input <b>506</b> by converting the user speech input <b>506</b> to text. Continuing with this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the instructions engine <b>434</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines an instruction (or a set of instructions) based on the text version of the user speech input <b>506</b> (e.g., measure 16 fluid ounces in the pitcher).</p><p id="p-0080" num="0079">As shown in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, during the instance <b>510</b> (e.g., associated with time T<b>2</b>) of the first measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof outputs audible feedback <b>516</b> (e.g., &#x201c;Cannot estimate vessel's volume from the current view. Please get close and view the vessel from additional angles.&#x201d;) in response to detecting the user speech input <b>506</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the object volume determiner <b>442</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) is unable to determine one or more estimated dimensions (or measurements) for the pitcher <b>502</b> (e.g., currently empty in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>), such as the estimated available volume for the pitcher <b>502</b>, due to the lack of depth information for the pitcher <b>502</b> from the current POV. Therefore, continuing with this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) generates the audible feedback <b>516</b> in order to remedy the aforementioned inability of the electronic device <b>120</b> to determine one or more estimated dimensions (or measurements) for the pitcher <b>502</b>.</p><p id="p-0081" num="0080">As shown in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, during the instance <b>510</b> (e.g., associated with time T<b>2</b>) of the first measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof also generates and displays a bounding box <b>512</b> (or other XR overlay) proximate to the pitcher <b>502</b> within the XR environment <b>128</b> in order to highlight the user's intent to interact with the pitcher <b>502</b>. For example, the bounding box <b>512</b> (or other XR overlay) may correspond to a frame overlay, a glow effect, a spotlight effect, a visual pointer, and/or the like.</p><p id="p-0082" num="0081">For example, in response to the audible feedback <b>516</b> in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, the FOV <b>111</b> of an exterior-facing image sensor of the electronic device <b>120</b> changes to a top-down view of the pitcher <b>502</b> on the countertop <b>504</b> in <figref idref="DRAWINGS">FIGS. <b>5</b>C and <b>5</b>D</figref> (e.g., the user <b>150</b> ambulates to the countertop <b>504</b> with the electronic device <b>120</b> in hand).</p><p id="p-0083" num="0082">As shown in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, during the instance <b>520</b> (e.g., associated with time T<sub>3</sub>) of the first measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof outputs audible feedback <b>526</b> (e.g., &#x201c;Estimated available volume for vessel determined. Please start pouring the liquid to the fill line, and I will tell you when to stop.&#x201d;) after determining the one or more estimated dimensions (or measurements) for the pitcher <b>502</b> such as its estimated available volume while currently empty. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the object volume determiner <b>442</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines the estimated available volume for the empty pitcher <b>502</b>. Continuing with this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the object the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) generates and displays the XR overlay <b>522</b> indicating a fill line for water according to the instruction (or a set of instructions) from the user speech input <b>506</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> (e.g., measure 16 fluid ounces in the pitcher).</p><p id="p-0084" num="0083">For example, in response to the audible feedback <b>526</b> in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, the user <b>150</b> starts to fill the pitcher <b>502</b> with water <b>532</b> to accomplish their intended task (e.g., measure 16 fluid ounces in the pitcher). As shown in <figref idref="DRAWINGS">FIG. <b>5</b>D</figref>, during the instance <b>530</b> (e.g., associated with time T<sub>4</sub>) of the first measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof outputs audible feedback <b>536</b> (e.g., &#x201c;Please stop pouring. The vessel now contains 16 fluid ounces.&#x201d;) after determining that the estimated current fill volume for the pitcher <b>502</b> satisfies the instruction from the user speech input <b>506</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> (e.g., measure 16 fluid ounces in the pitcher). For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the current fill volume determiner <b>444</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines an estimated current fill volume of the pitcher <b>502</b>. Continuing with this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the object the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines whether the estimated current fill volume of the pitcher <b>502</b> satisfies the instruction from the user speech input <b>506</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> and generates the audible feedback <b>536</b>.</p><p id="p-0085" num="0084">As shown in <figref idref="DRAWINGS">FIG. <b>5</b>E</figref>, during the instance <b>540</b> (e.g., associated with time T<b>5</b>) of the first measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof generates and displays a visual indicator <b>542</b> (or other XR overlay) after determining that the estimated current fill volume for the pitcher <b>502</b> satisfies the instruction from the user speech input <b>506</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> (e.g., measure 16 fluid ounces in the pitcher). For example, the visual indicator <b>542</b> (or other XR overlay) indicates that the instruction from the user speech input <b>506</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> (e.g., measure 16 fluid ounces in the pitcher) is complete and serves as a reminder that the pitcher <b>502</b> currently holds 16 fluid ounces of water <b>532</b>. One of ordinary skill in the art will appreciate that the visual indicator <b>542</b> may take myriad forms and be replaced with audible reminder in various other implementations.</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>J</figref> illustrate a sequence of instances <b>600</b>, <b>610</b>, <b>620</b>, <b>630</b>, <b>640</b>, <b>650</b>, <b>660</b>, <b>670</b>, <b>680</b>, and <b>690</b> of a second measurement scenario in accordance with some implementations. While certain specific features are illustrated, those skilled in the art will appreciate from the present disclosure that various other features have not been illustrated for the sake of brevity, and so as not to obscure more pertinent aspects of the implementations disclosed herein.</p><p id="p-0087" num="0086">As shown in <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>J</figref>, the second measurement scenario includes a physical environment <b>505</b> and an XR environment <b>128</b> displayed on the display <b>122</b> of the electronic device <b>120</b>. The electronic device <b>120</b> presents the XR environment <b>128</b> to the user <b>150</b> while the user <b>150</b> is physically present within the physical environment <b>505</b> (e.g., a home kitchen) that includes a bowl <b>614</b> on a countertop <b>504</b> within the FOV <b>111</b> of an exterior-facing image sensor of the electronic device <b>120</b>. As such, in some implementations, the user <b>150</b> holds the electronic device <b>120</b> in his/her hand(s) similar to the operating environment <b>100</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0088" num="0087">In other words, in some implementations, the electronic device <b>120</b> is configured to present XR content and to enable optical see-through or video pass-through of at least a portion of the physical environment <b>505</b> on the display <b>122</b>. For example, the electronic device <b>120</b> corresponds to a mobile phone, tablet, laptop, near-eye system, wearable computing device, or the like.</p><p id="p-0089" num="0088">As shown in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, during the instance <b>600</b> (e.g., associated with time T<sub>1</sub>) of the second measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof detects a user speech input <b>602</b> (e.g., &#x201c;I would like to follow the recipe on this index card.&#x201d;) via one or more microphones. In <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>D</figref>, the FOV <b>111</b> of an exterior-facing image sensor of the electronic device <b>120</b> corresponds to a perspective view of a bowl <b>614</b> on the countertop <b>504</b>. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the NLP <b>432</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) processes the user speech input <b>602</b> by converting the user speech input <b>602</b> to text. Continuing with this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the instructions engine <b>434</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines an instruction or a set of instructions based on the text version of the user speech input <b>602</b> and also by performing text/character recognition on the index card <b>605</b> within the FOV <b>111</b> of an exterior-facing image sensor of the electronic device <b>120</b>. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the instructions engine <b>434</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) may separate the chocolate chip cookie recipe on the index card <b>605</b> into a series of sequential or non-sequential tasks.</p><p id="p-0090" num="0089">As shown in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, during the instance <b>610</b> (e.g., associated with time T<b>2</b>) of the second measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof detects a user speech input <b>612</b> (e.g., &#x201c;Let's start with the chocolate chips.&#x201d;) via one or more microphones. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the NLP <b>432</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) processes the user speech input <b>612</b> by converting the user speech input <b>612</b> to text. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the instructions engine <b>434</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines that the user <b>150</b> intends on starting with a first task relative to the chocolate chip cookie recipe on the index card <b>605</b> in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> that corresponds to measuring out two cups of chocolate chips.</p><p id="p-0091" num="0090">As shown in <figref idref="DRAWINGS">FIG. <b>6</b>C</figref>, during the instance <b>620</b> (e.g., associated with time T<sub>3</sub>) of the second measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof outputs audible feedback <b>622</b> (e.g., &#x201c;Please start filling the vessel with 2 cups of chocolate chips. I will tell you when to stop.&#x201d;) in response to detecting the user speech input <b>612</b> in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>. As shown in <figref idref="DRAWINGS">FIG. <b>6</b>C</figref>, during the instance <b>620</b>, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof also generates and displays a bounding box <b>624</b> (or other XR overlay) proximate to the bowl <b>614</b> within the XR environment <b>128</b> in order to highlight the user's intent to interact with the bowl <b>614</b>. For example, the bounding box <b>624</b> (or other XR overlay) may correspond to a frame overlay, a glow effect, a spotlight effect, a visual pointer, and/or the like.</p><p id="p-0092" num="0091">For example, in response to the audible feedback <b>622</b> in <figref idref="DRAWINGS">FIG. <b>6</b>C</figref>, the user <b>150</b> starts to fill the bowl <b>614</b> with chocolate chips <b>634</b> to accomplish their intended task (e.g., measure 2 cups of chocolate chips). As shown in <figref idref="DRAWINGS">FIG. <b>6</b>D</figref>, during the instance <b>630</b> (e.g., associated with time T<sub>4</sub>) of the second measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof outputs audible feedback <b>632</b> (e.g., &#x201c;I cannot estimate the current volume of chocolate chips in the vessel. Please show me a different angle of the vessel.&#x201d;) after determining that the estimated current fill volume for the bowl <b>614</b> is incalculable. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the object volume determiner <b>442</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) was able to determine one or more estimated dimensions (or measurements) for the bowl <b>614</b> while empty in <figref idref="DRAWINGS">FIG. <b>6</b>C</figref>. However, in this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the current fill volume determiner <b>444</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) is unable to determine the estimated current fill volume for the bowl <b>614</b> due to the lack of depth information for the bowl <b>614</b> from the current POV. Therefore, continuing with this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) generates the audible feedback <b>632</b> in order to remedy the aforementioned inability of the electronic device <b>120</b> to determine the estimated current fill volume for the bowl <b>614</b>.</p><p id="p-0093" num="0092">For example, in response to the audible feedback <b>632</b> in <figref idref="DRAWINGS">FIG. <b>6</b>D</figref>, the FOV <b>111</b> of an exterior-facing image sensor of the electronic device <b>120</b> changes to a top-down view of the bowl <b>614</b> on the countertop <b>504</b> in <figref idref="DRAWINGS">FIGS. <b>6</b>E and <b>6</b>F</figref> (e.g., the user <b>150</b> ambulates to the countertop <b>504</b> with the electronic device <b>120</b> in hand).</p><p id="p-0094" num="0093">As shown in <figref idref="DRAWINGS">FIG. <b>6</b>E</figref>, during the instance <b>640</b> (e.g., associated with time T<b>5</b>) of the second measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof outputs audible feedback <b>642</b> (e.g., &#x201c;You are very close to two cups of chocolate chips. Only add a bit more by following the fill line.&#x201d;) after determining that the estimated current fill volume for the bowl <b>614</b> does not satisfy the task from the user speech input <b>612</b> in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> (e.g., measure 2 cups of chocolate chips). For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the current fill volume determiner <b>444</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines an estimated current fill volume of the chocolate chips <b>634</b> within the bowl <b>614</b>. Continuing with this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the object the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines whether the estimated current fill volume of the chocolate chips <b>634</b> within the bowl <b>614</b> satisfies the task from the user speech input <b>612</b> in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> and generates the audible feedback <b>642</b>. Further continuing with this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the object the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) generates and displays the XR overlay <b>644</b> indicating a fill line for the appropriate amount of the chocolate chips <b>634</b>.</p><p id="p-0095" num="0094">For example, in response to the audible feedback <b>642</b> in <figref idref="DRAWINGS">FIG. <b>6</b>E</figref>, the user <b>150</b> adds more chocolate chips <b>634</b> to the bowl <b>614</b> to accomplish their intended task (e.g., measure 2 cups of chocolate chips). As shown in <figref idref="DRAWINGS">FIG. <b>6</b>F</figref>, during the instance <b>650</b> (e.g., associated with time T<sub>6</sub>) of the second measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof outputs audible feedback <b>652</b> (e.g., &#x201c;Please stop now, unless you intend to exceed the recipe's recommended amount of chocolate chips.&#x201d;) after determining that the estimated current fill volume of the chocolate chips <b>634</b> within the bowl <b>614</b> satisfies the task from the user speech input <b>612</b> in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the current fill volume determiner <b>444</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines an estimated current fill volume of the chocolate chips <b>634</b> within the bowl <b>614</b>. Continuing with this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the object the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines whether the estimated current fill volume of the chocolate chips <b>634</b> within the bowl <b>614</b> satisfies the first task from the user speech input <b>612</b> in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> and generates the audible feedback <b>652</b>.</p><p id="p-0096" num="0095">As shown in <figref idref="DRAWINGS">FIG. <b>6</b>G</figref>, during the instance <b>660</b> (e.g., associated with time T<sub>7</sub>) of the second measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof generates and displays a visual indicator <b>664</b> (or other XR overlay) after determining that the estimated current fill volume for the bowl <b>614</b> satisfies the task from the user speech input <b>612</b> in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> (e.g., measure 2 cups of chocolate chips). For example, the visual indicator <b>664</b> (or other XR overlay) indicates that the task from the user speech input <b>612</b> in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is complete and serves as a reminder that the bowl <b>614</b> currently holds 2 cups of chocolate chips. One of ordinary skill in the art will appreciate that the visual indicator <b>664</b> may take myriad forms and be replaced with audible reminder in various other implementations.</p><p id="p-0097" num="0096">As shown in <figref idref="DRAWINGS">FIG. <b>6</b>G</figref>, during the instance <b>660</b> (e.g., associated with time T<sub>7</sub>) of the second measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof also detects a user speech input <b>662</b> (e.g., &#x201c;Let's move onto the butter. Please mark out a quarter cup of butter for me&#x201d;) via one or more microphones. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the NLP <b>432</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) processes the user speech input <b>662</b> by converting the user speech input <b>662</b> to text. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the instructions engine <b>434</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines that the user <b>150</b> intends on moving onto a second task relative to the chocolate chip cookie recipe on the index card <b>605</b> in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> that corresponds to measuring out 0.25 cup of butter. In <figref idref="DRAWINGS">FIG. <b>6</b>G</figref>, a whole stick of butter <b>665</b> is present within the physical environment <b>505</b>. In this example, the whole stick of butter <b>665</b> is located on the countertop <b>504</b>.</p><p id="p-0098" num="0097">As shown in <figref idref="DRAWINGS">FIG. <b>6</b>H</figref>, during the instance <b>670</b> (e.g., associated with time T<sub>8</sub>) of the second measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof outputs audible feedback <b>672</b> (e.g., &#x201c;Please show a top-down view of the butter for an accurate apportionment marker.&#x201d;) in response to detecting the user speech input <b>662</b> in <figref idref="DRAWINGS">FIG. <b>6</b>G</figref>. As shown in <figref idref="DRAWINGS">FIG. <b>6</b>H</figref>, during the instance <b>670</b>, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof also generates and displays a bounding box <b>674</b> (or other XR overlay) proximate to the whole stick of butter <b>665</b> within the XR environment <b>128</b> in order to highlight the user's intent to interact with the whole stick of butter <b>665</b>. For example, the bounding box <b>674</b> (or other XR overlay) may correspond to a frame overlay, a glow effect, a spotlight effect, a visual pointer, and/or the like.</p><p id="p-0099" num="0098">For example, in response to the audible feedback <b>672</b> in <figref idref="DRAWINGS">FIG. <b>6</b>H</figref>, the FOV <b>111</b> of an exterior-facing image sensor of the electronic device <b>120</b> changes to a top-down view of the countertop <b>504</b> including the whole stick of butter <b>665</b> and the bowl <b>614</b> in <figref idref="DRAWINGS">FIGS. <b>61</b> and <b>6</b>J</figref> (e.g., the user <b>150</b> ambulates to the countertop <b>504</b> with the electronic device <b>120</b> in hand).</p><p id="p-0100" num="0099">As shown in <figref idref="DRAWINGS">FIG. <b>6</b>I</figref>, during the instance <b>680</b> (e.g., associated with time T<sub>9</sub>) of the second measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof outputs audible feedback <b>682</b> (e.g., &#x201c;Please cut the stick of butter at the apportionment marker and use the portion to the right of the arrow.&#x201d;) after determining the one or more estimated dimensions (or measurements) for the whole stick of butter <b>665</b> such as its estimated mass. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the object volume determiner <b>442</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines the estimated mass of the whole stick of butter <b>665</b>. Continuing with this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the object the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) generates the XR overlay <b>684</b> indicating an apportionment marker for measuring out 0.25 cup of butter from the whole stick of butter <b>665</b>.</p><p id="p-0101" num="0100">For example, in response to the audible feedback <b>682</b> in <figref idref="DRAWINGS">FIG. <b>6</b>I</figref>, the user <b>150</b> cuts the whole stick of butter <b>665</b> into portions <b>665</b><i>a </i>and <b>665</b><i>b </i>to accomplish their intended task (e.g., measure 0.25 cup of butter). As shown in <figref idref="DRAWINGS">FIG. <b>6</b>J</figref>, during the instance <b>690</b> (e.g., associated with time T<sub>10</sub>) of the second measurement scenario, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof outputs audible feedback <b>692</b> (e.g., &#x201c;Well done! Please keep the highlighted portion of the butter for the recipe.&#x201d;) after determining that the estimated mass of the portion <b>665</b><i>a </i>of the whole stick of butter <b>665</b> satisfies the second task relative to the chocolate chip cookie recipe on the index card <b>605</b> in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> associated with measuring out 0.25 cup of butter.</p><p id="p-0102" num="0101">As shown in <figref idref="DRAWINGS">FIG. <b>6</b>J</figref>, during the instance <b>690</b>, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof also generates and displays a visual indicator <b>694</b> (or other XR overlay) after determining that the portion of butter <b>665</b><i>a </i>satisfies the task from the user speech input <b>662</b> in <figref idref="DRAWINGS">FIG. <b>6</b>G</figref> (e.g., measure 0.25 cup of butter). For example, the visual indicator <b>694</b> (or other XR overlay) indicates that the task from the user speech input <b>662</b> in <figref idref="DRAWINGS">FIG. <b>6</b>G</figref> is complete and serves as a reminder that the portion of butter <b>665</b><i>a </i>corresponds to 0.25 cup of butter. One of ordinary skill in the art will appreciate that the visual indicator <b>694</b> may take myriad forms and be replaced with audible reminder in various other implementations.</p><p id="p-0103" num="0102">As shown in <figref idref="DRAWINGS">FIG. <b>6</b>J</figref>, during the instance <b>690</b>, the electronic device <b>120</b>, the controller <b>110</b>, or a suitable combination thereof further generates and displays a bounding box <b>696</b> (or other XR overlay) proximate to the portion of butter <b>665</b><i>a </i>within the XR environment <b>128</b> in order to highlight its correspondence with the visual indicator <b>694</b>. For example, the bounding box <b>696</b> (or other XR overlay) may correspond to a frame overlay, a glow effect, a spotlight effect, a visual pointer, and/or the like.</p><p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart representation of a method <b>700</b> of measuring physical objects in accordance with some implementations. In various implementations, the method <b>700</b> is performed by an electronic device including one or more processors, non-transitory memory, and a depth sensor (e.g., the controller <b>110</b> in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>; the electronic device <b>120</b> in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>3</b></figref>; or a suitable combination thereof), or a component thereof. In some implementations, the method <b>700</b> is performed by processing logic, including hardware, firmware, software, or a combination thereof. In some implementations, the method <b>700</b> is performed by a processor executing code stored in a non-transitory computer-readable medium (e.g., a memory). In various implementations, some operations in method <b>700</b> are, optionally, combined and/or the order of some operations is, optionally, changed.</p><p id="p-0105" num="0104">In some instances, as noted above, eyeballing volume and mass measurements for cooking or home improvement projects can be futile at best. Furthermore, estimating volume and mass measurements with a single camera is likewise a difficult task. In various implementations, a system estimates volume, dimensions, surface area, mass, and/or the like of a physical object by using an onboard depth sensor and/or image sensor. The estimates may also be accompanied with extended reality (XR) markers to aid a user in completing a cooking task, home improvement task, or the like.</p><p id="p-0106" num="0105">As represented by block <b>7</b>-<b>1</b>, the method <b>700</b> includes obtaining (e.g., receiving, retrieving, or determining/generating) a task associated with a physical object with a physical environment. For example, the task corresponds to an instruction, a portion of a set of instructions, or the end result of the set of instructions. As one example, the task corresponds to measuring out or apportioning a portion of the physical object, such as a stick of butter, for a recipe. As another example, the task corresponds to pouring a set amount of liquid into the physical object&#x2014;a vessel such as a measuring cup, mixing bowl, or the like. In some implementations, the task is provided by the user via voice input such as &#x201c;I'd like to fill this cup with 16 ounces of water.&#x201d; In some implementations, the task is inferred from a recipe card or other list based at least in part on text/object recognition, semantic segmentation, or the like. In some implementations, the task corresponds to eating the physical object and the XR overlay estimates the calories and nutritional profile for the physical object.</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>E</figref> illustrate a sequence of example instances of a first measurement scenario according to a first set of instructions (e.g., a verbal user request to fill a cup with X fluid ounces of water) in accordance with some implementations. In this example, the user <b>150</b> completes a single task related to measuring out 16 fluid ounces of water into the pitcher <b>502</b> Similarly, <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>J</figref> illustrate a sequence of example instances of a second measurement scenario according to a second set of instructions (e.g., a physical index card with a chocolate chip cookie recipe thereon) in accordance with some implementations. In this example, the user <b>150</b> completes two separate tasks related to chocolate chip cookie recipe: (A) measuring out two cups of chocolate chips <b>634</b> into a bowl <b>614</b> in <figref idref="DRAWINGS">FIGS. <b>6</b>B-<b>6</b>F</figref>; and (B) measuring out a quarter cup of butter from a whole stick of butter <b>665</b> in <figref idref="DRAWINGS">FIGS. <b>6</b>G-<b>6</b>I</figref>.</p><p id="p-0108" num="0107">In some implementations, obtaining the task includes performing text recognition on a physical or virtual object that includes a set of instructions. As one example, the task is obtained from a physical recipe card, a recipe on a website, a recipe in an email, or the like. In <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, for example, the electronic device or a component thereof (e.g., the instructions engine <b>434</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines an instruction or a set of instructions based on a text version of the user speech input <b>602</b> in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> and also by performing text/character recognition on the index card <b>605</b> within the FOV <b>111</b> of an exterior-facing image sensor of the electronic device <b>120</b>. For example, the electronic device or a component thereof (e.g., the instructions engine <b>434</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) may separate the chocolate chip cookie recipe on the index card <b>605</b> into a series of sequential or non-sequential tasks.</p><p id="p-0109" num="0108">In some implementations, obtaining the task includes performing natural language processing on speech data associated with a set of instructions. In <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, for example, the electronic device or a component thereof (e.g., the instructions engine <b>434</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines an instruction (or a set of instructions) based on the text version of the user speech input <b>506</b> (e.g., measure 16 fluid ounces in the pitcher).</p><p id="p-0110" num="0109">In some implementations, the task corresponds to apportioning the physical object. In some implementations, the XR overlay indicates a manner in which to apportion the physical object in order to achieve the task. For example, the XR overlay corresponds to a marker for measuring out 1 tablespoon of butter, 0.25 pounds of a wheel of cheese, or the like. In <figref idref="DRAWINGS">FIG. <b>6</b>I</figref>, for example, the electronic device or a component thereof (e.g., the object the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) generates the XR overlay <b>684</b> indicating an apportionment marker for measuring out 0.25 cup of butter from the whole stick of butter <b>665</b>.</p><p id="p-0111" num="0110">In some implementations, the physical object corresponds to a vessel, and the task corresponds to filling the physical object with another physical object (e.g., a liquid, semi-liquid, solid, or semi-solid substance such as water, oil, flour, seeds, chocolate chips, or the like). In some implementations, the XR overlay indicates a manner in which to fill the physical object in order to achieve the task. For example, the XR overlay corresponds to a marker for 16 fluid ounces of water, 2 cups of chocolate chips, or the like relative to the vessel size. In <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, for example, the electronic device or a component thereof (e.g., the object the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) generates and displays the XR overlay <b>522</b> indicating a fill line for water according to the instruction (or a set of instructions) from the user speech input <b>506</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> (e.g., measure 16 fluid ounces in the pitcher). As another example, in <figref idref="DRAWINGS">FIG. <b>6</b>E</figref>, the electronic device or a component thereof (e.g., the object the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) generates and displays the XR overlay <b>644</b> indicating a fill line for the appropriate amount of the chocolate chips <b>634</b> according to the recipe card <b>605</b> in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>.</p><p id="p-0112" num="0111">As represented by block <b>7</b>-<b>2</b>, the method <b>700</b> includes obtaining depth information, via the depth sensor, associated with a physical object in a physical setting. In some implementations, the depth information is collected by a depth sensor using techniques known in the art such as structured light, time-of-flight, LiDAR, or the like. In some implementations, the depth information corresponds to a mesh, point cloud, or the like of the physical environment. In some implementations, the depth information corresponds to a mesh, point cloud, or the like of a portion of the physical environment such as one or more physical objects that the user intends on interacting with. For example, with reference to <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>E</figref>, the electronic device obtains depth information associated with the physical environment <b>505</b> (e.g., the home kitchen) or one or more physical objects therein (e.g., the pitcher <b>502</b>).</p><p id="p-0113" num="0112">In some implementations, the depth information corresponds to a mesh for at least the physical object. In some implementations, the electronic device obtains (e.g., receives, retrieves, or determines/generates) a mesh that represents the physical environment (including the physical object) or at least the physical object itself.</p><p id="p-0114" num="0113">In some implementations, the depth information corresponds to a point cloud for at least the physical object. In some implementations, the electronic device obtains (e.g., receives, retrieves, or determines/generates) a point cloud that represents the physical environment (including the physical object) or at least the physical object itself.</p><p id="p-0115" num="0114">As represented by block <b>7</b>-<b>3</b>, the method <b>700</b> includes determining one or more measurements for the physical object based at least in part on the depth information. For example, the electronic device estimates the available (or unfilled) volume of a mixing bowl, a measuring cup, a stick of butter, etc. In some implementations, the electronic device may also leverage other input devices, such as an image sensor (for computer vision purposes), to determine the one or more measurements more accurately. In some implementations, the electronic device may prompt the user for additional angles or perspectives of the physical object if the one or more measurements cannot be determined.</p><p id="p-0116" num="0115">In some implementations, the one or more measurements correspond to at least one of a volume of the physical object, spatial dimensions of the physical object, a mass of the physical object, or a surface area of the physical object.</p><p id="p-0117" num="0116">As shown in <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>C</figref>, for example, the electronic device or a component thereof (e.g., the object volume determiner <b>442</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) determines the estimated available volume for the empty pitcher <b>502</b>. However, in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, the electronic device or a component thereof outputs audible feedback <b>516</b> (e.g., &#x201c;Cannot estimate vessel's volume from the current view. Please get close and view the vessel from additional angles.&#x201d;) in response to detecting the user speech input <b>506</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. For example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the object volume determiner <b>442</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) is unable to determine one or more estimated dimensions (or measurements) for the pitcher <b>502</b> (e.g., currently empty in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>), such as the estimated available volume for the pitcher <b>502</b>, due to the lack of depth information for the pitcher <b>502</b> from the current POV. Therefore, continuing with this example, the electronic device <b>120</b>, the controller <b>110</b>, a suitable combination of the electronic device <b>120</b> and the controller <b>110</b>, or a component thereof (e.g., the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) generates the audible feedback <b>516</b> in order to remedy the aforementioned inability of the electronic device <b>120</b> to determine one or more estimated dimensions (or measurements) for the pitcher <b>502</b>.</p><p id="p-0118" num="0117">As represented by block <b>7</b>-<b>4</b>, the method <b>700</b> includes obtaining (e.g., receiving, retrieving, or determining/generating) a graphical overlay based at least in part on the task and the one or more measurements for the physical object. For example, the graphical overlay corresponds to an XR overlay obtained from the virtual content library <b>461</b>. In another example, the graphical overlay corresponds to an XR overlay that is generated on-the-fly.</p><p id="p-0119" num="0118">As represented by block <b>7</b>-<b>5</b>, the method <b>700</b> includes causing presentation of the graphical overlay adjacent to a representation of the physical object, wherein the representation is obtained using sensor readings of the physical object. In some implementations, the XR overlay is composited with video pass-through or optical see-through of a physical environment including the physical object. In some implementations, the graphical overlay occludes the physical object. In some implementations, the graphical overlay is presented adjacent to (but not overlapping on) the physical object.</p><p id="p-0120" num="0119">In some implementations, the representation of the physical object corresponds optical see-through or video pass-through data associated with the physical environment. In this example, the sensor readings may correspond to image data of the physical environment captured by an exterior-facing image sensor. In some implementations, the device captures image data (e.g., the sensor readings) of the physical environment and performs object and/or semantic segmentation techniques on the image data in order to classify the physical object. In this example, the representation of the physical object corresponds to a 3D model obtained from the virtual content library <b>461</b> based on the classification for the physical object.</p><p id="p-0121" num="0120">In <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, for example, the electronic device or a component thereof (e.g., the object the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) generates and displays the XR overlay <b>522</b> indicating a fill line for water according to the instruction (or a set of instructions) from the user speech input <b>506</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> (e.g., measure 16 fluid ounces in the pitcher). As another example, in <figref idref="DRAWINGS">FIG. <b>6</b>E</figref>, the electronic device or a component thereof (e.g., the object the prompt/interrupt handler <b>446</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) generates and displays the XR overlay <b>644</b> indicating a fill line for the appropriate amount of the chocolate chips <b>634</b> according to the recipe card <b>605</b> in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>.</p><p id="p-0122" num="0121">While various aspects of implementations within the scope of the appended claims are described above, it should be apparent that the various features of implementations described above may be embodied in a wide variety of forms and that any specific structure and/or function described above is merely illustrative. Based on the present disclosure one skilled in the art should appreciate that an aspect described herein may be implemented independently of any other aspects and that two or more of these aspects may be combined in various ways. For example, an apparatus may be implemented and/or a method may be practiced using any number of the aspects set forth herein. In addition, such an apparatus may be implemented and/or such a method may be practiced using other structure and/or functionality in addition to or other than one or more of the aspects set forth herein.</p><p id="p-0123" num="0122">It will also be understood that, although the terms &#x201c;first,&#x201d; &#x201c;second,&#x201d; etc. may be used herein to describe various elements, these elements should not be limited by these terms. These terms are only used to distinguish one element from another. For example, a first node could be termed a second node, and, similarly, a second node could be termed a first node, which changing the meaning of the description, so long as all occurrences of the &#x201c;first node&#x201d; are renamed consistently and all occurrences of the &#x201c;second node&#x201d; are renamed consistently. The first node and the second node are both nodes, but they are not the same node.</p><p id="p-0124" num="0123">The terminology used herein is for the purpose of describing particular implementations only and is not intended to be limiting of the claims. As used in the description of the implementations and the appended claims, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will also be understood that the term &#x201c;and/or&#x201d; as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items. It will be further understood that the terms &#x201c;comprises&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.</p><p id="p-0125" num="0124">As used herein, the term &#x201c;if&#x201d; may be construed to mean &#x201c;when&#x201d; or &#x201c;upon&#x201d; or &#x201c;in response to determining&#x201d; or &#x201c;in accordance with a determination&#x201d; or &#x201c;in response to detecting,&#x201d; that a stated condition precedent is true, depending on the context. Similarly, the phrase &#x201c;if it is determined [that a stated condition precedent is true]&#x201d; or &#x201c;if [a stated condition precedent is true]&#x201d; or &#x201c;when [a stated condition precedent is true]&#x201d; may be construed to mean &#x201c;upon determining&#x201d; or &#x201c;in response to determining&#x201d; or &#x201c;in accordance with a determination&#x201d; or &#x201c;upon detecting&#x201d; or &#x201c;in response to detecting&#x201d; that the stated condition precedent is true, depending on the context.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>at an electronic device including one or more processors, a non-transitory memory, and a depth sensor:<claim-text>obtaining a task associated with a physical object within a physical environment;</claim-text><claim-text>obtaining depth information, via the depth sensor, associated with the physical environment;</claim-text><claim-text>determining one or more measurements for the physical object based at least in part on the depth information;</claim-text><claim-text>generating a graphical overlay for the task based at least in part on the task associated with the physical object and the one or more measurements for the physical object; and</claim-text><claim-text>causing presentation of the graphical overlay relative to a representation of the physical object, wherein the representation is obtained using sensor readings of the physical object.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>obtaining a set of instructions; and</claim-text><claim-text>determining the task associated with a physical object within a physical environment based on the set of instructions.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein obtaining the set of instructions includes performing text recognition on a physical or virtual object that includes the set of instructions.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein obtaining the set of instructions includes performing natural language processing on speech data associated with the set of instructions.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the set of instructions includes a sequence of multiple tasks, and wherein the task corresponds to one of the sequence of multiple tasks.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the task corresponds to apportioning the physical object.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the graphical overlay indicates a manner in which to apportion the physical object in order to achieve the task.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the physical object corresponds to a vessel, and wherein the task corresponds to filling the physical object with another physical object.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the graphical overlay indicates a manner in which to fill the physical object in order to achieve the task.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more measurements corresponds to at least one of a volume of the physical object, spatial dimensions of the physical object, a mass of the physical object, or a surface area of the physical object.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the depth information corresponds to one of a mesh for at least the physical object or a point cloud for at least the physical object.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A device comprising:<claim-text>a depth sensor;</claim-text><claim-text>one or more processors;</claim-text><claim-text>a non-transitory memory; and</claim-text><claim-text>one or more programs stored in the non-transitory memory, which, when executed by the one or more processors, cause the device to:<claim-text>obtain a task associated with a physical object within a physical environment;</claim-text><claim-text>obtain depth information, via the depth sensor, associated with the physical environment;</claim-text><claim-text>determine one or more measurements for the physical object based at least in part on the depth information;</claim-text><claim-text>generate a graphical overlay for the task based at least in part on the task associated with the physical object and the one or more measurements for the physical object; and</claim-text><claim-text>cause presentation of the graphical overlay relative to a representation of the physical object, wherein the representation is obtained using sensor readings of the physical object.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the one or more programs further cause the device to:<claim-text>obtain a set of instructions; and</claim-text><claim-text>determine the task associated with a physical object within a physical environment based on the set of instructions.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein obtaining the set of instructions includes performing text recognition on a physical or virtual object that includes the set of instructions.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein obtaining the set of instructions includes performing natural language processing on speech data associated with the set of instructions.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the set of instructions includes a sequence of multiple tasks, and wherein the task corresponds to one of the sequence of multiple tasks.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A non-transitory memory storing one or more programs, which, when executed by one or more processors of a device with a depth sensor, cause the device to:<claim-text>obtain a task associated with a physical object within a physical environment;</claim-text><claim-text>obtain depth information, via the depth sensor, associated with the physical environment;</claim-text><claim-text>determine one or more measurements for the physical object based at least in part on the depth information;</claim-text><claim-text>generate a graphical overlay for the task based at least in part on the task associated with the physical object and the one or more measurements for the physical object; and</claim-text><claim-text>cause presentation of the graphical overlay relative to a representation of the physical object, wherein the representation is obtained using sensor readings of the physical object.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory memory of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the one or more programs further cause the device to:<claim-text>obtain a set of instructions; and</claim-text><claim-text>determine the task associated with a physical object within a physical environment based on the set of instructions.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory memory of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein obtaining the set of instructions includes performing text recognition on a physical or virtual object that includes the set of instructions.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory memory of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein obtaining the set of instructions includes performing natural language processing on speech data associated with the set of instructions.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The non-transitory memory of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the set of instructions includes a sequence of multiple tasks, and wherein the task corresponds to one of the sequence of multiple tasks.</claim-text></claim></claims></us-patent-application>