<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004742A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004742</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363490</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>292</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00771</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00718</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>292</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30232</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>2209</main-group><subgroup>27</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Storage and Processing of Intermediate Features in Neural Networks</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Western Digital Technologies, Inc.</orgname><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Xiong</last-name><first-name>Shaomin</first-name><address><city>Fremont</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Hirano</last-name><first-name>Toshiki</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Ayad</last-name><first-name>Ramy</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Kah</last-name><first-name>Damien</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods described herein provide for the use and storage of intermediate layer data within a neural network processing system. A first neural network, such as an object detection neural network may receive and process raw video image data to generate output utilized for metadata creation. Secondary neural networks may be configured to accept input data from one or more intermediate layers of the first neural network instead of the raw video image data. In this way, the initial data processed by the intermediate layers of the first neural network can be stored and utilized as a shortcut for processing additional features or attributes within the video image data. This alleviates the need to process video image data multiple times in different neural networks. The intermediate layer data can be stored in a different and often cheaper storage system and recalled faster and with fewer resources for future use.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="105.66mm" wi="158.75mm" file="US20230004742A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="251.46mm" wi="168.06mm" orientation="landscape" file="US20230004742A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="236.98mm" wi="138.35mm" orientation="landscape" file="US20230004742A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="243.59mm" wi="162.98mm" orientation="landscape" file="US20230004742A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="259.42mm" wi="163.07mm" orientation="landscape" file="US20230004742A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="257.98mm" wi="167.64mm" orientation="landscape" file="US20230004742A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="219.03mm" wi="153.16mm" file="US20230004742A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="223.18mm" wi="153.25mm" file="US20230004742A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="241.98mm" wi="150.71mm" file="US20230004742A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="241.98mm" wi="146.64mm" file="US20230004742A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to neural network processing. More particularly, the present disclosure technically relates to various systems and methods to store and process intermediate feature data generated within neural networks, such as image and video data focused neural networks.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">As technology has grown over the last decade, the quantity of time-series data such as video content has increased dramatically. This increase in time-series data has generated a greater demand for automatic classification. In response, neural networks and other artificial intelligence methods have been increasingly utilized to generate automatic classifications, specific detections, and segmentations. In the case of video processing, computer vision trends have progressively focused on object detection, image classification, and other segmentation tasks to parse semantic meaning from video content.</p><p id="p-0004" num="0003">However, as time-series data and the neural networks used to analyze them have increased in size and complexity, a higher computational demand is created. More data to process requires more processing power to compile all the data. Likewise, more complex neural networks require more processing power to parse the data. Traditional methods of handling these problems include trading a decrease in output accuracy for increased processing speed, or conversely, increasing the output accuracy for a decrease in processing speed. The current state of the art suggests that increasing both output accuracy and speed is achieved through providing an increase in computational power.</p><p id="p-0005" num="0004">Increasingly, there is a need for more efficient processing of video data at the edges (or end points) of networks where video content is created, where edge device prices are important (limiting processing power per cost point) and the energy consumed for increasing computation power can be a constrained resource. Further, the rising volume of video data makes transmitting the data to some distant location for computation and storage less and less practical. This is particularly true of video cameras used in surveillance applications, where it is highly desirable to process data internal to the camera itself before sending the results of the computations elsewhere. There is also a need to store data efficiently in an edge device since there may be a need to subsequently analyze the video data at a later time using different criteria.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">Systems and methods for storage and processing of intermediate features in neural networks in accordance with embodiments of the invention are disclosed. In many embodiments, a device includes a processor configured to process video data which includes storing received video data into a primary storage device type, and inputting the video data into a first artificial neural network. The first artificial neural network includes at least an input layer, one or more hidden layers, and an output layer, generating intermediate layer data by utilizing the outputs of one or more of the intermediate layers of the first artificial neural network. The intermediate layer data is stored into a second storage device type which is different from the first device type, is used as input data for one or more secondary artificial neural networks.</p><p id="p-0007" num="0006">In various embodiments, the second storage device type is different from the first device type in an access speed of read or write operations.</p><p id="p-0008" num="0007">In further embodiments, the device further includes (a) a first storage device of the first storage device type configured to store the received video data and (b) a second storage device of the second storage device type configured to store the intermediate layer</p><p id="p-0009" num="0008">In a variety of embodiments, the device further includes a storage device, the storage device comprising: (a) first media of the first storage device type, the first media configured to store the received video data and (b) second media of the second storage device type, the second media configured to store the intermediate layer.</p><p id="p-0010" num="0009">In additional embodiments, the first artificial neural network is an object detection neural network.</p><p id="p-0011" num="0010">In yet further embodiments, the object detection neural network is a convolutional neural network.</p><p id="p-0012" num="0011">In still additional embodiments, the second storage device type is a fast-access storage device.</p><p id="p-0013" num="0012">In more embodiments, the plurality of secondary artificial neural networks includes one or more attribute detection neural networks.</p><p id="p-0014" num="0013">In yet more additional embodiments, the plurality of secondary artificial neural networks includes at least a tracking neural network.</p><p id="p-0015" num="0014">In additional further embodiments, the plurality of secondary artificial neural networks includes at least a recognition neural network.</p><p id="p-0016" num="0015">In still more embodiments, the intermediate layer data can be utilized as input data for a first and a second secondary artificial neural network of the secondary artificial neural networks.</p><p id="p-0017" num="0016">In a variety of embodiments, the input data utilized from the intermediate layer data can be formatted as a tensor set of intermediate layer data generated from a selection of intermediate layer outputs from the first artificial neural network.</p><p id="p-0018" num="0017">In more embodiments again, the first secondary artificial neural network utilizes a first tensor set of intermediate layer data and the second secondary artificial neural network utilizes a second tensor set of intermediate layer data.</p><p id="p-0019" num="0018">In more further embodiments, the first tensor set of intermediate layer data is generated from a first group of intermediate hidden layer data outputs from the first artificial neural network, and the second tensor set of intermediate layer data is generated from a second group of intermediate hidden layer data outputs from the first artificial neural network.</p><p id="p-0020" num="0019">In more additional embodiments, the output of the first and secondary artificial neural networks are utilized to generate a plurality of metadata associated with the video data.</p><p id="p-0021" num="0020">In certain embodiments, a one of the secondary artificial neural networks is selected to generate new metadata after the plurality of metadata has been generated.</p><p id="p-0022" num="0021">In some embodiments, the selected secondary artificial neural network utilizes intermediate hidden layer data stored within the second storage device type.</p><p id="p-0023" num="0022">In a number of embodiments, method for reducing subsequent processing of video data, includes defining a target to search within a video data pool includes a plurality of video data sources and associated intermediate layer data for each video data source, determining at least one or more features associated with the defined target, searching for the target within the plurality of video data source intermediate layer data, locating the target within multiple video data sources, and determining a path for the defined target.</p><p id="p-0024" num="0023">In still more various embodiments, the video data pool includes a plurality of security cameras.</p><p id="p-0025" num="0024">In many additional embodiments, the intermediate layer data is generated by a primary artificial neural network and the searching of the target includes utilizing a plurality of secondary artificial neural networks.</p><p id="p-0026" num="0025">In a variety of embodiments, a device includes a processor configured to process video data the processing includes inputting the video data into a primary artificial neural network the first artificial neural network includes at least an input layer, one or more hidden layers, and an output layer, generating intermediate layer data by utilizing the outputs of one or more of the intermediate layers of the first artificial neural network, storing the intermediate layer data, and utilizing the intermediate layer data as input data for a plurality of secondary artificial neural networks.</p><p id="p-0027" num="0026">In particular embodiments, the intermediate layer data is utilized as input data for a first and a second secondary artificial neural networks of the secondary artificial neural networks.</p><p id="p-0028" num="0027">In certain embodiments, the input data utilized from the intermediate layer data is formatted as a tensor set of intermediate layer data generated from a selection of intermediate layer outputs from the first artificial neural network.</p><p id="p-0029" num="0028">In some further embodiments, the first secondary artificial neural network utilizes a first tensor set of intermediate layer data and the second secondary artificial neural network utilizes a second tensor set of intermediate layer data.</p><p id="p-0030" num="0029">In a number of embodiments, the first tensor set of intermediate layer data is generated from a first group of intermediate hidden layer data outputs from the first artificial neural network and the second tensor set of intermediate layer data is generated from a second group of intermediate hidden layer data outputs from the first artificial neural network.</p><p id="p-0031" num="0030">In yet more embodiments, the output of the first and secondary artificial neural networks are utilized to generate a plurality of metadata associated with the video data.</p><p id="p-0032" num="0031">In further additional embodiments, one of the secondary artificial neural networks is selected to generate new metadata after the plurality of metadata has been generated.</p><p id="p-0033" num="0032">In still more embodiments, the selected secondary artificial neural network utilizes intermediate hidden layer data stored.</p><p id="p-0034" num="0033">In various additional embodiments, the first artificial neural network is an object detection neural network, and the secondary artificial neural networks comprise one or more of: an attribute detection neural network, a tracking neural network, and a recognition neural network.</p><p id="p-0035" num="0034">Although the description above contains many specificities, these should not be construed as limiting the scope of the invention but as merely providing illustrations of some of the presently preferred embodiments of the invention. Various other embodiments are possible within its scope. Accordingly, the scope of the invention should be determined not by the embodiments illustrated, but by the appended claims and their equivalents.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0036" num="0035">The above, and other, aspects, features, and advantages of several embodiments of the present disclosure will be more apparent from the following description as presented in conjunction with the following several figures of the drawings.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a conceptual illustration of a video image data processing system in accordance with an embodiment of the disclosure;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a conceptual illustration of an artificial neural network in accordance with an embodiment of the disclosure;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a conceptual illustration of a neural network video processing and storage system in accordance with an embodiment of the disclosure;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a conceptual illustration of a neural network video processing and storage system in accordance with an embodiment of the disclosure;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a conceptual illustration of a neural network video processing and storage system in accordance with an embodiment of the disclosure;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of a process using a neural network video processing and storage system in accordance with an embodiment of the disclosure;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of a process using a neural network video processing and storage system in accordance with an embodiment of the disclosure;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart of a process utilizing intermediate layer data in a neural network video processing and storage system in accordance with an embodiment of the disclosure; and</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart of a process using a neural network video processing and storage system in accordance with an embodiment of the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0046" num="0045">Corresponding reference characters indicate corresponding components throughout the several figures of the drawings. Elements in the several figures are illustrated for simplicity and clarity and have not necessarily been drawn to scale. For example, the dimensions of some of the elements in the figures might be emphasized relative to other elements for facilitating understanding of the various presently disclosed embodiments. In addition, common, but well-understood, elements that are useful or necessary in a commercially feasible embodiment are often not depicted in order to facilitate a less obstructed view of these various embodiments of the present disclosure.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0047" num="0046">In response to the problems described above, systems and methods are discussed herein that describe processes for utilizing filters (i.e., intermediate layers) to streamline the processing of video image data within neural networks. Specifically, many embodiments of the disclosure can limit the amount of processing required for video image data by utilizing and/or sharing the outputs of intermediate (or hidden) layers of the neural network. Use is also made of different types of video data storage having different characteristics depending upon the nature of the data stored therein.</p><p id="p-0048" num="0047">Embodiments of the present disclosure can be utilized in a variety of fields including general video analytics, facial recognition, object segmentation, object recognition, autonomous driving, traffic flow detection, drone navigation/operation, stock counting, inventory control, and other automation-based tasks that generate time-series based data. The use of these embodiments can result in fewer required computational resources to produce similarly accurate results compared to a traditional convolutional or other neural network. In this way, more deployment options may become available as computational resources increase and become more readily available on smaller and less expensive electronic devices.</p><p id="p-0049" num="0048">Aspects of the present disclosure may be embodied as an apparatus, system, method, or computer program product. Accordingly, aspects of the present disclosure may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, or the like) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a &#x201c;function,&#x201d; &#x201c;module,&#x201d; &#x201c;apparatus,&#x201d; or &#x201c;system.&#x201d; Furthermore, aspects of the present disclosure may take the form of a computer program product embodied in one or more non-transitory computer-readable storage media storing computer-readable and/or executable program code. Many of the functional units described in this specification have been labeled as functions, in order to emphasize their implementation independence more particularly. For example, a function may be implemented as a hardware circuit comprising custom VLSI circuits or gate arrays, off-the-shelf semiconductors such as logic chips, transistors, field-programmable gate arrays (&#x201c;FPGAs&#x201d;) or other discrete components. A function may also be implemented in programmable hardware devices such as programmable array logic, programmable logic devices, or the like.</p><p id="p-0050" num="0049">&#x201c;Neural network&#x201d; refers to any logic, circuitry, component, chip, die, package, module, system, sub-system, or computing system configured to perform tasks by imitating biological neural networks of people or animals. Neural network, as used herein, may also be referred to as an artificial or deep neural network (DNN). Examples of neural networks that may be used with various embodiments of the disclosed solution include, but are not limited to, convolutional neural networks, feed forward neural networks, radial basis neural network, recurrent neural networks, modular neural networks, and the like. Certain neural networks may be designed for specific tasks such as object detection and/or image classification. Examples of neural networks suitable for object detection include, but are not limited to, Region-based Convolutional Neural Network (RCNN), Faster Region-based Convolutional Neural Network (Faster R-CNN), You Only Look Once (YOLO), and the like. Examples of neural networks suitable for image classification may include, but are not limited to, Googlenet Inception, Resnet, Mobilenet, Densenet and Efficientnet.</p><p id="p-0051" num="0050">A neural network may include both the logic, software, firmware, and/or circuitry for implementing the neural network as well as the data and metadata for operating the neural network. One or more of these components for a neural network may be embodied in one or more of a variety of repositories, including in one or more files, databases, folders, or the like. The neural network used with embodiments disclosed herein may employ one or more of a variety of learning models including, but not limited to, supervised learning, unsupervised learning, and reinforcement learning. These learning models may employ various backpropagation techniques.</p><p id="p-0052" num="0051">Functions or other computer-based instructions may also be implemented at least partially in software for execution by various types of processors. An identified function of executable code may, for instance, comprise one or more physical or logical blocks of computer instructions that may, for instance, be organized as an object, procedure, or function. Nevertheless, the executables of an identified function need not be physically located together but may comprise disparate instructions stored in different locations which, when joined logically together, comprise the function and achieve the stated purpose for the function.</p><p id="p-0053" num="0052">Indeed, a function of executable code may include a single instruction, or many instructions, and may even be distributed over several different code segments, among different programs, across several storage devices, or the like. Where a function or portions of a function are implemented in software, the software portions may be stored on one or more computer-readable and/or executable storage media. Any combination of one or more computer-readable storage media may be utilized. A computer-readable storage medium may include, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing, but would not include propagating signals. In the context of this document, a computer readable and/or executable storage medium may be any tangible and/or non-transitory medium that may contain or store a program for use by or in connection with an instruction execution system, apparatus, processor, or device.</p><p id="p-0054" num="0053">Computer program code for carrying out operations for aspects of the present disclosure may be written in any combination of one or more programming languages, including an object-oriented programming language such as Python, Java, Smalltalk, C++, C#, Objective C, or the like, conventional procedural programming languages, such as the &#x201c;C&#x201d; programming language, scripting programming languages, assembly languages, and/or other similar programming languages. The program code may execute partly or entirely on one or more of a user's computer and/or on a remote computer or server over a data network or the like.</p><p id="p-0055" num="0054">A component, as used herein, comprises a tangible, physical, non-transitory device. For example, a component may be implemented as a hardware logic circuit comprising custom VLSI circuits, gate arrays, or other integrated circuits; off-the-shelf semiconductors such as logic chips, transistors, or other discrete devices; and/or other mechanical or electrical devices. A component may also be implemented in programmable hardware devices such as field programmable gate arrays, programmable array logic, programmable logic devices, or the like. A component may comprise one or more silicon integrated circuit devices (e.g., chips, die, die planes, packages) or other discrete electrical devices, in electrical communication with one or more other components through electrical lines of a printed circuit board (PCB) or the like. Each of the functions, logics and/or modules described herein, in certain embodiments, may alternatively be embodied by or implemented as a component.</p><p id="p-0056" num="0055">A circuit, as used herein, comprises a set of one or more electrical and/or electronic components providing one or more pathways for electrical current. In certain embodiments, a circuit may include a return pathway for electrical current, so that the circuit is a closed loop. In another embodiment, however, a set of components that does not include a return pathway for electrical current may be referred to as a circuit (e.g., an open loop). For example, an integrated circuit may be referred to as a circuit regardless of whether the integrated circuit is coupled to ground (as a return pathway for electrical current) or not. In various embodiments, a circuit may include a portion of an integrated circuit, an integrated circuit, a set of integrated circuits, a set of non-integrated electrical and/or electrical components with or without integrated circuit devices, or the like. In some embodiments, a circuit may include custom VLSI circuits, gate arrays, logic circuits, or other integrated circuits; off-the-shelf semiconductors such as logic chips, transistors, or other discrete devices; and/or other mechanical or electrical devices. A circuit may also be implemented as a synthesized circuit in a programmable hardware device such as field programmable gate array, programmable array logic, programmable logic device, or the like (e.g., as firmware, a netlist, or the like). A circuit may comprise one or more silicon integrated circuit devices (e.g., chips, die, die planes, packages) or other discrete electrical devices, in electrical communication with one or more other components through electrical lines of a printed circuit board (PCB) or the like. Each of the functions, logics, and/or modules described herein, in certain embodiments, may be embodied by or implemented as a circuit.</p><p id="p-0057" num="0056">Reference throughout this specification to &#x201c;one embodiment,&#x201d; &#x201c;an embodiment,&#x201d; or similar language means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the present disclosure. Thus, appearances of the phrases &#x201c;in one embodiment,&#x201d; &#x201c;in an embodiment,&#x201d; and similar language throughout this specification may, but do not necessarily, all refer to the same embodiment, but mean &#x201c;one or more but not all embodiments&#x201d; unless expressly specified otherwise. The terms &#x201c;including,&#x201d; &#x201c;comprising,&#x201d; &#x201c;having,&#x201d; and variations thereof mean &#x201c;including but not limited to&#x201d;, unless expressly specified otherwise. An enumerated listing of items does not imply that any or all of the items are mutually exclusive and/or mutually inclusive, unless expressly specified otherwise. The terms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; also refer to &#x201c;one or more&#x201d; unless expressly specified otherwise.</p><p id="p-0058" num="0057">Further, as used herein, reference to reading, writing, loading, storing, buffering, and/or transferring data can include the entirety of the data, a portion of the data, a set of the data, and/or a subset of the data. Likewise, reference to reading, writing, loading, storing, buffering, and/or transferring non-host data can include the entirety of the non-host data, a portion of the non-host data, a set of the non-host data, and/or a subset of the non-host data.</p><p id="p-0059" num="0058">Lastly, the terms &#x201c;or&#x201d; and &#x201c;and/or&#x201d; as used herein are to be interpreted as inclusive or meaning any one or any combination. Therefore, &#x201c;A, B or C&#x201d; or &#x201c;A, B and/or C&#x201d; mean &#x201c;any of the following: A; B; C; A and B; A and C; B and C; A, B and C.&#x201d; An exception to this definition will occur only when a combination of elements, functions, steps, or acts are in some way inherently mutually exclusive.</p><p id="p-0060" num="0059">Aspects of the present disclosure are described below with reference to schematic flowchart diagrams and/or schematic block diagrams of methods, apparatuses, systems, and computer program products according to embodiments of the disclosure. It will be understood that each block of the schematic flowchart diagrams and/or schematic block diagrams, and combinations of blocks in the schematic flowchart diagrams and/or schematic block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a computer or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor or other programmable data processing apparatus, create means for implementing the functions and/or acts specified in the schematic flowchart diagrams and/or schematic block diagrams block or blocks.</p><p id="p-0061" num="0060">It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. Other steps and methods may be conceived that are equivalent in function, logic, or effect to one or more blocks, or portions thereof, of the illustrated figures. Although various arrow types and line types may be employed in the flowchart and/or block diagrams, they are understood not to limit the scope of the corresponding embodiments. For instance, an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted embodiment.</p><p id="p-0062" num="0061">In the following detailed description, reference is made to the accompanying drawings, which form a part thereof. The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects, embodiments, and features described above, further aspects, embodiments, and features will become apparent by reference to the drawings and the following detailed description. The description of elements in each figure may refer to elements of proceeding figures. Like numbers may refer to like elements in the figures, including alternate embodiments of like elements.</p><p id="p-0063" num="0062">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a conceptual illustration of a video image data processing system <b>100</b> in accordance with an embodiment of the disclosure is shown. In many embodiments, it may be desired to monitor one or more visual areas by installing security cameras <b>150</b> within those areas. The security cameras <b>150</b> can generate a plurality of video image data (i.e., &#x201c;video content&#x201d;) that can be processed. In a number of embodiments, the processing of the video image data will be configured to determine if one or more specific objects are within the monitored visual areas. In a number of embodiments, this detection may be presented as an inference map image from an object detection neural network which can be a segmentation or panoptic map. These maps can be utilized as a classification as to whether a specific object is present within the input data or not. These maps can be generated as an output from a neural network such as, but not limited to, a convolutional neural network (CNN). By way of example, and not limitation, video image data processing can be established to detect the presence of one or more pedestrians within the monitored visual areas. It will be understood by those skilled in the art that the video image data processing may be performed by processors internal to security cameras <b>150</b> or elsewhere in the system or in some combination thereof. The video image data processing may be implemented in software operating in conventional processors (e.g., CPU, MPU, GPU, RISC, etc.), and/or software operating in specifically purposed processors optimized to implement neural networks&#x2014;or some combination thereof. In fact, the entire system may be considered a processor or a distributed processor.</p><p id="p-0064" num="0063">Often, the video image data processing system <b>100</b> will process the video image data within a centralized video processing server <b>110</b>, although some embodiments may offload various processing tasks to other devices such as, but not limited to servers <b>130</b> which may be specialized or edge type servers, or internal to the security cameras <b>150</b> (themselves edge network devices). A video processing server <b>110</b> is often connected to a network <b>120</b> such as the Internet as depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. A plurality of security cameras <b>150</b> can also be attached to the network <b>120</b> such that they are communicatively coupled to the video processing server <b>110</b> which comprises one or more processors for processing video image data like, for example, CPUs, MPUs, GPUs, etc. Although the embodiment of <figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts security cameras <b>150</b>, it will be understood by those skilled in the art that any video image data capture device may be utilized as required by the desired application.</p><p id="p-0065" num="0064">The security cameras <b>150</b> can be wired directly to the network <b>120</b> or may be wirelessly connected via one or more wireless access points <b>160</b>. In this way, a variety of potential deployment arrangements may be realized to properly cover the desired areas for surveillance. In theory, there is no limit to the number of deployed security cameras <b>150</b> or other video image data capture devices that may be communicatively coupled with the video processing server <b>110</b>. The limitations experienced may relate to the available bandwidth of the network <b>120</b> and computational resources of the video processing server <b>110</b> or other supplemental server <b>130</b>. As discussed below, many embodiments of the video image data processing system <b>100</b> can handle the bulk of the processing locally by security cameras <b>150</b> or other edge network devices <b>140</b> to minimize network traffic and reduce the need for centralized computing resources like video processing server <b>110</b> and servers <b>130</b>.</p><p id="p-0066" num="0065">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a conceptual illustration of an artificial neural network <b>200</b> in accordance with an embodiment of the disclosure is shown. At a high level, the artificial neural network <b>200</b> comprises an input layer <b>202</b>, one or more intermediate layers <b>204</b>, and an output layer <b>206</b>. The artificial neural network <b>200</b> comprises a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process the signal and then trigger additional artificial neurons within the next layer of the neural network. As those skilled in the art will recognize, the artificial neural network <b>200</b> depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is shown as an illustrative example and various embodiments may comprise artificial neural networks that can accept more than one type of input and can provide more than one type of output.</p><p id="p-0067" num="0066">In a typical embodiment, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function (called an activation function) of the sum of the artificial neuron's inputs. The connections between artificial neurons are called &#x201c;edges&#x201d; or axons. Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold (trigger threshold) such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals propagate from the first layer (the input layer <b>202</b>) to the last layer (the output layer <b>206</b>), possibly after traversing one or more intermediate layers (also called hidden layers) <b>204</b>.</p><p id="p-0068" num="0067">The inputs to an artificial neural network may vary depending on the problem being addressed. In object detection, the inputs may be data representing pixel values for certain pixels within an image or frame. In one embodiment the artificial neural network <b>200</b> comprises a series of hidden layers in which each neuron is fully connected to neurons of the next layer. The artificial neural network <b>200</b> may utilize an activation function such as sigmoid, nonlinear, or a rectified linear unit (ReLU), upon the sum of the weighted inputs for example. The last layer in the artificial neural network may implement a regression function such as SoftMax regression to produce the classified or predicted classifications output for object detection as output <b>210</b>. In further embodiments, a sigmoid function can be used and position prediction may need raw output transformation into linear and/or non-linear coordinates.</p><p id="p-0069" num="0068">In certain embodiments, the artificial neural network <b>200</b> is trained prior to deployment and to conserve operational resources. However, some embodiments may utilize ongoing training of the artificial neural network <b>200</b> especially when operational resource constraints such as die area and performance are less critical.</p><p id="p-0070" num="0069">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a conceptual illustration of a neural network video processing and storage system in accordance with an embodiment of the disclosure is shown. Present in <figref idref="DRAWINGS">FIG. <b>3</b></figref> are a video data source <b>305</b>, a first artificial neural network <b>310</b>, a first storage device type <b>320</b> (shown as &#x201c;first type storage&#x201d;), a second storage device type <b>330</b> (shown as &#x201c;second type storage&#x201d;), a second artificial neural network <b>350</b>, a N<sup>th </sup>artificial neural network <b>360</b>, and a metadata generator <b>390</b>. The video data source <b>305</b> may be coupled to an input of the first artificial neural network <b>310</b> and to the first storage device type <b>320</b>. A first output of the first artificial neural network <b>310</b> may be coupled to the metadata generator <b>390</b>. A second output of the first artificial neural network <b>310</b> may be coupled to the second data storage device type <b>330</b>. The second output of the first artificial neural network <b>310</b> may also be coupled to an input of the second artificial neural network <b>350</b> and to an input of the N<sup>th </sup>artificial neural network <b>360</b>. The metadata generator <b>390</b> may be coupled to an output of the second artificial neural network <b>350</b> and an output of the N<sup>th </sup>artificial neural network <b>360</b>.</p><p id="p-0071" num="0070">Although not shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, there may be additional artificial neural networks present that may be coupled in parallel with the second artificial neural network <b>350</b> and the N<sup>th </sup>artificial neural network <b>360</b>. Each additional artificial neural network has an input that may be coupled to the second output of the first (or &#x201c;primary&#x201d;) artificial neural network <b>310</b> and an output that may be coupled to the metadata generator <b>390</b>. It is contemplated that there may be an arbitrary number of these additional artificial neural networks, the exact number of which may be a matter of application need or design choice. The second (or &#x201c;secondary&#x201d;) artificial neural network <b>350</b>, the N<sup>th </sup>artificial neural network <b>360</b>, and any others present but not shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be described collectively as secondary artificial neural networks. In a number of embodiments, the primary artificial neural network is a first neural network that processes video image data and outputs data into one or more secondary artificial neural networks.</p><p id="p-0072" num="0071">In many embodiments, the video data source <b>305</b> presents raw video image data from one or more surveillance or security cameras or other devices that may be analyzed by the first artificial neural network <b>310</b>. Internal to the first artificial neural network <b>310</b> may be an input layer, a series of intermediate layers, and an output layer. Each intermediate layer may output intermediate layer data which may then be input to the next intermediate layer (or input to the output layer in the case of the last intermediate layer). The first output of the first artificial neural network <b>310</b> may be the output of its output layer. The data at this first output may be presented to the metadata generator <b>390</b> which may generate metadata (e.g., the results of the computations). The second output of the first artificial neural network <b>310</b> may be the output of one or more of its intermediate layers.</p><p id="p-0073" num="0072">In many embodiments, the first artificial neural network <b>310</b> and the secondary artificial neural networks (<b>350</b>, <b>360</b>, etc.) may implement models for detecting objects and/or their attributes and/or the like. In a number of embodiments, these models may comprise coefficients for use in multiply-accumulate operations performed by each of the neurons (or nodes) present in the artificial neural networks <b>310</b>, <b>350</b>, <b>360</b>, etc. Typically, these models may be derived from a training sequence performed before deployment, though in some embodiments the training can occur within the neural network video processing and storage system.</p><p id="p-0074" num="0073">The raw video data from video data source <b>305</b> may be stored by first data storage device type <b>320</b>. In various embodiments, the first data storage device type <b>320</b> may be a fast-writing storage device optimized for continuous video writing. In many embodiments, the data stored in first data storage device type <b>320</b> may typically be archival data which may only be accessed occasionally if additional analysis is required later, so it may be optimized for fast writing to keep up with the continuous raw video data from video data source <b>305</b>.</p><p id="p-0075" num="0074">In more embodiments, the second data storage device type <b>330</b> may be a fast-access storage device. In further embodiments, the first data storage device type <b>320</b> and second data storage device type <b>330</b> may be a single device such as, but not limited to, a hybrid hard disk drive with one or more solid state memory arrays available within the drive. Alternatively, certain embodiments may use a hybrid storage system comprising rotating magnetic recording media and solid state memory arrays. The intermediate layer data from the second output of the first artificial neural network <b>310</b> may be used repeatedly for either current or future analysis by the secondary neural networks <b>350</b>, <b>360</b>, and any others not shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. This may entail a high volume of random access reads and second data storage device type <b>330</b> which may be optimized for fast read access to the data.</p><p id="p-0076" num="0075">In many embodiments, the first artificial neural network <b>310</b> may comprise, for example, an object detection neural network, though other types of neural networks may be used. In some embodiments, the second artificial neural network <b>350</b>, the Nth artificial neural network <b>360</b>, and any other secondary neural networks present may use the intermediate layer data from first artificial neural network <b>310</b> to obtain more detail from, for example, the objects detected by first artificial neural network <b>310</b>.</p><p id="p-0077" num="0076">If a detected object is a person, for example, a secondary neural network comprising a recognition neural network may, for example, identify the person; and/or a secondary neural network comprising a tracking neural network may, for example, follow the path taken by the person across the field of vision of multiple cameras; and/or a secondary neural network comprising an attribute detection neural network may, for example, identify any of a number of attributes (or characteristics) like, for example, age, gender, hair color, the presence of glasses or a mask, the color or type of clothing, shoes, etc. If a detected object is an animal or some inanimate object, for example, secondary artificial neural networks may be used to track or identify similar appropriate attributes. The data at the outputs of the secondary artificial neural networks may be presented to metadata generator <b>390</b> which may generate metadata (e.g., the results of these computations). In some embodiments, one or more of these artificial neural networks may comprise, for example, a convolutional neural network.</p><p id="p-0078" num="0077">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a conceptual illustration of a neural network video processing and storage system in accordance with an embodiment of the disclosure is shown. Present in <figref idref="DRAWINGS">FIG. <b>4</b></figref> are video data source <b>405</b>, first artificial neural network <b>410</b>, fast-writing storage device <b>420</b>, fast-access storage device <b>430</b>, first neural network group <b>440</b>, second neural network group <b>445</b>, and metadata generator <b>490</b>.</p><p id="p-0079" num="0078">First neural network group <b>440</b> may comprise a plurality of artificial neural networks including a tracking neural network <b>450</b> and recognition neural network <b>460</b>, but other secondary artificial neural networks may also be present. Second neural network group <b>445</b> may comprise a plurality of secondary artificial neural networks including first attribute neural network <b>470</b> and N<sup>th </sup>attribute neural network <b>479</b>, but other secondary artificial neural networks for other attributes may also be present.</p><p id="p-0080" num="0079">The video data source <b>405</b> may be coupled to an input of the first artificial neural network <b>410</b> and to the fast-writing storage device <b>420</b> (shown as &#x201c;first type storage&#x201d;). A first output of first artificial neural network <b>410</b> may be coupled to the metadata generator <b>490</b>. Second and third outputs of the first artificial neural network <b>410</b> may be coupled to one or more of the fast-access storage devices <b>430</b> (shown as &#x201c;second type storage&#x201d;). The second output of first artificial neural network <b>410</b> may also be coupled to an input of the tracking neural network <b>450</b>, to an input of the recognition neural network <b>460</b>, and to the inputs of any other artificial neural networks that may be present in the first neural network group <b>440</b>. The third output of the first artificial neural network <b>410</b> may also be coupled to an input of the first attribute neural network <b>470</b>, to an input of the N<sup>th </sup>attribute neural network <b>479</b>, and to the inputs of any other artificial neural networks present in the second neural network group <b>445</b>. The outputs of the artificial neural networks of first neural network group <b>440</b> and second neural network group <b>445</b> may be coupled to metadata generator <b>490</b>.</p><p id="p-0081" num="0080">The video data source <b>405</b> can present raw video data from one or more surveillance or security cameras or other devices that may be analyzed by first artificial neural network <b>410</b>. In many embodiments, internal to the first artificial neural network <b>410</b> may be an input layer <b>411</b>, a series of intermediate layers <b>412</b>, <b>413</b>, <b>414</b>, and <b>415</b>, and an output layer (not shown). Although four intermediate layers are shown in the figure, it will be appreciated that an arbitrary number of intermediate layers may be present depending on the application need and design choice. Each intermediate layer may output intermediate layer data which may then be input to the next intermediate layer (or input to the output layer in the case of the last intermediate layer <b>415</b>). The first output of the first artificial neural network <b>410</b> may be the output of its output layer. The data at this first output may be presented to the metadata generator <b>490</b> which may generate various metadata (e.g., the results of the computations).</p><p id="p-0082" num="0081">In additional embodiments, the first artificial neural network <b>410</b> may further comprise a first layer group <b>418</b> and a second layer group <b>419</b>. The first layer group <b>418</b> may comprise internal intermediate layers <b>412</b>, <b>413</b> and <b>414</b> (shown as &#x201c;hidden layers&#x201d;), though it will be understood that any number internal layers may be present. The second layer group <b>419</b> may comprise internal layers <b>413</b>, <b>414</b>, and <b>415</b>, though it will be understood that any number internal layers may be present. The first layer group <b>418</b> and the second layer group <b>419</b> may share a number of intermediate layers like, for example, intermediate layers <b>413</b> and <b>414</b> though it will be understood that any number internal layers may be shared. One or more of the intermediate layers comprising the second layer group <b>419</b> may be coupled to the second output of the first artificial neural network <b>410</b>, while one or more of the intermediate layers comprising the first layer group <b>418</b> may be coupled to the third output of the first artificial neural network <b>410</b>.</p><p id="p-0083" num="0082">These layer groups <b>418</b>, <b>419</b> may be grouped together to generate unique intermediate layer data. In certain embodiments, various secondary artificial neural networks may be optimized to expect input of a certain type. The layer groups <b>418</b>, <b>419</b> may be selected or configured to output intermediate layer data in a format that the various types of secondary artificial neural networks expect. In this way, intermediate layer data may be comprised of not only each hidden/intermediate layers output, but also various combinations of those intermediate layers. These combinations allow for an increased number of uses within secondary neural networks, which can save processing time, storage, and energy by omitting at least part of the video image data processing in these secondary artificial neural networks.</p><p id="p-0084" num="0083">In many embodiments, the first artificial neural network <b>410</b> and the secondary artificial neural networks comprising first layer group <b>418</b> and the second layer group <b>419</b> may implement models for detecting objects and/or their attributes and/or the like. These models may comprise coefficients for use in multiply-accumulate operations performed by each of the neurons (or nodes) present in the artificial neural networks <b>410</b>, <b>450</b>, <b>460</b>, <b>470</b>, <b>479</b>, and any other secondary artificial neural networks present. Typically, these models may be derived from a training sequence performed before the deployment, though in some embodiments the training can occur within the neural network video processing and storage system.</p><p id="p-0085" num="0084">The raw video data from video data source <b>405</b> may be stored by fast-writing storage device <b>420</b>. The data stored in fast-writing storage device <b>420</b> is typically archival data which may only be accessed occasionally if additional analysis is required later, so the fast-writing storage device <b>420</b> may be optimized for fast writing to keep up with the continuous raw video data from the video data source <b>405</b>.</p><p id="p-0086" num="0085">The intermediate layer data from the second and third outputs of first artificial neural network <b>410</b> may be stored in one or more fast-access storage devices <b>430</b>. The intermediate layer data may be used repeatedly for either current or future analysis by the secondary neural networks. This may entail a high volume of random access reads and second data storage device type <b>430</b> may be optimized for fast read access to the data.</p><p id="p-0087" num="0086">In many embodiments, first artificial neural network <b>410</b> may comprise, for example, an object detection neural network, though other types of neural network may be used. In some embodiments, the tracking neural network <b>450</b>, the recognition neural network <b>460</b>, and the attribute neural networks present <b>470</b>, <b>479</b> may use the intermediate layer data from first artificial neural network <b>410</b> to obtain more detail from, for example, the objects detected by first artificial neural network <b>410</b>.</p><p id="p-0088" num="0087">If an object detected by the first artificial neural network <b>410</b> is a person, for example, the recognition neural network <b>460</b> may identify the person. The tracking neural network <b>450</b> may also follow the path taken by the person across the field of vision of multiple cameras. Additionally, one or more of the attribute detection neural networks in second neural network group <b>445</b> may identify any of a number of attributes (or characteristics) like, for example, age, gender, hair color, the presence of glasses or a mask, the color or type of clothing, shoes, etc. In some embodiments, if a detected object is an animal or some inanimate object, the secondary artificial neural networks in first neural network group <b>440</b> and in second neural network group <b>445</b> may be used to track or identify similar appropriate attributes. The data at the outputs of the secondary artificial neural networks may be presented to the metadata generator <b>490</b> which may generate metadata associated with the video image data. In certain embodiments, one or more of these artificial neural networks may comprise, for example, a convolutional neural network.</p><p id="p-0089" num="0088">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a conceptual illustration of a neural network video processing and storage system in accordance with an embodiment of the disclosure is shown. Present in <figref idref="DRAWINGS">FIG. <b>5</b></figref> are a video data source <b>505</b>, first neural network <b>510</b>, a first storage device <b>520</b> (shown as &#x201c;first type storage&#x201d;), metadata generator <b>590</b>, and a plurality of secondary artificial neural networks including, but not limited to, tracking neural network <b>550</b>, recognition neural network <b>560</b>, and a plurality of attribute neural networks from first attribute neural network <b>570</b> through N<sup>th </sup>attribute neural network <b>580</b>. Also present in <figref idref="DRAWINGS">FIG. <b>5</b></figref> is an intermediate layer matrix which may comprise first tensor set of intermediate layer data (<b>541</b>A, <b>541</b>B . . . <b>541</b>N), second tensor set of intermediate layer data (<b>542</b>A, <b>542</b>B . . . <b>542</b>N), third tensor set of intermediate layer data (<b>543</b>A, <b>543</b>B . . . <b>543</b>N), and Nth tensor set of intermediate layer data (<b>544</b>A, <b>544</b>B . . . <b>544</b>N).</p><p id="p-0090" num="0089">In the embodiment depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the video data source <b>505</b> may be coupled to an input of first artificial neural network <b>510</b> and to first storage device <b>520</b> which may be a fast-writing storage device. A first output of first artificial neural network <b>510</b> may be coupled to metadata generator <b>590</b>. First artificial neural network <b>510</b> comprises input layer <b>511</b>, intermediate layers <b>512</b>, <b>513</b>, and <b>514</b> (shown as &#x201c;hidden layers&#x201d;), and output layer <b>515</b> (shown as &#x201c;hidden layer N)&#x201d;. While three intermediate layers <b>512</b>, <b>513</b>, and <b>514</b> are shown in the figure, it is understood that an arbitrary number of intermediate layers may be present which may be dependent on application need and/or design choice. The intermediate layer data of each layer may be connected to the second storage device <b>530</b> which may be a fast-access storage device. In different embodiments, one, some, or all of the intermediate layer data may be output from first artificial neural network <b>510</b> and stored in the second storage device <b>530</b>.</p><p id="p-0091" num="0090">The intermediate layer data outputs of the first artificial neural network <b>510</b> may be coupled to the inputs of the plurality of secondary artificial neural networks including, but not limited to, tracking neural network <b>550</b>, recognition neural network <b>560</b>, and first attribute neural network <b>470</b> through N<sup>th </sup>attribute neural network <b>580</b>. This coupling may take place through the intermediate layer matrix which may comprise first tensor set of intermediate layer data (<b>541</b>A, <b>541</b>B . . . <b>541</b>N), second tensor set of intermediate layer data (<b>542</b>A, <b>542</b>B . . . <b>542</b>N), third tensor set of intermediate layer data (<b>543</b>A, <b>543</b>B . . . <b>543</b>N), and Nth tensor set of intermediate layer data (<b>544</b>A, <b>544</b>B . . . <b>544</b>N).</p><p id="p-0092" num="0091">It is understood that the number of tensor sets of intermediate layer data <b>541</b>, <b>542</b>, and <b>543</b> through <b>544</b> may be equal to the number of secondary artificial neural networks <b>550</b>, <b>560</b>, and <b>570</b> through <b>580</b>. It is also understood that the number of inputs in each tensor set of intermediate layer data may be equal to the number of intermediate layer data outputs (A, B N) from first artificial network <b>510</b>. The outputs of the secondary artificial neural networks including, but not limited to, the tracking neural network <b>550</b>, the recognition neural network <b>560</b>, and the first attribute neural network <b>470</b> through N<sup>th </sup>attribute neural network <b>580</b> may be coupled to metadata generator <b>590</b>.</p><p id="p-0093" num="0092">In many embodiments, the first artificial neural network <b>510</b> and the secondary artificial neural networks <b>550</b>, <b>560</b>, and <b>570</b> through <b>580</b> may implement models for detecting objects and/or their attributes and/or the like. These models may comprise coefficients for use in multiply-accumulate operations performed by each of the neurons (or nodes) present in the secondary artificial neural networks <b>510</b>, <b>550</b>, <b>560</b>, and <b>570</b> through <b>580</b>. Typically, these models may be derived from a training sequence performed before the deployment, though in some embodiments the training can occur within the neural network video processing and storage system.</p><p id="p-0094" num="0093">In some embodiments the intermediate layer matrix, which may comprise the first tensor set of intermediate layer data (<b>541</b>A, <b>541</b>B . . . <b>541</b>N), the second tensor set of intermediate layer data (<b>542</b>A, <b>542</b>B . . . <b>542</b>N), the third tensor set of intermediate layer data (<b>543</b>A, <b>543</b>B . . . <b>543</b>N), and the Nth tensor set of intermediate layer data (<b>544</b>A, <b>544</b>B . . . <b>544</b>N), may be fully populated, while in alternative embodiments it may only be partially populated. In many embodiments the intermediate layer matrix may be implemented using hardware switches, while in other embodiments the interconnections may be implemented in software or may be built into the model coefficients. For example, setting the coefficients to zero for unwanted connections may effectively remove the undesired part of the models from the secondary artificial neural networks.</p><p id="p-0095" num="0094">The raw video data from video data source <b>505</b> may be stored by first storage device <b>520</b> which may be a fast-writing storage device. The data stored in first storage device <b>520</b> is typically archival data which may only be accessed occasionally if additional analysis is required later, so first storage device <b>520</b> may be optimized for fast writing to keep up with the continuous raw video data from video data source <b>505</b>.</p><p id="p-0096" num="0095">The intermediate layer data from the first artificial neural network <b>510</b> may be stored in the second storage device <b>530</b> (shown as &#x201c;second type storage&#x201d;) which may be a fast-access storage device. The intermediate layer data may be used repeatedly for either current or future analysis by the secondary neural networks. This may entail a high volume of random access reads and second storage device <b>530</b> may be optimized for fast read access to the data.</p><p id="p-0097" num="0096">If an object detected by the first artificial neural network <b>510</b> is a person, for example, the recognition neural network <b>560</b> may, for example, identify the person; and/or tracking neural network <b>550</b> may, for example, follow the path taken by the person across the field of vision of multiple cameras; and/or one or more of the attribute neural networks <b>570</b> through <b>580</b> may, for example, identify any of a number of attributes (or characteristics) like, for example, age, gender, hair color, the presence of glasses or a mask, the color or type of clothing, shoes, etc. If a detected object is an animal or some inanimate object, for example, the attribute neural networks <b>570</b> through <b>580</b> may be used to track or identify similar appropriate attributes. The data at the outputs of the secondary artificial neural networks may be presented to metadata generator <b>590</b> which may generate metadata (e.g., the results of these computations). In some embodiments, one or more of these artificial neural networks may comprise, for example, a convolutional neural network.</p><p id="p-0098" num="0097">Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a flowchart of a process <b>600</b> using a neural network video processing and storage system in accordance with an embodiment of the disclosure is shown. Process <b>600</b> may begin by receiving video data for processing (block <b>610</b>). The video data may come from a variety of sources like, for example, a surveillance or security camera or some other device. In many embodiments, the process <b>600</b> may then store the video data in a first data storage type (block <b>620</b>). The data stored in the first data storage device type may be archival data, which may only be accessed occasionally if additional analysis is required later, so it may be optimized for fast writing to keep up with the continuous raw video data from video data source.</p><p id="p-0099" num="0098">The process <b>600</b> may also process the received video data through a first neural network (block <b>630</b>). The first neural network may comprise an input layer, one or more intermediate layers, and an output layer. The intermediate layers may produce intermediate layer data that may be output from the first neural network (block <b>640</b>). The intermediate layer data may then be stored in a second data storage type (block <b>650</b>). The volume of the intermediate layer data may be substantially less than the received video data. The intermediate layer may be used repeatedly for analysis and the nature of the data may require many random-access reads, so the second data storage type may be optimized for fast read data access.</p><p id="p-0100" num="0099">The first and second data storage types may be implemented using the same or different persistent memory technologies. For example, in some embodiments flash non-volatile memory (NVM) integrated circuits may be used for both data storage types. The first data storage type might use multiple bit per memory cell (e.g., multi-level cell (MLC), tri-level cell (TLC), quad-level cell (QLC)) technology (maybe along with video compression) to reduce the memory footprint of the received video data, while second data storage type might use single bit per memory cell (e.g., single level cell (SLC)) technology which has faster read access times. In further embodiments, hard disk drives (HDD) maybe used. In this case, the inner portion of each platter may be used for first data type storage (due to the slower disk velocity) while the outer portion of each platter may be used for faster read access times (due to the greater disk velocity). In still other embodiments, a hybrid system combining an HDD with a solid state drive (SSD) could be used, where the first data storage type may be implemented in a less expensive per bit and larger capacity HDD, and the second data storage type may be implemented in a faster but more expensive per bit solid state drive (SSD). In some alternate embodiments, second data storage type by also be maintained for a short period in DRAM or in fast NVM technologies like, for example, MRAM, RRAM, PCM, etc.</p><p id="p-0101" num="0100">The intermediate layer data may also be sent to the inputs of one or more secondary neural networks (block <b>660</b>). Metadata may then be generated based upon the outputs of the first neural network and the one or more secondary neural networks (block <b>670</b>). This may provide a more efficient method of neural network modelling due to the use of the intermediate layer data from the first neural network by the one or more secondary neural networks.</p><p id="p-0102" num="0101">This may work particularly well when the first neural network is an object identification neural network, and the secondary neural networks perform functions related to the object being identified. The secondary neural networks can perform functions like, for example, recognizing someone if the identified object is a person, a pet or other animal, or a unique inanimate object; and/or determining different attributes about the identified person (age, gender, etc.), animal (species, fur color, presence of a collar, etc.), or inanimate object (shape, material, etc.).</p><p id="p-0103" num="0102">The sharing and/or reusing of the intermediate layer data takes advantage of the computation, time, and energy required to obtain it because it may be used by many different secondary neural network models. If additional analysis is needed at a future time, new models can be trained and deployed in one or more of the secondary neural networks and only the additional time and energy of running the necessary secondary neural networks is needed.</p><p id="p-0104" num="0103">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a flowchart of a process <b>700</b> using a neural network video processing and storage system in accordance with an embodiment of the disclosure is shown. Process <b>700</b> may begin by receiving video data from one or more security cameras for processing (block <b>710</b>). The process <b>700</b> may then store the video data in one or more fast-writing storage devices (block <b>720</b>). This data may be accessed at a future time if additional video processing is needed. The data stored in the fast-writing storage devices may be archival data, which may only be accessed occasionally if additional analysis is required later, so it may be optimized for fast writing to keep up with the continuous raw video data from video data source.</p><p id="p-0105" num="0104">The process <b>700</b> may also process the received video data in an object detection neural network (block <b>730</b>). The object detection neural network may comprise an input layer, one or more intermediate layers, and an output layer. The intermediate layers may each produce intermediate layer data that may be output from the object detection neural network (block <b>740</b>). The intermediate layer data may then be stored in one or more fast-access storage devices (block <b>750</b>).</p><p id="p-0106" num="0105">The volume (i.e., file size) of the intermediate layer data may be substantially less than the received video data. The intermediate layer data may be used repeatedly for analysis and the nature of the data may require many random-access reads, so the second data storage type may be optimized for fast read data access.</p><p id="p-0107" num="0106">The fast-writing storage devices and the fast-access storage devices may be implemented using the same or different persistent memory technologies. For example, in some embodiments flash non-volatile memory (NVM) integrated circuits may be used for both data storage device types. The fast-writing storage devices might use multiple bit per memory cell technology (e.g., multi-level cell (MLC), tri-level cell (TLC), quad-level cell (QLC)) to reduce the memory footprint of the received video data, while the fast-access storage devices might use single bit per memory cell technology (e.g., single level cell (SLC)) which has faster read access times. In other embodiments, hard disk drives (HDD) maybe used. In this case, the inner portion of each platter may be used for the fast-writing storage devices (due to the slower disk velocity) while the outer portion of each platter may be used for the fast-access storage devices (due to the greater disk velocity). In still other embodiments, a hybrid system combining a HDD with a solid state drive (SSD) could be used, where the fast-writing storage devices may be implemented in a less expensive per bit and larger capacity HDD, and the fast-access storage devices may be implemented in a faster but more expensive per bit solid state drive (SSD). In some alternate embodiments, the fast-access storage devices by also be maintained for a short period in DRAM or in fast NVM technologies like, for example, MRAM, RRAM, PCM, etc.</p><p id="p-0108" num="0107">The intermediate layer data may also be sent to the inputs of one or more object attribute detection neural networks (block <b>760</b>). Metadata may then be generated based on the output of the object detection neural network and the outputs of the one or more object attribute detection neural networks (block <b>770</b>). This may provide a more efficient method of neural network modelling due to the use of the intermediate layer data from the first neural network by the one or more secondary neural networks.</p><p id="p-0109" num="0108">In various embodiments, this arrangement may work particularly well because the first neural network is an object identification neural network, and the secondary neural networks are attribute neural networks that perform functions related to the object being identified. The attribute neural networks can perform functions like, for example, recognizing someone if the identified object is a person, a pet or other animal, or a unique inanimate object; and/or tracking the movement of the identified person, animal, or inanimate object; and/or determining different attributes about the identified person (age, gender, etc.), animal (species, fur color, presence of a collar, etc.), or inanimate object (shape, material, etc.).</p><p id="p-0110" num="0109">The sharing and/or reusing of the intermediate layer data takes advantage of the computation, time, and energy required to obtain it because it may be used by many different secondary neural network attribute models. If additional analysis is needed at a future time, new models can be trained and deployed in one or more of the secondary neural networks and only the additional time and energy of running the necessary secondary neural networks is needed.</p><p id="p-0111" num="0110">Referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a flowchart of a process <b>800</b> of utilizing intermediate layer data in a neural network video processing and storage system in accordance with an embodiment of the disclosure is shown. In many embodiments, the process <b>800</b> may use intermediate layer data to determine the path of a person, animal, or object from one location to another in a video data pool like, for example, in the surveillance of a public area like an airport or shopping mall. Process <b>800</b> may begin by defining a target to search for in the video data pool (block <b>810</b>). The target may be a person or object of interest and the video data pool may be a plurality of video sources like, for example, surveillance or security cameras.</p><p id="p-0112" num="0111">The features of the target may then be determined (block <b>820</b>). If the target is a person, the features might be attributes like facial features, age, gender, glasses, hair color, etc. If the target is an object, the features might be attributes like shape, color, surface texture, markings, number array, vector, etc.</p><p id="p-0113" num="0112">The features of available video data sources in the video data pool may then be accessed (block <b>830</b>). If the features of the target are attributes already known to the video data pool, then a target match may then be searched for in all the available video data sources (block <b>840</b>). If the target is a person, the search might first involve seeking all candidates identified as persons, in the stored metadata. The metadata for each candidate would be examined attribute by attribute. Candidates would be filtered out as they failed to match attributes. If successful, a series of video frames containing the target from the various available video sources.</p><p id="p-0114" num="0113">In a number of embodiments, sorting the frames by timestamp (or other temporal marker) and noting additional aspects such as, the location of the video source may allow for the path of the defined target to be determined (block <b>850</b>). As those skilled in the art will recognize, the processing of location and other features and/or attributes can be done numerically which may then be correlated through various means such as, but not limited to, similarity or range of similar values. If the search fails to find a match, then additional secondary models for new but distinctive features of the target may need to be created or utilized to generate metadata for these new features.</p><p id="p-0115" num="0114">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a flowchart of a process <b>900</b> for using intermediate layer data to generate and utilize new attribute extraction neural networks in accordance with an embodiment of the disclosure is shown. In many embodiments, the process <b>900</b> may begin by determining one or more new attributes to model (block <b>910</b>). For example, it may be desired to track an attribute like, for example, the color of an identified person's hat. In this example, an attribute model to track this has not previously been developed. Thus, the process <b>900</b> may generate and train the new attribute extraction neural network via a model or models with new training data (block <b>920</b>).</p><p id="p-0116" num="0115">In a variety of embodiments, the newly generate attribute extraction neural network will configured to have one or more features within the intermediate layer data as an input. A selection of which combinations of intermediate layer data to input into the new attribute extraction neural network model can occur (block <b>930</b>). In additional embodiments, utilizing and processing previously stored intermediate layer data as training data can aid in determining which features from the intermediate layer data should be selected for successful processing of subsequent stored or captured video image data. Once selected, these selected features can be utilized as input data for the new attribute extraction neural network (block <b>940</b>). Once the new attribute extraction neural network is receiving input data, one or more new attributes can be determined within video image data (block <b>950</b>).</p><p id="p-0117" num="0116">Information as herein shown and described in detail is fully capable of attaining the presently described embodiments of the present disclosure, and is, thus, representative of the subject matter that is broadly contemplated by the present disclosure. The scope of the present disclosure fully encompasses other embodiments that might become obvious to those skilled in the art, and is to be limited, accordingly, by nothing other than the appended claims. Any reference to an element being made in the singular is not intended to mean &#x201c;one and only one&#x201d; unless explicitly so stated, but rather &#x201c;one or more.&#x201d; All structural and functional equivalents to the elements of the above-described preferred embodiment and additional embodiments as regarded by those of ordinary skill in the art are hereby expressly incorporated by reference and are intended to be encompassed by the present claims.</p><p id="p-0118" num="0117">Moreover, no requirement exists for a system or method to address each and every problem sought to be resolved by the present disclosure, for solutions to such problems to be encompassed by the present claims. Furthermore, no element, component, or method step in the present disclosure is intended to be dedicated to the public regardless of whether the element, component, or method step is explicitly recited in the claims. Various changes and modifications in form, material, work-piece, and fabrication material detail can be made, without departing from the spirit and scope of the present disclosure, as set forth in the appended claims, as might be apparent to those of ordinary skill in the art, are also encompassed by the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A device comprising:<claim-text>a processor configured to process video data wherein the processing includes:<claim-text>storing received video data into a first storage device type;</claim-text><claim-text>inputting the video data into a primary artificial neural network wherein the first artificial neural network comprises at least an input layer, one or more hidden layers, and an output layer;</claim-text><claim-text>generating intermediate layer data by utilizing the outputs of one or more of the intermediate layers of the first artificial neural network;</claim-text><claim-text>storing the intermediate layer data into a second storage device type different from the first device type; and</claim-text><claim-text>utilizing the intermediate layer data as input data for one or more secondary artificial neural networks.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second storage device type is different from the first device type in an access speed of read or write operations.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The device of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising: (a) a first storage device of the first storage device type configured to store the received video data and (b) a second storage device of the second storage device type configured to store the intermediate layer.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The device of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising a storage device, the storage device comprising: (a) first media of the first storage device type, the first media configured to store the received video data and (b) second media of the second storage device type, the second media configured to store the intermediate layer.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first artificial neural network is an object detection neural network.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The device of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the object detection neural network is a convolutional neural network.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of secondary artificial neural networks comprises one or more attribute detection neural networks.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of secondary artificial neural networks comprises at least a tracking neural network.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of secondary artificial neural networks comprises at least a recognition neural network.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the intermediate layer data is utilized as input data for a first and a second secondary artificial neural networks of the secondary artificial neural networks.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The device of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the input data utilized from the intermediate layer data is formatted as a tensor set of intermediate layer data generated from a selection of intermediate layer outputs from the first artificial neural network.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first secondary artificial neural network utilizes a first tensor set of intermediate layer data and the second secondary artificial neural network utilizes a second tensor set of intermediate layer data.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein:<claim-text>the first tensor set of intermediate layer data is generated from a first group of intermediate hidden layer data outputs from the first artificial neural network; and</claim-text><claim-text>the second tensor set of intermediate layer data is generated from a second group of intermediate hidden layer data outputs from the first artificial neural network.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the output of the first and secondary artificial neural networks are utilized to generate a plurality of metadata associated with the video data.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein one of the secondary artificial neural networks is selected to generate new metadata after the plurality of metadata has been generated.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the selected secondary artificial neural network utilizes intermediate hidden layer data stored within the second storage device type.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A method for reducing subsequent processing of video data, comprising:<claim-text>defining a target to search within a video data pool comprising a plurality of video data sources and associated intermediate layer data for each video data source;</claim-text><claim-text>determining at least one or more features associated with the defined target;</claim-text><claim-text>searching for the target within the plurality of video data source intermediate layer data;</claim-text><claim-text>locating the target within multiple video data sources; and</claim-text><claim-text>determining a path for the defined target.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the video data pool comprises a plurality of security cameras.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the intermediate layer data is generated by a primary artificial neural network and the searching of the target comprises utilizing a plurality of secondary artificial neural networks.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A device comprising:<claim-text>a processor configured to process video data wherein the processing includes:<claim-text>inputting the video data into a primary artificial neural network wherein the first artificial neural network comprises at least an input layer, one or more hidden layers, and an output layer;</claim-text><claim-text>generating intermediate layer data by utilizing the outputs of one or more of the intermediate layers of the first artificial neural network;</claim-text><claim-text>storing the intermediate layer data; and</claim-text><claim-text>utilizing the intermediate layer data as input data for a plurality of secondary artificial neural networks.</claim-text></claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The device of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the intermediate layer data is utilized as input data for a first and a second secondary artificial neural networks of the secondary artificial neural networks.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The device of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the input data utilized from the intermediate layer data is formatted as a tensor set of intermediate layer data generated from a selection of intermediate layer outputs from the first artificial neural network.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The device of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the first secondary artificial neural network utilizes a first tensor set of intermediate layer data and the second secondary artificial neural network utilizes a second tensor set of intermediate layer data.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The device of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein:<claim-text>the first tensor set of intermediate layer data is generated from a first group of intermediate hidden layer data outputs from the first artificial neural network; and</claim-text><claim-text>the second tensor set of intermediate layer data is generated from a second group of intermediate hidden layer data outputs from the first artificial neural network.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The device of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the output of the first and secondary artificial neural networks are utilized to generate a plurality of metadata associated with the video data.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The device of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein one of the secondary artificial neural networks is selected to generate new metadata after the plurality of metadata has been generated.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The device of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the selected secondary artificial neural network utilizes intermediate hidden layer data stored.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The device of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the first artificial neural network is an object detection neural network, and the secondary artificial neural networks comprise one or more of: an attribute detection neural network, a tracking neural network, and a recognition neural network.</claim-text></claim></claims></us-patent-application>