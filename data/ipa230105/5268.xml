<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005269A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005269</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17853875</doc-number><date>20220629</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2021-0085994</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>77</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0002</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>7715</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND APPARATUS FOR DETECTING REAL-TIME ABNORMALITY IN VIDEO SURVEILLANCE SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>INDUSTRY ACADEMY COOPERATION FOUNDATION SEJONG UNIVERSITY</orgname><address><city>Seoul</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KIM</last-name><first-name>Young Gab</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>KIM</last-name><first-name>Yu Jun</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure provides a method and apparatus for detecting an abnormal event from a monitoring image accurately and speedily in a video surveillance system. A method of detecting an abnormal event in a series of temporally successive images includes: generating a predicted current frame based on a previous frame temporally ahead of an actual current frame and a subsequent frame temporally behind the actual current frame; calculating an anomaly score indicating a difference between the predicted current frame and the actual current frame; and determining that an abnormality is included in the actual current frame when the anomaly score satisfies a predetermined condition.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="179.32mm" wi="119.89mm" file="US20230005269A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="154.86mm" wi="122.85mm" orientation="landscape" file="US20230005269A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="197.70mm" wi="141.48mm" orientation="landscape" file="US20230005269A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="230.89mm" wi="151.72mm" orientation="landscape" file="US20230005269A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="224.71mm" wi="152.48mm" file="US20230005269A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="215.48mm" wi="139.02mm" file="US20230005269A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="206.59mm" wi="152.48mm" file="US20230005269A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="168.99mm" wi="136.31mm" file="US20230005269A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="127.34mm" wi="142.07mm" file="US20230005269A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="167.56mm" wi="143.17mm" file="US20230005269A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="203.62mm" wi="121.92mm" file="US20230005269A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="203.54mm" wi="117.69mm" file="US20230005269A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="203.62mm" wi="117.94mm" file="US20230005269A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="76.88mm" wi="144.36mm" file="US20230005269A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">The present application claims the benefit of priority to Korean Patent Application No. 10-2021-0085994 filed on Jun. 30, 2021 with the Korean Intellectual Property Office (KIPO), the entire content of which is incorporated herein by reference.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to a video surveillance system and, more particularly, to a method and apparatus for detecting an abnormal event in real-time based on an acquired video.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">With the exponential increase in the number and scale of video surveillance systems, the time and costs required to detect an abnormal situation in a monitoring target area based on monitoring images acquired by surveillance cameras increase also. Programs automatically detecting moving objects in the monitoring images to detect the abnormal situation are widely being used, but human operators may still be needed to verify a detection result against a system malfunction.</p><p id="p-0005" num="0004">The employment of deep learning in the detection programs is increasing to improve a detection accuracy of the surveillance system. However, the video surveillance system employing the deep learning may reveal a low detection speed, which may be an obstacle to rapidly respond to a dangerous situation such as a robbery, violence, or murder that may happen in the monitoring target area.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">Provided are a method and apparatus capable of detecting an abnormal event from a monitoring image accurately and speedily in a video surveillance system.</p><p id="p-0007" num="0006">According to an aspect of an exemplary embodiment, a method of detecting an abnormal event in a series of temporally successive images includes: generating a predicted current frame based on a previous frame temporally ahead of an actual current frame and a subsequent frame temporally behind the actual current frame; calculating an anomaly score indicating a difference between the predicted current frame and the actual current frame; and determining that an abnormality is included in the actual current frame when the anomaly score satisfies a predetermined condition.</p><p id="p-0008" num="0007">The previous frame may be temporally ahead of the actual current frame by a plurality of frames, and the subsequent frame may be temporally behind the actual current frame by the plurality of frames.</p><p id="p-0009" num="0008">The predicted current frame may be generated by an artificial neural network comprising a first subnetwork receiving the previous frame as an input and a second subnetwork receiving the subsequent frame as an input. Each of the first and second subnetworks may include at least one layer stage, and each of the layer stage may have at least one convolutional layer.</p><p id="p-0010" num="0009">The operation of generating the predicted current frame may include: receiving, in a layer stage of the first subnetwork, a second feature map from a corresponding layer stage of the second subnetwork to concatenate the second feature map with a first feature map generated by the layer stage of the first subnetwork; and receiving, in the corresponding layer stage of the second subnetwork, the first feature map from the layer stage of the first subnetwork to concatenate the first feature map with the second feature map generated by the corresponding layer stage of the second subnetwork.</p><p id="p-0011" num="0010">The artificial neural network may be used after being trained in advance to generate the predicted current frame based on the previous frame and the subsequent frame in a normal situation where the actual current frame contains no abnormality.</p><p id="p-0012" num="0011">The operation of calculating the anomaly score may include: calculating a plurality of local anomaly scores by moving a window with respect to the actual current frame horizontally and vertically in a unit of a predetermined stride and performing a predetermined operation on pixel value differences between pixels in the predicted current frame and corresponding pixels in the actual current frame for an image frame portion overlapping the window at each window location; and determining the anomaly score by averaging or summing the plurality of local anomaly scores calculated according to a movement of the window.</p><p id="p-0013" num="0012">Each of the plurality of local anomaly scores may be calculated by averaging or summing the pixel value differences between pixels in the predicted current frame and corresponding pixels in the actual current frame for the image frame portion overlapping the window.</p><p id="p-0014" num="0013">The operation of determining the anomaly score may include determining the anomaly score by averaging only a predetermined number of local anomaly scores selected in an order of magnitude among the plurality of local anomaly scores calculated according to the movement of the window.</p><p id="p-0015" num="0014">A size of the window may be set to decrease as the window moves upward with respect to the actual current frame.</p><p id="p-0016" num="0015">The method may further include preprocessing of the series of temporally successive images to convert to black-and-white images or adjust resolutions of the images before generating the predicted current frame.</p><p id="p-0017" num="0016">According to another aspect of an exemplary embodiment, an apparatus for detecting an abnormal situation in a series of temporally successive images includes a processor and a memory storing program instructions to be executed by the processor. The program instructions, when executed by the processor, causes the processor to: generate a predicted current frame based on a previous frame temporally ahead of an actual current frame and a subsequent frame temporally behind the actual current frame; calculate an anomaly score indicating a difference between the predicted current frame and the actual current frame; and determine that an abnormality is included in the actual current frame when the anomaly score satisfies a predetermined condition.</p><p id="p-0018" num="0017">The program instructions causing the processor to generate the predicted current frame may include instructions to: configure an artificial neural network comprising a first subnetwork receiving the previous frame as an input and a second subnetwork receiving the subsequent frame as an input; and generate the predicted current frame by the artificial neural network. Each of the first and second subnetworks may include at least one layer stage, each layer stage having at least one convolutional layer.</p><p id="p-0019" num="0018">The program instructions causing the processor to configure the artificial neural network may include instructions to: receive, in a layer stage of the first subnetwork, a second feature map from a corresponding layer stage of the second subnetwork and concatenate the second feature map with a first feature map generated by the layer stage of the first subnetwork; and receive, in the corresponding layer stage of the second subnetwork, the first feature map from the layer stage of the first subnetwork and concatenate the first feature map with the second feature map generated by the corresponding layer stage of the second subnetwork.</p><p id="p-0020" num="0019">An exemplary embodiment of the present disclosure enables to detect an abnormal event from the monitoring image accurately and speedily in the video surveillance system. Evaluations of the detection performance carried out using datasets used in many studies showed that the detection method according to an exemplary embodiment of the present disclosure achieves detecting the abnormality in real-time with a high detection accuracy.</p><p id="p-0021" num="0020">In particular, the current frame in a situation where the actual current frame is assumed to contain no abnormality can be predicted using an artificial neural network having subnetworks combined with each other, and the abnormal event may be detected very precisely by comparing the predicted current frame and the actual current frame.</p><p id="p-0022" num="0021">In calculating the anomaly score while moving the window, a cascade sliding window method, which reduces the size of the window as the window moves upward, enables to take a perspective of each object into account in the comparison of the predicted current frame and the actual current frame without increasing the computational burden, which facilitates the precise and speedy detection with little error.</p><p id="p-0023" num="0022">Further areas of applicability will become apparent from the description provided herein. It should be understood that the description and specific examples are intended for purposes of illustration only and are not intended to limit the scope of the present disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0024" num="0023">In order that the disclosure may be well understood, there will now be described various forms thereof, given by way of example, reference being made to the accompanying drawings, in which:</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic block diagram of a video surveillance system according to an exemplary embodiment of the present disclosure;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating an abnormality detecting process according to an exemplary embodiment of the present disclosure;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an architecture of an artificial neural network shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> in detail;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart illustrating a process of deriving an anomaly score for a current frame based on a squared error image;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows pseudo-codes of an algorithm for implementing the process of deriving the anomaly score shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a sliding and resizing of a window;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram of an abnormality detecting apparatus according to an exemplary embodiment of the present disclosure;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a table summarizing the AUC values for each dataset according to the window decrease step;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a table summarizing frame-level AUCs of the abnormality detecting method according to an exemplary embodiment of the present disclosure and other latest detection methods;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a processing speed per frame according to an exemplary embodiment of the present disclosure for each dataset;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a table summarizing the processing speed per frame of the abnormality detecting method according to an exemplary embodiment of the present disclosure and another method;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIGS. <b>12</b>A-<b>12</b>C</figref> show examples of actual frames, predicted frames, and visualizations of squared errors of the actual frames and respective predicted frames for image frames containing abnormal situations; and</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows the results of an ablation study.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0038" num="0037">The drawings described herein are for illustration purposes only and are not intended to limit the scope of the present disclosure in any way.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0039" num="0038">For a more clear understanding of the features and advantages of the present disclosure, exemplary embodiments of the present disclosure will be described in detail with reference to the accompanying drawings. However, it should be understood that the present disclosure is not limited to particular embodiments and includes all modifications, equivalents, and alternatives falling within the idea and scope of the present disclosure.</p><p id="p-0040" num="0039">The terminologies including ordinals such as &#x201c;first&#x201d; and &#x201c;second&#x201d; designated for explaining various components in this specification are used to discriminate a component from the other ones but are not intended to be limiting to a specific component. For example, a second component may be referred to as a first component and, similarly, a first component may also be referred to as a second component without departing from the scope of the present disclosure. As used herein, the term &#x201c;and/or&#x201d; includes any and all combinations of one or more associated items.</p><p id="p-0041" num="0040">When a component is referred to as being &#x201c;connected&#x201d; or &#x201c;coupled&#x201d; to another component, it means that the component is connected or may be connected logically or physically to the other component. In other words, it is to be understood that the component may be connected or coupled to the other component indirectly through an object therebetween instead of being directly connected or coupled to the other component.</p><p id="p-0042" num="0041">The terminologies are used herein for the purpose of describing particular embodiments only and are not intended to limit the disclosure. The singular forms include plural referents unless the context clearly dictates otherwise. Also, the expressions &#x201c;&#x2dc; comprises,&#x201d; &#x201c;&#x2dc; includes,&#x201d; &#x201c;&#x2dc; constructed,&#x201d; &#x201c;&#x2dc; configured&#x201d; are used to refer to a presence of a combination of enumerated features, numbers, processing steps, operations, elements, or components, but are not intended to exclude a possibility of a presence or addition of another feature, number, processing step, operation, element, or component.</p><p id="p-0043" num="0042">Unless defined otherwise, all terms used herein, including technical or scientific terms, have the same meaning as commonly understood by those of ordinary skill in the art to which the present disclosure pertains. Terms such as those defined in a commonly used dictionary should be interpreted as having meanings consistent with meanings in the context of related technologies and should not be interpreted as having ideal or excessively formal meanings unless explicitly defined in the present application.</p><p id="p-0044" num="0043">Hereinafter, embodiments of the present disclosure will be described in more detail with reference to the accompanying drawings. In describing the present disclosure, in order to facilitate an overall understanding thereof, the same components are designated by the same reference numerals in the drawings and are not redundantly described here. Also, detailed descriptions of well-known functions or configurations that may obscure the subject matter of the present disclosure will be omitted for simplicity.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic block diagram showing an overall configuration of a video surveillance system according to an exemplary embodiment of the present disclosure. The video surveillance system includes a plurality of surveillance cameras <b>10</b>A-<b>10</b>N distributed in a monitoring target area to acquire a monitoring image for a part of the monitoring target area and a monitoring control server <b>20</b> accessible by the plurality of surveillance cameras <b>10</b>A-<b>10</b>N. In an exemplary embodiment, each of the plurality of surveillance cameras <b>10</b>A-<b>10</b>N can be connected to the monitoring control server <b>20</b> through an IP network based on a wired network and/or a wireless network.</p><p id="p-0046" num="0045">Each of the cameras <b>10</b>A-<b>10</b>N acquires the monitoring image of its surrounding area with an appropriate zoom ratio while panning and tilting in response to a control signal from the monitoring control server <b>20</b>. The monitoring control server <b>20</b> may detect a moving object or detect an abnormal event contained in the monitoring images from the monitoring cameras <b>10</b>A-<b>10</b>N, and control panning, tilting, and zooming of the monitoring cameras <b>10</b>A-<b>10</b>N as necessary. In addition, the monitoring control server <b>20</b> may generate an alarm when an abnormal event is detected in the images acquired by the monitoring cameras <b>10</b>A-<b>10</b>N. In this specification including the appended claims, the term &#x201c;abnormality&#x201d; or &#x201c;abnormal event&#x201d; refers to a dangerous situation such as a robbery, violence, or murder, but the types of the abnormality are not limited thereto.</p><p id="p-0047" num="0046">The monitoring control server <b>20</b> performs operations required for processing the image data, detecting moving objects, detecting of abnormal events, displaying images, and generating the alarm, and may include at least one processor such as a graphic processor unit (GPU) as described below. The monitoring control server may serve as an abnormality detecting apparatus that performs abnormality detecting operations through executions of program instructions by at least one processor. Alternatively, however, the operations such as the processing the image data, the detecting the moving objects, and detecting the abnormal events may be performed by each of the monitoring cameras <b>10</b>A-<b>10</b>N rather than the monitoring control server <b>20</b>, and an operation result may be transmitted to the monitoring control server <b>20</b> by the monitoring camera.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating an abnormality detecting process according to an exemplary embodiment of the present disclosure. The abnormality detecting process according to the present embodiment may include preprocessing of an image (S<b>100</b>), predicting a current frame by an artificial neural network (S<b>110</b>), calculating a difference image between a predicted current frame and an actual current frame (S<b>120</b>), calculating an anomaly score based on a pixel value distribution in the difference image (S<b>130</b>), and determining whether an abnormality has occurred in the current frame based on the anomaly score (S<b>140</b>).</p><p id="p-0049" num="0048">In the operation S<b>100</b> of preprocessing the image, an input color image is converted into a black-and-white image. In addition, the size of the input image may be changed to a certain size, e.g. 256&#xd7;256&#xd7;1, where &#x2018;256&#xd7;256&#x2019; indicates resolutions in horizontal and vertical directions, and &#x2018;1&#x2019; represents a number of channels. An image frame on which the preprocessing has been completed may be stored in a frame memory (not shown). In an exemplary embodiment, the frame memory may store at least five frames from at least a second previous frame F<sub>t&#x2212;2 </sub>to a second subsequent frame F<sub>t+2 </sub>with respect to the current frame F<sub>t</sub>.</p><p id="p-0050" num="0049">In the operation S<b>110</b> of predicting the current frame by the artificial neural network, images of a previous frame and a subsequent frame of the current frame are received as inputs, and the predicted current frame is generated by a Cross U-Net artificial neural network according to the present disclosure. In an exemplary embodiment, the images used as inputs in the artificial neural network are the second previous frame F<sub>t&#x2212;2 </sub>and the second subsequent frame F<sub>t+2 </sub>of the current frame F<sub>t</sub>. The artificial neural network generates the predicted current frame {circumflex over (F)}<sub>t </sub>based on these image frames.</p><p id="p-0051" num="0050">Before being applied to a real abnormality detection operation, the artificial neural network may be trained to accurately predict the current frame from the previous and subsequent frames in a normal situation. Accordingly, the trained artificial neural network can predict the current frame better in a normal situation than in an abnormal event, and there is little difference between the predicted current frame and the actual current frame. The present disclosure uses this difference to detect the abnormal event. That is, when there is a difference larger than a certain degree between the predicted current frame determined and the actual current frame, the abnormality detecting apparatus determines that an abnormality has occurred in the current frame.</p><p id="p-0052" num="0051">In the operation <b>120</b>, the difference image between the predicted current frame {circumflex over (F)}<sub>t </sub>and the actual current frame F<sub>t </sub>is calculated. The word &#x201c;difference image&#x201d; is used herein to refer to an array of numbers generated by subtracting pixel values of two image frames in pixel-by-pixel. The difference image may be displayed on the screen to be visually recognized as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, but the present disclosure is not limited thereto and the difference image may be stored into a frame memory or a conventional storage device such as RAM to be used for subsequent calculations. Meanwhile, each pixel value in the &#x201c;difference image&#x201d; may be an absolute value or a squared value of the difference in the pixel values of corresponding pixels of the two image frames rather than a simple difference in the pixel values. In the exemplary embodiment detailed below, the &#x201c;difference image&#x201d; is a square error image in which the pixel value of each pixel is the square of the difference in the pixel values of the corresponding pixels of the two image frames {circumflex over (F)}<sub>t </sub>and F<sub>t</sub>. Hereinbelow, squared error image may be denoted by a symbol of ({circumflex over (F)}<sub>t</sub>&#x2212;F<sub>t</sub>)<sup>2</sup>.</p><p id="p-0053" num="0052">In the operation <b>130</b>, the anomaly score which is an index indicating a possibility that an abnormal event is included in the current frame is calculated based on the pixel value distribution in the difference image. In an exemplary embodiment, in order to obtain the anomaly score, a local anomaly score can be calculated by sliding a window having a certain size to move relatively with respect to the image frame, i.e., the current frame or the squared error image, in a unit of a certain stride horizontally and vertically. The local anomaly score may be calculated as an average of the squared errors for the pixels in an image frame portion overlapping the window for each sliding step. Afterwards, an average value of at least some of all the local anomaly scores calculated over the entire region of the image frame may be determined as the anomaly score (S) for the entire image frame. In case that the anomaly score (S) is determined using only some of the local anomaly scores, the anomaly score (S) may be determined by selecting a certain number of the largest local anomaly scores and averaging the selected anomaly scores.</p><p id="p-0054" num="0053">If the anomaly score S is determined in this way, it can be said that the larger the anomaly score S is, the higher the probability of including the abnormal event in the current frame is. Accordingly, in operation <b>140</b>, it may be determined that an abnormality has occurred in the current frame when the anomaly score S is greater than a certain threshold.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an architecture of the artificial neural network shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> in detail.</p><p id="p-0056" num="0055">The Cross U-Net artificial neural network shown in the drawing, which is an artificial neural network model newly proposed by the present disclosure, includes two sub-networks based on U-Net. A first subnetwork located in the upper portion of the drawing receives the second previous frame F<sub>t&#x2212;2 </sub>of the current frame F<sub>t </sub>as the input, and the second subnetwork located in the lower portion of the drawing receives the second subsequent frame F<sub>t+2 </sub>of the current frame F<sub>t </sub>as the input. The artificial neural network generates the predicted current frame from the second previous frame F<sub>t&#x2212;2 </sub>and the second subsequent frame F<sub>t+2</sub>. As mentioned above, the second previous frame F<sub>t&#x2212;2 </sub>and the second subsequent frame F<sub>t+2 </sub>may be image frames having undergone the preprocessing operation to be converted into the black-and-white image and changed to the size of 256&#xd7;256&#xd7;1. However, the present disclosure is not limited thereto.</p><p id="p-0057" num="0056">Each subnetwork includes a contracting path and an expansive path, which may be arranged symmetrically to make the subnetwork seem to have a U-shaped architecture. The contracting path includes repetitive applications of two 3&#xd7;3 convolutions, a 2&#xd7;2 max-pooling operation for downsampling feature maps, and a concatenation with feature maps copied and cropped from the other subnetwork. A number of feature channels is doubled in each stage in the contracting path. The expansive path includes repetitive applications of a 2&#xd7;2 up-convolution for upsampling the feature maps, a concatenation with feature maps copied and cropped from a corresponding stage in the contracting path, and two 3&#xd7;3 convolutions. The number of feature channels is halved in each stage in the expansive path.</p><p id="p-0058" num="0057">Specifically, in the embodiment of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the Cross U-Net artificial neural network may generate the predicted current frame {circumflex over (F)}<sub>t </sub>through a first through tenth stages. Among the first through tenth stages, the first through fifth stages may be on the contracting path, and the fifth through ninth stages may be on the expansive path.</p><p id="p-0059" num="0058">In the first stage, two convolutional layers in the first subnetwork indicated by arrows receive the second previous frame F<sub>t&#x2212;2 </sub>and perform convolutions using kernels having a size of 3&#xd7;3. Feature maps generated by the convolutions are downsampled by a 2&#xd7;2 maxpooling in a pooling layer. In the first subnetwork, feature maps of 64 channels are transferred from the first stage to the second stage. Similarly, two convolutional layers in the first stage of the second subnetwork receive the second subsequent frame F<sub>t+2 </sub>and perform convolutions using kernels having a size of 3&#xd7;3. Feature maps generated by the convolutions are downsampled by a 2&#xd7;2 maxpooling in a pooling layer. Feature maps of 64 channels are transferred from the first stage to the second stage in the second subnetwork also.</p><p id="p-0060" num="0059">All or part of the feature maps of 64 channels transferred from the first stage to the second stage in the first subnetwork are copied and cropped and provided to the second subnetwork. Also, all or part of the feature maps of 64 channels transferred from the first stage to the second stage in the second subnetwork are copied and cropped and provided to the first subnetwork. The feature maps provided to the first subnetwork by the second subnetwork are concatenated to the feature maps of the first subnetwork. Two convolutional layers in the second stage of the first subnetwork receive the concatenated feature maps and perform convolutions using 3&#xd7;3 kernels. The feature maps generated by the convolutions are downsampled by a 2&#xd7;2 maxpooling in a pooling layer. Feature maps of 128 channels are transferred from the second stage to the third stage in the first subnetwork. Similarly, the feature maps provided to the second subnetwork by the first subnetwork are concatenated to the feature maps of the second subnetwork. Convolutional layers in the second stage of the second subnetwork receive the concatenated feature maps and perform convolutions using 3&#xd7;3 kernels. The feature maps generated by the convolutions are downsampled by a 2&#xd7;2 maxpooling in a pooling layer. Feature maps of 128 channels are transferred from the second stage to the third stage in the second subnetwork.</p><p id="p-0061" num="0060">The third and fourth stages of the first and second subnetworks operate similarly to the second stages. In the first and second subnetworks, feature maps of 256 channels are transferred from the third stage to the fourth stage, and feature maps of 512 channels are transferred from the fourth stage to the fifth stage. Before the feature maps generated by the convolutions in the fourth stage of the first and second subnetworks are downsampled by the 2&#xd7;2 maxpooling in the pooling layer, a dropout may be applied to the feature maps.</p><p id="p-0062" num="0061">Convolutional layers in the fifth stage of the first subnetwork perform convolutions on feature maps received from the fourth stage using 3&#xd7;3 kernels to generate feature maps of 1024 channels. The fifth stage of the first subnetwork may include an up-convolution layer, which performs an up-convolution including a 2&#xd7;2 upsampling and 2&#xd7;2 convolutions on the feature maps. The 2&#xd7;2 upsampling doubles the dimension of the feature maps and the 2&#xd7;2 convolutions reduce the number of channels by a half. As a result, the 1024-channel feature maps generated by the convolutions may be converted into 512-channel feature maps having doubled dimensions and applied to the sixth stage. Similarly, in the second subnetwork, convolutional layers in the fifth stage perform convolutions on the feature maps received from the fourth stage using 3&#xd7;3 kernels to generate feature maps of 1024 channels. The fifth stage of the second subnetwork may include an up-convolution layer, which performs the up-convolution including the 2&#xd7;2 upsampling and the 2&#xd7;2 convolutions on the feature maps. The 2&#xd7;2 upsampling doubles the dimension of the feature maps and the 2&#xd7;2 convolutions reduce the number of channels by a half. As a result, the 1024-channel feature maps generated by the convolutions may be converted into 512-channel feature maps having doubled dimensions and applied to the sixth stage. In the fifth stage of the first and second subnetworks, before the feature maps go through the up-convolution, the dropout may be applied to the feature maps.</p><p id="p-0063" num="0062">Meanwhile, the feature maps generated by convolutions in the fourth stage of the first subnetwork are copied and cropped and provided to the sixth stage. The path of this feature map is indicated by a skip connection. In addition, the skip connections are also formed between the third stage and the seventh stage, between the second stage and the eighth stage, and between the first stage and the ninth stage. That is, the feature maps generated by convolutions in the third stage are copied and cropped and provided to the seventh stage, the feature maps generated by convolutions in the second stage are copied and cropped and provided to the eighth stage, and the feature maps generated by the convolutions in the first stage are copied and cropped and provided to the ninth stage. The skip connections are similarly formed in the second subnetwork.</p><p id="p-0064" num="0063">In the sixth stage of the first subnetwork, the 512-channel feature maps received from the up-convolution layer of the fifth stage are concatenated with the 512-channel feature maps transferred from the fourth stage to form feature maps of 1024 channels. Convolutional layers perform convolutions on the concatenated feature maps using 3&#xd7;3 kernels to generate feature maps of 512 channels. The sixth stage of the first subnetwork may include an up-convolution layer, which performs the up-convolution including the 2&#xd7;2 upsampling and the 2&#xd7;2 convolutions on the 512-channel feature maps. The 2&#xd7;2 upsampling doubles the dimension of the feature maps and the 2&#xd7;2 convolutions reduce the number of channels by a half. As a result, the 512-channel feature maps generated by the convolutions may be converted into 256-channel feature maps having doubled dimensions and applied to the seventh stage. Similarly, in the sixth stage of the second subnetwork, the 512-channel feature maps received from the up-convolution layer of the fifth stage are concatenated with the 512-channel feature maps transferred from the fourth stage to form feature maps of 1024 channels. Convolutional layers perform convolutions on the concatenated feature maps using 3&#xd7;3 kernels to generate feature maps of 512 channels. The sixth stage of the second subnetwork may include an up-convolution layer, which performs the up-convolution including the 2&#xd7;2 upsampling and the 2&#xd7;2 convolutions on the 512-channel feature maps. The 2&#xd7;2 upsampling doubles the dimension of the feature maps and the 2&#xd7;2 convolutions reduce the number of channels by a half. As a result, the 512-channel feature maps generated by the convolutions may be converted into 256-channel feature maps having doubled dimensions and applied to the seventh stage.</p><p id="p-0065" num="0064">The seventh to ninth stages of the first and second subnetworks operate similarly to the sixth stages. In each of the first and second subnetworks, the seventh stage concatenates the 256-channel feature maps from the sixth stage with another 256-channel feature maps copied from the third stage and performs the 3&#xd7;3 convolutions and the up-convolution to output 128-channel feature maps. The eighth stage concatenates the 128-channel feature maps from the seventh stage with another 128-channel feature maps copied from the second stage and performs the 3&#xd7;3 convolutions and the up-convolution to output 64-channel feature maps. The ninth stage concatenates the 64-channel feature maps from the eighth stage with another 64-channel feature maps copied from the first stage and performs the 3&#xd7;3 convolutions to output feature maps of 2 channels.</p><p id="p-0066" num="0065">A final stage includes a concatenation layer and a convolutional layer. The concatenation layer receives the feature maps of 2 channels from each of the ninth stages of the first and second subnetworks and concatenates the feature maps of 4 channels to output concatenated feature maps of 4 channels. The convolutional layer performs point-wise convolutions on the concatenated feature maps of 4 channels using a 1&#xd7;1 kernel to output a convolution result as the predicted current frame {circumflex over (F)}<sub>t</sub>.</p><p id="p-0067" num="0066">As described above, according to an exemplary embodiment of the present disclosure, the predicted current frame {circumflex over (F)}<sub>t </sub>is generated by combining the first subnetwork processing the second previous frame F<sub>t&#x2212;2 </sub>and the second subnetwork processing the second subsequent frame F<sub>t+2</sub>. Further, connections are established between the corresponding stages of the first and second subnetworks so as to allow exchanges and concatenations of the feature maps in the corresponding stages. Such a combination of the subnetworks enables to predict the current frame more precisely in the absence of an anomaly.</p><p id="p-0068" num="0067">In the neural network model described above, all the convolutional layers may use the Rectified Linear Unit (ReLU) as an activation function. Also, a pixel-wise mean squared error expressed by Equation 1 may be used as a loss function.</p><p id="p-0069" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>L</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mover accent="true">        <mi>F</mi>        <mi>&#x2c6;</mi>       </mover>       <mo>,</mo>       <mi>F</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mfrac>       <mn>1</mn>       <mrow>        <mi>h</mi>        <mo>&#xb7;</mo>        <mi>w</mi>       </mrow>      </mfrac>      <mo>&#x2062;</mo>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>h</mi>       </munderover>       <mrow>        <munderover>         <mo>&#x2211;</mo>         <mrow>          <mi>j</mi>          <mo>=</mo>          <mn>1</mn>         </mrow>         <mi>w</mi>        </munderover>        <msup>         <mrow>          <mo>(</mo>          <mrow>           <msub>            <mover accent="true">             <mi>F</mi>             <mi>&#x2c6;</mi>            </mover>            <mi>ij</mi>           </msub>           <mo>-</mo>           <msub>            <mi>F</mi>            <mi>ij</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>         <mn>2</mn>        </msup>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mn>1</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0070" num="0068">Here, &#x2018;w&#x2019; denotes a horizontal dimension of the frame, i.e., a horizontal resolution, and &#x2018;h&#x2019; denotes a vertical dimension of the frame, i.e., a vertical resolution. &#x2018;F<sub>ij</sub>&#x2019; denotes a pixel value of a pixel at a position (i,j) in the actual current frame, and &#x2018;{circumflex over (F)}<sub>ij</sub>&#x2019; denotes a pixel value of the pixel at the position (i,j) in the predicted current frame. The value ({circumflex over (F)}<sub>ij</sub>&#x2212;F<sub>ij</sub>)<sup>2 </sup>may be obtained as a squared error for the pixel in a squared error image used and need not be calculated separately.</p><p id="p-0071" num="0069">The neural network may be trained to accurately predict the current frame {circumflex over (F)}<sub>t </sub>from the second previous frame F<sub>t&#x2212;2 </sub>and the second subsequent frame F<sub>t+2 </sub>in a normal condition other than an abnormal situation before the network is actually applied to an abnormality detection as mentioned above, and the training result is reflected in the filters, i.e., the kernels, used for the convolutional operations. Accordingly, the trained neural network can predict the current frame quite well, so that there is little significant difference between the predicted current frame {circumflex over (F)}<sub>t </sub>and the actual current frame F<sub>t </sub>except the abnormality. The method and apparatus according to the present disclosure detects the abnormal events based on this feature. That is, the abnormality detecting apparatus according to an exemplary embodiment of the present disclosure may determine that an abnormal event has occurred in the current frame F<sub>t </sub>when there is a difference greater than a certain degree between the predicted current frame {circumflex over (F)}<sub>t </sub>determined by the neural network and the actual current frame F<sub>t</sub>.</p><p id="p-0072" num="0070"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart illustrating a process of deriving the anomaly score for the current frame based on the squared error image, and <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows pseudo-codes of an algorithm for implementing the process of deriving the anomaly score shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a sliding and resizing of a window.</p><p id="p-0073" num="0071">As mentioned above, a local anomaly score may be calculated while moving a window in a unit of a certain stride horizontally and vertically with respect to the image frame. The local anomaly score may be calculated as an average of squared errors for pixels in an image frame portion overlapping the window. Then, an average of at least some of the local anomaly scores calculated over the entire region of the image frame may be determined as the anomaly score for the entire image frame.</p><p id="p-0074" num="0072">First, the abnormality detecting apparatus may determine whether a squared error image is prepared, that is, if the squared error I<sub>ij</sub>=({circumflex over (F)}<sub>ij</sub>&#x2212;F<sub>ij</sub>)<sup>2 </sup>that is a squared value of a difference between the pixel values of the predicted current frame {circumflex over (F)}<sub>t </sub>and the actual current frame F<sub>t </sub>is calculated or not for all the pixels of the image frame (operation <b>300</b>). If the squared error image is not prepared, the abnormality detecting apparatus may prepare the square error image. Alternatively, however, the operation <b>300</b> may be performed later than an operation <b>302</b> or another operation. Further, the operation <b>300</b> of preparing the squared error image may be omitted, and the squared error may be calculated whenever necessary after the operation <b>302</b>, i.e., during process of deriving the anomaly score shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0075" num="0073">Subsequently, the coordinates of a bottom left corner of the window are initialized to (0, 0). Accordingly, the window will be positioned at the bottom left corner in the squared error image (operation <b>302</b>).</p><p id="p-0076" num="0074">In operation <b>304</b>, the abnormality detecting apparatus obtains the local anomaly score p<sub>k </sub>by calculating the average of the squared errors for the pixels in the region overlapping the window, as expressed by Equation 2. That is, the local anomaly score p<sub>k </sub>may be the average of pixel values in the squared error image patch corresponding to the window.</p><p id="p-0077" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>p</mi>      <mi>k</mi>     </msub>     <mo>=</mo>     <mrow>      <munderover>       <mo>&#x2211;</mo>       <mrow>        <mi>i</mi>        <mo>=</mo>        <mi>x</mi>       </mrow>       <mrow>        <mi>x</mi>        <mo>+</mo>        <mover accent="true">         <mi>s</mi>         <mi>&#x2c6;</mi>        </mover>       </mrow>      </munderover>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>j</mi>         <mo>=</mo>         <mi>y</mi>        </mrow>        <mrow>         <mi>y</mi>         <mo>+</mo>         <mover accent="true">          <mi>s</mi>          <mi>&#x2c6;</mi>         </mover>        </mrow>       </munderover>       <msub>        <mi>I</mi>        <mrow>         <mi>i</mi>         <mo>,</mo>         <mi>j</mi>        </mrow>       </msub>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mn>2</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0078" num="0075">In Equation 2 and <figref idref="DRAWINGS">FIG. <b>5</b></figref>, &#x2018;s&#x2019; denotes a width and height of the image frame and &#x2018;&#x15d;&#x2019; denotes a width and height of the window. It is assumed that the width and the height of the image frame are the same as each other and the width and the height of the window are the same as each other. However, the present disclosure is not limited thereto, and the width and the height of the image frame may be different from each other and the width and the height of the window may be different from each other.</p><p id="p-0079" num="0076">Next, the window is translated to the right by a certain horizontal stride (operation <b>306</b>). The horizontal stride may have the same size as the width of the window. After the window is translated, the local anomaly score p<sub>k </sub>is calculated for the translated window. The translation of the window and the calculation of the local anomaly score may continue until the window reaches a right edge of the image frame.</p><p id="p-0080" num="0077">When the window reaches the right edge of the image frame (operation <b>308</b>) but the window does not reach a top right corner of the image frame (operation <b>310</b>), the window is translated to the left edge again and translated upward by a certain vertical stride (operation <b>312</b>). The vertical stride may have the same size as the height of the window. After the window is translated upward vertically, the size of the window may be reduced by a certain reduction size (operation <b>314</b>). The reduction of the window size may be accomplished only with respect to the height. Alternatively, however, the reduction of the window size may be accomplished with respect to the width in addition to the height. The reduction of the window size takes into account a phenomenon that a size of an object decreases as the object moves away from the camera, i.e., as the object moves toward an upper side of the image frame, due to a perspective difference of the objects in the image frame. Accordingly, the window gets smaller gradually as it is translated upward in the image frame in accordance with an exemplary embodiment of the present disclosure. Considering that the window size becomes smaller as it is translated upward, a technique of calculating the anomaly score while moving the window as shown in <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref> may be referred to as a &#x2018;cascade sliding window&#x2019; method.</p><p id="p-0081" num="0078">After the window is translated in the operation <b>312</b>, the calculation of the local anomaly score p<sub>k </sub>for the moved window and an additional translation of the window may be continued (operations <b>304</b> and <b>306</b>). The process of calculating the local anomaly score p<sub>k </sub>while moving the window to the right and upward in the operations <b>304</b>-<b>314</b> may continue until the window reaches the top right corner of the image frame.</p><p id="p-0082" num="0079">After it is determined that the window reached the top right corner of the image frame (operation <b>310</b>), the average of at least some of the local anomaly scores p<sub>k </sub>among the local anomaly scores p<sub>k </sub>calculated for all the window positions may be calculated by Equation 3 and determined as an anomaly score S for the image frame (operation <b>316</b>). Though an average of all the local anomaly scores p<sub>k </sub>may be determined as the anomaly score S for the image frame, an average of only some of the local anomaly scores p<sub>k </sub>may be determined as the anomaly score S for the image frame. For example, after sorting all local anomaly scores p<sub>k </sub>in an ascending or descending order, only a certain number of local anomaly scores p<sub>k </sub>may be averaged to determine the anomaly score S. A higher anomaly score may indicate a higher probability that the current frame contains an abnormal event.</p><p id="p-0083" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>S</mi>     <mo>=</mo>     <mrow>      <mfrac>       <mn>1</mn>       <mi>n</mi>      </mfrac>      <mo>&#x2062;</mo>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>n</mi>       </munderover>       <msub>        <mi>p</mi>        <mi>i</mi>       </msub>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mn>3</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0084" num="0080">When moving the window to the right by the horizontal stride in the operation <b>306</b>, a space for moving the window to the right may be insufficient and an x-coordinate of a rightmost column of a next window <b>410</b>M may become larger than an x-coordinate of a rightmost column of the image frame <b>400</b>. In such a case, a window position <b>410</b>M&#x2032; of the next window may be determined such the rightmost column of the next window coincides with the rightmost column of the image frame <b>400</b>, and then the local anomaly score p<sub>k </sub>may be calculated for the adjusted window position (See steps <b>8</b>-<b>10</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, and <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0085" num="0081">Similarly, when moving the window upwards by the vertical stride in the operation <b>312</b>, a space for moving the window upwards may be insufficient and a y-coordinate of a top row of the next window <b>410</b>P may become larger than a y-coordinate of a top row of the image frame <b>400</b>. In such a case, a window position <b>410</b>P&#x2032; of the next window may be determined such the top row of the next window coincides with the top row of the image frame <b>400</b>, and then the local anomaly score p<sub>k </sub>may be calculated for the adjusted window position (See steps <b>17</b>-<b>19</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, and <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0086" num="0082">Although it was assumed above that the width and the height of the image frame are the same as each other and denoted commonly by &#x2018;s&#x2019; in the embodiment shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the width and the height of the image frame may be different from each other. Also, although it was assumed above that the width and the height of the window are the same as each other and denoted commonly by &#x2018;&#x15d;&#x2019;, the width and the height of the window may be different from each other. The window decrease step &#x2018;d&#x2019; may also be set differently for the horizontal and vertical directions.</p><p id="p-0087" num="0083"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram of an abnormality detecting apparatus according to an exemplary embodiment of the present disclosure. The abnormality detecting apparatus according to the present embodiment may include at least one processor <b>520</b>, a memory <b>540</b>, and a storage <b>560</b>. As mentioned above, the abnormality detecting apparatus may be implemented by the monitoring control server <b>20</b> in the video surveillance system shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In such a case, the monitoring control server <b>20</b> may receive the monitoring image acquired by each of the monitoring cameras <b>10</b>A-<b>10</b>N and detect abnormal events in the monitoring images received from the monitoring cameras <b>10</b>A-<b>10</b>N. Alternatively, however, each of the monitoring cameras <b>10</b>N-<b>10</b>N may serve as the abnormality detecting apparatus instead of the monitoring control server <b>20</b> and provide the monitoring control server <b>20</b> with a detection result.</p><p id="p-0088" num="0084">The processor <b>520</b> may execute program instructions stored in the memory <b>540</b> and/or the storage <b>560</b>. The processor <b>520</b> may be at least one central processing unit (CPU), a graphics processing unit (GPU), or another kind of dedicated processor suitable for performing processes according to the present disclosure.</p><p id="p-0089" num="0085">The memory <b>540</b> may include, for example, a volatile memory such as a read only memory (ROM) and a nonvolatile memory such as a random access memory (RAM). The memory <b>540</b> may load the program instructions stored in the storage <b>560</b> to provide to the processor <b>520</b> so that the processor <b>520</b> may execute the program instructions.</p><p id="p-0090" num="0086">The storage <b>560</b> may include an intangible recording medium suitable for storing the program instructions, data files, data structures, and a combination thereof. Any device capable of storing data that may be readable by a computer system may be used for the storage. Examples of the storage medium may include magnetic media such as a hard disk, a floppy disk, and a magnetic tape, optical media such as a compact disk read only memory (CD-ROM) and a digital video disk (DVD), magneto-optical medium such as a floptical disk, and semiconductor memories such as ROM, RAM, a flash memory, and a solid-state drive (SSD).</p><p id="p-0091" num="0087">The program instructions stored in the storage <b>560</b> may include an abnormality detection program for implementing the abnormality detecting method according to the present disclosure. The program instructions, when executed by the processor <b>520</b>, may cause the processor <b>520</b> to: generate a predicted current frame based on a previous frame temporally ahead of an actual current frame and a subsequent frame temporally behind the actual current frame; calculate an anomaly score indicating a difference between the predicted current frame and the actual current frame; and determine that an abnormality is included in the actual current frame when the anomaly score satisfies a predetermined condition. The program instructions may be executed by the processor <b>520</b> after being loaded into the memory <b>540</b> under a control of the processor <b>520</b> to implement the method according to the present invention.</p><p id="p-0092" num="0088">The inventors have tested the accuracy and speed of the abnormality detecting apparatus according to an exemplary embodiment of the present disclosure using CUHK Avenue, UCSD Ped2, and ShanghaiTech Campus datasets, which are commonly used in the research of the image analysis. The characteristics of each dataset are as follows.</p><p id="p-0093" num="0089">The CUHK Avenue dataset contains 16 training videos and 21 test videos with a resolution of 640&#xd7;360 and a frame rate of 25 frames per second (fps). This dataset has the characteristics that the object gets smaller as the object moves away from the camera.</p><p id="p-0094" num="0090">The UCSD Ped2 dataset contains 16 training videos and 12 test videos with a resolution of 360&#xd7;240 and the frame rate of 10 fps. This dataset has the characteristics that the size of the object remains almost the same when the object moves away from the camera.</p><p id="p-0095" num="0091">The ShanghaiTech Campus dataset contains 330 training videos and 107 test videos with a resolution of 856&#xd7;480 and the frame rate of 24 fps. Unlike the CUHK Avenue and UCSD Ped2 datasets, this dataset has the characteristics that the videos have been recorded at 13 locations at various angles and with various light conditions.</p><p id="p-0096" num="0092">The inventors trained the Cross U-Net model which is the artificial neural network according to the present disclosure with the training videos of the datasets and obtained the anomaly scores for each frame of the test video using the trained Cross U-Net model and the cascade sliding window method. The inventors obtained a receiver operating characteristic (ROC) curve based on the anomaly score for each frame and the inclusion of an actual abnormal event, and then obtained a frame-level area under the ROC curve (AUC) using the ROC curve.</p><p id="p-0097" num="0093"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a table summarizing the AUC values for each dataset according to the window decrease step. The inventors obtained the AUC values of 90.77% from the Avenue dataset, 96.99% from the UCSD Ped2 dataset, and 72.48% from the ShanghaiTech dataset. In case of the CUHK Avenue dataset, the highest AUC was obtained when the window reduction unit was 4 unlike the other two datasets, which show that the cascade sliding window method is effective for the CUHK Avenue dataset having the characteristics that the object gets smaller as the object moves away from the camera.</p><p id="p-0098" num="0094"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a table comparing the frame-level AUC of the abnormality detecting method according to the present disclosure and other latest detection methods for the datasets. It can be seen that the method of the present disclosure achieved the highest AUC for the CUHK Avenue dataset, and achieved relatively high AUCs for the other datasets. The detection methods compared with the method of the present disclosure in the table of <figref idref="DRAWINGS">FIG. <b>9</b></figref> includes:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0095">Appearance and Motion DeepNet (AMDN) disclosed in Xu, D., Yan, Y., Ricci, E., et al., 2017<i>. Detecting anomalous events in videos by learning deep representations of appearance and motion</i>. Comput. Vision Image Understand. 156, 117-127;</li>    <li id="ul0001-0002" num="0096">a method disclosed in Zaheer, M. Z., A. Mahmood, M. Astrid, et al., 2020a. <i>CLAWS: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection</i>. European Conference on Computer Vision, Springer;</li>    <li id="ul0001-0003" num="0097">Convolution Auto-Encoder (Conv-AE) disclosed in Hasan, M., Choi, J., Neumann, J., et al., 2016<i>. Learning temporal regularity in video sequences</i>. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition;</li>    <li id="ul0001-0004" num="0098">Convolutional Long Short Term Memory (ConvLSTM) disclosed in Luo, W., Liu, W., Gao, S., 2017b. <i>Remembering history with convolutional LSTM for anomaly detection. </i>2017 IEEE International Conference on Multimedia and Expo (ICME). IEEE;</li>    <li id="ul0001-0005" num="0099">Temporally-coherent sparse coding (TSC) disclosed in Luo, W., Liu, W., Gao, S., 2017a. <i>A revisit of sparse coding based anomaly detection in stacked rnn framework</i>. Proceedings of the IEEE International Conference on Computer Vision;</li>    <li id="ul0001-0006" num="0100">a method disclosed in Liu, W., Luo, W., Lian, D., et al., 2018<i>. Future frame prediction for anomaly detection&#x2014;a new baseline</i>. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition;</li>    <li id="ul0001-0007" num="0101">Stacked Recurrent Neural Network Autoencoder (sRNN-AE) disclosed in Luo, W., Liu, W., Lian, D., et al., 2019<i>. Video anomaly detection with sparse coding inspired deep neural networks</i>. IEEE Trans. Pattern Anal. Mach. Intell;</li>    <li id="ul0001-0008" num="0102">a method disclosed in Tang, Y., Zhao, L., Zhang, S., et al., 2020<i>. Integrating prediction and reconstructionfor anomaly detection</i>. Pattern Recogn. Lett. 129, 123-130;</li>    <li id="ul0001-0009" num="0103">Anomalynet disclosed in Zhou, J. T., Du, J., Zhu, H., et al., 2019a. <i>Anomalynet: An anomaly detection network for video surveillance</i>. IEEE Trans. Inf. Forensics Secur. 14 (10), 2537-2550;</li>    <li id="ul0001-0010" num="0104">Message-Passing Encoder-Decoder Recurrent Neural Network (MPED-RNN) disclosed in Morais, R., Le, V., Tran, T., et al., 2019<i>. Learning regularity in skeleton trajectories for anomaly detection in videos</i>. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition;</li>    <li id="ul0001-0011" num="0105">Nguyen, T.-N., Meunier, J., 2019<i>. Anomaly detection in video sequence with appearance</i>-<i>motion correspondence</i>. Proceedings of the IEEE/CVF International Conference on Computer Vision;</li>    <li id="ul0001-0012" num="0106">Spatio-temporal adversarial network (STAN) disclosed in Lee, S., Kim, H. G., Ro, Y. M., 2018. STAN: <i>Spatio</i>-<i>temporal adversarial networks for abnormal event detection</i>. IEEE international Conference on Acoustics, Speech and Signal Processing (ICASSP);</li>    <li id="ul0001-0013" num="0107">a method disclosed in Chen, D., Wang, P., Yue, L., Zhang, Y., Jia, T., 2020<i>. Anomaly detection in surveillance video based on bidirectional prediction</i>. Image and Vision Computing. 98, 103915; and</li>    <li id="ul0001-0014" num="0108">a method disclosed in Ionescu, R. T., Khan, F. S., Georgescu, M.-I., et al., 2019<i>. Object</i>-<i>centric auto</i>-<i>encoders and dummy anomalies for abnormal event detection in video</i>. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</li></ul></p><p id="p-0099" num="0109">In order to measure the speed of detecting the abnormal events which is one of the most important factors in the video surveillance system, the processing time per frame was measured by dividing into a preprocessing time, a current frame prediction time, and an abnormal event inference time. The &#x2018;preprocessing time&#x2019; includes a time to extract frames from the video and convert the frames into images before inputting the frames to the Cross U-Net model, a time to convert color images to black-and-white images, and a time to convert the size of the images to a prescribed size, e.g., 256&#xd7;256. The &#x2018;current frame prediction time&#x2019; refers to a time required by the Cross U-Net model to predict the current frame using the previous frame and the subsequent frame. The &#x2018;abnormal event inference time&#x2019; refers to a time to infer the anomaly score of the current frame using the cascade sliding window method.</p><p id="p-0100" num="0110"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a processing speed per frame according to an exemplary embodiment of the present disclosure for each dataset, measured in an environment of INTEL Core i9-10940X CPU operating at 3.30 GHz and equipped with NVIDIA TITAN RTX and GDDR6 of 24 GB. <figref idref="DRAWINGS">FIG. <b>10</b></figref> shows that the abnormality detecting method according to an exemplary embodiment of the present disclosure revealed a processing time of 31 milliseconds (ms) per each frame (i.e., processing speed of about 32 fps) for the CUHK Avenue dataset, 33 ms (i.e., about 30 fps) for the UCSD Ped2 dataset, and 41 ms (i.e., about 24 fps) for the ShanghaiTech Campus dataset. This means that the abnormality detecting method according to an exemplary embodiment is capable of detecting an abnormal event in real-time in a video in the CUHK Avenue dataset with the frame rate of 25 fps, in a video in the UCSD Ped2 dataset with the frame rate of 10 fps, and in the video in the ShanghaiTech dataset with the frame rate of 24 fps.</p><p id="p-0101" num="0111">Comparing the processing speed for each frame with the method of Ionescu et al. which has shown the higher frame-level AUC for the UCSD Ped2 dataset and the ShanghaiTech Campus dataset than the method of the present disclosure, the abnormality detecting method according to an exemplary embodiment is 4 times faster in case of the CUHK Avenue dataset, 7 times faster in case of the UCSD Ped2 dataset, and 3 times faster in case of the ShanghaiTech Campus dataset as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0102" num="0112">In order to analyze the abnormal situations that the method of the present disclosure infers well and other abnormal situations that the method does not infer well, the frames including the abnormal situations were classified into the frames resulting in high anomaly scores and the frames resulting in low anomaly scores. Further, for each frame, the actual frame, the predicted frame, and the difference between the actual frame and the predicted frame were visualized. <figref idref="DRAWINGS">FIGS. <b>12</b>A-<b>12</b>C</figref> show examples of the actual frames, the predicted frames, and visualizations of the squared errors of the actual frames and respective predicted frames for the image frames containing abnormal situations. Specifically, <figref idref="DRAWINGS">FIG. <b>12</b>A</figref> shows examples of the actual frames, the predicted frames, and visualizations of the squared errors of the frames for the images in the CUHK Avenue dataset. <figref idref="DRAWINGS">FIG. <b>12</b>B</figref> shows examples of the actual frames, the predicted frames, and visualizations of the squared errors of the frames for the images in the UCSD Ped2 dataset. <figref idref="DRAWINGS">FIG. <b>12</b>C</figref> shows examples of the actual frames, the predicted frames, and visualizations of the squared errors of the frames for the images in the ShanghaiTech Campus dataset.</p><p id="p-0103" num="0113">As shown in <figref idref="DRAWINGS">FIGS. <b>12</b>A-<b>12</b>C</figref>, the abnormality detecting method according to an exemplary embodiment of the present disclosure revealed high anomaly scores for rides such as a bicycle, an automobiles, and a skateboard; and people who perform unusual behaviors such as a person who dances, a person who throws an object, a person who runs, and a person who walks in a wrong direction. Meanwhile, the abnormality detecting method revealed low anomaly scores for stationary objects, dark objects, and partially visible objects such as an occluded object.</p><p id="p-0104" num="0114">An ablation study was conducted to check whether the abnormality detecting method of the present disclosure using the Cross U-Net model shows a better performance than the methods using simplified models that do not employ some features of the Cross U-Net model. In the ablation study, compared were four models: the simplest model having no connection for the concatenation; a &#x2018;CC&#x2019; model that employs cross connections between the contracting paths of two subnetworks to enable the exchange of the feature maps between the subnetworks and the concatenation of the feature maps received from the other subnetwork; a &#x2018;CE&#x2019; model that employs skip connections between the contracting path and the expansive path of each subnetwork to enable the cropping of the feature maps in the contracting path and the concatenation of the feature maps in the expansive path; and the full Cross U-Net model. <figref idref="DRAWINGS">FIG. <b>13</b></figref> shows the results of the ablation study. It can be seen in the drawing that the full Cross U-Net model revealed the best performance.</p><p id="p-0105" num="0115">The apparatus and method according to an exemplary embodiment of the present disclosure may be implemented by computer-readable programs or codes stored in an intangible computer-readable storage medium readable by the monitoring camera or a monitoring control server. The computer-readable storage medium may be any type of data storage device capable of storing data that can thereafter be read by a computer system. In addition, the computer-readable recording medium may also be distributed over network-coupled computer systems so that the computer-readable program or code is stored and executed in a distributed fashion.</p><p id="p-0106" num="0116">The computer-readable recording medium may include a hardware device specially constructed to store and execute a program instruction, for example, a ROM, a RAM, and a flash memory. The program instruction may include a high-level language code executable by a computer through an interpreter in addition to a machine language code made by a compiler.</p><p id="p-0107" num="0117">Some aspects of the present disclosure have been described above in the context of a device but may be described using a method corresponding thereto. Here, blocks or the device corresponds to operations of the method or characteristics of the operations of the method. Similarly, aspects of the present disclosure described above in the context of a method may be described using blocks or items corresponding thereto or characteristics of a device corresponding thereto. Some or all of the operations of the method may be performed, for example, by (or using) a hardware device such as a microprocessor, a programmable computer or an electronic circuit. In some exemplary embodiments, at least one of the most important operations of the method may be performed by such a device.</p><p id="p-0108" num="0118">In some exemplary embodiments, a programmable logic device such as a field-programmable gate array may be used to perform some or all of functions of the methods described herein. In some exemplary embodiments, the field-programmable gate array may be operated with a microprocessor to perform one of the methods described herein. In general, the methods are preferably performed by a certain hardware device.</p><p id="p-0109" num="0119">Although the embodiments of the present disclosure have been described in detail, it should be understood that various substitutions, additions, and modifications are possible without departing from the scope and spirit of the present disclosure, and the scope of the present disclosure is limited by the claims and the equivalents thereof.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005269A1-20230105-M00001.NB"><img id="EMI-M00001" he="8.47mm" wi="76.20mm" file="US20230005269A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005269A1-20230105-M00002.NB"><img id="EMI-M00002" he="8.81mm" wi="76.20mm" file="US20230005269A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005269A1-20230105-M00003.NB"><img id="EMI-M00003" he="7.79mm" wi="76.20mm" file="US20230005269A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of detecting an abnormality in a series of temporally successive images, comprising:<claim-text>generating a predicted current frame based on a previous frame temporally ahead of an actual current frame and a subsequent frame temporally behind the actual current frame;</claim-text><claim-text>calculating an anomaly score indicating a difference between the predicted current frame and the actual current frame; and</claim-text><claim-text>determining that an abnormality is included in the actual current frame when the anomaly score satisfies a predetermined condition.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the previous frame is temporally ahead of the actual current frame by a plurality of frames, and the subsequent frame is temporally behind the actual current frame by the plurality of frames.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the predicted current frame is generated by an artificial neural network comprising a first subnetwork receiving the previous frame as an input and a second subnetwork receiving the subsequent frame as an input,<claim-text>wherein each of the first and second subnetworks comprises at least one layer stage, each layer stage having at least one convolutional layer.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein generating the predicted current frame comprises:<claim-text>receiving, in a layer stage of the first subnetwork, a second feature map from a corresponding layer stage of the second subnetwork to concatenate the second feature map with a first feature map generated by the layer stage of the first subnetwork; and</claim-text><claim-text>receiving, in the corresponding layer stage of the second subnetwork, the first feature map from the layer stage of the first subnetwork to concatenate the first feature map with the second feature map generated by the corresponding layer stage of the second subnetwork.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the artificial neural network is used after being trained in advance to generate the predicted current frame based on the previous frame and the subsequent frame in a normal situation where the actual current frame contains no abnormality.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein calculating the anomaly score comprises:<claim-text>calculating a plurality of local anomaly scores by moving a window with respect to the actual current frame horizontally and vertically in a unit of a predetermined stride and performing a predetermined operation on pixel value differences between pixels in the predicted current frame and corresponding pixels in the actual current frame for an image frame portion overlapping the window at each window location; and</claim-text><claim-text>determining the anomaly score by averaging or summing the plurality of local anomaly scores calculated according to a movement of the window.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein each of the plurality of local anomaly scores is calculated by averaging or summing the pixel value differences between pixels in the predicted current frame and corresponding pixels in the actual current frame for the image frame portion overlapping the window.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein determining the anomaly score comprises:<claim-text>determining the anomaly score by averaging only a predetermined number of local anomaly scores selected in an order of magnitude among the plurality of local anomaly scores calculated according to the movement of the window.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein a size of the window is set to decrease as the window moves upward with respect to the actual current frame.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>preprocessing the series of temporally successive images to convert to black-and-white images or adjust resolutions of the images before generating the predicted current frame.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An apparatus for detecting an abnormality in a series of temporally successive images, comprising:<claim-text>a processor; and</claim-text><claim-text>a memory storing program instructions to be executed by the processor,</claim-text><claim-text>wherein the program instructions, when executed by the processor, causes the processor to:</claim-text><claim-text>generate a predicted current frame based on a previous frame temporally ahead of an actual current frame and a subsequent frame temporally behind the actual current frame;</claim-text><claim-text>calculate an anomaly score indicating a difference between the predicted current frame and the actual current frame; and</claim-text><claim-text>determine that an abnormality is included in the actual current frame when the anomaly score satisfies a predetermined condition.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the previous frame is temporally ahead of the actual current frame by a plurality of frames, and the subsequent frame is temporally behind the actual current frame by the plurality of frames.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the program instructions causing the processor to generate the predicted current frame comprises instructions to:<claim-text>configure an artificial neural network comprising a first subnetwork receiving the previous frame as an input and a second subnetwork receiving the subsequent frame as an input; and</claim-text><claim-text>generate the predicted current frame by the artificial neural network,</claim-text><claim-text>wherein each of the first and second subnetworks comprises at least one layer stage, each layer stage having at least one convolutional layer.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the program instructions causing the processor to configure the artificial neural network comprises instructions to:<claim-text>receive, in a layer stage of the first subnetwork, a second feature map from a corresponding layer stage of the second subnetwork and concatenate the second feature map with a first feature map generated by the layer stage of the first subnetwork; and</claim-text><claim-text>receive, in the corresponding layer stage of the second subnetwork, the first feature map from the layer stage of the first subnetwork and concatenate the first feature map with the second feature map generated by the corresponding layer stage of the second subnetwork.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the program instructions causing the processor to configure the artificial neural network comprises instructions to:<claim-text>train the artificial neural network to generate the predicted current frame based on the previous frame and the subsequent frame in a normal situation where the actual current frame contains no abnormality.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the program instructions causing the processor to calculate the anomaly score comprises instructions to:<claim-text>calculate a plurality of local anomaly scores by moving a window with respect to the actual current frame horizontally and vertically in a unit of a predetermined stride and performing a predetermined operation on pixel value differences between pixels in the predicted current frame and corresponding pixels in the actual current frame for an image frame portion overlapping the window at each window location; and</claim-text><claim-text>determine the anomaly score by averaging or summing the plurality of local anomaly scores calculated according to a movement of the window.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the program instructions causing the processor to calculate local anomaly scores comprises instructions to:<claim-text>determine each of the plurality of local anomaly scores by averaging or summing the pixel value differences between pixels in the predicted current frame and corresponding pixels in the actual current frame for the image frame portion overlapping the window.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the program instructions causing the processor to determine the anomaly score comprises instructions to:<claim-text>calculate an average of only a predetermined number of local anomaly scores selected in an order of magnitude among the plurality of local anomaly scores calculated according to the movement of the window.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein a size of the window is set to decrease as the window moves upward with respect to the actual current frame.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the program instructions comprise instructions to:<claim-text>perform a preprocessing of the series of temporally successive images to convert to black-and-white images or adjust resolutions of the images before generating the predicted current frame.</claim-text></claim-text></claim></claims></us-patent-application>