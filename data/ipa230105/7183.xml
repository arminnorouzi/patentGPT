<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007184A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007184</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17850831</doc-number><date>20220627</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2021-109537</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232121</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232122</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232935</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">CONTROL APPARATUS FOR DETECTING AND DISPLAYING LINE-OF-SIGHT POSITION, CONTROL METHOD THEREOF, AND RECORDING MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>CANON KABUSHIKI KAISHA</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Tamaki</last-name><first-name>Yoshihito</first-name><address><city>Kanagawa</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A control apparatus includes at least one processor configured to acquire information corresponding to a position at which an observer stares, wherein the information indicates a shift in the position, and control a display unit to display an indicator corresponding to the position, wherein, in a case where a shift in the position is continuously smaller than a first threshold a first number of times, a display position of the indicator is not shifted.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="106.09mm" wi="158.75mm" file="US20230007184A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="242.91mm" wi="165.35mm" orientation="landscape" file="US20230007184A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="181.27mm" wi="102.36mm" file="US20230007184A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="170.18mm" wi="102.45mm" file="US20230007184A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="128.52mm" wi="116.42mm" file="US20230007184A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="112.18mm" wi="130.81mm" file="US20230007184A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="207.43mm" wi="118.45mm" file="US20230007184A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="204.13mm" wi="163.07mm" file="US20230007184A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="252.73mm" wi="77.89mm" orientation="landscape" file="US20230007184A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="231.90mm" wi="119.97mm" file="US20230007184A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="252.73mm" wi="76.54mm" orientation="landscape" file="US20230007184A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="225.47mm" wi="127.42mm" file="US20230007184A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="244.35mm" wi="168.91mm" file="US20230007184A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="219.79mm" wi="101.18mm" file="US20230007184A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><heading id="h-0002" level="1">Field of the Disclosure</heading><p id="p-0002" num="0001">The present disclosure relates to a technique for detecting and displaying a line-of-sight position of an observer.</p><heading id="h-0003" level="1">Description of the Related Art</heading><p id="p-0003" num="0002">A technique for detecting a line-of-sight position of an observer has been known. Japanese Patent Application Laid-Open No. 2020-119093 discusses a technique for detecting a line-of-sight position of an observer looking into a display unit and displaying an indicator indicating the detected line-of-sight position of the observer on the display unit.</p><p id="p-0004" num="0003">According to Japanese Patent Application Laid-Open No. 2020-119093, because the detected line-of-sight position is visually recognizable, user convenience can be improved. In a case where information about the line-of-sight position of an observer is used as it is, a shift in the line-of-sight position that is not intended by the observer is reflected on the display, which may result in deterioration in display quality and convenience.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">Embodiments of the present disclosure are directed to providing a control apparatus capable of improving display quality and convenience in display of a line--of-sight position and a control method of the control apparatus.</p><p id="p-0006" num="0005">According to an aspect of the present disclosure, a control apparatus includes at least one processor or circuit configured to perform the operations of the following units: an acquisition unit configured to acquire information corresponding to a position at which an observer stares, and a control unit configured to control a display unit to display an indicator corresponding to the position, wherein, in a case where a shift in the position is continuously smaller than a first threshold a first number of times, the control unit does not shift a display position of the indicator,</p><p id="p-0007" num="0006">Further features of various embodiments will become apparent from the following description of exemplary embodiments with reference to the attached drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a configuration according to a first exemplary embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a correspondence relationship between pupil surfaces and photoelectric conversion units of pixels of an imaging apparatus according to a second exemplary embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a correspondence relationship between a pupil surface and an aperture of pixels of the imaging apparatus according to the second exemplary embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of a configuration of a line-of-sight input operation unit according to the first exemplary embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates another example of the configuration of the line-of-sight input operation unit according to the first exemplary embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a main flowchart illustrating a line-of-sight detection processing method of an electronic apparatus, line-of-sight position state determination, and a determination result according to the first exemplary embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating line-of-sight detection processing according to the first exemplary embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C</figref> illustrate line-of-sight state determination processing according to the first exemplary embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. <b>9</b>A to <b>9</b>C</figref> illustrate stationary display determination according to the first exemplary embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref> illustrate a line-of-sight confirmation operation according to the first exemplary embodiment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. <b>11</b>A to <b>11</b>C</figref> illustrate dynamical display determination according to the first exemplary embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart illustrating focus determination, line-of-sight determination, and an imaging operation according to the second exemplary embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart illustrating an imaging subroutine according to the second exemplary embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0021" num="0020">Suitable exemplary embodiments will be described in detail with reference to the accompanying drawings. A first exemplary embodiment will be described.</p><heading id="h-0007" level="2">[Configuration of Electronic Apparatus]</heading><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a configuration of an imaging apparatus including an electronic apparatus according to an exemplary embodiment of the present disclosure. In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a lens unit <b>150</b> includes an interchangeable shooting lens. A lens <b>103</b> is normally formed of a plurality of lenses. For simplicity, the lens <b>103</b> is illustrated as a single lens in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. A communication terminal <b>6</b> is a communication terminal for the lens unit <b>150</b> to communicate with a digital camera <b>100</b>, and a communication terminal <b>10</b> is a communication terminal for the digital camera <b>100</b> to communicate with the lens unit <b>150</b>. The lens unit <b>150</b> communicates with a system control unit <b>50</b> via these communication terminals <b>6</b> and <b>10</b>. The operation enables a lens system control circuit <b>4</b> included in the lens unit <b>150</b> to control an aperture <b>102</b> via an aperture drive circuit <b>2</b> and perform focusing by shifting the position of the lens <b>103</b> via an auto focus (AF) drive circuit <b>3</b>.</p><p id="p-0023" num="0022">A shutter <b>101</b> is a focal plane shutter that can freely control the exposure time of an imaging unit <b>22</b> under the control of the system control unit <b>50</b>. The imaging unit <b>22</b> is an imaging element including a charge-coupled device (CCD) sensor, a complementary metal-oxide semiconductor (CMOS) sensor, or the like, which converts an optical image into an electrical signal. An analog/digital (A/D) converter <b>23</b> converts an analog signal into a digital signal. The A/D converter <b>23</b> is used to convert an analog signal output by the imaging unit <b>22</b> into a digital signal. The signal obtained from the imaging unit <b>22</b> is used not only for imaging but also for exposure control and focus detection control. The imaging unit <b>22</b> is provided with pixels each having a divided photoelectric conversion unit with respect to a single micro lens. By dividing a photoelectric conversion unit, an entrance pupil is divided so that a phase difference detection signal can be obtained from each of the divided photoelectric conversion units. An imaging signal can also be obtained by adding up the signals obtained from the divided photoelectric conversion units.</p><p id="p-0024" num="0023">Use of such pixels is advantageous in that the pixels can serve as both focus detection pixels and imaging pixels.</p><p id="p-0025" num="0024">An image processing unit <b>24</b> performs predetermined pixel interpolation, resizing processing such as reduction, or color conversion processing on data output by the A/D converter <b>23</b> or data output by a memory control unit <b>15</b>. The image processing unit <b>24</b> performs predetermined computation processing by using the captured image data, and the system control unit <b>50</b> performs exposure control processing and ranging control processing based on the obtained computation result. The operation enables through-the-lens (TTL) AF processing, auto exposure (AE) processing, and flash preliminary emission (EF) processing. The image processing unit <b>24</b> also performs predetermined computation processing by using the captured image data and performs TTL auto white balance (AWB) processing based on the obtained computation result.</p><p id="p-0026" num="0025">The data output by the A/D converter <b>23</b> is directly written in the memory <b>32</b> via the image processing unit <b>24</b> and the memory control unit <b>15</b> or via the memory control unit <b>15</b>. The memory <b>32</b> stores image data obtained by the imaging unit <b>22</b> and converted into digital data by the A/D converter <b>23</b> and image data to be displayed on a display unit <b>28</b> serving as display means. The memory <b>32</b> has a sufficient storage capacity for storing a predetermined number of still images or a predetermined time of video or audio data.</p><p id="p-0027" num="0026">The memory <b>32</b> also serves as a memory for image display (a video memory). A digital/analog (D/A) converter <b>19</b> converts data for image display stored in the memory <b>32</b> into an analog signal and supplies the analog signal to the display unit <b>28</b>. In this way, the image data for display written in the memory <b>32</b> is displayed on the display unit <b>28</b> via the D/A converter <b>19</b>. The display unit <b>28</b> displays an image in accordance with the analog signal output by the D/A converter <b>19</b> on a display device such as a liquid crystal display (LCD). A digital signal obtained by A/D-conversion by the A/D converter <b>23</b> and accumulated in the memory <b>32</b> is D/A-converted by the D/A converter <b>19</b>. Next, the obtained analog signal is sequentially transferred to the display unit <b>28</b> to be displayed. In this way, the display unit <b>28</b> functions as an electronic viewfinder and performs through-the-lens image display (live view display). The display unit <b>28</b> may be provided with an electronic viewfinder having an eyepiece unit (not illustrated) into which the user looks or with a display on the rear side of the digital camera <b>100</b>. The display unit <b>28</b> may be provided with both the electronic viewfinder and the rear-side display.</p><p id="p-0028" num="0027">A non-volatile memory <b>56</b> is electrically erasable and recordable memory. For example, an electrically erasable programmable read-only memory (EEPROM) or the like is used. The non-volatile memory <b>56</b> stores constants for operations of the system control unit <b>50</b>, a program, etc. The term &#x201c;program&#x201d; used herein refers to a program for executing various flowcharts which will be described below in the present exemplary embodiment.</p><p id="p-0029" num="0028">The system control unit <b>50</b> controls the entire digital camera <b>100</b>. The system control unit <b>50</b> includes a line-of-sight display unit, a line-of-sight state determination unit, a line-of-sight information determination unit, a subject detection unit, and a line-of-sight acquisition possibility determination unit. The system control unit <b>50</b> executes the program recorded in the non-volatile memory <b>56</b> described above to realize individual processing according to the present exemplary embodiment, which will be described below. A random access memory (RAM) is used as a system memory <b>52</b>. Constants for operations of the system control unit <b>50</b>, variables, a program read from the non-volatile memory <b>56</b>, and the like are loaded into the system memory <b>52</b>. The system control unit <b>50</b> also performs display control processing by controlling the memory <b>32</b>, the D/A converter <b>19</b>, the display unit <b>28</b>, and the like. The display unit <b>28</b> includes an electronic viewfinder (EVF) into which the user looks, a thin-film transistor (TFT) liquid crystal display into which the user does not look but which enables a touch operation or the like. The system memory control unit <b>50</b> also performs display control processing by controlling the display unit <b>28</b> based on information obtained by an eye sensor of the EVF of the display unit <b>28</b>, the information indicating whether an eye of the user is close to or away from the EVF.</p><p id="p-0030" num="0029">A system timer <b>53</b> is a time measurement unit that measures time needed for various controls and time of a built-in clock,</p><p id="p-0031" num="0030">A power supply switch <b>72</b> is an operation member for switching ON/OFF of the power of the digital camera <b>100</b>.</p><p id="p-0032" num="0031">A mode selection switch <b>60</b>, a first shutter switch <b>62</b>, a second shutter switch <b>64</b>, and an operation unit <b>70</b> are operation members for inputting various kinds of operational instructions to the system control unit <b>50</b>.</p><p id="p-0033" num="0032">Aline-of-sight acquisition unit and a line-of-sight confirmation unit are included in the operation unit <b>70</b>.</p><p id="p-0034" num="0033">The mode selection switch <b>60</b> switches an operation mode of the system control unit <b>50</b> to any one of a still image recording mode, a video shooting mode, a reproduction mode, etc. The still image recording mode includes an auto shooting mode, an auto scene determination mode, a manual mode, an aperture priority mode (Av mode), and a shutter speed priority mode (Tv mode). There are also various kinds of scene modes, which are scene-specific shooting settings, a program AE mode, a custom mode, etc. The mode selection switch <b>60</b> directly switches the mode to any one of these modes included in menu buttons. Alternatively, after switching to the menu buttons by using the mode selection switch <b>60</b>, the mode may be switched to any one of these modes included in the menu buttons by using another operation member. Similarly, the video shooting mode may include a plurality of modes.</p><p id="p-0035" num="0034">The first shutter switch <b>62</b> is turned on when the photographer presses a shutter button <b>61</b> of the digital camera <b>100</b> halfway down, which is so-called half press (a capturing preparation instruction), and generates a first shutter switch signal SW<b>1</b>. The first shutter switch signal SW<b>1</b> starts an operation such as auto focus (AF) processing, auto exposure (AE) processing, auto white balance (AWVB) processing, and flash preliminary emission (EF) processing.</p><p id="p-0036" num="0035">The second shutter switch <b>64</b> is turned on when the photographer fully presses the shutter button <b>61</b>, which is so-called full press (a capturing instruction), and generates a second shutter switch signal SW<b>2</b>. When the second shutter switch signal SW<b>2</b> is turned on, the system control unit <b>50</b> starts a series of image capturing processing from reading of a signal from the imaging unit <b>22</b> to writing of image data in a recording medium <b>200</b>.</p><p id="p-0037" num="0036">Each operation member of the operation unit <b>70</b> is assigned a function as appropriate for an individual scene by selecting a function icon from various function icons displayed on the display unit <b>28</b> and serves as a corresponding one of various function buttons. Examples of the function buttons include an end button, a return button, an image forwarding button, a jump button, a narrow-down button, and an attribute change button. For example, when a menu button is pressed, a menu screen on which various settings can be made is displayed on the display unit <b>28</b>. The user can intuitively perform various kinds of settings by using the menu screen displayed on the display unit <b>28</b>, four-direction buttons of up, down, left, and right, and a SET button.</p><p id="p-0038" num="0037">The operation unit <b>70</b> includes various kinds of operation members serving as an input unit for receiving operations from the user.</p><p id="p-0039" num="0038">The operation unit <b>70</b> includes an electronic button, a cross key, and the like for performing menu selection, mode selection, and reproduction of a captured video, for example.</p><p id="p-0040" num="0039">A power supply control unit <b>80</b> includes a battery detection circuit, a DC-DC converter, a switch circuit for switching blocks to be energized, and the like, and detects attachment and removal of a battery, the kind of the battery, and the remaining amount of the battery. The power supply control unit <b>80</b> controls the DC-DC converter based on the detection result and an instruction from the system control unit <b>50</b> and supplies a necessary voltage for a necessary period to each unit including the recording medium <b>200</b>.</p><p id="p-0041" num="0040">A power supply unit <b>30</b> includes a primary battery such as an alkaline battery or a lithium battery, a secondary battery such as a nickel-cadmium (NiCd) battery, a nickel metal hydride (NiMH) battery, or a lithium ion (Li) battery, or an alternating current (AC) adapter. A recording medium interface (I/F) <b>18</b> is an interface with the recording medium <b>200</b> such as a memory card or a hard disk. The recording medium <b>200</b> is a recording medium such as a memory card for recording a captured image and includes a semiconductor memory, a magnetic disk, or the like.</p><p id="p-0042" num="0041">A communication unit <b>54</b> is connected wirelessly or via a wire cable and transmits and receives video signals and audio signals. The communication unit <b>54</b> can also be connected to a wireless local area network (LAN) or the Internet. The communication unit <b>54</b> can transmit an image (including a through-the-lens image) captured by the imaging unit <b>22</b> and an image recorded in the recording medium <b>200</b> and can receive image data and other various kinds of information from an external apparatus.</p><p id="p-0043" num="0042">An orientation detection unit <b>55</b> detects orientation of the digital camera <b>100</b> with respect to the direction of gravity. Based on the orientation detected by the orientation detection unit <b>55</b>, it is possible to determine whether the image captured by the imaging unit <b>22</b> is an image captured by the digital camera <b>100</b> horizontally held or an image captured by the digital camera <b>100</b> vertically held. The system control unit <b>50</b> can add direction information corresponding to the orientation detected by the orientation detection unit <b>55</b> to an image file of an image captured by the image capturing unit <b>22</b> or can rotate and record the image. An acceleration sensor, a gyro sensor, or the like can be used as the orientation detection unit <b>55</b>.</p><heading id="h-0008" level="2">[Configuration of Line-of-Sight Detection]</heading><p id="p-0044" num="0043">In the present exemplary embodiment, a line-of-sight input operation unit <b>701</b> is provided as one operation member of the operation unit <b>70</b>. The line-of-sight input operation unit <b>701</b> is the line-of-sight acquisition unit, which is an operation member for detecting the portion of the display unit <b>28</b> to which the line of sight of the user is directed. <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of the line-of-sight input operation unit <b>701</b>.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a configuration that realizes a method discussed in Japanese Patent Application Laid-Open No. 2020-119093. In this method, a rotation angle of an optical axis of an eyeball <b>501</b><i>a </i>of the user who looks into a finder visual field is detected, and the line of sight of the user is detected from the detected rotation angle. A live view display image captured through a lens unit <b>100</b> is displayed on the display unit <b>28</b>. <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an image sensor <b>701</b><i>a</i>, a light receiving lens <b>701</b><i>b</i>, a dichroic mirror <b>701</b> c, an eyepiece lens <b>701</b><i>d</i>, and an illumination light source <b>701</b><i>e</i>. The illumination light source <b>701</b><i>e </i>projects infrared light onto the eyeball <b>501</b><i>a</i>. The infrared light reflected by the eyeball <b>501</b><i>a </i>is reflected by the dichroic mirror <b>701</b><i>c </i>and is captured by the image sensor ,<b>01</b><i>a</i>. The captured eyeball image is converted into a digital signal by an A/D converter (not illustrated) and is transmitted to the system control unit <b>50</b>. The system control unit <b>50</b> serving as the line-of-sight acquisition unit extracts a pupil area or the like from the captured eyeball image and calculates the line of sight of the user,</p><p id="p-0046" num="0045">The line-of-sight input operation unit <b>701</b> is not limited to the above-described method, and a method in which both eyes of the user are captured to detect the line of sight may be used. <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example of the line-of-sight input operation unit <b>701</b> different from that illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0047" num="0046">In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a live view image captured through the lens unit <b>100</b> is displayed on the display unit <b>28</b> provided on the rear surface of the digital camera <b>100</b>. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a camera <b>701</b><i>f </i>that captures a face <b>500</b> of the user who is observing the display unit <b>28</b> is provided on the rear surface of the camera <b>100</b>. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, an angle of a view captured by the camera <b>701</b><i>f </i>is indicated by a dotted line. An illumination light source <b>701</b><i>e </i>(not illustrated) projects light onto the face of the user, and an eyeball image is captured by the camera <b>701</b><i>f</i>. In this way, the line of sight of the user is calculated. The line-of-sight input operation unit <b>701</b> is not limited to the method, and any configuration that can detect the portion of the display unit <b>28</b> at which the user stares may be used.</p><heading id="h-0009" level="2">[Line-of-Sight Detection Processing]</heading><p id="p-0048" num="0047">A line-of-sight detection processing method and a display method based on line-of-sight position state determination and a result of the line-of-sight position state determination according to the first exemplary embodiment will be described with reference to <figref idref="DRAWINGS">FIGS. <b>6</b>, <b>7</b>, and <b>8</b>A to <b>8</b>C</figref>.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a main flowchart illustrating the line-of-sight detection processing method of the electronic apparatus, the line-of-sight position state determination, and the determination result according to the first exemplary embodiment. <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an operation in a case where the user performs a line-of-sight confirmation operation while viewing an image displayed on the display unit <b>28</b> serving as display means. The operation is realized mainly by the system control unit <b>50</b>. While the present exemplary embodiment will be described with reference to the block diagram of the imaging apparatus in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a similar operation can be performed by an electronic apparatus including at least the system control unit <b>50</b>, the display unit <b>28</b>, and the operation unit <b>70</b>.</p><p id="p-0050" num="0049">In step S<b>1</b>, the system control unit <b>50</b> displays image data on the display unit <b>28</b>. The image for display is, for example, a reduced image that matches the resolution of the display unit <b>28</b>. The system control unit <b>50</b> displays the image at a predetermined frame rate, and the user can check the displayed image through the display unit <b>28</b>.</p><p id="p-0051" num="0050">In step S<b>2</b>, the system control unit <b>50</b> causes the line-of-sight acquisition possibility determination unit to determine whether line-of-sight information can be acquired by the line-of-sight acquisition possibility determination unit. In a case where the EVF included in the display unit <b>28</b> has an eye sensor, the system control unit <b>50</b> determines whether the line-of-sight information can be acquired based on information indicating whether the user's eye is close to the EVF or away from the EVE. When the user looks into the EVF, the eve sensor detects that the user is in an eye-contact state, and the system control unit <b>50</b> determines that the line-of-sight input operation unit <b>701</b> serving as line-of-sight acquisition means can acquire line-of-sight information. In contrast, when the user does not look into the EVF, the eye sensor detects that the user is not in an eye-contact state, and the system control unit <b>50</b> determines that the line--of-sight input operation unit <b>701</b> serving as the line-of-sight acquisition cannot acquire line-of-sight information. Also, with a TFT liquid crystal display included in the display unit <b>28</b>, the system control unit <b>50</b> determines whether line-of-sight information can be acquired by the line-of-sight input operation unit <b>701</b>.</p><p id="p-0052" num="0051">In step S<b>2</b>, in a case where the system control unit <b>50</b> determines that the line-of-sight information can be acquired, the processing proceeds to step S<b>3</b> In step S<b>3</b>, the system control unit <b>50</b> starts line-of-sight detection. From step S<b>4</b>, the line-of-sight input operation unit <b>701</b> acquires and accumulates information indicating which position (line-of-sight position) on the display unit <b>28</b> is observed by the user, at predetermined time intervals in association with the display image observed by the user. In step S<b>2</b>, in a case where the system control unit <b>50</b> determines that the line-of-sight information cannot be acquired, the line-of-sight input operation unit <b>701</b> does not acquire and accumulate the line-of-sight information at the timing of the determination.</p><p id="p-0053" num="0052">In step S<b>4</b>, the system control unit <b>50</b> determines whether the number of line-of-sight information acquisition and accumulation operations performed in step S<b>3</b> is equal to or more than a predetermined number N. The predetermined number N is determined based on the number of line-of-sight information items acquired and accumulated to enable line-of-sight state determination performed in the subsequent step S<b>5</b>.</p><p id="p-0054" num="0053">In step S<b>2</b>, in a case where the system control unit <b>50</b> determines that the line-of-sight information cannot be acquired, because the line-of-sight information is not acquired or accumulated, the line-of-sight accumulation number is not incremented and compared with the predetermined number N. In a case where the system control unit <b>50</b> determines that the number of line-of-sight information items acquired and accumulated in step S<b>3</b> is equal to or more than the predetermined number N (YES in step S<b>4</b>), the line-of-sight state determination in step S<b>5</b>, which is performed by the line-of-sight state determination unit, is possible. Thus, the processing proceeds to step S<b>5</b>. In a case where the system control unit <b>50</b> determines that the number of line-of-sight information items acquired and accumulated in step S<b>3</b> is less than the predetermined number N (NO in step S<b>4</b>), the line-of-sight state determination in step S<b>5</b>, which is performed by the line-of-sight state determination unit, is impossible. Thus, the processing proceeds to step S<b>6</b>, skipping step S<b>5</b>.</p><p id="p-0055" num="0054">In step S<b>5</b>, the system control unit <b>50</b> performs the line-of-sight state determination, which is performed by the line-of-sight state determination imit. The processing will be described in detail below.</p><p id="p-0056" num="0055">In step S<b>6</b>, by using the line-of-sight position information acquired in step S<b>3</b> or S<b>5</b>, the system control unit <b>50</b> causes the display unit <b>28</b> to display the line-of-sight position. In step S<b>4</b>, in a case where the system control unit <b>50</b> determines that the line-of-sight accumulation number is equal to or greater than the predetermined number N (YES in step S<b>4</b>), the line-of-sight position acquired in step S<b>5</b> is used. In step S<b>4</b>, in a case where the system control unit <b>50</b> determines that the line-of-sight accumulation number is smaller than the predetermined number N (NO in step S<b>4</b>), the line-of-sight position acquired in step S<b>3</b> is used.</p><p id="p-0057" num="0056">In step S<b>7</b>, the system control unit <b>50</b> determines whether the line-of-sight confirmation operation has been performed by the user. In a case where the line-of-sight confirmation operation has been performed by the user (YES in step S<b>7</b>), the system control unit <b>50</b> stationarily displays the line-of-sight position on the display unit <b>28</b> and ends the line-of-sight position state determination and the display processing. The display of the line-of-sight position may be maintained on the display unit <b>28</b>, The display method may be changed. In a case where the line-of-sight confirmation operation has not been performed by the user (NO in step S<b>7</b>), the processing returns to step S<b>2</b>, in which the line-of-sight information acquisition possibility is determined by the line-of-sight acquisition possibility determination unit, and new line-of-sight information is acquired. In step S<b>7</b>, in a case where the line-of-sight confirmation operation has been performed (YES in step S<b>7</b>), the system control unit <b>50</b> resets the number of line-of-sight information acquisition and accumulation operations performed in step S<b>3</b> and line-of-sight position shift information &#x394;S<sub>j</sub>.</p><heading id="h-0010" level="2">[Line-of-Sight Position State Determination]</heading><p id="p-0058" num="0057">The line-of-sight state determination in step S<b>5</b> will be described with reference to a sub-flowchart in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. A series of operations in the sub-flowchart is also realized mainly by the system control unit <b>50</b>.</p><p id="p-0059" num="0058">In step S<b>501</b>, the system control unit <b>50</b> acquires line-of-sight position shift information &#x394;S<sub>j </sub>by using line-of-sight position information S<sub>j </sub>acquired in step S<b>3</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The line-of-sight position information SI has been determined to be equal to or more than the predetermined number N in step S<b>4</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The line-of-sight position shift information &#x394;S<sub>j </sub>includes a horizontal position shift amount &#x394;S<sub>jH</sub>, a vertical position shift amount &#x394;S<sub>jV</sub>, and a position shift amount &#x394;S<sub>jr </sub>between two points, which are calculated by the following equations (1), (2), and (3), respectively.</p><p id="p-0060" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i><sub>i</sub><i>={x</i><sub>i</sub><i>,y</i><sub>i</sub>}<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0061" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x394;<i>S</i><sub>jH</sub><i>=x</i><sub>j+1</sub><i>&#x2212;x</i><sub>j </sub>(<i>j=</i>1to<i>N&#x2212;</i>1)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0062" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x394;<i>S</i><sub>jV</sub><i>=y</i><sub>j+1</sub><i>&#x2212;y</i><sub>j </sub>(<i>j</i>=1to <i>N&#x2212;</i>1)&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0063" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x394;<i>S</i><sub>jr</sub>=&#x221a;{square root over ((<i>x</i><sub>j+1</sub><i>&#x2212;x</i><sub>j</sub>)<sup>2</sup>+(<i>y</i><sub>j+1</sub><i>&#x2212;y</i><sub>j</sub>)<sup>2</sup>)}(<i>j=</i>1&#x2dc;<i>N&#x2212;</i>1)&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0064" num="0059">In step S<b>502</b>, the system control unit <b>50</b> determines the line-of-sight position shift information &#x394;S<sub>j </sub>acquired in step S<b>501</b> by using a first determination number N<sub>1 </sub>and a first threshold Th<sub>1</sub>. The predetermined number N and the determinations number JN<sub>1 </sub>have the following relationship (<b>4</b>).</p><p id="p-0065" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>N&#x2265;JN</i><sub>1</sub>&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0066" num="0060">In the present exemplary embodiment, JN<sub>1 </sub>will be described as a fixed value of 5. The value of JN<sub>1 </sub>may be changed in accordance with conditions. In step S<b>502</b>, in a case where the system control unit <b>50</b> determines that the line-of-sight position shift information &#x394;S<sub>j </sub>corresponding to the first determination number JN<sub>1 </sub>acquired in step S<b>501</b> has been smaller than the first threshold Th<sub>1 </sub>since a determination time (YES in step S<b>502</b>), the processing proceeds to step S<b>505</b>. In step S<b>505</b>, a position at which the line-of-sight position is fixed to be stationarily displayed (maintain a stationary state) on the display unit <b>28</b> is calculated. In a case where the system control unit <b>50</b> determines that any of the line-of-sight position shift information &#x394;S<sub>j </sub>corresponding to the first determination number JN<sub>1 </sub>acquired in step S<b>501</b> has reached the first threshold Th<sub>1 </sub>or more since the determination time (NO in step S<b>502</b>), the processing proceeds to step S<b>503</b>. In step S<b>503</b>, the line-of-sight position shift information &#x394;S<sub>j </sub>is determined based on a second threshold.</p><p id="p-0067" num="0061">In step S<b>503</b>, the system control unit <b>50</b> determines the line-of-sight position shift information &#x394;S<sub>j </sub>acquired in step S<b>501</b> by using a second determination number JN<sub>2 </sub>and a second threshold Th<sub>2</sub>. The predetermined number N and the determination number JN<sub>2 </sub>have the following relationship (<b>5</b>).</p><p id="p-0068" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>N&#x2265;JN</i><sub>2</sub>&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0069" num="0062">In the present exemplary embodiment, JN<sub>2 </sub>will be described as a fixed value of 5. The value of JN<sub>2 </sub>may be changed in accordance with conditions. In step S<b>503</b>, in a case where the system control unit <b>50</b> determines that the line-of-sight position shift information &#x394;S<sub>j </sub>corresponding to the second determination number JN<sub>2 </sub>acquired in step S<b>501</b> has been greater than the second threshold Th<sub>2 </sub>since a determination time (YES in step S<b>503</b>), the processing proceeds to step S<b>504</b>. In step S<b>504</b>, a position at which the line-of-sight position starts moving to be dynamically (movably) displayed on the display unit <b>28</b> is calculated. In a case where the system control unit <b>50</b> determines that any of the line-of-sight position shift information &#x394;S<sub>j </sub>corresponding to the second determination number JN<sub>2 </sub>acquired in step S<b>501</b> has reached the second threshold Th<sub>z </sub>or less since the determination time (NO in step S<b>503</b>), the same display as the previous display is maintained. Thus, the processing proceeds to step S<b>506</b>, and the line-of-sight display position to be displayed on the display unit <b>28</b> is determined by using the line-of-sight position information S<sub>i </sub>acquired in step S<b>3</b>.</p><p id="p-0070" num="0063">Step S<b>504</b> is performed in a case where the system control unit <b>50</b> determines that the line-of-sight position shift information &#x394;S<sub>j </sub>is greater than the second threshold Th, in step S<b>503</b>. In step S<b>504</b>, the position to start dynamical display of the line-of-sight is calculated, and the processing proceeds to step S<b>506</b>,</p><p id="p-0071" num="0064">Step S<b>505</b> is performed in a case where the system control unit <b>50</b> determines that the line-of-sight position shift information &#x394;S<sub>j </sub>is smaller than the first threshold Th<sub>1 </sub>in step S<b>502</b>. In step S<b>505</b>, the position to start stationary display of the line-of-sight is calculated, and the processing proceeds to step S<b>506</b>. When the series of processes is completed, the processing returns to step S<b>6</b> in the main flowchart in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0072" num="0065">In step S<b>506</b>, the line-of-sight display position is determined.</p><heading id="h-0011" level="2">[Determination on Whether to Fix Display Position and Calculation of Display Position]</heading><p id="p-0073" num="0066">The determination method for stationarily displaying the line-of-sight position performed in step S<b>502</b> and the calculation of a fixed position of the display will be described with reference to <figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C and <b>9</b>A to <b>9</b>C</figref>. <figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C</figref> illustrate an example in which a captured video or a video being captured is displayed on the display unit <b>28</b> and the user performs a line-of-sight confirmation operation on a specific subject with his or her line of sight. A stationary display method used for the line-of-sight information display by using the first determination number JN<sub>1 </sub>and the first threshold Th<sub>1 </sub>will be described with reference to <figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C</figref>. <figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C</figref> illustrate excerpts of images displayed on the display unit <b>28</b> at the moments of time t<sub>1</sub>(<figref idref="DRAWINGS">FIG. <b>8</b>A</figref>), time t<sub>2 </sub>(<figref idref="DRAWINGS">FIG. <b>8</b>B</figref>), and time t<sub>3 </sub>(<figref idref="DRAWINGS">FIG. <b>8</b>C</figref>) from the left. <figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C</figref> illustrate line-of-sight information display Sm and a subject Trg that the line of sight of the user is following.</p><p id="p-0074" num="0067"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> illustrates the moment when the subject Trg riding a bicycle has moved from the left to right on the screen. The user recognizes the subject Trg and starts to focus his or her line of sight on the subject Trg. At this moment, the line-of-sight information display Sm does not match with the subject Trg, and the line-of-sight information display Sm varies as indicated by broken lines.</p><p id="p-0075" num="0068"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates the moment when the subject Trg slows down the bicycle to stop. <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates the subject Trg and the line-of-sight information display Sm at the time t<sub>2 </sub>after a lapse of time from the time ti in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, At this moment, the line-of-sight information display Sm starts to approximately match the subject Trg. The line-of-sight information display Sm varies as indicated by broken lines.</p><p id="p-0076" num="0069"><figref idref="DRAWINGS">FIG. <b>8</b>C</figref> illustrates the moment when the subject Trg has stopped the bicycle and stands still. <figref idref="DRAWINGS">FIG. <b>8</b>C</figref> illustrates the subject Trg and the line-of-sight information display Sm at the time t<sub>s </sub>after a lapse of time from the time t<sub>2 </sub>in <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>. At this moment, the line-of-sight information display Sm matches the subject Trg, and the line-of-sight information display Sm is stationarily displayed by using the first determination number JN<sub>1 </sub>and the first threshold Th<sub>1</sub>. The processing illustrated in <figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C</figref> will be described by using line-of-sight position shift information &#x394;S<sub>j </sub>, the first determination number JN<sub>1</sub>, and the first threshold Th<sub>1 </sub>in <figref idref="DRAWINGS">FIGS. <b>9</b>A to <b>9</b>C</figref>. <figref idref="DRAWINGS">FIGS. <b>9</b>A to <b>9</b>C</figref> illustrate stationary display determination using the first determination number JN<sub>1 </sub>and the first threshold Th. In <figref idref="DRAWINGS">FIGS. <b>9</b>A to <b>9</b>C</figref>, the first determination number JN<sub>1 </sub>is set to 5. The first determination number JN<sub>1 </sub>is not limited to the number <b>5</b> and may be changed in accordance with the line-of-sight characteristics of the user, the viewing state of the display unit <b>28</b>, or a manual operation by the user.</p><p id="p-0077" num="0070"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> illustrates a horizontal position shift amount &#x394;Sj<sub>H </sub>included in the line-of-sight position shift information &#x394;S<sub>j </sub>at time t. Two solid lines drawn horizontally represent a threshold Th<sub>1H </sub>for the horizontal position shift amount included in the first threshold Th<sub>1</sub>. A gray-hatched portion represents a determination number JN<sub>1H </sub>for the horizontal position shift amount included in the first determination number JN<sub>1</sub>.</p><p id="p-0078" num="0071"><figref idref="DRAWINGS">FIG. <b>9</b>B</figref> illustrates a vertical position shift amount &#x394;S<sub>jV </sub>included in the line-of-sight position shift information &#x394;S<sub>j </sub>at time t. Two solid lines drawn horizontally represent a threshold Th<sub>1V </sub>for the vertical position shift amount included in the first threshold Th<sub>1</sub>. A gray-hatched portion represents a determination number JN<sub>1V </sub>for the vertical position shift amount included in the first determination number JN<sub>1</sub>.</p><p id="p-0079" num="0072"><figref idref="DRAWINGS">FIG. <b>9</b>C</figref> illustrates a position shift amount &#x394;S<sub>jr </sub>between two points included in the line-of-sight position shift information &#x394;S<sub>j </sub>at time t. One solid line drawn horizontally represents a threshold Th<sub>1r </sub>for the position shift amount between two points included in the first threshold Th<sub>1</sub>. A gray-hatched portion represents a determination number JN<sub>1r </sub>for the position shift amount between two points included in the first determination number JN<sub>1</sub>.</p><p id="p-0080" num="0073"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> illustrates a case where the stationary display determination of the line-of-sight information display is performed based on the line-of-sight position shift information &#x394;S<sub>jH </sub>corresponding to the horizontal position shift amount. The line-of-sight position shift information &#x394;S<sub>jH </sub>corresponding to the horizontal position shift amount from the time ti to the time t gradually decreases but still exceeds the threshold Th<sub>1H </sub>for the horizontal position shift amount. Thereafter, the line-of-sight position shift information &#x394;S<sub>jH </sub>corresponding to the horizontal position shift amount from the time t<sub>2 </sub>to the time t<sub>3 </sub>transitions within the threshold Th<sub>1H </sub>for the horizontal position shift amount and converges with time, At this point, because the line-of-sight position shift information &#x394;S<sub>jH </sub>corresponding to the horizontal position shift amount is within the threshold Th<sub>1H </sub>for the horizontal position shift amount after the time t<sub>2</sub>, a count-up operation is performed. In the present exemplary embodiment, because the determination number JN<sub>1H </sub>for the horizontal position shift amount is set to 5, the line-of-sight information display on the display unit <b>28</b> is changed to be stationary as a fixed value from the time t<sub>3</sub>, which is the fifth determination time. An average value of the horizontal position shift amounts corresponding to the determination number JN<sub>1H </sub>is used as the fixed value of the position of the displayed line-of-sight information. Alternatively, the horizontal position shift amount corresponding to the last determination number in the determination number JN<sub>1H </sub>may be used as the fixed value.</p><p id="p-0081" num="0074"><figref idref="DRAWINGS">FIG. <b>9</b>B</figref> illustrates a case where the stationary display determination of the line-of-sight information display is performed based on the line-of-sight position shift information &#x394;S<sub>jV </sub>corresponding to the vertical position shift amount, The line-of-sight position shift information &#x394;S<sub>j </sub>v corresponding to the vertical position shift amount from the time t<sub>1 </sub>to the time t<sub>2 </sub>gradually decreases but is still lower than the threshold Th<sub>1V </sub>for the vertical position shift amount. Thereafter, the line-of-sight position shift information &#x394;S<sub>jV </sub>corresponding to the vertical position shift amount from the time t<sub>2 </sub>to the time t<sub>3 </sub>transitions within the threshold Th<sub>1V </sub>for the vertical position shift amount and converges with time. At this point, because the line-of-sight position shift information &#x394;S<sub>jV </sub>corresponding to the vertical position shift amount is within the threshold Th<sub>1V </sub>for the vertical position shift amount after the time t<sub>2</sub>, a count-up operation is performed. In the present exemplary embodiment, because the determination number JN<sub>1V </sub>for the vertical position shift amount is set to 5, the line-of-sight information display on the display unit <b>28</b> is changed to be stationary as a fixed value from the time t<sub>3</sub>, which is the fifth determination time. An average value of the vertical position shift amounts corresponding to the determination number JN<sub>1V </sub>is used as the fixed value of the position of the displayed line-of-sight information. Alternatively, the vertical position shift amount corresponding to the last determination number in the determination number JN<sub>1V </sub>may be used as the fixed value.</p><p id="p-0082" num="0075"><figref idref="DRAWINGS">FIG. <b>9</b>C</figref> illustrates a case where the stationary display determination of the line-of-sight information display is performed based on the line-of-sight position shift information &#x394;S<sub>jr </sub>corresponding to the position shift amount between two points. Unlike &#x394;S<sub>jH </sub>and &#x394;S<sub>jV </sub>described above, &#x394;S<sub>jr </sub>takes a positive value. Thus, only a positive value is set as the threshold Th<sub>1r </sub>for the position shift amount between two points. The line-of-sight position shift information &#x394;S<sub>j </sub>r corresponding to the position shift amount between two points from the time t<sub>1 </sub>to the time t<sub>2 </sub>gradually decreases but still exceeds the threshold Th<sub>1r </sub>for the position shift amount between two points. Thereafter, the line-of-sight position shift information &#x394;S<sub>jr </sub>corresponding to the position shift amount between two points from the time t<sub>2 </sub>to the time t<sub>b </sub>transitions within the threshold Th<sub>1r </sub>for the position shift amount between two points and converges with time. At this point, because the line-of-sight position shift information &#x394;S<sub>jr </sub>corresponding to the position shift amount between two points is within the threshold Th<sub>1r </sub>for the position shift amount between two points after the time t<sub>2</sub>, a count-up operation is performed. In the present exemplary embodiment, because the determination number JN<sub>1r </sub>for the position shift amount between two points is set to 5, the line-of-sight information display on the display unit <b>28</b> is changed to be stationary as a fixed value from the time t<sub>3</sub>, which is the fifth determination time. An average value of the position shift amounts between two points corresponding to the determination number JN<sub>1r </sub>is used as the fixed value of the position of the displayed line-of-sight information. Alternatively, the position shift amount between two points corresponding to the last determination number in the determination number JN<sub>1r </sub>may be used as the fixed value.</p><p id="p-0083" num="0076">As described above, by stationarily displaying the line-of-sight information display in accordance with a change in the line-of-sight information, it is possible to provide an electronic apparatus that improves display quality and convenience.</p><heading id="h-0012" level="2">[Determination of Whether to Shift Display Position and Calculation of Display Position]</heading><p id="p-0084" num="0077">The determination method for stationarily displaying the line-of-sight position performed in step S<b>503</b> and the calculation of a fixed position of the display will be described with reference to <figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C and <b>11</b>A to <b>11</b>C</figref>. <figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref> illustrate an example in which a captured video or a video being captured is displayed on the display unit <b>28</b> and the user performs a line-of-sight confirmation operation on a specific subject with his or her line of sight. A stationary display method performed on the line-of-sight information display by using the second determination number JN<sub>2 </sub>and the second threshold Th<sub>2 </sub>will be described with reference to <figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref>. <figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref> illustrate excerpts of images displayed on the display unit <b>28</b> at the moments of time t<sub>1</sub>&#x2032; (<figref idref="DRAWINGS">FIG. <b>10</b>A</figref>), time t<sub>2</sub>&#x2032; (<figref idref="DRAWINGS">FIG. <b>10</b>B</figref>), and time t<sub>3</sub>&#x2032; (<figref idref="DRAWINGS">FIG. <b>10</b>C</figref>) from the left. As in <figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C</figref>, <figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref> illustrates the line-of-sight information display Sm and a subject Trg&#x2032; that the user is following with the line of sight. The subject is different from that in FIGS. SA to <b>8</b>C.</p><p id="p-0085" num="0078"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> illustrates the moment when the automobile Trg&#x2032;, which is the subject, is about to move from the left to right on the screen. The user continuously recognizes the automobile Trg&#x2032;, and the line of sight of the user matches the automobile Trg&#x2032;. At this moment, the line-of-sight information display Sm matches the automobile Trg&#x2032;, and the line-of-sight information display Sm is stationary by the processing in step S<b>502</b> described above. The line-of-sight information display Sm may be moving before the processing in step S<b>502</b> is performed.</p><p id="p-0086" num="0079"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrates the moment when the subject Trg&#x2032; starts moving and increases the speed. <figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrates the subject Trg&#x2032; and the line-of-sight information display Sm at the time t<sub>2</sub>&#x2032; after a lapse of time from the time t<sub>1 </sub>&#x2032; in <figref idref="DRAWINGS">FIG. <b>10</b>A</figref>. At this moment, the line-of-sight information display Sm starts to deviate from the subject Trg&#x2032;. The line-of-sight information display Sm is, however, stationary by the processing in step S<b>502</b> described above. The line-of-sight information display Sm may be moving before the processing in step S<b>502</b> is performed.</p><p id="p-0087" num="0080"><figref idref="DRAWINGS">FIG. <b>10</b>C</figref> illustrates the moment when the automobile, which is the subject Trg&#x2032;, is stopped. <figref idref="DRAWINGS">FIG. <b>10</b>C</figref> illustrates the subject Trg&#x2032; and the line-of-sight information display Sm at the time t<sub>3</sub>&#x2032; after a lapse of time from the time t<sub>2</sub>&#x2032; in <figref idref="DRAWINGS">FIG. <b>10</b>B</figref>. At this moment, the line-of-sight information display Sm does not match the subject Trg&#x2032; and is dynamically displayed by using the second determination number JN<sub>2 </sub>and the second threshold Th<sub>2</sub>. Thus, the line-of-sight information display Sm is displayed in a varying manner as indicated by broken lines.</p><p id="p-0088" num="0081">The sequence of the operations illustrated in <figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref> will be described by using line-of-sight position shift information &#x394;S<sub>j </sub>, the second determination number JN<sub>2</sub>, and the second threshold Th<sub>2 </sub>in <figref idref="DRAWINGS">FIGS. <b>11</b>A to <b>11</b>C</figref>. <figref idref="DRAWINGS">FIGS. <b>11</b>A to <b>11</b>C</figref> illustrate stationary display determination using the second determination number JN<sub>2 </sub>and the second threshold Th<sub>2 </sub>according to the present exemplary embodiment. In <figref idref="DRAWINGS">FIGS. <b>11</b>A to <b>11</b>C</figref>, the second determination number JN<sub>2 </sub>is set to 5. The second determination number JN<sub>2 </sub>is not limited to the number and may be changed in accordance with the line-of-sight characteristics of the user, the viewing state of the display unit <b>28</b>, or a manual operation by the user.</p><p id="p-0089" num="0082"><figref idref="DRAWINGS">FIG. <b>11</b>A</figref> illustrates a horizontal position shift amount &#x394;S<sub>jH </sub>included in the line-of-sight position shift information &#x394;S<sub>j </sub>at time t&#x2032;. Two solid lines drawn horizontally represent a threshold Th<sub>2H </sub>for the horizontal position shift amount included in the second threshold Th<sub>2</sub>. A gray-hatched portion represents a determination number JN<sub>2H </sub>for the horizontal position shift amount included in the second determination number JN<sub>2</sub>,</p><p id="p-0090" num="0083"><figref idref="DRAWINGS">FIG. <b>11</b>B</figref> illustrates a vertical position shift amount &#x394;S<sub>jV </sub>included in the line-of-sight position shift information &#x394;S<sub>j </sub>at the time t&#x2032;. Two solid lines drawn horizontally represent a threshold Th<sub>2 </sub>for the vertical position shift amount included in the second threshold Th<sub>2</sub>. A gray-hatched portion represents a determination number JN<sub>2v </sub>for the vertical position shift amount included in the second determination number JN<sub>2</sub>.</p><p id="p-0091" num="0084"><figref idref="DRAWINGS">FIG. <b>9</b>C</figref> illustrates a position shift amount &#x394;S<sub>jr </sub>between two points included in the line-of-sight position shift information &#x394;S<sub>j</sub>at the time t&#x2032;. One solid line drawn horizontally represents a threshold Th<sub>a </sub>for the position shift amount between two points included in the second threshold Th<sub>2</sub>. A gray-hatched portion represents a determination number JN<sub>2 </sub>for the position shift amount between two points included in the second determination number JN<sub>2</sub>.</p><p id="p-0092" num="0085"><figref idref="DRAWINGS">FIG. <b>11</b></figref> A illustrates a case where the dynamical display determination of the line-of-sight information display is performed based on the line-of-sight position shift information &#x394;S<sub>jH </sub>corresponding to the horizontal position shift amount. The line-of-sight position shift information &#x394;S<sub>jH </sub>corresponding to the horizontal position shift amount from the time t<sub>1</sub>&#x2032; to the time t<sub>2</sub>&#x2032; gradually increases and is about to exceed the threshold Th<sub>1H </sub>for the horizontal position shift amount. Thereafter, the line-of-sight position shift information &#x394;S<sub>j </sub>corresponding to the horizontal position shift amount from the time ta&#x2032; to the time t<b>3</b>&#x2032; transitions over the threshold Than for the horizontal position shift amount and shifts along with the subject Trg&#x2032; with time. At this point, because the line-of-sight position shift information &#x394;S<sub>jH </sub>corresponding to the horizontal position shift amount is over the threshold Th<sub>2H </sub>for the horizontal position shift amount after the time t<sub>2</sub>&#x2032;, a count-up operation is performed. In the present exemplary embodiment, because the determination number JN<sub>2H </sub>for the horizontal position shift amount is set to 5, the line-of-sight information display on the display unit <b>28</b> is changed to be dynamical from the time t<b>3</b>&#x2032;, which is the fifth determination time. An average value of the horizontal position shift amounts corresponding to the determination number JN<sub>2H </sub>is used as the value for displaying the position of the line-of-sight information. Alternatively, the horizontal position shift amount corresponding to the last determination number in the determination number JN<sub>2H </sub>maybe used as the fixed value. Even if the transition is within the threshold Th<sub>21H </sub>for the horizontal position shift amount, when the line-of-sight position shift information &#x394;S<sub>jH </sub>corresponding to the horizontal position shift amount constantly shifts in the same direction successively for the determination number JN<sub>2H </sub>for the horizontal position shift amount, the following operation may be performed. That is, the line-of-sight information display may be changed to be dynamically displayed.</p><p id="p-0093" num="0086"><figref idref="DRAWINGS">FIG. <b>11</b>B</figref> illustrates a case where the dynamical display determination of the line-of-sight information display is performed based on the line-of-sight position shift information &#x394;S<sub>jV </sub>corresponding to the vertical position shift amount. The line-of-sight position shift information &#x394;S<sub>jV </sub>corresponding to the vertical position shift amount from the time t to the time t<sub>2</sub>&#x2032; has approximately constant values and is constantly lower than the threshold Th<sub>2V </sub>for the vertical position shift amount. At this point, because the line-of-sight position shift information &#x394;S<sub>j </sub>v corresponding to the vertical position shift amount is within the threshold Th<sub>2V </sub>for the vertical position shift amount after the time t<sub>2</sub>, a count-up operation is not performed. Therefore, based on the line-of-sight position shift information &#x394;S<sub>j </sub>v corresponding to the vertical position shift amount, the line-of-sight information display on the display unit <b>28</b> is determined to maintain its stationary display as a fixed value even at the time t<sub>3</sub>&#x2032;.</p><p id="p-0094" num="0087"><figref idref="DRAWINGS">FIG. <b>1</b><i>l </i></figref>C illustrates a case where the dynamical display determination of the line-of-sight information display is performed based on the line-of-sight position shift information &#x394;S<sub>jr </sub>corresponding to the position shift amount between two points. Unlike &#x394;<sub>jH </sub>and &#x394;S<sub>jV </sub>described above, &#x394;S<sub>jr </sub>takes a positive value. Thus, only a positive value is set as the threshold Th<sub>2r </sub>for the position shift amount between two points. The line-of-sight position shift information &#x394;S<sub>jr </sub>corresponding to the position shift amount between two points from the time t<sub>1</sub>&#x2032; to the time t<sub>2</sub>&#x2032; gradually increases but is still lower than the threshold Th<sub>2</sub>r for the position shift amount between two points. Thereafter, the line-of-sight position shift information &#x394;S<sub>jr </sub>corresponding to the position shift amount between two points from the time t<sub>2</sub>&#x2032; to the time t transitions over the threshold Th<sub>2 </sub>for the position shift amount between two points and changes with time. At this point, because the line-of-sight position shift information &#x394;S<sub>j </sub>r corresponding to the position shift amount between two points is more than the threshold Th<sub>2</sub>- for the position shift amount between two points after the time t<sub>2</sub>&#x2032;, a count-up operation is performed. In the present exemplary embodiment, because the determination number JN<sub>2</sub>r for the position shift amount between two points is set to 5, the line-of-sight information display on the display unit <b>28</b> is changed to be dynamical as a fixed value from the time t<sub>3</sub>&#x2032;, which is the fifth determination time. An average value of the position shift amounts between two points corresponding to the determination number JN<sub>2r </sub>is used as the fixed value of the position of the displayed line-of-sight information. Alternatively, the position shift amount between two points corresponding to the last determination number in the determination number JN<sub>2r </sub>may be used as the fixed value.</p><p id="p-0095" num="0088">As described above, by dynamically displaying the line-of-sight information display in accordance with a change in the line-of-sight information, it is possible to provide an electronic apparatus that improves display quality and convenience.</p><p id="p-0096" num="0089">While the determination using the line--of-sight position shift amount &#x394;S<sub>j </sub>has been described with reference to <figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C and <b>11</b>A to <b>11</b>C</figref>, stationary or dynamical display determination may be further performed based on whether the line-of-sight position shifts in a direction within a predetermined range. For example, a range of &#xb1;45 degrees can be selected as the predetermined range. In this way, it is possible to determine a state in which the line of sight varies and shifts.</p><p id="p-0097" num="0090">As described above, by stationarily or dynamically displaying the line-of-sight position based on the line-of-sight position shift amount, it is possible to provide an electronic apparatus that improves display quality and convenience. In the present exemplary embodiment, the method for performing the stationary or dynamical display determination based on the line-of-sight position shift amount has been described. The determination, however, may be performed by using a change angle range of the line-of-sight position information, vector information, or the like. The determination may be performed by using not only the line-of-sight position information but also information such as gyro information of the electronic apparatus.</p><p id="p-0098" num="0091">In the present exemplary embodiment, the same value is used as the first threshold Th<sub>1 </sub>for the horizontal position, the vertical position, and the position between two points. The first threshold Th<sub>1</sub>, however, may be changed to a different value. Alternatively, different values may be set for different positions.</p><p id="p-0099" num="0092">A second exemplary embodiment will be described with reference to <figref idref="DRAWINGS">FIGS. <b>12</b> and <b>13</b></figref>. In the present exemplary embodiment, a line-of-sight position display method combined with a focus detection apparatus will be described.</p><heading id="h-0013" level="2">[Description of Focus Detection Operation]</heading><p id="p-0100" num="0093"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a configuration of pixels and a correspondence relationship between pupil surfaces and photoelectric conversion units according to the present exemplary embodiment. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates two photoelectric conversion units <b>201</b>, pupil surfaces <b>253</b>, a micro lens <b>251</b>, and a color filter <b>252</b>. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a photoelectric conversion unit <b>201</b><i>a </i>(a first focus detection pixel) and a photoelectric conversion unit <b>20</b><i>b </i>(a second focus detection pixel) are provided as the two photoelectric conversion units <b>201</b> Light that passes through the pupil surface <b>253</b><i>a </i>enters the photoelectric conversion unit <b>201</b><i>a</i>. Light that passes through a pupil surface <b>253</b><i>b </i>enters the photoelectric conversion unit <b>201</b><i>b</i>. With the above configuration, focus detection can be performed using signals obtained from the photoelectric conversion units <b>201</b><i>a </i>and <b>201</b><i>b</i>. An image signal can be generated by adding the signals obtained from the photoelectric conversion units <b>201</b><i>a </i>and <b>201</b><i>b. </i></p><p id="p-0101" num="0094">In the present exemplary embodiment, the pixels illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> are provided in the entire screen area of the imaging unit <b>22</b> so that any subject appearing on the screen can be focused by phase difference detection.</p><p id="p-0102" num="0095">While the present exemplary embodiment will be described using the focus detection method described above, the focus detection method is not limited to the example. For example, the focus detection may be performed by providing the imaging unit <b>22</b> with pixels dedicated to focus detection illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, which will be described below. Alternatively, the imaging unit <b>22</b> may be provided with only pixels for image capturing, without providing pixels for focus detection, and focus detection using a contrast method may be performed.</p><p id="p-0103" num="0096"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a configuration of a pixel dedicated to focus detection and a correspondence relationship between a pupil surface and a photoelectric conversion unit. In contrast to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a pixel dedicated to focus detection. The shape of a pupil surface <b>253</b> is determined by an aperture <b>254</b>. Because only the light that passes through the pupil surface <b>253</b> is detected in the configuration, the other pixel to form a pair, that is, a pixel to detect light that passes through a pupil surface on the right (not illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>) needs to be provided so as to acquire focus detection signals. By providing the focus detection pixel illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> and the imaging pixels in the entire screen area of the imaging unit <b>22</b>, any subject appearing on the screen can be focused by phase difference detection.</p><p id="p-0104" num="0097">The digital camera <b>100</b> described above can perform shooting using center-point AF or face AF.</p><p id="p-0105" num="0098">The center-point AF is an AF mode in which one center point on an imaging screen is focused. The face AF is an AF mode in which a face on an imaging screen detected by a face detection function is focused.</p><p id="p-0106" num="0099">The face detection function will be described. The system control unit <b>50</b> transmits image data about a face detection target to the image processing unit <b>24</b>. The system control unit <b>50</b> controls the image processing unit <b>24</b> to apply a horizontal band-pass filter to the image data. The system control unit <b>50</b> controls the image processing unit <b>24</b> to apply a vertical band-pass filter to the processed image data. Edge components are detected from the image data with these horizontal and vertical band-pass filters.</p><p id="p-0107" num="0100">Next, the system control unit <b>50</b> performs pattern matching on the detected edge components and extracts a candidate group of eyes, noses, mouths, and ears. The system control unit <b>50</b> determines eyes that satisfy predetermined conditions (for example, a distance between two eyes, an inclination, or the like) as pairs of eyes from the candidate group of extracted eyes and narrows down the components having the pairs of eyes as the candidate group of eyes. The system control unit <b>50</b> associates the narrowed-down eyes in the candidate group with other parts (nose, mouth, ears) that form a face corresponding to the eyes and filters the formed face through a predetermined non-face condition filter to detect the face. Depending on the face detection result, the system control unit <b>50</b> outputs the above face information and ends the processing. When ending the processing, the system control unit <b>50</b> stores feature amounts such as the number of faces in the system memory <b>52</b>. The method for realizing the face detection function is not limited to the method described above. The number of faces, sizes, parts, and the like may be similarly detected by a method using known machine learning. The type of subject is not limited to a human face, An animal, a vehicle, or the like may also be detected.</p><p id="p-0108" num="0101">As described above, it is possible to detect subject information by performing image analysis on image data on live view display or reproduction display and extracting feature amounts of the image data. In the present exemplary embodiment, the face information has been described as an example of the subject information. The subject information includes various types of information such as about red-eye determination, eye detection, eye-closure detection, and smiling face detection.</p><p id="p-0109" num="0102">Face AE, face FE, and face WB can be performed simultaneously with the face AF. The face AE is to optimize the exposure of the entire screen in accordance with the brightness of a detected face, The face FE is to adjust flash light around a detected face. The face WB is to optimize the white balance (WB) of the entire screen in accordance with the color of a detected face.</p><p id="p-0110" num="0103">A line-of-sight detection processing method of the electronic apparatus, line-of-sight position state determination, and a determination result according to a second exemplary embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart illustrating focus detection, line-of-sight detection, and a shooting operation of the electronic apparatus according to the present exemplary embodiment. <figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an operation performed at live view shooting, in which shooting is performed during a live view state (video shooting state) such as a shooting standby state, and the operation is realized mainly by the system control unit <b>50</b>.</p><p id="p-0111" num="0104">In step S<b>11</b>, in accordance with the control of the system control unit <b>50</b>, the imaging unit <b>22</b> is driven to acquire imaging data. Because the imaging data acquired is not for an image for recording, which will be described below, but for an image for detection and display, imaging data whose size is smaller than that of a recording image is acquired. In step S<b>11</b>, the imaging unit <b>22</b> acquires an image having a sufficient resolution for performing focus detection, subject detection, or live view display. Because the driving operation in the step is for video shooting for live view display, shooting is performed by using an electronic shutter that performs charge accumulation and readout for time suitable for the frame rate for live view display. The live view displayed here is for the photographer to confirm the shooting range and shooting conditions and may be, for example, 30 frames per second (shooting intervals of 33.3 ms) or 60 frames per second (shooting intervals of 16.6 ns).</p><p id="p-0112" num="0105">In step S<b>12</b>, the system control unit <b>50</b> acquires focus detection data obtained from a first focus detection pixel and a second focus detection pixel included in a focus detection area from among the imaging data obtained in step S<b>11</b>. The system control unit <b>50</b> generates an imaging signal by adding output signals of the first focus detection pixel and the second focus detection pixel and acquires image data obtained by causing the image processing unit <b>24</b> to perform interpolation processing or the like. Thus, the system control unit <b>50</b> can acquire the image data and the focus detection data from a single shooting. In a case where the imaging pixel is configured separately from the first and second focus detection pixels, image data is obtained by performing interpolation processing or the like on the focus detection pixels.</p><p id="p-0113" num="0106">In step S<b>13</b>, the system control unit <b>50</b> generates an image for live view display by using the image processing unit <b>24</b> based on the image data obtained in step S<b>12</b> and displays the generated image on the display unit <b>28</b>. The image for live view display is, for example, a reduced image that matches the resolution of the display unit <b>28</b>. The imaging processing unit <b>24</b> may perform reduction processing when generating the image data in step S<b>12</b> in this case, the system control unit <b>50</b> displays the image data obtained in step S<b>12</b> on the display unit <b>28</b>. As described above, because shooting and display are performed at a predetermined frame rate during live view display, the photographer can adjust the composition and exposure conditions during shooting through the display unit <b>28</b>. As described above, in the present exemplary embodiment, a human face, an animal, etc. can be detected as a subject. In step S<b>3</b>, a frame or the like indicating an area of the detected subject is also displayed at the start of the live view display. As in step SI of the main flowchart of the first exemplary embodiment, instead of a live view image, an already acquired image may be displayed.</p><p id="p-0114" num="0107">Next, steps S<b>14</b> to S<b>19</b> are performed. Because these steps are the same as steps S<b>2</b> to S<b>7</b> in the first exemplary embodiment, description thereof will be omitted.</p><p id="p-0115" num="0108">In step S<b>20</b>, the system control unit <b>50</b> determines whether SW<b>1</b> is ON. In a case where the system control unit <b>50</b> does not detect ON of the SW<b>1</b> (or detects OFF of the SW<b>1</b>) (NO in step S<b>20</b>), the processing proceeds to step S<b>28</b> and the system control unit <b>50</b> determines whether the main switch included in the operation unit <b>70</b> is turned off. In contrast, in a case where the system control unit <b>50</b> detects ON of the SW<b>1</b> (YES in step S<b>20</b>), the processing proceeds to step S<b>21</b>, In step S<b>21</b>, a focus detection area to be focused is set, and focus detection is performed. In the step, the system control unit <b>50</b> sets a focus detection area by using the line-of-sight position detected in step S<b>18</b>. The line-of-sight position detected in step S<b>18</b> has a deviation with respect to the position of the subject intended by the photographer due to various factors.</p><p id="p-0116" num="0109">In step S<b>21</b>, the system control unit <b>50</b> sets the focus detection area by using the line-of-sight position information on which processing described below has been performed. At the time of the setting, the line-of-sight position and the central position of the focus detection area may be aligned. In a case where there is a candidate for the focus detection area by different means such as the subject detection unit a detected subject area may be associated with the line-of-sight position, and the subject area closest to the line-of-sight position may be selected as the focus detection area. After step S<b>21</b>, the setting of the focus detection area using the line-of-sight position information and the focus detection processing are repeatedly executed every time shooting is performed. Next, the focus detection processing will be described.</p><p id="p-0117" num="0110">By using focus detection data corresponding to the set focus detection area, a defocus amount and a defocus direction are obtained for the individual focus detection area. In the present exemplary embodiment, the system control unit <b>50</b> generates an image signal for focus detection, calculates a deviation amount (phase difference) of the focus detection signal, and obtains a defocus amount and a defocus direction from the calculated deviation amount.</p><p id="p-0118" num="0111">The system control unit <b>50</b> performs shading correction and filter processing on the first focus detection signal and the second focus detection signal obtained as image signals for focus detection from the set focus detection area, reduces a light amount difference of the pair of signals, and extracts a signal of a spatial frequency for performing phase difference detection. The system control unit <b>50</b> performs shifting processing for relatively shifting the filtered first focus detection signal and the filtered second focus detection signal in respective pupil division directions and calculates a correlation amount indicating the degree of coincidence of the signals.</p><p id="p-0119" num="0112">A correlation amount COR is calculated by equation (6) using a filtered k-th first focus detection signal A(k), a filtered second focus detection signal B(k), a range W of the number k corresponding to the individual focus detection area, a shift s<b>1</b> amount by the shifting processing, and a shift range &#x393;1 of the shift amount s<b>1</b>.</p><p id="p-0120" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>COR</i>(<i>s</i><sub>1</sub>)=&#x3a3;<sub>k&#x2208;W</sub><i>|A</i>(<i>k</i>)&#x2212;<i>B</i>(<i>k&#x2212;s</i><sub>1</sub>)|<i>s</i><sub>1</sub>&#x2208;&#x393;1&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0121" num="0113">By performing the shifting processing of the shift amount s<b>1</b>, the k-th first focus detection signal A(k) and the (k&#x2212;s<b>1</b>)-th second focus detection signal B(k&#x2212;s<b>1</b>) are made to correspond to each other and subtracted from each other to generate a shift subtraction signal. The system control unit <b>50</b> calculates an absolute value of the generated shift subtraction signal, calculates the sum of the numbers k in the range W corresponding to the focus detection area, and calculates a correlation amount COR(s<b>1</b>) As needed, the correlation amount calculated for an individual row may be added up over a plurality of rows per shift amount.</p><p id="p-0122" num="0114">Areal-valued shift amount at which the correlation amount becomes minimum is calculated from the correlation amount by sub-pixel calculation and is set as an image deviation amount p<b>1</b>. A detected defocus amount is obtained by multiplying the calculated image deviation amount p<b>1</b> by a conversion coefficient K<b>1</b> corresponding to an image height of the focus detection area, an F value of an imaging lens (an imaging optical system), and an exit pupil distance.</p><p id="p-0123" num="0115">In step S<b>22</b>, the system control unit <b>50</b> drives the lens based on the defocus amount detected in the selected focus detection area. In a case where the detected defocus amount is less than a predetermined value, the lens does not necessarily need to be driven.</p><p id="p-0124" num="0116">In step S<b>23</b>, the system control unit <b>50</b> performs acquisition of an image for detection/display and live view display performed in step S<b>1</b> and the focus detection processing performed in step S<b>6</b>. The subject area detected as described above and the line-of-sight position information are also superimposed in the live view display. The processing in step S<b>23</b> may be performed in parallel with the processing for driving the lens in step S<b>22</b>. The focus detection area may be changed to correspond to the line-of-sight position obtained in the live view display continually updated. When the focus detection processing is completed, the processing proceeds to step S<b>24</b>. In step S<b>24</b>, the system control unit <b>50</b> determines ON/OFF of the second shutter switch <b>64</b> (SW<b>2</b>) that indicates an instruction to start imaging. A release (imaging trigger) switch, which is one of the operation members of the operation unit <b>70</b>, can detect two stages of ON/OFF in accordance with a pressing amount, and the above SW<b>2</b> corresponds to the second stage of ON/OFF of the release (imaging trigger) switch. In a case where the system control unit <b>50</b> does not detect ON of the SW<b>2</b> in step S<b>24</b> (NO in step S<b>24</b>), the processing returns to step S<b>20</b>, and the system control unit <b>50</b> determines ON/OFF of the SW<b>1</b>.</p><p id="p-0125" num="0117">The system control unit <b>50</b> performs subject detection in an image for detection acquired in step S<b>1</b> and acquires information about the position and range of the detected subject. In the present exemplary embodiment, the line-of-sight position information is changed by using the acquired information about the detected subject and the line-of-sight position information obtained in step S<b>18</b>. Comparing the acquired information about the detected subject with the line-of-sight position information, in a case where the line-of-sight position is included in the detected subject range for a first determination number, the above-described stationary display determination is performed, and the line-of-sight information is stationarily displayed on the display unit <b>28</b>. Even in a case where the line-of-sight position is not within the detected subject range, for example, a range around the detected subject range may be set as the determination range. Instead of being stationarily displayed, the line-of-sight information display may not be displayed.</p><p id="p-0126" num="0118">In the present exemplary embodiment, a predetermined number N, a first determination number JN<sub>1 </sub>and a second determination number JN<sub>2 </sub>may be changed in accordance with an AF mode being set The AF mode includes &#x201c;one-shot AF mode&#x201d; in which the above-described AF control operation is performed only once when the SW<b>1</b> is detected in step S<b>20</b> and &#x201c;servo AF mode&#x201d; in which the AF control operation is continuously performed while the SW<b>1</b> detection is continued. The one-shot AF mode is often used, for example, when the composition is preset and a low-speed to stationary subject is captured. Thus, the first determination number and the second determination number are set to a larger number than those in the case where the servo AF mode is set, and the first threshold and the second threshold are also set to a larger value. In this way, the line-of-sight position display can be finely adjusted with respect to the subject. As a result, good line-of-sight display can be performed.</p><p id="p-0127" num="0119">In contrast, the servo AF mode is often used, for example, when a medium-speed to high-speed subject that needs a framing operation is captured. Thus, the first determination number and the second determination number are set to a smaller number than those in the case where the one-shot AF mode is set, and the first threshold and the second threshold are also set to a smaller value. In this way, the dynamical display can be quickly performed. As a result, good line-of-sight display can be performed even for a quickly moving subject.</p><p id="p-0128" num="0120">Instead of changing the setting in accordance with the AF mode as described above, the user may freely set the first determination number, the second determination number, the first threshold, and the second threshold.</p><p id="p-0129" num="0121">In a case where the system control unit <b>50</b> detects ON of the SW<b>2</b> in step S<b>24</b> (YES in step S<b>24</b>), the processing proceeds to step S<b>25</b>. In step S<b>25</b>, the system control unit <b>50</b> determines whether to perform image recording. In the present exemplary embodiment, processing for acquiring images during continuous shooting is switched between processing for image recording and processing for imaging/display and focus detection. The switching may be alternately performed. For example, the processing for imaging/display and focus detection may be performed once in three times. In this way, highly accurate focus detection can be performed without significantly reducing the number of images captured per unit time.</p><p id="p-0130" num="0122">In step S<b>25</b>, in a case where the system control unit <b>50</b> determines that processing for image recording is to be performed (YES in step S<b>25</b>), the processing proceeds to step S<b>26</b>, and an imaging subroutine is performed. Details of the imaging subroutine will be described below. When the imaging subroutine has been performed, the processing returns to step S<b>24</b>, and the system control unit <b>50</b> determines whether the SW<b>2</b> is ON, that is, determines whether continuous shooting has been instructed.</p><p id="p-0131" num="0123">In step S<b>25</b>, in a case where the system control unit <b>50</b> determines that processing for imaging/display and focus detection are to be performed (NO in step S<b>25</b>), the processing proceeds to step S<b>27</b>, and the system control unit <b>50</b> performs the processing for imaging/display and focus detection during continuous shooting. The imaging/display and focus detection processing during continuous shooting is performed in the same manner as in step S<b>23</b>. The difference is that, in accordance with a shooting frame speed of continuous shooting, processing for generating a recorded image, and the like, a display period, a display update rate (intervals), and a display delay of an image captured in step S<b>27</b> are different from those in the processing in step S<b>23</b>. The system control unit <b>50</b> serving as the display unit performs these display control operations. As described above the obtained line-of-sight position information is used for setting the focus detection area and association with the detected subject area, for example. The details will be described below. When the imaging/display and focus detection processing during continuous shooting has been performed in step S<b>27</b>, the processing returns to step S<b>24</b>, and the system control unit <b>50</b> determines whether the SW<b>2</b> is ON, that is, determines whether continuous shooting has been instructed. In a case where the system control unit <b>50</b> does not detect ON of the SW<b>1</b> (or detects OFF of the SW<b>1</b>) in step S<b>20</b> (NO in step S<b>20</b>) and detects that the main switch is OFF in step S<b>28</b> (YES in step S<b>28</b>), the focus detection and shooting operation are ended. In a case where the system control unit <b>50</b> does not detect that the main switch is OFF in step S<b>28</b> (NO in step S<b>28</b>), the processing returns to step S<b>12</b>, and image data and focus detection data is acquired.</p><p id="p-0132" num="0124">The imaging subroutine performed in step S<b>26</b> of <figref idref="DRAWINGS">FIG. <b>12</b></figref> will be described in detail with r<sub>e</sub>ference to a flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>. A series of operations of the subroutine is also realized mainly by the system control unit <b>50</b>.</p><p id="p-0133" num="0125">In step S<b>2601</b>, the system control unit <b>50</b> performs exposure control processing to determine shooting conditions (shutter speed, aperture value, and shooting sensitivity). The exposure control processing can be performed by the system control unit <b>50</b> based on luminance information about the image data, and any known technique can be used. In the present exemplary embodiment, details such as the timing of obtaining image data used when the exposure control processing is performed will be described below. The operation of the shutter <b>101</b> is controlled based on the aperture value and shutter speed determined in step S<b>2601</b>. The system control unit <b>50</b> causes the imaging unit <b>22</b> to perform charge accumulation during a period in which the imagin<sup>g </sup>unit <b>22</b> is exposed via the shutter <b>101</b>.</p><p id="p-0134" num="0126">When the exposure period ends, in step S<b>2602</b>, the system control unit <b>50</b> reads an image for high-pixel still image shooting, that is, reads all pixels. The system control unit <b>50</b> reads an image of either the first focus detection pixel or the second focus detection pixel. A signal read from the focus detection pixel is used for detecting the focus state of a subject during image reproduction. Thus, the amount of data to be read may be reduced by limiting the area of a recorded image obtained by high-pixel still image shooting or reducing the resolution of all pixels. In a case where a signal is obtained from either the first focus detection pixel or the second focus detection pixel, a focus detection signal of the other focus detection pixel can be obtained by calculating the difference from the image for high-pixel still image shooting. In the present exemplary embodiment, to prioritize the S/N of the signal of the image for high-pixel still image shooting, the image signal for high-pixel still image shooting and one of the focus detection signals are read out and recorded, and the other focus detection signal is obtained by calculation. Hereinafter, the processing for the image is performed on an image signal for high-pixel still image shooting and one of the focus detection signals.</p><p id="p-0135" num="0127">In step S<b>2603</b>, the system control unit <b>50</b> causes the image processing unit <b>24</b> to perform defective pixel correction processing on the read image data. In step S<b>2604</b>, the system control unit <b>50</b> causes the image processing unit <b>24</b> to apply image processing such as demosaic (color interpolation) processing, white balance processing, y correction (gradation correction) processing, color conversion processing, and edge enhancement processing, encoding processing, or the like to the image data on which the defective pixel correction has been performed. In step S<b>2605</b>, the system control unit <b>50</b> records the image signal for high-pixel still image shooting and one of the focus detection signals in the memory <b>32</b> as an image data file.</p><p id="p-0136" num="0128">In step S<b>2606</b>, the system control unit <b>50</b> records, in association with the captured image recorded in step S<b>2605</b>, attribute information about the camera body in the memory <b>32</b> and the memory in the system control unit <b>50</b>. Examples of the attribute information about the camera body include information as follows: -shooting conditions (aperture value, shutter speed, shooting sensitivity, etc.); -information related to image processing applied by the image processing unit <b>24</b>; -light receiving sensitivity distribution information about imaging pixels and focus detection pixels of the imaging unit <b>22</b>;<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0129">vignetting information about an imaging light beam in the camera body;</li>        <li id="ul0002-0002" num="0130">,information about distance from an attachment surface between the camera body and the lens unit to the imaging unit <b>22</b>; and</li>        <li id="ul0002-0003" num="0131">manufacturing error information.</li>    </ul>    </li></ul></p><p id="p-0137" num="0132">Because the light receiving sensitivity distribution information about imaging pixels and focus detection pixels of the imaging unit <b>22</b> is information depending on an on-chip micro lens ML and a photoelectric conversion unit PD, information related to these members may be recorded. The light receiving sensitivity distribution information is information about sensitivity corresponding to a position at a predetermined distance on the optical axis from an imaging element. The light receiving sensitivity distribution information may include a change in sensitivity with respect to the incident angle of light.</p><p id="p-0138" num="0133">In step S<b>2607</b>, the system control unit <b>50</b> records, in association with the captured image recorded in step S<b>2605</b>, attribute information about the lens unit in the memory <b>32</b> and the memory in the system control unit <b>50</b>. Examples of the attribute information about the lens unit include information about an exit pupil, frame information, focal length and F-number information upon shooting, aberration information, manufacturing error information, subject distance information associated with a focus lens position upon shooting, and the like. In step S<b>2608</b>, the system control unit <b>50</b> records image-related information about the captured image in the memory <b>32</b> and the memory in the system control unit <b>50</b>. The image-related information may include information related to a focus detection operation before shooting, subject movement information, information related to accuracy of the focus detection operation, and the like.</p><p id="p-0139" num="0134">In step S<b>2609</b>, the system control unit <b>50</b> displays a preview of the captured image on the display unit <b>28</b>, The operation enables the photographer to have a quick check on the captured image. Because the image used for the preview display performed in step S<b>2609</b> is for the purpose of simple check on the image, various kinds of processing performed in steps S<b>2603</b> and <b>2604</b> may not be performed. In a case where these various kinds of processing are not performed, the preview display in step S<b>2609</b> can be performed in parallel with the processing after step S<b>2603</b> so that the time lag from the exposure to the display can be reduced.</p><p id="p-0140" num="0135">When the processing in step S<b>2609</b> is completed, the system control unit <b>50</b> ends the imaging subroutine in step S<b>26</b>, and the processing proceeds to S<b>24</b> in the main routine. In the present exemplary embodiment, during continuous shooting, the acquired image is displayed in both cases where the image is recorded in the imaging subroutine in step S<b>26</b> and where the imaging/display and focus detection processing is performed during continuous shooting in step S<b>27</b>.</p><p id="p-0141" num="0136">As described above, it is possible to provide an electronic apparatus that improves display quality and convenience by stationarily or dynamically displaying the line-of-sight position based on the line-of-sight position shift amount. In the present exemplary embodiment, the method for determining whether to perform stationary display or dynamical display in accordance with the AF mode or the subject detection has been described. The determination method, however, may be changed in accordance with the shooting mode such as moving image shooting or still image shooting and a difference in the operation methods of the shooting devices such as shooting using a viewfinder or shooting using a rear-side liquid crystal screen.</p><p id="p-0142" num="0137">While the exemplary embodiments are implemented by a digital camera, the exemplary embodiments may be applied to any apparatus that performs line-of-sight detection. For example, the exemplary embodiments can be implemented by a head-mounted display, a smartphone, a personal computer (PC), and the like.</p><p id="p-0143" num="0138">In the operations described with reference to the flowcharts in the above exemplary embodiments, the order of the steps may be changed as appropriate as long as the same object can be achieved.</p><p id="p-0144" num="0139">In some exemplary embodiments of the present disclosure, a program that realizes at least one function of the above exemplary embodiments may be supplied to a system or an apparatus via a network or a storage medium, Some exemplary embodiments can also be realized by causing at least one processor included in a computer in the system or the apparatus to read and execute the program. Some exemplary embodiments can also be realized by a circuit (for example, an application-specific integrated circuit (ASIC)) that realizes at least one function of the above exemplary embodiments.</p><p id="p-0145" num="0140">Some embodiment(s) can also be realized by a computer of a system or apparatus that reads out and executes computer-executable instructions (e.g., one or more programs) recorded on a storage medium (which may also be referred to more fully as a &#x2018;non-transitory computer-readable storage medium&#x2019;) to perform the functions of one or more of the above-described embodiment(s) and/or that includes one or more circuits (e.g., application specific integrated circuit (ASIC)) for performing the functions of one or more of the above-described embodiment(s), and by a method performed by the computer of the system or apparatus by, for example, reading out and executing the computer-executable instructions from the storage medium to perform the functions of one or more of the above-described embodiment(s) and/or controlling the one or more circuits to perform the functions of one or more of the above-described embodiment(s). The computer may comprise one or more processors (e.g., central processing unit (CPU), micro processing unit (MPU)) and may include a network of separate computers or separate processors to read out and execute the computer-executable instructions. The computer-executable instructions may be provided to the computer, for example, from a network or the storage medium. The storage medium may include, for example, one or more of a hard disk, a random-access memory (RAM), a read only memory (ROM), a storage of distributed computing systems, an optical disk (such as a compact disc (CD), digital versatile disc (DVD), or Blu-ray Disc (BD)), a flash memory device, a memory card, and the like.</p><p id="p-0146" num="0141">While the present disclosure has described exemplary embodiments, it is to be understood that some embodiments are not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.</p><p id="p-0147" num="0142">This application claims priority to Japanese Patent Application No. 2021-109537, which was filed on Jun. 30, 2021 and which is hereby incorporated by reference herein in its entirety.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A control apparatus comprising:<claim-text>at least one processor configured to:</claim-text><claim-text>acquire information corresponding to a position at which an observer stares; and</claim-text><claim-text>control a display unit to display an indicator corresponding to the position,</claim-text><claim-text>in a case where a change of the position, based on the information corresponding to the position, is continuously smaller than a first threshold a first number of times, a display position of the indicator is not shifted.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The control apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one processor is further configured to, in a case where the change of the position is continuously greater than a second threshold a second number of times, shift the display position of the indicator.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The control apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the at least one processor is further configured to, in a case where the change of the position continuously occurs in a predetermined direction, shift the display position of the indicator.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The control apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>,<claim-text>wherein the case where the change of the position in the predetermined direction continuously occurs is a case where a shift within a range corresponding to the predetermined direction occurs at least twice.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The control apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the at least one processor is further configured to detect a position of a subject, and</claim-text><claim-text>wherein, in a case where the position approximately matches the position of the subject, the display position of the indicator is not shifted.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The control apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the at least one processor is further configured to detect a position of a subject, and</claim-text><claim-text>wherein, in a case where the position approximately matches the position of the subject, the indicator is not displayed.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The control apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one processor is further configured to change at least one of the first number, the first threshold, the second number, and the second threshold based on at least one of a horizontal position shift amount, a vertical position shift amount, and a position shift amount between two points.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The control apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the at least one processor is farther configured to control driving of a focus lens to drive a focus lens in a first driving mode, and repeatedly drive the focus lens in a second driving mode, and</claim-text><claim-text>wherein, in a case where the second driving mode is selected, the first threshold and the second threshold are smaller than the first threshold and the second threshold are in a case where the first driving mode is selected.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The control apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein, in a case where the second autofocus mode is selected, the first number and the second number are smaller than the first number and the second number are in a case where the first autofocus mode is selected.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A control method comprising:<claim-text>acquiring information corresponding to a position at which an observer stares, wherein the information indicates a shift in the position; and</claim-text><claim-text>controlling a display unit to display an indicator corresponding to the position,</claim-text><claim-text>wherein, in the controlling, in a case where the shift in the position is continuously smaller than a first threshold a first number of times, the display position of the indicator is not shifted.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A non-transitory computer-readable storage medium storing a program for causing a computer to execute operations comprising:<claim-text>acquiring information corresponding to a position at which an observer stares, wherein the information indicates a shift in the position; and</claim-text><claim-text>controlling a display unit to display an indicator corresponding to the position,</claim-text><claim-text>wherein, in the controlling, in a case where the shift in the position is continuously smaller than a first threshold a first number of times, the display position of the indicator is not shifted.</claim-text></claim-text></claim></claims></us-patent-application>