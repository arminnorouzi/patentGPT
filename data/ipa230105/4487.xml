<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004488A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004488</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17881257</doc-number><date>20220804</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>02</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>1027</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>023</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>1027</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2212</main-group><subgroup>1044</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MEMORY REDUCTION IN A SYSTEM BY OVERSUBSCRIBING PHYSICAL MEMORY SHARED BY COMPUTE ENTITIES SUPPORTED BY THE SYSTEM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17367061</doc-number><date>20210702</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11455239</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17881257</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Microsoft Technology Licensing, LLC</orgname><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SHAH</last-name><first-name>Monish Shantilal</first-name><address><city>Sammamish</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>HSU</last-name><first-name>Lisa Ru-feng</first-name><address><city>Durham</city><state>NC</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>BERGER</last-name><first-name>Daniel Sebastian</first-name><address><city>Seattle</city><state>WA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods and systems related to memory reduction in a system by oversubscribing physical memory shared among compute entities are provided. A portion of the memory includes a combination of a portion of a first physical memory of a first type and a logical pooled memory associated with the system. A logical pooled memory controller is configured to: (1) track both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is a known-pattern page and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory, and (2) allow the write operation to write data to any available space in the second physical memory of the first type only up to an extent of physical memory that corresponds to the portion of the logical pooled memory previously allocated to the compute entity.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="101.09mm" wi="158.75mm" file="US20230004488A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="222.33mm" wi="155.28mm" orientation="landscape" file="US20230004488A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="237.07mm" wi="99.65mm" orientation="landscape" file="US20230004488A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="251.46mm" wi="167.30mm" orientation="landscape" file="US20230004488A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="213.36mm" wi="102.02mm" orientation="landscape" file="US20230004488A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="222.84mm" wi="90.09mm" orientation="landscape" file="US20230004488A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="257.05mm" wi="145.12mm" orientation="landscape" file="US20230004488A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="238.08mm" wi="117.09mm" orientation="landscape" file="US20230004488A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="247.31mm" wi="172.13mm" file="US20230004488A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="256.79mm" wi="175.26mm" file="US20230004488A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/367,061, filed on Jul. 2, 2021, entitled &#x201c;MEMORY REDUCTION IN A SYSTEM BY OVERSUBSCRIBING PHYSICAL MEMORY SHARED BY COMPUTE ENTITIES SUPPORTED BY THE SYSTEM,&#x201d; the entire contents of which are hereby incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Multiple users or tenants may share systems, including computing systems and communications systems. Computing systems may include the public cloud, the private cloud, or a hybrid cloud having both public and private portions. The public cloud includes a global network of servers that perform a variety of functions, including storing and managing data, running applications, and delivering content or services, such as streaming videos, provisioning electronic mail, providing office productivity software, or handling social media. The servers and other components may be located in data centers across the world. While the public cloud offers services to the public over the Internet, businesses may use private clouds or hybrid clouds. Both private and hybrid clouds also include a network of servers housed in data centers.</p><p id="p-0004" num="0003">Multiple tenants may use compute, storage, and networking resources associated with the servers in the cloud. As an example, compute entities associated with different tenants may be allocated a certain amount of the compute and memory resources. In many such situations, the allocated resources to various compute entities, including the memory resources, may be under-utilized. In other systems, such as communications systems, including base stations, similar underutilization of the memory resources may occur.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">In one example, the present disclosure relates to a method comprising allocating a portion of a memory associated with a system to a compute entity, where the portion of the memory comprises a combination of a portion of a first physical memory of a first type and a portion of a logical pooled memory for use with a plurality of compute entities associated with the system. As used herein, the term &#x201c;compute entity&#x201d; encompasses, but is not limited to, any executable code (in the form of hardware, firmware, software, or in any combination of the foregoing) that implements a functionality, a virtual machine, an application, a service, a micro-service, a container, a unikernel for serverless computing, or a part of the aforementioned. As used herein the term &#x201c;logical pooled memory&#x201d; refers to memory that includes overcommitted physical memory that is shared by a plurality of compute entities, which may correspond to a single host or multiple hosts. The logical pooled memory may be mapped to a second physical memory of the first type, and where an amount of the logical pooled memory indicated as being available for allocation to the plurality of compute entities may be greater than an amount of the second physical memory of the first type. The method may further include indicating to a logical pooled memory controller associated with the logical pooled memory that all pages associated with the logical pooled memory initially allocated to any of the plurality of compute entities are known-pattern pages. The method may further include the logical pooled memory controller tracking both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is a known-pattern page and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities. The method may further include in response to a write operation initiated by the compute entity, the logical pooled memory controller allowing writing of the data to any available space in the second physical memory of the first type only up to an extent of physical memory that corresponds to the portion of the logical pooled memory previously allocated to the compute entity.</p><p id="p-0006" num="0005">In another example, the present disclosure relates to a system comprising a memory for use with a plurality of compute entities, where a portion of the memory comprises a combination of a portion of a first physical memory of a first type and a portion of a logical pooled memory associated with the system. The logical pooled memory may be mapped to a second physical memory of the first type and an amount of the logical pooled memory indicated as being available for allocation to the plurality of compute entities may be greater than an amount of the second physical memory of the first type. The system may further include a logical pooled memory controller, coupled to the logical pooled memory, configured to: (1) track both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is a known-pattern page and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities, and (2) in response to a write operation initiated by the compute entity to write any data other than a known-pattern, allow the write operation to write the data to any available space in the second physical memory of the first type only up to an extent of physical memory that corresponds to the portion of the logical pooled memory previously allocated to the compute entity.</p><p id="p-0007" num="0006">In yet another example, the present disclosure relates to a system including a plurality of host servers configurable to execute one or more of a plurality of compute entities. The system may further include a memory, where a portion of the memory comprises a combination of a portion of a first physical memory of a first type and a portion of a logical pooled memory shared among the plurality of host servers. The logical pooled memory may be mapped to a second physical memory of the first type and an amount of the logical pooled memory indicated as being available for allocation to the plurality of compute entities may be greater than an amount of the second physical memory of the first type. The system may further include a logical pooled memory controller, coupled to the logical pooled memory associated with the system, configured to: (1) track both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is a known-pattern page and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities, and (2) in response to a write operation initiated by a compute entity, being executed by a processor associated with any of the plurality of host servers, to write any data other than a known pattern, allow the write operation to write the data to any available space in the second physical memory of the first type only up to an extent of physical memory that corresponds to the portion of the logical pooled memory previously allocated to the compute entity.</p><p id="p-0008" num="0007">This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008">The present disclosure is illustrated by way of example and is not limited by the accompanying figures, in which like references indicate similar elements. Elements in the figures are illustrated for simplicity and clarity and have not necessarily been drawn to scale,</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a system environment including host servers coupled with a logical pooled memory system in accordance with one example;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an oversubscription process flow ordance with one example;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of a system including a VM scheduler, a machine learning (ML) system, and a host server including a logical pooled memory system configured to allow for oversubscription in accordance with one example;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of a logical pooled memory system configured to allow for oversubscription of memory in accordance with one example;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a diagram of an example implementation of the mapping table using a translation look-aside buffer (TLB) in accordance with one example;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a block diagram of an example system including multiple host servers coupled with a logical pooled memory system configured to allow for oversubscription of memory;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a block diagram of an example system for implementing at least some of the methods for allowing oversubscription of the pooled memory allocated to compute entities;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a flow chart of an example method for oversubscribing physical memory for use with compute entities; and</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a flow chart of another example method for oversubscribing physical memory for use with compute entities.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">Examples described in this disclosure relate to memory reduction in a system by oversubscribing physical memory shared among compute entities supported by the system. Certain examples relate to oversubscribing physical memory for use with virtual machines in a multi-tenant computing system. The multi-tenant computing system may be a public cloud, a private cloud, or a hybrid cloud. The public cloud includes a global network of servers that perform a variety of functions, including storing and managing data, running applications, and delivering content or services, such as streaming videos, electronic mail, office productivity software, or social media. The servers and other components may be located in data centers across the world. While the public cloud offers services to the public over the Internet, businesses may use private clouds or hybrid clouds. Both private and hybrid clouds also include a network of servers housed in data centers. Compute entities may be executed using compute and memory resources of the data center. As used herein, the term &#x201c;compute entity&#x201d; encompasses, but is not limited to, any executable code (in the form of hardware, firmware, software, or in any combination of the foregoing) that implements a functionality, a virtual machine, an application, a service, a micro-service, a container, a unikernel for serverless computing, or a part of the aforementioned. Alternatively, compute entities may be executing on hardware associated with an edge-compute device, on-premises servers, or other types of systems, including communications systems, such as base stations (e.g., 5G or 6G base stations).</p><p id="p-0020" num="0019">Consistent with the examples of the present disclosure, compute entities are allocated a combination of the local physical memory and the logical pooled memory with the assumption that the memory usage will typically be less than or equal to the memory allocated to a compute entity. This allows for the oversubscription of the installed memory in the system by the compute entities since more compute entities can be supported by the system without having to deploy additional physical memory. As an example, in a multi-tenant computing or communication system, each tenant is assigned some portion of the total system memory. It is observed that all tenants do not use their entire allocation of memory at the same time. Thus, as an example, the host servers in a data center may be allocated logical pooled memory exposed by a pooled memory system. Each of the VMs may be allocated a combination of the local physical memory on the host server and a fraction of the logical pooled memory available to the host server. The logical pooled memory may be a combination of physical pooled memory and known-pattern pages that are not backed by any physical memory. In sum, as used herein the term &#x201c;logical pooled memory&#x201d; refers to memory that includes overcommitted physical memory that is shared by a plurality of compute entities, which may correspond to a single host or multiple hosts. In this example, overcommitted means that the logical pooled memory has a smaller amount of physical memory than the total amount of memory that is indicated as being available to a compute entity. As used herein the term &#x201c;known-pattern pages&#x201d; refers to any pages that include only zeros, only ones, or some other known pattern of values or symbols. The VMs may use the local physical memory first and once that is in use, the VMs may access the fraction of the logical pooled memory. In a multi-tenant computing system, when all of the VMs are allocated memory in this fashion, they are unlikely to exhaust the entire allocation of the physical pooled memory backing the logical pooled memory. This is why the actual physical memory behind the logical pooled memory controller can be less than the logical pooled memory exposed to the host servers. This, in turn, may allow for reduction in the amount of the physical memory (e.g., DRAM) that needs to be deployed as part of the multi-tenant computing system or communication system.</p><p id="p-0021" num="0020">Although with the above implementation of the logical pooled memory, some of the portion of the logical pooled memory will remain unused, it is difficult to predict which portion of the logical pooled memory will be unused at a given time. Indeed, different portions of the logical pooled memory may be in use at different times. Therefore, a mechanism is needed to flexibly map the pooled physical memory to the logical pooled memory space exposed to the compute entities such that logical pooled memory space that is in use is backed by the physical pooled memory while leaving the unused physical pooled memory unmapped. In certain examples, the mapping function may be performed using a page mapping table. Other mechanisms may also be used. In addition, in some instances the use of memory may further be optimized by compressing the data before storing the data into the physical pooled memory, If data compression is used, then the mapping function may further be modified to keep track of the compressed memory pages.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a system environment <b>100</b> including host servers (e.g., host servers <b>110</b>, <b>120</b>, and <b>130</b>) coupled with a logical pooled memory system <b>150</b> in accordance with one example. Each host server may be configured to execute several compute entities. In this example, host server <b>110</b> may be executing compute entities <b>112</b> (e.g., CE-<b>0</b>, CE-<b>1</b>, and CE-<b>2</b>), host server <b>120</b> may be executing compute entities <b>122</b> (e.g., CE-<b>3</b>, CE-<b>4</b>, and CE-<b>5</b>), and host server <b>130</b> may be executing compute entities <b>132</b> (e.g., CE-<b>6</b>, CE-<b>7</b>, and CE-N). Logical pooled memory system <b>180</b> may include logical pooled memory <b>190</b>, which may include several memory modules. Although not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, logical pooled memory system <b>180</b> may include a logical pooled memory controller (described later). As explained later in more detail, logical pooled memory <b>190</b> may include an address space that is larger than the physical memory configured to support the logical address space. In this example logical pooled memory <b>190</b> may include memory modules: PM-<b>1</b>, PM-<b>2</b>, and PM-M, where M is an integer greater than one. As used herein the term &#x201c;memory modules&#x201d; includes any memory that is based on a memory technology that can act as cacheable memory. Examples of such memory modules include, but are not limited to, dual-in-line memory modules (DIMMs) or single-in-line memory modules (SIMMs). As used herein the term &#x201c;cacheable memory&#x201d; includes any type of memory where the data obtained via a read operation associated with the memory can be copied to a memory cache such that the next time the same data is accessed, it can be obtained from the memory cache. Memory may be dynamic random access memory (DRAM), flash memory, static random access memory (SRAM), phase change memory, magnetic random access memory, or any other type of memory technology that can allow the memory to act as a cacheable memory.</p><p id="p-0023" num="0022">With continued reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, each of host servers <b>110</b>, <b>120</b>, and <b>130</b> may be coupled via links to memory modules included in logical pooled memory <b>190</b>. As an example, host server <b>110</b> is shown as coupled with memory module PM-<b>1</b> via link <b>142</b>, coupled with memory module PM-<b>2</b> via link <b>144</b>, and coupled with memory module PM-<b>3</b> via link <b>146</b>. Host server <b>120</b> is shown as coupled with memory module PM-<b>1</b> via link <b>152</b>, coupled with memory module PM-<b>2</b> via link <b>154</b>, and coupled with memory module PM-<b>3</b> via link <b>156</b>. As another example, host server <b>130</b> is shown as coupled with memory module PM-<b>1</b> via link <b>162</b>, coupled with memory module PM-<b>2</b> via link <b>164</b>, and coupled with memory module PM-M via link <b>166</b>.</p><p id="p-0024" num="0023">Still referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in this example, fabric manager <b>170</b> may manage the assignment of memory ranges to respective host servers. A processor (e.g., a CPU) serving a compute entity may issue load or store instructions. The load or store instructions may result in a read or write to a local memory (e.g., a DRAM associated with a CPU of a host server configured to execute the compute entities) or a read/write to a memory module associated with logical pooled memory <b>190</b>. In this example, a load or store instruction that results in a read/write to a memory module associated with logical pooled memory <b>160</b> may be translated by fabric manager <b>170</b> into a transaction that is completed using one of the links (e.g., link <b>142</b>, <b>144</b>, or <b>146</b>). In one example, fabric manager <b>170</b> may be implemented using a fabric manager based on the Compute Express Link specification.</p><p id="p-0025" num="0024">With continued reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, compute entities <b>112</b>, <b>122</b>, and <b>132</b> may be served compute, storage, and networking resources using a set of clusters that may be included as part of a data center, As used in this disclosure, the term data center may include, but is not limited to, some or all of the data centers owned by a cloud service provider, some or all of the data centers owned and operated by a cloud service provider, some or all of the data centers owned by a cloud service provider that are operated by a customer of the service provider, any other combination of the data centers, a single data center, or even some clusters in a particular data center, In one example, each cluster may include several identical servers. Thus, a cluster may include servers including a certain number of CPU cores and a certain amount of memory. Alternatively, compute entities <b>112</b>, <b>122</b>, and <b>132</b> may be executing on hardware associated with an edge-compute device, on-premises servers, or other types of systems, including communications systems, such as base stations (e.g., 5G or 6G base stations). Although <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows system environment <b>100</b> as having a certain number of components, including compute entities and memory components, arranged in a certain manner, system environment <b>100</b> may include additional or fewer components, arranged differently.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an example of an oversubscription process flow. This example illustrates the oversubscription process using virtual machines (VMs) as example compute entities that are being executed using hardware associated with a host server. Moreover, this example assumes that the host server is located in a data center including clusters of host servers. This example further assumes that the dusters of host servers are part of a multi-tenant cloud. The oversubscription process, however, could be used in other settings, including when compute entities are executing on hardware associated with an edge-compute device, on-premises servers, or other types of systems, including communications systems, such as base stations (e.g., 5G or 6G base stations). <b>1000261</b> hi response to a request from a user to initialize a VM, a VM scheduler may schedule a VM for execution using hardware resources associated with the host server. As part of this process, the VM may be allocated certain compute resources (e.g., CPU(s)) and memory (e.g., DRAM associated with the CPU(s)). The memory associated with a host server may be better utilized by using oversubscription alone or using oversubscription in combination with pooling. As an example, the memory may be oversubscribed with an expectation that in most use-cases, the VM will use only the memory that has been typically utilized by such a VM or predicted for use by such a VM. In one example, an average of the peak use by a VM may be allocated as the local physical memory to the VM. Thus, a certain amount of memory may be allocated to VMs knowing that such memory may not necessarily be available to such VMs, including, for example, in a peak usage or another high memory use scenario. Pooling may help with oversubscription and oversubscription may help with pooling when these two techniques are used together. This is because the memory from the pool may be used for VMs that had been allocated memory that was oversubscribed.</p><p id="p-0027" num="0026">Still referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an example oversubscription process with respect to a request for a 32 GB memory (<b>210</b>) by a VM is shown. In this example, in response to this request, the hypervisor (or another coordinator for allocating resources to VMs) may allocate 19 GB of physical memory and 13 GB of memory that is mapped to a logical pooled memory. In this example, the allocation of 19 GB memory may be based on the predicted use of memory by the requesting VM. In this example, the pooled memory may have a logical pooled memory of 256 GB with physical pooled memory of 128 GB. In this manner, the 13 GB of memory that is mapped to the pool may or may not be available since the logical pooled memory of 256 GB is shared with other host servers supporting additional VMs. The VM having been allocated 19 GB of physical memory and 13 GB of logical memory that is mapped to the pooled memory may execute without placing any demand on the pooled memory as in most cases the memory actually used by the VM may be less than 19 GB (<b>220</b>). Alternatively, in rare cases, the VM's use of memory may exceed 19 GB (<b>230</b>). In such a case, additional memory may be allocated from physical pooled memory <b>262</b> out of logical pooled memory <b>260</b>. As part of the oversubscription process, a high watermark may be associated with the use of physical pooled memory <b>262</b>, such that when the use of this memory by the VMs approaches the high watermark, certain mitigation processes may be initiated. As an example, one the high watermark is reached, the logical pooled memory controller may start compressing pages before storing them in physical pooled memory <b>262</b>. In another example, physical pooled memory <b>262</b> may be backed up by additional physical memory that may have a higher latency and thus it likely may be cheaper to deploy than the memory associated with physical pooled memory <b>262</b>. In such a situation, once the high watermark is reached, pages may be written to the slower memory.</p><p id="p-0028" num="0027">To ensure that the compute entity first uses the portion of the physical memory (e.g., 19 GB) allocated to it before using the portion of the logical pooled memory (e.g., 13 GB) allocated to it, a software mechanism may be used to enable the compute entity to distinguish between the physical memory and the logical pooled memory. In one example, to ensure better performance in host servers that support Non-Uniform Memory Access (NUMA) nodes, the pooled memory may be exposed as a compute-less virtual NUMA node to the VM. This in one for the VM to distinguish between the local physical memory and the logical pooled memory, which may have a higher latency. Accordingly, the VM may continue using the local physical memory first and then once that is exhausted it may start using the pooled memory exposed as the compute-less virtual NUMA node to the VM. Other mechanisms may also be used to allow the VM to differentiate between the local physical memory and the logical pooled memory. Although <figref idref="DRAWINGS">FIG. <b>2</b></figref> describes the oversubscribing process with a total of 32 GB requested by the VM and a logical pooled memory of 256 GB, the VM may request additional or fewer amount of memory (e.g., anywhere between 4 GB to 128 GB). Similarly, the size of the logical pooled memory may be smaller or bigger depending upon the number of compute entities being served by a computing system, Moreover, although <figref idref="DRAWINGS">FIG. <b>2</b></figref> describes the amount of local physical memory as 19 GB and the logical pooled memory as 13 GB, these amounts may vary among compute entities. In addition, the oversubscription process does not require pooling of the memory among the host servers (or other systems), it only requires a plurality of compute entities sharing the logical pooled memory.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of a system <b>300</b> including a VM scheduler <b>302</b>, a machine learning (ML) system <b>304</b>, and a host server <b>310</b> including resources <b>330</b> configured to allow for oversubscription in accordance with one example. Host server <b>310</b> may also include a hypervisor <b>320</b>, compute resources (e.g., CPU-<b>0</b> <b>332</b> and CPU-<b>1</b> <b>338</b>), local memory resources (e.g., local memory-<b>0</b> <b>340</b> and local memory-<b>1</b> <b>342</b>), a fabric manager <b>350</b>, and logical pooled memory system <b>360</b>. VM scheduler <b>302</b> may be configured to receive requests for VMs from any of tenants associated with a multi-tenant cloud and schedule the placement of the VMs (e.g., VM <b>312</b>, VM <b>314</b>, VM <b>316</b>, and VM <b>318</b> on host server <b>310</b>). In this example, each of resources <b>330</b> may be implemented as a hardware component, such as using one or more of SoCs, controllers, processing cores, memory modules, ASICs, FPGAs, CPLDs, PLAs, silicon IP, and the like. Moreover, in this example, the functionality associated with each of VM scheduler <b>302</b>, ML system <b>304</b>, and hypervisor <b>320</b> may be implemented via instructions, when executed by a processor, that are stored in a non-transitory media. Although <figref idref="DRAWINGS">FIG. <b>3</b></figref> describes system <b>300</b> with respect to VMs, other compute entities may also be used with system <b>300</b> in a similar manner. In addition, VM scheduler <b>302</b> may be configured to handle requests from compute entities included in an edge-compute device, on-premises servers, or other types of systems, including communications systems, such as base stations (e.g., 5G or 6G base stations).</p><p id="p-0030" num="0029">ML system <b>304</b> may be configured to provide resource predictions, including predicted memory usage, to VM scheduler <b>302</b>. A request to deploy a set of VMs may arrive at VM scheduler <b>302</b>. In this example, VM scheduler <b>302</b> may then send a query to ML system <b>304</b> to request predicted memory usage. ML system <b>304</b> may provide memory usage predictions. Using these predictions, VM scheduler <b>302</b> may allocate memory resources to VMs, Alternatively, VM scheduler <b>302</b> may have been provided memory usage predictions or other memory usage information such that the VM scheduler <b>302</b> need not query ML system <b>304</b> every time a VM needs to be placed or scheduled for execution. As explained earlier with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the allocated memory may be a combination of physical memory (e.g., local DRAM) and a mapping to a logical pooled memory.</p><p id="p-0031" num="0030">With continued reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, ML system <b>304</b> may include several components having instructions configured to perform various functions associated with ML system <b>304</b>. ML system <b>304</b> may include both offline training components and online prediction components. Offline training components may be responsible for training of the various machine language models, validating the models, and publishing the validated models. Online prediction components may generate predictions concerning various aspects, including predictions related to memory usage by VMs and any behavior patterns associated with the VMs. Prior to training the ML models, features that allow the ML models to predict a metric based on the inputs may be selected. The training phase may include using backward propagation, or other techniques, allowing the ML models to learn a relationship between certain input parameters and certain predictions based on the parameters. As an example, neural network models trained using stochastic gradient descent may be used to predict memory usage for a compute entity, such as a VM. Other types of ML models, including Bayesian models may be used.</p><p id="p-0032" num="0031">In general, one may implement a supervised learning algorithm that can be trained based on input data and once it is trained it can make predictions or prescriptions based on the training. Any of the learning and inference techniques such as Linear Regression, Support Vector Machine (SVM) set up for regression, Random Forest set up for regression, Gradient-boosting trees set up for regression and neural networks may be used. Linear regression may include modeling the past relationship between independent variables and dependent output variables. Neural networks may include artificial neurons used to create an input layer, one or more hidden layers, and an output layer. Each layer may be encoded as matrices or vectors of weights expressed in the form of coefficients or constants that might have been obtained via off-line training of the neural network. Neural networks may be implemented as Recurrent Neural Networks (RNNs), Long Short Term Memory (LSTM) neural networks, or Gated Recurrent Unit (GRUs). All of the information required by a supervised learning-based model may be translated into vector representations corresponding to any of these techniques.</p><p id="p-0033" num="0032">Taking the LSTM example, an LSTM network may comprise a sequence of repeating RNN layers or other types of layers. Each layer of the LSTM network may consume an input at a given time step, e.g., a layer's state from a previous time step, and may produce a new set of outputs or states. In the case of using the LSTM, a single chunk of content may be encoded into a single vector or multiple vectors. As an example, a word or a combination of words (e.g., a phrase, a sentence, or a paragraph) may be encoded as a single vector. Each chunk may be encoded into an individual layer (e.g., a particular time step) of an LSTM network. An LSTM layer may be described using a set of equations, such as the ones below:</p><p id="p-0034" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><sub>t</sub>=&#x3c3;(<i>W</i><sub>xi</sub><i>+W</i><sub>hi</sub><i>h</i><sub>t&#x2212;1</sub><i>+W</i><sub>ci</sub><i>c</i><sub>t&#x2212;1</sub><i>+b</i><sub>i </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0035" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>f</i><sub>t</sub>=&#x3c3;(<i>W</i><sub>xf</sub><i>x</i><sub>t</sub><i>+W</i><sub>hf</sub><i>h</i><sub>t&#x2212;1</sub><i>+W</i><sub>cf</sub><i>c</i><sub>t&#x2212;1</sub><i>+b</i><sub>f</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0036" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>c</i><sub>t</sub><i>=f</i><sub>t</sub><i>c</i><sub>t&#x2212;1</sub><i>i</i><sub>t</sub>tanh (<i>W</i><sub>xc</sub><i>x</i><sub>i</sub><i>+W</i><sub>hc</sub><i>h</i><sub>t&#x2212;1</sub><i>+b</i><sub>c</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0037" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>o</i><sub>t</sub>=&#x3c3;(<i>W</i><sub>xo</sub><i>t+W</i><sub>ho</sub><i>h</i><sub>t&#x2212;1</sub><i>+W</i><sub>co</sub><i>c</i><sub>t</sub><i>+b</i><sub>o</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0038" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>h<sub>t</sub>o<sub>t </sub>tanh (c<sub>t</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0039" num="0033">In this example, inside each LSTM layer, the inputs and hidden states may be processed using a combination of vector operations (e.g., dot-product, inner product, or vector addition) or non-linear operations, if needed. The instructions corresponding to the machine learning system could be encoded as hardware corresponding to an Ail processor. hi this case, some or all of the functionality associated with ML system <b>304</b> may be hard-coded or otherwise provided as part of an All processor. As an example, an All processor may be implemented using an FPGA with the requisite functionality.</p><p id="p-0040" num="0034">In another example, the memory allocated to a VM may be dynamically changed or determined. As an example, the behavior change may correspond to the workloads served by the VM over time and that may be used to dynamically change the memory allocation. One example of the behavior change may relate to VMs that service web-searching workloads or other similar workloads. Such VMs may have diurnal patterns, such that the workloads require more resources during the day, but not so much during the night. In one example, using the Fast Fourier Transform (FFT) algorithm one may detect periodicity of the behavior associated with the VMs. The FFT may be used to detect periodicity at multiple time scales, but a VM's memory allocation may only be dynamically changed when the workloads handled by the VM has a periodicity consistent with certain patterns, e.g., diurnal patterns. The periodicity analysis may produce ground-truth labels that can also be used in training an ML model to predict that a VM will likely execute a diurnal pattern or another pattern. In another example other features (e.g., cloud subscription ID, the user who created the VM, VM type, VM size, and the guest operating system) may also be used to statically or dynamically change the extent of the physical memory and the pooled memory allocated to a VM.</p><p id="p-0041" num="0035">In yet another example, instead of predicting memory usage or determining behavior/sage patterns, the VMs may be allocated exclusively the local physical memory in response to a request for scheduling the VM. At a later time, as the local physical memory associated with the host server (or a collection of host servers) starts to reach a certain utilization level, the scheduled VM may be allocated a combination of the local physical memory and the mapped logical pooled memory. In this manner, memory may be dynamically allocated to a VM that is being currently executed.</p><p id="p-0042" num="0036">Still referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, hypervisor <b>320</b> may manage the virtual machines, including VMs <b>312</b>, <b>314</b>, <b>316</b>, and <b>318</b>. CPU-<b>0</b> <b>332</b> may include one or more processing cores and CPU-<b>1</b> <b>338</b> may include one or more processing cores. Instructions corresponding to hypervisor <b>320</b>, when executed by any of the processing cores, may provide the management functionality provided by hypervisor <b>320</b>. Hypervisor <b>320</b> may ensure that any threads associated with any given VM are scheduled only on the logical cores of its group. CPU-<b>0</b> <b>332</b> may be coupled to a local memory-<b>0</b> <b>340</b> and CPU-<b>1</b> <b>338</b> may be coupled to another local memory-<b>1</b> <b>342</b>. Each of local memory-<b>0</b> <b>340</b> and local memory-<b>1</b> <b>342</b> may include one or more memory modules. Examples of such memory modules include, but are not limited to, dual-in-line memory modules (DIMMs) or single-in-line memory modules (SIMMs). Any memory type that is a &#x201c;cacheable memory&#x201d; may be used to implement the memory modules. As used herein the term &#x201c;cacheable memory&#x201d; includes any type of memory where the data obtained via a read operation associated with the memory can be copied to a memory cache such that the next time the same data is accessed, it can be obtained from the memory cache, Memory may be dynamic random access memory (DRAM), flash memory, static random access memory (SRAM), phase change memory, magnetic random access memory, or any other type of memory technology that can allow the memory to act as a cacheable memory.</p><p id="p-0043" num="0037">With continued reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in this example, CPU-<b>0</b> <b>332</b> may be coupled to fabric manager <b>350</b> via a fabric interface-<b>0</b> <b>334</b> and CPU-<b>1</b> <b>338</b> may be coupled to fabric manager <b>350</b> via another fabric interface-<b>1</b> <b>336</b>. Fabric manager <b>350</b> may allow any of the VMs being executed by processing cores associated with CPU-<b>0</b> <b>332</b> and CPU-<b>1</b> <b>338</b> to access logical pooled memory system <b>360</b>. In one example, any of the CPUs associated with host server <b>310</b> may issue load or store instructions to the logical address space set up by hypervisor <b>320</b>. The load or store instructions may result in a read or write to a local memory (e.g., either local memory-<b>0</b> <b>240</b> or local memory-<b>1</b> <b>342</b> depending on the CPU issuing load or store instructions) or a read/write transaction associated with logical pooled memory system <b>360</b>. In one example, assuming the local memory is DRAM, any of the relevant double-data rate (DDR) protocols associated with accessing DRAMs may be used by the CPUs to read/write the local memory. In this example, a load or store instruction that results in a read/write to a memory module associated with logical pooled memory system <b>360</b> may be translated by fabric manager <b>350</b> (or another similar sub-system) into a transaction that is completed using any of the links coupling the VMs with memory associated with logical pooled memory system <b>360</b>. The read/write transactions may be executed using a protocol associated with fabric manager <b>350</b> and logical pooled memory system <b>360</b>. The memory modules associated with logical pooled memory system <b>360</b> when implemented as DRAM memory modules may be accessed by a controller associated with logical pooled memory system <b>360</b> using any of the relevant DDR protocols.</p><p id="p-0044" num="0038">In one example, fabric manager <b>350</b> may be implemented using a fabric manager based on the Compute Express Link (CXL) specification. In this example, memory modules associated with logical pooled memory system <b>360</b> may be configured as Type 3 CXL devices. The logical address space exposed by hypervisor <b>320</b> to the virtual machines may be at least a subset of the address range exposed by a controller associated with the CXL bus/links. Thus, as part of this example, transactions associated with CXL.io protocol, which is a PCIe-based non coherent I/O protocol, may be used to configure the memory devices and the links between the CPUs and the memory modules included in logical pooled memory system <b>360</b>. The CXL.io protocol may also be used by the CPUs associated with host server <b>310</b> in device discovery, enumeration, error reporting, and management. Alternatively, any other I/O protocol that supports such configuration transactions may also be used. The memory access to the memory modules may be handled via the transactions associated with CXL.mem protocol, which is a memory access protocol that supports memory transactions. As an example, load instructions and store instructions associated with any of the CPUs of host server <b>310</b> may be handled via CXL.mem protocol. Alternatively, any other protocols that allow the translation of the CPU load/store instructions into read/write transactions associated with memory modules included in logical pooled memory system <b>360</b> may also be used. Although <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows host server <b>310</b> including certain components arranged in a certain manner, host server <b>310</b> may include additional or fewer components, arranged differently. As an example, although <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows local memory coupled to the CPUs, no local memory may be coupled to the CPUs. Instead, the CPUs may access memory associated with logical pooled memory system <b>360</b> only. In addition, although <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows the VMs hosted by a single host server, VMs <b>312</b>, <b>314</b>, <b>316</b>, and <b>318</b> may be hosted using a group of host servers (e.g., a cluster of host servers in a data center). Logical pooled memory system <b>360</b> may be shared across the host servers or may be dedicated for each host server.</p><p id="p-0045" num="0039"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of a logical pooled memory system <b>400</b> configured to allow for oversubscription of memory in accordance with one example. In one example, logical pooled memory system <b>360</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be implemented as logical pooled memory system <b>400</b>. In this example, logical pooled memory system <b>400</b> may include logical pooled memory <b>410</b>, a logical pooled memory controller <b>450</b>, and pooled memory control structures <b>480</b>. Logical pooled memory <b>410</b> may include physical pooled memory <b>412</b> and known-pattern pages <b>414</b>. Physical pooled memory <b>412</b> may include memory modules with physical memory (e.g., DRAM). Physical pooled memory <b>412</b> may correspond to a logical address space. Known-pattern pages <b>414</b> may represent the logical address space that is not backed by any physical memory. Logical pooled memory controller <b>450</b> may manage access to logical pooled memory <b>410</b> by interacting with pooled memory data structures <b>480</b>. Logical pooled memory controller <b>450</b> may be implemented as any combination of hardware, firmware, or software instructions. Instructions corresponding to logical pooled memory controller <b>450</b> may be stored in a memory associated with logical pooled memory system <b>400</b>. Such instructions when executed by a processor associated with logical pooled memory system <b>400</b> may provide at least some of the functionality associated with logical pooled memory controller <b>450</b>. Other functionality may be provided by the control logic associated with logical pooled memory controller <b>450</b>, including finite state machines and other logic.</p><p id="p-0046" num="0040">In this example, pooled memory control structures <b>480</b> may include a mapping table <b>482</b> and a physical memory free pages list <b>484</b>. Both mapping table <b>482</b> and physical memory free pages list <b>484</b> may be stored in physical pooled memory <b>412</b> or another memory (e.g., a cache associated with logical pooled memory controller <b>450</b>). Mapping table <b>482</b> may maintain a mapping between the logical address space and the physical address space associated with physical pooled memory <b>412</b>. Mapping table <b>482</b> may also keep track of the status of pages in terms of whether a page is a known-pattern page or not. A known-pattern page may not have any allocated physical space as part of physical pooled memory <b>412</b>. In this example, the physical memory addresses associated with known-pattern pages are considered invalid (identified as not-applicable (N/A) in mapping table <b>482</b>. If compression is used to free up additional space in physical pooled memory <b>412</b>, then it can also be tracked using mapping table <b>482</b>. Table 1 below shows an example mapping table <b>482</b>.</p><p id="p-0047" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="35pt" align="center"/><colspec colname="2" colwidth="56pt" align="center"/><colspec colname="3" colwidth="49pt" align="center"/><colspec colname="4" colwidth="63pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="4" rowsep="1">TABLE 1</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry/><entry>KNOWN-</entry><entry/><entry>PHYSICAL </entry></row><row><entry/><entry>PAGE</entry><entry>PATTERN</entry><entry/><entry>MEMORY</entry></row><row><entry/><entry>NUMBER</entry><entry>PAGE</entry><entry>COMPRESSED</entry><entry>ADDRESS</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="35pt" align="center"/><colspec colname="2" colwidth="56pt" align="center"/><colspec colname="3" colwidth="49pt" align="char" char="."/><colspec colname="4" colwidth="63pt" align="left"/><tbody valign="top"><row><entry/><entry>0</entry><entry>1</entry><entry>0</entry><entry>N/A</entry></row><row><entry/><entry>1</entry><entry>0</entry><entry>0</entry><entry>ADDR-10</entry></row><row><entry/><entry>2</entry><entry>1</entry><entry>0</entry><entry>N/A</entry></row><row><entry/><entry>3</entry><entry>0</entry><entry>1</entry><entry>ADDR-112</entry></row><row><entry/><entry>.</entry><entry>.</entry><entry>0</entry><entry>.</entry></row><row><entry/><entry>.</entry><entry>.</entry><entry/><entry>.</entry></row><row><entry/><entry>.</entry><entry>.</entry><entry/><entry>.</entry></row><row><entry/><entry>4095&#x2003;&#x2002;</entry><entry>0</entry><entry>0</entry><entry>ADDR-515</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0048" num="0041">As shown above in Table 1, mapping table <b>482</b> may be indexed by page numbers. A known-pattern page indicator (e.g., KNOWN-PATTERN PAGE flag) may be associated with each page. A logic high value (e.g., 1) may indicate to logical pooled memory controller <b>450</b> that no physical memory space has been allocated in physical pooled memory <b>412</b> corresponding to the page number. A logic low value (e.g., 0) may indicate to logical pooled memory controller <b>450</b> that physical memory space has been allocated in physical pooled memory <b>412</b> corresponding to the page number. Logical pooled memory controller <b>450</b> may modify the known-pattern page indicator based on any status change associated with a page number. At the point of scheduling or creation of a VM, the hypervisor (or the host operating system) may map some of the memory for the VM to the physical memory associated with the host server and rest to logical pooled memory <b>410</b>. At this time, the hypervisor (or the operating system) may issue a command to logical pooled memory controller <b>450</b> to set the known-pattern page indicator to a logic high value (e.g., 1) for the pages associated with the logical pooled memory allocated to the VM. The VM may also write a known-pattern to the logical pooled memory allocated to it at the time the VM is initialized. Logical pooled memory controller <b>450</b> may detect the write operation that is writing a known pattern as an indication that no physical memory needs to be allocated to the VM at this point. In addition, logical pooled memory controller <b>450</b> may ensure that the known-pattern page indicator for the corresponding pages is accurately reflected in mapping table <b>482</b>.</p><p id="p-0049" num="0042">At the outset, logical pooled memory controller <b>450</b> may map all of the logical pooled memory for the VM to known-pattern pages <b>414</b>. A newly scheduled VM may also work with the hypervisor to ensure that any pages corresponding to logical pooled memory <b>410</b> allocated to the VM are identified as known-pattern pages in mapping table <b>482</b>. At a later time, when the VM has used up all of the local physical memory allocated to it and it starts writing a not known-pattern (e.g., non-zeros) to the logical memory address space, then the logical pooled memory controller <b>450</b> may start allocating pages from within the logical address space (exposed by logical pooled memory controller <b>450</b>) corresponding to physical pooled memory <b>412</b> to the VM. Physical memory free pages list <b>484</b> may include a list of pages corresponding to physical pooled memory <b>412</b> that are free and are not mapped by mapping table <b>482</b>. Table <b>2</b> below shows an example physical memory free pages list <b>484</b>. This example lists the pages corresponding to physical pooled memory <b>412</b> that are free to be allocated.</p><p id="p-0050" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>PHYSICAL MEMORY FREE LIST</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>1024</entry></row><row><entry>1025</entry></row><row><entry>1029</entry></row><row><entry>1065</entry></row><row><entry>.</entry></row><row><entry>.</entry></row><row><entry>.</entry></row><row><entry>4055</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0051" num="0043">Although the address translation is described as a single-level address translation, the address translation may be a multi-level address translation, and thus, it may require the use of additional mapping tables. In any case, logical pool memory controller <b>450</b> may be configured to maintain the mapping tables and ensure proper tracking of the known-pattern pages to allow for the pooling and the oversubscription of memory allocated to VMs, as described earlier. Moreover, although mapping table <b>482</b> is described in relation to pages, other groupings of memory, including memory blocks, may also be used with mapping table <b>482</b>. Also, the functionality of mapping table <b>482</b> may be implemented using registers or other similar hardware structures associated with processors and controllers. In addition, although mapping table <b>482</b> is described as being stored in physical pooled memory <b>412</b>, at least a portion of the mapping table may be stored in a cache associated with logical pooled memory controller <b>450</b> to speed up the address translation process.</p><p id="p-0052" num="0044"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a diagram of an implementation of the mapping table using a translation look-aside buffer (TLB) <b>530</b> in accordance with one example. As explained earlier with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, mapping table <b>482</b> may be used by logical pooled memory controller <b>450</b> to translate logical addresses received from a hypervisor (or another source) into physical addresses associated with physical pooled memory <b>412</b>. In this example, logical pooled memory controller <b>520</b> may include a cache implemented as a translation lookaside buffer (TLB) <b>530</b>. TLB <b>530</b> may be implemented as a content-addressable memory (CAM), a fully-associative cache, a two-way set-associative cache, a direct mapped cache, or another type of suitable cache. TLB <b>530</b> may be implemented to accomplish both for translation of the logical pooled addresses to the physical pooled memory addresses and for checking the known-pattern page status of the pages. For each address translation request, logical pooled memory controller <b>520</b> may first check TLB <b>530</b> to determine if the address translation is already cached. If the address translation is found cached in TLB <b>530</b>, a TLB HIT signal may be generated. In addition, the physical address corresponding to the page number that resulted in the TLB HIT signal may be provided to physical pooled memory <b>560</b> to allow for access to the data or instructions stored at the physical address. Alternatively, if the address translation had not been cached, the TLB MISS signal may be generated. This, in turn, may result in a table walk through mapping table <b>562</b> unless the known-pattern page flag is set to a logical true value. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, physical memory free pages list <b>564</b> may also be stored in physical pooled memory <b>560</b>.</p><p id="p-0053" num="0045"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a block diagram of a system <b>600</b> including multiple host servers coupled with a logical pooled memory system <b>670</b> configured to allow for oversubscription of memory. System <b>600</b> may include host server <b>610</b>, host server <b>620</b>, and host server <b>630</b> coupled to a logical pooled memory system <b>670</b>. As explained earlier, each host server may host multiple compute entities (e.g., VMs), which in turn may have been purchased or otherwise paid for by different tenants of system <b>600</b>. Each server may include local memory (e.g. memory modules described with respect to host server <b>310</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>) and may have access to logical pooled memory, which may be a portion of the logical pooled memory included in logical pooled memory system <b>670</b>. As described earlier, a fabric manager (implemented based on the CXL specification) may expose a subset of the logical pooled memory associated with logical pooled memory system <b>670</b> to each of hosts <b>610</b>, <b>620</b>, and <b>630</b>. Logical pooled memory system <b>670</b> may be implemented in a similar manner as described earlier.</p><p id="p-0054" num="0046">Each of the compute entities that has been scheduled or otherwise made available via host server <b>610</b> to a tenant may be allocated a portion of local memory <b>612</b> and a portion of pooled memory <b>614</b>. As described earlier the allocation to each compute entity may be based on an oversubscription to the logical pooled memory managed by logical pooled memory system <b>670</b>. In this example, each compute entity hosted by host server <b>610</b> may be coupled via a link <b>616</b> (or a collection of links) to logical pooled memory system <b>670</b>. Each of the compute entities that has been scheduled or otherwise made available via host server <b>620</b> to a tenant may be allocated a portion of local memory <b>622</b> and a portion of pooled memory <b>624</b>. As described earlier the allocation to each compute entity hosted by host server <b>620</b> may be based on an oversubscription to the logical pooled memory managed by logical pooled memory system <b>670</b>. In this example, each compute entity hosted by host server <b>620</b> may be coupled via a link <b>626</b> (or a collection of links) to logical pooled memory system <b>670</b>. Each of the compute entities that has been scheduled or otherwise made available via host server <b>630</b> to a tenant may be allocated a portion of local memory <b>632</b> and a portion of pooled memory <b>634</b>. As described earlier the allocation to each compute entity hosted by host server <b>630</b> may be based on an oversubscription to the logical pooled memory managed by logical pooled memory system <b>670</b>. In this example, each compute entity hosted by host server <b>630</b> may be coupled via a link <b>636</b> (or a collection of links) to logical pooled memory system <b>670</b>. As an example, load and store instructions associated with any of the CPUs of each of host servers <b>610</b>, <b>620</b>, and <b>630</b> may be handled via CXL.mem protocol.</p><p id="p-0055" num="0047">Alternatively, any other protocols that allow the translation of the CPU load/store instructions into read/write transactions associated with memory modules included in logical pooled memory system <b>670</b> may also be used. Although <figref idref="DRAWINGS">FIG. <b>6</b></figref> shows system <b>600</b> as including certain components arranged in a certain manner, system <b>600</b> may include additional or fewer components, arranged differently.</p><p id="p-0056" num="0048"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a block diagram of a system <b>700</b> for implementing at least some of the methods for allowing oversubscription of the pooled memory allocated to compute entities. System <b>700</b> may include processor(s) <b>702</b>, I/O component(s) <b>704</b>, memory <b>706</b>, presentation component(s) <b>708</b>, sensors <b>710</b>, database(s) <b>712</b>, networking interfaces <b>714</b>, and I/O port(s) <b>716</b>, which may be interconnected via bus <b>720</b>. Processor(s) <b>702</b> may execute instructions stored in memory <b>706</b>. I/O component(s) <b>704</b> may include components such as a keyboard, a mouse, a voice recognition processor, or touch screens. Memory <b>706</b> may be any combination of non-volatile storage or volatile storage (e.g., flash memory, DRAM, SRAM, or other types of memories). Presentation component(s) <b>708</b> may include displays, holographic devices, or other presentation devices. Displays may be any type of display, such as LCD, LED, or other types of display. Sensor(s) <b>710</b> may include telemetry or other types of sensors configured to detect, and/or receive, information (e.g., collected data). Sensor(s) <b>710</b> may include telemetry or other types of sensors configured to detect, and/or receive, information (e.g., memory usage by various compute entities being executed by host servers in a data center). Sensor(s) <b>710</b> may include sensors configured to sense conditions associated with CPUs, memory or other storage components, FPGAs, motherboards, baseboard management controllers, or the like. Sensor(s) <b>710</b> may also include sensors configured to sense conditions associated with racks, chassis, fans, power supply units (PSUs), or the like. Sensor(s) <b>710</b> may also include sensors configured to sense conditions associated with Network Interface Controllers (NICs), Top-of-Rack (TOR) switches, Middle-of-Rack (MOR) switches, routers, power distribution units (PDUs), rack level uninterrupted power supply (UPS) systems, or the like.</p><p id="p-0057" num="0049">Still referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, database(s) <b>712</b> may be used to store any of the data collected or logged and as needed for the performance of methods described herein. Database(s) <b>712</b> may be implemented as a collection of distributed databases or as a single database. Network interface(s) <b>714</b> may include communication interfaces, such as Ethernet, cellular radio, Bluetooth radio, UWB radio, or other types of wireless or wired communication interfaces. I/O port(s) <b>716</b> may include Ethernet ports, Fiber-optic ports, wireless ports, or other communication or diagnostic ports. Although Fla <b>7</b> shows system <b>700</b> as including a certain number of components arranged and coupled in a certain way, it may include fewer or additional components arranged and coupled differently. In addition, the functionality associated with system <b>700</b> may be distributed, as needed.</p><p id="p-0058" num="0050"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a flow chart <b>800</b> of an example method for oversubscribing physical memory for use with compute entities. In one example, steps associated with this method may be executed by various components of the systems described earlier. Step <b>810</b> may include allocating a portion of a memory associated with a system to a compute entity, where the portion of the memory comprises a combination of a portion of a first physical memory of a first type and a portion of a logical pooled memory associated with the system. The logical pooled memory is mapped to a second physical memory of the first type and an amount of the logical pooled memory indicated as being available for allocation to the plurality of compute entities is greater than an amount of the second physical memory of the first type, As described earlier with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a virtual machine, which is one type of compute entity, may be allocated 19 GB of physical memory (e.g., memory modules, including DRAM configured as local memory for the CPUs) and 13 GB of logical pooled memory, which may be part of logical pooled memory <b>260</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> including physical pooled memory <b>262</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, which also may be memory modules, including DRAM. As used herein the term &#x201c;type&#x201d; used in the phrase &#x201c;first type&#x201d; refers to whether the memory is a cacheable memory type or not. As described earlier, the term &#x201c;cacheable memory&#x201d; includes any type of memory where the data obtained via a read operation associated with the memory can be copied to a memory cache such that the next time the same data is accessed, it can be obtained from the memory cache. Memory may be dynamic random access memory (DRAM), flash memory, static random access memory (SRAM), phase change memory, magnetic random access memory, or any other type of memory technology that can allow the memory to act as a cacheable memory. In sum, the first physical memory of the first type and the second physical memory of the first type need to be of the cacheable type, but they could be different type in other ways (e.g., DRAM vs. phase change memory or SRAM vs. DRAM).</p><p id="p-0059" num="0051">Step <b>820</b> may include indicating to a logical pooled memory controller associated with the logical pooled memory that all pages associated with the logical pooled memory initially allocated to any of the plurality of compute entities are known-pattern pages. As an example, a hypervisor (e.g., hypervisor <b>320</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>) may indicate to the logical pooled memory controller (e.g., logical pooled memory controller <b>450</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) the status of initially allocated page as known-pattern pages (e.g., known-pattern pages <b>414</b>).</p><p id="p-0060" num="0052">Step <b>830</b> may include the logical pooled memory controller tracking both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is a known-pattern page and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities. In this example, as part of this step, as explained earlier with respect to FIGs, <b>4</b> and <b>5</b>, logical pooled memory controller (e.g., logical pooled memory controller <b>450</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) may track the status of pages (a known-pattern page or not) and the relationship between page numbers and physical addresses in the physical memory (e.g., physical pooled memory <b>412</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) using a mapping table (e.g., mapping table <b>482</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>). As explained earlier, to speed up the access to the logical pooled memory at least a portion of the mapping table may be implemented as a translation lookaside buffer. In addition, the mapping table (or a similar structure) may be implemented using registers.</p><p id="p-0061" num="0053">Step <b>840</b> may include in response to a write operation initiated by the compute entity, the logical pooled memory controller allowing writing of the data to any available space in the second physical memory of the first type only up to an extent of physical memory that corresponds to the portion of the logical pooled memory previously allocated to the compute entity. As explained earlier, when a compute entity (e.g., a VM) has used up all of the local physical memory allocated to it and it starts writing an unknown pattern to the logical memory address space, then the hypervisor may start allocating pages from within the logical address space (exposed by logical pooled memory controller <b>450</b>) corresponding to physical memory (e.g., physical pooled memory <b>412</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) to the compute entity. The logical pooled memory controller may access a free pages list (e.g., physical memory free pages list <b>484</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) to access pages that are free and are not mapped by the mapping table. In addition, as explained earlier, the compute entity may not be allocated more physical memory managed by the logical pooled memory controller than previously allocated to the compute entity (e.g., 13 GB in the example described with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>). This method may allow for the oversubscription of the installed memory in the system by the compute entities since more compute entities can be supported by the system without having to deploy additional physical memory (e.g., DRAM).</p><p id="p-0062" num="0054"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a flow chart <b>900</b> of another example method for oversubscribing physical memory for use with compute entities. In one example, steps associated with this method may be executed by various components of the systems described earlier. Step <b>910</b> may include allocating a portion of a memory associated with a system to a compute entity, where the portion of the memory comprises a combination of a portion of a first physical memory of a first type and a portion of a logical pooled memory associated with the system. The logical pooled memory may be mapped to a second physical pooled memory of the first type and an amount of the logical pooled memory indicated as being available for allocation to the plurality of compute entities may be greater than an amount of the second physical pooled memory of the first type. As described earlier with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a virtual machine, which is one type of compute entity, may be allocated 19 GB of physical memory (e.g., memory modules, including DRAM configured as local memory for the CPUs) and 13 GB of logical pooled memory, which may be part of logical pooled memory <b>260</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> including physical pooled memory <b>262</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, which also may be memory modules including DRAM. As an example, the logical pooled memory may be logical pooled memory included as part of logical pooled memory system <b>670</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref> and the physical pooled memory may correspond to physical memory (e.g., DRAM) included as part of logical pooled memory system <b>670</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0063" num="0055">Step <b>920</b> may include translating any load or store instructions directed to the logical pooled memory into memory transactions for completion via a respective link between each of the plurality of compute entities and each of physical memory devices included as part of the second physical pooled memory of the first type. In one example, as explained earlier the fabric manager (e.g., fabric manager <b>350</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> or fabric manager <b>650</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>) implemented using a fabric manager based on the Compute Express Link (CXL) specification may translate the load and store instructions into transactions associated with the CXL specification. The logical address space exposed to the compute entities may be at least a subset of the address range exposed by a controller associated with the CXL bus/links.</p><p id="p-0064" num="0056">Step <b>930</b> may include indicating to a logical pooled memory controller associated with the logical pooled memory that all pages associated with the logical pooled memory initially allocated to any of the plurality of compute entities are known-pattern pages. As an example, a hypervisor (e.g., hypervisor <b>320</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> or a hypervisor associated with any of host servers <b>610</b>, <b>620</b>, and <b>630</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>) may indicate to the logical pooled memory controller (e.g., logical pooled memory controller <b>450</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) the status of initially allocated page as known-pattern pages (e.g., known-pattern pages <b>414</b>). A combination of hypervisors may also coordinate with each other to manage memory allocation to compute entities being supported by a computing system or a communication system having multiple hypervisors managing a shared logical pooled memory.</p><p id="p-0065" num="0057">Step <b>940</b> may include the logical pooled memory controller tracking both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is a known-pattern page and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities. In this example, as part of this step, as explained earlier with respect to <figref idref="DRAWINGS">FIGS. <b>4</b>-<b>6</b></figref>, logical pooled memory controller (e.g., logical pooled memory controller <b>450</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) may track the status of pages (a known-pattern page or not) and the relationship between page numbers and physical addresses in the physical memory (e.g., physical pooled memory <b>412</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) using a mapping table (e.g., mapping table <b>482</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>). As explained earlier, to speed up the access to the logical pooled memory at least a portion of the mapping table may be implemented as a translation lookaside buffer. In addition, the mapping table (or a similar structure) may be implemented using registers.</p><p id="p-0066" num="0058">Step <b>950</b> may include in response to a write operation initiated by the compute entity to write any data other than a known pattern, the logical pooled memory controller allowing writing of the data to any available space in the second physical pooled memory of the first type only up to an extent of physical memory that corresponds to the portion of the logical pooled memory previously allocated to the compute entity. As explained earlier, when a compute entity (e.g., a VM) has used up all of the local physical memory allocated to it and it starts writing an unknown pattern to the logical memory address space, then the hypervisor may start allocating pages from within the logical address space (exposed by logical pooled memory controller <b>450</b>) corresponding to physical memory (e.g., physical pooled memory <b>412</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> or physical pooled memory associated with logical pooled memory system <b>670</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>) to the compute entity. The logical pooled memory controller may access a free pages list (e.g., physical memory free pages list <b>484</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) to access pages that are free and are not mapped by the mapping table. In addition, as explained earlier, the compute entity may not be allocated more physical memory managed by the logical pooled memory controller than previously allocated to the compute entity (e.g., 13 GB in the example described with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>). This method may also allow for the oversubscription of the installed pooled memory in the system by the compute entities since more compute entities can be supported by the system without having to deploy additional physical memory (e.g., DRAM).</p><p id="p-0067" num="0059">In conclusion, the present disclosure relates to a method comprising allocating a portion of a memory associated with a system to a compute entity, where the portion of the memory comprises a combination of a portion of a first physical memory of a first type and a portion of a logical pooled memory for use with a plurality of compute entities associated with the system. The logical pooled memory may be mapped to a second physical memory of the first type, and where an amount of the logical pooled memory indicated as being available for allocation to the plurality of compute entities may be greater than an amount of the second physical memory of the first type. The method may further include indicating to a logical pooled memory controller associated with the logical pooled memory that all pages associated with the logical pooled memory initially allocated to any of the plurality of compute entities are known-pattern pages. The method may further include the logical pooled memory controller tracking both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is a known-pattern page and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities. The method may further include in response to a write operation initiated by the compute entity the logical pooled memory controller allowing writing of the data to any available space in the second physical memory of the first type only up to an extent of physical memory that corresponds to the portion of the logical pooled memory previously allocated to the compute entity.</p><p id="p-0068" num="0060">In this method, an amount of the logical pooled memory associated with the system may equal an amount of the second physical memory of the first type combined with an amount of memory corresponding to known-pattern pages backed by no physical memory of any type. The tracking both the status of whether the page of the logical pooled memory allocated to any of the plurality of compute entities is a known-pattern page and a relationship between the logical memory addresses and the physical memory addresses may comprise maintaining a mapping table. In one example, the mapping table may be implemented using a translation lookaside buffer.</p><p id="p-0069" num="0061">The method may further include allocating the portion of the first physical memory of the first type based on a predicted use of the first physical memory of the first type by the compute entity. The method may also comprise dynamically changing an amount of the portion of the first physical memory of the first type allocated to the compute entity based on a usage pattern associated with a use of the first physical memory of the first type allocated to the compute entity.</p><p id="p-0070" num="0062">The method may further include exposing the portion of the logical pooled memory allocated to the compute entity via a software mechanism to allow the compute entity to distinguish between the portion of the logical pooled memory allocated to the compute entity and the portion of the first physical memory of the first type allocated to the compute entity. The logical pooled memory may be coupled to a processor for executing any of the plurality of compute entities that have been allocated at least a portion of the logical pooled memory associated with the system via a respective link managed by a fabric manager.</p><p id="p-0071" num="0063">In another example, the present disclosure relates to a system comprising a memory for use with a plurality of compute entities, where a portion of the memory comprises a combination of a portion of a first physical memory of a first type and a portion of a logical pooled memory associated with the system. The logical pooled memory may be mapped to a second physical memory of the first type and an amount of the logical pooled memory indicated as being available for allocation to the plurality of compute entities may be greater than an amount of the second physical memory of the first type. The system may further include a logical pooled memory controller, coupled to the logical pooled memory, configured to: (1) track both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is a known-pattern page and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities, and (2) in response to a write operation initiated by the compute entity to write any data other than a known-pattern, allow the write operation to write the data to any available space in the second physical memory of the first type only up to an extent of physical memory that corresponds to the portion of the logical pooled memory previously allocated to the compute entity.</p><p id="p-0072" num="0064">As part of the system, an amount of the logical pooled memory associated with the system may equal an amount of the second physical memory of the first type combined with an amount of memory corresponding to known-pattern pages backed by no physical memory of any type. The logical pooled memory controller may further be configured to maintain a mapping table to track both the status of whether the page of the logical pooled memory allocated to any of the plurality of compute entities is the known-pattern page and the relationship between the logical memory addresses and the physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities. The mapping table may be implemented using a translation lookaside buffer.</p><p id="p-0073" num="0065">As part of the system, in one example, a scheduler may be configured to allocate the portion of the first physical memory of the first type based on a predicted use of the first physical memory of the first type by the compute entity. The scheduler may also be configured to dynamically change an amount of the portion of the first physical memory of the first type allocated to the compute entity based on a usage pattern associated with a use of the first physical memory of the first type allocated to the compute entity. The logical pooled memory controller may further be configured to indicate to each of the plurality of compute entities that all pages associated with the logical pooled memory initially allocated to any of the plurality of compute entities are known-pattern pages.</p><p id="p-0074" num="0066">In yet another example, the present disclosure relates to a system including a plurality of host servers configurable to execute one or more of a plurality of compute entities. The system may further include a memory, where a portion of the memory comprises a combination of a portion of a first physical memory of a first type and a portion of a logical pooled memory shared among the plurality of host servers. The logical pooled memory may be mapped to a second physical memory of the first type and an amount of the logical pooled memory indicated as being available for allocation to the plurality of compute entities may be greater than an amount of the second physical memory of the first type. The system may further include a logical pooled memory controller, coupled to the logical pooled memory associated with the system, configured to: (1) track both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is a known-pattern page and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities, and (2) in response to a write operation initiated by a compute entity, being executed by a processor associated with any of the plurality of host servers, to write any data other than a known pattern, allow the write operation to write the data to any available space in the second physical memory of the first type only up to an extent of physical memory that corresponds to the portion of the logical pooled memory previously allocated to the compute entity.</p><p id="p-0075" num="0067">As part of the system, an amount of the logical pooled memory associated with the system may equal an amount of the second physical memory of the first type combined with an amount of memory corresponding to known-pattern pages backed by no physical memory of any type. The logical pooled memory controller may further be configured to maintain a mapping table to track both the status of whether the page of the logical pooled memory allocated to any of the plurality of compute entities is the known-pattern page and the relationship between the logical memory addresses and the physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities, The mapping table may be implemented using a translation lookaside buffer.</p><p id="p-0076" num="0068">As part of the system, in one example, a scheduler may be configured to allocate the portion of the first physical memory of the first type based on a predicted use of the first physical memory of the first type by the compute entity. The scheduler may also be configured to dynamically change an amount of the portion of the first physical memory of the first type allocated to the compute entity based on a usage pattern associated with a use of the first physical memory of the first type allocated to the compute entity. The logical pooled memory controller may further be configured to indicate to each of the plurality of compute entities that all pages associated with the logical pooled memory initially allocated to any of the plurality of compute entities are known-pattern pages.</p><p id="p-0077" num="0069">It is to be understood that the methods, modules, and components depicted herein are merely exemplary. Alternatively, or in addition, the functionality described herein can be performed, at least in part, by one or more hardware logic components. For example, and without limitation, illustrative types of hardware logic components that can be used include Field-Programmable Gate Arrays (FPGAs), Application-Specific Integrated Circuits (ASICs), Application-Specific Standard Products (ASSPs), System-on-a-Chip systems (SOCs), Complex Programmable Logic Devices (CPLDs), etc, In an abstract, but still definite sense, any arrangement of components to achieve the same functionality is effectively &#x201c;associated&#x201d; such that the desired functionality is achieved. Hence, any two components herein combined to achieve a particular functionality can be seen as &#x201c;associated with&#x201d; each other such that the desired functionality is achieved, irrespective of architectures or inter-medial components. Likewise, any two components so associated can also be viewed as being &#x201c;operably connected,&#x201d; or &#x201c;coupled,&#x201d; to each other to achieve the desired functionality. Merely because a component, which may be an apparatus, a structure, a system, or any other implementation of a functionality, is described herein as being coupled to another component does not mean that the components are necessarily separate components. As an example, a component A described as being coupled to another component B may be a sub-component of the component B, the component B may be a sub-component of the component A, or components A and B may be a combined sub-component of another component C.</p><p id="p-0078" num="0070">The functionality associated with some examples described in this disclosure can also include instructions stored in a non-transitory media. The term &#x201c;non-transitory media&#x201d; as used herein refers to any media storing data and/or instructions that cause a machine to operate in a specific manner. Exemplary non-transitory media include non-volatile media and/or volatile media. Non-volatile media include, for example, a hard disk, a solid-state drive, a magnetic disk or tape, an optical disk or tape, a flash memory, an EPROM, NVRAM, PRAM, or other such media, or networked versions of such media. Volatile media include, for example, dynamic memory such as DRAM, SRAM, a cache, or other such media, Non-transitory media is distinct from, but can be used in conjunction with transmission media. Transmission media is used for transferring data and/or instruction to or from a machine. Exemplary transmission media include coaxial cables, fiber-optic cables, copper wires, and wireless media, such as radio waves.</p><p id="p-0079" num="0071">Furthermore, those skilled in the art will recognize that boundaries between the functionality of the above described operations are merely illustrative. The functionality of multiple operations may be combined into a single operation, and/or the functionality of a single operation may be distributed in additional operations. Moreover, alternative embodiments may include multiple instances of a particular operation, and the order of operations may be altered in various other embodiments.</p><p id="p-0080" num="0072">Although the disclosure provides specific examples, various modifications and changes can be made without departing from the scope of the disclosure as set forth in the claims below. Accordingly, the specification and figures are to be regarded in an illustrative rather than a restrictive sense, and all such modifications are intended to be included within the scope of the present disclosure. Any benefits, advantages, or solutions to problems that are described herein with regard to a specific example are not intended to be construed as a critical, required, or essential feature or element of any or all the claims.</p><p id="p-0081" num="0073">Furthermore, the terms &#x201c;a&#x201d; or &#x201c;an,&#x201d; as used herein, are defined as one or more than one. Also, the use of introductory phrases such as &#x201c;at least one&#x201d; and &#x201c;one or more&#x201d; in the claims should not be construed to imply that the introduction of another claim element by the indefinite articles &#x201c;a&#x201d; or &#x201c;an&#x201d; limits any particular claim containing such introduced claim element to inventions containing only one such element, even when the same claim includes the introductory phrases &#x201c;one or more&#x201d; or &#x201c;at least one&#x201d; and indefinite articles such as &#x201c;a&#x201d; or &#x201c;an&#x201d; The same holds true for the use of definite articles.</p><p id="p-0082" num="0074">Unless stated otherwise, terms such as &#x201c;first&#x201d; and &#x201c;second&#x201d; are used to arbitrarily distinguish between the elements such terms describe. Thus, these terms are not necessarily intended to indicate temporal or other prioritization of such elements.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed:</us-claim-statement><claims id="claims"><claim id="CLM-01-20" num="01-20"><claim-text><b>1</b>.-<b>20</b>. (canceled)</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A method comprising:<claim-text>allocating a portion of a memory associated with a system to a compute entity, wherein the portion of the memory comprises a combination of a portion of a first physical memory and a portion of a logical pooled memory for use with a plurality of compute entities associated with the system, wherein the logical pooled memory is mapped to a second physical memory, and wherein an amount of the logical pooled memory indicated as being available for allocation to the plurality of compute entities is greater than an amount of the second physical memory;</claim-text><claim-text>indicating to a logical pooled memory controller associated with the logical pooled memory that a set of pages associated with the logical pooled memory initially allocated to a subset of the plurality of compute entities are not backed by any of the second physical memory; and</claim-text><claim-text>the logical pooled memory controller tracking both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is backed by the second physical memory and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising, in response to a write operation initiated by the compute entity, the logical pooled memory controller allowing writing of data to any available space in the second physical memory of the first type only up to an extent of physical memory that corresponds to the portion of the logical pooled memory previously allocated to the compute entity,</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein tracking both the status of whether the page of the logical pooled memory allocated to any of the plurality of compute entities is backed by the second physical memory and the relationship between the logical memory addresses and the physical memory addresses comprises maintaining a mapping table.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the mapping table is implemented using a translation lookaside buffer.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising allocating the portion of the first physical memory of the first type based on a predicted use of the first physical memory of the type by the compute entity.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising dynamically changing an amount of the portion of the first physical memory of the first type allocated to the compute entity based on a usage pattern associated with a use of the first physical memory of the first type allocated to the compute entity.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising exposing the portion of the logical pooled memory allocated to the compute entity via a software mechanism to allow the compute entity to distinguish between the portion of the logical pooled memory allocated to the compute entity and the portion of the first physical memory of the first type allocated to the compute entity.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the logical pooled memory is coupled to a processor for executing any of the plurality of compute entities that have been allocated at least a portion of the logical pooled memory associated with the system via a respective link managed by a fabric manager,</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. A system comprising:<claim-text>a memory, wherein a portion of the memory comprises a combination of a portion of a first physical memory of a first type and a portion of a logical pooled memory associated with the system, wherein the logical pooled memory is mapped to a second physical memory of the first type, and wherein an amount of the logical pooled memory indicated as being available for allocation to a plurality of compute entities is greater than an amount of the second physical memory of the first type; and</claim-text><claim-text>a logical pooled memory controller, coupled to the logical pooled memory associated with the system, configured to: (1) track both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is backed by the second physical memory and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities, and (2) in response to a write operation initiated by a compute entity, allow the write operation to write the data to any available space in the second physical memory of the first type only up to an extent of physical memory that corresponds to a portion of the logical pooled memory previously allocated to the compute entity.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The system of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the logical pooled memory controller is further configured to maintain a mapping table to track both the status of whether the page of the logical pooled memory allocated to any of the plurality of compute entities is backed by the second physical memory and the relationship between the logical memory addresses and the physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The system of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the mapping table is implemented using a translation lookaside buffer.</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The system of <claim-ref idref="CLM-00029">claim 29</claim-ref>, further comprising a scheduler configured to allocate the portion of the first physical memory of the first type to the compute entity based on a predicted use of the first physical memory of the first type by the compute entity.</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The system of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the scheduler is configured to dynamically change an amount of the portion of the first physical memory of the first type allocated to the compute entity based on a usage pattern associated with a use of the first physical memory of the first type allocated to the compute entity.</claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The system of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the logical pooled memory controller is further configured to expose the portion of the logical pooled memory allocated to the compute entity via a software mechanism to allow the compute entity to distinguish between the portion of the logical pooled memory allocated to the compute entity and the portion of the first physical memory of the first type allocated to the compute entity.</claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. A system comprising:<claim-text>a plurality of host servers configurable to execute one or more of a plurality of compute entities;</claim-text><claim-text>a memory, wherein a portion of the memory comprises a combination of a portion of a first physical memory of a first type and a portion of a logical pooled memory associated with the system, wherein the logical pooled memory is mapped to a second physical memory of the first type, and wherein an amount of the logical pooled memory indicated as being available for allocation to the plurality of compute entities is greater than an amount of the second physical memory of the first type; and</claim-text><claim-text>a logical pooled memory controller, coupled to the logical pooled memory associated with the system, configured to: (1) track both a status of whether a page of the logical pooled memory allocated to any of the plurality of compute entities is backed by the second physical memory and a relationship between logical memory addresses and physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities, and (2) in response to a write operation initiated by a compute entity, allow the write operation to write the data to any available space in the second physical memory of the first type only up to an extent of physical memory that corresponds to a portion of the logical pooled memory previously allocated to the compute entity.</claim-text></claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. The system of <claim-ref idref="CLM-00035">claim 35</claim-ref>, wherein the logical pooled memory controller is further configured to maintain a mapping table to track both the status of whether the page of the logical pooled memory allocated to any of the plurality of compute entities is backed by the second physical memory and the relationship between the logical memory addresses and the physical memory addresses associated with any allocated logical pooled memory to any of the plurality of compute entities.</claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The system of claim <b>36</b>, wherein the mapping table is implemented using a translation lookaside buffer.</claim-text></claim><claim id="CLM-00038-1" num="00038-1"><claim-text><b>38</b>. The system of <claim-ref idref="CLM-00035">claim 35</claim-ref>, further comprising a scheduler configured to allocate the portion of the first physical memory of the first type to the compute entity based on a predicted use of the first physical memory of the first type by the compute entity.</claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. The system of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein the scheduler is configured to dynamically change an amount of the portion of the first physical memory of the first type allocated to the compute entity based on a usage pattern associated with a use of the first physical memory of the first type allocated to the compute entity,</claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. The system of <claim-ref idref="CLM-00039">claim 39</claim-ref>, wherein the logical pooled memory controller is further configured to expose the portion of the logical pooled memory allocated to the compute entity via a software mechanism to allow the compute entity to distinguish between the portion of the logical pooled memory allocated to the compute entity and the portion of the first physical memory of the first type allocated to the compute entity.</claim-text></claim></claims></us-patent-application>