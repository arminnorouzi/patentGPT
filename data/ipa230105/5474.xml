<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005475A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005475</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363063</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>M</subclass><main-group>3</main-group><subgroup>493</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>063</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>M</subclass><main-group>3</main-group><subgroup>4936</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>2015</main-group><subgroup>225</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>2015</main-group><subgroup>0638</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SOLUTION GUIDED RESPONSE GENERATION FOR DIALOG SYSTEMS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>International Business Machines Corporation</orgname><address><city>Armonk</city><state>NY</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>GUNASEKARA</last-name><first-name>Chulaka</first-name><address><city>New Hyde Park</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>GANHOTRA</last-name><first-name>Jatin</first-name><address><city>White Plains</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Joshi</last-name><first-name>Sachindra</first-name><address><city>Gurgaon</city><country>IN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A processor may receive first voice data associated with a first user utterance in conversation in a guided dialog system. The processor may identify from the first voice data a first topic of a set of topics associated with the first user utterance. The processor may identify a first solution associated with the first topic, the first solution having one or more solution segments for accomplishing a task related to the topic. The processor may generate a first response for a second user based on a first solution segment of the first solution and the first voice data.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="152.15mm" wi="90.00mm" file="US20230005475A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="211.41mm" wi="100.16mm" orientation="landscape" file="US20230005475A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="208.70mm" wi="92.03mm" file="US20230005475A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="178.56mm" wi="148.34mm" file="US20230005475A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="185.50mm" wi="144.10mm" file="US20230005475A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="200.83mm" wi="136.65mm" file="US20230005475A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">The present disclosure relates generally to the field of dialog systems, and more specifically to solution guided generation of responses for dialog systems.</p><p id="p-0003" num="0002">Dialog systems are intelligent machines that can understand language and conduct a written or verbal conversation with a user. Two common methods of creating conversation systems are: subject matter experts (&#x201c;SMEs&#x201d;) manually creating dialog flows using domain knowledge and data driven modeling. Data driven modeling includes learning from chatlogs where problem solving is implicitly learnt from both chatlogs and external knowledge, which provides more of a basis to generate responses.</p><p id="p-0004" num="0003">SME based modeling is time-consuming, costly, and manual. Further, learning from chatlogs is difficult as models need to learn language as well as business logic. In either case, identifying necessary external information and representing it is challenging. Accordingly, solutions for data driven dialog systems that are less costly, labor-intensive, and easier to model are needed.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0005" num="0004">Embodiments of the present disclosure include a method, computer program product, and system for solution guided generation of responses for dialog systems.</p><p id="p-0006" num="0005">In some embodiments, a processor may receive first voice data associated with a first user utterance in conversation in a guided dialog system. The processor may identify from the first voice data a first topic of a set of topics associated with the first user utterance. The processor may identify a first solution associated with the first topic, the first solution having one or more solution segments for accomplishing a task related to the topic. The processor may generate a first response for a second user based on a first solution segment of the first solution and the first voice data.</p><p id="p-0007" num="0006">In some embodiments, a processor may receive first voice data associated with a first user utterance in conversation in a guided dialog system. The processor may identify from the first voice data a first topic of a set of topics associated with the first user utterance. The processor may identify a first solution associated with the first topic, the first solution having one or more solution segments for accomplishing a task related to the topic. The processor may generate the first solution using a document corpus from an entity relevant to the first topic. The processor may generate a first response for a second user based on a first solution segment of the first solution and the first voice data.</p><p id="p-0008" num="0007">In some embodiments, a processor may receive first voice data associated with a first user utterance in conversation in a guided dialog system. The processor may identify from the first voice data a first topic of a set of topics associated with the first user utterance. The processor may identify a first solution associated with the first topic, the first solution having one or more solution segments for accomplishing a task related to the topic. The processor may generate the first solution using a text generation artificial intelligence model that generates solutions from sample conversations. The processor may generate a first response for a second user based on a first solution segment of the first solution and the first voice data.</p><p id="p-0009" num="0008">In some embodiments, the first response may be generated using a sequence to sequence machine learning model.</p><p id="p-0010" num="0009">In some embodiments, the first topic may be identified using a text classification model.</p><p id="p-0011" num="0010">In some embodiments, a processor may receive second voice data associated with a second user utterance in the conversation in the guided dialog system. The processor may confirm that the second user utterance is not associated with another topic of the set of topics. The processor may generate a second response for the second user based on the first voice data, the first response of the second user, the second voice data, and a second solution segment of the first solution.</p><p id="p-0012" num="0011">In some embodiments, a processor may receive third voice data associated with a third user utterance in the conversation in the guided dialog system. The processor may identify a second topic associated with the third user utterance. The processor may identify a second solution associated with the second topic. The processor may generate a third response for the second user based on the first voice data, the first response of the second user, the second voice data, the second response of the second user, the third voice data, and a solution segment of the second solution.</p><p id="p-0013" num="0012">The above summary is not intended to describe each illustrated embodiment or every implementation of the present disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013">The drawings included in the present disclosure are incorporated into, and form part of, the specification. They illustrate embodiments of the present disclosure and, along with the description, serve to explain the principles of the disclosure. The drawings are only illustrative of certain embodiments and do not limit the disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an exemplary system for solution guided generation of responses, in accordance with aspects of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of an exemplary method for solution guided generation of responses, in accordance with aspects of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> illustrates a cloud computing environment, in accordance with aspects of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> illustrates abstraction model layers, in accordance with aspects of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a high-level block diagram of an example computer system that may be used in implementing one or more of the methods, tools, and modules, and any related functions, described herein, in accordance with aspects of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0020" num="0019">While the embodiments described herein are amenable to various modifications and alternative forms, specifics thereof have been shown by way of example in the drawings and will be described in detail. It should be understood, however, that the particular embodiments described are not to be taken in a limiting sense. On the contrary, the intention is to cover all modifications, equivalents, and alternatives falling within the spirit and scope of the disclosure.</p><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">Aspects of the present disclosure relate generally to the field of dialog systems, and more specifically to solution guided generation of responses for dialog systems. While the present disclosure is not necessarily limited to such applications, various aspects of the disclosure may be appreciated through a discussion of various examples using this context.</p><p id="p-0022" num="0021">In some embodiments, a processor may receive first voice data associated with a first user utterance in conversation in a guided dialog system. In some embodiments, the processor may identify from the first voice data a first topic of a set of topics associated with the first user utterance. In some embodiments, the first topic may be identified using a text classification model.</p><p id="p-0023" num="0022">In some embodiments, the user utterance may relate to a request, issue, concern, or problem to address, resolve, solve, or accomplish through the conversation in the guided dialog system. In some embodiments, the conversation may be a communication between a caller (e.g., first user) and an agent (e.g., a second user) where the caller and agent take turns speaking or responding to the other. In some embodiments, the first voice data may include a text transcript of at least a portion of the conversation spoken by the first user during the turn(s) of the first user to speak in the conversation. In some embodiments, the guided dialog system may be utilized by a virtual assistant to help users accomplish a wide variety of tasks and may include conversation between an agent and users to accomplish those tasks.</p><p id="p-0024" num="0023">In some embodiments, the text classification model may analyze the text spoken by the first user and assign a set of predefined tags or categories (e.g., the first topic) to the text based on the context of the text. In some embodiments, the text classification model may utilize natural language processing for sentiment analysis, topic detection, intent detection, entity identification, and language detection. In some embodiments, the topics identified may include, but are not limited to, one or a combination of symptoms, topics, actions, intents, requests, issues, concerns, or problems or any other identifiers associated with a task, request, issue, concern, or problem the user would like assistance with related to the services or systems with respect to which the dialog system operates.</p><p id="p-0025" num="0024">For example, the guided dialog system may be initiated by a caller making a verbal request to extend a due data for payment of a utility bill. The caller may make an initial request to the system by stating that &#x201c;I would like an extension for paying my electric bill.&#x201d; The entire user utterance may be transcribed and provided to an artificial intelligence model with natural language processing capability that identifies a topic of the user utterance from the voice data. The topic of the user utterance may relate to a topic about which the user is inquiring, a problem with which the user would like assistance, an issue or concern that the caller would like addressed, an action that the user would like performed, etc. From the user utterance &#x201c;I would like an extension for paying my electric bill,&#x201d; a topic of &#x201c;payment extension&#x201d; may be identified.</p><p id="p-0026" num="0025">In some embodiments, the processor may identify a first solution associated with the first topic. In some embodiments, the first solution may have one or more solution segments for accomplishing a task related to the topic. In some embodiments, the processor may identify the first solution based on the identification of the first topic. In some embodiments, the one or more solution segments may be a series of steps, actions, or communications through which the task related to the topic may be accomplished. For example, for the topic &#x201c;payment extension,&#x201d; the processor may identify a solution that includes a series of steps through which the agent may accomplish the task of obtaining a payment extension for the caller. To obtain a payment extension for the caller, the solution segments may include the steps of: confirming the phone number of the caller; sending a pin number to the caller and requesting that the caller provide the pin number to the agent; asking the caller for the date on which he/she would like to settle the payment (e.g., obtain an extension until); reminding the caller that it is important to comply with the new payment arrangement and that failure to comply may result in late fees; recording the payment extension in a database; informing the caller that the payment extension has been implemented; and providing the caller with a reference number.</p><p id="p-0027" num="0026">In some embodiments, the one or more solution segments may include subtasks that need to be performed, including: obtaining various types of information from the user, informing the caller of various consequences of performing the action, verifying relevant background information from the caller (e.g., verifying the user's identity or account information), and obtaining information related to the task that needs to be accomplished (e.g., when would the user like to extend the due date until). In some embodiments, the subtasks or solution segments may include: communication exchanges, inquiries, instructions to be provided, questions to be asked, answers to be provided, information to be obtained, user response to be received, etc. In some embodiments, the solution and the solution segments may be a road map or instructions for how to accomplish the task through communications with the user via the guided dialog system.</p><p id="p-0028" num="0027">In some embodiments, the solution may be selected by an artificial intelligence (&#x201c;AI&#x201d;) model that links the selected solution to the topic identified from the voice data. In some embodiments, the AI model may be a text classification model that utilizes natural language processing. In some embodiments, the solution selecting model may be trained using data sets that link a solution and a topic to text from conversations between two users (e.g., a caller and an agent).</p><p id="p-0029" num="0028">In some embodiments, the processor may generate a first response for a second user based on a first solution segment of the first solution and the first voice data. For example, based on the first voice data, &#x201c;I would like an extension for paying my electric bill,&#x201d; and the first solution segment of the first solution, &#x201c;confirm the phone number of the caller,&#x201d; the first response for the agent (e.g., the second user) may be: &#x201c;I can help you with that, but first I need to verify your identity. Can you please provide the phone number associated with the account?&#x201d;. In some embodiments, the agent (e.g., the second user) is an automated agent that relays the response to the user/caller.</p><p id="p-0030" num="0029">In some embodiments, the first response may be generated using a sequence to sequence machine learning model. In some embodiments, the sequence to sequence model may be a deep learning model that generates text. In some embodiments, the sequence to sequence model may be a deep learning model that generates text by using a recurrent neural network (RNN), a Long Short-Term Memory (LSTM) or a Grated Recurrent Unit (GRU) architecture. In some embodiments, the context for each item is the output from the previous step. In some embodiments, the primary components of the sequence to sequence model are an encoder and a decoder network. In some embodiments, the encoder turns each item into a corresponding hidden vector containing the item and its context. In some embodiments, the decoder reverses the process, turning the vector into an output item, using the previous output as the input context. In some embodiments, the sequence to sequence model may include BART, generative pre-trained transformer 2 (&#x201c;GPT2&#x201d;), generative pre-trained transformer 3 (&#x201c;GPT3&#x201d;), etc. In some embodiments, the sequence to sequence model may be trained to take both the conversation context (e.g., user utterance(s) and response(s) by the agent) and identified solutions (or solution segments) as the inputs resulting in the outputted generated response for the second user.</p><p id="p-0031" num="0030">In some embodiments, the processor may receive second voice data associated with a second user utterance in the conversation in the guided dialog system. In some embodiments, the processor may confirm that the second user utterance is not associated with another topic of the set of topics. In some embodiments, the processor may generate a second response for the second user based on the first voice data, the first response of the second user, the second voice data, and a second solution segment of the first solution.</p><p id="p-0032" num="0031">Continuing the previous example, the second voice data may be &#x201c;my phone number is 123 345 6443.&#x201d; The processor may analyze the text provided by the user to determine that the information does not relate to a second topic. The processor may then generate a second response based on the conversation so far and the second solution segment of the first solution. The processor may input into machine learning model the first and second voice data and the first response of the agent:</p><p id="p-0033" num="0032">First user: &#x201c;I would like an extension for paying my electric bill.&#x201d;</p><p id="p-0034" num="0033">Agent: &#x201c;I can help you with that, but first I need to verify your identity. Can you please provide the phone number associated with the account?&#x201d;.</p><p id="p-0035" num="0034">First user: &#x201c;My phone number is 123 345 6443.&#x201d;</p><p id="p-0036" num="0035">The processor may also input into the machine learning model the second solution segment of the first solution, &#x201c;sending a pin number to the caller and requesting that the caller provide the pin number to the agent&#x201d; to generate a second response &#x201c;Our system will be sending you a pin number by text message to your phone. Please provide me with the four digit number sent to you.&#x201d;</p><p id="p-0037" num="0036">In some embodiments, the processor may receive third voice data associated with a third user utterance in the conversation in the guided dialog system. In some embodiments, the processor may identify a second topic associated with the third user utterance. In some embodiments, the processor may identify a second solution associated with the second topic. In some embodiments, the processor may generate a third response for the second user based on the first voice data, the first response of the second user, the second voice data, the second response of the second user, the third voice data, and a solution segment of the second solution.</p><p id="p-0038" num="0037">Continuing the prior example, the third user utterance may be &#x201c;the pin number I received was 3476, but actually, I would like to update the address associated with this account.&#x201d; The processor may identify the second topic &#x201c;updating account information&#x201d; in the third user utterance. The processor may identify a second solution providing a series of steps to update account information. The processor may then generate a third response for the agent based on the entire conversation history (e.g., the first, second, third user utterance and the first and second responses of the second user/agent) and a solution segment of the solution for updating account information. The solution segment for the updating account information solution may be to &#x201c;verify the type of account information to be updated.&#x201d; The third response generated for the agent may be &#x201c;You would like to update the address on your account profile. Is that correct?&#x201d; The third response may be generated based on the conversation history and the solution segments for the solution associated with updating account information.</p><p id="p-0039" num="0038">In some embodiments, each of the first, second, and third response may be predictively generated based on patterns detected by the machine learning model.</p><p id="p-0040" num="0039">In some embodiments, the solution (e.g., first solution or second solution) may be identified and prepared by a subject matter expert. In some embodiments, the one or more steps (e.g., solution segments) included in the solution may be manually created, generated, derived, or prepared by a subject matter expert. In some embodiments, the subject matter expert may be able to identify the series of solution segments that are needed to accomplish the task related to the topic based on the subject matter expert's knowledge of the processes, steps, or actions that may be taken to accomplish the task.</p><p id="p-0041" num="0040">In some embodiments, the subject matter expert may review sample exchanges between a user and an agent and identify steps or processes that are needed to accomplish a task related to the topic based on the communications between the user and agent. In some embodiments, the subject matter expertise may review any other relevant reference materials (e.g., manuals, how to websites, procedure bulletins) that provide information regarding how to accomplish the task (e.g., information that helps the subject matter expert determine, identify, or breakdown what steps need to be taken). Regardless of the subject matter expert embodiment used, the information provided by the subject matter expert is tagged with an annotation and presented to the disclosed system for storage and subsequent use during a conversation. Further, the information provided by the subject matter expert may be analyzed by a natural language processing system and stored/tagged by topic/subtopic.</p><p id="p-0042" num="0041">In some embodiments, the solution (e.g., first solution or second solution) may be generated using a text generation artificial intelligence model that generates solutions from sample conversations. In some embodiments, the text generation artificial intelligence model may generate the required solution token by a token grounded on the available conversation context. In some embodiments, the text generation artificial intelligence model may include BART, GPT2, and GPT3.</p><p id="p-0043" num="0042">In some embodiments, the text generation model may be trained using transcripts of conversations between a caller and an agent that have been annotated by a subject matter expert. In some embodiments, the sample transcripts may be annotated to identify portions of the conversations that relate to a solution component. In some embodiments, the text generation artificial intelligence model may be trained to associate portions of the conversation (e.g., language spoken by the caller or the agent) with a solution component.</p><p id="p-0044" num="0043">In some embodiments, once the text generation model is trained, the text generation model may be utilized to generate solutions by applying the text generation model to a corpus of conversations. In some embodiments, based on the sample conversations provided as input, the text generation model may be able to output additional solutions (e.g., solutions that are made of a series of solution segments) from the sample conversations in the conversation corpus.</p><p id="p-0045" num="0044">In some embodiments, the solution (e.g., first solution or second solution) may be generated using a document corpus from an entity relevant to the topic (e.g., the first topic or second topic, respectively). In some embodiments, the entity relevant to the topic may be an individual, group of individuals, organization, database, library, etc. that has or may provide access to information that relates to the topic, the solution associated with the topic, and/or one or more solution segments for accomplishing a task related to the topic.</p><p id="p-0046" num="0045">In some embodiments, solutions may be generated using the document corpus using a rule-based method. In some embodiments, the rule-based method may identify portions of documents in the document corpus that relate to topics, solutions, or solution segments. In some embodiments, the rules may describe how to identify topics, solutions, or solution segments and the conversation text (e.g., from a user utterance) with which they are associated.</p><p id="p-0047" num="0046">For example, a solution may be generated using rules that relate to the Document Object Model (&#x201c;DOM&#x201d;) elements on a specified set of webpages (e.g., webpages of an organization that provides step by step instructions to solve frequently encountered problems with products purchased from the organization). In some embodiments, the solutions that are generated using rules (e.g., related to DOM elements on webpages) may be reviewed by a subject matter expert (e.g., the author of the webpages). In some embodiments, feedback from the subject matter experts regarding the solutions may be used to update the rules based on which the solutions are generated.</p><p id="p-0048" num="0047">In some embodiments, a subject matter expert may review certain documents (e.g., user manuals for a company's products) from the document corpus and generate or draft a solution from the passages. For example, the subject matter expert may provide annotations that identify which portions of the passages led to which subcomponent of the solution. In some embodiments, the annotations may be used to train a text generation model to generate other solution from similar documents. In some embodiments, the text generation model may learn how to link passages of user manuals to solution segments. In some embodiments, the text generation model may then generate new solutions from user manuals that it receives as input.</p><p id="p-0049" num="0048">Referring now to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a block diagram of a system <b>100</b> for solution guided generation of responses is illustrated. System <b>100</b> includes a user device <b>102</b> and a system device <b>104</b>. The system device <b>104</b> includes conversation context database <b>106</b>, solution selector <b>108</b>, solution <b>110</b>, response generator <b>112</b>, and response provider <b>114</b>. The user device <b>102</b> and system device <b>104</b> are configured to be in communication with each other. The user device <b>102</b> and system device <b>104</b> may be any devices that contain a processor configured to perform one or more of the functions or steps described in this disclosure.</p><p id="p-0050" num="0049">In some embodiments, system device <b>104</b> receives from the user device <b>102</b> first voice data associated with a first user utterance in a conversation. The first voice data is stored in the conversation context database <b>106</b>. The solution selector <b>108</b> of the system device <b>104</b> identifies from the first voice data a first topic of a set of topics associated with the first user utterance and identifies a first solution <b>110</b> associated with the first topic. The first solution <b>110</b> has one or more solution segments for accomplishing a task related to the topic. The response generator <b>112</b> of the system device <b>104</b> generates a first response for a second user (e.g., an agent in the guided dialog system) based on a first solution segment of the first solution and the first voice data. In some embodiments, the response generator uses a sequence to sequence machine learning model to generate the response. The first response is communicated to the first user (e.g., to the user device <b>102</b>) via the response provider <b>114</b>.</p><p id="p-0051" num="0050">In some embodiments, the first response is stored in the conversation context database <b>106</b> and used to generate a second response for the agent. In some embodiments, the system device <b>104</b> receives second voice data associated with a second user utterance in the conversation. In some embodiments, the solution selector <b>108</b> confirms that the second user utterance is not associated with another topic of the set of topics. In some embodiments, the solution selector may use text classification model <b>116</b> to confirm that the second user utterance is not associated with another topic of the set of topics. In some embodiments, the response generator <b>112</b> generates a second response for the agent based on the first voice data, the first response of the agent, the second voice data, and a second solution segment of the first solution.</p><p id="p-0052" num="0051">In some embodiments, the first and second voice data and the first and second responses are stored in the conversation context database <b>106</b> and used to generate a third response for the agent. In some embodiments, the system device <b>104</b> receives third voice data associated with a third user utterance in the conversation. In some embodiments, the solution selector <b>108</b> identifies a second topic associated with the third user utterance. In some embodiments, the solution selector <b>108</b> identifies a second solution associated with the second topic. In some embodiments, the response generator <b>112</b> generates a third response for the agent based on the first voice data, the first response of the agent, the second voice data, the second response of the agent, the third voice data, and a solution segment of the second solution.</p><p id="p-0053" num="0052">Referring now to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, illustrated is a flowchart of an exemplary method <b>200</b> for solution guided generation of responses, in accordance with embodiments of the present disclosure. In some embodiments, a processor of a system may perform the operations of the method <b>200</b>. In some embodiments, method <b>200</b> begins at operation <b>202</b>. At operation <b>202</b>, the processor receives first voice data associated with a first user utterance in conversation in a guided dialog system. In some embodiments, method <b>200</b> proceeds to operation <b>204</b>, where the processor identifies from the first voice data a first topic of a set of topics associated with the first user utterance. In some embodiments, method <b>200</b> proceeds to operation <b>206</b>. At operation <b>206</b>, the processor identifies a first solution associated with the first topic, the first solution having one or more solution segments for accomplishing a task related to the topic. In some embodiments, method <b>200</b> proceeds to operation <b>208</b>. At operation <b>208</b>, the processor generates a first response for a second user based on a first solution segment of the first solution and the first voice data.</p><p id="p-0054" num="0053">As discussed in more detail herein, it is contemplated that some or all of the operations of the method <b>200</b> may be performed in alternative orders or may not be performed at all; furthermore, multiple operations may occur at the same time or as an internal part of a larger process.</p><p id="p-0055" num="0054">It is to be understood that although this disclosure includes a detailed description on cloud computing, implementation of the teachings recited herein are not limited to a cloud computing environment. Rather, embodiments of the present disclosure are capable of being implemented in conjunction with any other type of computing environment now known or later developed.</p><p id="p-0056" num="0055">Cloud computing is a model of service delivery for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, network bandwidth, servers, processing, memory, storage, applications, virtual machines, and services) that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service. This cloud model may include at least five characteristics, at least three service models, and at least four deployment models.</p><p id="p-0057" num="0056">Characteristics are as follows:</p><p id="p-0058" num="0057">On-demand self-service: a cloud consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with the service's provider.</p><p id="p-0059" num="0058">Broad network access: capabilities are available over a network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, laptops, and PDAs).</p><p id="p-0060" num="0059">Resource pooling: the provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to demand. There is a sense of portion independence in that the consumer generally has no control or knowledge over the exact portion of the provided resources but may be able to specify portion at a higher level of abstraction (e.g., country, state, or datacenter).</p><p id="p-0061" num="0060">Rapid elasticity: capabilities can be rapidly and elastically provisioned, in some cases automatically, to quickly scale out and rapidly released to quickly scale in. To the consumer, the capabilities available for provisioning often appear to be unlimited and can be purchased in any quantity at any time.</p><p id="p-0062" num="0061">Measured service: cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.</p><p id="p-0063" num="0062">Service Models are as follows:</p><p id="p-0064" num="0063">Software as a Service (SaaS): the capability provided to the consumer is to use the provider's applications running on a cloud infrastructure. The applications are accessible from various client devices through a thin client interface such as a web browser (e.g., web-based e-mail). The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings.</p><p id="p-0065" num="0064">Platform as a Service (PaaS): the capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including networks, servers, operating systems, or storage, but has control over the deployed applications and possibly application hosting environment configurations.</p><p id="p-0066" num="0065">Infrastructure as a Service (IaaS): the capability provided to the consumer is to provision processing, storage, networks, and other fundamental computing resources where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, deployed applications, and possibly limited control of select networking components (e.g., host firewalls).</p><p id="p-0067" num="0066">Deployment Models are as follows:</p><p id="p-0068" num="0067">Private cloud: the cloud infrastructure is operated solely for an organization. It may be managed by the organization or a third party and may exist on-premises or off-premises.</p><p id="p-0069" num="0068">Community cloud: the cloud infrastructure is shared by several organizations and supports a specific community that has shared concerns (e.g., mission, security requirements, policy, and compliance considerations). It may be managed by the organizations or a third party and may exist on-premises or off-premises.</p><p id="p-0070" num="0069">Public cloud: the cloud infrastructure is made available to the general public or a large industry group and is owned by an organization selling cloud services.</p><p id="p-0071" num="0070">Hybrid cloud: the cloud infrastructure is a composition of two or more clouds (private, community, or public) that remain unique entities but are bound together by standardized or proprietary technology that enables data and application portability (e.g., cloud bursting for load-balancing between clouds).</p><p id="p-0072" num="0071">A cloud computing environment is service oriented with a focus on statelessness, low coupling, modularity, and semantic interoperability. At the heart of cloud computing is an infrastructure that includes a network of interconnected nodes.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, illustrated is a cloud computing environment <b>310</b> is depicted. As shown, cloud computing environment <b>310</b> includes one or more cloud computing nodes <b>300</b> with which local computing devices used by cloud consumers, such as, for example, personal digital assistant (PDA) or cellular telephone <b>300</b>A, desktop computer <b>300</b>B, laptop computer <b>300</b>C, and/or automobile computer system <b>300</b>N may communicate. Nodes <b>300</b> may communicate with one another. They may be grouped (not shown) physically or virtually, in one or more networks, such as Private, Community, Public, or Hybrid clouds as described hereinabove, or a combination thereof.</p><p id="p-0074" num="0073">This allows cloud computing environment <b>310</b> to offer infrastructure, platforms and/or software as services for which a cloud consumer does not need to maintain resources on a local computing device. It is understood that the types of computing devices <b>300</b>A-N shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> are intended to be illustrative only and that computing nodes <b>300</b> and cloud computing environment <b>310</b> can communicate with any type of computerized device over any type of network and/or network addressable connection (e.g., using a web browser).</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, illustrated is a set of functional abstraction layers provided by cloud computing environment <b>310</b> (<figref idref="DRAWINGS">FIG. <b>3</b>A</figref>) is shown. It should be understood in advance that the components, layers, and functions shown in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> are intended to be illustrative only and embodiments of the disclosure are not limited thereto. As depicted below, the following layers and corresponding functions are provided.</p><p id="p-0076" num="0075">Hardware and software layer <b>315</b> includes hardware and software components. Examples of hardware components include: mainframes <b>302</b>; RISC (Reduced Instruction Set Computer) architecture based servers <b>304</b>; servers <b>306</b>; blade servers <b>308</b>; storage devices <b>311</b>; and networks and networking components <b>312</b>. In some embodiments, software components include network application server software <b>314</b> and database software <b>316</b>.</p><p id="p-0077" num="0076">Virtualization layer <b>320</b> provides an abstraction layer from which the following examples of virtual entities may be provided: virtual servers <b>322</b>; virtual storage <b>324</b>; virtual networks <b>326</b>, including virtual private networks; virtual applications and operating systems <b>328</b>; and virtual clients <b>330</b>.</p><p id="p-0078" num="0077">In one example, management layer <b>340</b> may provide the functions described below. Resource provisioning <b>342</b> provides dynamic procurement of computing resources and other resources that are utilized to perform tasks within the cloud computing environment. Metering and Pricing <b>344</b> provide cost tracking as resources are utilized within the cloud computing environment, and billing or invoicing for consumption of these resources. In one example, these resources may include application software licenses. Security provides identity verification for cloud consumers and tasks, as well as protection for data and other resources. User portal <b>346</b> provides access to the cloud computing environment for consumers and system administrators. Service level management <b>348</b> provides cloud computing resource allocation and management such that required service levels are met. Service Level Agreement (SLA) planning and fulfillment <b>350</b> provide pre-arrangement for, and procurement of, cloud computing resources for which a future requirement is anticipated in accordance with an SLA.</p><p id="p-0079" num="0078">Workloads layer <b>360</b> provides examples of functionality for which the cloud computing environment may be utilized. Examples of workloads and functions which may be provided from this layer include: mapping and navigation <b>362</b>; software development and lifecycle management <b>364</b>; virtual classroom education delivery <b>366</b>; data analytics processing <b>368</b>; transaction processing <b>370</b>; and solution-guided generation of responses for dialog systems <b>372</b>.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>4</b></figref>, illustrated is a high-level block diagram of an example computer system <b>401</b> that may be used in implementing one or more of the methods, tools, and modules, and any related functions, described herein (e.g., using one or more processor circuits or computer processors of the computer), in accordance with embodiments of the present disclosure. In some embodiments, the major components of the computer system <b>401</b> may comprise one or more CPUs <b>402</b>, a memory subsystem <b>404</b>, a terminal interface <b>412</b>, a storage interface <b>416</b>, an I/O (Input/Output) device interface <b>414</b>, and a network interface <b>418</b>, all of which may be communicatively coupled, directly or indirectly, for inter-component communication via a memory bus <b>403</b>, an I/O bus <b>408</b>, and an I/O bus interface unit <b>410</b>.</p><p id="p-0081" num="0080">The computer system <b>401</b> may contain one or more general-purpose programmable central processing units (CPUs) <b>402</b>A, <b>402</b>B, <b>402</b>C, and <b>402</b>D, herein generically referred to as the CPU <b>402</b>. In some embodiments, the computer system <b>401</b> may contain multiple processors typical of a relatively large system; however, in other embodiments the computer system <b>401</b> may alternatively be a single CPU system. Each CPU <b>402</b> may execute instructions stored in the memory subsystem <b>404</b> and may include one or more levels of on-board cache.</p><p id="p-0082" num="0081">System memory <b>404</b> may include computer system readable media in the form of volatile memory, such as random access memory (RAM) <b>422</b> or cache memory <b>424</b>. Computer system <b>401</b> may further include other removable/non-removable, volatile/non-volatile computer system storage media. By way of example only, storage system <b>426</b> can be provided for reading from and writing to a non-removable, non-volatile magnetic media, such as a &#x201c;hard drive.&#x201d; Although not shown, a magnetic disk drive for reading from and writing to a removable, non-volatile magnetic disk (e.g., a &#x201c;floppy disk&#x201d;), or an optical disk drive for reading from or writing to a removable, non-volatile optical disc such as a CD-ROM, DVD-ROM or other optical media can be provided. In addition, memory <b>404</b> can include flash memory, e.g., a flash memory stick drive or a flash drive. Memory devices can be connected to memory bus <b>403</b> by one or more data media interfaces. The memory <b>404</b> may include at least one program product having a set (e.g., at least one) of program modules that are configured to carry out the functions of various embodiments.</p><p id="p-0083" num="0082">One or more programs/utilities <b>428</b>, each having at least one set of program modules <b>430</b> may be stored in memory <b>404</b>. The programs/utilities <b>428</b> may include a hypervisor (also referred to as a virtual machine monitor), one or more operating systems, one or more application programs, other program modules, and program data. Each of the operating systems, one or more application programs, other program modules, and program data or some combination thereof, may include an implementation of a networking environment. Programs <b>428</b> and/or program modules <b>430</b> generally perform the functions or methodologies of various embodiments.</p><p id="p-0084" num="0083">Although the memory bus <b>403</b> is shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> as a single bus structure providing a direct communication path among the CPUs <b>402</b>, the memory subsystem <b>404</b>, and the I/O bus interface <b>410</b>, the memory bus <b>403</b> may, in some embodiments, include multiple different buses or communication paths, which may be arranged in any of various forms, such as point-to-point links in hierarchical, star or web configurations, multiple hierarchical buses, parallel and redundant paths, or any other appropriate type of configuration. Furthermore, while the I/O bus interface <b>410</b> and the I/O bus <b>408</b> are shown as single respective units, the computer system <b>401</b> may, in some embodiments, contain multiple I/O bus interface units <b>410</b>, multiple I/O buses <b>408</b>, or both. Further, while multiple I/O interface units are shown, which separate the I/O bus <b>408</b> from various communications paths running to the various I/O devices, in other embodiments some or all of the I/O devices may be connected directly to one or more system I/O buses.</p><p id="p-0085" num="0084">In some embodiments, the computer system <b>401</b> may be a multi-user mainframe computer system, a single-user system, or a server computer or similar device that has little or no direct user interface, but receives requests from other computer systems (clients). Further, in some embodiments, the computer system <b>401</b> may be implemented as a desktop computer, portable computer, laptop or notebook computer, tablet computer, pocket computer, telephone, smartphone, network switches or routers, or any other appropriate type of electronic device.</p><p id="p-0086" num="0085">It is noted that <figref idref="DRAWINGS">FIG. <b>4</b></figref> is intended to depict the representative major components of an exemplary computer system <b>401</b>. In some embodiments, however, individual components may have greater or lesser complexity than as represented in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, components other than or in addition to those shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> may be present, and the number, type, and configuration of such components may vary.</p><p id="p-0087" num="0086">As discussed in more detail herein, it is contemplated that some or all of the operations of some of the embodiments of methods described herein may be performed in alternative orders or may not be performed at all; furthermore, multiple operations may occur at the same time or as an internal part of a larger process.</p><p id="p-0088" num="0087">The present disclosure may be a system, a method, and/or a computer program product at any possible technical detail level of integration. The computer program product may include a computer readable storage medium (or media) having computer readable program instructions thereon for causing a processor to carry out aspects of the present disclosure.</p><p id="p-0089" num="0088">The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be, for example, but is not limited to, an electronic storage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing. A non-exhaustive list of more specific examples of the computer readable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a portable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable combination of the foregoing. A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through a waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.</p><p id="p-0090" num="0089">Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the Internet, a local area network, a wide area network and/or a wireless network. The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. A network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing/processing device.</p><p id="p-0091" num="0090">Computer readable program instructions for carrying out operations of the present disclosure may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware instructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++, or the like, and procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider). In some embodiments, electronic circuitry including, for example, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry, in order to perform aspects of the present disclosure.</p><p id="p-0092" num="0091">Aspects of the present disclosure are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the disclosure. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.</p><p id="p-0093" num="0092">These computer readable program instructions may be provided to a processor of a computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0094" num="0093">The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or other device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0095" num="0094">The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present disclosure. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s). In some alternative implementations, the functions noted in the blocks may occur out of the order noted in the Figures. For example, two blocks shown in succession may, in fact, be accomplished as one step, executed concurrently, substantially concurrently, in a partially or wholly temporally overlapping manner, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.</p><p id="p-0096" num="0095">The descriptions of the various embodiments of the present disclosure have been presented for purposes of illustration, but are not intended to be exhaustive or limited to the embodiments disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments. The terminology used herein was chosen to best explain the principles of the embodiments, the practical application or technical improvement over technologies found in the marketplace, or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.</p><p id="p-0097" num="0096">Although the present disclosure has been described in terms of specific embodiments, it is anticipated that alterations and modification thereof will become apparent to the skilled in the art. Therefore, it is intended that the following claims be interpreted as covering all such alterations and modifications as fall within the true spirit and scope of the disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method, the method comprising:<claim-text>receiving, by a processor, first voice data associated with a first user utterance in conversation in a guided dialog system;</claim-text><claim-text>identifying from the first voice data a first topic of a set of topics associated with the first user utterance;</claim-text><claim-text>identifying a first solution associated with the first topic, the first solution having one or more solution segments for accomplishing a task related to the topic; and</claim-text><claim-text>generating a first response for a second user based on a first solution segment of the first solution and the first voice data.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first response is generated using a sequence to sequence machine learning model.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first topic is identified using a text classification model.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving second voice data associated with a second user utterance in the conversation in the guided dialog system;</claim-text><claim-text>confirming that the second user utterance is not associated with another topic of the set of topics; and</claim-text><claim-text>generating a second response for the second user based on the first voice data, the first response of the second user, the second voice data, and a second solution segment of the first solution.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:<claim-text>receiving third voice data associated with a third user utterance in the conversation in the guided dialog system;</claim-text><claim-text>identifying a second topic associated with the third user utterance;</claim-text><claim-text>identifying a second solution associated with the second topic; and</claim-text><claim-text>generating a third response for the second user based on the first voice data, the first response of the second user, the second voice data, the second response of the second user, the third voice data, and a solution segment of the second solution.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. A computer-implemented method, the method comprising:<claim-text>receiving, by a processor, first voice data associated with a first user utterance in conversation in a guided dialog system;</claim-text><claim-text>identifying from the first voice data a first topic of a set of topics associated with the first user utterance;</claim-text><claim-text>identifying a first solution associated with the first topic, the first solution having one or more solution segments for accomplishing a task related to the topic, wherein the first solution is generated using a text generation artificial intelligence model that generates solutions from sample conversations; and</claim-text><claim-text>generating a first response for a second user based on a first solution segment of the first solution and the first voice data.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the first response is generated using a sequence to sequence machine learning model.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the first topic is identified using a text classification model.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>receiving second voice data associated with a second user utterance in the conversation in the guided dialog system;</claim-text><claim-text>confirming that the second user utterance is not associated with another topic of the set of topics; and</claim-text><claim-text>generating a second response for the second user based on the first voice data, the first response of the second user, the second voice data, and a second solution segment of the first solution.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>receiving third voice data associated with a third user utterance in the conversation in the guided dialog system;</claim-text><claim-text>identifying a second topic associated with the third user utterance;</claim-text><claim-text>identifying a second solution associated with the second topic; and</claim-text><claim-text>generating a third response for the second user based on the first voice data, the first response of the second user, the second voice data, the second response of the second user, the third voice data, and a solution segment of the second solution.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A computer-implemented method, the method comprising:<claim-text>receiving, by a processor, first voice data associated with a first user utterance in conversation in a guided dialog system;</claim-text><claim-text>identifying from the first voice data a first topic of a set of topics associated with the first user utterance;</claim-text><claim-text>identifying a first solution associated with the first topic, the first solution having one or more solution segments for accomplishing a task related to the topic, wherein the first solution is generated using a document corpus from an entity relevant to the first topic; and</claim-text><claim-text>generating a first response for a second user based on a first solution segment of the first solution and the first voice data.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first response is generated using a sequence to sequence machine learning model.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first topic is identified using a text classification model.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>receiving second voice data associated with a second user utterance in the conversation in the guided dialog system;</claim-text><claim-text>confirming that the second user utterance is not associated with another topic of the set of topics; and</claim-text><claim-text>generating a second response for the second user based on the first voice data, the first response of the second user, the second voice data, and a second solution segment of the first solution.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:<claim-text>receiving third voice data associated with a third user utterance in the conversation in the guided dialog system;</claim-text><claim-text>identifying a second topic associated with the third user utterance;</claim-text><claim-text>identifying a second solution associated with the second topic; and</claim-text><claim-text>generating a third response for the second user based on the first voice data, the first response of the second user, the second voice data, the second response of the second user, the third voice data, and a solution segment of the second solution.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A system comprising:<claim-text>a memory; and</claim-text><claim-text>a processor in communication with the memory, the processor being configured to perform operations comprising:<claim-text>receiving first voice data associated with a first user utterance in conversation in a guided dialog system;</claim-text><claim-text>identifying from the first voice data a first topic of a set of topics associated with the first user utterance;</claim-text><claim-text>identifying a first solution associated with the first topic, the first solution having one or more solution segments for accomplishing a task related to the topic; and</claim-text><claim-text>generating a first response for a second user based on a first solution segment of the first solution and the first voice data.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the first response is generated using a sequence to sequence machine learning model.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the first topic is identified using a text classification model.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, the processor being further configured to perform operations comprising:<claim-text>receiving second voice data associated with a second user utterance in the conversation in the guided dialog system;</claim-text><claim-text>confirming that the second user utterance is not associated with another topic of the set of topics; and</claim-text><claim-text>generating a second response for the second user based on the first voice data, the first response of the second user, the second voice data, and a second solution segment of the first solution.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, the processor being further configured to perform operations comprising:<claim-text>receiving third voice data associated with a third user utterance in the conversation in the guided dialog system;</claim-text><claim-text>identifying a second topic associated with the third user utterance;</claim-text><claim-text>identifying a second solution associated with the second topic; and</claim-text><claim-text>generating a third response for the second user based on the first voice data, the first response of the second user, the second voice data, the second response of the second user, the third voice data, and a solution segment of the second solution.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A computer program product comprising a computer readable storage medium having program instructions embodied therewith, the program instructions executable by a processor to cause the processor to perform operations, the operations comprising:<claim-text>receiving first voice data associated with a first user utterance in conversation in a guided dialog system;</claim-text><claim-text>identifying from the first voice data a first topic of a set of topics associated with the first user utterance;</claim-text><claim-text>identifying a first solution associated with the first topic, the first solution having one or more solution segments for accomplishing a task related to the topic; and</claim-text><claim-text>generating a first response for a second user based on a first solution segment of the first solution and the first voice data.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the first response is generated using a sequence to sequence machine learning model.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the first topic is identified using a text classification model.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, the processor being further configured to perform operations comprising:<claim-text>receiving second voice data associated with a second user utterance in the conversation in the guided dialog system;</claim-text><claim-text>confirming that the second user utterance is not associated with another topic of the set of topics; and</claim-text><claim-text>generating a second response for the second user based on the first voice data, the first response of the second user, the second voice data, and a second solution segment of the first solution.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The computer program product of <claim-ref idref="CLM-00024">claim 24</claim-ref>, the processor being further configured to perform operations comprising:<claim-text>receiving third voice data associated with a third user utterance in the conversation in the guided dialog system;</claim-text><claim-text>identifying a second topic associated with the third user utterance;</claim-text><claim-text>identifying a second solution associated with the second topic; and</claim-text><claim-text>generating a third response for the second user based on the first voice data, the first response of the second user, the second voice data, the second response of the second user, the third voice data, and a solution segment of the second solution.</claim-text></claim-text></claim></claims></us-patent-application>