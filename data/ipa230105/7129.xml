<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007130A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007130</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17784834</doc-number><date>20191225</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>1</main-group><subgroup>00127</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>2201</main-group><subgroup>3205</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MANAGEMENT SYSTEM, MANAGEMENT METHOD, AND RECORDING MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NEC Corporation</orgname><address><city>Minato-ku, Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>NAKATSUKA</last-name><first-name>Masaya</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>NEC Corporation</orgname><role>03</role><address><city>Minato-ku, Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2019/050789</doc-number><date>20191225</date></document-id><us-371c12-date><date>20220613</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A management system is configured to include a first data acquisition unit, a second data acquisition unit, and a comparison unit. The first data acquisition unit acquires first image data obtained by capturing an image of a first object, and identification information about the owner of the first object. The second data acquisition unit acquires second image data obtained by capturing an image of a second object. The comparison unit <b>3</b> identifies the identification information about the owner of the first object by comparing the characteristics of the surface pattern of the first object as represented by the first image data with the characteristics of the surface pattern of the second object as represented by the second image data.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="103.89mm" wi="158.75mm" file="US20230007130A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="163.83mm" wi="77.13mm" orientation="landscape" file="US20230007130A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="157.82mm" wi="112.35mm" orientation="landscape" file="US20230007130A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="186.69mm" wi="83.40mm" orientation="landscape" file="US20230007130A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="224.03mm" wi="147.40mm" orientation="landscape" file="US20230007130A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="179.41mm" wi="112.27mm" orientation="landscape" file="US20230007130A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="182.20mm" wi="112.52mm" orientation="landscape" file="US20230007130A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="194.14mm" wi="127.25mm" orientation="landscape" file="US20230007130A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="219.54mm" wi="112.86mm" orientation="landscape" file="US20230007130A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="140.04mm" wi="149.44mm" orientation="landscape" file="US20230007130A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="202.69mm" wi="151.64mm" orientation="landscape" file="US20230007130A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="204.55mm" wi="153.59mm" orientation="landscape" file="US20230007130A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="234.61mm" wi="145.37mm" orientation="landscape" file="US20230007130A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="202.69mm" wi="151.30mm" orientation="landscape" file="US20230007130A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="130.73mm" wi="142.16mm" orientation="landscape" file="US20230007130A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="202.35mm" wi="153.33mm" orientation="landscape" file="US20230007130A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="210.14mm" wi="53.42mm" orientation="landscape" file="US20230007130A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="172.97mm" wi="127.25mm" orientation="landscape" file="US20230007130A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="179.66mm" wi="148.93mm" orientation="landscape" file="US20230007130A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="177.97mm" wi="89.32mm" orientation="landscape" file="US20230007130A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="207.77mm" wi="152.74mm" orientation="landscape" file="US20230007130A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="202.10mm" wi="153.67mm" orientation="landscape" file="US20230007130A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="136.48mm" wi="114.98mm" orientation="landscape" file="US20230007130A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="211.67mm" wi="145.03mm" orientation="landscape" file="US20230007130A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="228.77mm" wi="147.15mm" orientation="landscape" file="US20230007130A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="210.48mm" wi="98.81mm" orientation="landscape" file="US20230007130A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="174.58mm" wi="129.20mm" orientation="landscape" file="US20230007130A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="182.71mm" wi="151.13mm" orientation="landscape" file="US20230007130A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="211.07mm" wi="131.83mm" orientation="landscape" file="US20230007130A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="130.56mm" wi="149.10mm" orientation="landscape" file="US20230007130A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="209.30mm" wi="152.99mm" orientation="landscape" file="US20230007130A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="206.59mm" wi="152.82mm" orientation="landscape" file="US20230007130A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="216.83mm" wi="144.36mm" orientation="landscape" file="US20230007130A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="221.49mm" wi="148.76mm" orientation="landscape" file="US20230007130A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="147.15mm" wi="110.49mm" orientation="landscape" file="US20230007130A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to a technique for managing an object, and particularly relates to a technique for managing an object using a surface shape unique to an individual.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">When there is an object whose owner is unknown in a facility or the like used by a large number of people, such as a lost item in public transport, it is often difficult to specify the owner of the object. In a facility or the like with a high security management level, it is also often difficult to determine whether an object possessed by a leaving person is the belongings of the person or another object of the same or similar type. On the other hand, in such a facility that many people use, it is desirable that check of belongings at entry/exit is simplified as much as possible. Therefore, it is desirable to have a technique that facilitates specification of the owner of an object while reducing the burden on the user required to check the belongings. As a technique that facilitates specification of the owner of another object of the same or similar type, for example, a technique such as PTL 1 is disclosed.</p><p id="p-0004" num="0003">PTL 1 relates to a management system that specifies the owner of an object based on a mark identifier added to the object. In PTL 1, at the time of purchase of an object, a mark is inscribed at a different position for each object, and a mark identifier is added. The information of the mark identifier for each object is registered in association with the information of the owner of the object. When it is desired to specify the owner of an object, by reading the mark identifier of the object and collating it with registered information, the owner is specified. PTL 2 discloses a technique for identifying an object by reading information of a tag attached to the object, the tag in which identification information of the owner is recorded. PTL 3 discloses a technique for identifying an object by reading a two-dimensional barcode in which identification information unique to an individual is recorded.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0005" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0004">[PTL 1] WO 2019/111811</li>    <li id="ul0001-0002" num="0005">[PTL 2] JP 2006-154977 A</li>    <li id="ul0001-0003" num="0006">[PTL 3] JP 2016-177464 A</li></ul></p><heading id="h-0005" level="1">SUMMARY OF INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0006" num="0007">However, the technique of PTL 1 is not sufficient in the following points. In PTL 1, a mark identifier is added to an object at the time of purchase of the object, and registered in association with owner information. In PTL 1, a tool for adding, to an object, a mark identifier that does not disappear by use or custody, and a technique for inscribing a mark on the object to add the mark identifier are required. Therefore, since it is not easy for the owner himself to register the identification information of the object after purchase, there is a possibility that the information of the mark identifier does not exist in a specific target object when the owner is specified. Also in PTLs 2 and 3, it is necessary to attach in advance a tag or a two-dimensional barcode in which identification information is recorded to an object at the time of sale or the like, and it is difficult for the owner to register the identification information later. Therefore, the techniques of PTLs 1 to 3 are not sufficient as a technique for specifying the owner of an object without requiring complicated work by the user.</p><p id="p-0007" num="0008">In order to solve the above problems, an object of the present invention is to provide a management system capable of specifying the owner of an object without requiring complicated work.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0008" num="0009">In order to solve the above problem, a management system of the present invention includes a first data acquisition unit, a second data acquisition unit, and a collation unit. The first data acquisition unit acquires first image data in which a first object is photographed and identification information of the owner of the first object. The second data acquisition unit acquires second image data in which a second object is photographed. The collation unit specifies the identification information of the owner of the first object by collating the feature of the surface pattern of the first object in the first image data with the feature of the surface pattern of the second object in the second image data.</p><p id="p-0009" num="0010">A management method of the present invention includes acquiring the first image data in which the first object is photographed and the identification information of the owner of the first object. The management method of the present invention includes acquiring the second image data in which the second object is photographed. The management method of the present invention includes specifying the identification information of the owner of the first object by collating the feature of the surface pattern of the first object in the first image data with the feature of the surface pattern of the second object in the second image data.</p><p id="p-0010" num="0011">A recording medium of the present invention records a computer program for causing a computer to execute processing. The computer program causes the computer to execute processing of acquiring the first image data in which the first object is photographed and the identification information of the owner of the first object. The computer program causes the computer to execute processing of acquiring the second image data in which the second object is photographed. The computer program causes the computer to execute processing of specifying the identification information of the owner of the first object by collating the feature of the surface pattern of the first object in the first image data with the feature of the surface pattern of the second object in the second image data.</p><heading id="h-0008" level="1">Advantageous Effects of Invention</heading><p id="p-0011" num="0012">According to the present invention, it is possible to specify the owner of an object without requiring complicated work.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0012" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a view illustrating a configuration of a first example embodiment of the present invention.</p><p id="p-0013" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a view illustrating an operation flow of a management system of the first example embodiment of the present invention.</p><p id="p-0014" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view illustrating a configuration of a second example embodiment of the present invention.</p><p id="p-0015" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a view illustrating an application example of a management system of the second example embodiment of the present invention.</p><p id="p-0016" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a view illustrating a configuration of a user information management device of the second example embodiment of the present invention.</p><p id="p-0017" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a view illustrating a configuration of a collation device of the second example embodiment of the present invention.</p><p id="p-0018" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a view illustrating a configuration of a manager terminal device of the second example embodiment of the present invention.</p><p id="p-0019" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a view illustrating an example of a photographing method of an image of an object of the second example embodiment of the present invention.</p><p id="p-0020" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a view illustrating an operation flow of the user information management device of the second example embodiment of the present invention.</p><p id="p-0021" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a view illustrating an operation flow of the manager terminal device of the second example embodiment of the present invention.</p><p id="p-0022" num="0023"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a view illustrating an operation flow of the collation device of the second example embodiment of the present invention.</p><p id="p-0023" num="0024"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a view illustrating another application example and modification of a management system of the second example embodiment of the present invention.</p><p id="p-0024" num="0025"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an operation flow of the manager terminal device of the second example embodiment of the present invention, and is a view illustrating an operation flow in the another application example illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0025" num="0026"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is an operation flow of the user information management device of the second example embodiment of the present invention, and is a view illustrating an operation flow in the another application example illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0026" num="0027"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is an operation flow of the collation device of the second example embodiment of the present invention, and is a view illustrating an operation flow in the another application example illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0027" num="0028"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a view illustrating a configuration of a third example embodiment of the present invention.</p><p id="p-0028" num="0029"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a view illustrating a configuration of an entry/exit device of the third example embodiment of the present invention.</p><p id="p-0029" num="0030"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a view illustrating a configuration of an object management device of the third example embodiment of the present invention.</p><p id="p-0030" num="0031"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a view illustrating a configuration of a collation device of the third example embodiment of the present invention.</p><p id="p-0031" num="0032"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a view illustrating an operation flow of the entry/exit device of the third example embodiment of the present invention.</p><p id="p-0032" num="0033"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a view illustrating an operation flow of the object management device of the third example embodiment of the present invention.</p><p id="p-0033" num="0034"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a view illustrating an operation flow of the collation device of the third example embodiment of the present invention.</p><p id="p-0034" num="0035"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a view illustrating an application example of a management system of the third example embodiment of the present invention.</p><p id="p-0035" num="0036"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a view illustrating the application example of the management system of the third example embodiment of the present invention.</p><p id="p-0036" num="0037"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a view illustrating a configuration of a fourth example embodiment of the present invention.</p><p id="p-0037" num="0038"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a view illustrating a configuration of an entry/exit device of the fourth example embodiment of the present invention.</p><p id="p-0038" num="0039"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a view illustrating a configuration of an object management device of the fourth example embodiment of the present invention.</p><p id="p-0039" num="0040"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a view illustrating a configuration of a terminal device of the fourth example embodiment of the present invention.</p><p id="p-0040" num="0041"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a view illustrating an operation flow of the terminal device of the fourth example embodiment of the present invention.</p><p id="p-0041" num="0042"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a view illustrating an operation flow of the entry/exit device of the fourth example embodiment of the present invention.</p><p id="p-0042" num="0043"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a view illustrating an operation flow of the object management device of the fourth example embodiment of the present invention.</p><p id="p-0043" num="0044"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a view illustrating an application example of a management system of the fourth example embodiment of the present invention.</p><p id="p-0044" num="0045"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a view illustrating the application example of the management system of the fourth example embodiment of the present invention.</p><p id="p-0045" num="0046"><figref idref="DRAWINGS">FIG. <b>33</b></figref> is a view illustrating an example of another configuration of the present invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">EXAMPLE EMBODIMENT</heading><heading id="h-0011" level="1">First Example Embodiment</heading><p id="p-0046" num="0047">The first example embodiment of the present invention will be described in detail with reference to the drawings. <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a view illustrating the configuration of the management system of the present example embodiment. <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a view illustrating the operation flow of the management system of the present example embodiment.</p><p id="p-0047" num="0048">The management system of the present example embodiment includes a first data acquisition unit <b>1</b>, a second data acquisition unit <b>2</b>, and a collation unit <b>3</b>. The first data acquisition unit <b>1</b> acquires the first image data in which the first object is photographed and the identification information of the owner of the first object. The second data acquisition unit <b>2</b> acquires the second image data in which the second object is photographed. The collation unit <b>3</b> specifies the identification information of the owner of the first object by collating the feature of the surface pattern of the first object in the first image data with the feature of the surface shape of the second object in the second image data. By specification of the identification information of the owner by the collation unit <b>3</b>, it is possible to determine whether the second object is belongings of the owner of the first object.</p><p id="p-0048" num="0049">The surface pattern refers to a surface pattern unique to an individual naturally occurring in the manufacturing process of an object. For example, the surface pattern is a fine groove, unevenness, or the like of the object surface. The surface pattern is different for each individual even for the same type of object. The surface pattern is also called an object fingerprint because it is unique to an object like a fingerprint of a human finger.</p><p id="p-0049" num="0050">Next, the operation of the management system of the present example embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>. The first data acquisition unit <b>1</b> acquires image data of the first object and the identification information of the owner of the first object (step S<b>1</b>). The second data acquisition unit <b>2</b> acquires image data of the second object (step S<b>2</b>). Using the first image data and the second image data, the collation unit <b>3</b> collates the feature of the surface pattern of the first object with the feature of the surface pattern of the second object, and specifies the identification information of the owner of the first object (step S<b>3</b>).</p><p id="p-0050" num="0051">When collating the feature of the surface pattern of the first object with the feature of the surface pattern of the second object and specifying the identification information of the owner of the first object, the collation unit <b>3</b> compares the surface pattern of the second object with the feature of the surface pattern of the first object, and determines whether the feature of the surface pattern of the first object is similar to the feature of the surface pattern of the second object by comparison. The collation unit <b>3</b> calculates cosine similarity, for example, in a case where both the feature of the surface pattern of the first object and the feature of the surface pattern of the second object are represented by a feature vector. The feature vector is, for example, multidimensional data indicating positions and feature amounts (density gradient of an image and the like) of a plurality of feature points of the surface pattern.</p><p id="p-0051" num="0052">Upon determining that the surface pattern of the second object is similar to the surface pattern of the first object, the collation unit <b>3</b> specifies the identification information of the owner of the first object because the second object is the first object. This makes it possible to determine that the second object is belongings of the owner of the first object.</p><p id="p-0052" num="0053">As an application example of the management system of the present example embodiment, it is a case where, using the surface pattern unique to each object, the owner of the object appearing in the second to image data is determined by determining whether the object appearing in the first image data registered in advance together with the identification information of the owner is similar to the object appearing in the second image data acquired at another time. By using, for identification of the object, the image data in which the surface pattern unique to each object is photographed, it is possible to identify whether to be the same object even if the objects are of the same or similar type and the difference cannot be visually discriminated. For example, when the surface patterns of the second object and the first object match, the second object and the first object are the same object, and from the identification information of the owner, it is possible to determine that the owner of the second object is the owner of the first object. By using image data in which the surface pattern unique to each object is photographed, it is possible to perform collation if there is image data in which the surface of the object is photographed. As a result, it is possible to obtain a highly accurate collation result while reducing the burden on the user. As described above, use of the management system of the present example embodiment makes it possible to specify the owner of an object without requiring complicated work.</p><heading id="h-0012" level="1">Second Example Embodiment</heading><p id="p-0053" num="0054">The second example embodiment of the present invention will be described in detail with reference to the drawings. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view illustrating an outline of the configuration of the management system of the present example embodiment. The management system of the present example embodiment includes a user information management device <b>10</b>, a collation device <b>20</b>, a user terminal device <b>30</b>, a manager terminal device <b>40</b>, and an image-capturing device <b>50</b>.</p><p id="p-0054" num="0055">The management system of the present example embodiment is assumed that the first image data of the surface pattern and the identification information of the owner of the first object whose owner is made clear by the identification information are transmitted from the user terminal device <b>30</b> to the user information management device <b>10</b> in advance and managed. Furthermore, it is assumed that the owner loses the first object, then the object is reported to the manager, and is managed as the second object by the user information management device <b>10</b>. In this case, in order to search for the lost object, the collation device <b>20</b> of the management system acquires the first image and the identification information, as well as the second image data of the surface pattern of the second object whose owner is unknown due to the loss of the first object. The collation device <b>20</b> collates the first image with the second image. Then, in a case where the feature of the surface pattern of the first object is similar to the feature of the surface pattern of the second object, the collation device <b>20</b> specifies the owner of the second object from the identification information by specifying the identification information of the owner of the first object. In the management system of the present example embodiment, image data in which the object surface pattern is photographed is used as image data of the surface pattern of the object used for collation. In the second example embodiment, the surface pattern of an object is described as an object fingerprint.</p><p id="p-0055" num="0056">The management system of the present example embodiment can be used as a lost item management system in the lost and found in public transport as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, for example. In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the user terminal device <b>30</b>, which is a terminal device such as a smartphone owned by the user, inputs user information and acquires image data of the object fingerprint of the belongings. The user information includes information for identifying an individual such as a name, and contact information such as a telephone number and an e-mail address. The user information may include information of an account of a social networking service (SNS). The user information and the image data of the object fingerprint of the belongings are sent to the user information management device <b>10</b> run by a public transport operator or another operator, and the user information and the data of the object fingerprint, which is the identification information of the individual, are stored in association with each other. The lost item management system as in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be installed in public transport that runs the lost and found, or may have a configuration in which a part of the management system is installed in an operator other than public transport and accessed from the lost and found via the network. For example, the lost item management system may be configured to access, via the network, the user information management device <b>10</b> and the collation device <b>20</b> managed by an operator other than public transport from the manager terminal device <b>40</b> installed in the lost and found. The user information management device <b>10</b> and the collation device <b>20</b> may be managed by different operators and connected to each other via the network.</p><p id="p-0056" num="0057">In the lost and found that handles lost items in public transport or the like, the object fingerprint of a lost item whose owner is unknown is photographed by the image-capturing device <b>50</b>. The lost item management server, that is, the manager terminal device <b>40</b> sends the collation device <b>20</b> the image data of the object fingerprint of the lost item photographed by the image-capturing device <b>50</b>. The collation device <b>20</b> collates the object fingerprint photographed by the image-capturing device <b>50</b> of the lost and found with an object fingerprint registered in the user information management device <b>10</b>, and specifies the identification information of the owner of the first object when the feature of the object fingerprint of the first object is similar to the feature of the object fingerprint of the second object. When there is image data in which the object fingerprints are similar to each other, the lost item associated with the object fingerprint photographed by the image-capturing device <b>50</b> is determined to be the belongings of the owner associated with the object fingerprint registered in the user information management device <b>10</b>.</p><p id="p-0057" num="0058">Similarity is not limited to a case where the object fingerprint photographed by the image-capturing device <b>50</b> and the object fingerprint registered in the user information management device <b>10</b> match by 100%, and may include an allowable value of matching in a range of equal to or more than 90%, for example, matching by equal to or more than 95%. The reference value in the range of similarity may be a value other than the value described above. The reference of the range of similarity may be set using an index other than a numerical value as long as it can indicate whether two objects are similar.</p><p id="p-0058" num="0059">The configuration of each device of the management system of the present example embodiment will be described.</p><p id="p-0059" num="0060">[User Information Management Device]</p><p id="p-0060" num="0061">First, the configuration of the user information management device <b>10</b> will be described. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a view illustrating the configuration of the user information management device <b>10</b>. The user information management device <b>10</b> includes a user information input unit <b>11</b>, a user information management unit <b>12</b>, a user information storage unit <b>13</b>, a data output unit <b>14</b>, and a data request input unit <b>15</b>. The user information management device <b>10</b> is a device that manages the information registered by the user, the identification information and contact of the user, and the image data of the object fingerprint of the user's belongings.</p><p id="p-0061" num="0062">The user information input unit <b>11</b> receives the user information sent from the user terminal device <b>30</b>, that is, the identification information of the user and the contact user information, and the image data of the object fingerprint of the user's belongings. The user information input unit <b>11</b> outputs the user information and the image data to the user information management unit <b>12</b>.</p><p id="p-0062" num="0063">The user information management unit <b>12</b> stores, in the user information storage unit <b>13</b>, the user information and the image data of the object fingerprint of the user's belongings in association with each other. As the identification information of the user in the user information, an identifier (ID) assigned to each user is used. As the identification information of the user, information of the contact of the user such as the telephone number or the mail address may be used instead of the ID exclusively assigned. As the identification information of the user, information associated with the individual, such as an account of an SNS, can also be used.</p><p id="p-0063" num="0064">Based on a request from the collation device <b>20</b>, the user information management unit <b>12</b> reads the image data of the object fingerprint from the user information storage unit <b>13</b> and sends it to the collation device <b>20</b> via the data output unit <b>14</b>.</p><p id="p-0064" num="0065">The user information storage unit <b>13</b> stores the user information and the image data of the object fingerprint of the user's belongings in association with each other.</p><p id="p-0065" num="0066">The data output unit <b>14</b> transmits the image data of the object fingerprint to the collation device <b>20</b>.</p><p id="p-0066" num="0067">The data request input unit <b>15</b> receives a request for image data of the object fingerprint from the collation device <b>20</b>. The data request input unit <b>15</b> outputs the request for image data to the user information management unit <b>12</b>.</p><p id="p-0067" num="0068">Each processing in the user information input unit <b>11</b>, the user information management unit <b>12</b>, the data output unit <b>14</b>, and the data request input unit <b>15</b> is performed by executing a computer program on the central processing unit (CPU). The computer program for performing each processing is recorded in, for example, a hard disk drive. The CPU executes a computer program for performing each processing by reading the computer program onto the memory.</p><p id="p-0068" num="0069">The user information storage unit <b>13</b> includes a storage device such as a nonvolatile semiconductor storage device or a hard disk drive, or a combination of those storage devices. The user information storage unit <b>13</b> may be provided outside the user information management device <b>10</b> and connected via the network. The user information management device <b>10</b> may be configured by combining a plurality of information processing devices.</p><p id="p-0069" num="0070">[Collation Device]</p><p id="p-0070" num="0071">The configuration of the collation device <b>20</b> will be described. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a view illustrating the configuration of the collation device <b>20</b>. The collation device <b>20</b> includes a collation request input unit <b>21</b>, a data acquisition unit <b>22</b>, a collation unit <b>23</b>, a collation result notification unit <b>24</b>, and a data storage unit <b>25</b>.</p><p id="p-0071" num="0072">The collation request input unit <b>21</b> receives input of a collation request of the object fingerprint from the manager terminal device <b>40</b>. The collation request input unit <b>21</b> receives the image data of the object fingerprint of the collation target object and the collation request from the manager terminal device <b>40</b>. The collation request input unit <b>21</b> outputs, to the collation unit <b>23</b>, the image data of the object fingerprint of the collation target and the collation request.</p><p id="p-0072" num="0073">The data acquisition unit <b>22</b> requests the image data of the object fingerprint registered in the user information management device <b>10</b>, and acquires the image data of the object fingerprint from the user information management device <b>10</b>. The data acquisition unit <b>22</b> outputs the acquired image data to the collation unit <b>23</b>.</p><p id="p-0073" num="0074">The collation unit <b>23</b> collates the object fingerprint of the image data for which the collation request has been received from the manager terminal device <b>40</b> with the object fingerprint of the image data registered in the user information management device <b>10</b>, and determines the presence or absence of similarity. The collation unit <b>23</b> detects a feature point for each of the object fingerprints of the two pieces of image data, and determines whether the two object fingerprints are of the same object based on a similarity, which is a ratio at which the arrangement of feature points match each other. When the similarity of the arrangement of the feature points is equal to or greater than a preset reference, the collation unit <b>23</b> regards that the two object fingerprints are the object fingerprints of the same object.</p><p id="p-0074" num="0075">When there is no object fingerprint similar to the object fingerprint of the image data for which the collation request has been received, the collation unit <b>23</b> sends, to the manager terminal device <b>40</b> via the collation result notification unit <b>24</b>, information indicating that there is no image having a similar object fingerprint. Upon detecting an object fingerprint similar to the object fingerprint of the image data for which the collation request has been received, the collation unit <b>23</b> sends, to the manager terminal device <b>40</b> via the collation result notification unit <b>24</b>, the user information associated with the image data of the object fingerprint.</p><p id="p-0075" num="0076">The collation result notification unit <b>24</b> sends the manager terminal device <b>40</b> the collation result received from the collation unit <b>23</b>.</p><p id="p-0076" num="0077">The data storage unit <b>25</b> stores image data for which the object fingerprint is collated and the user information associated with the image data received from the user information management device <b>10</b>.</p><p id="p-0077" num="0078">Each processing in the collation request input unit <b>21</b>, the data acquisition unit <b>22</b>, the collation unit <b>23</b>, and the collation result notification unit <b>24</b> is performed by executing a computer program on the CPU. The computer program for performing each processing is recorded in, for example, a hard disk drive. The CPU executes a computer program for performing each processing by reading the computer program onto the memory.</p><p id="p-0078" num="0079">The data storage unit <b>25</b> includes a storage device such as a nonvolatile semiconductor storage device or a hard disk drive, or a combination of those storage devices.</p><p id="p-0079" num="0080">[Manager Terminal Device]</p><p id="p-0080" num="0081">The configuration of the manager terminal device <b>40</b> will be described. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a view illustrating the configuration of the manager terminal device <b>40</b>. The manager terminal device <b>40</b> includes an image data input unit <b>41</b>, an object management unit <b>42</b>, a data storage unit <b>43</b>, an image data transmission unit <b>44</b>, an information input unit <b>45</b>, and a collation result output unit <b>46</b>.</p><p id="p-0081" num="0082">The image data input unit <b>41</b> receives the image data of the object fingerprint of the management target object from the image-capturing device <b>50</b>. In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the image data input unit <b>41</b> acquires the image data of the object fingerprint of a lost item from the image-capturing device <b>50</b>. The image data input unit <b>41</b> outputs the image data of the object fingerprint to the object management unit <b>42</b>.</p><p id="p-0082" num="0083">The object management unit <b>42</b> stores, in the data storage unit <b>43</b>, the image data of the object fingerprint input from the image-capturing device <b>50</b> via the image data input unit <b>41</b>. The object management unit <b>42</b> sends the image data of the object fingerprint photographed by the image-capturing device <b>50</b> to the collation device <b>20</b> via the image data transmission unit <b>44</b>, and requests collation of the object fingerprint. The object management unit <b>42</b> acquires information of the collation result from the collation device <b>20</b> via the information input unit <b>45</b>, and outputs the collation result via the collation result output unit <b>46</b>.</p><p id="p-0083" num="0084">The data storage unit <b>43</b> stores the image data of the object fingerprint photographed by the image-capturing device <b>50</b>.</p><p id="p-0084" num="0085">The image data transmission unit <b>44</b> transmits the image data photographed by the image-capturing device <b>50</b> to the collation device <b>20</b>. The image data transmission unit <b>44</b> requests the collation device <b>20</b> for whether there is image data similar to the object fingerprint of the object photographed by the image-capturing device <b>50</b>.</p><p id="p-0085" num="0086">The collation result output unit <b>46</b> outputs information of the owner of the object photographed by the image-capturing device <b>50</b> based on the collation result. When indicating that the collation result has nothing similar to the object photographed by the image-capturing device <b>50</b>, the collation result output unit <b>46</b> outputs that the owner is unknown.</p><p id="p-0086" num="0087">Each processing in the image data input unit <b>41</b>, the object management unit <b>42</b>, the image data transmission unit <b>44</b>, the information input unit <b>45</b>, and the collation result output unit <b>46</b> is performed by executing a computer program on the CPU. The computer program for performing each processing is recorded in, for example, a nonvolatile semiconductor storage device. The CPU executes a computer program for performing each processing by reading the computer program onto the memory. The data storage unit <b>43</b> includes a nonvolatile semiconductor storage device. The above is the configuration of the manager terminal device <b>40</b>.</p><p id="p-0087" num="0088">The image-capturing device <b>50</b> photographs the surface shape of an object and generates image data of the object fingerprint. The image-capturing device <b>50</b> includes a complementary metal oxide semiconductor (CMOS) image sensor. As the image-capturing unit <b>31</b>, an image sensor other than the CMOS may be used as long as it can photograph the object fingerprint. The image-capturing device <b>50</b> may be configured to include a lens module capable of changing magnification to photograph two images of the entire object and the object fingerprint on the surface of the object.</p><p id="p-0088" num="0089"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a view schematically illustrating an example of the configuration in which the image-capturing device <b>50</b> photographs image data of the object fingerprint to be input to the manager terminal device <b>40</b>. In <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a belt conveyor <b>62</b> conveys an object <b>61</b>, which is a lost item. The image-capturing device <b>50</b> photographs the object fingerprint of the object <b>61</b> conveyed on the belt conveyor <b>62</b> and outputs image data of the object fingerprint.</p><p id="p-0089" num="0090">[Operation Description]</p><p id="p-0090" num="0091">The operation of the management system of the present example embodiment will be described. <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a view illustrating the operation flow of the user information management device <b>10</b> illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a view illustrating the operation flow of the manager terminal device <b>40</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a view illustrating the operation flow of the collation device <b>20</b> illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0091" num="0092">First, the user operates a camera of the user terminal device <b>30</b> to register information of himself and image data of the object fingerprint of belongings. The user inputs the user's own name and contact to the user terminal device <b>30</b> as user information. As one or both of the user name and contact of the user information, information stored in advance in the user terminal device <b>30</b> may be used. The user terminal device <b>30</b> transmits the user information and the image data of the object fingerprint of the user's belongings to the user information management device <b>10</b>.</p><p id="p-0092" num="0093">The user information and the image data of the object fingerprint of the user's belongings sent to the user information management device <b>10</b> are input to the user information input unit <b>11</b> of the user information management device <b>10</b>. In <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>8</b></figref>, upon acquiring the user information and the image data of the object fingerprint of the user's belongings (step S<b>21</b>), the user information input unit <b>11</b> sends the user information management unit <b>12</b> the user information and the image data of the object fingerprint of the user's belongings.</p><p id="p-0093" num="0094">Upon receiving the user information and the image data of the object fingerprint of the user's belongings, the user information management unit <b>12</b> stores, in the user information storage unit <b>13</b>, the user information and the image data of the object fingerprint of the user's belongings in association with each other (step S<b>22</b>).</p><p id="p-0094" num="0095">Next, in the manager terminal device <b>40</b>, the image-capturing device <b>50</b> acquires the image data of the object fingerprint of the object <b>61</b>, and the image data of the object fingerprint is input to the manager terminal device <b>40</b>.</p><p id="p-0095" num="0096">As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, upon photographing the object fingerprint of the object <b>61</b>, the image-capturing device <b>50</b> sends image data of the object fingerprint to the manager terminal device <b>40</b>. Upon photographing the object fingerprint, identification information for management of the object <b>61</b> may be associated with the image data of the object fingerprint. For example, the object <b>61</b> may be placed on a tray and conveyed, and the identification information of the tray may be used as the identification information of the object <b>61</b>. In case of such configuration, the information on the tray is taken in by a reader reading an IC chip, a barcode, or the like added to the tray. The object <b>61</b> may be allocated with the identification information for management based on the order of photographing the object fingerprint. An image of the entire object <b>61</b> may be captured simultaneously with the object fingerprint. By capturing an image of the entire object <b>61</b>, the type of the object <b>61</b> can be classified.</p><p id="p-0096" num="0097">In <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>9</b></figref>, the data of the object fingerprint of the object photographed by the image-capturing device <b>50</b> is input to the image data input unit <b>41</b> of the manager terminal device <b>40</b>. Upon acquiring the image data of the object fingerprint (step S<b>31</b>), the image data input unit <b>41</b> sends the image data of the object fingerprint to the object management unit <b>42</b>. Upon receiving the image data of the object fingerprint, the object management unit <b>42</b> stores the image data of the object fingerprint in the data storage unit <b>43</b>. Upon storing the image data of the object fingerprint, the object management unit <b>42</b> sends the image data of the object fingerprint to the image data transmission unit <b>44</b>. Upon receiving the image data of the object fingerprint, the object management unit <b>42</b> sends the collation device <b>20</b> the image data of the object fingerprint and a collation request (step S<b>32</b>).</p><p id="p-0097" num="0098">The image data of the object fingerprint and the collation request are input to the collation request input unit <b>21</b> of the collation device <b>20</b>. Upon receiving the image data of the object fingerprint and the collation request, the collation request input unit <b>21</b> sends the collation unit <b>23</b> the image data of the object fingerprint and the collation request. In <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>10</b></figref>, the image data of the object fingerprint and the collation request are acquired (step S<b>41</b>), and the collation unit <b>23</b> stores the image data of the object fingerprint in the data storage unit <b>25</b>. Upon storing the image data of the object fingerprint, the collation unit <b>23</b> requests the data acquisition unit <b>22</b> for the image data of the object fingerprint held by the user information management device <b>10</b>.</p><p id="p-0098" num="0099">Upon receiving the request for the image data of the object fingerprint, the data acquisition unit <b>22</b> sends the request for the image data of the object fingerprint to the user information management device <b>10</b>.</p><p id="p-0099" num="0100">The request for the image data of the object fingerprint is input to the user information management device <b>10</b>. In <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>80</b></figref>, upon acquiring the request for the image data of the object fingerprint (step S<b>23</b>), the user information management device <b>10</b> associates the user information with the image data of the object fingerprint and sends the data of the object fingerprint to the collation device <b>20</b> (step S<b>24</b>). When there is an unsent image in the stored image data of the object fingerprint, the user information management device <b>10</b> may repeat the processing of transmitting the image data of the object fingerprint. When the transmission of the stored object fingerprint to the collation device <b>20</b> is completed (No in step S<b>25</b>), the user information management device <b>10</b> ends the transmission of the image data of the object fingerprint to the collation device <b>20</b>. In <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>10</b></figref>, the image data of the object fingerprint sent to the collation device <b>20</b> is input to the data acquisition unit <b>22</b>. Upon acquiring the image data of the object fingerprint (step S<b>42</b>), the data acquisition unit <b>22</b> sends the image data of the object fingerprint to the collation unit <b>23</b>.</p><p id="p-0100" num="0101">The collation unit <b>23</b> collates the image data of the object fingerprint sent from the user information management device <b>10</b> with the image data of the object fingerprint sent from the manager terminal device <b>40</b> stored in the data storage unit <b>25</b> (step S<b>43</b>). When the object fingerprint sent from the manager terminal device <b>40</b> is similar to the object fingerprint sent from the user information management device <b>10</b> (Yes in step S<b>44</b>), the collation unit <b>23</b> extracts the user information associated with the image data of the object fingerprint sent from the user information management device <b>10</b>. The collation unit <b>23</b> notifies, via the data acquisition unit <b>22</b>, the user information management device <b>10</b> that the collation is completed.</p><p id="p-0101" num="0102">The collation device <b>20</b> may perform collation a plurality of times on the image data for which collation has been requested. When collation is performed a plurality of times, the frequency of collation may be changed according to the lapse of time or the number of times of collation. For example, in a case where a certain period of time has elapsed from the time when collation is newly requested, it is possible to specify the owner of the object discovered after the lapse of time while maintaining the frequency of the image data for which the collation is newly requested with a high possibility of specifying the owner by increasing the interval of collation.</p><p id="p-0102" num="0103">Upon extracting the user information associated with the image data of the object fingerprint, the collation unit <b>23</b> sends the user information to the collation result notification unit <b>24</b>. Upon receiving the user information, the collation result notification unit <b>24</b> sends the manager terminal device <b>40</b> a collation result including the user information (step S<b>45</b>).</p><p id="p-0103" num="0104">The user information sent to the manager terminal device <b>40</b> is input to the information input unit <b>45</b> of the manager terminal device <b>40</b>. In <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>9</b></figref>, upon receiving the collation result (step S<b>33</b>), the object management unit <b>42</b> checks the content of the collation result. When the collation result includes the user information and the owner of the object has been specified (Yes in step S<b>34</b>), the user information is sent to the collation result output unit <b>46</b>. Upon receiving the user information, the collation result output unit <b>46</b> outputs the user information as information of the owner of the object (step S<b>35</b>). For example, the collation result output unit <b>46</b> outputs, to a display device, as display data, the name and contact of the owner included in the user information as information of the owner of the object. The worker who sees the display notifies the owner of the object that there is an object in custody. In a case where the user information includes an e-mail address, the collation result output unit <b>46</b> may transmit, to the mail address, an e-mail notifying that the object is in custody. In a case where the user information includes an SNS account, the collation result output unit <b>46</b> may notify the SNS account that the object is in custody. In a case where a function of receiving a notification to the SNS is implemented in the user terminal device <b>30</b> as an application program, the collation result output unit <b>46</b> may notify the application program. The application program of the SNS may have a function of registering image data of belongings and user information into the user information management device <b>10</b>.</p><p id="p-0104" num="0105">In <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>10</b></figref>, when the object fingerprint stored in the data storage unit <b>25</b> is not similar to the object fingerprint sent from the user information management device <b>10</b> (No in step S<b>44</b>), the collation unit <b>23</b> checks whether there is uncollated image data. When there is uncollated image data (Yes in step S<b>46</b>), the process returns to step S<b>42</b>, and the collation unit <b>23</b> next repeats the operation of comparing the object fingerprint sent from the user information management device <b>10</b> with the object fingerprint stored in the data storage unit <b>25</b>.</p><p id="p-0105" num="0106">When there is no uncollated image data and there is no similar object fingerprint even if collation is performed for all the image data (No in step S<b>46</b>), the collation unit <b>23</b> sends the collation result notification unit <b>24</b> a collation result indicating that there is no image data of a similar object fingerprint. Upon receiving the collation result indicating that there is no image data of a similar object fingerprint, the collation result notification unit <b>24</b> sends the manager terminal device <b>40</b> the collation result indicating that there is no image data of a similar object fingerprint (step S<b>47</b>). The collation result indicating that there is no image data of a similar object fingerprint sent to the manager terminal device <b>40</b> is input to the information input unit <b>45</b> of the manager terminal device <b>40</b>.</p><p id="p-0106" num="0107">In <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>9</b></figref>, upon receiving the collation result (step S<b>33</b>), the object management unit <b>42</b> of the manager terminal device <b>40</b> checks the content of the collation result. When the collation result indicates that there is no image data of a similar object fingerprint and the owner has not been specified (No in step S<b>34</b>), the object management unit <b>42</b> sends the collation result output unit <b>46</b> information indicating that the owner of the object is unknown. Upon receiving the information that the owner of the object is unknown, the collation result output unit <b>46</b> outputs information that the owner of the object is unknown (step S<b>36</b>). For example, the collation result output unit <b>46</b> outputs, to the display device as display data, information that the owner of the object is unknown. The worker who sees the display keeps the object in custody as an object whose owner is unknown.</p><p id="p-0107" num="0108">[Modification]</p><p id="p-0108" num="0109">Another configuration example of the management system of the second example embodiment will be described. In the above example, the owner information and the image data of the object fingerprint are registered in advance in the user information management device <b>10</b>. Instead of such configuration, the user information management device <b>10</b> may be notified of information of a target object when the user notices a lost item or a dropped item. <figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates an example of a case where a configuration of notifying the user information management device <b>10</b> of information of a target object when the user notices a lost item or a dropped item is applied to the lost item management system in the lost and found in public transport as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0109" num="0110">In the example of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the user terminal device <b>30</b>, which is a terminal device such as a smartphone owned by the user, inputs user information and acquires image data of the object fingerprint of the belongings. The user information includes information for identifying an individual such as a name, and contact information such as a telephone number and an e-mail address. The user information and the image data of the object fingerprint of the belongings are stored in the user terminal device <b>30</b>. When noticing that his own belongings is lost, the user of the user terminal device <b>30</b> sends the image data of the object fingerprint of the belongings stored in the user terminal device <b>30</b> to the user information management device <b>10</b> run by a public transport operator or another operator.</p><p id="p-0110" num="0111">In the lost and found of public transport or the like, the object fingerprint of a lost item whose owner is unknown is photographed by the image-capturing device <b>50</b>. The lost item management server, that is, the manager terminal device <b>40</b> sends the collation device <b>20</b> the image data of the object fingerprint of the lost item photographed by the image-capturing device <b>50</b>. The collation device <b>20</b> collates the object fingerprint transmitted by the user to the user information management device <b>10</b> with the object fingerprint photographed by the image-capturing device <b>50</b> of the lost and found, and checks whether there is image data having a similar object fingerprint. When there are image data in which the object fingerprints are similar to each other, the lost item associated with the object fingerprint photographed by the image-capturing device <b>50</b> matches the object associated with the object fingerprint transmitted from the user terminal device <b>30</b> by the user, and is determined to be the user's belongings.</p><p id="p-0111" num="0112">It is described an operation in a case where the management system of the present example embodiment is applied to the configuration as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> that notifies the user information management device <b>10</b> of information of a target object when the user notices a lost item or a dropped item.</p><p id="p-0112" num="0113"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a view illustrating the operation flow of the manager terminal device <b>40</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is a view illustrating the operation flow of the user information management device <b>10</b> illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. <figref idref="DRAWINGS">FIG. <b>14</b></figref> is a view illustrating the operation flow of the collation device <b>20</b> illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0113" num="0114">First, the user operates the camera of the user terminal device <b>30</b> to register information of the object fingerprint of the belongings. The user inputs the user's own name and contact to the user terminal device <b>30</b>. As the name and contact of the user, information stored in advance in the user terminal device <b>30</b> may be used. The information input by the user is stored in the data storage unit in the user terminal device <b>30</b>. When image data of a plurality of objects are stored, the above operation is repeated.</p><p id="p-0114" num="0115">On the other hand, the image-capturing device <b>50</b> photographs the object fingerprint of the item in custody, and sends the manager terminal device <b>40</b> the image data of the object fingerprint of the object <b>61</b>, which is the item in custody. Upon photographing the object fingerprint of the object <b>61</b>, the image-capturing device <b>50</b> sends the image data of the object fingerprint to the manager terminal device <b>40</b>. When photographing the object fingerprint, identification information for identifying the object <b>61</b> may be associated with the image data of the object fingerprint. For example, the object <b>61</b> may be placed on a tray and conveyed, and the identification information of the tray may be used as the identification information of the object <b>61</b>. In case of such configuration, the information on the tray is taken in by a reader reading an IC chip, a barcode, or the like added to the tray. An image of the entire object <b>61</b> may be captured simultaneously with the object fingerprint. By capturing an image of the entire object <b>61</b>, the type of the object <b>61</b> can be classified.</p><p id="p-0115" num="0116">In <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>12</b></figref>, the data of the object fingerprint of the item in custody is input to the image data input unit <b>41</b>. Upon acquiring the image data of the object fingerprint (step S<b>61</b>), the image data input unit <b>41</b> sends the image data of the object fingerprint to the object management unit <b>42</b>. Upon receiving the image data of the object fingerprint, the object management unit <b>42</b> stores the image data of the object fingerprint into the data storage unit <b>43</b> (step S<b>62</b>).</p><p id="p-0116" num="0117">When whereabouts of the user's belongings becomes unknown, the user of the user terminal device <b>30</b> operates an operation unit of the user terminal device <b>30</b> to select image data of the belongings that has become unknown. The user terminal device <b>30</b> transmits a collation request and the image data to the user information management device <b>10</b>.</p><p id="p-0117" num="0118">The collation request and the image data of the object fingerprint of the user's belongings sent to the user information management device <b>10</b> are input to the user information input unit <b>11</b> of the user information management device <b>10</b>. In <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>13</b></figref>, upon acquiring the collation request and the image data of the object fingerprint (step S<b>71</b>), the user information input unit <b>11</b> sends the user information management unit <b>12</b> the user information and the image data of the object fingerprint of the user's belongings.</p><p id="p-0118" num="0119">Upon receiving the collation request and the image data of the object fingerprint of the user's belongings, the user information management unit <b>12</b> stores, in the user information storage unit <b>13</b>, the image data of the object fingerprint of the user's belongings and the user information attached to the image data (step S<b>72</b>).</p><p id="p-0119" num="0120">Upon storing the image data of the object fingerprint, the object management unit <b>42</b> sends the image data transmission unit <b>44</b> the image data of the object fingerprint and a collation request. Upon receiving the image data of the object fingerprint and the collation request, the object management unit <b>42</b> sends the collation device <b>20</b> the image data of the object fingerprint and the collation request (step S<b>73</b>).</p><p id="p-0120" num="0121">The image data of the object fingerprint and the collation request sent from the user information management device <b>10</b> are input to the collation request input unit <b>21</b> of the collation device <b>20</b>. In <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>14</b></figref>, upon acquiring the image data of the object fingerprint and the collation request (step S<b>81</b>), the collation request input unit <b>21</b> sends the collation unit <b>23</b> the image data of the object fingerprint and a collation request. Upon receiving the image data of the object fingerprint and the collation request, the collation unit <b>23</b> stores the image data of the object fingerprint into the data storage unit <b>25</b>. Upon storing the image data of the object fingerprint, the collation unit <b>23</b> requests the data acquisition unit <b>22</b> for the image data of the object fingerprint held by the manager terminal device <b>40</b>.</p><p id="p-0121" num="0122">Upon receiving the request for the image data of the object fingerprint, the data acquisition unit <b>22</b> sends the request for the image data of the object fingerprint to the manager terminal device <b>40</b>.</p><p id="p-0122" num="0123">In <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>12</b></figref>, the request for the image data of the object fingerprint is input to the information input unit <b>45</b>. Upon acquiring the request for the image data of the object fingerprint (step S<b>63</b>), the information input unit <b>45</b> sends the object management unit <b>42</b> the request for the image data of the object fingerprint. Upon receiving the request for the image data of the object fingerprint, the object management unit <b>42</b> reads the image data of the object fingerprint from the data storage unit <b>43</b> and sends it to the image data transmission unit <b>44</b>. Upon receiving the image data of the object fingerprint, the image data transmission unit <b>44</b> sends the data of the object fingerprint to the collation device <b>20</b> (step S<b>64</b>). In a state where the collation result has not been received (No in step S<b>65</b>), when there is an unsent image among the stored image data of the object fingerprint (Yes in step S<b>67</b>), the process returns to step S<b>64</b>, and the processing of transmission of the image data of the object fingerprint is repeated. When the transmission of the stored object fingerprint to the collation device <b>20</b> is completed (No in step S<b>67</b>), the transmission of the image data of the object fingerprint to the collation device <b>20</b> is ended.</p><p id="p-0123" num="0124">The image data of the object fingerprint sent to the collation device <b>20</b> is input to the data acquisition unit <b>22</b>. In <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>14</b></figref>, upon receiving the image data of the object fingerprint of the item in custody (step S<b>82</b>), the data acquisition unit <b>22</b> sends the image data of the object fingerprint to the collation unit <b>23</b>.</p><p id="p-0124" num="0125">The collation unit <b>23</b> collates the image data of the object fingerprint sent from the manager terminal device <b>40</b> with the image data of the object fingerprint of the user's belongings stored in the data storage unit <b>25</b> (step S<b>83</b>). When the object fingerprint of the user's belongings is similar to the object fingerprint sent from the manager terminal device <b>40</b> (Yes in step S<b>84</b>), the collation unit <b>23</b> transmits, to the user information management device <b>10</b> and the manager terminal device <b>40</b> via the collation result notification unit <b>24</b>, a collation result indicating that the object fingerprints are similar to each other (step S<b>85</b>). The collation unit <b>23</b> transmits the collation result to be sent to the user information management device <b>10</b> in association with the information of the place where the user's belongings is in custody. The collation unit <b>23</b> transmits the collation result to be sent to the manager terminal device <b>40</b> in association with the user information.</p><p id="p-0125" num="0126">In <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>13</b></figref>, upon receiving the collation result (step S<b>74</b>), the user information management unit <b>12</b> of the user information management device <b>10</b> transmits the collation result to the manager terminal device <b>40</b> via the data output unit <b>14</b> (step S<b>75</b>).</p><p id="p-0126" num="0127">In <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>14</b></figref>, when the object fingerprint of the user's belongings is not similar to the object fingerprint sent from the manager terminal device <b>40</b> in step S<b>83</b> (No in step S<b>84</b>), the collation unit <b>23</b> checks whether there is uncollated image data. When there is uncollated image data (Yes in step S<b>86</b>), the process returns to step S<b>82</b>, and the collation unit <b>23</b> repeats the operation of comparing the object fingerprint to be next sent from the manager terminal device <b>40</b> with the object fingerprint stored in the data storage unit <b>25</b>.</p><p id="p-0127" num="0128">When there is no uncollated image data and there is no similar image data even if collation is performed for all the image data (No in step S<b>86</b>), the collation unit <b>23</b> sends the collation result notification unit <b>24</b> a collation result indicating that there is no image data of an object having a similar object fingerprint.</p><p id="p-0128" num="0129">Upon receiving the collation result, the collation result notification unit <b>24</b> transmits, to the user information management device <b>10</b>, the collation result indicating that there is no image data of an object having a similar object fingerprint (step S<b>87</b>).</p><p id="p-0129" num="0130">In <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>13</b></figref>, upon receiving the collation result from the collation device <b>20</b> (step S<b>74</b>), the user information management unit <b>12</b> of the user information management device <b>10</b> transmits the collation result to the manager terminal device <b>40</b> via the data output unit <b>14</b> (step S<b>75</b>).</p><p id="p-0130" num="0131">Upon receiving the collation result, the user terminal device <b>30</b> outputs the collation result to a display unit. When the collation result is a result indicating that there is something matching the belongings, the user terminal device <b>30</b> displays, on the display unit, information of the place where it is in custody. When the collation result is a result indicating that there is nothing matching the belongings, the user terminal device <b>30</b> displays, on the display unit, information indicating that the belongings has not been found.</p><p id="p-0131" num="0132">Upon receiving the collation result (Yes in step S<b>65</b>), the object management unit <b>42</b> stops transmission of the image data. The object management unit <b>42</b> notifies, via the collation result output unit <b>46</b>, the worker of the owner information included in the collation result.</p><p id="p-0132" num="0133">In the configuration as in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the collation request is made when the user notices the occurrence of a lost item such as a lost item, so that the collation target can be narrowed down to the lost items occurring within a certain period and the lost item notified from the user. This makes it possible to reduce the processing amount required for collation of the object fingerprint, and specify the owner.</p><p id="p-0133" num="0134">In the configuration as in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the image data of the object fingerprint photographed by the user terminal device <b>30</b> may be registered in advance into the user information management device <b>10</b> or another server having a storage device. In such configuration, when noticing a lost item, the user selects an object to search for from objects registered in advance, and transmits, to the user information management device <b>10</b>, information on the object of the target to search for. In such configuration, in a case where there is no collation request from the user side, collation may be performed in response to a request from the manager terminal device <b>40</b> side.</p><p id="p-0134" num="0135">The management system of the present example embodiment can notify the owner of occurrence and custody of a lost item because data of the object fingerprint of the object has been registered even if the owner does not notice the loss when the lost item occurs. Therefore, it is possible to suppress the place and cost required for custody of the lost item. Even in a case where objects of the same design or a similar design are present at the same time as lost items, it is possible to suppress a workload required for specifying which object belongs to which person. Even in a case where objects of the same design or a similar design are present at the same time as lost items, it is possible to return the object to the correct owner without being confused with another person.</p><p id="p-0135" num="0136">In the above example, the lost and found in a station, that is, a railway operator has been described as an example, but the management system of the present example embodiment can also be used for management of lost items in other transport such as buses, airplanes, and ships other than railways. The present invention is particularly effective in object management not only in transport but also in facilities used by many people such as public facilities, commercial facilities, sports grounds, and cultural facilities. The present invention can be used not only in a facility but also in an administrative agency for lost item management in a public space.</p><p id="p-0136" num="0137">The management system of the present example embodiment can be used to specify a source of a fallen object by registering object fingerprints of actually used components together with identification information of vehicles and airframes for components that are likely to fall off, such as cars, trains, and aircrafts. Use for such applications makes it possible to make clear the whereabouts of responsibility of a fallen object, and it is also possible to suppress continuation of operation with the component being dropped and improve safety.</p><p id="p-0137" num="0138">In the above example, the manager terminal device <b>40</b> is installed in only one place, but the manager terminal device <b>40</b> may be installed in a plurality of places such as different facilities of different operators or the same operator, and each may be configured to request the collation device <b>20</b> for collation. A plurality of the user information management devices <b>10</b> may be installed, and the collation device <b>20</b> may access each of the user information management devices <b>10</b> to acquire image data of the object fingerprint used for collation. All or any two of the user information management device <b>10</b>, the collation device <b>20</b>, and the manager terminal device <b>40</b> may be installed at the same place, or may be installed as an integrated device.</p><p id="p-0138" num="0139">In the management system of the present example embodiment, the object fingerprint sent from the user terminal device <b>30</b> and the object fingerprint photographed by the image-capturing device <b>50</b> and sent from the manager terminal device <b>40</b> are collated by the collation device <b>20</b>. When the collation result that the object fingerprints are similar is obtained in the collation device <b>20</b>, the object associated with the object fingerprint sent from the user terminal device <b>30</b> and the object associated with the object fingerprint photographed by the image-capturing device <b>50</b> and sent from the manager terminal device <b>40</b> can be regarded as the same object. Therefore, by collating the object fingerprint, it is possible to discriminate that the owner of the object for which the image-capturing device <b>50</b> has photographed the object fingerprint is the user terminal device <b>30</b>.</p><p id="p-0139" num="0140">Since the management system of the present example embodiment only needs to obtain image data of the object fingerprint acquired by photographing the surface shape of an object, the user or the like is not required to have a high skill. Since a pattern unique to an object is used, it is possible to discriminate individual objects even if the objects are of the same type. Therefore, use of the management system of the present example embodiment makes it possible to specify the owner of an object without requiring complicated work.</p><heading id="h-0013" level="1">Third Example embodiment</heading><p id="p-0140" num="0141">The third example embodiment of the present invention will be described in detail with reference to the drawings. <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a view illustrating the configuration of the management system of the present example embodiment. The management system of the present example embodiment includes an entry/exit device <b>70</b>, an object management device <b>80</b>, and a collation device <b>90</b>. In the second example embodiment, the owner of an object away from the owner's hand is specified. The management system of the present example embodiment is characterized by specifying, by collation of object fingerprints, whether an object possessed by a leaving person from a zone where entry/exit is managed is identical to an object possessed by the person at the time of entry, and controlling a gate that manages entry/exit depending on whether the belongings is the same.</p><p id="p-0141" num="0142">The configuration of each device of the management system of the present example embodiment will be described.</p><p id="p-0142" num="0143">[Collation Device]</p><p id="p-0143" num="0144">The configuration of the entry/exit device <b>70</b> will be described. <figref idref="DRAWINGS">FIG. <b>16</b></figref> is a view illustrating the configuration of the entry/exit device <b>70</b>. The entry/exit device <b>70</b> includes a gate <b>71</b>, an entry side reading unit <b>72</b>, an entry side image-capturing unit <b>73</b>, an exit side reading unit <b>74</b>, an exit side image-capturing unit <b>75</b>, a gate control unit <b>76</b>, an entry side door <b>77</b>, and an exit side door <b>78</b>.</p><p id="p-0144" num="0145">The gate <b>71</b> is a body unit of the entry/exit device that manages entry into the zone managed by opening and closing of a door and exit from the managed zone.</p><p id="p-0145" num="0146">The entry side reading unit <b>72</b> reads ID of an entering person. The entry side reading unit <b>72</b> reads the ID of the entering person from a contactless IC card held over a reading unit by the entering person. The entry side reading unit <b>72</b> may read an identification number unique to the IC card. The entry side reading unit <b>72</b> reads information from the IC card by near-field communication. The entry side reading unit <b>72</b> may be configured to optically read identification information indicated by a two-dimensional barcode or the like instead of the IC card.</p><p id="p-0146" num="0147">The entry side image-capturing unit <b>73</b> photographs the object fingerprint of an object possessed by the entering person. The entry side image-capturing unit <b>73</b> includes a camera using a CMOS image sensor.</p><p id="p-0147" num="0148">The exit side reading unit <b>74</b> reads the ID of a leaving person. The exit side reading unit <b>74</b> reads the ID of the leaving person from a contactless IC card held over the reading unit by the leaving person. The exit side reading unit <b>74</b> may read an identification number unique to the IC card. The exit side reading unit <b>74</b> reads information from the IC card by near-field communication. The exit side reading unit <b>74</b> may be configured to optically read identification information indicated by a two-dimensional barcode or the like instead of the IC card. The entry side reading unit <b>72</b> and the exit side reading unit <b>74</b> may specify entering persons and leaving persons by biometric authentication such as face authentication.</p><p id="p-0148" num="0149">The exit side image-capturing unit <b>75</b> photographs the object fingerprint of an object possessed by the leaving person. The exit side image-capturing unit <b>75</b> includes a camera using a CMOS image sensor.</p><p id="p-0149" num="0150">The gate control unit <b>76</b> manages entry/exit by controlling opening and closing of the entry side door <b>77</b> and the exit side door <b>78</b>. The gate control unit <b>76</b> sends the object management device <b>80</b> the data acquired by the entry side reading unit <b>72</b>, the entry side image-capturing unit <b>73</b>, the exit side reading unit <b>74</b>, and the exit side image-capturing unit <b>75</b>. The gate control unit <b>76</b> receives, from the object management device <b>80</b>, a collation result as to whether the belongings of the entering person and the leaving person match.</p><p id="p-0150" num="0151">The gate control unit <b>76</b> includes one or a plurality of semiconductor devices. The processing in the gate control unit <b>76</b> may be performed by executing a computer program on the CPU.</p><p id="p-0151" num="0152">By opening and closing, the entry side door <b>77</b> and the exit side door <b>78</b> manage whether entering persons and leaving persons can pass through.</p><p id="p-0152" num="0153">In the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, the entry side and the exit side gates are separate, but entry and exit may be performed bidirectionally in the same passage lane. The gate on the entry side and the gate on the exit side may be installed at distant positions. In a case of such configuration, the gate control unit <b>76</b> may be provided on each of the entry side and the exit side.</p><p id="p-0153" num="0154">[Object Management Device]</p><p id="p-0154" num="0155">The configuration of the object management device <b>80</b> will be described. <figref idref="DRAWINGS">FIG. <b>17</b></figref> is a view illustrating the configuration of the object management device <b>80</b>. The object management device <b>80</b> includes an entering person information acquisition unit <b>81</b>, an information management unit <b>82</b>, an entering person information storage unit <b>83</b>, a leaving person information acquisition unit <b>84</b>, a collation request unit <b>85</b>, a collation result input unit <b>86</b>, and a check result output unit <b>87</b>.</p><p id="p-0155" num="0156">The entering person information acquisition unit <b>81</b> acquires, from the entry/exit device <b>70</b>, identification information of an entering person and image data of the object fingerprint of belongings of the entering person.</p><p id="p-0156" num="0157">The information management unit <b>82</b> stores, into the entering person information storage unit <b>83</b>, the identification information of the entering person in association with the image data of the object fingerprint of the belongings of the entering person. The information management unit <b>82</b> requests the collation device <b>90</b> for collation of the object fingerprint of the belongings of the entering person of the identification information corresponding to the identification information of the leaving person with the object fingerprint of the belongings of the leaving person. Based on the collation result sent from the collation device <b>90</b>, the information management unit <b>82</b> determines whether the belongings of the leaving person and the belongings of the entering person match together. The information management unit <b>82</b> receives a collation result that the object fingerprint of the object possessed by the leaving person matches the object fingerprint, and specifies the identification information. At this time, the information management unit <b>82</b> determines that the belongings of the leaving person are the same object as the belongings of the entering person associated with the specified identification information.</p><p id="p-0157" num="0158">The entering person information storage unit <b>83</b> stores the identification information of the entering person and the image data of the object fingerprint of the belongings of the entering person.</p><p id="p-0158" num="0159">The leaving person information acquisition unit <b>84</b> acquires, from the entry/exit device <b>70</b>, the identification information of the leaving person and the image data of the object fingerprint of the belongings of the leaving person.</p><p id="p-0159" num="0160">The collation request unit <b>85</b> transmits, to the collation device <b>90</b>, the image data of the object fingerprint of the belongings of the entering person whose identification information matches the identification information of the leaving person and the image data of the object fingerprint of the belongings of the leaving person, and requests collation of the object fingerprints of the two pieces of image data.</p><p id="p-0160" num="0161">The collation result input unit <b>86</b> acquires, from the collation device <b>90</b>, a collation result between the object fingerprint of the belongings of the entering person whose identification information matches the identification information of the leaving person and the object fingerprint of the belongings of the leaving person.</p><p id="p-0161" num="0162">The check result output unit <b>87</b> transmits, to the entry/exit device <b>70</b>, a determination result as to whether the belongings match at the time of entry and at the time of exit.</p><p id="p-0162" num="0163">Each processing in the entering person information acquisition unit <b>81</b>, the information management unit <b>82</b>, the leaving person information acquisition unit <b>84</b>, the collation request unit <b>85</b>, the collation result input unit <b>86</b>, and the check result output unit <b>87</b> is performed by executing a computer program on the CPU. The computer program for performing each processing is recorded in, for example, a hard disk drive. The CPU executes a computer program for performing each processing by reading the computer program onto the memory.</p><p id="p-0163" num="0164">The entering person information storage unit <b>83</b> includes a storage device such as a nonvolatile semiconductor storage device or a hard disk drive, or a combination of those storage devices.</p><p id="p-0164" num="0165">[Collation Device]</p><p id="p-0165" num="0166">The configuration of the collation device <b>90</b> will be described. <figref idref="DRAWINGS">FIG. <b>18</b></figref> is a view illustrating the configuration of the collation device <b>90</b>. The collation device <b>90</b> includes a collation request input unit <b>91</b>, a collation unit <b>92</b>, and a collation result output unit <b>93</b>.</p><p id="p-0166" num="0167">The collation request input unit <b>91</b> receives input of image data of the object fingerprint of the belongings at the time of entry and image data of the object fingerprint of the belongings at the time of exit. The collation request input unit <b>91</b> outputs the received image data to the collation unit <b>92</b>.</p><p id="p-0167" num="0168">The collation unit <b>92</b> collates the object fingerprint of the belongings at the time of entry with the object fingerprint of the belongings at the time of exit, and determines the presence or absence of similarity. The collation unit <b>92</b> collates whether the image of the object fingerprint at the time of entry and the object fingerprint at the time of exit are similar, and if they are similar, regards that the owner of the object to which a second object fingerprint corresponds matches the owner of the object associated with a first object fingerprint, and specifies the identification information associated with the image data of the first object fingerprint. The collation unit <b>92</b> outputs the collation result to the collation result output unit <b>93</b>.</p><p id="p-0168" num="0169">The collation result output unit <b>93</b> sends, to the object management device <b>80</b>, a collation result as to whether the object fingerprint of the belongings at the time of entry matches the object fingerprint of the belongings at the time of exit.</p><p id="p-0169" num="0170">Each processing in the collation request input unit <b>91</b>, the collation unit <b>92</b>, and the collation result output unit <b>93</b> is performed by executing a computer program on the CPU. The computer program for performing each processing is recorded in, for example, a hard disk drive. The CPU executes a computer program for performing each processing by reading the computer program onto the memory.</p><p id="p-0170" num="0171">[Operation Description]</p><p id="p-0171" num="0172">The operation of the management system of the present example embodiment will be described. <figref idref="DRAWINGS">FIG. <b>19</b></figref> is a view illustrating the operation flow of the entry/exit device <b>70</b> illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref>. <figref idref="DRAWINGS">FIG. <b>20</b></figref> is a view illustrating the operation flow of the object management device <b>80</b> illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>. <figref idref="DRAWINGS">FIG. <b>21</b></figref> is a view illustrating the operation flow of the collation device <b>90</b> illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>.</p><p id="p-0172" num="0173">The user holds the IC card over the entry side reading unit <b>72</b>. The entry side reading unit <b>72</b> reads the identification information of the IC card or the identification information of the user recorded in the IC card. Upon reading the identification information, the entry side reading unit <b>72</b> sends the identification information to the gate control unit <b>76</b>.</p><p id="p-0173" num="0174">The user holds the belongings over a camera of the entry side image-capturing unit <b>73</b>. In <figref idref="DRAWINGS">FIGS. <b>16</b> and <b>19</b></figref>, the entry side image-capturing unit <b>73</b> photographs the object fingerprint of the belongings to acquire image data (step S<b>91</b>). Upon photographing the object fingerprint, the entry side image-capturing unit <b>73</b> sends the image data of the object fingerprint to the gate control unit <b>76</b>.</p><p id="p-0174" num="0175">Upon receiving the identification information and the image data of the object fingerprint, the gate control unit <b>76</b> controls the entry side door <b>77</b> to bring the door into an opening state, and closes the door when the user enters. The gate control unit <b>76</b> transmits the identification information and the image data of the object fingerprint to the object management device <b>80</b> as entering person information (step S<b>92</b>).</p><p id="p-0175" num="0176">The entering person information is input to the entering person information acquisition unit <b>81</b> of the object management device <b>80</b>. In <figref idref="DRAWINGS">FIGS. <b>17</b> and <b>20</b></figref>, upon acquiring the entering person information (step S<b>101</b>), the entering person information acquisition unit <b>81</b> transmits the entering person information to the information management unit <b>82</b>. Upon receiving the entering person information, the information management unit <b>82</b> stores the entering person information into the entering person information storage unit <b>83</b> (step S<b>102</b>).</p><p id="p-0176" num="0177">Next, the operation in a case where the user leaves will be described. The user holds the IC card over the exit side reading unit <b>74</b>. The exit side reading unit <b>74</b> reads the identification information of the IC card or the identification information of the user recorded in the IC card. Upon reading the identification information, the exit side reading unit <b>74</b> sends the identification information to the gate control unit <b>76</b>. The user holds the belongings over a camera of the exit side image-capturing unit <b>75</b>. In <figref idref="DRAWINGS">FIGS. <b>16</b> and <b>19</b></figref>, the exit side image-capturing unit <b>75</b> photographs the object fingerprint of the belongings to acquire image data (step S<b>93</b>). Upon photographing the object fingerprint, the exit side image-capturing unit <b>75</b> sends the image data of the object fingerprint to the gate control unit <b>76</b>. Upon receiving the identification information and the image data of the object fingerprint, the gate control unit <b>76</b> transmits the identification information and the image data of the object fingerprint to the object management device <b>80</b> as leaving person information (step S<b>94</b>).</p><p id="p-0177" num="0178">The leaving person information is input to the leaving person information acquisition unit <b>84</b> of the object management device <b>80</b>. In <figref idref="DRAWINGS">FIGS. <b>17</b> and <b>20</b></figref>, upon acquiring the leaving person information (step S<b>103</b>), the leaving person information acquisition unit <b>84</b> transmits the entering person information to the information management unit <b>82</b>. Upon receiving the leaving person information, the information management unit <b>82</b> reads, from the entering person information storage unit <b>83</b>, the image data of the object fingerprint of the entering person whose identification information matches the identification information of the leaving person information.</p><p id="p-0178" num="0179">Upon reading the image data of the object fingerprint of the entering person, the information management unit <b>82</b> sends the collation request unit <b>85</b> the image data of the object fingerprint of the entering person associated with the identification information, the image data of the object fingerprint of the leaving person associated with the identification information, and a collation request of the two pieces of image data. Upon receiving the image data of the object fingerprint and the like, the collation request unit <b>85</b> sends the collation device <b>90</b> the image data of the object fingerprint, the identification information, and the collation request (step S<b>104</b>).</p><p id="p-0179" num="0180">The image data of the object fingerprint is input to the collation request input unit <b>91</b> of the collation device <b>90</b>. Upon acquiring the image data of the object fingerprint of the collation target in <figref idref="DRAWINGS">FIGS. <b>18</b> and <b>21</b></figref> (step S<b>111</b>), the collation request input unit <b>91</b> sends the image data to the collation unit <b>92</b>. Upon receiving the image data of the object fingerprint, the collation unit <b>92</b> collates the image of the object fingerprint at the time of entry with the image of the object fingerprint at the time of exit, and determines the presence or absence of similarity (step S<b>112</b>). The collation unit <b>92</b> collates whether the image of the object fingerprint at the time of entry and the object fingerprint at the time of exit are similar, and if they are similar, regards that the owner of the object to which the second object fingerprint corresponds matches the owner of the object associated with the first object fingerprint, and specifies the identification information of the entering person or the leaving person associated with the image data of the first object fingerprint.</p><p id="p-0180" num="0181">Upon collating the image data of the object fingerprint, the collation unit <b>92</b> sends the collation result output unit <b>93</b> a collation result including the presence or absence of similarity of the object fingerprint and the specification result of the identification information. Upon receiving the collation result, the collation result output unit <b>93</b> outputs the collation result to the object management device <b>80</b> (step S<b>113</b>).</p><p id="p-0181" num="0182">The collation result is input to the collation result input unit <b>86</b>. In <figref idref="DRAWINGS">FIGS. <b>17</b> and <b>20</b></figref>, upon acquiring the collation result (step S<b>105</b>), the collation result input unit <b>86</b> sends the collation result to the information management unit <b>82</b>. Upon receiving the collation result, the information management unit <b>82</b> checks whether the belongings at the time of entry and at the time of exit match each other with reference to the collation result.</p><p id="p-0182" num="0183">When the object fingerprints of the two pieces of image data are similar to each other (Yes in step S<b>106</b>), the information management unit <b>82</b> transmits, to the entry/exit device <b>70</b> via the check result output unit <b>87</b>, a notification of the collation result indicating that the belongings at the time of entry and at the time of exit match each other (step S<b>107</b>). When the object fingerprints of the two pieces of image data are not similar to each other (No in step S<b>106</b>), the information management unit <b>82</b> transmits, to the entry/exit device <b>70</b> via the check result output unit <b>87</b>, a notification of the collation result indicating that the belongings at the time of entry and at the time of exit mismatch each other (step S<b>108</b>).</p><p id="p-0183" num="0184">In <figref idref="DRAWINGS">FIGS. <b>16</b> and <b>19</b></figref>, upon acquiring the check result of whether the belongings match by the collation of the object fingerprint (step S<b>95</b>), the gate control unit <b>76</b> checks whether the belongings match. When the belongings match (Yes in step S<b>96</b>), the gate control unit <b>76</b> controls the exit side door <b>78</b> to bring the door into an opening state, lets the leaving person to pass, and closes the gate when the leaving person leaves (step S<b>97</b>).</p><p id="p-0184" num="0185">When the belongings do not match (No in step S<b>96</b>), the gate control unit <b>76</b> maintains the door of the exit side door <b>78</b> in a state of being closed not to permit the leaving person to leave, and notifies the leaving person that the belongings do not match (step S<b>98</b>). When the belongings do not match, the gate control unit <b>76</b> may perform control of issuing an alert to notify the manager of being a leaving person who is not permitted to leave.</p><p id="p-0185" num="0186"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a view schematically illustrating an application example of the management system of the present example embodiment. In the example of <figref idref="DRAWINGS">FIG. <b>22</b></figref>, the management system is applied to a ticket gate of a railway station. In the example of <figref idref="DRAWINGS">FIG. <b>22</b></figref>, the identification information of an entering person is read at the time of entry from the IC card for paying the fare, and is stored in association with the object fingerprint of the belongings of the entering person. At the time of exit, the identification information of a leaving person is read from the same IC card, and the object fingerprint of the belongings of the leaving person is acquired. The object fingerprint of the belongings of the entering person and the object fingerprint of the belongings of the leaving person whose identification information match are collated, and in a case where the object fingerprints are similar and the belongings of the leaving person matches the belongings of the entering person, the leaving person is permitted to leave. When there are belongings at the time of exit that does not match the belongings at the time of entry, the leaving person is notified that there is shortage in the belongings.</p><p id="p-0186" num="0187">By managing the belongings of the entering person and the leaving person in this manner, it is possible to prevent a person from leaving with an object possessed at the time of entry being mislaid in the zone. It is possible to prevent a person from leaving with an object different from the object possessed at the time of entry.</p><p id="p-0187" num="0188"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a view schematically illustrating an example in which the belongings management system of the present example embodiment is modified and applied. In the configuration of <figref idref="DRAWINGS">FIG. <b>23</b></figref>, image data of the object fingerprint of the belongings of the entering person is stored in advance via the terminal device of the entering person. In the configuration of <figref idref="DRAWINGS">FIG. <b>23</b></figref>, the image data of the object fingerprint of the belongings of the entering person stored in advance is stored in a server having a storage device, and is connected to the object management device <b>80</b> via the network. The image data of the object fingerprint of the belongings of the entering person may be stored in the object management device <b>80</b>.</p><p id="p-0188" num="0189">In the configuration of <figref idref="DRAWINGS">FIG. <b>23</b></figref>, the object fingerprint acquired at the time of exit is collated with the object fingerprint of the belongings of the entering person registered in advance, and in a case where the object fingerprints are similar to each other, it is determined that the belongings match each other, and the leaving person is permitted to leave. When there is an object fingerprint of the belongings at the time of exit that is not similar to the registered object fingerprint, the leaving person is notified that there is shortage in the belongings. In the configuration of <figref idref="DRAWINGS">FIG. <b>23</b></figref>, it is not necessary to acquire image data of the object fingerprint at the time of entry, and therefore it is possible to simplify the operation at the time of entry. Entry to a certain zone may be managed by two stages of gates, the first stage of gates may acquire image data of the object fingerprint, and the second stage of gates may take over the image data of the object fingerprint from the first stage of gates, and perform collation with the object fingerprint photographed at the time of exit.</p><p id="p-0189" num="0190">Although <figref idref="DRAWINGS">FIGS. <b>22</b> and <b>23</b></figref> exemplify entry/exit at a station, that is, in a railway operator, the management system of the present example embodiment can also be used for management of belongings of entering/leaving persons in other transport such as buses, airplanes, and ships other than railways. In particular, the present invention is more effective in applications requiring a high security level, such as management of carry-in items to aircraft. The present invention can also be applied to a case where the gate at the time of entry and the gate at the time of exit are installed at places separated from each other such as aircraft and railway. The management system of the present example embodiment may be applied to a case where entry of a person is not at the same time as luggage such as reception and delivery of checked luggage in aircraft or the like.</p><p id="p-0190" num="0191">The management system of the present example embodiment can be applied to management of belongings of entering/leaving persons not only in transport but also in facilities used by many people such as public facilities, commercial facilities, sports grounds, and cultural facilities. When the object possessed by the entering person and the object in the zone where entry/exit is managed are of the same or similar type, by collating the object fingerprints of the belongings at the time of entry and at the time of exit, it is possible to prevent the entering person from taking out objects other than the belongings due to replacement or errors.</p><p id="p-0191" num="0192">The management system of the present example embodiment can also be applied to management of carrying-in tools for maintenance of factories and equipment. For example, when performing maintenance of factory machinery and transport equipment, at the time of or prior to entry to the zone where work is performed, by acquiring the object fingerprints of carried-in tools and by collating the object fingerprints with the object fingerprints acquired from the belongings at the time of exit, it is possible to determine whether objects possessed at the time of entry are taken out. Such configuration makes it possible to prevent occurrence of defect caused by tools being mislaid in factory machinery and transport equipment. In the case of such configuration, in a case where the same tool is carried in every time, by collating using the image data of the object fingerprints registered in advance and the image data of the object fingerprints photographed at the time of exit, it is possible to improve the convenience at the time of entry.</p><p id="p-0192" num="0193">The management system of the present example embodiment can also be applied to an application of acquiring object fingerprints of belongings such as a plastic bottle at the time of entry in a stadium or the like, and specifying the person who has thrown or abandoned the plastic bottle or the like when it is thrown or abandoned.</p><p id="p-0193" num="0194">The management system of the present example embodiment can also be used for management of shoes in a restaurant or the like. For example, by acquiring and storing, in association with each other, identification information of the user and the image of the object fingerprint of the shoes taken off by each user at the time of entry, and performing collation using the identification information of each user and the object fingerprint of the shoes to be delivered at the time of exit, it is possible to prevent the user from mistakenly taking shoes when leaving the restaurant. Since many shoes have similar or identical designs, the accuracy and efficiency of management are improved by determining as to whether the objects match by collation of the object fingerprints. The management system of the present example embodiment can also be applied to a case of managing a coat, a bag, and the like in a cloakroom in a hotel, a restaurant, or other facilities. In a case of performing management of shoes or management in a cloakroom, entry/exit management by a gate need not be performed.</p><p id="p-0194" num="0195">Although the object management device <b>80</b> and the collation device <b>90</b> are separate devices, the two devices may be configured as an integrated device.</p><p id="p-0195" num="0196">In the management system of the present example embodiment, the collation device <b>90</b> collates the object fingerprint of the belongings of the entering person and the object fingerprint of the belongings of the leaving person acquired by the entry/exit device <b>70</b>, and the object management device <b>80</b> determines whether the same object is possessed at the time of entry and the time of exit. Since the management system of the present example embodiment performs collation using the object fingerprint unique to the object, it is possible to identify individual objects even if the objects are of the same type. Therefore, it is possible to determine whether the same object is held between at the time of entry and at the time of exit without erroneously recognizing a similar object to be identical. Therefore, by applying the management system of the present example embodiment to management of the belongings of entering/leaving persons, it is possible to prevent them from leaving in a state of not possessing what they possessed at the time of entry or in a state of possessing something different from they possessed at the time of entry.</p><heading id="h-0014" level="1">Fourth Example embodiment</heading><p id="p-0196" num="0197">The fourth example embodiment of the present invention will be described in detail with reference to the drawings. <figref idref="DRAWINGS">FIG. <b>24</b></figref> is a view illustrating the configuration of a management system of the present example embodiment. In the third example embodiment, by collating the object fingerprints of the belongings of the entering person and the leaving person, it is checked whether the belongings between at the time of entry and at the time of exit match each other. In addition to such configuration, the management system of the present example embodiment uses, as information of the belongings of the entering person, information selected by the entering person from among objects registered in advance.</p><p id="p-0197" num="0198">The management system of the present example embodiment includes an entry/exit device <b>100</b>, an object management device <b>110</b>, the collation device <b>90</b>, and a user terminal device <b>120</b>. The configuration and function of the collation device <b>90</b> of the present example embodiment are the same as those of the third example embodiment. Therefore, description will be given below with reference to <figref idref="DRAWINGS">FIG. <b>18</b></figref>, which is a view illustrating the configuration of the collation device <b>90</b>.</p><p id="p-0198" num="0199">[Entry/Exit Device]</p><p id="p-0199" num="0200">The configuration of the entry/exit device <b>100</b> will be described. <figref idref="DRAWINGS">FIG. <b>25</b></figref> is a view illustrating the configuration of the entry/exit device <b>100</b>. The entry/exit device <b>100</b> includes the gate <b>71</b>, an entry side reading unit <b>101</b>, the exit side reading unit <b>74</b>, the exit side image-capturing unit <b>75</b>, a gate control unit <b>102</b>, the entry side door <b>77</b>, and the exit side door <b>78</b>. The configurations and functions of the gate <b>71</b>, the exit side reading unit <b>74</b>, the exit side image-capturing unit <b>75</b>, the entry side door <b>77</b>, and the exit side door <b>78</b> of the entry/exit device <b>100</b> of the present example embodiment are the same as the parts having the same names in the third example embodiment.</p><p id="p-0200" num="0201">The entry side reading unit <b>101</b> reads a belongings list of the entering person. The entry side reading unit <b>101</b> reads the belongings list of the entering person from the user terminal device <b>120</b> held by the entering person over the reading unit. The entry side reading unit <b>101</b> and the user terminal device <b>120</b> perform wireless communication based on near-field communication (NFC) standard, for example.</p><p id="p-0201" num="0202">The gate control unit <b>102</b> manages entry/exit by controlling opening and closing of the doors of the entry side door <b>77</b> and the exit side door <b>78</b>. The gate control unit <b>102</b> sends the object management device <b>110</b> data of the belongings list acquired by the entry side reading unit <b>101</b> and data acquired by the exit side reading unit <b>74</b> and the exit side image-capturing unit <b>75</b>. The gate control unit <b>102</b> receives, from the object management device <b>110</b>, a collation result as to whether the belongings of the entering person and the leaving person match.</p><p id="p-0202" num="0203">The gate control unit <b>102</b> includes one or a plurality of semiconductor devices. The processing in the gate control unit <b>102</b> may be performed by executing a computer program on the CPU.</p><p id="p-0203" num="0204">In the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>25</b></figref>, the entry side and the exit side gates are separate, but entry and exit may be performed bidirectionally in the same passage lane. The gate on the entry side and the gate on the exit side may be installed at distant positions. In a case of such configuration, the gate control unit <b>102</b> may be provided on each of the entry side and the exit side.</p><p id="p-0204" num="0205">[Object Management Device]</p><p id="p-0205" num="0206">The configuration of the object management device <b>110</b> will be described. <figref idref="DRAWINGS">FIG. <b>26</b></figref> is a view illustrating the configuration of the object management device <b>110</b>. The object management device <b>110</b> includes an entering person information acquisition unit <b>111</b>, an information management unit <b>112</b>, the entering person information storage unit <b>83</b>, the leaving person information acquisition unit <b>84</b>, the collation request unit <b>85</b>, the collation result input unit <b>86</b>, and the check result output unit <b>87</b>.</p><p id="p-0206" num="0207">The configurations and functions of the entering person information storage unit <b>83</b>, the leaving person information acquisition unit <b>84</b>, the collation request unit <b>85</b>, the collation result input unit <b>86</b>, and the check result output unit <b>87</b> of the present example embodiment are the same as the parts having the same names of the third example embodiment.</p><p id="p-0207" num="0208">The entering person information acquisition unit <b>111</b> acquires a belongings list of the entering person. The belongings list includes identification information of the entering person and image data of the object fingerprint of an object carried in by the entering person as belongings.</p><p id="p-0208" num="0209">The information management unit <b>112</b> stores, into the entering person information storage unit <b>83</b>, the identification information of the entering person and the image data of the object fingerprint in the belongings list. The information management unit <b>112</b> requests the collation device <b>90</b> for collation of the object fingerprint of the entering person of the identification information corresponding to the identification information of the leaving person with the object fingerprint of the belongings of the leaving person. Based on the collation result sent from the collation device <b>90</b>, the information management unit <b>112</b> determines whether the belongings at the time of entry and the belongings at the time of exit match each other.</p><p id="p-0209" num="0210">[User Terminal Device]</p><p id="p-0210" num="0211">The configuration of the user terminal device <b>120</b> will be described. <figref idref="DRAWINGS">FIG. <b>27</b></figref> is a view illustrating the configuration of the user terminal device <b>120</b>. The user terminal device <b>120</b> includes an image-capturing unit <b>121</b>, a terminal control unit <b>122</b>, a data storage unit <b>123</b>, an operation unit <b>124</b>, a communication unit <b>125</b>, and a display unit <b>126</b>.</p><p id="p-0211" num="0212">The image-capturing unit <b>121</b> photographs the object fingerprint of the user's belongings. The image-capturing unit <b>121</b> includes a CMOS image sensor. As the image-capturing unit <b>121</b>, an image sensor other than the CMOS may be used as long as it can photograph the object fingerprint.</p><p id="p-0212" num="0213">The terminal control unit <b>122</b> performs overall control of the user terminal device <b>120</b>. The terminal control unit <b>122</b> generates a belongings list based on a selection result of the user. The belongings list includes identification information of the user and data of the object fingerprint of the belongings.</p><p id="p-0213" num="0214">Each processing in the terminal control unit <b>122</b> is performed by executing a computer program on the CPU. The computer program for performing each processing is recorded in, for example, a nonvolatile semiconductor storage device. The CPU executes a computer program for performing each processing by reading the computer program onto the memory.</p><p id="p-0214" num="0215">The data storage unit <b>123</b> stores image data of the object fingerprint photographed by the image-capturing unit <b>121</b>. The data storage unit <b>123</b> stores, as the user information, information such as the name and contact of the user. The data storage unit <b>123</b> includes a nonvolatile semiconductor storage device.</p><p id="p-0215" num="0216">The operation unit <b>124</b> receives input of a user's operation. The operation unit <b>124</b> receives input of user information, and input at the time of performing operation at the time of photographing by the image-capturing unit <b>121</b> and selecting belongings at the time of creating the belongings list. For example, the operation unit <b>124</b> may be formed as a module integrated with the display unit <b>126</b> as a touchscreen input device.</p><p id="p-0216" num="0217">The communication unit <b>125</b> communicates with other devices. The communication unit <b>125</b> performs near-field communication, for example.</p><p id="p-0217" num="0218">The display unit <b>126</b> displays information necessary for operation of the user terminal device <b>120</b>. The display unit <b>126</b> displays object candidates when the belongings list is generated. The display unit <b>126</b> includes a liquid crystal display device or an organic EL display device.</p><p id="p-0218" num="0219">[Operation Description]</p><p id="p-0219" num="0220">The operation of the management system of the present example embodiment will be described. <figref idref="DRAWINGS">FIG. <b>28</b></figref> is a view illustrating the operation flow of the user terminal device <b>120</b> illustrated in <figref idref="DRAWINGS">FIG. <b>27</b></figref>. <figref idref="DRAWINGS">FIG. <b>29</b></figref> is a view illustrating the operation flow of the entry/exit device <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>25</b></figref>. <figref idref="DRAWINGS">FIG. <b>30</b></figref> is a view illustrating the operation flow of the object management device <b>110</b> illustrated in <figref idref="DRAWINGS">FIG. <b>26</b></figref>. The operation flow of the collation device <b>90</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>21</b></figref>. The configuration of the collation device <b>90</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>18</b></figref>.</p><p id="p-0220" num="0221">First, the user photographs the object fingerprint of the belongings using a camera of the image-capturing unit <b>121</b> of the user terminal device <b>120</b>. Upon photographing the object fingerprint, the image-capturing unit <b>121</b> sends image data of the object fingerprint to the terminal control unit <b>122</b>. In <figref idref="DRAWINGS">FIGS. <b>27</b> and <b>28</b></figref>, upon acquiring the image data of the object fingerprint of the user's belongings (step S<b>121</b>), the terminal control unit <b>122</b> stores the image data of the object fingerprint into the data storage unit <b>123</b> (step S<b>122</b>). In a case where image data of the object fingerprints of a plurality of belongings are photographed, the image data are each stored in the data storage unit <b>123</b>.</p><p id="p-0221" num="0222">Next, the operation when the user enters a zone managed by the management system will be described. With reference to the information of the candidate of object displayed on the display unit <b>126</b>, the user operates the operation unit <b>124</b> to select an object to carry in the management target zone as belongings. The operation unit <b>124</b> transmits information of the object selected by the user to the terminal control unit <b>122</b>. Upon acquiring the selection result of the belongings (step S<b>123</b>), the terminal control unit <b>122</b> reads, from the data storage unit <b>123</b>, the image data associated with the information of the object selected by the user. After the image data is read, data in which the identification information and the image data of the object carried in by the user as the belongings are combined is generated as a belongings list (step S<b>124</b>). In a case where there are a plurality of belongings, the belongings list includes identification information and image data of each carry-in object.</p><p id="p-0222" num="0223">The user holds the user terminal device <b>120</b> over the entry side reading unit <b>101</b>. Upon detecting that the holding over at the entry side reading unit <b>101</b>, the terminal control unit <b>122</b> transmits the data of the belongings list to the entry side reading unit <b>101</b> via the communication unit <b>125</b> (step S<b>125</b>).</p><p id="p-0223" num="0224">The entry side reading unit <b>101</b> reads the data of the belongings list transmitted from the communication unit <b>125</b>. The entry side reading unit <b>101</b> sends the data of the belongings list to the gate control unit <b>102</b>. In <figref idref="DRAWINGS">FIGS. <b>25</b> and <b>29</b></figref>, upon acquiring the data of the belongings list (step S<b>131</b>), the gate control unit <b>102</b> controls the entry side door <b>77</b> to bring the door into an opening state, and closes the door when the entering person enters. The gate control unit <b>102</b> sends the data of the belongings list to the object management device <b>110</b> (step S<b>132</b>).</p><p id="p-0224" num="0225">The data of the belongings list is input to the entering person information acquisition unit <b>111</b> of the object management device <b>110</b>. In <figref idref="DRAWINGS">FIGS. <b>26</b> and <b>30</b></figref>, upon acquiring the data of the belongings list (step S<b>141</b>), the entering person information acquisition unit <b>111</b> sends the data of the belongings list to the information management unit <b>112</b>. Upon receiving the entering person information, the information management unit <b>112</b> stores, into the entering person information storage unit <b>83</b>, the image data included in the data of the belongings list (step S<b>142</b>).</p><p id="p-0225" num="0226">Next, the operation in a case where the user leaves will be described. The leaving person holds the user terminal device <b>120</b> over the exit side reading unit <b>74</b>. The exit side reading unit <b>74</b> reads the identification information of the user from the user terminal device <b>120</b>. Upon reading the identification information, the exit side reading unit <b>74</b> sends the identification information to the gate control unit <b>102</b>.</p><p id="p-0226" num="0227">The user holds the belongings over a camera of the exit side image-capturing unit <b>75</b>. The exit side image-capturing unit <b>75</b> photographs the object fingerprint of the belongings. Upon photographing the object fingerprint, the exit side image-capturing unit <b>75</b> sends the image data of the object fingerprint to the gate control unit <b>102</b>. In <figref idref="DRAWINGS">FIGS. <b>25</b> and <b>29</b></figref>, upon acquiring the identification information and the image data of the object fingerprint (step S<b>133</b>), the gate control unit <b>102</b> sends the object management device <b>110</b>, as the leaving person information, the identification information and the image data of the object fingerprint (step S<b>134</b>).</p><p id="p-0227" num="0228">The leaving person information is input to the leaving person information acquisition unit <b>114</b> of the object management device <b>110</b>. In <figref idref="DRAWINGS">FIGS. <b>26</b> and <b>30</b></figref>, upon acquiring the leaving person information (step S<b>143</b>), the leaving person information acquisition unit <b>84</b> sends the leaving person information to the information management unit <b>112</b>. Upon receiving the leaving person information, the information management unit <b>112</b> reads, from the entering person information storage unit <b>83</b>, the image data of the object fingerprint of the entering person whose identification information matches the identification information of the leaving person information.</p><p id="p-0228" num="0229">After the image data of the belongings list of the entering person is read, the image data of the object fingerprint of the belongings list of the entering person and the image data of the object fingerprint of the leaving person are sent to the collation request unit <b>85</b> together with a collation request.</p><p id="p-0229" num="0230">Upon receiving the image data of the object fingerprints and the collation request, the collation request unit <b>85</b> sends the image data of the object fingerprints and the collation request to the collation device <b>90</b> (step S<b>144</b>).</p><p id="p-0230" num="0231">The image data of the object fingerprints is input to the collation request input unit <b>91</b>. In <figref idref="DRAWINGS">FIGS. <b>18</b> and <b>21</b></figref>, upon acquiring the image data of the object fingerprints of the collation target (step S<b>111</b>), the collation request input unit <b>91</b> sends the image data to the collation unit <b>92</b>. Upon receiving the image data of the object fingerprints, the collation unit <b>92</b> collates the image of the object fingerprint at the time of entry with the image of the object fingerprint at the time of exit, and determines the presence or absence of collation (step S<b>112</b>).</p><p id="p-0231" num="0232">Upon collating the image data of the object fingerprints, the collation unit <b>92</b> sends the collation result to the collation result output unit <b>93</b>. Upon receiving the collation result, the collation result output unit <b>93</b> sends the collation result to the object management device <b>110</b> (step S<b>113</b>).</p><p id="p-0232" num="0233">The collation result is input to the collation result input unit <b>86</b> of the object management device <b>110</b>. Upon receiving the collation result, collation result input unit <b>86</b> sends the collation result to the information management unit <b>112</b>. In <figref idref="DRAWINGS">FIGS. <b>26</b> and <b>30</b></figref>, upon receiving the collation result (step S<b>145</b>), the information management unit <b>112</b> checks whether the belongings at the time of entry and at the time of exit match each other with reference to the collation result.</p><p id="p-0233" num="0234">When the object fingerprints of the two pieces of image data are similar to each other (Yes in step S<b>146</b>), the information management unit <b>112</b> sends the entry/exit device <b>100</b>, via the check result output unit <b>87</b>, the collation result indicating that the belongings at the time of entry and at the time of exit match each other (step S<b>147</b>). When the object fingerprints of the two pieces of image data are not similar to each other (No in step S<b>146</b>), the information management unit <b>112</b> sends the entry/exit device <b>100</b>, via the check result output unit <b>87</b>, a collation result indicating that the belongings at the time of entry and at the time of exit mismatch each other (step S<b>148</b>).</p><p id="p-0234" num="0235">In <figref idref="DRAWINGS">FIGS. <b>25</b> and <b>29</b></figref>, upon acquiring the check result of whether the belongings match by the collation of the object fingerprint (step S<b>135</b>), the gate control unit <b>102</b> checks whether the belongings match. When the belongings match (Yes in step S<b>136</b>), the gate control unit <b>102</b> controls the exit side door <b>78</b> to bring the door into an opening state, lets the leaving person to pass, and closes the gate when the leaving person leaves (step S<b>137</b>).</p><p id="p-0235" num="0236">When the belongings do not match (No in step S<b>136</b>), the gate control unit <b>102</b> maintains the door of the exit side door <b>78</b> in a state of being closed not to permit the leaving person to leave, and notifies the leaving person that the belongings do not match (step S<b>138</b>). When the belongings do not match, the gate control unit <b>76</b> may perform control of issuing an alert to notify the manager of being a leaving person who is not permitted to leave.</p><p id="p-0236" num="0237"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a view schematically illustrating an application example of the management system of the present example embodiment. In the example of <figref idref="DRAWINGS">FIG. <b>31</b></figref>, the management system is applied to an entrance of a public facility. In <figref idref="DRAWINGS">FIG. <b>31</b></figref>, an image of the object fingerprint of the user's belongings is photographed and stored in advance. In the example of <figref idref="DRAWINGS">FIG. <b>31</b></figref>, the user operates the terminal device to generate a belongings list. The belongings list includes image data of the object fingerprint of belongings and identification information of the user. The belongings list is sent to the entry/exit device side at the time of entry, and is stored in association with the object fingerprint of the belongings of the entering person. At the time of exit, the belongings list is acquired by reading the identification information of the leaving person from the same terminal device, and photographing the object fingerprint of the belongings of the leaving person. The object fingerprint of the belongings of the entering person and the object fingerprint of the belongings of the leaving person whose identification information match are collated, and in a case where the object fingerprints are similar to each other, it is determined that the object possessed at the time of exit matches the belongings of the entering person, the leaving person is permitted to leave. When there is an object possessed at the time of exit that does not match the belongings of the entering person, the leaving person is notified that there is shortage of the object possessed by the leaving person.</p><p id="p-0237" num="0238"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a view schematically illustrating an example in which the management system of the present example embodiment is modified and provided. In the example of <figref idref="DRAWINGS">FIG. <b>32</b></figref>, the image data of the object fingerprint of the belongings of the entering person is sent to the entry/exit device side as a belongings list similarly to <figref idref="DRAWINGS">FIG. <b>31</b></figref>. In the example of <figref idref="DRAWINGS">FIG. <b>32</b></figref>, it is configured that when there is an object possessed at the time of exit that does not match the object at the time of entry, exit is not permitted until they match.</p><p id="p-0238" num="0239">The management system of the present example embodiment, similarly to the third example embodiment, can also be applied to management of belongings of entering/leaving persons in facilities used by many people such as transport, public facilities, commercial facilities, sports grounds, and cultural facilities.</p><p id="p-0239" num="0240">The management system of the present example embodiment can also be applied to management of carrying-in tools for maintenance of factories and equipment. For example, when performing maintenance of factory machinery and transport equipment, at the time of or prior to entry to the zone where work is performed, it is possible to generate a belongings list of carried-in tools from tools possessed by the worker and whose image data of object fingerprints have been registered. By entering with reading the belongings list and collating the object fingerprints with the object fingerprints acquired from the belongings at the time of exit, it is possible to determine whether objects possessed at the time of entry are taken out. Such configuration makes it possible to prevent occurrence of defect caused by tools being mislaid in factory machinery and transport equipment.</p><p id="p-0240" num="0241">In the management system of the present example embodiment, the collation device <b>90</b> collates the object fingerprints included in the belongings list of the entering person with the object fingerprint of the belongings of the leaving person, and the object management device <b>110</b> determines whether the same object is possessed at the time of entry and at the time of exit. Therefore, the management system of the present example embodiment is suitable to be applied to a case where objects to frequently carry in are determined in advance, and objects to carry in from among them are different for each entry. In such a case, since the collation between at the time of entry and at the time of exit can be performed in a more simplified manner, the management system of the present example embodiment can improve convenience of the user in entry/exit management while accurately managing the belongings.</p><p id="p-0241" num="0242">The management systems of the third and fourth example embodiments may be applied only to check at the time of exit. For example, by registering in advance an object to always carry when going out, and when going out from a house or a workplace, by collating the object fingerprint registered at the entrance or the doorway with the object fingerprint of the belongings, it may be checked whether there is no shortage in the belongings. In a case of checking shortage in belongings or the like at the entrance of a house or the like, entry/exit by a gate may be eliminated. It is also possible to register in advance the object fingerprint of an object prohibited to take out, and check whether the prohibited object is taken out. Such configuration makes it possible to prevent shortage of belongings at the time of going out, and possible to prevent an object of another person from being taken out when the same or similar type object is possessed. In a case of applying only to check at the time of exit, a belongings list may be created from objects registered in advance, and the overage or shortage of the belongings may be checked at the time of going out.</p><p id="p-0242" num="0243">The management system of the third example embodiment can also be used for an umbrella management system in an umbrella stand. In the umbrella management system, when a user places an umbrella on an umbrella stand at the time of entry to a facility or the like, the user holds the umbrella over a camera to acquire the object fingerprint of the umbrella, and data is stored together with identification information of the user read from an ID card or the like. At that time, management of entry/exit of the user may be omitted. When the user takes out the umbrella from the umbrella stand, the object fingerprint of the umbrella is acquired by the user holding the umbrella over the camera, and whether the objects match is checked by collation with the object fingerprint when the umbrella is placed based on the identification information of the user read from the ID card or the like. In an umbrella stand check system, in a case where image data of the object fingerprint of the umbrella and the identification information of the owner are registered in advance, the object fingerprint of the umbrella may be acquired only when being taken out. Since many umbrellas have the same design or similar designs, it is possible to manage the umbrellas while achieving both management accuracy and convenience by simplifying and applying the entry/exit management section from the management system of the third example embodiment.</p><p id="p-0243" num="0244">Processing in each device of the management system of each example embodiment can be performed by executing a computer program on a computer. <figref idref="DRAWINGS">FIG. <b>33</b></figref> illustrates an example of the configuration of a computer <b>200</b> that executes a computer program for performing each processing in a learning device. The computer <b>200</b> includes a CPU <b>201</b>, a memory <b>202</b>, a storage device <b>203</b>, and an interface (I/F) unit <b>204</b>.</p><p id="p-0244" num="0245">The CPU <b>201</b> reads and executes a computer program for performing each processing from the storage device <b>203</b>. The memory <b>202</b> includes a dynamic random access memory (DRAM), and temporarily stores a computer program executed by the CPU <b>201</b> and data being processed. The storage device <b>203</b> stores a computer program executed by the CPU <b>201</b>. The storage device <b>203</b> includes, for example, a nonvolatile semiconductor storage device. As the storage device <b>203</b>, another storage device such as a hard disk drive may be used. The I/F unit <b>204</b> is an interface that inputs/outputs data to/from another device of the management system, a terminal of the network of the management target, and the like. The computer <b>200</b> may further include a communication module that communicates with another information processing device via a communication network.</p><p id="p-0245" num="0246">The computer program performed in each processing can be stored in a recording medium and distributed. As a recording medium, for example, a magnetic tape for data recording or a magnetic disk such as a hard disk can be used. As the recording medium, an optical disk such as a compact disc read only memory (CD-ROM) can also be used. A nonvolatile semiconductor storage device may be used as the recording medium.</p><p id="p-0246" num="0247">A part or the entirety of the above example embodiments can be described as the following supplementary notes, but are not limited to the following.</p><p id="p-0247" num="0248">(Supplementary Note 1)</p><p id="p-0248" num="0249">A management system including:</p><p id="p-0249" num="0250">a first data acquisition means configured to acquire first image data in which a first object is photographed and identification information of an owner of the first object;</p><p id="p-0250" num="0251">a second data acquisition means configured to acquire second image data in which a second object is photographed; and</p><p id="p-0251" num="0252">a collation means configured to specify the identification information of an owner of the first object by collating a feature of a surface pattern of the first object in the first image data with a feature of a surface pattern of the second object in the second image data.</p><p id="p-0252" num="0253">(Supplementary Note 2)</p><p id="p-0253" num="0254">The management system according to supplementary note 1, further including:</p><p id="p-0254" num="0255">a result output means configured to output information associated with the identification information of an owner of the first object having a feature of a surface pattern similar to a surface pattern of the second object.</p><p id="p-0255" num="0256">(Supplementary Note 3)</p><p id="p-0256" num="0257">The management system according to supplementary note 1 or 2, further including:</p><p id="p-0257" num="0258">a data storage means configured to store the first image data of each of a plurality of the first objects, in which</p><p id="p-0258" num="0259">the collation means collates the first image data selected from a plurality of pieces of the first image data having been stored with the second image data, and specifies the identification information of an owner of the first object.</p><p id="p-0259" num="0260">(Supplementary Note 4)</p><p id="p-0260" num="0261">The management system according to supplementary note 1 or 2, further including:</p><p id="p-0261" num="0262">a second image-capturing means configured to photograph the second object and output the second image data; and</p><p id="p-0262" num="0263">an object management means configured to request the collation means for collation of the second image data with the first image data.</p><p id="p-0263" num="0264">(Supplementary Note 5)</p><p id="p-0264" num="0265">The management system according to supplementary note 1, wherein</p><p id="p-0265" num="0266">the second data acquisition means further acquires identification information of an owner of the second object, and</p><p id="p-0266" num="0267">the collation means collates a feature of a surface pattern of the second object with a feature of a surface pattern of the first object whose identification information matches identification information of the second object.</p><p id="p-0267" num="0268">(Supplementary Note 6)</p><p id="p-0268" num="0269">The management system according to supplementary note 5, wherein</p><p id="p-0269" num="0270">the first data acquisition means acquires identification information of an owner of the first object and image data of the first object when an owner of the first object enters a zone where an entering person is managed, and</p><p id="p-0270" num="0271">the second data acquisition means acquires identification information of a leaving person from the zone, and acquires, as image data of the second object, image data of an object possessed by the leaving person.</p><p id="p-0271" num="0272">(Supplementary Note 7)</p><p id="p-0272" num="0273">The management system according to supplementary note 6, wherein</p><p id="p-0273" num="0274">the first data acquisition means acquires the first image data of the first object carried into the zone by an entering person as a list generated based on the first image data photographed in advance.</p><p id="p-0274" num="0275">(Supplementary Note 8)</p><p id="p-0275" num="0276">The management system according to supplementary note 6 or 7, further including:</p><p id="p-0276" num="0277">a data storage means configured to store image data of a plurality of objects, in which</p><p id="p-0277" num="0278">the first data acquisition means acquires the first image data of the first object by reading from among the first image data stored in the data storage means based on the list.</p><p id="p-0278" num="0279">(Supplementary Note 9)</p><p id="p-0279" num="0280">The management system according to any of supplementary notes 6 to 8, further including:</p><p id="p-0280" num="0281">a gate that manages entry to a zone where an entering person is managed and exit from the zone; and</p><p id="p-0281" num="0282">a gate control means configured to control the gate based on a collation result by the collation means of the management system.</p><p id="p-0282" num="0283">(Supplementary Note 10)</p><p id="p-0283" num="0284">The management system according to supplementary note 9, wherein</p><p id="p-0284" num="0285">when image data of the first object and image data of the second object do not match, the gate control means controls the gate in such a way as not to permit a leaving person to leave.</p><p id="p-0285" num="0286">(Supplementary Note 11)</p><p id="p-0286" num="0287">A management system including:</p><p id="p-0287" num="0288">an entering person information acquisition means configured to acquire first image data in which a surface pattern of an object possessed by an entering person is photographed;</p><p id="p-0288" num="0289">a leaving person information acquisition means configured to acquire second image data in which a surface pattern of an object possessed by a leaving person is photographed; and</p><p id="p-0289" num="0290">a gate control means configured to control a gate based on a result of collating between a feature of a surface pattern of a first object in first image data and a feature of a surface pattern of a second object in the second image data.</p><p id="p-0290" num="0291">(Supplementary Note 12)</p><p id="p-0291" num="0292">The management system according to supplementary note 11, wherein</p><p id="p-0292" num="0293">the entering person information acquisition means acquires the first image data selected in a terminal device possessed by the entering person, and</p><p id="p-0293" num="0294">the leaving person information acquisition means acquires the second image data in which a surface pattern of an object possessed by the leaving person is photographed at the gate.</p><p id="p-0294" num="0295">(Supplementary Note 13)</p><p id="p-0295" num="0296">A management method including:</p><p id="p-0296" num="0297">acquiring first image data in which a first object is photographed and identification information of an owner of the first object;</p><p id="p-0297" num="0298">acquiring second image data in which a second object is photographed; and</p><p id="p-0298" num="0299">specifying the identification information of an owner of the first object by collating a feature of a surface pattern of the first object in the first image data with a feature of a surface pattern of the second object in the second image data.</p><p id="p-0299" num="0300">(Supplementary Note 14)</p><p id="p-0300" num="0301">The management method according to supplementary note 13, further including: outputting information associated with the identification information of an owner of the first object having a feature of a surface pattern similar to a surface pattern of the second object.</p><p id="p-0301" num="0302">(Supplementary Note 15)</p><p id="p-0302" num="0303">The management method according to supplementary note 13 or 14, further including:</p><p id="p-0303" num="0304">storing the first image data of each of a plurality of the first objects; and</p><p id="p-0304" num="0305">collating the first image data selected from a plurality of pieces of the first image data having been stored with the second image data, and specifying the identification information of an owner of the first object.</p><p id="p-0305" num="0306">(Supplementary Note 16)</p><p id="p-0306" num="0307">The management method according to any of supplementary notes 13 to 15, further including:</p><p id="p-0307" num="0308">photographing the second object and outputting the second image data; and</p><p id="p-0308" num="0309">requesting for collation between the second image data and the first image data from a transmission side of the second image data.</p><p id="p-0309" num="0310">(Supplementary Note 17)</p><p id="p-0310" num="0311">The management method according to supplementary note 13, further including:</p><p id="p-0311" num="0312">facquiring identification information of an owner of the second object; and</p><p id="p-0312" num="0313">collating a feature of a surface pattern of the second object with a feature of a surface pattern of the first object whose identification information matches identification information of the second object.</p><p id="p-0313" num="0314">(Supplementary Note 18)</p><p id="p-0314" num="0315">The management method according to supplementary note 17, further including:</p><p id="p-0315" num="0316">acquiring identification information of an owner of the first object and image data of the first object when an owner of the first object enters a zone where an entering person is managed; and</p><p id="p-0316" num="0317">acquiring identification information of a leaving person from the zone, and acquiring, as image data of the second object, image data of an object possessed by the leaving person.</p><p id="p-0317" num="0318">(Supplementary Note 19)</p><p id="p-0318" num="0319">The management method according to supplementary note 18, further including: acquiring the first image data of the first object carried into the zone by an entering person as a list generated based on the first image data photographed in advance.</p><p id="p-0319" num="0320">(Supplementary Note 20)</p><p id="p-0320" num="0321">The management method according to supplementary note 19, further including:</p><p id="p-0321" num="0322">storing image data of a plurality of objects; and</p><p id="p-0322" num="0323">acquiring the first image data of the first object by reading, based on the list, from among the first image data having been stored.</p><p id="p-0323" num="0324">(Supplementary Note 21)</p><p id="p-0324" num="0325">The management method according to any of supplementary notes 18 to 20, further including:</p><p id="p-0325" num="0326">controlling, based on a collation result, a gate that manages entry to the zone where an entering person is managed and exit from the zone.</p><p id="p-0326" num="0327">(Supplementary Note 22)</p><p id="p-0327" num="0328">The management method according to supplementary note 21, further including: controlling the gate in such a way as not to permit a leaving person to leave when image data of the first object and image data of the second object do not match.</p><p id="p-0328" num="0329">(Supplementary Note 23)</p><p id="p-0329" num="0330">A management method including:</p><p id="p-0330" num="0331">acquiring first image data in which a surface pattern of an object possessed by an entering person is photographed;</p><p id="p-0331" num="0332">acquiring second image data in which a surface pattern of an object possessed by a leaving person is photographed; and</p><p id="p-0332" num="0333">controlling a gate based on a result of collating between a feature of a surface pattern of a first object in first image data and a feature of a surface pattern of a second object in the second image data.</p><p id="p-0333" num="0334">(Supplementary Note 24)</p><p id="p-0334" num="0335">The management method according to supplementary note 23, further including:</p><p id="p-0335" num="0336">acquiring the first image data selected in a terminal device possessed by the entering person; and</p><p id="p-0336" num="0337">acquiring the second image data in which an object possessed by the leaving person is photographed at the gate.</p><p id="p-0337" num="0338">(Supplementary Note 25)</p><p id="p-0338" num="0339">A management method including:</p><p id="p-0339" num="0340">acquiring image data in which a surface pattern of an object is photographed;</p><p id="p-0340" num="0341">receiving selection of image data of the object to be used for collation of belongings from the image data of each of the plurality of objects; and</p><p id="p-0341" num="0342">outputting a surface pattern of selected image data as image data for collating with a feature of a surface pattern of an object photographed separately and specifying whether objects match.</p><p id="p-0342" num="0343">(Supplementary Note 26)</p><p id="p-0343" num="0344">A recording medium recording a computer program for causing a computer to execute</p><p id="p-0344" num="0345">processing of acquiring first image data in which a first object is photographed and identification information of an owner of the first object,</p><p id="p-0345" num="0346">processing of acquiring second image data in which a second object is photographed, and</p><p id="p-0346" num="0347">processing of specifying the identification information of an owner of the first object by collating a feature of a surface pattern of the first object in the first image data with a feature of a surface pattern of the second object in the second image data.</p><p id="p-0347" num="0348">(Supplementary Note 27)</p><p id="p-0348" num="0349">A recording medium recording a computer program for causing a computer to execute</p><p id="p-0349" num="0350">processing of acquiring first image data in which a surface pattern of an object possessed by an entering person is photographed,</p><p id="p-0350" num="0351">processing of acquiring second image data in which a surface pattern of an object possessed by a leaving person is photographed, and</p><p id="p-0351" num="0352">processing of collating between a feature of a surface pattern of a first object in first image data and a feature of a surface pattern of a second object in the second image data, and outputting a collation result for controlling a gate.</p><p id="p-0352" num="0353">The present invention has been described above using the above-described example embodiments as exemplary examples. However, the present invention is not limited to the above-described example embodiments. It will be understood by those of ordinary skill in the art that various aspects may be made therein without departing from the spirit and scope of the present invention as defined by the claims.</p><heading id="h-0015" level="1">REFERENCE SIGNS LIST</heading><p id="p-0353" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0354"><b>1</b> first data acquisition unit</li>    <li id="ul0002-0002" num="0355"><b>2</b> second data acquisition unit</li>    <li id="ul0002-0003" num="0356"><b>3</b> collation unit</li>    <li id="ul0002-0004" num="0357"><b>10</b> user information management device</li>    <li id="ul0002-0005" num="0358"><b>11</b> user information input unit</li>    <li id="ul0002-0006" num="0359"><b>12</b> user information management unit</li>    <li id="ul0002-0007" num="0360"><b>13</b> user information storage unit</li>    <li id="ul0002-0008" num="0361"><b>14</b> data output unit</li>    <li id="ul0002-0009" num="0362"><b>15</b> data request input unit</li>    <li id="ul0002-0010" num="0363"><b>20</b> collation device</li>    <li id="ul0002-0011" num="0364"><b>21</b> collation request input unit</li>    <li id="ul0002-0012" num="0365"><b>22</b> data acquisition unit</li>    <li id="ul0002-0013" num="0366"><b>23</b> collation unit</li>    <li id="ul0002-0014" num="0367"><b>24</b> collation result notification unit</li>    <li id="ul0002-0015" num="0368"><b>25</b> data storage unit</li>    <li id="ul0002-0016" num="0369"><b>30</b> user terminal device</li>    <li id="ul0002-0017" num="0370"><b>40</b> manager terminal device</li>    <li id="ul0002-0018" num="0371"><b>41</b> image data input unit</li>    <li id="ul0002-0019" num="0372"><b>42</b> object management unit</li>    <li id="ul0002-0020" num="0373"><b>43</b> data storage unit</li>    <li id="ul0002-0021" num="0374"><b>44</b> image data transmission unit</li>    <li id="ul0002-0022" num="0375"><b>45</b> information input unit</li>    <li id="ul0002-0023" num="0376"><b>46</b> collation result output unit</li>    <li id="ul0002-0024" num="0377"><b>50</b> image-capturing device</li>    <li id="ul0002-0025" num="0378"><b>61</b> object</li>    <li id="ul0002-0026" num="0379"><b>62</b> belt conveyor</li>    <li id="ul0002-0027" num="0380"><b>70</b> entry/exit device</li>    <li id="ul0002-0028" num="0381"><b>71</b> gate</li>    <li id="ul0002-0029" num="0382"><b>72</b> entry side reading unit</li>    <li id="ul0002-0030" num="0383"><b>73</b> entry side image-capturing unit</li>    <li id="ul0002-0031" num="0384"><b>74</b> exit side reading unit</li>    <li id="ul0002-0032" num="0385"><b>75</b> exit side image-capturing unit</li>    <li id="ul0002-0033" num="0386"><b>76</b> gate control unit</li>    <li id="ul0002-0034" num="0387"><b>77</b> entry side door</li>    <li id="ul0002-0035" num="0388"><b>78</b> exit side door</li>    <li id="ul0002-0036" num="0389"><b>80</b> object management device</li>    <li id="ul0002-0037" num="0390"><b>81</b> entering person information acquisition unit</li>    <li id="ul0002-0038" num="0391"><b>82</b> information management unit</li>    <li id="ul0002-0039" num="0392"><b>83</b> entering person information storage unit</li>    <li id="ul0002-0040" num="0393"><b>84</b> leaving person information acquisition unit</li>    <li id="ul0002-0041" num="0394"><b>85</b> collation request unit</li>    <li id="ul0002-0042" num="0395"><b>86</b> collation result input unit</li>    <li id="ul0002-0043" num="0396"><b>87</b> check result output unit</li>    <li id="ul0002-0044" num="0397"><b>90</b> collation device</li>    <li id="ul0002-0045" num="0398"><b>91</b> collation request input unit</li>    <li id="ul0002-0046" num="0399"><b>92</b> collation unit</li>    <li id="ul0002-0047" num="0400"><b>93</b> collation result output unit</li>    <li id="ul0002-0048" num="0401"><b>100</b> entry/exit device</li>    <li id="ul0002-0049" num="0402"><b>101</b> entry side reading unit</li>    <li id="ul0002-0050" num="0403"><b>102</b> gate control unit</li>    <li id="ul0002-0051" num="0404"><b>110</b> object management device</li>    <li id="ul0002-0052" num="0405"><b>111</b> entering person information acquisition unit</li>    <li id="ul0002-0053" num="0406"><b>112</b> information management unit</li>    <li id="ul0002-0054" num="0407"><b>120</b> user terminal device</li>    <li id="ul0002-0055" num="0408"><b>121</b> image-capturing unit</li>    <li id="ul0002-0056" num="0409"><b>122</b> terminal control unit</li>    <li id="ul0002-0057" num="0410"><b>123</b> data storage unit</li>    <li id="ul0002-0058" num="0411"><b>124</b> operation unit</li>    <li id="ul0002-0059" num="0412"><b>125</b> communication unit</li>    <li id="ul0002-0060" num="0413"><b>126</b> display unit</li>    <li id="ul0002-0061" num="0414"><b>200</b> computer</li>    <li id="ul0002-0062" num="0415"><b>201</b> CPU</li>    <li id="ul0002-0063" num="0416"><b>202</b> memory</li>    <li id="ul0002-0064" num="0417"><b>203</b> storage device</li>    <li id="ul0002-0065" num="0418"><b>204</b> I/F unit</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A management system comprising:<claim-text>at least one memory storing instructions; and</claim-text><claim-text>at least one processor configured to access the at least one memory and execute the instructions to:</claim-text><claim-text>acquire first image data in which a first object is photographed and identification information of an owner of the first object;</claim-text><claim-text>acquire second image data in which a second object is photographed; and</claim-text><claim-text>specify the identification information of an owner of the first object by collating a feature of a surface pattern of the first object in the first image data with a feature of a surface pattern of the second object in the second image data.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The management system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the at least one processor is further configured to execute the instructions to:</claim-text><claim-text>output information associated with the identification information of an owner of the first object having a feature of a surface pattern similar to a surface pattern of the second object.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The management system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the at least one processor is further configured to execute the instructions to:</claim-text><claim-text>store the first image data of each of a plurality of the first objects;</claim-text><claim-text>collate the first image data selected from a plurality of pieces of the first image data having been stored with the second image data; and</claim-text><claim-text>specify the identification information of an owner of the first object.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The management system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the at least one processor is further configured to execute the instructions to:</claim-text><claim-text>photograph the second object;</claim-text><claim-text>output the second image data; and</claim-text><claim-text>request the collation means for collation of the second image data with the first image data.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The management system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the at least one processor is further configured to execute the instructions to:</claim-text><claim-text>acquire identification information of an owner of the second object; and</claim-text><claim-text>collate a feature of a surface pattern of the second object with a feature of a surface pattern of the first object whose identification information matches identification information of the second object.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The management system according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the at least one processor is further configured to execute the instructions to:</claim-text><claim-text>acquire identification information of an owner of the first object and image data of the first object when an owner of the first object enters a zone where an entering person is managed;</claim-text><claim-text>acquire identification information of a leaving person from the zone; and</claim-text><claim-text>acquire, as image data of the second object, image data of an object possessed by the leaving person.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The management system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the at least one processor is further configured to execute the instructions to:</claim-text><claim-text>acquire the first image data of the first object carried into the zone by an entering person as a list generated based on the first image data photographed in advance.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The management system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the at least one processor is further configured to execute the instructions to:</claim-text><claim-text>store image data of a plurality of objects in a storage; and</claim-text><claim-text>acquire the first image data of the first object by reading from among the first image data stored in the storage based on the list.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The management system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the at least one processor is further configured to execute the instructions to:</claim-text><claim-text>control a gate based on a collation result, wherein the gate that manages entry to a zone where an entering person is managed and exit from the zone.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The management system according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the at least one processor is further configured to execute the instructions to:</claim-text><claim-text>when image data of the first object and image data of the second object do not match, control the gate in such a way as not to permit a leaving person to leave.</claim-text></claim-text></claim><claim id="CLM-11-12" num="11-12"><claim-text><b>11</b>.-<b>12</b>. (canceled)</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A management method comprising:<claim-text>acquiring first image data in which a first object is photographed and identification information of an owner of the first object;</claim-text><claim-text>acquiring second image data in which a second object is photographed; and</claim-text><claim-text>specifying the identification information of an owner of the first object by collating a feature of a surface pattern of the first object in the first image data with a feature of a surface pattern of the second object in the second image data.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The management method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>outputting information associated with the identification information of an owner of the first object having a feature of a surface pattern similar to a surface pattern of the second object.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The management method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>storing the first image data of each of a plurality of the first objects; and</claim-text><claim-text>collating the first image data selected from a plurality of the first image data having been stored with the second image data, and specifying the identification information of an owner of the first object.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. (canceled)</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The management method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>acquiring identification information of an owner of the second object; and</claim-text><claim-text>collating a feature of a surface pattern of the second object with a feature of a surface pattern of the first object whose identification information matches identification information of the second object.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The management method according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising:<claim-text>acquiring identification information of an owner of the first object and image data of the first object when an owner of the first object enters a zone where an entering person is managed; and</claim-text><claim-text>acquiring identification information of a leaving person from the zone, and acquiring, as image data of the second object, image data of an object possessed by the leaving person.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The management method according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising:<claim-text>acquiring the first image data of the first object carried into the zone by an entering person as a list generated based on the first image data photographed in advance.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The management method according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising:<claim-text>storing image data of a plurality of objects; and</claim-text><claim-text>acquiring the first image data of the first object by reading, based on the list, from among the first image data having been stored.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The management method according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising:<claim-text>controlling, based on a collation result, a gate that manages entry to the zone where an entering person is managed and exit from the zone.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The management method according to <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising:<claim-text>controlling the gate in such a way as not to permit a leaving person to leave when image data of the first object and image data of the second object do not match.</claim-text></claim-text></claim><claim id="CLM-23-25" num="23-25"><claim-text><b>23</b>.-<b>25</b>. (canceled)</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. A non-transitory recording medium recording a computer program for causing a computer to execute<claim-text>processing of acquiring first image data in which a first object is photographed and identification information of an owner of the first object,</claim-text><claim-text>processing of acquiring second image data in which a second object is photographed, and</claim-text><claim-text>processing of specifying the identification information of an owner of the first object by collating a feature of a surface pattern of the first object in the first image data with a feature of a surface pattern of the second object in the second image data.</claim-text></claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. (canceled)</claim-text></claim></claims></us-patent-application>