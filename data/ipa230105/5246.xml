<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005247A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005247</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940224</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>776</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>776</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23222</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23218</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>87</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>00006</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>0655</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>141</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">PROCESSING SYSTEM, IMAGE PROCESSING METHOD, LEARNING METHOD, AND PROCESSING DEVICE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2020/010541</doc-number><date>20200311</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17940224</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>OLYMPUS CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>OHARA</last-name><first-name>Satoshi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>OLYMPUS CORPORATION</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A processing system includes a processor with hardware. The processor is configured to perform processing of acquiring a detection target image captured by an endoscope apparatus, controlling the endoscope apparatus based on control information, detecting a region of interest included in the detection target image based on the detection target image for calculating estimated probability information representing a probability of the detected region of interest, identifying the control information for improving the estimated probability information related to the region of interest within the detection target image based on the detection target image, and controlling the endoscope apparatus based on the identified control information.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="149.52mm" wi="102.95mm" file="US20230005247A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="91.02mm" wi="154.52mm" file="US20230005247A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="176.53mm" wi="104.90mm" file="US20230005247A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="242.82mm" wi="155.87mm" orientation="landscape" file="US20230005247A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="212.01mm" wi="154.01mm" file="US20230005247A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="243.25mm" wi="138.01mm" orientation="landscape" file="US20230005247A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="196.09mm" wi="147.32mm" file="US20230005247A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="203.88mm" wi="95.76mm" file="US20230005247A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="232.24mm" wi="155.87mm" file="US20230005247A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="242.49mm" wi="117.43mm" file="US20230005247A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="232.49mm" wi="162.14mm" orientation="landscape" file="US20230005247A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="194.82mm" wi="157.65mm" file="US20230005247A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="237.49mm" wi="160.87mm" file="US20230005247A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="235.12mm" wi="162.73mm" orientation="landscape" file="US20230005247A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="118.45mm" wi="151.21mm" file="US20230005247A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="235.80mm" wi="162.31mm" orientation="landscape" file="US20230005247A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation of International Patent Application No. PCT/JP2020/010541, having an international filing date of Mar. 11, 2020, which designated the United States, the entirety of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">It has been known that the image diagnosis support device for supporting a physician's diagnosis by means of an endoscopic image is configured to utilize machine learning to perform processing of detecting the lesion and acquiring estimated probability indicating the degree of detection accuracy. The neural network has been known as the trained model generated by machine learning. International Publication No. WO 2019/088121 discloses the system which provides support information by estimating information concerning name/position of a lesion, and probability thereof based on a CNN (Convolutional Neural Network) so that the estimated information is superposed on an endoscopic image.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">In accordance with one of some aspect, there is provided a processing system comprising a processor including hardware, wherein the processor is configured to perform processing of: acquiring a detection target image captured by an endoscope apparatus; controlling the endoscope apparatus based on control information; detecting a region of interest included in the detection target image based on the detection target image for calculating estimated probability information representing a probability of the detected region of interest; identifying the control information for improving the estimated probability information related to the region of interest within the detection target image based on the detection target image; and controlling the endoscope apparatus based on the identified control information.</p><p id="p-0005" num="0004">In accordance with one of some aspect, there is provided an image processing method comprising: acquiring a detection target image captured by an endoscope apparatus; detecting a region of interest included in the detection target image to calculate estimated probability information representing a probability of the detected region of interest based on the detection target image; and when using information for controlling the endoscope apparatus as control information, identifying the control information for improving the estimated probability information related to the region of interest within the detection target image based on the detection target image.</p><p id="p-0006" num="0005">In accordance with one of some aspect, there is provided a learning method for generating a trained model, comprising: acquiring an image captured by an endoscope apparatus as an input image; when using information for controlling the endoscope apparatus as control information, acquiring first control information as the control information for acquiring the input image; acquiring second control information as the control information for improving estimated probability information which represents a probability of the region of interest detected from the input image; and generating trained model by performing machine learning of a relationship among the input image, the first control information, and the second control information.</p><p id="p-0007" num="0006">In accordance with one of some aspect, there is provided a processing device comprising a processor including hardware, wherein the processor is configured to perform processing of: acquiring a detection target image captured by an endoscope apparatus; detecting a region of interest included in the detection target image to calculate estimated probability information which represents a probability of the detected region of interest based on the detection target image; identifying the control information for improving the estimated probability information related to the region of interest within the detection target image based on the detection target image; and outputting the identified control information.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a configuration example of a processing system;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an external appearance of an endoscope system;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a configuration example of an endoscope system;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a control information example;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a configuration example of a learning device;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. <b>6</b>A and <b>6</b>B</figref> each illustrate a configuration example of a neural network;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> illustrates a training data example for NN1;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates an example of input/output operation of NN1;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a flowchart representing NN1 learning processing;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> illustrates a training data example for NN2;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>9</b>B</figref> illustrates a data example for acquiring training data;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b>C</figref> illustrates an example of input/output operation of NN2;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates a flowchart representing NN2 learning processing;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates a flowchart representing detection processing and control information identifying processing;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example of display images and timing for acquiring a detection target image;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. <b>13</b>A and <b>13</b>B</figref> each illustrate a display screen example;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> illustrates a training data example for NN2;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>14</b>B</figref> illustrates an example of data for acquiring training data;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>14</b>C</figref> illustrates an example of input/output operation of NN2;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates a flowchart representing detection processing and control information identifying processing;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>16</b>A</figref> illustrates a training data example for NN2;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>16</b>B</figref> illustrates an example of input/output operation of NN2; and</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates a flowchart representing detection processing and control information identifying processing.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DESCRIPTION OF EXEMPLARY EMBODIMENTS</heading><p id="p-0031" num="0030">The following disclosure provides many different embodiments, or examples, for implementing different features of the provided subject matter. These are, of course, merely examples and are not intended to be limiting. In addition, the disclosure may repeat reference numerals and/or letters in the various examples. This repetition is for the purpose of simplicity and clarity and does not in itself dictate a relationship between the various embodiments and/or configurations discussed. Further, when a first element is described as being &#x201c;connected&#x201d; or &#x201c;coupled&#x201d; to a second element, such description includes embodiments in which the first and second elements are directly connected or coupled to each other, and also includes embodiments in which the first and second elements are indirectly connected or coupled to each other with one or more other intervening elements in between.</p><p id="p-0032" num="0031">Exemplary embodiments are described below. Note that the following exemplary embodiments do not in any way limit the scope of the content defined by the claims laid out herein. Note also that all of the elements described in the present embodiment should not necessarily be taken as essential elements.</p><heading id="h-0006" level="1">1. System Configuration</heading><p id="p-0033" num="0032">The generally employed system as disclosed in International Publication No. WO 2019/088121 is configured to display an estimated probability that represents the degree of estimation accuracy using the trained model. In the case of low estimated probability, the estimated result is not displayed to suppress provision of the low-accurate information to the user. The foregoing technique does not consider measures to be taken for improving the estimated probability derived from the trained model.</p><p id="p-0034" num="0033">For example, in order to cope with the low estimated probability owing to a dark captured image of lesion, such measures as raising the dimming target value in the dimming processing may be taken to capture the bright image of lesion, resulting in improved estimated probability. However, the control performed by the generally employed technique merely allows the estimated probability to be displayed, or not to be displayed because of low estimated probability. In order to control the light source and the like, the user has to determine the need of changing the light intensity of the light source based on the displayed image and the like. Furthermore, the user has to execute the specific operation for changing the light intensity based on the determination. In other words, the generally employed system provides only the result of processing to the input image, leaving the task to the user for capturing the image suitable for detection of the lesion or the like.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a configuration of a processing system <b>100</b> according to the present embodiment. The processing system <b>100</b> includes an acquisition section <b>110</b>, a processing section <b>120</b>, and a control section <b>130</b>.</p><p id="p-0036" num="0035">The acquisition section <b>110</b> is an interface for acquiring images, for example, an interface circuit for acquiring a signal from an image sensor <b>312</b> via a signal line included in a universal cable <b>310</b><i>c</i>. The acquisition section <b>110</b> may be provided with a pre-processing section <b>331</b> to be described later referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The processing system <b>100</b> may be included in an information processing device for acquiring the image signal output from a scope section <b>310</b> via the network. In this case, the acquisition section <b>110</b> serves as a communication interface such as a communication chip.</p><p id="p-0037" num="0036">The processing section <b>120</b> and the control section <b>130</b> are constituted by hardware as described below. The hardware includes at least one of a circuit for processing digital signals and a circuit for processing analog signals. The hardware may be configured to include one or more circuit devices, or one or more circuit elements, which are mounted on the circuit substrate. For example, an IC (Integrated Circuit), an FPGA (Field-programmable Gate Array), and the like may be employed for one or more circuit devices. For example, a resistance, a capacitor, and the like may be employed for one or more circuit elements.</p><p id="p-0038" num="0037">The following processor may be employed for implementing the processing section <b>120</b> and the control section <b>130</b>. The processing system <b>100</b> includes a memory for storing information, and a processor operated based on the information stored in the memory. The information may be programs and various kinds of data, for example. The processor includes the hardware. Various types of processors may be used, for example, a CPU (Central Processing Unit), a GPU (Graphics Processing Unit), a DSP (Digital Signal Processor), and the like. The memory may be a semiconductor memory such as a SRAM (Static Random Access Memory) and a DRAM (Dynamic Random Access Memory), a register, a magnetic storage device such as an HDD (Hard Disk Drive), and an optical storage device such as an optical disk device. For example, the memory stores computer readable instructions. The processor executes the instruction to implement functions of the processing section <b>120</b> and the control section <b>130</b> as processing. The instruction herein may be an instruction set constituting the program, or the one for commanding a hardware circuit of the processor to carry out the operation. The processing section <b>120</b> and the control section <b>130</b> may be implemented by a single processor or different processors. The function of the processing section <b>120</b> may be implemented through distributed processing performed by multiple processors. This applies to the control section <b>130</b>.</p><p id="p-0039" num="0038">The acquisition section <b>110</b> acquires a detection target image captured by an endoscope apparatus. The endoscope apparatus herein partially or fully constitutes an endoscope system <b>300</b> to be described later referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, for example. The endoscope apparatus constitutes a part of the endoscope system <b>300</b> including a scope section <b>310</b>, a light source device <b>350</b>, and a processing device <b>330</b>.</p><p id="p-0040" num="0039">The control section <b>130</b> controls the endoscope apparatus based on the control information. As described later referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, operations for controlling the endoscope apparatus performed by the control section <b>130</b> include controlling operations of a light source <b>352</b> of the light source device <b>350</b>, imaging conditions using the image sensor <b>312</b>, image processing performed by the processing device <b>330</b>, and the like.</p><p id="p-0041" num="0040">Based on the trained model and the detection target image, the processing section <b>120</b> detects a region of interest included in the detection target image, and calculates the estimated probability information indicating accuracy of the detected region of interest. Based on the detection target image, the processing section <b>120</b> of the present embodiment identifies the control information for improving the estimated probability information related to the region of interest in the detection target image. Based on the identified control information, the control section <b>130</b> controls the endoscope apparatus.</p><p id="p-0042" num="0041">The trained model herein is acquired through machine learning for calculating the estimated probability information of the region of interest in the input image. More specifically, the trained model is acquired through the machine learning based on a data set having the input image correlated with information for identifying the region of interest included in the input image. Upon input of an image, the trained model outputs a result of detecting the region of interest of the target image, and the estimated probability information indicating a probability of the detection result. Although description will be given below of an example in which the estimated probability information is the estimated probability itself, the estimated probability information may be the information as an index indicating the estimation probability, and may be different from the estimated probability.</p><p id="p-0043" num="0042">The region of interest in the present embodiment refers to the region to be observed by the user with relatively higher priority than the other region. If the user is a physician who performs diagnosis and treatment, the region of interest corresponds to the region which reflects a lesion part, for example. If the physician requires to observe bubbles or feces, the region reflecting the bubbles or feces may be the region of interest. That is, the target of user's interest varies depending on the observation purpose. In the observation, the region to be observed by the user with relatively higher priority than that of the other region becomes the region of interest.</p><p id="p-0044" num="0043">The processing system <b>100</b> of the present embodiment performs not only processing of obtaining the estimated probability, but also processing of identifying the control information for improving the estimated probability. The control information includes information data corresponding to various parameter types as described later referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. For example, in order to improve the estimated probability, the processing section <b>120</b> determines which parameter type should be changed and what value should be used for the parameter value of the parameter type.</p><p id="p-0045" num="0044">The control section <b>130</b> performs the control using the control information identified by the processing section <b>120</b>. Accordingly, the newly acquired detection target image as the control result is expected to have the estimated probability of the region of interest further improved than the one before changing the control information. In other words, the processing system <b>100</b> of the present embodiment itself is allowed to carry out the cause analysis for improving the estimated probability and modification of control conditions. This makes it possible to provide highly reliable information while suppressing increase in the load of the user.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a configuration of the endoscope system <b>300</b> including the processing system <b>100</b>. The endoscope system <b>300</b> includes the scope section <b>310</b>, the processing device <b>330</b>, a display section <b>340</b>, and the light source device <b>350</b>. For example, the processing system <b>100</b> is included in the processing device <b>330</b>. The physician performs an endoscopy on a patient using the endoscope system <b>300</b>. The configuration of the endoscope system <b>300</b> is not limited to the one as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, but may be variously modified by omitting a part of the components, or adding other components. The following explanation will be made with respect to the use of a flexible mirror for diagnosis of digestive organs as an exemplified case. The scope section <b>310</b> of the present embodiment may be a hard mirror used for a laparoscopic surgery operation (laparoscopic procedures). The endoscope system <b>300</b> is not limited to the medical endoscope for in-vivo observation, but may be formed into to an industrial endoscope.</p><p id="p-0047" num="0046">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the processing device <b>330</b> is a single unit connected to the scope section <b>310</b> via a connector <b>310</b><i>d </i>as an exemplified case, which is not limited thereto. For example, the processing device <b>330</b> may be partially or fully configured by a PC (Personal Computer) and other information processing device such as a server system, which are connectable via the network. For example, cloud computing may be used to implement the processing device <b>330</b>. The network herein may be a private network such as Intranet, or public telecommunication network such as Internet. The network may be wired or wireless. That is, configuration of the processing system <b>100</b> is not limited to the one included in the device to be connected to the scope section <b>310</b> via the connector <b>310</b><i>d</i>. Functions of the processing system <b>100</b> may be partially or fully implemented by other devices such as a PC, or by cloud computing.</p><p id="p-0048" num="0047">The scope section <b>310</b> includes an operation section <b>310</b><i>a</i>, a flexible insertion section <b>310</b><i>b</i>, and a universal cable <b>310</b><i>c </i>including a signal line. The scope section <b>310</b> is a tubular insertion device for inserting the tubular insertion section <b>310</b><i>b </i>into a body cavity. The connector <b>310</b><i>d </i>is attached to an end of the universal cable <b>310</b><i>c</i>. The scope section <b>310</b> is detachably attached to the light source device <b>350</b> and the processing device <b>330</b> using the connector <b>310</b><i>d</i>. As described later referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a light guide <b>315</b> is inserted into the universal cable <b>310</b><i>c</i>. The scope section <b>310</b> emits illumination light of the light source device <b>350</b> from a leading end of the insertion section <b>310</b><i>b </i>through the light guide <b>315</b>.</p><p id="p-0049" num="0048">For example, the insertion section <b>310</b><i>b </i>includes the leading end portion, a curve portion which can be curved, and a flexible tube portion, which are positioned from the leading end to a base end of the insertion section <b>310</b><i>b</i>. The insertion section <b>310</b><i>b </i>is inserted into an object. The leading end portion of the insertion section <b>310</b><i>b </i>constitutes a hard tip end member formed at the leading end of the scope section <b>310</b>. An objective optical system <b>311</b> or an image sensor <b>312</b> which will be described later are attached to the leading end portion, for example.</p><p id="p-0050" num="0049">The curve portion can be curved in a desired direction by operating a curving operation member provided in the operation section <b>310</b><i>a</i>. The curving operation member includes left-right and up-down curving operation knobs, for example. The operation section <b>310</b><i>a </i>may be provided with various operation buttons, for example, a release button, a gas/water supply button, or the like in addition to the curving operation member.</p><p id="p-0051" num="0050">The processing device <b>330</b> is a video processor configured to perform predetermined image processing to a received captured signal, and generate a captured image. The video signal of the generated captured image is output from the processing device <b>330</b> to the display section <b>340</b> on which a live captured image is displayed. A structure of the processing device <b>330</b> will be described later. The display section <b>340</b> may be a liquid crystal display, an EL (Electro-Luminescence) display, or the like.</p><p id="p-0052" num="0051">The light source device <b>350</b> is capable of emitting normal light for a normal light observation mode. If the endoscope system <b>300</b> has a special light observation mode in addition to the normal light observation mode, the light source device <b>350</b> emits the normal light for the normal light observation mode and the special light for the special light observation mode selectively.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates configuration of components constituting the endoscope system <b>300</b>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> partially omits the structure of the scope section <b>310</b> for simplification.</p><p id="p-0054" num="0053">The light source device <b>350</b> includes the light source <b>352</b> which emits illustration light. The light source <b>352</b> may be a xenon light source, an LED (light emitting diode), or a laser light source. The light source <b>352</b> may be other light source without being limited to the light emission type.</p><p id="p-0055" num="0054">The insertion section <b>310</b><i>b </i>includes the objective optical system <b>311</b>, the image sensor <b>312</b>, an illumination lens <b>314</b>, and a light guide <b>315</b>. The light guide <b>315</b> guides the illumination light from the light source <b>352</b> to the leading end of the insertion section <b>310</b><i>b</i>. The illumination lens <b>314</b> irradiates the object with the illumination light guided by the light guide <b>315</b>. The objective optical system <b>311</b> forms the reflected light reflected from the object into an object image. The objective optical system <b>311</b> may be configured to have a focus lens which makes the object image forming position variable in accordance with a position of the focus lens. For example, the insertion section <b>310</b><i>b </i>may be configured to have a not shown actuator which drives the focus lens based on the control from a control section <b>332</b>. In this case, the control section <b>332</b> performs an AF (AutoFocus) control operation.</p><p id="p-0056" num="0055">The image sensor <b>312</b> receives the light from the object via the objective optical system <b>311</b>. The image sensor <b>312</b> may be a monochrome sensor, or an element provided with a color filter. The color filter may be a known Bayer filter, a complementary color filter, or any other filter. The complementary color filter includes a cyan color filter, a magenta color filter, and a yellow color filter.</p><p id="p-0057" num="0056">The processing device <b>330</b> performs operations for image processing and overall system controlling. The processing device <b>330</b> includes the pre-processing section <b>331</b>, the control section <b>332</b>, a storage section <b>333</b>, a detection processing section <b>334</b>, a control information identifying section <b>335</b>, and a post-processing section <b>336</b>. For example, the pre-processing section <b>331</b> corresponds to the acquisition section <b>110</b> of the processing system <b>100</b>. The detection processing section <b>334</b> and the control information identifying section <b>335</b> correspond to the processing section <b>120</b> of the processing system <b>100</b>. The control section <b>332</b> corresponds to the control section <b>130</b> of the processing system <b>100</b>.</p><p id="p-0058" num="0057">The pre-processing section <b>331</b> performs A/D conversion for converting an analog signal sequentially output from the image sensor <b>312</b> into a digital image, and various kinds of correction processing to the A/D converted image data. The image sensor <b>312</b> may be provided with an A/D conversion circuit by omitting the A/D conversion performed by the pre-processing section <b>331</b>. The correction processing herein includes processing such as color matrix correction, structure highlighting, noise reduction, AGC (automatic gain control), and the like as described later referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, for example. The pre-processing section <b>331</b> may be configured to perform other correction processing such as white balance processing. The pre-processing section <b>331</b> outputs the processed image to the detection processing section <b>334</b> and the control information identifying section <b>335</b> as the detection target image. The pre-processing section <b>331</b> outputs the processed image to the post-processing section <b>336</b> as a display image.</p><p id="p-0059" num="0058">The detection processing section <b>334</b> performs detection processing for detecting the region of interest from the detection target image. The detection processing section <b>334</b> outputs an estimated probability representing probability of the detected region of interest. For example, the detection processing section <b>334</b> is operated in accordance with the trained model information stored in the storage section <b>333</b> so that the detection processing is performed.</p><p id="p-0060" num="0059">In the present embodiment, a single kind of region of interest may be detected. For example, in the case of a polyp as the region of interest, the detection processing may be performed for identifying the position and size of the polyp in the detection target image. In the present embodiment, multiple types of regions of interest may be set. For example, in the known classification process, the polyp is classified into TYPE1, TYPE2A, TYPE2B, and TYPE3 depending on the condition. The detection processing of the present embodiment may include processing of classifying the polyp into one of the types as described above besides the processing of merely detecting the position and size of the polyp. The estimated probability in this case is the information indicating probability of the position/size of the region of interest, and the classification result.</p><p id="p-0061" num="0060">The control information identifying section <b>335</b> performs processing of identifying the control information for improving the estimated probability based on the detection target image. The processing performed by the detection processing section <b>334</b> and the control information identifying section <b>335</b> will be described in detail later.</p><p id="p-0062" num="0061">The post-processing section <b>336</b> performs post-processing based on the detection result of the region of interest, which has been derived from the detection processing section <b>334</b>, and outputs the post-processed image to the display section <b>340</b>. The post-processing herein is performed for adding the detection result based on the detection target image to the display image, for example. As an explanation will be made later referring to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, for example, the display image and the detection target image are alternately acquired. As an explanation will be made later referring to <figref idref="DRAWINGS">FIG. <b>13</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>13</b>B</figref>, the detection result of the region of interest refers to, in a narrow sense, the information for supporting the user with diagnosis or the like. Accordingly, the detection target image of the present embodiment may be rephrased as a support image for supporting the user. The detection result of the region of interest may be rephrased as support information.</p><p id="p-0063" num="0062">The control section <b>332</b> is interconnected to the image sensor <b>312</b>, the pre-processing section <b>331</b>, the detection processing section <b>334</b>, the control information identifying section <b>335</b>, and the light source <b>352</b> so that the respective sections and components are controlled. Specifically, the control section <b>332</b> controls the respective components of the endoscope system <b>300</b> based on the control information.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of the control information. The control information includes light source control information for controlling the light source <b>352</b>, imaging control information for controlling an imaging condition for the image sensor <b>312</b>, and image processing control information for controlling the image processing performed by the processing device <b>330</b>.</p><p id="p-0065" num="0064">The light source control information includes, for example, wavelength, light quantity ratio, light quantity, duty, and light distribution, as parameter types. The imaging control information includes an imaging frame rate. The image processing control information includes a color matrix, structure highlighting, noise reduction and AGC. The control information is, for example, a set of the parameter type and specific parameter value related to the parameter type. An explanation will be made with respect to specific examples of the respective parameter types and parameter values.</p><p id="p-0066" num="0065">The wavelength represents a wavelength band of the illumination light. For example, the light source device <b>350</b> is capable of emitting the normal light and the special light, and includes multiple light sources such as a red LED, a green LED, a blue LED, a green narrowband light LED, a blue narrowband light LED, and the like. The light source device <b>350</b> emits the normal light which contains R light, G light, and B light by lighting the red LED, the green LED, and the blue LED, respectively. For example, the B light has its wavelength band ranging from 430 to 500 nm, the G light has its wavelength band ranging from 500 to 600 nm, and the R light has its wavelength band ranging from 600 to 700 nm. The light source device <b>350</b> emits the special light which contains the G2 light and the B2 light by lighting the green narrowband light LED and the blue narrowband light LED. For example, the B2 light has its wavelength band ranging from 390 to 445 nm, and the G2 light has its wavelength band ranging from 530 to 550 nm. The special light herein refers to the illumination light for NBI (Narrow Band Imaging). The light with other wavelength band using infrared light or the like has been known as the special light. The light as described above is broadly applicable to the present embodiment.</p><p id="p-0067" num="0066">The parameter value indicating the wavelength is expressed as binary information for identifying whether the light is the normal light or NBI, for example. Alternatively, the parameter value may be set as the information for determining lighting on/off of each of the multiple LEDs. In this case, the parameter value related to the wavelength is expressed as data with bit number corresponding to the number of LEDs. The control section <b>332</b> controls lighting on/off of multiple light sources based on the parameter value related to the wavelength. Alternatively, the light source device <b>350</b> may be configured to include a white light source and a filter, and to switch the light between the normal light and the special light based on insertion/retraction or rotation of the filter. In this case, the control section <b>332</b> controls the filter based on the parameter value of the wavelength.</p><p id="p-0068" num="0067">The light quantity refers to intensity of light emitted from the light source device <b>350</b>. For example, the endoscope system <b>300</b> of the present embodiment may be configured to perform known automatic dimming processing. The automatic dimming processing is performed by, for example, determining brightness in reference to the captured image, and automatically adjust the light quantity of the illumination light based on the determination result. In the dimming processing, a dimming target value is set as a target value of brightness. The parameter value indicating the light quantity is, for example, a target light quantity value. Adjustment of the target light quantity value allows brightness of the region of interest on the image to be optimized.</p><p id="p-0069" num="0068">The light quantity ratio refers to a ratio of intensity among multiple lights emittable by the light source device <b>350</b>. For example, the light quantity ratio of the normal light corresponds to the intensity ratio among R light, G light, and B light. The parameter value related to the light quantity ratio is, for example, a numerical value obtained by normalizing each emission intensity of multiple LEDs. Based on the foregoing parameter values of light quantity and the foregoing parameter values of light quantity ratio, the control section <b>332</b> determines each emission intensity of the respective light sources, for example, the current value to be supplied to each of the light sources. The light quantity may be adjusted in accordance with the duty to be described later. The light quantity ratio in this case corresponds to, for example, the duty ratio among the respective RGB light sources. Control of the light quantity ratio allows cancellation of variation in color owing to difference among individual patients, for example.</p><p id="p-0070" num="0069">The duty refers to a relationship between light-on time and light-off time of the light source <b>352</b>, and in a narrow sense, the ratio of the light-on time to the period corresponding to the single imaging frame. The parameter value related to the duty may be, for example, the numerical value indicating the ratio as described above, or the numerical value indicating the light-on time. For example, in the case where the light source <b>352</b> performs pulse emission with repetition of light-on and light-off, duty control is performed by changing the light-on time per frame. Alternatively, the duty control may be performed by switching the continuous emission state of the light source <b>352</b> to the pulse emission state. If the light-on time is reduced without changing the light quantity per unit time, the image is darkened because of reduced total light quantity per frame. For the purpose of reducing the duty, the control section <b>332</b> may be configured to perform the control operation for raising the dimming target value described above to maintain the total light quantity per frame. Reduction in the duty may suppress blurring of the image. Increase in the duty may suppress deterioration in the LED as the total light quantity is maintained even in the case of suppressing the light quantity per unit time.</p><p id="p-0071" num="0070">The light distribution refers to the direction-dependent light intensity. For example, in the case where multiple irradiation ports for irradiating different areas are attached to the leading end of the insertion section <b>310</b><i>b</i>, quantities of light emitted from the respective irradiation ports are regulated to change the light distribution. The parameter value related to the light distribution refers to, for example, the information to identify each quantity of light emitted from the irradiation ports. The use of an optical system such as a lens and a filter may control the light quantity of the illumination light in the respective directions. Specific structures for changing the light distribution may be variously modified.</p><p id="p-0072" num="0071">For example, assuming that the insertion section <b>310</b><i>b </i>is moving in a lumen-like object, the object around the center of the image is farther from the leading end of the insertion section <b>310</b><i>b </i>than the object in the periphery of the image. As a result, the area around the periphery of the image becomes brighter than the area around the center of the image. If the leading end of the insertion section <b>310</b><i>b </i>substantially faces the wall surface of the object, the distance from the leading end to the object around the center of the image hardly differs from the distance from the leading end to the object in the periphery of the image. Accordingly, the area around the center of the image becomes brighter than the area in the periphery of the image. As described above, the direction in which the image is likely to become brighter or darker is variable depending on the circumstance. Control of the light distribution allows optimization of brightness of the desired region of the image.</p><p id="p-0073" num="0072">The frame rate refers to the number of images captured per unit time. The parameter value related to the frame rate is the numerical value indicating the number of frames per second, for example. Raising the frame rate may increase the number of captured images acquired per unit time so that blurring of the image is suppressed. Reducing the frame rate may prolong the irradiation time taken by the light source <b>352</b>. This allows the bright image to be easily acquired.</p><p id="p-0074" num="0073">The color matrix refers to a matrix for obtaining each pixel value after correction based on the original RGB pixel values, for example. A parameter value related to the color matrix is a set of numerical values indicating the respective elements of the matrix. The parameter value is not limited to the matrix by itself. It is possible to use other information based on which the color tone is adjustable through conversion of RGB pixel values. The use of the color matrix allows suppression of variation in color in the similar manner to the case of controlling the light quantity ratio of the light source <b>352</b>.</p><p id="p-0075" num="0074">The structure highlighting processing refers to the digital highlighting filter processing, for example. A parameter value related to the structure highlighting refers to the information for identifying the filter property of the digital filter, for example, a set of numerical values indicating the space filter size, and the respective elements of the filter. Performing the structure highlighting processing allows the shape of the region of interest to be clarified. Excessively performing the structure highlighting processing may result in the risk of generating artifact.</p><p id="p-0076" num="0075">Noise reduction processing refers to the smoothing filter processing, for example. A parameter value related to the noise reduction processing refers to information for identifying the smoothing filter. The information for identifying the smoothing filter may be, for example, the value of u of the Gaussian filter, or a set of numerical values indicating the respective elements of the filter. The degree of noise reduction may be adjusted by changing the given number of filter applications. In this case, a parameter value related to the noise reduction refers to a numerical value indicating the number of applications of the smoothing filter. Performing the noise reduction processing reduces the noise included in the image so that visibility of the region of interest is improved. Excessively performing noise reduction processing may make the edge or the like of the region of interest dull.</p><p id="p-0077" num="0076">A parameter value related to AGC refers to a numerical value indicating the gain. Increase in the gain allows the image to be brightened, but may increase the noise as well. Decrease in the gain allows suppression of noise increase, but resultant brightness of the image may be insufficient.</p><p id="p-0078" num="0077">As described above, the control information includes various kinds of parameter type information data. Adjustment of the parameter values of parameter types changes properties of the acquired image. The control information of the present embodiment may fully contain information on all the parameter types as described above or partially omit the information. The control information may contain information on other parameter types related to the light source, imaging, and image processing.</p><p id="p-0079" num="0078">The parameter values optimum for raising the estimated probability of the region of interest vary depending on conditions. The condition herein represents the patient as the imaging object, an organ, a type of region of interest, relative position/posture between the leading end of the insertion section <b>310</b><i>b </i>and the object, and the like. Accordingly, the process of identifying the optimum control information may impose heavy load to the user. Meanwhile, the method of the present embodiment allows automatic identification of the control information that the processing system <b>100</b> regards as appropriate as described above.</p><heading id="h-0007" level="1">2. Processing Flow</heading><p id="p-0080" num="0079">The processing of the present embodiment will be described in detail. The learning processing for generating the trained model will be described first, and then, inference processing using the trained model will be described. The inference processing herein refers to the processing of detecting a region of interest, which is performed by the detection processing section <b>334</b>. In the following description, an explanation will be made with respect to an example of using the trained model for processing of identifying the control information, which is performed by the control information identifying section <b>335</b>.</p><heading id="h-0008" level="1">2.1 Learning Processing</heading><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a configuration example of a learning device <b>400</b>. The learning device <b>400</b> includes an acquisition section <b>410</b> and a learning section <b>420</b>. The acquisition section <b>410</b> acquires training data to be used for learning. A set of training data includes input data, and data correlated with a correct label corresponding to the input data. The learning section <b>420</b> performs machine learning based on a large volume of acquired training data to generate the trained model. The detailed explanations of the training data and the specific flow of the learning processing will be made later.</p><p id="p-0082" num="0081">The learning device <b>400</b> refers to an information processing device such as a PC and a server system. The learning device <b>400</b> may be implemented through distributed processing performed by multiple devices. The learning device <b>400</b> may also be implemented by, for example, cloud computing using multiple servers. The learning device <b>400</b> may be integrated with the processing system <b>100</b>, or serve as a device independent from the processing system <b>100</b>.</p><p id="p-0083" num="0082">An explanation will be made briefly with respect to the machine learning using the neural network. The method for machine learning according to the present embodiment is not limited to the one to be described below. In the present embodiment, the machine learning may be carried out using other model, for example, SVM (support vector machine), or through the process established by developing various methods such as the neural network and the SVM.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a schematic view which illustrates the neural network including an input layer for receiving a data input, an intermediate layer for performing arithmetic operations based on an output from the input layer, and an output layer for outputting data based on an output from the intermediate layer. <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> illustrates an exemplary network with a two-layered intermediate layer. However, the intermediate layer may have one layer, or three or more layers. The number of nodes included in each layer is not limited to the example illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, but may be variously modified. In the present embodiment, it is preferable to employ the deep learning using multilayer neural network in consideration of accuracy. The multilayer herein refers to four or more layers in a narrow sense.</p><p id="p-0085" num="0084">As <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> illustrates, the node included in a given layer is bound to the node included in the adjacent layer. Each weighting coefficient is set for binding the respective nodes. Each of the nodes multiplies the output from the node in the previous stage by the weighting coefficient to obtain a total value of multiplication results. The node obtains its output by adding a bias to the total value, and applying an activation function to the addition result. The processing is sequentially performed from the input layer to the output layer so that an output of the neural network is obtained. Various functions such as the sigmoid function and ReLU function each known as the activation function are broadly applicable to the present embodiment.</p><p id="p-0086" num="0085">Learning through the neural network refers to the processing which determines the appropriate weighting coefficient. The weighting coefficient herein includes the bias. Specifically, the learning device <b>400</b> inputs the input data of the training data to the neural network, and performs the arithmetic operation in the forward direction using the corresponding weighting coefficient so that an output is obtained. The learning section <b>420</b> of the learning device <b>400</b> obtains an error function based on the output and the correct label of the training data. The weighting coefficient is updated to make the error function small. For example, it is possible to use an error back propagation method for updating the weighting coefficient from the output layer to the input layer.</p><p id="p-0087" num="0086">The neural network may be implemented by a CNN (Convolutional Neural Network), for example. <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a schematic view which illustrates the CNN. The CNN includes a convolutional layer which performs a convolutional operation and a pooling layer. The convolutional layer performs the filter processing. The pooling layer performs a pooling operation for reducing longitudinal/lateral size. <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> illustrates an example of the network in which the convolutional layer and the pooling layer perform arithmetic operations multiple times, and a fully connected layer performs an arithmetic operation so that an output is obtained. The fully connected layer performs the arithmetic operation for connecting all nodes in the previous layer to the nodes of the given layer. The arithmetic operation corresponds to the one performed in each of the respective layers as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>. Although not illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, when using the CNN, the arithmetic operation with the activation function is performed similar to the case as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>. The CNN known to be variously configured may be broadly applied to the present embodiment. For example, the CNN of the present embodiment allows the use of the known RPN (Region Proposal Network).</p><p id="p-0088" num="0087">The same processing procedure as the one illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is used for the CNN. That is, the learning device <b>400</b> inputs the input data of the training data to the CNN, and obtains an output by performing the filter processing and the pooling operation using the corresponding filter property. The error function is obtained based on the output and the correction label. The weighting coefficient including the filter property is updated to make the error function small. For example, the error back propagation method may also be used for updating the weighting coefficient for the CNN.</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> illustrates an example of the training data for generating the trained model used for detection processing performed by the detection processing section <b>334</b>. In the following description, the trained model used for the detection processing will be referred to as an NN1. <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates an input/output operation of the NN1.</p><p id="p-0090" num="0089">As <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> illustrates, the training data include input images and annotation data attached to the input images. The input image is captured by the scope section <b>310</b>, specifically, an in-vivo image in a narrow sense. The annotation data refer to the information data for identifying the region of interest in the input image, which are attached by a user with specialized knowledge such as physicians. For example, as <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> illustrates, the annotation data are used for identifying position/size of a rectangular area which contains the region of interest. The rectangular area will be hereinafter referred to as a detection frame. The annotation data are configured by combining coordinates of an upper left end point and coordinates of a lower right end point of the detection frame. Alternatively, the annotation data may be configured to identify the region of interest by a unit of pixel. For example, the annotation data may be formed into a mask image having a pixel included in the region of interest set to a first pixel value, and a pixel which is not included in the region of interest set to a second pixel value.</p><p id="p-0091" num="0090">As <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates, the NN1 receives an input of the input image, and performs an arithmetic operation in the forward direction to output the detection result and the estimated probability information. The NN1 sets a predetermined number of detection frame candidates on the input image, and performs processing of obtaining probability that the detection frame candidate is identified as the detection frame. In this case, the detection frame candidate having the probability being equal to or higher than the predetermined value is output as the detection frame. The probability correlated to the detection frame candidate employed for the detection frame is set as the estimated probability. Alternatively, the NN1 performs processing of obtaining the probability that each pixel of the input image is included in the region of interest. In this case, the set of pixels, which exhibits high probability represents the detection result of the region of interest. The estimated probability information is determined based on the probability set correlated to the pixel classified as the region of interest. Such information is expressed as statistics describing multiple numerical values representing the probability, for example. The statistic herein may be an average value, a median, or other values.</p><p id="p-0092" num="0091">The NN1 may be configured to perform processing of classifying the region of interest. For example, the NN1 may be configured to receive an input of the input image, and perform an arithmetic operation in the forward direction to output the position/size of the region of interest, and type of the region of interest as detection results. The NN1 outputs the estimated probability information indicating the probability of the detection result. For example, the NN1 performs processing to obtain probability that the object included in the detection frame candidate is a polyp of TYPE1, TYPE2A, TYPE2B, TYPE3, and the normal mucous membrane. That is, it is possible for the detection result of the region of interest according to the present embodiment to include the type of the region of interest.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart illustrating NN1 learning processing. First, in steps S<b>101</b> and S<b>102</b>, the acquisition section <b>410</b> acquires an input image, and annotation data attached to the input image. For example, the learning device <b>400</b> stores a large volume of training data in which the input image and the annotation data are correlated, in a not shown storage section. In steps S<b>101</b> and S<b>102</b>, the processing is performed for retrieving one of the training data, for example.</p><p id="p-0094" num="0093">In step S<b>103</b>, the learning section <b>420</b> obtains the error function. Specifically, the learning section <b>420</b> inputs the input image to the NN1, and performs an arithmetic operation in the forward direction based on the corresponding weighting coefficient. The learning section <b>420</b> obtains the error function based on comparison between the operation result and the annotation data.</p><p id="p-0095" num="0094">In step S<b>104</b>, the learning section <b>420</b> updates the weighting coefficient to make the error function small. Processing in step S<b>104</b> may be performed using the error back propagation method as described above. The series of processing operations in steps S<b>101</b> to S<b>104</b> correspond to one cycle of the learning processing based on one set of the training data.</p><p id="p-0096" num="0095">In step S<b>105</b>, the learning section <b>420</b> determines whether or not the learning processing is to be finished. For example, the learning section <b>420</b> may be configured to finish the learning processing after the processing in steps from S<b>101</b> to S<b>104</b> the predetermined number of times. Alternatively, the learning device <b>400</b> may be configured to hold a part of a large volume of training data as verification data. The verification data are used for confirming accuracy of the learning result, and not used for updating the weighting coefficient. The learning section <b>420</b> may be configured to finish the learning processing if an accuracy rate of the estimation processing using the verification data exceeds a predetermined threshold.</p><p id="p-0097" num="0096">If No is obtained in step S<b>105</b>, the process returns to step S<b>101</b> where the learning processing is continuously performed based on the next training data. If Yes is obtained in step S<b>105</b>, the learning processing is finished. The learning device <b>400</b> transmits the generated trained model information to the processing system <b>100</b>. Referring to an example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the trained model information is stored in the storage section <b>333</b>. It has been known that various methods such as batch learning and mini batch learning are implemented for machine learning. Those methods may be broadly applied to the present embodiment.</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> illustrates an example of training data for generating the trained model used for the control information identifying processing performed by the control information identifying section <b>335</b>. The trained model used for the control information identifying processing will be referred to as an NN2. <figref idref="DRAWINGS">FIG. <b>9</b>B</figref> illustrates an example of data for generating the training data as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>. <figref idref="DRAWINGS">FIG. <b>9</b>C</figref> illustrates an input/output operation of the NN2.</p><p id="p-0099" num="0098">As <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> illustrates, the training data include a second input image and control information used for acquiring the second input image. The input image input to the NN1 and the second input image both have been captured using the scope section <b>310</b>. Those images may be shared or different from each other. <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> illustrates an example that the control information includes N parameter types. The N parameter types will be expressed as P<sub>1 </sub>to P<sub>N</sub>. Each of the parameter types P<sub>1 </sub>to P<sub>N </sub>corresponds to, for example, one of the wavelength, light quantity ratio, light quantity, duty, light distribution, frame rate, color matrix, structure highlighting, noise reduction, and AGC, respectively. Parameter values of the parameter types P<sub>1 </sub>to P<sub>N </sub>will be expressed as p<sub>1 </sub>to p<sub>N</sub>, respectively. The parameter values p<sub>1 </sub>to p<sub>N </sub>are related to the light source control information, the imaging control information, and the image processing control information. Based on the parameter values, emission of illumination light, imaging, and image processing are performed so that the second input image is acquired. As <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> illustrates, the training data include the information for identifying a priority parameter type.</p><p id="p-0100" num="0099">The priority parameter type refers to the parameter type which should be preferentially changed for improving the estimated probability. The improved estimated probability represents that a second estimated probability of the second image acquired using the control information which has been changed is obtained, which becomes higher than a first estimated probability of the first image acquired using the given control information. The priority parameter type of P<sub>k </sub>represents that it has been determined that change in the parameter value of P<sub>k </sub>is likely to improve the estimated probability higher than the one derived from change in the parameter value of the parameter type except P<sub>k</sub>. The value of k is an integer equal to or larger than 1 and equal to or smaller than N.</p><p id="p-0101" num="0100">For example, in the training data acquisition stage, the given object which contains the region of interest is continuously captured while changing the control information so that the image is acquired. In the foregoing circumstance, the parameter value is changed by each type. Assuming that the control information as indicated by C1 is set to an initial value, referring to C2, only the parameter value of P<sub>1 </sub>is changed from p<sub>1 </sub>to p<sub>1</sub>&#x2032; as the initial value so that the image is acquired. This applies to the subsequent information hereinafter. The estimated probability is obtained by inputting the image acquired at each timing to the NN1 as described above. In an example of the case, the estimated probability is ranged from 0 to 100%.</p><p id="p-0102" num="0101">Referring to the example of <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>, in the control information in the initial state, the estimated probability is as low as 40%. The control information is changed to modify property of the acquired image so that the estimated probability to be output upon input of such image to the NN1 is changed as well. For example, it is assumed that the estimated probability is maximized when changing the parameter value of the parameter type P<sub>k </sub>as indicated by C3 of <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>. In this case, the parameter type dominant over the estimated probability is P<sub>k</sub>, and accordingly, it is conceivable that preferential change in the parameter type P<sub>k </sub>allows improvement of the estimated probability. That is, the acquisition section <b>410</b> sets the image of C1 to the second input image, the parameter values p<sub>1 </sub>to p<sub>N </sub>of C1 to the control information, and the type P<sub>k </sub>to the priority parameter type to acquire a single set of training data. The data for acquiring the training data are not limited to those illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>, but may be variously modified.</p><p id="p-0103" num="0102">As <figref idref="DRAWINGS">FIG. <b>9</b>C</figref> illustrates, the NN2 receives inputs of the second input image and the control information, and performs the arithmetic operation in the forward direction to output the probability that change in the parameter type is recommended. For example, the output layer of the NN2 includes N nodes, and outputs N sets of output data.</p><p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating learning processing of the NN2. First, in steps S<b>201</b> and S<b>202</b>, the acquisition section <b>410</b> acquires an image group corresponding to the one illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>, and a control information group as a set of control information upon acquisition of the images, respectively. In step S<b>203</b>, the acquisition section <b>410</b> inputs the respective images to the generated NN1 to acquire an estimated probability group. The data as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref> are acquired by performing processing in steps S<b>201</b> to S<b>203</b>.</p><p id="p-0105" num="0104">Next, in step S<b>204</b>, the acquisition section <b>410</b> performs processing of acquiring the training data based on the data as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>. Specifically, the acquisition section <b>410</b> acquires a large volume of training data having the second input image, the control information, and the priority parameter type, correlated with one another, and stores those data.</p><p id="p-0106" num="0105">In step S<b>205</b>, the acquisition section <b>410</b> retrieves one set of training data. In step S<b>206</b>, the learning section <b>420</b> obtains the error function. Specifically, the learning section <b>420</b> inputs the second input image and the control information of the training data to the NN2, and performs the arithmetic operation in the forward direction based on the corresponding weighting coefficient. For example, as <figref idref="DRAWINGS">FIG. <b>9</b>C</figref> illustrates, N data are obtained as operation results, indicating the probability that each parameter type should be recommended as the priority parameter type. If the output layer of the NN2 is a known softmax layer, the N data are probability data having the sum total set to 1. If the priority parameter type included in the training data is P<sub>k</sub>, the correct label is formed as the data having the probability of recommending change in P<sub>1 </sub>to P<sub>k&#x2212;1 </sub>set to 0, the probability of recommending change in P<sub>k </sub>set to 1, and the probability of recommending P<sub>k+1 </sub>to P<sub>N </sub>set to 0. The learning section <b>420</b> obtains the error function based on comparison between N probability data obtained through the arithmetic operation in the forward direction and N probability data each as the correct label.</p><p id="p-0107" num="0106">In step S<b>207</b>, the learning section <b>420</b> performs processing of updating the weighting coefficient to make the error function small. As described above, the error back propagation method or the like may be used for performing the processing in step S<b>207</b>. In step S<b>208</b>, the learning section <b>420</b> determines whether or not learning processing is to be finished. The learning processing may be finished based on the number of times of updating the weighting coefficient as described above, or the accuracy rate of the estimation processing using the verification data. If No is obtained in step S<b>208</b>, the process returns to step S<b>205</b> where the learning processing is continuously performed based on the subsequent training data. If Yes is obtained in step S<b>208</b>, the learning processing is finished. The learning device <b>400</b> transmits the information of the generated trained model to the processing system <b>100</b>.</p><p id="p-0108" num="0107">The processing performed by the learning device <b>400</b> of the present embodiment may be implemented as the learning method. The learning method of the present embodiment allows generation of the trained model by performing the processing of acquiring an image captured by the endoscope apparatus as the input image, acquiring the control information for obtaining the input image when using information for controlling the endoscope apparatus as control information, acquiring the control information for improving the estimated probability information which represents the probability of the region of interest detected from the input image, and performing machine learning of a relationship among the input image, the control information for obtaining the input image, and the control information for improving the estimated probability information. The control information for improving the estimated probability information herein may be the priority parameter type as described above, a combination of the priority parameter type and the specific parameter value as a modification to be described later, or a set of multiple parameter types and parameter values for the respective parameter types as further modification to be described later.</p><heading id="h-0009" level="1">2.2 Inference Processing</heading><p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating processing to be performed by the processing system <b>100</b> of the present embodiment. First, in step S<b>301</b>, the acquisition section <b>110</b> acquires a detection target image. For example, the acquisition section <b>110</b> may be configured to acquire the detection target image once every two frames as described later referring to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. The acquisition section <b>110</b> is capable of acquiring the control information used for acquiring the detection target image from the control section <b>130</b>.</p><p id="p-0110" num="0109">In step S<b>302</b>, the processing section <b>120</b> (detection processing section <b>334</b>) inputs the detection target image to the NN1 to obtain a detection result of the region of interest, and the estimated probability representing its probability through an arithmetic operation. In step S<b>303</b>, the processing section <b>120</b> determines whether or not the estimated probability is equal to or higher than a given threshold. If Yes is obtained in step S<b>303</b>, the detection result of the region of interest is considered as being sufficiently reliable. The process then proceeds to step S<b>304</b> where the processing section <b>120</b> outputs the detection result of region of interest. For example, the detection result of the region of interest derived from the detection processing section <b>334</b> is transmitted to the post-processing section <b>336</b> for post-processing. Thereafter, the result is displayed on the display section <b>340</b>. As described later referring to <figref idref="DRAWINGS">FIG. <b>13</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>13</b>B</figref>, the processing section <b>120</b> outputs the estimated probability together with the detection result of region of interest.</p><p id="p-0111" num="0110">If No is obtained in step S<b>303</b>, the detection result of region of interest is considered as having low reliability. Display of such result may be non-beneficial for user's diagnosis and the like. Meanwhile, omission of the display itself may fail to notify the user of possibility that the region of interest exists. For this reason, in the present embodiment, the control information is changed.</p><p id="p-0112" num="0111">If No is obtained in step S<b>303</b>, the process proceeds to step S<b>305</b> where the processing section <b>120</b> (control information identifying section <b>335</b>) performs processing of updating the control information. The processing section <b>120</b> inputs the detection target image acquired in step S<b>301</b>, and the control information to the NN2 so that the priority parameter type is identified.</p><p id="p-0113" num="0112">In step S<b>306</b>, the control section <b>130</b> (control section <b>332</b>) performs control for changing the parameter value of the priority parameter type. The acquisition section <b>110</b> acquires the detection target image based on the changed control information. In this case, the NN2 identifies the priority parameter type, but does not identify the specific parameter value. Accordingly, the control section <b>332</b> performs control for sequentially changing the parameter value of the priority parameter type. The acquisition section <b>410</b> acquires the detection target image group based on the parameter value group.</p><p id="p-0114" num="0113">In step S<b>307</b>, the processing section <b>120</b> inputs the respective detection target images of the detection target image group to the NN1 to obtain the detection result of the region of interest, and the estimated probability indicating probability of the result. The detection processing section <b>334</b> extracts the parameter value with the highest estimated probability, and the detection target image from the parameter value group and the detection target image group, respectively.</p><p id="p-0115" num="0114">In step S<b>308</b>, the detection processing section <b>334</b> determines whether or not the estimated probability of the region of interest in the extracted detection target image is equal to or higher than the given threshold. If Yes is obtained in step S<b>308</b>, the detection result of the region of interest is considered to be highly reliable. Accordingly, in step S<b>309</b>, the detection processing section <b>334</b> outputs the detection result of the region of interest.</p><p id="p-0116" num="0115">If No is obtained in step S<b>308</b>, the process proceeds to step S<b>310</b> where the control information identifying section <b>335</b> performs processing of updating the control information. The control information identifying section <b>335</b> inputs the detection target image extracted in step S<b>307</b>, and the control information used for acquiring the detection target image to the NN2 so that the priority parameter type is determined.</p><p id="p-0117" num="0116">The priority parameter type determined in step S<b>310</b> may be the parameter type P<sub>j </sub>(j is an integer that satisfies j&#x2260;i) which is different from the priority parameter type P<sub>1 </sub>determined in step S<b>305</b>. The processing in step S<b>310</b> may be performed in the case where the estimated probability is not sufficiently improved in spite of adjustment of the parameter type P<sub>i</sub>. The parameter type different from the parameter type P<sub>1 </sub>is set to the next priority parameter type to allow efficient improvement of the estimated probability. In step S<b>311</b>, the acquisition section <b>410</b> acquires the detection target image group based on the parameter value group in the control for sequentially changing the parameter value of the priority parameter type.</p><p id="p-0118" num="0117">In step S<b>310</b>, the processing result obtained in step S<b>305</b> may be utilized by omitting the processing using the NN2. For example, the control information identifying section <b>335</b> may be configured to select the parameter type determined in step S<b>305</b> that it should be recommended with the second priority as the priority parameter type.</p><p id="p-0119" num="0118">Referring to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the explanation has been made by developing the loop processing for clarifying changes in the detection target image, the control information upon acquisition of the detection target image, and the priority parameter type. As described with respect to the processing in steps S<b>301</b> to S<b>305</b>, the loop processing including such processing as acquisition of the detection target image and the control information, detection, determination of the estimated probability using the threshold, and identification of the control information is performed repeatedly until establishment of the condition that the estimated probability is equal to or higher than the threshold. In step S<b>311</b> onward, the similar loop is continuously performed. In the case where even continuously performing the loop fails to sufficiently improve the estimated probability, the processing section <b>120</b> finishes the processing when, for example, all the parameter types have been changed, or the loop processing has been performed the predetermined number of times.</p><p id="p-0120" num="0119">The processing which has been described referring to <figref idref="DRAWINGS">FIG. <b>11</b></figref> focuses on the single region of interest. It cannot be said that it is preferable to continuously perform the processing to the single detected region of interest the excessive number of times for the following reasons. That is, as the visual field of the imaging section varies depending on the operation condition of the insertion section <b>310</b><i>b</i>, it is presumed that the region of interest deviates from the detection target image, and a region of interest is newly detected in the detection target image. As described above, the maximum number of implementation of the loop processing is limited to allow smooth switching from the processing to the given region of interest to the processing to the different region of interest. Especially, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> and <figref idref="DRAWINGS">FIG. <b>15</b></figref> to be referred later, the number of the parameter types to be changed is small, which may need substantial time for improving the estimated probability. Accordingly, it is essential to suppress the number of loops.</p><p id="p-0121" num="0120">As described above, the processing section <b>120</b> of the processing system <b>100</b> is operated in accordance with the trained model to detect the region of interest included in the detection target image, and to calculate the estimated probability information related to the detected region of interest. The trained model herein corresponds to the NN1. The processing section <b>120</b> may be configured to be operated in accordance with the trained model so that the control information for improving the estimated probability is identified.</p><p id="p-0122" num="0121">The arithmetic operation performed by the processing section <b>120</b> in accordance with the trained model, that is, the one for outputting the output data based on the input data may be implemented either by software or hardware. In other words, software may be configured to perform a product-sum operation performed by each node in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, and the filter processing performed in the convolutional layer of the CNN. Alternatively, the arithmetic operation may be performed by the circuit device such as FPGA, or combination of software and hardware. Consequently, the operation of the processing section <b>120</b> in accordance with the instruction from the trained model may be performed in various forms. For example, the trained model includes an inference algorithm and the weighting coefficient used for the inference algorithm. The inference algorithm is used for filter arithmetic operation or the like based on the input data. In this case, the inference algorithm and the weighting coefficient both stored in the storage section may be retrieved by the processing section <b>120</b> to allow implementation of the software-based inference processing. The storage section is exemplified by the storage section <b>333</b> of the processing device <b>330</b>. However, other type of storage section may be used. Alternatively, the inference algorithm may be implemented by the FPGA or the like so that the weighting coefficient is stored in the storage section. The inference algorithm including the weighting coefficient may also be implemented by the FPGA or the like. In this case, the storage section for storing the trained model information is exemplified by an internal memory of the FPGA.</p><p id="p-0123" num="0122">The processing section <b>120</b> calculates first estimated probability information based on the first detection target image derived from the control using the first control information, and the trained model. If it is determined that probability represented by the first estimated probability information is lower than the given threshold, the processing section <b>120</b> performs processing of identifying the second control information as the one for improving the estimated probability information. The processing corresponds to those performed in steps S<b>302</b>, S<b>303</b>, and S<b>305</b> in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, for example.</p><p id="p-0124" num="0123">This makes it possible to attempt acquisition of more reliable information in the case of insufficient estimated probability, that is, low reliability of the detected region of interest.</p><p id="p-0125" num="0124">Provision of the information with low estimated probability to the user, which has been practiced in the generally employed method, may cause the risk of erroneous diagnosis by the user. Meanwhile, in the case where a high threshold of the estimated probability is set as a reference for determining whether such information is provided to the user, the user's erroneous diagnosis may be suppressed. However, the support information volume to be provided is reduced, leading to increase in lesion oversight errors. The method according to the present embodiment is capable of solving the above-described tradeoff problem. This makes it possible to attain highly accurate diagnosis by the user, and provision of information which suppresses the oversight error.</p><p id="p-0126" num="0125">The processing section <b>120</b> may be configured to calculate second estimated probability information based on the second detection target image derived from the control using the second control information, and the trained model. If it is determined that probability represented by the second estimated probability information is lower than the given threshold, the processing section <b>120</b> performs processing of identifying the third control information as the one for improving the estimated probability information. The processing corresponds to those performed in steps S<b>307</b>, S<b>308</b>, and S<b>310</b> in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, for example.</p><p id="p-0127" num="0126">This makes it possible to further change the control information in the case of insufficient estimated probability despite the change in the control information. It is therefore possible to improve probability of acquiring the highly reliable information, specifically, to search the appropriate control information broadly so that the estimated probability exceeds the given threshold.</p><p id="p-0128" num="0127">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the control information includes at least one of the light source control information for controlling the light source <b>352</b> which irradiates the object with illumination light, the imaging control information for controlling the imaging condition based on which the detection target image is captured, and the image processing control information for controlling the image processing to the signal of the captured image. As described above, the detection target image is acquired by performing the steps of allowing the light source <b>352</b> to emit light, allowing the image sensor <b>312</b> to receive light from the object, and processing the image signal as the light reception result. Performing the control related to any one of those steps allows change in the property of the detection target image to be acquired. This makes it possible to adjust the estimated probability. As described above, however, the control section <b>130</b> does not have to control all of the light source, imaging, and image processing for improving the estimated probability. It is possible to omit controlling one or two of those processing operations.</p><p id="p-0129" num="0128">The control information may be the information indicating at least one of properties including the color tone, brightness, and the position of the region of interest in the image. In other words, the light source <b>352</b> or the like may be controlled so that at least one of the color tone, brightness, and the position of the region of interest in the detection target image approximates the desired value. The estimated probability may be improved by, for example, making the color tone of the region of interest resemble the color tone of the input image included in the training data, in a narrow sense, making the color tone resemble the color tone of the part corresponding to the region of interest in the input image. Alternatively, the estimated probability may be improved by controlling the light source <b>352</b> so that brightness of the region of interest in the detection target image is made optimum for the region of interest.</p><p id="p-0130" num="0129">The control information may be configured to include the first to the Nth (N is an integer equal to or larger than 2) parameter types. The processing section <b>120</b> may be configured to identify the second control information by changing the parameter type to the ith (i is an integer which satisfies 1&#x2264;i&#x2264;N) parameter type among the first to the Nth parameter types included in the first control information. This makes it possible to change the parameter type to the one with higher degree of contribution to the estimated probability, and further to improve the estimated probability by performing the efficient control. As described later, such control is performed more easily than the control for simultaneously changing parameter values of multiple parameter types.</p><p id="p-0131" num="0130">The processing section <b>120</b> calculates the second estimated probability information based on the second captured image derived from the control using the second control information, and the trained model. The processing section <b>120</b> may be configured to identify the third control information by changing the parameter type to the jth (j is an integer which satisfies 1&#x2264;j&#x2264;N, j&#x2260;i) parameter type among the first to the Nth parameter types included in the second control information when it is determined that probability represented by the second estimated probability information is lower than the given threshold.</p><p id="p-0132" num="0131">This allows an attempt to change the parameter value of the different parameter type when the change in the parameter value of the given parameter type has failed to sufficiently improve the estimated probability. Accordingly, the probability of improving the estimated probability may be increased.</p><p id="p-0133" num="0132">The processing section <b>120</b> may be configured to perform processing of identifying the second control information as the one for improving the estimated probability information based on the first control information and the first detection target image. That is, when identifying the control information, it is possible to use not only the detection target image but also the control information for acquiring such detection target image. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>C</figref>, the NN2 as the trained model for identifying the control information receives an input of a set of the image and the control information.</p><p id="p-0134" num="0133">The foregoing configuration allows the processing to be performed in consideration not only of the detection target image but also the condition by which such detection target image has been acquired. The resultant processing accuracy may be further improved than the case of inputting only the detection target image. However, the processing of identifying the second control information does not necessarily require the use of the first control information. For example, the control information may be omitted from the training data as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>. In this case, the input operation for processing of identifying the control information becomes less frequent, and accordingly, the processing load may be reduced. As the training data volume is reduced, the learning processing load may also be reduced.</p><p id="p-0135" num="0134">The processing section <b>120</b> may be configured to perform processing of identifying the control information for improving the estimated probability information based on the second trained model and the detection target image. The second trained model is acquired by machine learning with respect to the relationship between the second input image and the control information for improving the estimated probability information. The second trained model corresponds to the NN2 as described above.</p><p id="p-0136" num="0135">It is therefore possible to utilize the machine learning for processing of identifying the control information as well as the detection processing. Accordingly, accuracy of identifying the control information is improved to increase the estimated probability to be equal to or higher than the threshold acceleratedly. However, as described later, the processing of identifying the control information may be modified as the processing without utilizing the machine learning.</p><p id="p-0137" num="0136">The processing to be performed by the processing system <b>100</b> of the present embodiment may be implemented as the image processing method. The image processing method of the present embodiment is implemented by acquiring the detection target image captured by the endoscope apparatus, detecting the region of interest included in the detection target image based on the trained model acquired by machine learning for calculating the estimated probability information representing probability of the region of interest in the input image, and the detection target image, calculating the estimated probability information related to the detected region of interest, and identifying the control information for improving the estimated probability information related to the region of interest in the detection target image based on the detection target image on the assumption that the information used for controlling the endoscope apparatus is set as the control information.</p><heading id="h-0010" level="1">2.3 Background Processing and Display Processing</heading><p id="p-0138" num="0137"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a relationship between the imaging frame and an image acquired in each frame. For example, the acquisition section <b>110</b> of the processing system <b>100</b> detects the region of interest and acquires the detection target image as the target for calculating the estimated probability information in the given frame. The acquisition section <b>110</b> acquires the display image used for display on the display section <b>340</b> in the frame different from the given frame.</p><p id="p-0139" num="0138">That is, the method according to the present embodiment may be configured to separate the detection target image as the target for processing of detection and control information identification performed by the processing system <b>100</b> from the display image to be displayed for the user. This makes it possible to manage the control information utilized for acquiring the detection target image and the display control information for acquiring the display image separately.</p><p id="p-0140" num="0139">As described above, the property of the image to be acquired is changed by modifying the control information. Frequent or rapid change in the control information of the display image makes the change in the image greater. This may cause the risk of interference with the user's observation and diagnosis. Separating the detection target image from the display image may suppress the rapid change in the control information for display image. That is, the display image property may be changed by the user himself. Even in the case of automatic change in the property, such change can be suppressed in a gradual manner. This may suppress interference with the user's diagnosis.</p><p id="p-0141" num="0140">It is possible not to display the detection target image so that the control information may be rapidly changed. Accordingly, the control for improving the estimated probability may be performed acceleratedly. In the case where light quantity of the LED as the light source <b>352</b> is adjusted in the generally employed automatic dimming control, the resultant light quantity change becomes gradual in a restricted manner compared with the light quantity change normally attained as the property of LED. Such control is performed for avoiding rapid brightness change which may cause the risk of interference with the user's diagnosis. The light quantity change of the detection target image hardly causes the problem despite the rapid change. The LED is allowed to fully exhibit its light quantity changing capability.</p><p id="p-0142" num="0141">The acquisition section <b>110</b> may be configured to acquire the detection target image and the display image alternately as illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>. This makes it possible to reduce the difference between the timing for acquiring the detection target image and the timing for acquiring the display image.</p><p id="p-0143" num="0142">The processing section <b>120</b> of the processing system <b>100</b> may be configured to display the detection result of the region of interest detected based on the detection target image, and the estimated probability information calculated based on the detection target image when the estimated probability information calculated based on the trained model (NN1) and the detection target image is equal to or higher than the given threshold.</p><p id="p-0144" num="0143"><figref idref="DRAWINGS">FIG. <b>13</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>13</b>B</figref> illustrate an example of the display image on the display section <b>340</b>. Referring to <figref idref="DRAWINGS">FIG. <b>13</b>A</figref>, a reference code A1 denotes a display image on the display section <b>340</b>. A reference code A2 denotes the region of interest captured as the display image. A reference code A3 denotes a detection frame detected on the detection target image. A reference code A4 denotes an estimated probability. As <figref idref="DRAWINGS">FIG. <b>13</b>A</figref> illustrates, the detection result of the region of interest and the estimated probability information are superposed on the display image and displayed to provide the user with the information related to the region of interest in the display image comprehensively.</p><p id="p-0145" num="0144">For example, the detection target image in this case has been acquired in the frame F<b>3</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, and the display image has been acquired in the frame F<b>4</b>. This makes it possible to reduce the difference in the timing of imaging between the detection target image and the display image. As the difference between the condition of the region of interest in the detection target image and the condition of the region of interest in the display image is reduced, the detection result based on the detection target image may be easily correlated with the display image. For example, the difference in the position/size of the region of interest between the detection target image and the display image is sufficiently reduced. The probability that the region of interest on the display image is included in the detection frame may be raised when superposing the detection frame. The display image and the detection target image herein are not limited to those images acquired in the consecutive frames. For example, depending on the time required for performing the detection processing, it is possible to superpose and display the detection result using the detection target image in the frame F<b>3</b> on the display image acquired in the timing behind the frame F<b>4</b>, for example, a not shown frame F<b>6</b> or F<b>8</b>.</p><p id="p-0146" num="0145">The processing section <b>120</b> may be configured to perform processing of displaying the region which includes at least the region of interest of the detection target image, and the display image, which are correlated with each other when the estimated probability information calculated based on the trained model NN1 and the detection target image is equal to or higher than the given threshold.</p><p id="p-0147" num="0146">For example, as <figref idref="DRAWINGS">FIG. <b>13</b>B</figref> illustrates, the processing section <b>120</b> may be configured to perform processing of displaying a part of the detection target image on the display section <b>340</b>. Referring to <figref idref="DRAWINGS">FIG. <b>13</b>B</figref>, a reference code B1 denotes a display region of the display section <b>340</b>. Similar to the reference codes A1 to A4 in <figref idref="DRAWINGS">FIG. <b>13</b>A</figref>, reference codes B2 to B5 denote the display image, the region of interest in the display image, the detection frame, and the estimated probability, respectively. The processing section <b>120</b> may be configured to display a part of the detection target image in a part of the display region, which is different from the one having the display image displayed thereon as indicated by B6.</p><p id="p-0148" num="0147">The detection target image having the region of interest with high estimated probability is regarded as the image which allows the user to easily identify the region of interest. Accordingly, display of a part of the detection target image which includes at least the region of interest enables to support the determination as to whether the region indicated by the detection frame is truly the region of interest. <figref idref="DRAWINGS">FIG. <b>13</b>B</figref> illustrates an example of partially displaying the detection target image. However, an entire detection target image may be displayed in a non-restrictive manner.</p><heading id="h-0011" level="1">3. Modification</heading><p id="p-0149" num="0148">Several modifications will be described hereinafter.</p><heading id="h-0012" level="1">3.1 Configuration of Trained Model</heading><p id="p-0150" num="0149">The explanation has been made with respect to an example that the NN1 as the trained model for detection processing, and the trained model NN2 for processing of identifying the control information are separated from each other. The NN1 and the NN2 may be implemented by a single trained model NN3.</p><p id="p-0151" num="0150">For example, the NN3 is a network which receives inputs of the detection target image and the control information, and outputs the detection result of the region of interest, the estimated probability information, and the priority parameter type. Various specific configurations of the NN3 are conceivable. For example, the NN3 may be configured as a model including a feature amount extraction layer sharedly used for the detection processing and processing of identifying the control information, a detection layer for performing the detection processing, and a control information identifying layer for performing processing of identifying the control information. The feature amount extraction layer receives inputs of the detection target image and the control information, and outputs the feature amount. The detection layer receives an input of the feature amount from the feature amount extraction layer, and outputs the detection result of the region of interest and the estimated probability information. The control information identifying layer receives an input of the feature amount from the feature amount extraction layer, and outputs the priority parameter type. For example, based on the training data illustrated in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, the weighting coefficient included in the feature amount extraction layer and the detection layer is learned. Based on the training data as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, the weighting coefficient included in the feature amount extraction layer and the control information identifying layer is learned. Depending on the training data format, the detection layer and the control information identifying layer may be targeted for learning simultaneously.</p><heading id="h-0013" level="1">3.2 Modified Example Related to Processing of Identifying Control Information</heading><heading id="h-0014" level="1">Modified Example 1 of NN2</heading><p id="p-0152" num="0151"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> illustrates another example of training data for generating a trained model used for the processing of identifying the control information. <figref idref="DRAWINGS">FIG. <b>14</b>B</figref> illustrates an example of data for generating the training data as illustrated in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>. <figref idref="DRAWINGS">FIG. <b>14</b>C</figref> illustrates an input/output operation of the NN2.</p><p id="p-0153" num="0152">As <figref idref="DRAWINGS">FIG. <b>14</b>A</figref> illustrates, the training data include the second input image, the control information used for acquiring the second input image, the priority parameter type, and a recommended value for the priority parameter type. The control information is the same as the one in the example illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, including the parameter values p<sub>1 </sub>to p<sub>N </sub>of the parameter types P<sub>1 </sub>to P<sub>N</sub>, respectively. The priority parameter type P<sub>k </sub>which is the same as the one illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> indicates the information for identifying any one of the parameter types. The recommended value p<sub>k</sub>&#x2032; is the value recommended for the parameter value of the parameter type P<sub>k</sub>.</p><p id="p-0154" num="0153">For example, in the training data acquisition stage, a given object which contains the region of interest is sequentially captured while changing the control information so that the image is acquired. In such a case, data related to multiple kinds of parameter values for a given parameter type are acquired. An explanation will be made with respect to an example that M candidates are set for each of the parameter values p<sub>1 </sub>to p<sub>N </sub>so that the description is simplified. The parameter value p<sub>1 </sub>is selectable from M kinds of candidate values of p<sub>1 </sub>to p<sub>1M</sub>. This applies to the parameter values p<sub>2 </sub>to p<sub>N</sub>. The number of candidates for the parameter value may be made different in accordance with the parameter type.</p><p id="p-0155" num="0154">It is assumed that initial value of the parameter value is set to p<sub>11</sub>, p<sub>21</sub>, . . . , p<sub>N1 </sub>as indicated by D1. As for data in the range indicated by D2, the parameter value of the parameter type P<sub>1 </sub>is only changed to p<sub>12 </sub>to p<sub>1M </sub>sequentially, and the parameter values of the parameter types P<sub>2 </sub>to P<sub>N </sub>are fixed. Similarly, in the range indicated by D3, the parameter value of the parameter type P<sub>2 </sub>is only changed to p<sub>22 </sub>to p<sub>2M </sub>sequentially, and parameter values of other parameter types are fixed. This applies to subsequent data hereinafter. In the example as illustrated in <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>, each parameter value of the N parameter types is changed in M&#x2212;1 ways from the initial state indicated by D1. Based on the N&#xd7;(M&#x2212;1) different control information data, N&#xd7;(M&#x2212;1) pieces of images are acquired. The acquisition section <b>410</b> obtains the estimated probability by inputting each of the N&#xd7;(M&#x2212;1) images to the NN1.</p><p id="p-0156" num="0155">It is assumed that the estimated probability is maximized as a result of changing the parameter value p<sub>k </sub>of the parameter type P<sub>k </sub>to p<sub>k</sub>&#x2032; as one of values from p<sub>k2 </sub>to p<sub>kM</sub>. In such a case, the acquisition section <b>410</b> sets the image indicated by D1 to the second input image, the parameter values p<sub>11 </sub>to p<sub>N1 </sub>indicated by D1 to the control information, the parameter type P<sub>k </sub>to the priority parameter type, and the value p<sub>k</sub>&#x2032; to the recommended value so that the single set of training data are acquired. The data for acquiring the training data are not limited to those illustrated in <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>, but may be variously modified. It is not necessary to acquire all the N&#xd7;(M&#x2212;1) additional data, for example. The modification may be configured to omit a part of those data in consideration of the processing load.</p><p id="p-0157" num="0156">As <figref idref="DRAWINGS">FIG. <b>14</b>C</figref> illustrates, the NN2 receives inputs of the second input image and the control information, and performs an arithmetic operation in the forward direction to output the priority parameter type and the recommended value.</p><p id="p-0158" num="0157">For example, the output layer of the NN2 may be configured to include N&#xd7;M nodes, and output N&#xd7;M output data. The N&#xd7;M output nodes include M nodes for outputting probability data required to set the P<sub>1 </sub>to p<sub>11</sub>, P<sub>1 </sub>to p<sub>12</sub>, . . . , and P<sub>1 </sub>to p<sub>1M</sub>, respectively. Similarly, the output nodes include M nodes for outputting probability data required to set P<sub>2 </sub>to p<sub>21</sub>, P<sub>2 </sub>to p<sub>22</sub>, . . . , and P<sub>2 </sub>to p<sub>2M</sub>, respectively. This applies to subsequent data hereinafter. The NN2 according to the present modification outputs recommended probability data with respect to all the combinations of the N kinds of priority parameter types and M kinds of parameter values. If the node which allows the probability data to be maximized is determined, the priority parameter type and the recommended value may be identified. The specific configuration of the NN2 is not limited to the one as described above, but may be configured in another way for allowing identification of the priority parameter type and the recommended value.</p><p id="p-0159" num="0158">The flow of NN2 learning processing is similar to the one as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. In step S<b>206</b> of the flow, the learning section <b>420</b> inputs the second input image and the control information of the training data to the NN2, and performs an arithmetic operation in the forward direction based on the corresponding weighting coefficient so that N&#xd7;M probability data are obtained. The learning section <b>420</b> then obtains the error function by comparing the N&#xd7;M probability data with the priority parameter type/recommended value of the training data. For example, in the case where the priority parameter type included in the training data is P<sub>1</sub>, and the recommended value is p<sub>11</sub>, the correct label informs that the probability data for setting the P<sub>1 </sub>to p<sub>11 </sub>becomes 1, and all other probability data become 0.</p><p id="p-0160" num="0159"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart illustrating processing to be performed by the processing system <b>100</b> of the present embodiment. Steps S<b>401</b> to S<b>404</b> correspond to steps S<b>301</b> to S<b>304</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>. That is, the processing section <b>120</b> obtains the estimated probability by inputting the detection target image to the NN1. If the estimated probability is equal to or higher than the threshold, the output processing is performed, otherwise the control information is changed.</p><p id="p-0161" num="0160">If No is obtained in step S<b>403</b>, the process proceeds to step S<b>405</b> where the processing section <b>120</b> (control information identifying section <b>335</b>) performs processing of updating the control information. The processing section <b>120</b> inputs the detection target image acquired in step S<b>401</b>, and the control information to the NN2 to determine the priority parameter. As described above referring to <figref idref="DRAWINGS">FIG. <b>14</b>C</figref>, in the present modification, the parameter value to be recommended may be determined as well as the priority parameter type.</p><p id="p-0162" num="0161">In step S<b>406</b>, the control section <b>130</b> (control section <b>332</b>) performs the control for changing the parameter value of the priority parameter type to the recommended value. The acquisition section <b>110</b> acquires a detection target image based on the determined control information. Unlike the example as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, as the recommended value is identified, the control for sequentially changing the parameter value of the priority parameter type does not have to be performed. This makes it possible to improve the estimated probability in a shorter time than the example in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. As processing flow from steps S<b>407</b> to S<b>411</b> will be performed with repetition in the similar manner to the one as described above, explanations of those steps will be omitted.</p><heading id="h-0015" level="1">Modification 2 of NN2</heading><p id="p-0163" num="0162"><figref idref="DRAWINGS">FIG. <b>16</b>A</figref> illustrates another example of the training data for generating the trained model used for processing of identifying the control information. <figref idref="DRAWINGS">FIG. <b>16</b>B</figref> illustrates an input/output operation of the NN2.</p><p id="p-0164" num="0163">As illustrated in <figref idref="DRAWINGS">FIG. <b>16</b>A</figref>, the training data include the second input image, the control information used for acquiring the second input image, and the recommended control information. The control information contains the parameter values p<sub>1 </sub>to p<sub>N </sub>of the parameter types P<sub>1 </sub>to P<sub>N</sub>, respectively similar to the example as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>. The recommended control information contains parameter values p<sub>1</sub>&#x2032; to p<sub>N</sub>&#x2032; of the parameter types P<sub>1 </sub>to P<sub>N</sub>, respectively, which are recommended for improving the estimated probability.</p><p id="p-0165" num="0164">For example, in the training data acquisition stage, a given object which contains the region of interest is continuously captured while changing the control information so that the image is acquired. In such a case, parameter values are changeable in M&#x2212;1 ways for each of N kinds of parameter types. Accordingly, (M&#x2212;1)<sup>N </sup>data as a result of all combinations are acquired. The acquisition section <b>410</b> sets the control information which maximizes the estimated probability to the recommended control information to acquire a single set of training data. Depending on values of N and M, a large number of images have to be captured for acquiring the training data. The collection target data may be limited to a part of the (M&#x2212;<sub>1</sub>)<sup>N </sup>data. The data to be collected for acquiring the training data as illustrated in <figref idref="DRAWINGS">FIG. <b>16</b>A</figref> may be variously modified.</p><p id="p-0166" num="0165">As illustrated in <figref idref="DRAWINGS">FIG. <b>16</b>B</figref>, the NN2 receives inputs of the second input image and the control information, and performs an arithmetic operation in the forward direction to output the recommended value for each parameter value of the respective parameter types. For example, the output layer of the NN2 includes N nodes, and outputs N output data.</p><p id="p-0167" num="0166">The flow of NN2 learning processing is similar to the one as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. In step S<b>206</b> of the flow, the learning section <b>420</b> inputs the second input image and the control information of the training data to the NN2, and performs an arithmetic operation in the forward direction based on the corresponding weighting coefficient so that N recommended values are obtained. The learning section <b>420</b> then obtains the error function by comparing the N recommended values with N parameter values included in the recommended control information of the training data.</p><p id="p-0168" num="0167"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart illustrating processing to be performed by the processing system <b>100</b> of the present embodiment. Steps S<b>501</b> to S<b>504</b> correspond to steps S<b>301</b> to S<b>304</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>. That is, the processing section <b>120</b> obtains the estimated probability by inputting the detection target image to the NN1. If the estimated probability is equal to or higher than the threshold, the display processing is performed, otherwise the control information is changed.</p><p id="p-0169" num="0168">If No is obtained in step S<b>503</b>, the process proceeds to step S<b>505</b> where the processing section <b>120</b> (control information identifying section <b>335</b>) performs processing of updating the control information. The processing section <b>120</b> inputs the detection target image acquired in step S<b>501</b>, and the control information to the NN2 to determine the recommended control information. As described above, the control information herein is a set of recommended parameter values of the respective parameter types.</p><p id="p-0170" num="0169">In step S<b>506</b>, the control section <b>130</b> (control section <b>332</b>) performs the control for changing the parameter values of multiple parameter types to the recommended values. The acquisition section <b>110</b> acquires a detection target image based on the determined control information. Unlike the examples as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> and <figref idref="DRAWINGS">FIG. <b>15</b></figref>, multiple parameter types may be changed simultaneously. As illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, in the present modification, the processing may not be finished upon implementation of the output processing (steps S<b>504</b>, S<b>509</b>), and the similar loop processing may be continuously performed.</p><p id="p-0171" num="0170">As described above referring to <figref idref="DRAWINGS">FIGS. <b>16</b>A and <b>16</b>B</figref>, and <figref idref="DRAWINGS">FIG. <b>17</b></figref>, the control information contains the first to Nth (N is an integer equal to or larger than 2) parameter types. The processing section <b>120</b> may be configured to change two or more parameter types of the first to Nth parameter types which are included in the first control information so that the second control information is identified. Compared with the examples of <figref idref="DRAWINGS">FIG. <b>11</b></figref> and <figref idref="DRAWINGS">FIG. <b>15</b></figref>, collective change in the parameter values of multiple parameter types in this way makes it possible to further improve the estimated probability in a short time.</p><p id="p-0172" num="0000">&#x3c;Modification without Utilizing Machine Learning&#x3e;</p><p id="p-0173" num="0171">An example has been described above with respect to application of machine learning to processing of identifying the control information. However, application of machine learning is not essential. For example, the processing section <b>120</b> may be configured to determine a priority order of multiple parameter types included in the control information, and change the control information according to the priority order. For example, in the processing flow as illustrated in the flowchart of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, processing of changing the parameter value of the parameter type with the highest priority order is performed by omitting the processing in step S<b>305</b>.</p><p id="p-0174" num="0172">Alternatively, the processing system <b>100</b> may be configured to store database in which the current image and control information are correlated with the priority parameter type for improving the estimated probability. The processing section <b>120</b> performs processing of obtaining similarity between the detection target image/control information obtained upon acquisition of the detection target image, and the image/control information stored in the database. The processing section <b>120</b> performs processing of identifying the control information by changing the priority parameter type correlated with the data with the highest similarity.</p><heading id="h-0016" level="2">3.3 Operation Information</heading><p id="p-0175" num="0173">The acquisition section <b>110</b> of the processing system <b>100</b> may be configured to acquire operation information based on a user's operation of the endoscope apparatus. The processing section <b>120</b> identifies the control information for improving the estimated probability information based on the detection target image and the operation information. In other words, the operation information may be added as an input for the processing of identifying the control information.</p><p id="p-0176" num="0174">For example, user's operations of depressing a magnification button, or bringing the leading end of the insertion section <b>310</b><i>b </i>close to the object are regarded as being intentionally performed by the user who desires to thoroughly examine the object. Supposedly, the user desires to obtain, for example, support information for supporting classification and discrimination of polyp rather than the one for merely informing existence/non-existence of the polyp. In such a case, the processing section <b>120</b> changes the illumination light to NBI, for example. This makes it possible to improve the estimated probability of the detection result of the region of interest, and in a narrow sense, to improve the estimated accuracy of the detection result including the classification result of the region of interest. That is, in response to a predetermined operation by the user, the processing section <b>120</b> identifies the control information based on the user's operation to allow implementation of the control reflecting the user's intention. User's operations such as magnification and approximation may cause the imaging section to confront the object. Accordingly, it is considered that the estimated probability may be improved by controlling light distribution as the control information. It is possible to variously modify the specific parameter types and the parameter values of the control information to be identified based on the operation information.</p><p id="p-0177" num="0175">For example, the storage section of the processing system <b>100</b> may be configured to store the trained model for processing of identifying the control information, specifically, the trained model NN2_1 upon implementation of user's operation, and the trained model NN2_2 upon non-implementation of user's operation. Based on the user's operation, the processing section <b>120</b> switches the trained model used for the processing of identifying the control information. For example, when detecting depression of the magnification button, the zoom button, or the like, the processing section <b>120</b> determines that the predetermined operation by the user has been performed, and switches the trained model. Alternatively, based on the illumination light quantity and the image brightness, the processing section <b>120</b> determines that the user's operation for bringing the insertion section <b>310</b><i>b </i>close to the object has been performed. For example, in the case of bright image despite small illumination light quantity, it may be determined that the leading end of the insertion section <b>310</b><i>b </i>has been brought close to the object.</p><p id="p-0178" num="0176">For example, the NN2_1 is learned so that the parameter value of the NBI is likely to be selected as the parameter value related to the wavelength of the light source control information. This facilitates the detailed observation of the object using the NBI upon implementation of the user's operation. For example, the classification result in accordance with the NBI classification criteria is output as the detection result of the region of interest. For example, VS classification as a criterion for gastric lesion classification, or JNET, NICE classification, EC classification each as a criterion for colorectal lesion classification may be used for the NBI classification criteria.</p><p id="p-0179" num="0177">Meanwhile, the NN2_2 is learned so that the parameter value of the normal light is likely to be selected as the parameter value related to the wavelength of the light source control information, for example. Alternatively, the NN2_2 may be formed as a model learned to raise the probability for identifying the control information which allows the light source device <b>350</b> to emit amber light and purple light. The amber light has a peak wavelength in the wavelength range from 586 nm to 615 nm. The purple light has a peak wavelength in the wavelength range from 400 nm to 440 nm. The above-described light of both types is narrowband light having half bandwidth of several tens of nm, for example. The purple light is suitable for acquiring the characteristic of a mucous membrane surface blood vessel or the gland duct structure. The amber light is suitable for acquiring the characteristic of a deep blood vessel of membrane, redding, inflammation, or the like. Emission of the amber light and the purple light allows detection of the region of interest, for example, the lesion detectable based on the characteristic of mucous membrane surface blood vessel or gland duct structure, or the lesion detectable based on the characteristic of mucous membrane deep blood vessel, redding, inflammation or the like. The purple light and the amber light are likely to be used upon non-implementation of the user's operation. This makes it possible to improve the estimated probability with respect to broadly ranged lesion such as cancer and inflammatory disease.</p><p id="p-0180" num="0178">The configuration which differentiates the control information that is identified based on the operation information is not limited to the one for switching the trained model used for the identifying processing. For example, the NN2 is the model sharedly used upon implementation and non-implementation of the user's operation. Accordingly, the operation information may be used as an input to the NN2.</p><heading id="h-0017" level="2">3.4 Switching of Trained Model for Detection Processing</heading><p id="p-0181" num="0179">The processing section <b>120</b> may be configured to perform the first processing for detecting the region of interest included in the detection target image based on the first trained model and the detection target image, and the second processing for detecting the region of interest included in the detection target image based on the second trained model and the detection target image. In other words, multiple trained models NN1 for detection processing may be provided. The first trained model is referred to as NN1_1, and the second trained model is referred to as NN1_2 hereinafter.</p><p id="p-0182" num="0180">The processing section <b>120</b> selects one of the multiple trained models including the first trained model NN1_1 and the second trained model NN1_2. Upon selection of the first trained model NN1_1, the processing section <b>120</b> performs the first processing based on the first trained model NN1_1 and the detection target image, and calculates the estimated probability information to perform processing of identifying the control information for improving the calculated estimated probability information. Upon selection of the second trained model NN1_2, the processing section <b>120</b> performs the second processing based on the second trained model NN1_2 and the detection target image, and calculates the estimated probability information to perform processing of identifying the control information for improving the calculated estimated probability information.</p><p id="p-0183" num="0181">As described above, the trained model for the detection processing may be switched in accordance with circumstances. The processing of identifying the control information for improving the estimated probability information as the output of the first trained model may be the same as or different from the processing of identifying the control information for improving the estimated probability information as the output of the second trained model. For example, as described above, it is possible to use multiple trained models for the processing of identifying the control information. Explanations will be made below with respect to specific examples for switching the trained model.</p><p id="p-0184" num="0182">For example, the processing section <b>120</b> may be configured to be operable in any one of multiple determination modes including an existence determination mode for determining existence/non-existence of the region of interest included in the detection target image based on the first trained model NN1_1 and the detection target image, and a qualitative determination mode for determining a state of the region of interest included in the detection target image based on the second trained model NN1_2 and the detection target image. This makes it possible to perform processing in which either determination of existence/non-existence of the region of interest or determination of the state of the region of interest is prioritized. For example, the processing section <b>120</b> switches the trained model to be used for detection processing depending on whether all lesions have been found, the stage of the found lesion is required to be accurately classified, or the like.</p><p id="p-0185" num="0183">Specifically, the processing section <b>120</b> may be configured to determine whether or not the determination mode is shifted to the qualitative determination mode based on the detection result of the region of interest in the existence determination mode. For example, the processing section <b>120</b> determines to shift the determination mode to the qualitative determination mode in the case where the detected region of interest is large in size, the position of the region of interest is close to the center of the detection target image, the estimated probability is equal to or higher than the predetermined threshold, or the like in the existence determination mode.</p><p id="p-0186" num="0184">If it is determined that the determination mode is shifted to the qualitative determination mode, the processing section <b>120</b> calculates the estimated probability information based on the second trained model NN1_2 and the detection target image, and identifies the control information for improving the calculated estimated probability information. In this case, the trained model adapted to the circumstance is selected, and accordingly, the detection result desired by the user may be obtained. Furthermore, identifying the control information allows the detection result to be highly reliable.</p><p id="p-0187" num="0185">For example, the NN1_1 and NN1_2 are trained models each derived from learning using training data with different properties. For example, the first trained model NN1_1 is learned based on the training data in which a first learning image is correlated with the information for identifying existence/non-existence or position of the region of interest in the first learning image. The second trained model NN1_2 is learned based on the training data in which a second learning image is correlated with the information for identifying the state of the region of interest in the second learning image.</p><p id="p-0188" num="0186">The correct labels of the training data are thus differentiated from one another to allow discrimination of characteristics between the NN1_1 and the NN1_2. It is therefore possible to generate, as the NN1_1, the model exclusive to the processing of determining whether or not the region of interest exists in the detection target image. It is also possible to generate, as the NN1_2, the model exclusive to the processing of determining the state of the region of interest of the detection target image, for example, determining the corresponding lesion classification criteria as described above.</p><p id="p-0189" num="0187">The first learning image may be the image captured using white light. The second learning image may be the image captured using special light with wavelength band different from that of the white light, or the image captured in the state where the object is magnified in comparison with the first learning image. Among the training data, information to be input may be differentiated to allow discrimination of characteristics between the NN1_1 and the NN1_2. In consideration of such a case, upon transition from the existence determination mode to the qualitative determination mode, the processing section <b>120</b> may be configured to change the light source control information of the control information. Specifically, the processing section <b>120</b> performs the control for emitting the normal light as illumination light in the existence determination mode, and emitting the special light as illumination light in the qualitative determination mode. This makes it possible to link the operation for switching the trained model with change in the control information.</p><p id="p-0190" num="0188">A trigger for the switching operation of the trained model is not limited to the result of detecting the region of interest in the existence determination mode. For example, the acquisition section <b>110</b> may be configured to acquire operation information based on the user's operation to the endoscope apparatus as described above, and the processing section <b>120</b> may be configured to perform process of selecting the trained model based on the operation information. This also makes it possible to select the appropriate trained model depending on the observation desired by the user.</p><p id="p-0191" num="0189">An aspect based on which the trained model for detection processing is switched is not limited to the existence determination/qualitative determination. For example, the processing section <b>120</b> may be configured to select the first trained model if the imaging target is the first imaging target and to select the second trained model if the imaging target is the second imaging target, as a result of implementation of the process of identifying the imaging target captured as the detection target image. The imaging target herein is an internal organ to be captured, for example. For example, the processing section <b>120</b> selects the first trained model if a large intestine is captured, and selects the second trained model if a stomach is captured. The imaging target may be a fragmented part of the single internal organ. For example, the trained model may be selected in accordance with any one of the ascending colon, transverse colon, descending colon, and sigmoid colon. The imaging target may be discriminated by the classification other than the internal organ.</p><p id="p-0192" num="0190">The detection processing is thus performed using the trained model which varies depending on the imaging target to allow improvement of the detection processing accuracy, that is, improvement of estimated probability. The desired estimated probability thus can be easily attained by the processing as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> and the like.</p><p id="p-0193" num="0191">In the foregoing case, the first trained model is learned based on the training data in which the first learning image derived from capturing the first imaging target is correlated with the information for identifying the region of interest of the first learning image. The second trained model is learned based on the training data in which the second learning image derived from capturing the second imaging target is correlated with the information for identifying the region of interest of the second learning image. This makes it possible to generate the trained model exclusive to detection of the region of interest of each imaging target.</p><heading id="h-0018" level="2">3.5 Request for Changing Position/Posture of Imaging Section</heading><p id="p-0194" num="0192">The explanation has been made above with respect to an example of performing the processing of identifying the control information for improving the estimated probability. However, the processing which is different from the identifying processing may be performed for improving the estimated probability.</p><p id="p-0195" num="0193">For example, the processing section <b>120</b> may be configured to perform notifying processing which requests the user to change at least one of a position and a posture of the imaging section of the endoscope apparatus with respect to the region of interest when the probability represented by the estimated probability information is determined to be lower than the given threshold. The imaging section herein corresponds to the image sensor <b>312</b>, for example. Change in the position/posture of the imaging section corresponds to change in the position/posture of the leading end of the insertion section <b>310</b><i>b. </i></p><p id="p-0196" num="0194">For example, it is assumed that the image sensor <b>312</b> captures the region of interest from a diagonal direction. Capturing from the diagonal direction represents, for example, the state where the difference between the optical axis direction of the objective optical system <b>311</b> and the normal direction of an object plane is equal to or larger than the predetermined threshold. In this case, the region of interest in the detection target image is deformed to reduce its size on the image, for example. In this case, the resolution of the region of interest is low, which may cause the risk of failing to attain sufficient improvement of the estimated probability despite change in the control information. For this reason, the processing section <b>120</b> instructs the user to make the imaging section and the object confront with each other. The instruction may be displayed on the display section <b>340</b>, for example. A guide related to the moving direction and moving amount of the imaging section may be displayed. As a result, the resolution of the region of interest in the image may be raised to attain improvement of the estimated probability.</p><p id="p-0197" num="0195">In the case of a long distance between the region of interest and the imaging section, the region of interest on the image has its size reduced and its color darkened. Similarly, in this case, a mere adjustment of the control information may cause the risk of failing to attain sufficient improvement of the estimated probability. Accordingly, the processing section <b>120</b> instructs the user to bring the imaging section close to the object.</p><p id="p-0198" num="0196">In the circumstance as described above, the processing section <b>120</b> may be configured to perform notifying processing which requires to change the position/posture of the imaging section in response to a trigger that the estimated probability has failed to be equal to or higher than the threshold despite change in the control information. As a result, the control information is changed preferentially, so that the operation load to the user may be suppressed in the condition satisfied by the change. By enabling the request for changing the position/posture, the estimated probability may be improved even in the condition which cannot be satisfied by changing the control information.</p><p id="p-0199" num="0197">The trigger of the notifying processing for requiring change in the position/posture is not limited to the one as described above. For example, the NN2 described above may be configured as the trained model to output the information which identifies the control information, and further the information which identifies whether change in the position/posture is requested.</p><heading id="h-0019" level="2">3.6 Display Processing or Storage Processing</heading><p id="p-0200" num="0198">Under the condition that control information is sequentially changed to the first control information, the second control information, and the like, it is assumed that the estimated probability is lower than the threshold when using the first control information.</p><p id="p-0201" num="0199">The processing section <b>120</b> may be configured to display the detection result of the region of interest based on the first detection target image, and skip the display of the first estimated probability information. The processing section <b>120</b> may be configured to calculate the second estimated probability information based on the second detection target image acquired in the control using the second control information, and the NN1. The processing section may further be configured to perform processing of displaying the detection result of the region of interest based on the second detection target image, and the second estimated probability information if the second estimated probability information is equal to or higher than the given threshold.</p><p id="p-0202" num="0200">If the estimated probability is lower than the threshold when using the first control information and the second control information, but becomes equal to or higher than the threshold when using the third control information, the processing section <b>120</b> displays the detection result of the region of interest based on the second detection target image, and skips the display of the second estimated probability information. The processing section <b>120</b> may be configured to perform processing of displaying the detection result of the region of interest in the third detection target image acquired in the control using the third control information, and the third estimated probability information calculated based on the third detection target image.</p><p id="p-0203" num="0201">In the present embodiment, the above-described method allows improvement of the estimated probability despite the low estimated probability in the given timing. For example, if the estimated probability is lower than the threshold, such estimated probability may become equal to or higher than the threshold in the future by changing the control information. The present embodiment allows display of the detection result of the region of interest even if the estimated probability is lower than the threshold. This makes it possible to suppress oversight error of the region of interest. As the loop processing for determining the control information is performed repeatedly, the estimated probability value changes over time. The user is supposed to make sure whether or not the displayed region of interest is sufficiently reliable, and less interested in the time-series change in the estimated probability. Accordingly, the processing section <b>120</b> displays, for example, only the detection frame with no estimated probability in the updating loop processing of the control information, and displays the estimated probability together with the detection frame when the estimated probability is equal to or higher than the threshold. This makes it possible to attain the display comprehensible to the user.</p><p id="p-0204" num="0202">The method according to the present embodiment allows improvement of the estimated probability, whereas in the case where the original estimated probability is excessively low, the method may fail to make the estimated probability equal to or higher than the threshold despite change in the control information. Accordingly, the processing section <b>120</b> may be configured to set a second threshold smaller than the threshold. The processing section <b>120</b> displays only the detection frame if the estimated probability is equal to or higher than the second threshold, and lower than the threshold. If the estimated probability is equal to or higher than the threshold, the processing section <b>120</b> displays the detection frame and the estimated probability.</p><p id="p-0205" num="0203">The explanation has been made above with respect to the example for displaying the detection result of the region of interest and the display image, which are correlated with each other. The processing to be performed if the estimated probability is equal to or higher than the threshold is not limited to the display processing as described above.</p><p id="p-0206" num="0204">For example, if the estimated probability information calculated based on the trained model NN1 and the detection target image is equal to or higher than the given threshold, the processing section <b>120</b> may be configured to store the detection target image. This makes it possible to accumulate the image considered to contain the region of interest with high visibility. In the normal observation using the endoscope system <b>300</b>, the still image is not stored unless the user depresses the shutter button with clear intention. In the foregoing circumstance, there may be the case where the image with captured region of interest such as lesion cannot be stored. The use of the method for storing the movie may cause the heavy load of retrieving the region of interest from a large number of images. Meanwhile, as the storage processing is performed on the condition that the estimated probability is equal to or higher than the threshold, the information related to the region of interest may be appropriately stored.</p><p id="p-0207" num="0205">If the estimated probability information calculated based on the trained model NN1 and the detection target image is equal to or higher than the given threshold, the processing section <b>120</b> may be configured to store the displayed image corresponding to the detection target image. As a result, the image viewed by the user himself may also be the target of the storage processing.</p><p id="p-0208" num="0206">Although the embodiments to which the present disclosure is applied and the modifications thereof have been described in detail above, the present disclosure is not limited to the embodiments and the modifications thereof, and various modifications and variations in components may be made in implementation without departing from the spirit and scope of the present disclosure. The plurality of elements disclosed in the embodiments and the modifications described above may be combined as appropriate to implement the present disclosure in various ways. For example, some of all the elements described in the embodiments and the modifications may be deleted. Furthermore, elements in different embodiments and modifications may be combined as appropriate. Thus, various modifications and applications can be made without departing from the spirit and scope of the present disclosure. Any term cited with a different term having a broader meaning or the same meaning at least once in the specification and the drawings can be replaced by the different term in any place in the specification and the drawings.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A processing system comprising a processor including hardware, wherein<claim-text>the processor is configured to perform processing of:<claim-text>acquiring a detection target image captured by an endoscope apparatus;</claim-text><claim-text>controlling the endoscope apparatus based on control information;</claim-text><claim-text>detecting a region of interest included in the detection target image based on the detection target image for calculating estimated probability information representing a probability of the detected region of interest;</claim-text><claim-text>identifying the control information for improving the estimated probability information related to the region of interest within the detection target image based on the detection target image; and</claim-text><claim-text>controlling the endoscope apparatus based on the identified control information.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The processing system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the processor performs processing of:<claim-text>calculating first estimated probability information based on a first detection target image acquired based on control using first control information, and a trained model acquired by machine learning for calculating the estimated probability information; and</claim-text><claim-text>in a case where determination is made that the probability represented by the first estimated probability information is lower than a given threshold, identifying second control information which is the control information for improving the estimated probability information.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The processing system as defined in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the processor performs processing of:<claim-text>calculating second estimated probability information based on a second detection target image acquired based on control using the second control information and the trained model; and</claim-text><claim-text>in a case where determination is made that the probability represented by the second estimated probability information is lower than the given threshold, identifying third control information which is the control information for improving the estimated probability information.</claim-text></claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The processing system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the control information includes at least one of light source control information for controlling a light source which irradiates an object with illumination light, imaging control information for controlling an imaging condition for capturing the detection target image, and image processing control information for controlling image processing to a signal of the captured image.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The processing system as defined in <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the control information is information representing at least one of color tone, brightness, and a position in a display, which are related to the region of interest.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The processing system as defined in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the control information includes first to Nth (N is an integer equal to or larger than two) parameter types; and</claim-text><claim-text>the processor performs processing of identifying the second control information by changing two or more parameter types of the first to the Nth parameter types, which are included in the first control information.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The processing system as defined in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the control information includes first to Nth (N is an integer equal to or larger than two) parameter types; and</claim-text><claim-text>the processor performs processing of identifying the second control information by changing the ith (i is an integer which satisfies 1&#x2264;i&#x2264;N) parameter type of the first to the Nth parameter types, which are included in the first control information.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The processing system as defined in <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the processor performs processing of:<claim-text>calculating second estimated probability information based on a second detection target image acquired based on control using the second control information and the trained model; and</claim-text><claim-text>in a case where determination is made that the probability represented by the second estimated probability information is lower than the given threshold, identifying third control information by changing the jth (j is an integer which satisfies 1&#x2264;j&#x2264;N, j&#x2260;i) parameter type of the first to the Nth parameter types included in the second control information.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The processing system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the processor performs processing of:<claim-text>calculating the estimated probability information based on a first detection target image acquired based on control using first control information, and a trained model acquired by machine learning for calculating the estimated probability information; and</claim-text><claim-text>identifying second control information which is the control information for improving the estimated probability information based on the first control information and the first detection target image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The processing system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the processor is operatable to perform:<claim-text>first processing for detecting the region of interest included in the detection target image based on a first trained model acquired by machine learning for calculating the estimated probability information, and the detection target image; and</claim-text><claim-text>second processing for detecting the region of interest included in the detection target image based on a second trained model acquired by machine learning for calculating the estimated probability information, and the detection target image;</claim-text></claim-text><claim-text>the processor:<claim-text>performs processing of selecting one of multiple trained models including the first trained model and the second trained model;</claim-text><claim-text>in a case where the first trained model is selected, performs the first processing, and processing of calculating the estimated probability information based on the first trained model and the detection target image to identify the control information for improving the calculated estimated probability information; and</claim-text><claim-text>in a case where the second trained model is selected, performs the second processing and processing of calculating the estimated probability information based on the second trained model and the detection target image to identify the control information for improving the calculated estimated probability information.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The processing system as defined in <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>the processor performs processing of:<claim-text>acquiring operation information based on a user's operation to the endoscope apparatus; and</claim-text><claim-text>selecting the trained model based on the operation information.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The processing system as defined in <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>the processor is operatable in one of multiple determination modes including<claim-text>an existence determination mode for determining existence/non-existence of the region of interest included in the detection target image based on the first trained model and the detection target image; and</claim-text><claim-text>a qualitative determination mode for determining a state of the region of interest included in the detection target image based on the second trained model and the detection target image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The processing system as defined in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein<claim-text>the processor performs processing of:<claim-text>determining whether or not the determination mode is shifted to the qualitative determination mode based on a detection result of the region of interest in the existence determination mode; and</claim-text><claim-text>in a case where determination is made to shift the determination mode to the qualitative determination mode, calculating the estimated probability information based on the second trained model and the detection target image to identify the control information for improving the calculated estimated probability information.</claim-text></claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The processing system as defined in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein<claim-text>a first learning image used for learning the first trained model is an image captured using white light; and</claim-text><claim-text>a second learning image used for learning the second trained model is either an image captured using special light with a wavelength band different from that of the white light, or an image captured having an object magnified relative to the first learning image.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The processing system as defined in <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>the processor performs processing of:<claim-text>identifying an imaging target which has been captured to form the detection target image;</claim-text><claim-text>in a case where the imaging target is a first imaging target, selecting the first trained model; and</claim-text><claim-text>in a case where the imaging target is a second imaging target, selecting the second trained model.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The processing system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the processor performs processing of:<claim-text>acquiring the detection target image subjected to detection of the region of interest and calculation of the estimated probability information in a given frame; and</claim-text><claim-text>acquiring a display image to be displayed in a frame different from the given frame.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The processing system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in a case where the estimated probability information calculated based on a trained model acquired by machine learning for calculating the estimated probability information, and the detection target image is equal to or higher than a given threshold, the processor performs processing of displaying a detection result of the region of interest and the estimated probability information.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The processing system as defined in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the processor performs processing of:<claim-text>displaying a detection result of the region of interest based on the first detection target image, and skipping a display of the first estimated probability information;</claim-text><claim-text>calculating second estimated probability information based on a second detection target image acquired based on control using the second control information and the trained model; and</claim-text><claim-text>in a case where the second estimated probability information is equal to or higher than a given threshold, displaying a detection result of the region of interest based on the second detection target image, and the second estimated probability information.</claim-text></claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The processing system as defined in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein in a case where the estimated probability information calculated based on a trained model acquired by machine learning for calculating the estimated probability information, and the detection target image is equal to or higher than a given threshold, the processor performs processing of displaying a region of the detection target image, which includes at least the region of interest in correlation with the display image.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The processing system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in a case where determination is made that the probability represented by the estimated probability information is lower than a given threshold, the processor performs processing of notifying a user of request to change at least one of a position and a posture of an imaging device of the endoscope apparatus to the region of interest.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. An image processing method comprising:<claim-text>acquiring a detection target image captured by an endoscope apparatus;</claim-text><claim-text>detecting a region of interest included in the detection target image to calculate estimated probability information representing a probability of the detected region of interest based on the detection target image; and</claim-text><claim-text>when using information for controlling the endoscope apparatus as control information, identifying the control information for improving the estimated probability information related to the region of interest within the detection target image based on the detection target image.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. A learning method for generating a trained model, comprising:<claim-text>acquiring an image captured by an endoscope apparatus as an input image;</claim-text><claim-text>when using information for controlling the endoscope apparatus as control information, acquiring first control information as the control information for acquiring the input image;</claim-text><claim-text>acquiring second control information as the control information for improving estimated probability information which represents a probability of the region of interest detected from the input image; and</claim-text><claim-text>generating a trained model by performing machine learning of a relationship among the input image, the first control information, and the second control information.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. A processing device comprising a processor including hardware, wherein<claim-text>the processor is configured to perform processing of:<claim-text>acquiring a detection target image captured by an endoscope apparatus;</claim-text><claim-text>detecting a region of interest included in the detection target image to calculate estimated probability information which represents a probability of the detected region of interest based on the detection target image;</claim-text><claim-text>identifying the control information for improving the estimated probability information related to the region of interest within the detection target image based on the detection target image; and</claim-text><claim-text>outputting the identified control information.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>