<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007420A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007420</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17757063</doc-number><date>20201127</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>GB</country><doc-number>1918010.8</doc-number><date>20191209</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>30</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>2420</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">ACOUSTIC MEASUREMENT</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>University of York</orgname><address><city>York, North Yorkshire</city><country>GB</country></address></addressbook><residence><country>GB</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kearney</last-name><first-name>Gavin</first-name><address><city>York, Yorkshire</city><country>GB</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Armstrong</last-name><first-name>Calum</first-name><address><city>Yorshire</city><country>US</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/GB2020/053043</doc-number><date>20201127</date></document-id><us-371c12-date><date>20220608</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for determining subject specific digital audio data can comprise providing at least one respective audio signal input to each of a plurality of loudspeaker elements supported in a predetermined spatial relationship, in which respective locations of an effective point source of each loudspeaker element all lie in an imaginary surface that at least partially contains a spatial region where at least one aural cavity of a subject is located, thereby providing a distance between each respective location and each aural cavity of less than 1.5 meters. Responsive to at least one audio signal output from at least one of the loudspeaker elements, via at least one microphone element located at or within an aural cavity of the subject, respective subject specific audio data output is provided and is processed via an audio processing system, the subject specific audio data output, thereby providing subject specific digital audio data.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="194.14mm" wi="134.28mm" file="US20230007420A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="217.17mm" wi="136.31mm" file="US20230007420A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="113.79mm" wi="147.40mm" file="US20230007420A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="108.80mm" wi="151.47mm" file="US20230007420A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="132.00mm" wi="141.22mm" file="US20230007420A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="163.15mm" wi="136.14mm" file="US20230007420A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="133.77mm" wi="136.57mm" file="US20230007420A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="205.91mm" wi="79.59mm" file="US20230007420A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="194.39mm" wi="137.24mm" orientation="landscape" file="US20230007420A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="206.16mm" wi="77.89mm" orientation="landscape" file="US20230007420A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="176.02mm" wi="149.94mm" orientation="landscape" file="US20230007420A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="202.52mm" wi="87.63mm" orientation="landscape" file="US20230007420A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="175.43mm" wi="100.92mm" orientation="landscape" file="US20230007420A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="164.85mm" wi="136.23mm" file="US20230007420A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="166.62mm" wi="136.23mm" file="US20230007420A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="161.04mm" wi="79.50mm" file="US20230007420A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="205.91mm" wi="79.59mm" file="US20230007420A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="173.99mm" wi="153.42mm" orientation="landscape" file="US20230007420A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">The present invention relates to a method and apparatus for providing subject specific digital audio data and a subject specific digital audio profile. In particular, but not exclusively, the present invention relates to the near-field acoustic measurement of Head Related Transfer-Functions (HRTFs) of a subject (e.g. a person, a dummy mannequin, or an anthropomorphic model) to provide a binaural Ambisonics profile for that subject.</p><p id="p-0003" num="0002">It can be said that the physical characteristics of a person affect how they perceive sound. Example physical characteristics include, but at are not limited to, the size, shape, and composition of the person's torso, head, facial features, and ears. Consequently, when creating or recreating an audio experience for a person, one may wish to account for their physical characteristics to make the experience more immersive and realistic for that person.</p><p id="p-0004" num="0003">HRTFs quantify the cumulative effect of such physical characteristics on the perception of sound incoming from a given point in space relative to a listener across a band of frequencies. By convolving an audio signal with an HRTF, an audio signal can be transformed to behave as though it has been modified by a person's relevant physical characteristics. For example, after an appropriate mathematical operation involving an HRTF, or a set of HRTFs, associated with the same given point, a sound can be transformed to a form that, if played back over a pair of earphones or headphones, would sound to a listener as though the origin of the sound was that given point.</p><p id="p-0005" num="0004">One subject area in which HRTFs are utilised is the field of binaural audio. Binaural audio involves simulating a three-dimensional soundfield directly at the ears of a listener.</p><p id="p-0006" num="0005">The HRTFs used in research and industry are what is may be referred to as &#x2018;non-individual&#x2019;, i.e. they are HRTFs determined from a dummy mannequin or anthropomorphic model constructed to represent some average of the relevant physical characteristics of a population. This one-size-fits-all approach can lead to an unsatisfactory audio experience for a listener and is often unsuitable for applications where a high degree of immersion and/or accuracy of sound localisation is required.</p><p id="p-0007" num="0006">&#x2018;Personal&#x2019; or &#x2018;individual&#x2019; HRTFs can be determined from the output data of a microphone, located on or within the ear of a person, responsive to impulses of given frequencies, or a signal representative thereof, transmitted by a loudspeaker element at a predetermined location. Creation of such personal HRTFs conventionally require loudspeaker arrays supported over a considerable distance from a subject. This makes it costly to provide and inconvenient for a user/subject to access.</p><p id="p-0008" num="0007">Despite the disadvantages of non-individual HRTFs, they remain in use at least in part due to the above-mentioned impracticalities and cost-prohibitive nature of existing solutions for providing HRTF acoustic measurements of an individual. For example, these existing solutions for providing subject specific HRTFs require a large (and expensive) loudspeaker array and the measurement process is uncomfortable for the individual being measured. One reason that a large loudspeaker array is used to take acoustic measurements of an individual is due to the complications that arise when trying to measure HRTFs in the near-field as a result of the effects of distance on the properties of a propagating sound wave.</p><p id="p-0009" num="0008">It is an aim of the present invention to at least partly mitigate one or more of the above-mentioned problems.</p><p id="p-0010" num="0009">It is an aim of certain embodiments of the present invention to provide apparatus and a method for taking HRTF near-field acoustic measurements of a specific subject and for providing subject specific audio data.</p><p id="p-0011" num="0010">It is an aim of certain embodiments of the present invention to provide apparatus and a method for taking HRTF near-field acoustic measurements of a specific subject with greater proximity to the subject than conventional solutions allow.</p><p id="p-0012" num="0011">It is an aim of certain embodiments of the present invention to provide apparatus for taking HRTF acoustic measurements of a subject that is cheaper and more convenient to transport and construct and of a smaller physical footprint than conventional solutions allow.</p><p id="p-0013" num="0012">It is an aim of certain embodiments of the present invention to provide apparatus and a method for providing a subject specific binaural Ambisonic renderer determined from acoustic measurements taken in a near-field regime.</p><p id="p-0014" num="0013">It is an aim of certain embodiments of the present invention to provide apparatus and a method for providing a personal binaural Ambisonic renderer determined from acoustic measurements taken in a near-field regime.</p><p id="p-0015" num="0014">It is an aim of certain embodiments of the present invention to provide apparatus and a method for providing a binaural Ambisonic renderer determined from acoustic measurements of a subject taken in a near-field regime.</p><p id="p-0016" num="0015">It is an aim of certain embodiments of the present invention to provide apparatus and a method for providing HRTFs exhibiting the distance-related characteristics of far-field HRTF data from near-field HRTF data.</p><p id="p-0017" num="0016">It is an aim of certain embodiments of the present invention to provide apparatus and a method for providing HRTFs.</p><p id="p-0018" num="0017">It is an aim of certain embodiment of the present invention to provide a subject audio data profile determined from acoustic measurements taken in the near field regime.</p><p id="p-0019" num="0018">It is an aim of certain embodiment of the present invention to provide a personal audio data profile determined from acoustic measurements taken in the near field regime.</p><p id="p-0020" num="0019">It is an aim of certain embodiment of the present invention to provide a subject audio data profile for enabling more immersive and realistic binaural audio experiences.</p><p id="p-0021" num="0020">According to a first aspect of the present invention there is provided apparatus for providing subject specific digital audio data, comprising:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0021">a plurality of loudspeaker elements, each responsive to at least one respective audio signal input and supported in a predetermined spatial relationship in which respective locations of an effective point source of each loudspeaker element all lie in an imaginary surface that at least partially contains a spatial region where a subject comprising at least one aural cavity is locatable;</li>        <li id="ul0002-0002" num="0022">at least one microphone element locatable on or within an aural cavity of the subject, for providing a respective subject specific audio data output responsive to at least one physical characteristic of the subject and an audio signal output from at least one of the loudspeaker elements; and</li>        <li id="ul0002-0003" num="0023">an audio processing element for processing the subject specific audio data output and providing subject specific digital audio data for said subject, responsive thereto; wherein</li>        <li id="ul0002-0004" num="0024">a distance between each respective location and each aural cavity is less than 1.5 metres.</li>    </ul>    </li></ul></p><p id="p-0022" num="0025">Aptly the subject specific digital audio data comprises data that represents a superposition of sound, from the plurality of effective point sources of the loudspeaker elements, at the aural cavity responsive to at least one physical characteristic of the subject.</p><p id="p-0023" num="0026">Aptly each subject specific audio data output comprises a digital or analogue representation of a physical reverberation of an active element of the respective microphone element responsive to a superposition of sound, including sound from the plurality of effective point sources of the loudspeaker elements, at the active element.</p><p id="p-0024" num="0027">Aptly said a distance is selected to provide a wave front of sound from any one of the loudspeaker elements, at each aural cavity, that is not effectively planar.</p><p id="p-0025" num="0028">Aptly said a distance is selected to provide a near field sound wave provided by a superposition of sound, including sound from the plurality of effective point sources of the loudspeaker elements, at each aural cavity.</p><p id="p-0026" num="0029">Aptly the superposition of sound from the loudspeaker elements at each aural cavity is sufficiently complex that subsequent processing of the subject specific audio data output requires at least one Ambisonic processing step.</p><p id="p-0027" num="0030">Aptly each aural cavity of a subject comprises a sound receiving orifice opening into a channel; and supporting flesh or flesh imitating material surrounding the orifice and the channel.</p><p id="p-0028" num="0031">Aptly each subject comprises at least one physical characteristic responsive to a shape and size of the orifice and the channel and/or a density, surface texture and/or layering of the supporting flesh or flesh imitating material.</p><p id="p-0029" num="0032">Aptly the imaginary surface comprises a hemisphere or a portion of a hemisphere or a cylinder or a portion of a cylinder or a combined surface that includes a full or partial hemisphere portion and a full or partial cylindrical portion.</p><p id="p-0030" num="0033">Aptly the subject is a person, or a dummy mannequin, or an anthropomorphic model.</p><p id="p-0031" num="0034">Aptly the apparatus further comprises an alignment system for aligning the subject with respect to a predetermined location determined by the predetermined spatial relationship.</p><p id="p-0032" num="0035">Aptly the alignment system comprises at least one visual display.</p><p id="p-0033" num="0036">Aptly the alignment system comprises at least one video camera device.</p><p id="p-0034" num="0037">Aptly the alignment system comprises at least one laser.</p><p id="p-0035" num="0038">Aptly the visual display is responsive to at least one video camera device and/or at least one laser.</p><p id="p-0036" num="0039">Aptly a position of at least one of the plurality of loudspeaker elements is adjustable responsive to a determined height of the subject.</p><p id="p-0037" num="0040">Aptly the apparatus further comprises at least one linear actuator for adjusting a position of at least one of the plurality of loudspeaker elements responsive to a determined height of the subject.</p><p id="p-0038" num="0041">Aptly the apparatus further comprises at least one panel or body of sound-dampening material proximate to the support.</p><p id="p-0039" num="0042">Aptly at least a first group of the loudspeaker elements is connected to a further group of the loudspeaker elements via a hinged connection that allows the first group to be selectively located with respect to the further group.</p><p id="p-0040" num="0043">Aptly the plurality of loudspeaker elements is free-standing.</p><p id="p-0041" num="0044">Aptly the loudspeaker elements are supported via a support and the support comprises a modular rig.</p><p id="p-0042" num="0045">Aptly the loudspeaker elements are supported via a support and the support is portable.</p><p id="p-0043" num="0046">Aptly each said respective audio signal input is representative of an impulsive input.</p><p id="p-0044" num="0047">Aptly the subject specific digital audio data comprises an analogue-to-digital conversion of a respective subject specific analogue audio data output.</p><p id="p-0045" num="0048">Aptly the subject specific digital audio data comprises binaural subject specific digital audio data.</p><p id="p-0046" num="0049">Aptly the subject specific digital audio data comprises data representative of at least one Head Related Transfer Function (HRTF).</p><p id="p-0047" num="0050">Aptly the processed subject specific audio data comprises data representative of at least one near-field Head Related Transfer Function (HRTF).</p><p id="p-0048" num="0051">Aptly the processed subject audio data comprises data representative of at least one near-field compensated (NFC) Head Related Transfer Function (HRTF).</p><p id="p-0049" num="0052">Aptly the subject specific digital audio data comprises data representative of at least one synthesised far-field Head Related Transfer Function (HRTF).</p><p id="p-0050" num="0053">Aptly the subject specific digital audio data comprises a binaural Ambisonic renderer.</p><p id="p-0051" num="0054">Aptly the binaural Ambisonic renderer is a personal binaural Ambisonic renderer.</p><p id="p-0052" num="0055">Aptly the apparatus further comprises a control interface for receiving user input.</p><p id="p-0053" num="0056">Aptly the predetermined spatial relationship is a spatial relationship predetermined from a regular 2-dimensional shape or a regular 3-dimensional shape.</p><p id="p-0054" num="0057">Aptly the predetermined spatial relationship is determined from a Lebedev grid distribution.</p><p id="p-0055" num="0058">According to a second aspect of the present invention there is provided a method for determining subject specific digital audio data, comprising:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0059">providing at least one respective audio signal input to each of a plurality of loudspeaker elements supported in a predetermined spatial relationship, in which respective locations of an effective point source of each loudspeaker element all lie in an imaginary surface that at least partially contains a spatial region where at least one aural cavity of a subject is located, thereby providing a distance between each respective location and each aural cavity of less than 1.5 metres:</li>        <li id="ul0004-0002" num="0060">responsive to at least one audio signal output from at least one of the loudspeaker elements, via at least one microphone element located at or within an aural cavity of the subject, providing respective subject specific audio data output; and</li>        <li id="ul0004-0003" num="0061">via an audio processing system, processing the subject specific audio data output, thereby providing subject specific digital audio data.</li>    </ul>    </li></ul></p><p id="p-0056" num="0062">Aptly the method further comprises providing the subject specific digital audio data as data that represents a superposition of sound at the aural cavity responsive to at least one physical characteristic of the subject.</p><p id="p-0057" num="0063">Aptly the method further comprises providing the subject specific audio data output as a digital or analogue representation of a physical reverberation of an active element of a respective microphone element responsive to a superposition of sound at the active element.</p><p id="p-0058" num="0064">Aptly the method further comprises locating a subject that comprises a person or a dummy mannequin or an anthropomorphic model in a spatial region that is at least partially contained by an imaginary surface in which an effective point source of each loudspeaker element lies.</p><p id="p-0059" num="0065">Aptly the method further comprises prior to or subsequent to locating the subject in the spatial region, adjusting a height of at least one loudspeaker element with respect to a floor surface via which the subject is located.</p><p id="p-0060" num="0066">Aptly the method further comprises providing to each loudspeaker element as an impulse signal or a signal representative of an impulse, respective audio signal inputs.</p><p id="p-0061" num="0067">Aptly the method further comprises converting the subject specific audio data output via an analogue-digital conversion step thereby providing the subject specific digital audio data.</p><p id="p-0062" num="0068">Aptly the method further comprises providing at least one near field compensated (NFC) Head Related Transfer Function (HRTF) via application of a near field compensation audio processing step to the subject specific audio data output.</p><p id="p-0063" num="0069">Aptly the method further comprises modifying at least one NFC HRTF and providing at least one synthesised far-field HRTF.</p><p id="p-0064" num="0070">Aptly the method further comprises formatting a suitable collection of HRTFs and providing a subject specific binaural Ambisonic renderer.</p><p id="p-0065" num="0071">Aptly the predetermined spatial relationship is a spatial relationship predetermined from a regular 2-dimensional shape or a regular 3-dimensional shape.</p><p id="p-0066" num="0072">Aptly the predetermined spatial relationship is determined from a Lebedev grid distribution.</p><p id="p-0067" num="0073">According to a third aspect of the present invention there is provided a subject specific digital audio profile, determined from at least one analogue audio data output provided by at least one microphone element located on or within at least one aural cavity of a subject, that comprises a subject specific Ambisonics renderer that modifies digital audio input data according to at least one physical characteristic of a subject and provides personalised audio data output responsive thereto, wherein:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0074">the at least one microphone element is responsive to an audio signal output of at least one of a plurality of loudspeaker elements that are supported in a predetermined spatial relationship in which respective locations of an effective point source of each loudspeaker element all lie in an imaginary surface that at least partially contains a spatial region where a subject comprising at least one aural cavity is locatable, wherein a distance between each respective location and each aural cavity is less than 1.5 metres; and</li>        <li id="ul0006-0002" num="0075">the analogue audio data output is processed via a near-field compensation audio processing technique.</li>    </ul>    </li></ul></p><p id="p-0068" num="0076">Aptly the subject is a person, or a dummy mannequin, or an anthropomorphic model.</p><p id="p-0069" num="0077">Aptly each said respective audio signal input is representative of an impulsive input.</p><p id="p-0070" num="0078">Aptly the subject digital audio data comprises an analogue-to-digital conversion of the respective subject analogue audio data.</p><p id="p-0071" num="0079">Aptly the subject audio data comprises binaural subject digital audio data.</p><p id="p-0072" num="0080">Aptly the subject digital audio data comprises data representative of at least one Head Related Transfer Function (HRTF).</p><p id="p-0073" num="0081">Aptly the subject digital audio data comprises data representative of at least one near-field Head Related Transfer Function (HRTF).</p><p id="p-0074" num="0082">Aptly the subject digital audio data comprises data representative of at least one near-field compensated (NFC) Head Related Transfer Function (HRTF).</p><p id="p-0075" num="0083">Aptly the subject digital audio data comprises at least one synthesised far-field Head Related Transfer Function (HRTF).</p><p id="p-0076" num="0084">Aptly the predetermined spatial relationship is a spatial relationship predetermined from a regular 2-dimensional shape or a regular 3-dimensional shape.</p><p id="p-0077" num="0085">Aptly the predetermined spatial relationship is determined from a Lebedev grid distribution.</p><p id="p-0078" num="0086">Certain embodiments of the present invention provide acoustic measurements of a subject and subject digital audio data at a lower cost and/or with greater convenience than existing solutions.</p><p id="p-0079" num="0087">Certain embodiments of the present invention provide apparatus, for providing acoustic measurements of a subject and subject audio data, that occupies a reduced footprint and/or physical space than existing solutions.</p><p id="p-0080" num="0088">Certain embodiments of the present invention provide a method that provides subject digital audio data determined from acoustic measurements of a subject taken within greater proximity to the subject than existing solutions.</p><p id="p-0081" num="0089">Certain embodiments of the present invention provide HRTF data exhibiting the distance-related characteristics of far-field HRTF data from near-field HRTF data.</p><p id="p-0082" num="0090">Certain embodiments of the present invention provide a subject specific digital audio profile for enabling more immersive and realistic binaural audio experiences.</p><p id="p-0083" num="0091">Certain embodiments of the present invention provide a personal digital audio profile for enabling more immersive and realistic binaural audio experiences.</p><p id="p-0084" num="0092">Certain embodiments of the present invention provide a personal audio filter that affects the sound localisation characteristics of a sound according to the physical characteristics of a person.</p><p id="p-0085" num="0093">Certain embodiments of the present invention provide a subject specific audio filter that affects the sound localisation characteristics of a sound according to the physical characteristics of the subject.</p><p id="p-0086" num="0094">Certain embodiments of the present invention provide a loudspeaker array arranged according to a regular or approximately regular grid distribution that is height-adjustable located proximate to a subject.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><p id="p-0087" num="0095">Embodiments of the present invention will now be described hereinafter, by way of example only, with reference to the accompanying drawings in which:</p><p id="p-0088" num="0096"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an acoustic measurement chamber;</p><p id="p-0089" num="0097"><figref idref="DRAWINGS">FIG. <b>2</b><i>a </i></figref>illustrates an alternative view of part of an acoustic measurement chamber;</p><p id="p-0090" num="0098"><figref idref="DRAWINGS">FIG. <b>2</b><i>b </i></figref>illustrates an alternative view of part of an acoustic measurement chamber;</p><p id="p-0091" num="0099"><figref idref="DRAWINGS">FIG. <b>2</b><i>c </i></figref>illustrates a view of a loudspeaker;</p><p id="p-0092" num="0100"><figref idref="DRAWINGS">FIG. <b>2</b><i>d </i></figref>illustrates a view of a loudspeaker;</p><p id="p-0093" num="0101"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a content consumer consuming binaural Ambisonic content;</p><p id="p-0094" num="0102"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates the steps to take acoustic measurements of a subject;</p><p id="p-0095" num="0103"><figref idref="DRAWINGS">FIG. <b>5</b><i>a </i></figref>illustrates an anthropomorphic model;</p><p id="p-0096" num="0104"><figref idref="DRAWINGS">FIG. <b>5</b><i>b </i></figref>illustrates a dummy mannequin;</p><p id="p-0097" num="0105"><figref idref="DRAWINGS">FIG. <b>6</b><i>a </i></figref>illustrates a sweet spot of binaural audio reproduction;</p><p id="p-0098" num="0106"><figref idref="DRAWINGS">FIG. <b>6</b><i>b </i></figref>illustrates a further sweet spot of binaural audio reproduction;</p><p id="p-0099" num="0107"><figref idref="DRAWINGS">FIG. <b>6</b><i>c </i></figref>illustrates a further sweet spot of binaural audio reproduction;</p><p id="p-0100" num="0108"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates the group delay against frequency of an audio signal;</p><p id="p-0101" num="0109"><figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>illustrates a virtual loudspeaker array for head-centred Ambisonic decoding;</p><p id="p-0102" num="0110"><figref idref="DRAWINGS">FIG. <b>8</b><i>b </i></figref>illustrates a virtual loudspeaker array for BiRADIAL Ambisonic decoding;</p><p id="p-0103" num="0111"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates the frequencies at which different Ambisonic and HRTFs can be used;</p><p id="p-0104" num="0112"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates the steps of a method to determine a near-field time-aligned HRTF;</p><p id="p-0105" num="0113"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates the steps of a method to determine a near-field hybrid HRTF;</p><p id="p-0106" num="0114"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates the steps of a method to determine a synthesised far-field HRTF;</p><p id="p-0107" num="0115"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates the steps of a method to provide a subject specific binaural Ambisonic renderer; and</p><p id="p-0108" num="0116"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a combined workflow.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0109" num="0117">In the drawings like reference numerals refer to like parts.</p><p id="p-0110" num="0118"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an acoustic chamber <b>100</b>. The acoustic chamber <b>100</b> comprises a support structure <b>120</b> constructed from beams connected via brackets <b>160</b>. Optionally, the support structure <b>120</b> and the brackets <b>160</b> comprise a modular rig. Optionally, the modular rig is portable. The loudspeakers <b>110</b> are mounted to the support structure <b>120</b> according to a predetermined spatial relationship. This can approximate the positions the loudspeakers <b>110</b> would have relative to each other if the loudspeakers <b>110</b> were proximate to equally angularly distributed respective points on the surface of a Platonic solid (i.e. a cube, an octahedron, etc). In other words, the loudspeakers <b>110</b> can be arranged on the support structure such that each loudspeaker <b>110</b> corresponds to a point on an imaginary, regular three-dimensional solid, where the angular distribution of the points is approximately constant. By arranging the loudspeakers <b>110</b> in this way, the signals transmitted by each loudspeaker <b>110</b> will superpose at particular points in space producing &#x2018;sweet spots&#x2019; within the acoustic chamber <b>100</b> relative to a person <b>150</b> aligned at a reference point in the acoustic chamber <b>100</b>, increasing the quality of the acoustic measurements. Alternatively, the loudspeakers <b>110</b> can be arranged on the support structure <b>120</b> according to a Lebedev grid distribution. Optionally, the loudspeakers <b>110</b> can be arranged on the support structure <b>120</b> according to a regular two-dimensional polygon (i.e. a square, pentagon, etc).</p><p id="p-0111" num="0119">Linear actuators <b>140</b> can adjust the height of the support structure <b>120</b> suitable for a person <b>150</b> to stand (or, if appropriate, sit) inside the acoustic chamber <b>100</b>. A first portion <b>170</b><i>a </i>and second portion <b>170</b><i>b </i>of the support structure <b>120</b> are each connected to the remainder of the support structure <b>120</b> via hinges, allowing the first and second portions <b>170</b><i>a</i>, <b>170</b><i>b </i>to swing outwards, suitable for a person <b>150</b> to walk into the acoustic chamber <b>100</b>.</p><p id="p-0112" num="0120">A display <b>180</b> comprises a part of a self-alignment system that gives feedback to the person <b>150</b> so that the person <b>150</b> can align himself at a predetermined reference point in the acoustic chamber <b>100</b>. The self-alignment system further comprises at least one video camera that provides a video feed to the display <b>180</b> that can be overlaid with visual instructions on the display <b>180</b> that tell the person <b>150</b> how to adjust themselves within the chamber. Optionally, the self-alignment system further comprises at least one laser which measures the distance of a respective location of the person <b>150</b> from the laser.</p><p id="p-0113" num="0121">At least one ear <b>190</b> of the person <b>150</b> is located within the acoustic chamber <b>100</b>. Depending on the particular set of acoustic measurements that are desired, the combination of the signals transmitted by the loudspeakers <b>110</b> can generate a sweet spot centred in proximity to the centre of the head of the person <b>150</b>, a sweet spot centred in proximity to the orifice of one ear <b>190</b>, or two sweet spots each centred respectively in proximity to the opening of each of two ears of the person <b>150</b>. Ear-locatable microphones are located on or within at least one ear <b>190</b>. The ear-locatable microphones record sound transmitted by the loudspeakers <b>110</b> after the sound has been affected (e.g. via reflection, diffraction, and refraction) by the physical characteristics of the person <b>150</b>. Example physical characteristics include the size, shape, and composition of the body, torso, head, facial features, and ears of the person <b>150</b>. Optionally, &#x2018;composition&#x2019; may refer to the density and/or surface texture and/or layering of flesh or flesh imitating material. The acoustic chamber <b>100</b> is of a size such that when the person <b>150</b> is aligned at the centre of the acoustic chamber <b>100</b>, the loudspeakers <b>110</b> mounted to that support structure <b>120</b> are at sufficiently close distance to the person <b>150</b> such that the wave fronts of sound waves transmitted by the loudspeakers <b>110</b> are effectively non-planar. Such a distance may be referred to as &#x2018;near field&#x2019;. In the &#x2018;near field&#x2019; of a subject, small changes in the distance of the subject to a source are perceptually relevant. Aptly, the near-field represents a region of space close to the head of a subject/listener such that the wave front curvature of a sound wave are perceptually significant.</p><p id="p-0114" num="0122">It will be understood that instead of a person <b>150</b>, a dummy mannequin or anthropomorphic model can be located in the acoustic chamber <b>100</b> and microphones can be located on or within at least one artificial ear/aural cavity. An ear or an artificial ear is an example of an aural cavity.</p><p id="p-0115" num="0123">It will be understood that the acoustic chamber <b>100</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is an upstanding chamber. It will also be understood that a horizontal chamber could also be provided to obtain acoustic measurements in accordance with the present invention. A horizontal acoustic chamber might be an acoustic chamber where a subject is measured in a prone or supine position. A horizontal chamber might also be &#x2018;height&#x2019; adjustable. In this context a &#x2018;height&#x2019; of the horizontal acoustic chamber refers to the length of the chamber extending along the prone or supine subject from head to toe, or from head to base, from or top to bottom, etc.</p><p id="p-0116" num="0124">It will be understood that the acoustic chamber <b>100</b> has an associated imaginary surface that the size and shape of the support structure <b>120</b> resembles. For example, the acoustic chamber as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> has an associated imaginary surface comprising a hemispherical top connected to a cylindrical or tube-like body extending to the floor.</p><p id="p-0117" num="0125">In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the acoustic chamber <b>100</b> surrounds the person <b>150</b>. It will be understood that a partial acoustic chamber can also be used to take acoustic measurements. A partial acoustic chamber may have an associated imaginary surface, which at least partially contains the person <b>150</b>, for example comprising a hemispherical top connected to a semi-cylindrical body or a semi-hemispherical (i.e. a quarter sphere) top connected to a cylindrical body.</p><p id="p-0118" num="0126">It will be understood that sound-dampening material, such as acoustic foam, can be mounted to the outside of the acoustic chamber <b>100</b> and/or between the beams of the support structure <b>120</b> and/or positioned externally to at least partially surround the acoustic chamber <b>100</b>. By mounting acoustic foam to, or in proximity to, the acoustic chamber <b>100</b>, external noise can be reduced increasing the quality of acoustic measurements determined using the acoustic chamber <b>100</b>.</p><p id="p-0119" num="0127"><figref idref="DRAWINGS">FIG. <b>2</b><i>a </i></figref>illustrates a top portion of the acoustic chamber <b>100</b>. It will be understood that the top portion has an associated imaginary surface for example a hemisphere or a semi-hemisphere. Loudspeakers could be supported in a way that creates an imaginary surface that is a partial hemisphere and/or a partial cylinder.</p><p id="p-0120" num="0128"><figref idref="DRAWINGS">FIG. <b>2</b><i>b </i></figref>illustrates a first portion <b>170</b><i>a </i>of the support structure <b>120</b> in an open position and a second portion <b>170</b><i>b </i>of the support structure <b>120</b> in a closed position. An open position refers to a position where a first or a second portion of the support structure <b>120</b> is rotated, for example on a hinge connected to the remainder of the support structure <b>120</b>, relative to the remainder of the support structure <b>120</b> such that when both a first portion and a second portion of the support structure <b>120</b> are in an open position, the acoustic chamber <b>100</b> is suitable for a person to walk in and stand or sit within the acoustic chamber <b>100</b> (up to any height-adjustment).</p><p id="p-0121" num="0129"><figref idref="DRAWINGS">FIG. <b>2</b><i>c </i></figref>illustrates a loudspeaker driver <b>200</b> of a loudspeaker <b>110</b> mounted via a mounting bracket <b>210</b> to a beam of the support structure <b>120</b>. The loudspeaker driver <b>200</b> is an active element and is the component of the loudspeaker <b>110</b> that vibrates responsive to an audio signal input to the loudspeaker <b>110</b> to generate sound. A collection of the loudspeakers <b>200</b> can be referred to as a &#x2018;loudspeaker array&#x2019;. A collection of loudspeakers can be used to create one or more &#x2018;virtual loudspeaker array(s)&#x2019;. A virtual loudspeaker array can be created via the appropriate interference of sound waves transmitted by a collection of loudspeakers such that the resulting sound provides to an appropriately located listener the illusion of sources of sound that do not physically exist. Such illusory sources may be referred to as &#x2018;virtual sources&#x2019; or &#x2018;effective sources&#x2019;. Optionally, a loudspeaker array can be used to create two virtual loudspeaker arrays, one for each ear <b>190</b> of a person <b>150</b>, wherein each virtual loudspeaker array comprises virtual loudspeakers located at an equal distance from the respective ear the person <b>150</b>. Optionally, a loudspeaker array can be used to create a virtual loudspeaker array comprising virtual loudspeakers located at an equal distance from the ear <b>190</b> of the person <b>150</b>. Optionally, a loudspeaker array can be used to create a virtual loudspeaker array comprising virtual loudspeakers located at an equal distance from the centre of the head of a person <b>150</b>.</p><p id="p-0122" num="0130"><figref idref="DRAWINGS">FIG. <b>2</b><i>d </i></figref>illustrates a side view of a loudspeaker driver <b>200</b> of a loudspeaker <b>110</b> mounted via a mounting bracket <b>210</b> to a beam of the support structure <b>120</b>.</p><p id="p-0123" num="0131">Aptly, the acoustic chamber <b>100</b> provides apparatus for providing subject specific digital audio data. The acoustic chamber includes a plurality of loudspeaker elements <b>200</b>, each of these is responsive to at least one respective audio signal input and is supported in a predetermined spatial relationship in which respective locations of an effective point source of each loudspeaker element <b>200</b> all lie in an imaginary surface that at least partially contains a spatial region where a subject <b>150</b> comprising at least one aural cavity <b>190</b> is locatable. At least one microphone element is locatable on or within an aural cavity <b>190</b> of the subject <b>150</b>, for providing a respective subject specific audio data output responsive to at least one physical characteristic of the subject and an audio signal output from at least one of the loudspeaker elements <b>200</b>. An audio processing element can be included for processing the subject specific audio data output and providing subject specific digital audio data for said subject <b>150</b>, responsive thereto. A distance between each respective location and each aural cavity <b>190</b> is less than about 1.5 metres.</p><p id="p-0124" num="0132">Aptly, a distance between each respective location and each aural cavity <b>190</b> is about 1.5 metres. Aptly, a distance between each respective location and each aural cavity <b>190</b> is less than about 1.45 metres; or less than about 1.4 metres; or less than about 1.35 metres; or less than about 1.3 metres; or less than about 1.25 metres; or less than about 1.2 metres; or less than about 1.15 metres; or less than about 1.1 metres; or less than about 1.05 metres; or less than about 1 metre; or less than about 0.95 metres; or less than about 0.9 metres; or is any value selected from these ranges; or any sub-range constructed from the values contained within any of these ranges. Aptly, each respective location is within the near-field of each aural cavity <b>190</b>. Aptly, at least one respective location is within the near-field of each aural cavity <b>190</b>. Aptly, at least two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, or sixteen respective locations are within the near-field of each aural cavity <b>190</b>. Aptly, the acoustic chamber is adjustable to a height of 2 metres. Aptly the acoustic chamber is adjustable to a height of less than 2 metres. Aptly, the acoustic chamber is adjustable to a height of up to 2 metres. Aptly, the acoustic chamber is adjustable to height of, or above, 1 metre. Aptly, the acoustic chamber is adjustable to a height up to 1.5 metres; or up to 1.55 metres; or up to 1.6 metres; or up to 1.65 metres; or up to 1.7 metres; or up to 1.75 metres; or up to 1.8 metres; or up to 1.85 metres; or up to 1.9 metres; or up to 1.95 metres.</p><p id="p-0125" num="0133"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a scene <b>300</b> wherein a person <b>310</b> is listening to audio content on a computer <b>330</b> via a pair of headphones <b>320</b>. The audio content may a piece of music; a video (e.g. a film, a television programme, or an internet video); a computer game; or form a part of the production thereof. The headphones <b>320</b> may be on-ear or over-ear, or instead be in-ear earphones. The headphones <b>320</b> (or earphones) may be wired or wireless. The computer <b>330</b> may be a smartphone, a tablet, a laptop computer, a desktop computer, a music player, a server, a workstation, a pair of smart glasses, a Virtual Reality (VR) headset, or an augmented reality (AR) headset. A subject specific digital audio profile is stored in a memory unit, comprising part of either the computer <b>330</b> or the pair of headphones <b>320</b>, or at a location remote from the user (for example a cloud server) relating to the audio content being consumed. A memory unit is a computing device capable of storing digital data in a volatile or non-volatile form.</p><p id="p-0126" num="0134"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates the method steps to take acoustic measurements of a subject. In step <b>400</b>, an acoustic chamber is adjusted to a height suitable for the subject. The height of the acoustic chamber is adjusted relative to the height of the subject manually, or via the use of linear motors/actuators, or via the use of a platform, a chair, a stool, or the like.</p><p id="p-0127" num="0135">In step <b>410</b>, the subject is aligned relative to a reference point within the acoustic measurement chamber. The reference point is determined by the predetermined relationship according to which the acoustic measurement chamber is arranged. Optionally, the reference point is at a known location relative to a predicted sweet spot that may be generated by the loudspeakers <b>110</b> of an acoustic chamber <b>100</b>. At least an aural cavity (and optionally two) is located so that it is contained within an imaginary surface that contains the multiple loudspeaker effective point sources.</p><p id="p-0128" num="0136">The alignment step <b>410</b> may involve manual assistance and/or a self-alignment system. The self-alignment system may comprise at least one display connected to at least one video camera device. Optionally, the self-alignment system comprises at least one laser. Each laser can provide measurements of the distance of a part of the subject to the respective laser. The at least one display may display real-time video footage of the subject to the subject or to an external observer. The video camera devices and the displays may also be connected to a processing unit to provide simultaneously to the subject an overlay with real-time footage, so a subject or an external observer can more easily see the location of the head of the subject relative to the reference point. Adjusting the height of the acoustic chamber relative to the subject and aligning the subject relative to a reference point in the acoustic chamber can improve the accuracy of the acoustic measurements, and therefore the quality of the products of the audio processing of the acoustic measurements. A processing unit is a computing device capable of processing the video feeds of at least one video camera device and providing output to a display that shows real-time data to a subject or an external observer indicating a current position of the subject relative to the reference point. Optionally, a processing unit is a desktop computer, laptop computer, tablet, smartphone, server, or cloud computer. Optionally, the processing unit is capable of receiving data input, from at least one laser, that includes the distance of a part of the subject relative to the respective laser and providing output to a display responsive to the data input to aid the subject in the alignment process.</p><p id="p-0129" num="0137">In step <b>420</b>, at least one microphone element is placed on or within at least one ear or artificial ear or aural cavity of the subject.</p><p id="p-0130" num="0138">In step <b>430</b>, a first predetermined audio signal is played back through at least one of the loudspeaker elements. The predetermined audio signal may be an impulse of a particular frequency or a sinusoidal sweep of multiple frequencies that is inclusive thereof. A sinusoidal sweep of frequencies is an audio signal comprising a sinusoidal wave that progressively increases in frequency at a predetermined rate between a predetermined range of frequencies. Responsive to the predetermined audio signal and the physical characteristics of the subject, an audio signal (i.e. the HRTF associated with the first loudspeaker at given location) is captured by the at least one microphones and is recorded, in a digital data form, to a memory unit. This step is then repeated for as many impulse (or signal representative thereof) and loudspeaker element (of a particular location) combinations as desired. If the predetermined audio signal is a sinusoidal sweep of multiple frequencies, there may be a further step wherein a deconvolution technique is applied to the captured audio signal to determine an impulse-equivalent response of audio stimuli to the physical characteristics of the subject. A sinusoidal sweep may also be referred to as a sine sweep. Aptly, the deconvolution technique comprises a deconvolution step whereby the recorded signal is convolved with an inverted copy of the sine sweep in order to effectively simulate an impulsive stimuli.</p><p id="p-0131" num="0139">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref> by step <b>440</b>, the audio signal data can be processed to obtain HRTFs. Optionally, the digital data can be processed to obtain BiRADIAL HRTFs for one or both ears (or aural cavities) of a subject. Optionally, the digital data can be processed to obtain hybrid HRTFs. Optionally, the digital data can be processed to obtain synthesised far-field HRTFs. Optionally, the digital data can be processed to obtain a binaural Ambisonics renderer. The renderer is an element that converts from an audio file, which is encoded in a particular audio format, to a set of binaural signals suitable for a headphone setup. The Ambisonic renderer converts an Ambisonics audio file into a set of binaural signals suitable for the headphone setup. Ambisonic rendering defines a process of reproducing a soundfield from a finite number of fixed points with a particular angular resolution. Optionally, the digital data can be processed to obtain a personal digital data profile. Optionally, the digital data can be processed to obtain a subject specific digital data profile. Aptly, certain embodiments of the present invention provide an Ambisonics renderer that converts the Ambisonic audio input file to a binaural signal. Aptly, the Ambisonics renderer executes a two-step process that includes an Ambisonic decoder step, which produces loudspeaker signals, and then a renderer step, which convolves those signals with the HRTFs and sums them to produce a binaural signal. Aptly, this two-step process can be combined into a single step which is performed by the renderer directly in the Ambisonic domain.</p><p id="p-0132" num="0140">The Ambisonic audio file provides a surround-sound format that allows for the reproduction of a soundfield via an arbitrary loudspeaker layout, so long as there are a sufficient number of loudspeakers comprising the layout and, for a given number of loudspeakers, the loudspeakers are suitably arranged so that the signals from the loudspeakers appropriately interfere at a desired listening location. Via the steps in accordance with the present invention, a soundfield is decomposed into a component form based on the special mathematical functions known as &#x2018;spherical harmonics&#x2019;. By representing a soundfield in this way, certain transformations of the soundfield, such as rotational transformations, can be computed efficiently due to the natural mathematical symmetries of spherical harmonics.</p><p id="p-0133" num="0141">For a given order of an Ambisonic format, it is the components of the decomposed soundfield that are decoded to generate the signals that are sent each loudspeaker in a respective loudspeaker layout. The &#x2018;order&#x2019; of an Ambisonics format is determined from the number of components into which a soundfield is decomposed.</p><p id="p-0134" num="0142">Certain embodiments of the present invention provide a subject specific binaural Ambisonics renderer determined from near-field acoustic measurements of the subject. One advantage of the provided subject specific (e.g. a personal) Ambisonics renderer is that it provides a listener/user the benefit of lower-latency audio processing and a higher accuracy of sound localisation over conventional solutions. One area in which this is useful is when the head of the listener/user is being tracked in space and the head movements (e.g. rotational head movements) affect the sounds that the listener/user hears. This is useful in the context of professional computer gaming (which may also be referred to as &#x2018;eSports&#x2019;), for example, as a player who can more precisely and more quickly locate the source of an in-game sound has an advantage over his competitors.</p><p id="p-0135" num="0143"><figref idref="DRAWINGS">FIG. <b>5</b><i>a </i></figref>shows an anthropomorphic model of a human head. Conventionally, such models are used as dummy audience members at a specific recording event, such as a concert or recording studio session, and may feature a microphone and audio-specific electronics integrated into the ears and head of the model allowing direct binaural recording of audio content. Unfortunately, this means that conventionally such model may need to be present at each instance when a binaural recording is desired. In contrast with an acoustic chamber disclosed herein, it is possible to determine HTRFs of the model, which can then be applied to generic audio content to virtualise a binaural audio experience. This can allow cheaper and more convenient large-scale distribution of binaural audio content. It will be noted that the acoustic chamber as disclosed herein could be used in tandem with the audio electronics and microphone elements already included in an anthropomorphic model or dummy mannequin. For example, if an anthropomorphic model or dummy mannequin includes built-in microphones and amplifiers, these microphones and amplifiers can be used to record the signals incident proximate to an aural cavity or artificial ear of the anthropomorphic model or dummy mannequin and provide the digital data to a memory unit.</p><p id="p-0136" num="0144"><figref idref="DRAWINGS">FIG. <b>5</b><i>b </i></figref>shows a dummy mannequin. In addition to a head, this dummy features a torso. Optionally, the head and/or torso can be constructed from flesh imitating material. A dummy comprising a torso could be used to provide acoustic measurements that more closely resemble those of the average person (notwithstanding the other physical characteristics that affect the reflection, diffraction, and refraction of sound waves), and therefore one or more HRTFs or, an audio profile or audio renderer, determined from these acoustic measurements may be used to create a more immersive audio experience than those determined from measurements of a dummy without a torso.</p><p id="p-0137" num="0145"><figref idref="DRAWINGS">FIGS. <b>6</b><i>a</i>, <b>6</b><i>b</i>, and <b>6</b><i>c </i></figref>each show a respective approximate sweet spot <b>630</b> of surround sound reproduction relative to a subject <b>610</b> that is between virtual loudspeakers <b>620</b>. The loudspeakers <b>620</b> in each of <figref idref="DRAWINGS">FIGS. <b>6</b><i>a</i>-<b>6</b><i>c </i></figref>lie on a respective imaginary circle <b>600</b>. In <figref idref="DRAWINGS">FIG. <b>6</b><i>a</i></figref>, the sweet spot <b>630</b> is a sweet spot as it may appear at the centre of the virtual loudspeakers <b>620</b>. As each ear of a person samples a soundfield independently&#x2014;each from a different point in space&#x2014;a conventional implementation of Ambisonics may attempt to produce a sweet spot large enough to include both ears. However, to limit unwanted effects such as spatial-aliasing, rendering a sufficiently large sweet spot can require the limitation of the frequencies of sound that can be included in the soundfield or the use of high-order Ambisonics to achieve a satisfactory result.</p><p id="p-0138" num="0146"><figref idref="DRAWINGS">FIG. <b>6</b><i>b </i></figref>shows how time-aligned HRTFs can be used to overcome the aforementioned problem by manipulating the head-centred soundfield. In <figref idref="DRAWINGS">FIG. <b>6</b><i>b</i></figref>, there is a soundfield that has a sweet spot <b>630</b> at the location of an ear of a subject <b>610</b>. It will be understood that there is the simultaneous reproduction of the sound field at the location of the other ear. The soundfield can be manipulated by imposing a group delay on the head-centred HRTFs, producing time-aligned HRFFs, to ensure that each virtual loudspeaker feed arrives at each ear at the same time. However, by doing so the interaural time difference (ITD) is lost, affecting the sound localisation properties of the HRTFs. Therefore, hybrid HRTFs are constructed out of a combination of the head-centred HRTFs and the time-aligned HRTFs, with a crossover group delay as described in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, to achieve a balance of reducing comb filtering and spatial aliasing and improving the accuracy of sound localisation.</p><p id="p-0139" num="0147">In this context, the group delay of an audio signal is the time delay introduced during the reproduction of the audio signal into sound for the component frequencies of the audio signal.</p><p id="p-0140" num="0148"><figref idref="DRAWINGS">FIG. <b>6</b><i>c </i></figref>shows a sweet spot <b>630</b> around the left ear of a subject <b>610</b>. It will be understood that there exists a separate sweet spot for the left ear. Unlike the sweet spots in <figref idref="DRAWINGS">FIG. <b>6</b><i>b</i></figref>, the sweet spots in <figref idref="DRAWINGS">FIG. <b>6</b><i>c </i></figref>are created by a pair of independent virtual loudspeaker arrays (i.e. two separate groups of virtual loudspeakers create the sweet spots at each ear; one group for the left ear and one group for the right). This may be referred to as &#x2018;BiRADIAL&#x2019; Ambisonic Rendering. BiRADIAL Ambisonic Rendering can allow higher frequency reproduction at lower order Ambisonics.</p><p id="p-0141" num="0149"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a graph of the group delay of an HRTF audio signal plotted against frequency, showing the crossover frequency for an arbitrary order of Ambisonics. For a given order of Ambisonics, a crossover frequency can be determined, which describes the transition of the use of head-centred HRTFs to time-aligned HRTFs, by assigning a group delay to the HRTF of the corresponding frequency. This is to provide a balance between providing a sufficient sweet spot in the reproduced soundfield and preserving some of the binaural audio quality for an arbitrary sound comprising numerous frequencies. A curve <b>710</b> shows a relationship between group delay and frequency for frequencies below and up to the crossover frequency <b>720</b>. A curve <b>730</b> shows a relationship between group delay and frequency for frequencies in a crossover band of frequencies. A curve <b>740</b> shows a relationship between group delay and frequency for frequencies not included in the crossover band and above the crossover frequency <b>720</b>. By using a curve such as <b>730</b> to describe the crossover from head-centred HRTFs to time-aligned HRTFs, the effects of comb filtering are reduced.</p><p id="p-0142" num="0150"><figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>shows the virtual loudspeakers <b>840</b> in a virtual loudspeaker array rendered using a non-BiRADIAL Ambisonic decoding technique. Each virtual loudspeaker <b>840</b> provides two channels of sound (i.e. provides stereophonic sound). A first channel from each virtual loudspeaker <b>840</b> contributes to the sweet spot at a first ear <b>870</b> of a subject <b>810</b>, and a second channel from each virtual loudspeaker <b>840</b> contributes to the sweet spot at a second ear <b>880</b> of a subject <b>810</b>.</p><p id="p-0143" num="0151"><figref idref="DRAWINGS">FIG. <b>8</b><i>b </i></figref>shows two virtual loudspeaker arrays. A first virtual loudspeaker array is comprised of virtual loudspeakers <b>850</b>, and a second virtual loudspeaker array is comprised of virtual loudspeakers <b>860</b>. The virtual loudspeakers arrays are rendered using a BiRADIAL Ambisonic decoding technique; therefore, each of the virtual loudspeakers <b>850</b>, <b>860</b> provides one channel of sound (i.e. provides monophonic sound). The channels from the virtual loudspeakers <b>850</b> generate the sweet spot at a first ear <b>870</b> of a subject <b>810</b>, and the channels from each virtual loudspeaker <b>860</b> contributes to the sweet spot at a second ear <b>880</b> of a subject <b>810</b>.</p><p id="p-0144" num="0152"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a bar graph illustrating the frequencies at which different Ambisonic and HRTF techniques can be used. Optionally, first order Ambisonics render satisfactorily soundfields comprising frequencies up to around 700 Hz. It will be understood that higher order Ambisonic rendering can be used. Optionally, head-centred HRTFs are crossed-over to Time-Aligned HRTFs for frequencies above around 1500 Hz. It will be understood that other crossover frequencies can be used. Regarding the impact on sound localisation, interaural time differences (ITDs) are shown as dominant over interaural level differences (ILDs) for frequencies below around 1500 Hz; it will be understood that this frequency is given by way of example. ILDs are shown as dominant over ITDs for frequencies above around 1500 Hz; it will be understood that this frequency is given by way of example.</p><p id="p-0145" num="0153">In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, there is a block diagram showing the process of creating hybrid HRTFs comprising head-centred HRTFs and time-aligned HRTFs. In step <b>1000</b>, head-centred HRTFs of a specific subject are obtained, for example according to the method steps as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In step <b>1010</b>, an incremental time delay is introduced to the head-centred HRTFs for each ear, producing a set of intermediate HRTFs for each ear. The time delay may be, for example, according to the curve <b>730</b>. The time delay is introduced for frequencies in the head-centred HRTF signals up to the cross over frequency <b>720</b>. The time delay introduced is negligible for frequencies below the first-increment frequency <b>750</b>. In general, the time delay introduced can be determined individually for each HRTF because each HRTF corresponds to a particular location relative to the subject measured. For example, the location of each ear of a subject relative to the loudspeakers.</p><p id="p-0146" num="0154">In step <b>1020</b>, a Low-Pass Filter (LPF) effect is applied to each set of intermediate HRTFs that attenuates the amplitude of frequencies in the HRTF signal above the cross over frequency, producing a first set of time-aligned HRTFs for each ear.</p><p id="p-0147" num="0155">In step <b>1030</b>, a second copy of the head-centred HRTFs are time-aligned by introducing a fixed time delay for all frequencies, producing a second set of intermediate HRTFs for each ear, where the time delay is calculated according to the location of each ear relative to the loudspeakers.</p><p id="p-0148" num="0156">In step <b>1040</b>, a High-Pass Filter (HPF) effect is applied to the intermediate HRTFs that attenuates the amplitude of frequencies in the HRTF signal below the cross over frequency, producing a second set of time-aligned HRTFs for each ear.</p><p id="p-0149" num="0157">In step <b>1050</b>, the first and second sets of time-aligned HRTFs are combined for each ear, respectively, producing what is referred to as &#x2018;hybrid HRTFs&#x2019; for each ear. These hybrid HRTFs for each ear can also be packaged together into a single set of stereophonic hybrid HRTFs. Optionally, the first and second set of time-aligned HRTFs are combined via a linear phase crossover filter effect. It will be understood that alternative methods could be used to combine these two sets of HRTFs.</p><p id="p-0150" num="0158">In <figref idref="DRAWINGS">FIG. <b>11</b></figref>, there is a block diagram showing the process of creating hybrid HRTFs comprising head-centred HRTFs and BiRADIAL HRTFs. In step <b>1100</b>, head-centred HRTFs of a subject are obtained, for example according to the method steps as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0151" num="0159">In step <b>1110</b>, an incremental time delay is introduced to the head-centred HRTFs, producing a first set of intermediate HRTFs for each ear, for example according to the curve <b>730</b>, to a first copy of the head-centred HRTFs, up to the cross over frequency <b>720</b>. The time delay introduced is negligible for frequencies below the first-increment frequency <b>750</b>. In general, the time delay introduced can be determined individually for each HRTF because each HRTF corresponds to a particular location relative to the subject measured.</p><p id="p-0152" num="0160">In step <b>1120</b>, a Low-Pass Filter (LPF) effect is applied to the intermediate HRTFs for each ear that attenuates the amplitude of frequencies in the HRTF signal above the cross over frequency, producing a first set of time-aligned HRTFs for each ear.</p><p id="p-0153" num="0161">In step <b>1130</b>, BiRADIAL HRTFs for each ear of a subject are obtained, for example according to the method steps as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In step <b>1140</b>, a High-Pass Filter (HPF) effect is applied to the BiRADIAL HRTFs that attenuates the amplitude of frequencies in the HRTF signal below the cross over frequency, producing a set of truncated BiRADIAL HRTFs for each ear.</p><p id="p-0154" num="0162">In step <b>1150</b>, the truncated BiRADIAL HRTFs and time-aligned HRTFs are combined for each ear, respectively, producing another example of hybrid HRTFs for each ear. These hybrid HRTFs for each ear can also be packaged together to form stereophonic hybrid HRTFs. Optionally, the first and second set of time-aligned HRTFs are combined via linear phase crossover filter effect. It will be understood that alternative methods could be used to combine these two sets of HRTFs.</p><p id="p-0155" num="0163"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates the steps of a method to determine a synthesised far-field HRTF. In step <b>1200</b>, near-field time-aligned HRTFs or near-field BiRADIAL HRTFs are obtained, for example according to the above-discussed methods.</p><p id="p-0156" num="0164">In step <b>1210</b>, the near-field time-aligned HRTFs or near-field BiRADIAL HRTFs are distance-compensated and encoded into a spherical harmonic format.</p><p id="p-0157" num="0165">Conventionally, certain Ambisonics techniques are based on the assumption of plane wave theory, mathematically encoding a source into spherical harmonics assumes that the source has a planar wavefront. In accordance with the present invention, for acoustic measurements taken in the near-field, which thus involve sound waves having a non-planar wavefront, near-field compensation (NFC) steps are applied so the HRTFs are suitable for use in an Ambisonics renderer.</p><p id="p-0158" num="0166">The Ambisonic components, &#x3b2;<sub>mi</sub><sup>&#x3c3;</sup>, of a plane wave signal, s, of incidence (&#x3c6;,&#x3d1;) may be defined:</p><p id="p-0159" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;<sub>mi</sub><sup>&#x3c3;</sup><i>=s&#xb7;Y</i><sub>mi</sub><sup>&#x3c3;</sup>(&#x3c6;,&#x3d1;)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0160" num="0167">For a (radial) point source of position (&#x3c6;,&#x3b5;,r<sub>s</sub>) it is helpful to consider the near-field effect filter, &#x393;<sub>m</sub>, such that:</p><p id="p-0161" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;<sub>mi</sub><sup>&#x3c3;</sup><i>=S&#xb7;&#x393;</i><sub>m</sub>(<i>r</i><sub>s</sub>)&#xb7;<i>Y</i><sub>mi</sub><sup>&#x3c3;</sup>(&#x3c6;,&#x3d1;)&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0162" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x393;<sub>m</sub>(<i>r</i><sub>s</sub>)=<i>k&#xb7;d</i><sub>ref</sub><i>&#xb7;h</i><sub>m</sub><sup>&#x2212;</sup>(<i>kr</i><sub>s</sub>)&#xb7;<i>j</i><sup>&#x2212;(m+1)</sup>&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0163" num="0168">Where:</p><p id="p-0164" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mi>k</mi>  <mo>=</mo>  <mrow>   <mfrac>    <mrow>     <mn>2</mn>     <mo>&#x2062;</mo>     <mi>&#x3c0;</mi>     <mo>&#x2062;</mo>     <mi>f</mi>    </mrow>    <mi>c</mi>   </mfrac>   <mo>=</mo>   <mfrac>    <mi>&#x3c9;</mi>    <mi>c</mi>   </mfrac>  </mrow> </mrow></math></maths><ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0169">is the wave number;</li>        <li id="ul0008-0002" num="0170">d<sub>ref </sub>is the distance at which the source, s, was measured&#x2014;it is a compensation factor that derives from the equation:</li>    </ul>    </li></ul></p><p id="p-0165" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mi>pressure</mi>  <mo>=</mo>  <mfrac>   <mn>1</mn>   <mi>distance</mi>  </mfrac> </mrow></math></maths><ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0171">h<sub>m</sub><sup>&#x2212;</sup>(kr<sub>s</sub>) are the spherical Hankel functions of the second kind (divergent);</li>        <li id="ul0010-0002" num="0172">j is the imaginary number;</li>        <li id="ul0010-0003" num="0173">&#x393;<sub>m</sub>(r<sub>s</sub>) is the degree dependent filter that simulates to effect of a non-planar source.</li>    </ul>    </li></ul></p><p id="p-0166" num="0174">Equation 2 can be simplified into the following form:</p><p id="p-0167" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msubsup>       <mi>&#x3b2;</mi>       <mrow>        <mi>m</mi>        <mo>&#x2062;</mo>        <mi>i</mi>       </mrow>       <mi>&#x3c3;</mi>      </msubsup>      <mo>=</mo>      <mrow>       <mi>s</mi>       <mo>&#xb7;</mo>       <msubsup>        <mi>F</mi>        <mi>m</mi>        <mi>&#x3c4;</mi>       </msubsup>       <mo>&#xb7;</mo>       <mrow>        <msubsup>         <mi>Y</mi>         <mrow>          <mi>m</mi>          <mo>&#x2062;</mo>          <mi>i</mi>         </mrow>         <mi>&#x3c3;</mi>        </msubsup>        <mo>(</mo>        <mrow>         <mi>&#x3d5;</mi>         <mo>,</mo>         <mi>&#x3d1;</mi>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>     <mo>&#x2062;</mo>     <mtext></mtext>     <mrow>      <mi>Where</mi>      <mo>:</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00003-2" num="00003.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>F</mi>      <mi>m</mi>     </msub>     <mo>=</mo>     <mrow>      <mfrac>       <msub>        <mi>&#x393;</mi>        <mi>m</mi>       </msub>       <msub>        <mi>&#x393;</mi>        <mn>0</mn>       </msub>      </mfrac>      <mo>=</mo>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>0</mn>        </mrow>        <mi>m</mi>       </munderover>       <mrow>        <mfrac>         <mrow>          <mrow>           <mo>(</mo>           <mrow>            <mi>m</mi>            <mo>+</mo>            <mi>i</mi>           </mrow>           <mo>)</mo>          </mrow>          <mo>!</mo>         </mrow>         <mrow>          <mrow>           <mrow>            <mo>(</mo>            <mrow>             <mi>m</mi>             <mo>-</mo>             <mi>i</mi>            </mrow>            <mo>)</mo>           </mrow>           <mo>!</mo>          </mrow>          <mo>&#x2062;</mo>          <mrow>           <mi>i</mi>           <mo>!</mo>          </mrow>         </mrow>        </mfrac>        <mo>&#x2062;</mo>        <msup>         <mrow>          <mo>(</mo>          <mrow>           <mo>-</mo>           <mfrac>            <mi>jc</mi>            <mrow>             <mi>&#x3c9;</mi>             <mo>&#x2062;</mo>             <msub>              <mi>r</mi>              <mi>s</mi>             </msub>            </mrow>           </mfrac>          </mrow>          <mo>)</mo>         </mrow>         <mi>i</mi>        </msup>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0168" num="0175">Whereby F<sub>m </sub>are the degree dependent transfer functions which model the near-field effect of a signal originating from the point (&#x3c6;,&#x3d1;,r<sub>s</sub>) having been measured from the origin. The filters apply a phase shift and bass-boost to sources as they approach the origin and have a greater effect on higher order components. The near-field properties of the original source and the reproduction loudspeaker are considered when applying NFC.</p><p id="p-0169" num="0176">In step <b>1220</b>, mathematical functions representing an audio impulse source are encoded into a spherical harmonic format for a set of frequencies and are convolved with the HRTFs provided via step <b>1210</b>. Interaural Time Differences (ITDs) are determined for each HRIR from the position of the subject, of whom/which the acoustic measurements were taken, relative to the loudspeakers and the predetermined spatial relationship according to which the loudspeakers are arranged.</p><p id="p-0170" num="0177">In step <b>1230</b>, after introducing time delays, synthesised far-field (time-aligned or BiRADIAL) HRTFs are derived. Optionally, the synthesised far-field HRTFs are derived in a spherical harmonic format. The synthesised far-field HRTFs might also be referred to as far-field-equivalent HRTFs.</p><p id="p-0171" num="0178">Aptly, near-field (time-aligned or BiRADIAL) HRTFs may be encoded into spherical harmonic format in the form of a binaural Ambisonic renderer and distance compensated.</p><p id="p-0172" num="0179">Aptly, impulse input sources may also be encoded into spherical harmonic format. These may be convolved with the encoded time-aligned or BiRADIAL HRTFs (that form part of a binaural renderer) to produce synthesised far-field time-aligned or BiRADIAL HRTFs. However, time-aligned or BiRADIAL HRTFS can occasionally be limited in their use because they may not reproduce ITDs at low frequencies. Therefore, a time delay can be reintroduced at this point. This results in head-centred synthesised far-field HRTFs. These synthesised HRTFs may then be used in an Ambisonic renderer or indeed converted to hybrid HRTFs at this point for improved reproduction accuracy.</p><p id="p-0173" num="0180">It will be understood that synthesised far-field hybrid HRTFs may be determined in accordance with the present invention. Synthesised far-field hybrid HRTFs may be determined from near-field hybrid HRTFs that may be encoded into a spherical harmonic format and distance compensated. Impulse input sources, which may also be encoded into a spherical harmonic format, may be convolved with the near-field hybrid HRTFs to produce synthesised far-field hybrid HRTFs.</p><p id="p-0174" num="0181"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates a method for providing a subject specific Ambisonic renderer, for example a personal Ambisonic renderer. In step <b>1300</b>, acoustic measurements of a specific subject are obtained, for example according to the method as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0175" num="0182">In step <b>1310</b>, near-field hybrid (BiRADIAL or time-aligned) HRTFs or synthesised far-field hybrid HRTFs are determined for the specific subject.</p><p id="p-0176" num="0183">In step <b>1320</b>, where appropriate, the HRTFs is provided via step <b>1310</b> are distance-compensated. The HRTFs are then integrated into a subject specific Ambisonics renderer. A subject specific Ambisonics renderer might also be referred to as a subject specific Ambisonics decoder or a subject specific Ambisonics profile.</p><p id="p-0177" num="0184">In step <b>1330</b>, the subject specific Ambisonics renderer is then provided to the user in an appropriate file format via an appropriate means, for example via electronic file transfer, email, cloud computer access, or providing headphones with the subject specific renderer inbuilt/on board.</p><p id="p-0178" num="0185">In step <b>1340</b>, the subject specific Ambisonics renderer can then be integrated into software, such as a music player, video player, web-browser, operating system, video game, video game engine, and the like, or (if appropriate) an application programming interface (API) thereof, executed on a computer, smart phone, cloud server, cloud server, and the like to provide a subject specific binaural audio experience for the subject.</p><p id="p-0179" num="0186"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a combined workflow comprising the methods illustrated in <figref idref="DRAWINGS">FIGS. <b>10</b> through <b>13</b></figref>. The steps shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> that terminate at step <b>1</b> show an outline of how to produce a subject specific binaural Ambisonics renderer from near-field time-aligned HRTFs and near-field hybrid (time-aligned) HRTFs, for example via a combination of steps as described in the steps illustrated in <figref idref="DRAWINGS">FIGS. <b>4</b>, <b>10</b>, and <b>13</b></figref>.</p><p id="p-0180" num="0187">The steps shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> that terminate at step <b>2</b> show an outline of how to produce a subject specific binaural Ambisonics renderer from near-field BiRADIAL HRTFs and near-field hybrid (BiRADIAL) HRTF, for example via a combination of steps as described in the steps illustrated in <figref idref="DRAWINGS">FIGS. <b>4</b>, <b>11</b>, and <b>13</b></figref>.</p><p id="p-0181" num="0188">The steps shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> that terminate at step <b>3</b><i>a </i>show an outline of how to produce synthesised far-field HRTFs from time-aligned near-field HRTFs, for example via a combination of steps as described in the steps illustrated in <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>12</b></figref>.</p><p id="p-0182" num="0189">The steps shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> that terminate at step <b>3</b><i>b </i>show an outline of how to produce synthesised far-field HRTFs from BiRADIAL near-field HRTFs, for example via a combination of steps as described in the steps illustrated in <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>12</b></figref>.</p><p id="p-0183" num="0190">The steps shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> that terminate at step <b>4</b><i>a </i>show an outline of how to produce a subject specific binaural Ambisonics renderer from near-field time-aligned HRTFs via an intermediate far-field representation method, for example via a combination of steps as described in the steps illustrated in <figref idref="DRAWINGS">FIGS. <b>4</b>, <b>10</b>, <b>12</b>, and <b>13</b></figref>.</p><p id="p-0184" num="0191">The steps shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> that terminate at step <b>4</b><i>b </i>show an outline of how to produce a subject specific binaural Ambisonics renderer from near-field BiRADIAL HRTFs via an intermediate far-field representation method, for example via a combination of steps as described in the steps illustrated in <figref idref="DRAWINGS">FIGS. <b>4</b>, <b>11</b>, <b>12</b>, and <b>13</b></figref>.</p><p id="p-0185" num="0192">Throughout the description and claims of this specification, the words &#x201c;comprise&#x201d; and &#x201c;contain&#x201d; and variations of them mean &#x201c;including but not limited to&#x201d; and they are not intended to (and do not) exclude other moieties, additives, components, integers or steps. Throughout the description and claims of this specification, the singular encompasses the plural unless the context otherwise requires. In particular, where the indefinite article is used, the specification is to be understood as contemplating plurality as well as singularity, unless the context requires otherwise.</p><p id="p-0186" num="0193">Features, integers, characteristics or groups described in conjunction with a particular aspect, embodiment or example of the invention are to be understood to be applicable to any other aspect, embodiment or example described herein unless incompatible therewith. All of the features disclosed in this specification (including any accompanying claims, abstract and drawings), and/or all of the steps of any method or process so disclosed, may be combined in any combination, except combinations where at least some of the features and/or steps are mutually exclusive. The invention is not restricted to any details of any foregoing embodiments. The invention extends to any novel one, or novel combination, of the features disclosed in this specification (including any accompanying claims, abstract and drawings), or to any novel one, or any novel combination, of the steps of any method or process so disclosed.</p><p id="p-0187" num="0194">The reader's attention is directed to all papers and documents which are filed concurrently with or previous to this specification in connection with this application and which are open to public inspection with this specification, and the contents of all such papers and documents are incorporated herein by reference.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007420A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.67mm" wi="76.20mm" file="US20230007420A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230007420A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US20230007420A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003 MATH-US-00003-2" nb-file="US20230007420A1-20230105-M00003.NB"><img id="EMI-M00003" he="15.49mm" wi="76.20mm" file="US20230007420A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. Apparatus for providing subject specific digital audio data, comprising:<claim-text>a plurality of loudspeakers, each responsive to at least one respective audio signal input and supported in a predetermined spatial relationship in which respective locations of an effective point source of each loudspeaker all lie in an imaginary surface that at least partially contains a spatial region where a subject comprising at least one aural cavity is locatable;</claim-text><claim-text>at least one microphone locatable on or within an aural cavity of the subject, for providing a respective subject specific audio data output responsive to at least one physical characteristic of the subject and an audio signal output from at least one of the loudspeakers; and</claim-text><claim-text>an audio processor configured to process the subject specific audio data output and provide subject specific digital audio data for said subject, responsive thereto,</claim-text><claim-text>wherein a distance between each respective location and each aural cavity is less than 1.5 meters.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The apparatus as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the subject specific digital audio data comprises data that represents a superposition of sound, from the plurality of effective point sources of the loudspeakers, at the aural cavity responsive to at least one physical characteristic of the subject.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The apparatus as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein each subject specific audio data output comprises a digital or analogue representation of a physical reverberation of an active element of the respective microphone responsive to a superposition of sound, including sound from the plurality of effective point sources of the loudspeakers, at the active element.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The apparatus as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein said a distance is selected to provide a near field sound wave provided by a superposition of sound, including sound from the plurality of effective point sources of the loudspeakers, at each aural cavity.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The apparatus as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein each subject comprises at least one physical characteristic responsive to a shape and size of each aural cavity and/or a density, surface texture and/or layering of supporting flesh or flesh imitating material.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The apparatus as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the imaginary surface comprises a hemisphere or a portion of a hemisphere or a cylinder or a portion of a cylinder or a combined surface that includes a full or partial hemisphere portion and a full or partial cylindrical portion.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The apparatus as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the subject is a person, or a dummy mannequin, or an anthropomorphic model.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The apparatus as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a position of at least one of the plurality of loudspeakers is adjustable responsive to a determined height of the subject.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each said respective audio signal input is representative of an impulsive input and the subject specific digital audio data comprises data representative of at least one Head Related Transfer Function (HRTF).</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the predetermined spatial relationship is a spatial relationship predetermined from a regular 2-dimensional shape or a regular 3-dimensional shape.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A method for determining subject specific digital audio data, comprising:<claim-text>providing at least one respective audio signal input to each of a plurality of loudspeakers supported in a predetermined spatial relationship, in which respective locations of an effective point source of each loudspeaker all lie in an imaginary surface that at least partially contains a spatial region where at least one aural cavity of a subject is located, thereby providing a distance between each respective location and each aural cavity of less than 1.5 meters;</claim-text><claim-text>responsive to at least one audio signal output from at least one of the loudspeakers, via at least one microphone located at or within an aural cavity of the subject, providing respective subject specific audio data output; and</claim-text><claim-text>via an audio processing system, processing the subject specific audio data output, thereby providing subject specific digital audio data.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>providing the subject specific digital audio data as data that represents a superposition of sound at the aural cavity responsive to at least one physical characteristic of the subject.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>providing the subject specific audio data output as a digital or analogue representation of a physical reverberation of an active element of a respective microphone responsive to a superposition of sound at the active element.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>locating a subject that comprises a person or a dummy mannequin or an anthropomorphic model in a spatial region that is at least partially contained by an imaginary surface in which an effective point source of each loudspeaker lies.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method as claimed in <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:<claim-text>prior to or subsequent to locating the subject in the spatial region, adjusting a height of at least one loudspeaker with respect to a floor surface via which the subject is located.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>providing at least one near field compensated (NFC) Head Related Transfer Function (HRTF) via application of a near field compensation audio processing step to the subject specific audio data output and, optionally, modifying at least one NFC HRTF and providing at least one synthesised far-field HRTF.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>formatting a suitable collection of HRTFs and providing a subject specific binaural Ambisonic renderer.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the predetermined spatial relationship is a spatial relationship predetermined from a regular 2-dimensional shape or a regular 3-dimensional shape.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A subject specific digital audio profile, determined from at least one analogue audio data output provided by at least one microphone located on or within at least one aural cavity of a subject, that comprises a subject specific Ambisonics renderer that modifies digital audio input data according to at least one physical characteristic of a subject and provides personalized audio data output responsive thereto, wherein:<claim-text>the at least one microphone is responsive to an audio signal output of at least one of a plurality of loudspeakers that are supported in a predetermined spatial relationship in which respective locations of an effective point source of each loudspeaker all lie in an imaginary surface that at least partially contains a spatial region where a subject comprising at least one aural cavity is locatable, wherein a distance between each respective location and each aural cavity is less than 1.5 meters; and</claim-text><claim-text>the analogue audio data output is processed via a near-field compensation audio processing technique.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The subject specific digital audio profile as claimed in <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the subject digital audio data comprises data representative of at least one Head Related Transfer Function (HRTF).</claim-text></claim></claims></us-patent-application>