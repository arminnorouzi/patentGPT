<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005194A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005194</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782985</doc-number><date>20201103</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202010010816.8</doc-number><date>20200106</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>13</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>001</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>13</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE PROCESSING METHOD AND APPARATUS, READABLE MEDIUM AND ELECTRONIC DEVICE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BEIJING BYTEDANCE NETWORK TECHNOLOGY CO., LTD.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ZHUGE</last-name><first-name>Jingjing</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>FAN</last-name><first-name>Xu</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>CHEN</last-name><first-name>Ye</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Shupeng</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CN2020/126255</doc-number><date>20201103</date></document-id><us-371c12-date><date>20220606</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An image processing method includes: recognizing a target contour of a target object in a target image collected at a current moment determining, in the target contour, a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment taking a line segment composed of contour points between the starting contour point and the split contour point in the target contour as a first line segment, and taking a line segment except the first line segment in the target contour as a second line segment rendering the first line segment according to a first color, and rendering the second line segment according to a second color.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="191.43mm" wi="134.62mm" file="US20230005194A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="133.77mm" wi="151.89mm" file="US20230005194A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="192.96mm" wi="164.08mm" file="US20230005194A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="202.52mm" wi="136.65mm" file="US20230005194A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="245.45mm" wi="120.31mm" file="US20230005194A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="244.01mm" wi="139.28mm" file="US20230005194A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="202.69mm" wi="91.61mm" file="US20230005194A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="238.76mm" wi="89.15mm" file="US20230005194A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="235.20mm" wi="133.27mm" file="US20230005194A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">This application claims priority of the Chinese patent application entitled &#x201c;Image Processing Method and Apparatus, Readable Medium and Electronic Device&#x201d; filed with the Patent Office of China on Jan. 6, 2020, with the Application No. 202010010816.8, the disclosure of which is incorporated herein by reference in its entirety.</p><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to a field of image processing technologies, and more particularly, to an image processing method, an apparatus, a readable medium, and an electronic device.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0004" num="0003">With continuous development of terminal technology and image processing technology, image processing operations that may be provided on a terminal device are more and more diverse. For example, the terminal device may identify a contour of a specified object (e.g., a person, a cat, a dog, etc.) in an image; a mode of identifying the contour is relatively fixed, and the contour of the object may only be displayed according to a prescribed color or rule. With respect to multi-frame images included in a video, a state of an object in each frame of image may change, but the contour of the object is always displayed in a same mode in each frame of image, which cannot be associated with the dynamically changing object and cannot meet users' needs.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">An objective of the present disclosure is to provide an image processing method, an apparatus, a readable medium, and an electronic device, for solving the problem that the existing image processing method has a relatively fixed mode of identifying the contour, and can only display the contour of an object according to a prescribed color or rule, resulting in a technical problem that cannot be associated with the dynamically changing object and cannot meet users' needs.</p><p id="p-0006" num="0005">In a first aspect, the present disclosure provides an image processing method, the method comprises:</p><p id="p-0007" num="0006">recognizing a target contour of a target object in a target image collected at a current moment, the target contour comprises a plurality of contour points;</p><p id="p-0008" num="0007">determining, in the target contour, a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment, according to a time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, the first moment is a moment before the current moment;</p><p id="p-0009" num="0008">taking a line segment composed of contour points between the starting contour point and the split contour point in the target contour as a first line segment, and taking a line segment except the first line segment in the target contour as a second line segment;</p><p id="p-0010" num="0009">rendering the first line segment according to a first color, and rendering the second line segment according to a second color, the first color is different from the second color; and</p><p id="p-0011" num="0010">repeatedly executing a step of recognizing a target contour of a target object in the target image collected at a current moment, to a step of rendering a first line segment according to the first color, and rendering a second line segment according to the second color, until a line segment composed of contour points between the starting contour point and the final contour point in the target contour is rendered into the first color.</p><p id="p-0012" num="0011">In a second aspect, the present disclosure provides an image processing apparatus, the apparatus comprises:</p><p id="p-0013" num="0012">a recognizing module, configured to recognize a target contour of a target object in a target image collected at a current moment, the target contour comprises a plurality of contour points;</p><p id="p-0014" num="0013">a first determining module, configured to determine, in the target contour, a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment, according to a time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, the first moment is a moment before the current moment;</p><p id="p-0015" num="0014">a second determining module, configured to take a line segment composed of contour points between the starting contour point and the split contour point in the target contour as a first line segment, and take a line segment except the first line segment in the target contour as a second line segment; and</p><p id="p-0016" num="0015">a rendering module, configured to render the first line segment according to a first color, and render the second line segment according to a second color, the first color is different from the second color;</p><p id="p-0017" num="0016">repeatedly executing a step of recognizing a target contour of a target object in a target image collected at a current moment, to a step of rendering a first line segment according to the first color, and rendering a second line segment according to the second color, until a line segment composed of the contour points between the starting contour point and the final contour point in the target contour is rendered into the first color.</p><p id="p-0018" num="0017">In a third aspect, the present disclosure provides a computer-readable medium, on which a computer program is stored. The program, when executed by a processing apparatus, implements steps of the method described in the first aspect of the present disclosure.</p><p id="p-0019" num="0018">In a fourth aspect, the present disclosure provides an electronic device, which comprises:</p><p id="p-0020" num="0019">a storage apparatus, on which a computer program is stored;</p><p id="p-0021" num="0020">a processing apparatus, configured to execute the computer program in the storage apparatus, to implement steps of the method described in the first aspect of the present disclosure.</p><p id="p-0022" num="0021">Through the above-described technical solution, the present disclosure firstly recognizes a target contour of a target object from a target image collected at a current moment; then according to a time difference between the current moment and a first moment, determines a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment in the target contour; then takes a line segment composed of contour points between the starting contour point and the split contour point as a first line segment, and takes a line segment except the first line segment in the target contour as a second line segment; finally renders the first line segment according to a first color, and renders the second line segment according to a second color. The above-described steps are repeatedly executed until the line segment composed of contour points between the starting contour point and the final contour point is rendered into the first color. The present disclosure can implement an effect of dynamic stroke on the contour of the target object, within a certain range indicated by the starting contour position and the final contour position.</p><p id="p-0023" num="0022">Other features and advantages of the present disclosure will be described in detail in the detailed description that follows.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0024" num="0023">The above and other features, advantages and aspects of various embodiments of the present disclosure will become more apparent when taken in conjunction with the accompanying drawings and with reference to the following detailed implements. Throughout the drawings, the same or similar reference numbers refer to the same or similar elements. It should be understood that the drawings are schematic and that the components and elements are not necessarily drawn to scale. In the accompanying drawings:</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flow chart of an image processing method shown according to an exemplary embodiment;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of a target contour shown according to an exemplary embodiment;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow chart of another image processing method shown according to an exemplary embodiment;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow chart of another image processing method shown according to an exemplary embodiment;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart of another image processing method shown according to an exemplary embodiment;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow chart of another image processing method shown according to an exemplary embodiment;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a rendered target image shown according to an exemplary embodiment;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow chart of another image processing method shown according to an exemplary embodiment;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a rendered target image shown according to an exemplary embodiment;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram of an image processing apparatus shown according to an exemplary embodiment;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a block diagram of another image processing apparatus shown according to an exemplary embodiment;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a block diagram of another image processing apparatus shown according to an exemplary embodiment;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram of another image processing apparatus shown according to an exemplary embodiment;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a block diagram of another image processing apparatus shown according to an exemplary embodiment;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a block diagram of another image processing apparatus shown according to an exemplary embodiment; and</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a block diagram of a device shown according to an exemplary embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0041" num="0040">Embodiments of the present disclosure will be described in more detail below with reference to the accompanying drawings. While certain embodiments of the present disclosure are shown in the drawings, it should be understood that the present disclosure may be embodied in various forms and should not be construed as limited to the embodiments set forth herein. Rather, these embodiments are provided for a thorough and complete understanding of the present disclosure. It should be understood that the drawings and embodiments of the present disclosure are only for exemplary purposes, and are not intended to limit the protection scope of the present disclosure.</p><p id="p-0042" num="0041">It should be understood that the various steps described in the method embodiments of the present disclosure may be executed in different orders and/or in parallel. Furthermore, method embodiments may include additional steps and/or omitting executing the illustrated steps. The scope of the present disclosure is not limited in this regard.</p><p id="p-0043" num="0042">As used herein, the terms &#x201c;comprise&#x201d; &#x201c;include&#x201d; and variations thereof are open-ended inclusions, i.e., &#x201c;comprise but not limited to&#x201d;. The term &#x201c;based on&#x201d; is &#x201c;based at least in part on.&#x201d; The term &#x201c;one embodiment&#x201d; means &#x201c;at least one embodiment&#x201d;; the term &#x201c;another embodiment&#x201d; means &#x201c;at least one additional embodiment&#x201d;; the term &#x201c;some embodiments&#x201d; means &#x201c;at least some embodiments&#x201d;. Relevant definitions of other terms will be given in the description below.</p><p id="p-0044" num="0043">It should be noted that concepts such as &#x201c;first&#x201d;, &#x201c;second&#x201d;, etc. mentioned in the present disclosure are only used to distinguish different devices, modules or units, and are not used to limit the order of functions performed by these devices, modules or units or interdependence relationship.</p><p id="p-0045" num="0044">It should be noted that the modifications of &#x201c;a&#x201d; and &#x201c;a plurality&#x201d; mentioned in the present disclosure are illustrative rather than restrictive, and those skilled in the art should understand that unless the context clearly indicates otherwise, they should be understood as &#x201c;one or more&#x201d;.</p><p id="p-0046" num="0045">The names of messages or information exchanged between a plurality of devices in the embodiments of the present disclosure are only for illustrative purposes, and are not intended to limit the scope of these messages or information.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flow chart of an image processing method shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the method includes:</p><p id="p-0048" num="0047">Step <b>101</b>: recognizing a target contour of a target object in a target image collected at a current moment, the target contour includes a plurality of contour points.</p><p id="p-0049" num="0048">For example, the target image collected at the current moment may be, for example, a frame of image containing the target object collected at the current moment in a video shot by a user through a terminal device, or may also be a frame of image containing the target object at the current moment in a video played by the user on a display interface of the terminal device. The target object may be specified by the user, or may also be a preset object on the terminal device, for example, may be a living object such as a person, a cat, a dog, etc., or may also be any kind of object such as a doll, a robot, a car, a computer, a table and a chair, etc. Firstly, the target contour of the target object in the target image may be recognized according to a preset recognition algorithm; and the target contour includes a plurality of contour points. The contour point may be understood as an endpoint of each segment among a plurality of segments generated by dividing the target contour indicating the target object according to a preset interval; each contour point has a serial number; and the serial number is used to indicate an order of the contour point in the target contour. For example, the recognition algorithm may be any kind of image matting algorithm; and the target contour output by the recognition algorithm may have various representation forms. For example, the target contour may be a line segment (line segments connected end to end) that can represent the target contour, as shown in (a) of <figref idref="DRAWINGS">FIG. <b>2</b></figref>; then the target contour is divided into a plurality of line segments according to a preset interval; and an endpoint between each line segment is a contour point, that is, the contour points marked by serial numbers <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, . . . , <b>267</b> in (a) of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The target contour may also be a region that can represent the target contour (which may be understood as a middle region between two line segments of an inner contour and an outer contour of the target object), as shown in (b) of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, then the target contour is divided into a plurality of rectangular regions according to a preset interval; and four vertices of each rectangular region are contour points, that is, the contour points marked by serial numbers <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, . . . , <b>535</b> in (b) of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, where 0, 1, 2, 3 form a rectangular region, and so on. The following embodiments are described by taking the target contour shown in (a) of <figref idref="DRAWINGS">FIG. <b>2</b></figref> as an example, which will not be limited in the present disclosure.</p><p id="p-0050" num="0049">Step <b>102</b>: determining, in the target contour, a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment, according to a time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, the first moment is a moment before the current moment.</p><p id="p-0051" num="0050">For example, at the first moment (the moment before the current moment), the terminal device collects an initial image; the user may input a setting instruction through the terminal device; and the setting instruction may include a starting key point and a final key point set by the user, or may also include a first color set by the user. The starting key point and the final key point are used to identify a region range that needs to be stroked in the initial image; and the first color is used to indicate a color that needs to be rendered in the region range that needs to be stroked. Before the first moment, a display color of the target contour may be a second color. For example, the target object is a person; in the setting instruction, the starting key point is head top, the final key point is left hand; the first color is purple; and before that, the contour of the person is green (i.e., the second color). Then a dynamic stroke effect that needs to be achieved is: rendering the head to the left hand (which may be from the head to the left hand in a clockwise direction, or may also be from the head to the left hand in a counterclockwise direction) into purple, and rendering other portion in the target contour into green.</p><p id="p-0052" num="0051">At the first moment, the starting contour position corresponding to the starting key point and the final contour position corresponding to the final key point may be determined according to correspondence between different key points and contour points. The contour position (including: the starting contour position, the final contour position and the split contour position as mentioned later) may be understood as a position that can reflect a position of a contour point in the entire contour (including the target contour and the initial state contour as mentioned later). Since states such as posture, distance, etc. of the target object in images collected at different moments will change, and the corresponding contour contain different numbers of contour points; since it is impossible to accurately describe specified portions of the target object only by using the serial numbers of the contour points, a ratio of the serial number of the contour point to the number of contour points contained in the contour may be used as the contour position. For example, a serial number of a contour point is 20, and the number of contour points is 500, then the contour position may be 0.04. Then, at the current moment, according to the contour position (including the starting contour position and the final contour position) and the number of contour points contained in the target contour, the corresponding starting contour point and final contour point are determined. The starting contour point is a contour point corresponding to the starting key point in the target image; and similarly, the final contour point is a contour point corresponding to the final key point in the target image.</p><p id="p-0053" num="0052">Further, in a process of implementing dynamic stroke, a rendering duration of dynamic stroke may also be set, which may be set by the user at the first moment, or may also be a default value preset in the terminal device. The rendering duration may be understood as a duration required to gradually render the line segment between the starting key point and the final key point into the first color from the starting key point to the final key point, or may also be understood as the number of frames of image required to gradually render the line segment between the starting key point and the final key point into the first color. Therefore, the split contour point corresponding to the current moment may be determined according to the time difference between the current moment and the first moment (or the number of frames between the target image and the initial image); the split contour point may be understood as that, at the current moment, the line segment between the starting contour point and the split contour point needs to be rendered into the first color, and the other contour points need to be rendered into the second color.</p><p id="p-0054" num="0053">Step <b>103</b>: taking a line segment composed of contour points between the starting contour point and the split contour point in the target contour as a first line segment, and taking a line segment except the first line segment in the target contour as a second line segment.</p><p id="p-0055" num="0054">Step <b>104</b>: rendering the first line segment according to the first color, and rendering the second line segment according to the second color, the first color is different from the second color.</p><p id="p-0056" num="0055">For example, after the starting contour point and the split contour point are determined, the target contour may be divided into the first line segment and the second line segment according to the starting contour point and the split contour point. The first line segment is a line segment composed of contour points from the starting contour point to the split contour point in a preset direction (e.g., a clockwise direction or a counterclockwise direction); and the second line segment is a line segment except the first line segment in the target contour. Finally, the first line segment is rendered according to the first color, and the second line segment is rendered according to the second color.</p><p id="p-0057" num="0056">Steps <b>101</b> to <b>104</b> are repeatedly executed until, in the target contour, the line segment composed of contour points between the starting contour point and the final contour point is rendered into the first color.</p><p id="p-0058" num="0057">Taking the rendering duration of 5 seconds as an example, if the serial number of the starting contour point is 1, and the serial number of the final contour point is 100, there are 100 contour points between the starting contour point and the final contour point. It takes 5 seconds to gradually render the line segment composed of these 100 contour points into the first color, then, at the 1st second, contour points <b>1</b> to <b>20</b> are rendered according to the first color; and at the 2nd second, contour points <b>1</b> to <b>40</b> are rendered according to the first color, and so on, until the 5th second, contour points <b>1</b> to <b>100</b> (i.e., all contour points between the starting contour point and the final contour point) are rendered according to the first color to implement the effect of dynamic stroke.</p><p id="p-0059" num="0058">Or, taking the rendering duration as 100 frames as an example, if the serial number of the starting contour point is 1, and the serial number of the final contour point is 100, there are 100 contour points between the starting contour point and the final contour point. It takes 100 frames to gradually render the line segment composed of these <b>100</b> contour points into the first color, then, at the 1st frame, contour points <b>1</b> to <b>2</b> are rendered according to the first color, and at the 2nd frame, contour points <b>1</b> to <b>3</b> are rendered according to the first color, and so on, until the 100th frame, contour points <b>1</b> to <b>100</b> (i.e., all contour points between the starting contour point and the final contour point) are rendered according to the first color to implement the effect of dynamic stroke.</p><p id="p-0060" num="0059">It should be noted that, after the first line segment is rendered according to the first color and the second line segment is rendered according to the second color, different operations may be performed on the target image according to different needs of the user, for example, the target image may be displayed on the display interface of the terminal device, or the target image may also be stored in a specified storage path, or the target image may also be sent to a specified server for sharing, which will not be specifically limited in the present disclosure.</p><p id="p-0061" num="0060">To sum up, the present disclosure firstly recognizes a target contour of a target object from a target image collected at a current moment; then according to a time difference between the current moment and a first moment, determines a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment in the target contour; then takes a line segment composed of contour points between the starting contour point and the split contour point as a first line segment, and takes a line segment except the first line segment in the target contour as a second line segment; finally renders the first line segment according to a first color, and renders the second line segment according to a second color. The above-described steps are repeatedly executed until the line segment composed of the contour points between the starting contour point and the final contour point is rendered into the first color. The present disclosure can implement an effect of dynamic stroke on the contour of the target object, within a certain range indicated by the starting contour position and the final contour position.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow chart of another image processing method shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, step <b>102</b> includes:</p><p id="p-0063" num="0062">Step <b>1021</b>: determining the split contour position corresponding to the current moment according to the time difference between the current moment and the first moment and according to the starting contour position and the final contour position.</p><p id="p-0064" num="0063">Step <b>1022</b>: determining the starting contour point, the final contour point, and the split contour point corresponding to the split contour position in the target contour, according to a target number of contour points included in the target contour.</p><p id="p-0065" num="0064">For example, the split contour position corresponding to the current moment may be determined firstly according to the time difference between the current moment and the first moment (or the number of frames between the target image and the initial image). The split contour position is used to indicate which portion of the line segment indicated by the starting contour position and the final contour position needs to be rendered into the first color at the current moment, that is, the line segment indicated by the starting contour position to the split contour position is rendered into the first color.</p><p id="p-0066" num="0065">Split contour position=starting contour position+(final contour position-starting contour position)*(time difference/rendering duration).</p><p id="p-0067" num="0066">Taking the starting contour position as 0.2 and the final contour position as 0.4 as an example, the rendering duration is 5 seconds, and the time difference between the current moment and the first moment is 2 seconds, then the split contour position is 0.2+(0.4&#x2212;0.2)*(2/5)=0.28. The target number of contour points included in the target contour is 700, then the serial number of the starting contour point in the target contour is 0.2*700=140, that is, the 140th contour point; the serial number of the split contour point in the target contour is 0.28*700=196; and the serial number of the final contour point in the target contour is 0.4*700=280.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow chart of another image processing method shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, before step <b>101</b>, the method further includes:</p><p id="p-0069" num="0068">Step <b>105</b>: recognizing an initial state contour and a key point sequence of the target object in an initial image collected at the first moment.</p><p id="p-0070" num="0069">In a specific implementation scenario, the image collected by the terminal device at the first moment is the initial image, and then the initial state contour and the key point sequence of the target object in the initial image are recognized through the same recognition algorithm as in step <b>101</b>. The key point sequence includes one or more key points; and with respect to different target objects, the key points included in the key point sequence may be different. For example, if the target object is a person, the key point sequence may include key points such as: head top, right shoulder, left shoulder, left elbow, right elbow, left shoulder-inner side, right shoulder-inner side, left elbow-inner side, right elbow-inner side, left hand, right hand, left hip, right hip, left knee, right knee, left foot, right foot, left knee-inner side, right knee-inner side, thigh-inner side, etc. If the target object is a car, the key point sequence may include key points such as: front, left front light, right front light, left rear light, right rear light, left front door, right front door, left rear door, right rear door, left front wheel, right front wheel, left rear wheel, right rear wheel, rear, roof, etc.</p><p id="p-0071" num="0070">Step <b>106</b>: determining correspondence between key points included in the key point sequence and contour points included in the initial state contour according to a nearest neighbor algorithm.</p><p id="p-0072" num="0071">Step <b>107</b>: determining the starting contour position corresponding to a specified starting key point in the key point sequence and the final contour position corresponding to a specified final key point according to the correspondence.</p><p id="p-0073" num="0072">Exemplarily, the correspondence between the key points included in the key point sequence and the contour points included in the initial state contour may be obtained according to the nearest neighbor algorithm and a priori knowledge of the target object. The nearest neighbor algorithm may be a K Nearest Neighbor (kNN) algorithm. The correspondence may include a plurality of relationship records; and each relationship record includes a contour point corresponding to a key point in the initial state contour. Finally, according to the correspondence, the starting key point and the final key point set by the user through the terminal device at the first moment are determined, and the corresponding starting contour position and the corresponding final contour position are determined.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart of another image processing method shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the implementation mode of step <b>107</b> may include:</p><p id="p-0075" num="0074">Step <b>1071</b>: determining a first contour point corresponding to the starting key point and a second contour point corresponding to the final key point in the initial state contour, according to the correspondence.</p><p id="p-0076" num="0075">Step <b>1072</b>: determining the number of initial states of the contour points included in the initial state contour.</p><p id="p-0077" num="0076">Step <b>1073</b>: taking a ratio of a serial number of the first contour point to the number of initial states as the starting contour position, and taking a ratio of a serial number of the second contour point to the number of initial states as the final contour position.</p><p id="p-0078" num="0077">Specifically, when determining the starting contour position and the final contour position, the first contour point corresponding to the starting key point in the initial state contour and the second contour point corresponding to the final key point in the initial state contour may be determined firstly according to the correspondence. The first contour point is a contour point corresponding to the starting key point in the initial image, and corresponds to the starting contour point determined in step <b>102</b>, that is, a portion of the target object that is identified by the first contour point in the initial image is the same as a portion of the target object that is identified by the starting contour point in the target image. Similarly, the second contour point is a contour point corresponding to the final key point in the initial image, and corresponds to the final contour point determined in step <b>102</b>, that is, a portion of the target object that is identified by the second contour point in the initial image is the same as a portion of the target object that is identified by the final contour point in the target image.</p><p id="p-0079" num="0078">Then, the number of initial states of the contour points included in the initial state contour is determined; finally a ratio of the serial number of the first contour point to the number of initial states is taken as the starting contour position; and a ratio of the serial number of the second contour point to the number of initial states is taken as the final contour position. For example, if the starting key point specified by the user is head top, the serial number of the corresponding first contour point is 0, the final key point is the left hand, the serial number of the corresponding second contour point is 130, and the number of initial states is 536, then the starting contour position is 0/536=0, and the final contour position is 130/536=0.243.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow chart of another image processing method shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, step <b>104</b> may be implemented by steps of:</p><p id="p-0081" num="0080">Step <b>1041</b><i>a: </i>rendering each pixel point in the first line segment into the first color, and rendering a pixel point, a distance of which from the pixel point is less than a distance threshold, into the first color; and/or,</p><p id="p-0082" num="0081">Step <b>1041</b><i>b: </i>rendering each pixel point in the second line segment into the second color, and rendering a pixel point, a distance of which from the pixel point is less than the distance threshold, into the second color.</p><p id="p-0083" num="0082">For example, when rendering the first line segment and the second line segment, each pixel point in the first line segment and pixel points located in the vicinity of the pixel point (i.e., pixel points, distances of which from the pixel point are less than the distance threshold) may be all rendered into the first color; and each pixel point in the second line segment and pixel points located in the vicinity of the pixel point are all rendered into the second color. It may be understood that certain regions containing the target contour and located in the vicinity of the target contour are respectively rendered into the first color and the second color, which may make the rendered target contour more obvious and the effect of dynamic stroke more prominent. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, point A is the starting contour point, point B is the final contour point, and point C is the split contour point; according to the counterclockwise direction, line segment AC is the first line segment, and line segment CB and line segment BA are combined to form the second line segment; the line segment AC and a region located in the vicinity of the line segment AC are rendered into the first color; and the line segment CB and the,line segment BA, as well as a region located in the vicinity of the line segment CB and the line segment BA are rendered into the second color.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow chart of another image processing method shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, another implementation mode of step <b>104</b> may include:</p><p id="p-0085" num="0084">Step <b>1042</b><i>a: </i>rendering a first pixel point into the first color, and rendering a pixel point, a distance of which from the first pixel point is less than a first distance threshold, into the first color, the first pixel point is any pixel point on the first line segment.</p><p id="p-0086" num="0085">Step <b>1043</b><i>a: </i>taking a pixel point, a distance of which from the first pixel point is greater than or equal to the first distance threshold and less than a second distance threshold, as a first edge pixel point.</p><p id="p-0087" num="0086">Step <b>1044</b><i>a: </i>determining a first edge color according to a distance between the first edge pixel point and the first pixel point, and according to the first color, and rendering the first edge pixel point into the first edge color.</p><p id="p-0088" num="0087">and/or,</p><p id="p-0089" num="0088">Step <b>1042</b><i>b: </i>rendering a second pixel point into the second color, and rendering a pixel point, a distance of which from the second pixel point is less than the first distance threshold, into the second color, the second pixel point is any pixel point on the second line segment.</p><p id="p-0090" num="0089">Step <b>1043</b><i>b: </i>taking a pixel point, a distance of which from the second pixel point is greater than or equal to the first distance threshold and less than the second distance threshold, as a second edge pixel point.</p><p id="p-0091" num="0090">Step <b>1044</b><i>b: </i>determining a second edge color according to a distance between the second edge pixel point and the second pixel point, and according to the second color, and rendering the second edge pixel point into the second edge color.</p><p id="p-0092" num="0091">In another implementation mode, when rendering the first line segment, the first line segment may be rendered into the first color first. Then, according to the distance from the first line segment, different colors are selected to render the pixel points in the vicinity of the first line segment to achieve different effects of dynamic stroke. Taking the first pixel point as an example, a pixel point, a distance of which from the first pixel point is less than the first distance threshold, is rendered into the first color, that is, the pixel point is rendered into the same color as the first line segment. Then, a pixel point, a distance of which from the first pixel point is greater than or equal to the first distance threshold and less than the second distance threshold, is taken as the first edge pixel point. Finally, according to the distance between the first edge pixel point and the first pixel point, and the first color, the first edge color is determined; and the first edge pixel point is rendered into the first edge color. There may be a plurality of first edge pixel points; with respect to each edge pixel point, a first edge color is determined according to the distance of the edge pixel point from the first pixel point; and with respect to first edge pixel points, distances of which from the first pixel point are different, the corresponding first edge colors are also different. The execution steps of rendering the second line segment according to the second color are the same as the execution steps of rendering the first line segment according to the first color, and no details will be repeated here.</p><p id="p-0093" num="0092">Specifically, the implementation mode of step <b>1044</b><i>a </i>may be:</p><p id="p-0094" num="0093">Firstly, determining transparency of the first edge color, according to the distance between the first edge pixel point and the first pixel point, the transparency of the first edge color is positively correlated with the distance between the first edge pixel point and the first pixel point. The distance between the first edge pixel point and the first pixel point is zero, then transparency of the first color is 0% (i.e., the first pixel point and the pixel point, a distance of which from the first pixel point is less than the first distance threshold, are rendered into the first color). Transparency of the first edge color may be 1.5th power of the distance between the first edge pixel point and the first pixel point, and then multiplied by a preset scale factor k. Taking the scale factor of 1% as an example, if the distance between the first edge pixel point and the first pixel point is 10 pixels, then transparency of the first edge color=k*(<b>10</b>){circumflex over (&#x2003;)}1.5=31.6%; if the distance between the first edge pixel points and the first pixel point is 20 pixels, then transparency of the first edge color=k*(20){circumflex over (&#x2003;)}1.5=89.5%. An upper limit of transparency is 100%; and when transparency of the first edge color is 100%, the first edge pixel point is not rendered.</p><p id="p-0095" num="0094">Thereafter, taking chromaticity of the first color as chromaticity of the first edge color. Finally, rendering the first edge pixel point into the first edge color. It may be understood that chromaticity of the first edge color is the same as chromaticity of the first color, and transparency of the first edge color is higher than transparency of the first color; in this way, as the distance from the first pixel point increases, transparency of the first edge pixel point is getting higher and higher, thereby creating a gradient and neon effect, which is shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0096" num="0095">Similarly, an implementation mode of step <b>1044</b><i>b </i>may be: firstly, determining transparency of the second edge color according to the distance between the second edge pixel point and the second pixel point, the transparency of the second edge color is positively correlated with the distance between the second edge pixel point and the second pixel point; thereafter, taking chromaticity of the second color as chromaticity of the second edge color; and finally, rendering the second edge pixel point into the second edge color.</p><p id="p-0097" num="0096">In this way, the first line segment may be rendered into the first color; as the distance from the first line segment increases, the transparency of the region in the vicinity of the first line segment is becoming higher and higher, and the second line segment may also be rendered into the second color; as the distance from the second line segment increases, the transparency the region in the vicinity of the second line segment is becoming higher and higher, thereby creating a gradient and neon effect.</p><p id="p-0098" num="0097">To sum up, the present disclosure firstly recognizes a target contour of a target object from a target image collected at a current moment; then according to a time difference between the current moment and a first moment, determines a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment in the target contour; then takes a line segment composed of contour points between the starting contour point and the split contour point as a first line segment, and takes a line segment except the first line segment in the target contour as a second line segment; finally renders the first line segment according to a first color, and renders the second line segment according to a second color. The above-described steps are repeatedly executed until the line segment composed of the contour points between the starting contour point and the final contour point is rendered into the first color. The present disclosure can implement an effect of dynamic stroke on the contour of the target object, within a certain range indicated by the starting contour position and the final contour position.</p><p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram of an image processing apparatus shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the apparatus <b>200</b> includes:</p><p id="p-0100" num="0099">A recognizing module <b>201</b>, configured to recognize a target contour of a target object in a target image collected at a current moment, the target contour includes a plurality of contour points.</p><p id="p-0101" num="0100">A first determining module <b>202</b>, configured to determine, in the target contour, a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment, according to a time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, the first moment is a moment before the current moment.</p><p id="p-0102" num="0101">A second determining module <b>203</b>, configured to take a line segment composed of the contour points between the starting contour point and the split contour point in the target contour as a first line segment, and take a line segment except the first line segment in the target contour as a second line segment.</p><p id="p-0103" num="0102">A rendering module <b>204</b>, configured to render the first line segment according to a first color, and render the second line segment according to a second color, the first color is different from the second color.</p><p id="p-0104" num="0103">Repeatedly execute a step of recognizing a target contour of a target object in a target image collected at a current moment, to a step of rendering a first line segment according to the first color, and rendering a second line segment according to the second color, until the line segment composed of the contour points between the starting contour point and the final contour point in the target contour is rendered into the first color.</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a block diagram of another image processing apparatus shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the first determining module <b>202</b> includes:</p><p id="p-0106" num="0105">A first determining sub-module <b>2021</b>, configured to determine the split contour position corresponding to the current moment according to the time difference between the current moment and the first moment and according to the starting contour position and the final contour position.</p><p id="p-0107" num="0106">A second determining sub-module <b>2022</b>, configured to determine the starting contour point, the final contour point, and the split contour point corresponding to the split contour position in the target contour, according to a target number of contour points included in the target contour.</p><p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a block diagram of another image processing apparatus shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the apparatus <b>200</b> further includes:</p><p id="p-0109" num="0108">A recognizing module <b>201</b>, further configured to recognize an initial state contour and a key point sequence of the target object in an initial image collected at the first moment, before recognizing the target contour of the target object in the target image collected at the current moment.</p><p id="p-0110" num="0109">A third determining module <b>205</b>, configured to determine correspondence between key points included in the key point sequence and contour points included in the initial state contour according to a nearest neighbor algorithm.</p><p id="p-0111" num="0110">A fourth determining module <b>206</b>, configured to determine the starting contour position corresponding to a specified starting key point in the key point sequence and the final contour position corresponding to a specified final key point according to the correspondence.</p><p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram of another image processing apparatus shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the fourth determining module <b>206</b> includes:</p><p id="p-0113" num="0112">A contour point determining sub-module <b>2061</b>, configured to determine a first contour point corresponding to the starting key point and a second contour point corresponding to the final key point in the initial state contour, according to the correspondence.</p><p id="p-0114" num="0113">A quantity determining sub-module <b>2062</b>, configured to determine a number of initial states of the contour points included in the initial state contour.</p><p id="p-0115" num="0114">A position determining sub-module <b>2063</b>, configured to take a ratio of a serial number of the first contour point to a number of initial states as the starting contour position, and take a ratio of a serial number of the second contour point to the number of initial states as the final contour position.</p><p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a block diagram of another image processing apparatus shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the rendering module <b>204</b> includes:</p><p id="p-0117" num="0116">A first rendering sub-module <b>2041</b>, configured to render each pixel point in the first line segment into the first color, and render a pixel point, a distance of which from the pixel point is less than a distance threshold, into the first color; and/or,</p><p id="p-0118" num="0117">A second rendering sub-module <b>2042</b>, configured to render each pixel point in the second line segment into the second color, and render a pixel point, a distance of which from the pixel point is less than the distance threshold, into the second color.</p><p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a block diagram of another image processing apparatus shown according to an exemplary embodiment; as shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, the rendering module <b>204</b> includes:</p><p id="p-0120" num="0119">A third rendering sub-module <b>2043</b>, configured to render a first pixel point into the first color, and render a pixel point, a distance of which from the first pixel point is less than a first distance threshold, into the first color, the first pixel point is any pixel point on the first line segment.</p><p id="p-0121" num="0120">A third rendering sub-module <b>2043</b>, further configured to take a pixel point, a distance of which from the first pixel point is greater than or equal to the first distance threshold and less than a second distance threshold, as a first edge pixel point.</p><p id="p-0122" num="0121">A third rendering sub-module <b>2043</b>, further configured to determine a first edge color according to a distance between the first edge pixel point and the first pixel point and according to the first color, and render the first edge pixel point into the first edge color.</p><p id="p-0123" num="0122">and/or,</p><p id="p-0124" num="0123">A fourth rendering sub-module <b>2044</b>, configured to render a second pixel point into the second color, and render a pixel point, a distance of which from the second pixel point is less than the first distance threshold, into the second color, the second pixel point is any pixel point on the second line segment.</p><p id="p-0125" num="0124">A fourth rendering sub-module <b>2044</b>, further configured to take a pixel point, a distance of which from the second pixel point is greater than or equal to the first distance threshold and less than the second distance threshold, as a second edge pixel point.</p><p id="p-0126" num="0125">A fourth rendering sub-module <b>2044</b>, further configured to determine a second edge color according to a distance between the second edge pixel point and the second pixel point and according to the second color, and render the second edge pixel point into the second edge color.</p><p id="p-0127" num="0126">Specifically, the third rendering sub-module <b>2043</b> is configured to execute steps of:</p><p id="p-0128" num="0127">Step <b>1</b>): determining transparency of the first edge color according to the distance between the first edge pixel point and the first pixel point, the transparency of the first edge color is positively correlated with the distance between the first edge pixel point and the first pixel point.</p><p id="p-0129" num="0128">Step <b>2</b>): taking chromaticity of the first color as chromaticity of the first edge color.</p><p id="p-0130" num="0129">Step <b>3</b>): rendering the first edge pixel point into the first edge color.</p><p id="p-0131" num="0130">The fourth rendering sub-module <b>2044</b> is configured to execute steps of:</p><p id="p-0132" num="0131">Step <b>4</b>): determining transparency of the second edge color according to the distance between the second edge pixel point and the second pixel point, the transparency of the second edge color is positively correlated with the distance between the second edge pixel point and the second pixel point.</p><p id="p-0133" num="0132">Step <b>5</b>): taking chromaticity of the second color as chromaticity of the second edge color.</p><p id="p-0134" num="0133">Step <b>6</b>): rendering the second edge pixel point into the second edge color.</p><p id="p-0135" num="0134">With respect to the apparatus according to the above-described embodiment, the specific modes in which the respective modules execute operations have been described in detail in the embodiment of the method, and no details will be repeated here.</p><p id="p-0136" num="0135">To sum up, the present disclosure firstly recognizes a target contour of a target object from a target image collected at a current moment; then according to a time difference between the current moment and a first moment, determines a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment in the target contour; then takes a line segment composed of contour points between the starting contour point and the split contour point as a first line segment, and takes a line segment except the first line segment in the target contour as a second line segment; finally renders the first line segment according to a first color, and renders the second line segment according to a second color. The above-described steps are repeatedly executed until the line segment composed of the contour points between the starting contour point and the final contour point is rendered into the first color. The present disclosure can implement an effect of dynamic stroke on the contour of the target object, within a certain range indicated by the starting contour position and the final contour position.</p><p id="p-0137" num="0136">Referring next to <figref idref="DRAWINGS">FIG. <b>16</b></figref>, it shows a schematic structural diagram of an electronic device (e.g., a terminal device or a server in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) <b>300</b> suitable for implementing an embodiment of the present disclosure. The electronic device (i.e., the executing body of the above-described image processing method) according to the embodiment of the present disclosure may be a server; for example, the server may be a local server or a cloud server, or may also be a terminal device; the terminal device may include, but not limited to, a mobile terminal such as a mobile phone, a laptop, a digital broadcast receiver, a Personal Digital Assistant (PDA), a Portable Android Device (PAD), a Portable Multimedia Player (PMP), a vehicle-mounted terminal (e.g., a vehicle-mounted navigation terminal), etc., and a stationary terminal such as a digital TV, a desktop computer, etc. The user may upload the target image or the initial image by logging in to the server, or may also directly upload the target image or the initial image through the terminal device, or collect the target image or the initial image through the terminal device. The terminal device shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is only an example, and should not impose any limitation on the function and use scope of the embodiments of the present disclosure.</p><p id="p-0138" num="0137">As shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, the electronic device <b>300</b> may include a processing apparatus (e.g., a central processing unit, a graphics processor, etc.) <b>301</b>, which may execute various appropriate actions and processing according to a program stored in a Read-Only Memory (ROM) <b>302</b> or a program loaded from a storage apparatus <b>308</b> into a Random Access Memory (RAM) <b>303</b>. The RAM <b>303</b> further stores various programs and data required for operation of the electronic device <b>300</b>. The processing apparatus <b>301</b>, the ROM <b>302</b>, and the RAM <b>303</b> are connected with each other through a bus <b>304</b>. An input/output (I/O) interface <b>305</b> is also coupled to the bus <b>304</b>.</p><p id="p-0139" num="0138">Usually, apparatuses below may be coupled to the I/O interface <b>305</b>: an input apparatuses <b>306</b> including, for example, a touch screen, a touch pad, a keyboard, a mouse, a camera, a microphone, an accelerometer, a gyroscope, etc.; an output apparatuses <b>307</b> including, for example, a Liquid Crystal Display (LCD), a speaker, a vibrator, etc.; a storage apparatus <b>308</b> including, for example, a magnetic tape, a hard disk, etc.; and a communication apparatus <b>309</b>. The communication apparatus <b>309</b> may allow the electronic device <b>300</b> to perform wireless or wired communication with other device so as to exchange data. Although <figref idref="DRAWINGS">FIG. <b>16</b></figref> shows the electronic device <b>300</b> having various apparatuses, it should be understood that it is not required to implement or have all the apparatuses shown, and the electronic device <b>300</b> may alternatively implement or have more or fewer apparatuses.</p><p id="p-0140" num="0139">Particularly, according to some embodiments of the present disclosure, the flows as described above with reference to the flow charts may be implemented as computer software programs. For example, some embodiments of the present disclosure include a computer program product, which includes a computer program carried on a computer-readable medium, and the computer program contains program codes for executing the method illustrated in the flow chart. In some such embodiments, the computer program may be downloaded and installed from the network via the communication apparatus <b>309</b>, or installed from the storage apparatus <b>308</b>, or installed from the ROM <b>302</b>. When executed by the processing apparatus <b>301</b>, the computer program executes the above-described functions defined in the methods according to some embodiments of the present disclosure.</p><p id="p-0141" num="0140">It should be noted that, in some embodiments of the present disclosure, the above-described computer-readable medium may be a computer-readable signal medium or a computer-readable storage medium, or any combination of the above two. The computer-readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the above. More specific examples of the computer-readable storage medium may include, but not limited to: an electrical connection having one or more conductors, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM); an erasable programmable read-only memory (EPROM or flash memory); an optical fiber, a portable compact disc read-only memory (CD-ROM); an optical storage apparatus; a magnetic storage apparatus; or any suitable combination of the above. In the present disclosure, the computer-readable storage medium may be any tangible medium that contains or stores a program that may be used by or in conjunction with an instruction executing system, an apparatus, or a device. In the present disclosure, the computer-readable signal medium may include a data signal propagated in baseband or as a portion of a carrier wave, which carries a computer-readable program code therein. Such propagated data signals may take a variety of forms, including but not limited to, electromagnetic signals, optical signals, or any suitable combination of the above. The computer-readable signal medium may also be any computer-readable medium other than a computer-readable storage medium; and the computer-readable signal medium may transmit, propagate, or transport programs for use by or in combination with the instruction executing system, the apparatus, or the device. The program code embodied on the computer-readable medium may be transmitted by using any suitable medium, including, but not limited to, an electrical wire, an optical cable, a radio frequency (RF), etc., or any suitable combination of the above.</p><p id="p-0142" num="0141">In some implementation modes, the client and the server may communicate with any network protocol currently known or to be researched and developed in the future such as hypertext transfer protocol (HTTP), and may communicate (via a communication network) and interconnect with digital data in any form or medium. Examples of communication networks include a local area network (LAN), a wide area network (WAN), the Internet, and an end-to-end network (e.g., an ad hoc end-to-end network), as well as any network currently known or to be researched and developed in the future.</p><p id="p-0143" num="0142">The above-described computer-readable medium may be included in the above-described electronic device, or may also exist alone without being assembled into the electronic device.</p><p id="p-0144" num="0143">The above-described computer-readable medium carries one or more programs, and when the one or more programs are executed by the electronic device, the electronic device is cause to: recognizing a target contour of a target object in a target image collected at a current moment, the target contour comprises a plurality of contour points; determining, in the target contour, a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment, according to a time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, the first moment is a moment before the current moment; taking a line segment composed of contour points between the starting contour point and the split contour point in the target contour as a first line segment, and taking a line segment except the first line segment in the target contour as a second line segment; rendering the first line segment according to a first color, and rendering the second line segment according to a second color, the first color is different from the second color; and repeatedly executing a step of recognizing a target contour of a target object in the target image collected at a current moment, to a step of rendering a first line segment according to the first color, and rendering a second line segment according to the second color, until a line segment composed of contour points between the starting contour point and the final contour point in the target contour is rendered into the first color.</p><p id="p-0145" num="0144">The computer program codes for executing the operations according to some embodiments of the present disclosure may be written in one or more programming languages or a combination thereof; the above-described programming languages include object-oriented programming languages such as Java, Smalltalk, C++, and also include conventional procedural programming languages such as &#x201c;C&#x201d; language or similar programming languages. The program code may by executed entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer, or entirely on the remote computer or server. In the scenario related to the remote computer, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet service provider).</p><p id="p-0146" num="0145">The flow chart and block diagrams in the accompanying drawings illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present disclosure. In this regard, each block in the flow chart or block diagrams may represent a module, a program segment, or a portion of codes, which comprises one or more executable instructions for implementing specified logical function. It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the accompanying drawings. For example, two blocks shown in succession may, in fact, can be executed substantially concurrently, or the blocks may sometimes be executed in a reverse order, depending upon the functionality involved. It should also be noted that, each block of the block diagrams and/or flow charts, and combinations of blocks in the block diagrams and/or flow charts, may be implemented by special purpose hardware-based systems that execute the specified functions or operations, or may also be implemented by a combination of special purpose hardware and computer instructions.</p><p id="p-0147" num="0146">The units as described in some embodiments of the present disclosure may be implemented by means of software, or may also be implemented by means of hardware. The name of the module does not constitute a limitation on the module per se under certain circumstances, for example, the first determining module may also be described as a &#x201c;module for acquiring contour points&#x201d;.</p><p id="p-0148" num="0147">The functions described herein above may be executed, at least partially, by one or more hardware logic components. For example, without limitation, available exemplary types of hardware logic components include: a field programmable gate array (FPGA), an application specific integrated circuit (ASIC), an application specific standard product (ASSP), a system on chip (SOC), a complex programmable logical device (CPLD), etc.</p><p id="p-0149" num="0148">In the context of the present disclosure, a machine-readable medium may be a tangible medium that may contain or store a program for use by an instruction execution system, apparatus or device, or may contain or store a program for use in conjunction with an instruction execution system, apparatus or device. The machine-readable medium may be a machine-readable signal medium or a machine-readable storage medium. Machine-readable media may include, but are not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the above. More specific examples of the machine-readable storage medium may include: an electrical connection having one or more conductors, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM); an erasable programmable read-only memory (EPROM or flash memory); an optical fiber, a portable compact disc read-only memory (CD-ROM); an optical storage apparatus; a magnetic storage apparatus; or any suitable combination of the above.</p><p id="p-0150" num="0149">According to one or more embodiments of the present disclosure, example 1 provides an image processing method, which comprises: recognizing a target contour of a target object in a target image collected at a current moment, the target contour comprises a plurality of contour points; determining, in the target contour, a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment, according to a time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, the first moment is a moment before the current moment; taking a line segment composed of contour points between the starting contour point and the split contour point in the target contour as a first line segment, and taking a line segment except the first line segment in the target contour as a second line segment; rendering the first line segment according to a first color, and rendering the second line segment according to a second color, the first color is different from the second color; and repeatedly executing a step of recognizing a target contour of a target object in the target image collected at a current moment, to a step of rendering a first line segment according to the first color, and rendering a second line segment according to the second color, until a line segment composed of contour points between the starting contour point and the final contour point in the target contour is rendered into the first color.</p><p id="p-0151" num="0150">According to one or more embodiments of the present disclosure, example 2 provides the method of example 1, the determining, in the target contour, the starting contour point corresponding to the starting contour position, the final contour point corresponding to the final contour position, and the split contour point corresponding to the current moment, according to the time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, comprises: determining the split contour position corresponding to the current moment according to the time difference between the current moment and the first moment and according to the starting contour position and the final contour position; and determining the starting contour point, the final contour point, and the split contour point corresponding to the split contour position in the target contour, according to a target number of contour points included in the target contour.</p><p id="p-0152" num="0151">According to one or more embodiments of the present disclosure, example 3 provides the method of example 1, before the recognizing a target contour of a target object in a target image collected at a current moment, the method further comprises: recognizing an initial state contour and a key point sequence of the target object in an initial image collected at the first moment; determining correspondence between key points included in the key point sequence and contour points included in the initial state contour according to a nearest neighbor algorithm; and determining the starting contour position corresponding to a specified starting key point in the key point sequence and the final contour position corresponding to a specified final key point according to the correspondence.</p><p id="p-0153" num="0152">According to one or more embodiments of the present disclosure, example 4 provides the method of example 3, the determining the starting contour position corresponding to the specified starting key point in the key point sequence and the final contour position corresponding to the specified final key point according to the correspondence, comprises: determining a first contour point corresponding to the starting key point and a second contour point corresponding to the final key point in the initial state contour, according to the correspondence; determining a number of initial states of the contour points included in the initial state contour; and taking a ratio of a serial number of the first contour point to the number of initial states as the starting contour position, and taking a ratio of a serial number of the second contour point to the number of initial states as the final contour position.</p><p id="p-0154" num="0153">According to one or more embodiments of the present disclosure, example 5 provides the method of any one of example 1 to example 4, the rendering the first line segment according to the first color, and rendering the second line segment according to the second color, comprises: rendering each pixel point in the first line segment into the first color, and rendering a pixel point, a distance of which from the pixel point is less than a distance threshold, into the first color; and/or, rendering each pixel point in the second line segment into the second color, and rendering a pixel point, a distance of which from the pixel point is less than the distance threshold, into the second color.</p><p id="p-0155" num="0154">According to one or more embodiments of the present disclosure, example 6 provides the method of any one of example 1 to example 4, the rendering the first line segment according to a first color, and rendering the second line segment according to a second color, comprises: rendering a first pixel point into the first color, and rendering a pixel point, a distance of which from the first pixel point is less than a first distance threshold, into the first color, the first pixel point is any pixel point on the first line segment; taking a pixel point, a distance of which from the first pixel point is greater than or equal to the first distance threshold and less than a second distance threshold, as a first edge pixel point; and determining a first edge color according to a distance between the first edge pixel point and the first pixel point and according to the first color, and rendering the first edge pixel point into the first edge color; and/or, rendering a second pixel point into the second color, and rendering a pixel point, a distance of which from the second pixel point is less than the first distance threshold, into the second color, the second pixel point is any pixel point on the second line segment; taking a pixel point, a distance of which from the second pixel point is greater than or equal to the first distance threshold and less than the second distance threshold, as a second edge pixel point; and determining a second edge color according to a distance between the second edge pixel point and the second pixel point, and according to the second color, and rendering the second edge pixel point into the second edge color.</p><p id="p-0156" num="0155">According to one or more embodiments of the present disclosure, example 7 provides the method of example 6, the determining the first edge color according to the distance between the first edge pixel point and the first pixel point, and the first color, and the rendering the first edge pixel point into the first edge color, comprises: determining transparency of the first edge color according to the distance between the first edge pixel point and the first pixel point, the transparency of the first edge color is positively correlated with the distance between the first edge pixel point and the first pixel point; taking chromaticity of the first color as chromaticity of the first edge color; and rendering the first edge pixel point into the first edge color; and the determining a second edge color according to the distance between the second edge pixel point and the second pixel point, and the second color, and the rendering the second edge pixel point into the second edge color, comprises: determining transparency of the second edge color according to the distance between the second edge pixel point and the second pixel point, the transparency of the second edge color is positively correlated with the distance between the second edge pixel point and the second pixel point; taking chromaticity of the second color as chromaticity of the second edge color; and rendering the second edge pixel point into the second edge color.</p><p id="p-0157" num="0156">According to one or more embodiments of the present disclosure, example 8 provides an image processing apparatus, which comprises: a recognizing module, configured to recognize a target contour of a target object in a target image collected at a current moment, the target contour comprises a plurality of contour points; a first determining module, configured to determine, in the target contour, a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment, according to a time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, the first moment is a moment before the current moment; a second determining module, configured to take a line segment composed of contour points between the starting contour point and the split contour point in the target contour as a first line segment, and take a line segment except the first line segment in the target contour as a second line segment; and a rendering module, configured to render the first line segment according to a first color, and render the second line segment according to a second color, the first color is different from the second color; repeatedly executing a step of recognizing a target contour of a target object in a target image collected at a current moment, to a step of rendering a first line segment according to the first color, and rendering a second line segment according to the second color, until a line segment composed of the contour points between the starting contour point and the final contour point in the target contour is rendered into the first color.</p><p id="p-0158" num="0157">According to one or more embodiments of the present disclosure, example 9 provides a computer-readable medium, on which a computer program is stored, when executed by a processing apparatus, the program implements steps of the method according to any one of example 1 to example 7.</p><p id="p-0159" num="0158">According to one or more embodiments of the present disclosure, example 10 provides an electronic device, which comprises: a storage apparatus, on which a computer program is stored; a processing apparatus, configured to execute the computer program in the storage apparatus, to implement steps of the method according to any one of example 1 to example 7.</p><p id="p-0160" num="0159">According to one or more embodiments of the present disclosure, example 11 provides a computer program, comprising program codes, when a computer runs the computer program, the program codes execute steps of the method according to any one of example 1 to example 7.</p><p id="p-0161" num="0160">The above descriptions are merely some preferred embodiments of the present disclosure and illustrations of the applied technical principles. Those skilled in the art should understand that the scope of the invention involved in the embodiments of the present disclosure is not limited to the technical solution formed by the specific combination of the above-mentioned technical features, and should also cover, without departing from the above-mentioned inventive concept, other technical solutions formed by any combination of the above-mentioned technical features or their equivalent features, for example, the technical solution formed by replacing the above features with the technical features with similar functions disclosed in the embodiments of the present disclosure (but not limited to).</p><p id="p-0162" num="0161">Additionally, although operations are depicted in a specific order, it should not be construed as requiring that the operations be executed in the specific order as shown or in a sequential order. Under certain circumstances, multitasking and parallel processing may be advantageous. Likewise, although the above discussion contains several implementation-specific details, these should not be construed as limitations on the scope of the present disclosure. Certain features that are described in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment may also be implemented in various embodiments separately or in any suitable subcombination.</p><p id="p-0163" num="0162">Although the subject matter has been described in language specific to structural features and/or logical actions of methods, it should be understood that the subject matters defined in the appended claims are not necessarily limited to the specific features or actions described above. Conversely, the specific features and actions described above are merely example forms for implementing the claims. With respect to the apparatus in the above embodiments, the specific mode in which each module executes the operation has been described in detail in the embodiments of the method, and no details will be repeated here.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image processing method, comprising:<claim-text>recognizing a target contour of a target object in a target image collected at a current moment, wherein the target contour comprises a plurality of contour points;</claim-text><claim-text>determining, in the target contour, a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment, according to a time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, wherein the first moment is a moment before the current moment;</claim-text><claim-text>taking a line segment composed of contour points between the starting contour point and the split contour point in the target contour as a first line segment, and taking a line segment except the first line segment in the target contour as a second line segment; and</claim-text><claim-text>rendering the first line segment according to a first color, and rendering the second line segment according to a second color, wherein the first color is different from the second color.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, the determining, in the target contour, the starting contour point corresponding to the starting contour position, the final contour point corresponding to the final contour position, and the split contour point corresponding to the current moment, according to the time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, comprises:<claim-text>determining the split contour position corresponding to the current moment according to the time difference between the current moment and the first moment and according to the starting contour position and the final contour position; and</claim-text><claim-text>determining the starting contour point, the final contour point, and the split contour point corresponding to the split contour position in the target contour, according to a target number of contour points included in the target contour.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, before the recognizing a target contour of a target object in a target image collected at a current moment, the method further comprises:<claim-text>recognizing an initial state contour and a key point sequence of the target object in an initial image collected at the first moment;</claim-text><claim-text>determining correspondence between key points included in the key point sequence and contour points included in the initial state contour according to a nearest neighbor algorithm; and</claim-text><claim-text>determining the starting contour position corresponding to a specified starting key point in the key point sequence and the final contour position corresponding to a specified final key point according to the correspondence.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein, the determining the starting contour position corresponding to the specified starting key point in the key point sequence and the final contour position corresponding to the specified final key point according to the correspondence, comprises:<claim-text>determining a first contour point corresponding to the starting key point and a second contour point corresponding to the final key point in the initial state contour, according to the correspondence;</claim-text><claim-text>determining a number of initial states of the contour points included in the initial state contour; and</claim-text><claim-text>taking a ratio of a serial number of the first contour point to the number of initial states as the starting contour position, and taking a ratio of a serial number of the second contour point to the number of initial states as the final contour position.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, the rendering the first line segment according to the first color, and rendering the second line segment according to the second color, comprises:<claim-text>rendering each pixel point in the first line segment into the first color, and rendering a pixel point, a distance of which from the pixel point is less than a distance threshold, into the first color; and/or,</claim-text><claim-text>rendering each pixel point in the second line segment into the second color, and rendering a pixel point, a distance of which from the pixel point is less than the distance threshold, into the second color.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, the rendering the first line segment according to a first color, and rendering the second line segment according to a second color, comprises:<claim-text>rendering a first pixel point into the first color, and rendering a pixel point, a distance of which from the first pixel point is less than a first distance threshold, into the first color, wherein the first pixel point is any pixel point on the first line segment;</claim-text><claim-text>taking a pixel point, a distance of which from the first pixel point is greater than or equal to the first distance threshold and less than a second distance threshold, as a first edge pixel point; and</claim-text><claim-text>determining a first edge color according to a distance between the first edge pixel point and the first pixel point and according to the first color, and rendering the first edge pixel point into the first edge color;</claim-text><claim-text>and/or,</claim-text><claim-text>rendering a second pixel point into the second color, and rendering a pixel point, a distance of which from the second pixel point is less than the first distance threshold, into the second color, wherein the second pixel point is any pixel point on the second line segment;</claim-text><claim-text>taking a pixel point, a distance of which from the second pixel point is greater than or equal to the first distance threshold and less than the second distance threshold, as a second edge pixel point; and</claim-text><claim-text>determining a second edge color according to a distance between the second edge pixel point and the second pixel point, and according to the second color, and rendering the second edge pixel point into the second edge color.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein, the determining the first edge color according to the distance between the first edge pixel point and the first pixel point, and the first color, and the rendering the first edge pixel point into the first edge color, comprises:<claim-text>determining transparency of the first edge color according to the distance between the first edge pixel point and the first pixel point, wherein the transparency of the first edge color is positively correlated with the distance between the first edge pixel point and the first pixel point;</claim-text><claim-text>taking chromaticity of the first color as chromaticity of the first edge color; and</claim-text><claim-text>rendering the first edge pixel point into the first edge color; and</claim-text><claim-text>the determining a second edge color according to the distance between the second edge pixel point and the second pixel point, and the second color, and the rendering the second edge pixel point into the second edge color, comprises:</claim-text><claim-text>determining transparency of the second edge color according to the distance between the second edge pixel point and the second pixel point, wherein the transparency of the second edge color is positively correlated with the distance between the second edge pixel point and the second pixel point;</claim-text><claim-text>taking chromaticity of the second color as chromaticity of the second edge color; and</claim-text><claim-text>rendering the second edge pixel point into the second edge color.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. An image processing apparatus, comprising:<claim-text>a recognizing module, configured to recognize a target contour of a target object in a target image collected at a current moment, wherein the target contour comprises a plurality of contour points;</claim-text><claim-text>a first determining module, configured to determine, in the target contour, a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment, according to a time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, wherein the first moment is a moment before the current moment;</claim-text><claim-text>a second determining module, configured to take a line segment composed of contour points between the starting contour point and the split contour point in the target contour as a first line segment, and take a line segment except the first line segment in the target contour as a second line segment; and</claim-text><claim-text>a rendering module, configured to render the first line segment according to a first color, and render the second line segment according to a second color, wherein the first color is different from the second color.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A computer-readable medium, on which a computer program is stored, wherein, when executed by a processing apparatus, the program implements a method comprising:<claim-text>recognizing a target contour of a target obj ect in a target image collected at a current moment, wherein the target contour comprises a plurality of contour points;</claim-text><claim-text>determining, in the target contour, a starting contour point corresponding to a starting contour position, a final contour point corresponding to a final contour position, and a split contour point corresponding to the current moment, according to a time difference between the current moment and a first moment, and according to the starting contour position and the final contour position determined at the first moment, wherein the first moment is a moment before the current moment;</claim-text><claim-text>taking a line segment composed of contour points between the starting contour point and the split contour point in the target contour as a first line segment, and taking a line segment except the first line segment in the target contour as a second line segment; and</claim-text><claim-text>rendering the first line segment according to a first color, and rendering the second line segment according to a second color, wherein the first color is different from the second color.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An electronic device, comprising:<claim-text>a storage apparatus, on which a computer program is stored; and</claim-text><claim-text>a processing apparatus, configured to execute the computer program in the storage apparatus, to implement steps of the method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A computer program, comprising program codes, wherein, when a computer runs the computer program, the program codes execute steps of the method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein, before the recognizing a target contour of a target object in a target image collected at a current moment, the method further comprises:<claim-text>recognizing an initial state contour and a key point sequence of the target object in an initial image collected at the first moment;</claim-text><claim-text>determining correspondence between key points included in the key point sequence and contour points included in the initial state contour according to a nearest neighbor algorithm; and</claim-text><claim-text>determining the starting contour position corresponding to a specified starting key point in the key point sequence and the final contour position corresponding to a specified final key point according to the correspondence.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein, the determining the starting contour position corresponding to the specified starting key point in the key point sequence and the final contour position corresponding to the specified final key point according to the correspondence, comprises:<claim-text>determining a first contour point corresponding to the starting key point and a second contour point corresponding to the final key point in the initial state contour, according to the correspondence;</claim-text><claim-text>determining a number of initial states of the contour points included in the initial state contour; and</claim-text><claim-text>taking a ratio of a serial number of the first contour point to the number of initial states as the starting contour position, and taking a ratio of a serial number of the second contour point to the number of initial states as the final contour position.</claim-text></claim-text></claim></claims></us-patent-application>