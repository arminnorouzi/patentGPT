<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004386A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004386</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17892807</doc-number><date>20220822</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>13</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>38</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>3001</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30065</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>13</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30036</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>3824</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">NEURAL NETWORK COMPUTE TILE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16239760</doc-number><date>20190104</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11422801</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17892807</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15335769</doc-number><date>20161027</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10175980</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16239760</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Google LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Temam</last-name><first-name>Olivier</first-name><address><city>Antony</city><country>FR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Narayanaswami</last-name><first-name>Ravi</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Khaitan</last-name><first-name>Harshit</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Woo</last-name><first-name>Dong Hyuk</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A computing unit is disclosed, comprising a first memory bank for storing input activations and a second memory bank for storing parameters used in performing computations. The computing unit includes at least one cell comprising at least one multiply accumulate (&#x201c;MAC&#x201d;) operator that receives parameters from the second memory bank and performs computations. The computing unit further includes a first traversal unit that provides a control signal to the first memory bank to cause an input activation to be provided to a data bus accessible by the MAC operator. The computing unit performs one or more computations associated with at least one element of a data array, the one or more computations being performed by the MAC operator and comprising, in part, a multiply operation of the input activation received from the data bus and a parameter received from the second memory bank.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="204.13mm" wi="158.75mm" file="US20230004386A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="250.36mm" wi="182.80mm" file="US20230004386A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="247.99mm" wi="193.46mm" orientation="landscape" file="US20230004386A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="231.56mm" wi="142.41mm" orientation="landscape" file="US20230004386A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="249.09mm" wi="155.11mm" orientation="landscape" file="US20230004386A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="244.35mm" wi="170.43mm" orientation="landscape" file="US20230004386A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="222.33mm" wi="161.71mm" file="US20230004386A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation of U.S. application Ser. No. 16/239,760, filed Jan. 4, 2019, which is a continuation of U.S. application Ser. No. 15/335,769, filed Oct. 27, 2016, the contents of which are hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">This specification generally relates to a neural net compute tile for computation of Deep Neural Networks (&#x201c;DNN&#x201d;) layers that allows for reduced instruction bandwidth and instruction memory.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">In general, one innovative aspect of the subject matter described in this specification can be embodied in a computing unit for accelerating tensor computations. The computing unit comprising a first memory bank having a first data width for storing at least one of input activations or output activations, and a second memory bank having a second data width that is larger than the first data width for storing one or more parameters used in performing computations. The computing unit may further include at least one cell comprising at least one multiply accumulate (&#x201c;MAC&#x201d;) operator that receives parameters from the second memory bank and performs computations. The computing unit may still further include a first traversal unit in data communication with at least the first memory bank, the first traversal unit configured to provide a control signal to the first memory bank to cause an input activation to be provided to a data bus accessible by the MAC operator. The computing unit performs one or more computations associated with at least one element of a data array, the one or more computations being performed by the MAC operator and comprising, in part, a multiply operation of the input activation received from the data bus and a parameter received from the second memory bank.</p><p id="p-0005" num="0004">Another innovative aspect of the subject matter described in this specification can be embodied in a computer-implemented method for accelerating tensor computations. The computer-implemented method includes, sending, by a first memory bank having a first data width, a first input activation in response to the first memory bank receiving a control signal from a first traversal unit, the first memory bank being disposed in a computing unit, and wherein the first input activation is provided by a data bus that is accessible by at least cell of the computing unit. The method may further include, receiving, by the at least one cell, one or more parameters from a second memory bank having a second data width that is larger than the first data width, and wherein the at least one cell comprises at least one multiply accumulate (&#x201c;MAC&#x201d;) operator. The method may still further include, performing, by the MAC operator, one or more computations associated with at least one element of a data array, wherein the one or more computations comprise, in part, a multiply operation of at least the first input activation accessed from the data bus and at least one parameter received from the second memory bank.</p><p id="p-0006" num="0005">Another innovative aspect of the subject matter described in this specification can be embodied in a non-transitory computer-readable storage medium. The non-transitory computer-readable storage medium comprising instructions executable by one or more processors which, upon such execution, causes the one or more processors to perform operations comprising, sending, by a first memory bank having a first data width, a first input activation in response to the first memory bank receiving a control signal from a first traversal unit, the first memory bank being disposed in a computing unit, and wherein the first input activation is provided by a data bus that is accessible by at least cell of the computing unit. The performed operations may also include, receiving, by the at least one cell, one or more parameters from a second memory bank having a second data width that is larger than the first data width, and wherein the at least one cell comprises at least one multiply accumulate (&#x201c;MAC&#x201d;) operator. The performed operations may also further include, performing, by the MAC operator, one or more computations associated with at least one element of a data array, wherein the one or more computations comprise, in part, a multiply operation of at least the first input activation accessed from the data bus and at least one parameter received from the second memory bank.</p><p id="p-0007" num="0006">The subject matter described in this specification can be implemented in particular embodiments so as to realize one or more of the following advantages. Using registers to keep track of memory address values allow a program to iterate deeply-nested loops with one instruction. A tensor accessible from narrow memory and wide memory units, in a single compute tile, is traversed based on memory address values retrieved from registers. Memory address values correspond to elements of the tensor. Tensor computations occur in individual compute tiles based on execution of deep loop nests. Computations can be distributed across multiple tiles. Computational efficiency is enhanced and accelerated based on distributing tensor computations for a multilayer neural network across several compute tiles. Tensors can be traversed and tensor computations can be performed with a reduced number of instructions.</p><p id="p-0008" num="0007">The subject matter described in this specification can also be implemented in particular embodiments so as to realize other advantages. For example, by employing a memory hierarchy that couples a narrow low bandwidth memory that allows addressing flexibility to traverse a multi-dimensional array in any order with a high bandwidth wide memory, high utilization of the MAC operators can be achieved for DNN layers of very different dimensions and locality in computation can be maximally exploited.</p><p id="p-0009" num="0008">Other implementations of this and other aspects include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices. A system of one or more computers can be so configured by virtue of software, firmware, hardware, or a combination of them installed on the system that in operation cause the system to perform the actions. One or more computer programs can be so configured by virtue of having instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.</p><p id="p-0010" num="0009">The details of one or more implementations of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an example computation system.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example neural network compute tile.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example Tensor Traversal Unit (TTU) structure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example architecture that includes a narrow memory unit providing input activations to one or more multiply accumulate (MAC) operators.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example architecture that includes an output bus providing output activations to the narrow memory unit of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an example flow chart of process for performing tensor computations using the neural network compute tile of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0017" num="0016">Like reference numbers and designations in the various drawings indicate like elements.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0018" num="0017">The subject matter described in this specification relates to a hardware computing system including multiple computing units configured to accelerate machine learning inference workloads of a neural network layer. Each computing unit of the hardware computing system is self-contained and can independently execute computations required by a given layer of a multi-layer neural network.</p><p id="p-0019" num="0018">A neural network having multiple layers can be used to compute inferences. For example, given an input, the neural network can compute an inference for the input. The neural network computes this inference by processing the input through each of the layers of the neural network. In particular, the layers of the neural network each have a respective set of weights. Each layer receives an input and processes the input in accordance with the set of weights for the layer to generate an output.</p><p id="p-0020" num="0019">Therefore, in order to compute an inference from a received input, the neural network receives the input and processes it through each of the neural network layers in order to generate the inference, with the output from one neural network layer being provided as input to the next neural network layer. Data inputs to a neural network layer, e.g., either the input to the neural network or the outputs of the layer below the layer in the sequence, can be referred to as activation inputs to the layer.</p><p id="p-0021" num="0020">In some implementations, the layers of the neural network are arranged in a sequence. In other implementations, the layers are arranged in a directed graph. That is, any particular layer can receive multiple inputs, multiple outputs, or both. The layers of the neural network can also be arranged such that an output of a layer can be sent back as an input to a previous layer.</p><p id="p-0022" num="0021">The hardware computing system described in this specification can perform the computation of a neural network layer by distributing tensor computations across multiple compute tiles. A computation process performed within a neural network layer may include a multiplication of an input tensor including input activations with a parameter tensor including weights. The computation includes multiplying an input activation with a weight on one or more cycles and performing an accumulation of a products over many cycles.</p><p id="p-0023" num="0022">A tensor is a multi-dimensional geometric object and example multi-dimensional geometric objects include matrices and data arrays. In general, a software algorithm is executed by a computing tile to perform tensor computations by processing a nested loop to traverse an N-dimensional tensor. In one example computational process, each loop may be responsible for traversing a particular dimension of the N-dimensional tensor. For a given tensor construct, a compute tile may require access to an element of a particular tensor to execute a plurality of dot product computations associated with the tensor. Computation occurs when an input activation provided by a narrow memory structure is multiplied with a parameter or weight provided by a wide memory structure. Because the tensor is stored in a memory, a set of tensor indices may require translation to a set of memory addresses. In general, a tensor traversal unit of a compute tile executes control operations that provide the index of each dimension associated with the tensor and order in which index elements are traversed to perform computations. Tensor computations end when multiplication results are written to an output bus and stored in memory.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a block diagram of an example computing system <b>100</b> for accelerating tensor computations associated with deep neural networks (DNNs). The system <b>100</b> generally includes a controller <b>102</b>, a host interface <b>108</b>, an input/output (I/O) link <b>110</b>, multiple tiles including a first tile set <b>112</b> and a second tile set <b>114</b>, a classifier portion <b>116</b>, and data buses identified in a bus map <b>118</b> (which is shown for clarity, but is not included in the system <b>100</b>). Controller <b>102</b> generally includes data memory <b>104</b>, instruction memory <b>106</b>, and at least one processor configured to execute one or more instructions encoded in a computer readable storage medium. Instruction memory <b>106</b> may store one or more machine readable instructions that are executable by the one or more processors of controller <b>102</b>. Data memory <b>104</b> may be any of a variety of data storage mediums for storing and subsequently accessing a variety of data relating to computations that occur within system <b>100</b>.</p><p id="p-0025" num="0024">Controller <b>102</b> is configured to execute one or more instructions relating to tensor computations within system <b>100</b>, including instructions stored in instruction memory <b>106</b>. In some implementations, data memory <b>104</b> and instruction memory <b>106</b> are volatile memory unit or units. In some other implementations, data memory <b>104</b> and instruction memory <b>106</b> are non-volatile memory unit or units. Data memory <b>104</b> and instruction memory <b>106</b> may also be another form of computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In various implementations, controller <b>102</b> may also be referenced or referred to as core manager <b>102</b>.</p><p id="p-0026" num="0025">As depicted, host interface <b>108</b> is coupled to I/O link <b>110</b>, controller <b>102</b>, and classifier portion <b>116</b>. Host interface <b>108</b> receives instructions and data parameters from I/O link <b>110</b> and provides instructions and parameters to controller <b>102</b>. In general, instructions can be provided to one or more devices in system <b>100</b> through instruction bus <b>124</b> (described below) and parameters can be provided to one or more devices in system <b>100</b> through ring bus <b>128</b> (described below). In some implementations, instructions are received by controller <b>102</b> from host interface <b>118</b> at an initial time and stored in instruction memory <b>106</b> for execution by controller <b>102</b> at a later time.</p><p id="p-0027" num="0026">Classifier portion <b>116</b> is likewise coupled to controller <b>102</b> and tile 7 of second tile set <b>114</b>. In some implementations, classifier portion <b>116</b> is implemented as a separate tile within the system <b>100</b>. In alternative implementations, classifier portion <b>116</b> is disposed or located within controller <b>102</b> as a sub-circuit or sub-device of controller <b>102</b>. Classifier portion <b>116</b> is generally configured to perform one or more functions on accumulated pre-activation values that are received as outputs of fully connected layers. Fully connected layers may be partitioned across the tiles in tile sets <b>112</b> and <b>114</b>. Thus, each tile is configured to produce a subset of pre-activation values (i.e., linear outputs) which may be stored in a memory unit(s) of the tile. Classification results bus <b>120</b> provides a data path from classifier portion <b>116</b> to controller <b>102</b>. Data that includes post-function values (i.e., results) are provided to controller <b>102</b> from classifier portion <b>116</b> via classification results bus <b>120</b>.</p><p id="p-0028" num="0027">Bus map <b>118</b> shows data buses that provide one or more inter-connected data communication paths between tiles of first tile set <b>112</b> and second tile set <b>114</b>. Bus map <b>118</b> provides a legend for identifying a classification results bus <b>120</b>, CSR/master bus <b>122</b>, instruction bus <b>124</b>, mesh bus <b>126</b>, and ring bus <b>128</b> as depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In general, a tile is a core component within the accelerator architecture of system <b>100</b> and is the focal point for tensor computations that occur in the system. Each tile is an individual computing unit that cooperates with other tiles in the system to accelerate computations across one or more layers of a multi-layer neural network. Although tiles in tile sets <b>112</b>, <b>114</b> can share execution of tensor computations associated with a given instruction, an individual computing unit is a self-contained computational component configured to execute a subset of tensor computations independently relative other corresponding tiles within tile sets <b>112</b>, <b>114</b>.</p><p id="p-0029" num="0028">CSR bus <b>122</b> is a single master multiple slave bus that enables controller <b>102</b> to transmit one or more instructions that set program configurations and read status registers associated with one or more tiles. CSR bus <b>122</b> may be connected in a single daisy chain configuration with one master bus segment and multiple slave bus segments. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, CSR bus <b>122</b> provides communications coupling through a bus data path that connects tiles in tile sets <b>112</b>, <b>114</b> and controller <b>102</b> in a ring to host interface <b>110</b>. In some implementation, host interface <b>110</b> is the single master of the CSR bus ring and the entire CSR bus address space is memory mapped to a memory space in host interface <b>110</b>.</p><p id="p-0030" num="0029">CSR bus <b>122</b> may be used by host interface <b>110</b> to perform one or more operations including, for example, programming memory buffer pointers in controller <b>102</b> to enable controller <b>102</b> to begin fetching instructions from instruction memory <b>106</b>, updating/programming various tile settings (e.g., coefficient tables for polynomial approximation calculations) that remain static during one or more computations, and/or loading/reloading firmware to classification portion <b>116</b>. In one example, firmware reloads may include new functions to be applied to linear outputs (i.e., pre-activation values). Accordingly, every slave having access to CSR bus <b>122</b> will have a distinct node identifier (node ID) that is tied to the slave and identifies it. The node ID will be part of an instruction address and will be used, inspected or otherwise examined by the CSR slaves (i.e., controller <b>102</b>, tiles <b>112</b>, <b>114</b> and classifier <b>116</b>) to determine whether the CSR packet is addressed to the slave.</p><p id="p-0031" num="0030">In some implementations, one or more instructions can be transmitted by host interface <b>102</b> through controller <b>102</b>. The instructions may, for example, be 32-bits wide with the first 7-bits including header information indicating the instruction address/destination that is to receive and execute the instructions. The first 7-bits of the header may contain data parameters that represent a particular node ID. Slaves (e.g., each tile) on the CSR bus ring may therefore inspect the header of the instruction to determine if the request by the master (host interface <b>110</b>) was addressed to the tile inspecting the header. If the node ID of the header does not indicate that the destination is the inspecting tile, the inspecting tile will copy the input CSR instruction packet to the CSR bus input connected to the next tile for inspection by the next tile.</p><p id="p-0032" num="0031">Instruction bus <b>124</b> originates from controller <b>102</b> and, similar to CSR bus <b>122</b>, also provides communications coupling through a bus data path that connects tiles in tile sets <b>112</b>, <b>114</b> in a ring back to controller <b>102</b>. In one implementation, controller <b>102</b> broadcasts one or more instructions via instruction bus <b>124</b>. The instructions that are broadcast by controller <b>102</b> may differ from the instructions provided via CSR bus <b>122</b>. However, the manner in which a tile receives and/or consumes or executes the instruction received via bus <b>124</b> may be similar to the process for executing instructions received via CSR bus <b>122</b>.</p><p id="p-0033" num="0032">In one example, a header (i.e., a bitmap) of the instruction indicates, to a receiving tile, that the receiving tile needs to consume a particular instruction based on a bitmap associated with the instruction. The bitmap may have a particular width defined in terms of bits. The instruction is typically forwarded from one tile onto the next tile based on parameters of the instruction. In one implementation, the width of instruction bus <b>124</b> may be configured to be smaller than the size/width of the instruction. Thus, in such a configuration, transmission of the instructions will be over several cycles and bus stops of instruction bus <b>124</b> will have decoders to place instructions received at the tile in the appropriate target instruction buffer associated with that tile.</p><p id="p-0034" num="0033">As described further below, the tiles in tile sets <b>112</b>, <b>114</b> are generally configured to support two broad categories of instructions. The two broad categories may also be referred to as instruction types. The instruction types include a tensor operation (TensorOp) instruction and a direct memory access (DMAOp) instruction. In some implementations, DMAOp instructions have one or more specializations that are allowed to be concurrent. The one or more specializations may be referred to as DMAOp instruction subtypes or opcodes. In some cases, every unique and/or valid DMAOp instruction type/subtype tuple will have a separate instruction buffer within a particular tile.</p><p id="p-0035" num="0034">At a particular tile of tiles <b>112</b>, <b>114</b>, the bus stop associated with instruction bus <b>124</b> will examine the header bitmap to determine the instruction type/substype. The instruction may be received by the tile and subsequently written to an instruction buffer of the tile prior to execution of the instruction by the tile. The instruction buffer of the tile in which the instruction is written to may be determined by the type and subtype indicator/field of the instruction. The instruction buffers may include a first-in first-out (FIFO) control scheme that prioritizes consumption of one or more related instructions. Thus, under this FIFO control scheme, instructions of the same type/subtype will always be executed in the order in which the instruction arrived on the instruction bus.</p><p id="p-0036" num="0035">The different instruction buffers within a tile are the TensorOp instruction buffers and the DMAOp instruction buffers. As indicated above, instruction types include the TensorOp instruction and the DMAOp instruction. With regard to DMAOp instructions, instruction subtypes (indicating a &#x2018;write-to&#x2019; buffer location) include the following: 1) mesh inbound instruction buffer; 2) mesh outbound instruction buffer; 3) narrow-wide DMA instruction buffer; 4) wide-narrow DMA instruction buffer; and 5) ring bus DMA instruction buffer. These buffer locations will be described in more detail below with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Wide and narrow designations are used throughout the specification and generally refer to an approximate size in width (bits/bytes) of one or more memory units. As used herein, &#x201c;narrow&#x201d; may refer to one or more memory units each having a size or width of less than 16-bits and &#x201c;wide&#x201d; may refer to one or more memory units each having a size or width or less than 64-bits.</p><p id="p-0037" num="0036">Mesh bus <b>126</b> provides a data communications path that is distinct from CSR bus <b>122</b>, instruction bus <b>124</b>, and ring bus <b>128</b> (described below). As depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, mesh bus <b>126</b> provides a communications path that couples or connects each tile to its corresponding neighbor tile in both the X and Y dimensions. In various implementations, mesh bus <b>126</b> may be used to transport input activation quantities between one or more narrow memory units in adjacent tiles. As shown, mesh bus <b>126</b> does not allow direct forwarding of input activation data to non-adjacent tiles.</p><p id="p-0038" num="0037">In various implementations, mesh bus <b>126</b> and the various tiles connected via mesh bus <b>126</b> may have the following configuration. Four corner tiles of the mesh have two outbound ports and two inbound ports. Four edge tiles of the mesh have three inbound ports and three outbound ports. All non-edge, non-corner tiles have four inbound ports and four outbound ports. In general, given an example N&#xd7;N tile layout, edge tiles are tiles with only three neighbor tiles while corner tiles are tiles with two neighbor tiles. Regarding data flow methodology via mesh bus <b>126</b>, in general, every input activation that arrives via mesh bus <b>126</b> for a particular tile must be committed to one or more narrow memory units of the tile. Moreover, for tile configurations that have fewer than four inbound ports, DMAOp instructions may write zero values to the locations in the tile's narrow memory instead of waiting for data on an absent input port. Likewise, for tile configurations that have fewer than four outbound ports, DMAOp instructions will not execute the narrow memory reads and port writes related to transfers for any absent ports.</p><p id="p-0039" num="0038">In some implementations, a location or address of a narrow memory unit(s) that a particular input activation will be written to, or read from, will be generated by a Tensor Traversal Unit (hereinafter &#x201c;TTU&#x201d;) based on inbound/outbound DMAOp provided via mesh bus <b>126</b>. An inbound DMAOp and an outbound DMAOp may be executed concurrently and any required synchronization will be managed through sync flag control schemes administered by controller <b>102</b>. TTUs are described in further detail below with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0040" num="0039">Ring bus <b>128</b> originates from controller <b>102</b> and, similar to CSR bus <b>122</b> and instruction bus <b>124</b>, also provides communications coupling through a bus data path that connects tiles <b>112</b>, <b>114</b> in a ring back to controller <b>102</b>. In various implementations, ring bus <b>128</b> generally connects or couples all wide memory units (described in more detail below with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>) in all tiles <b>112</b>, <b>114</b>. Thus, a payload width of ring bus <b>128</b> corresponds to the width of the wide memory units disposed within each tile of tile sets <b>112</b>, <b>114</b>. As discussed above, ring bus <b>128</b> also includes a bitmap header indicating the tiles that need to consume payload data comprising instructions or parameters communicated via ring bus <b>128</b>.</p><p id="p-0041" num="0040">With regard to data (i.e., payload) received at a particular tile via ring bus <b>128</b>, in response to receiving the information, each tile will zero (i.e., clear out) position data indicated in the bitmap header that is unique to the receiving tile before forwarding the data on to another tile. Hence, when the header bitmap has no remaining bit set data indicating a particular tile that is to receive the payload, forwarding of the payload to another tile will stop. Payload data generally refers to activations and weights used by one or more tiles during tensor computations performed based on execution of deeply nested loops.</p><p id="p-0042" num="0041">In some implementations, controller <b>102</b> may be described as being a part of ring bus <b>128</b>. In one example, for DMAOp instructions executed within a particular tile, controller <b>102</b> may be used to pop the data/payload from ring bus stops and forward the payload to a ring bus stop in a next tile in the ring. Controller <b>102</b> may also cause the payload data to be committed to one or more wide memory units of the tile if such action is required by instructions in the bitmap header. The address of the one or more wide memory units to which the data needs to be written may be generated by DMAOp instructions within the particular tile.</p><p id="p-0043" num="0042">In various implementations, each tile of tile set <b>112</b>, <b>114</b> can either be a producer of payload data or a consumer of payload data. When a tile is a producer of payload data the tile reads the data from one or more of its wide memory units and multicasts the data over ring bus <b>128</b> for consumption by one or more other tiles. When a tile is a consumer of payload data the tile receives and writes the data to one or more wide memory units within the tile and forwards the payload data for consumption by one or more other tiles. With regard to movement of payload data via ring bus <b>128</b>, there typically will only be one producer/master of data on ring bus <b>128</b> at any given time. The DMAOp instruction execution order (e.g., FIFO control scheme) in all tiles will ensure there is only one producer/master of data on ring bus <b>128</b> at a given time.</p><p id="p-0044" num="0043">In some implementations, controller <b>102</b> uses a sync flag control architecture to ensure there is only one producer/master of payload data on ring bus <b>128</b> at a given time. In one example, every write by a tile to a ring output will trigger an increment of the corresponding sync flag count. Controller <b>102</b> may examine the payload data to determine the number of data chunks or segments that comprise the payload. Controller <b>102</b> then monitors execution by the tile to ensure the expected number of data segments are forwarded and/or consumed by the tile before another tile executes in master mode.</p><p id="p-0045" num="0044">An exception to ensuring there is only one producer/master of data on ring bus <b>128</b> at a given time occurs when there are local multicast groups connected via ring bus <b>128</b> that do not have an overlapping region on the ring bus. For example, tile 0 (master) may multicast (i.e., produce data) to a tile in Tile 0-Tile 3 grouping, while Tile 4 (master) may do the same to a tile in Tile 4-Tile 7 grouping. An important requirement of this dual master multicast methodology is that different multicast groups must not be allowed to see each other's data packets because packet overlap may occur and lead to one or more data computation errors.</p><p id="p-0046" num="0045">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, controller <b>102</b> provides a communications data path that couples or connects tiles in tile sets <b>112</b>, <b>114</b> to I/O <b>110</b> and includes several core functions. The core functions of controller <b>102</b> generally include feeding one or more I/O input activations to tiles in tile sets <b>112</b>, <b>114</b> feeding one or more input activations and parameters received from I/O <b>110</b> to the tiles, feeding one or more instructions received from I/O <b>110</b> to the tiles, sending I/O output activations to host interface <b>108</b>, and serving as a ring stop for CSR bus <b>122</b> as well as ring bus <b>128</b>. As described in more detail below, first tile set <b>112</b> and second tile set <b>114</b> each include multiple tiles that are used to perform one or more tensor computations that are executed based on a deep loop nest comprised of inner and outer loops.</p><p id="p-0047" num="0046">System <b>100</b> generally operates as follows. Host interface <b>108</b> will provide one or more instructions to controller <b>102</b> that define direct memory access operations (DMAOp) that occur for a given computation. Descriptors associated with instructions fed to controller <b>102</b> will include information required by the controller to facilitate large scale dot product computations associated with multi-dimensional data arrays (tensors). In general, controller <b>102</b> receives, from host interface <b>108</b>, input activations, tile instructions, and model parameters (i.e., weights) for executing tensor computations for a given layer of a neural network. Controller <b>102</b> may then cause the instructions to be multicast to tiles <b>112</b>, <b>114</b> in a data flow manner defined by the instruction(s). As discussed above, tiles consuming an instruction may then initiate a broadcast of a new/subsequent instruction to another tile based on bitmap data in the instruction header.</p><p id="p-0048" num="0047">With regard to data flow, input activations and parameters are transmitted to tiles of tile sets <b>112</b>, <b>114</b> via ring bus <b>128</b>. Each of tiles <b>112</b>, <b>114</b> will store a subset of the input activations needed to compute a subset of output activations that are assigned to that particular tile. DMAOp instructions for a tile will cause the input activation to be moved from wide memory to narrow memory. Computation within a tile begins when required input activations, parameters/weights and computation instructions (TTU operations, memory addresses, etc.) are available in the tile. Computations occurring within a tile ends when MAC operators (described below) within a tile complete all dot product operations defined by the instruction set and pre-activation functions are applied to the results (i.e., output activations) of the multiplication operations.</p><p id="p-0049" num="0048">Results of the one or more tensor computations include writing output activations of a compute layer to a narrow memory unit(s) of the tile performing the computation. For certain tensor computations, there will be a transfer of output edge activations to neighboring tiles via mesh bus <b>126</b>. Transfer of output edge activations to neighboring tiles are required to compute output activations for a subsequent layer when computations span multiple layers. When computations for all layers are complete, a DMAOp will move final activations to classifier tile <b>116</b> through ring bus <b>128</b>. Controller <b>102</b> will then read final activations from classifier tile <b>116</b> and execute a DMAOp to move the final activations to host interface <b>108</b>. In some implementations, classifier tile <b>116</b> performs computations of an output layer (i.e., the last layer) of the NN. In other implementations, the output layer of the NN is one of classifier layer, a regression layer, or another layer type that is generally associated with neural networks.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example neural network (NN) compute tile <b>200</b>. Generally, the example tile <b>200</b> may correspond to any of the tiles within first tile set <b>112</b> and second tile set <b>114</b> discussed above with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In various implementations, compute tile <b>200</b> may also be referenced or referred to as computing unit <b>200</b>. Each compute tile <b>200</b> is a self-contained computational unit configured to execute instructions independently relative other corresponding tiles within tile sets <b>112</b>, <b>114</b>. As discussed briefly above, each compute tile <b>200</b> executes two types of instructions, a TensorOp instruction and a DMAOp instruction. In general, each instruction type will include compute operations associated with deep loop nests and thus each instruction type will generally execute over multiple time epochs to ensure completion of all loop iterations.</p><p id="p-0051" num="0050">As discussed in more detail below, the different instruction types are executed by independent control units within compute tile <b>200</b> that synchronize on data through sync flag controls that are managed within compute tile <b>200</b>. The sync flag controls manage concurrency between executions of different instruction types within compute tile <b>200</b>. Each compute operation associated with each instruction type will be executed in strict order of issuance (i.e., First-In First-Out). With regard to the two instruction types, TensorOP and DMAOp, there are no ordering guarantees between these different instruction types and each type is treated by compute tile <b>200</b> as a separate thread of control.</p><p id="p-0052" num="0051">With regard to data flow constructs, compute tile <b>200</b> generally includes data path <b>202</b> and data path <b>205</b> that each provide a communications path for data flow into and out of compute tile <b>200</b>. As described above, system <b>100</b> includes three distinct data bus structures that are laid out in a ring configuration&#x2014;CSR bus <b>122</b>, instruction bus <b>124</b>, and ring bus <b>128</b>. Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, data path <b>205</b> corresponds to instruction bus <b>124</b>, while data path <b>202</b> generally corresponds to one of CSR bus <b>122</b> and ring bus <b>128</b>. As shown, data path <b>202</b> includes a ring output <b>203</b> providing an output path for data leaving compute tile <b>200</b> and a ring input <b>204</b> providing an input path for data entering compute tile <b>200</b>.</p><p id="p-0053" num="0052">Compute tile <b>200</b> further includes a TensorOp control <b>206</b> including a TensorOp tensor traversal unit (TTU) <b>226</b> and a DMAOp control <b>208</b> including a DMAOp TTU <b>228</b>. TensorOp control <b>206</b> generally manages writes to and reads from TensorOp TTU register <b>232</b> and administers traversal operations for execution by TensorOp TTU <b>226</b>. Likewise, DMAOp control <b>208</b> generally manages writes to and reads from DMAOp TTU register <b>234</b> and administers traversal operations for execution by DMAOp TTU <b>228</b>. TTU register <b>232</b> includes instruction buffers for storing one or more instructions comprising operations to be performed by TensorOp TTU <b>226</b> upon execution of the instructions by TensorOp control <b>206</b>. Likewise, TTU register <b>234</b> includes instruction buffers for storing one or more instructions comprising operations to be performed by TTU <b>208</b> upon execution of the instructions by DMAOp control <b>208</b>. As described further below, TTUs are used by compute tile <b>200</b> to traverse array elements of one or more tensors that generally reside in narrow memory <b>210</b> and wide memory <b>212</b>.</p><p id="p-0054" num="0053">In some implementations, certain instructions for execution by compute tile <b>200</b> arrive at the tile via data path <b>205</b> (i.e., a portion of instruction bus <b>124</b>). Compute tile <b>200</b> will examine the header bitmap to determine the instruction type (TensorOp or DMAOp) and the instruction substype (read operation or write operation). Instruction(s) received by compute tile <b>200</b> are subsequently written to a particular instruction buffer depending on the instruction type. In general, instructions are received and stored (i.e., written to the buffer) prior to execution of the instruction by a component of compute tile <b>200</b>. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the instruction buffers (i.e., TensorOp TTU register <b>232</b> and DMAOp TTU register <b>234</b>) may each include a first-in first-out (FIFO) control scheme that prioritizes consumption (execution) of one or more related instructions.</p><p id="p-0055" num="0054">As discussed briefly above, a tensor is a multi-dimensional geometric object and example multi-dimensional geometric objects include matrices and data arrays. An algorithm, including deeply nested loops, may be executed by compute tile <b>200</b> to perform tensor computations by iterating one or more nested loops to traverse an N-dimensional tensor. In one example computational process, each loop of the loop nest may be responsible for traversing a particular dimension of the N-dimensional tensor. As described herein, TensorOp control <b>206</b> generally administers one or more tensor operations that drive the sequence in which dimensional elements of a particular tensor construct are traversed and accessed to complete computations defined by the deep nested loops.</p><p id="p-0056" num="0055">Compute tile <b>200</b> further includes a narrow memory <b>210</b> and a wide memory <b>212</b>. Narrow and wide designations generally refer to a size in width (bits/bytes) of the memory units of narrow memory <b>210</b> and wide memory <b>212</b>. In some implementations, narrow memory <b>210</b> includes memory units each having a size or width of less than 16-bits and wide memory <b>212</b> includes memory units each having a size or width or less than 32-bits. Generally, compute tile <b>200</b> receives input activations via data path <b>205</b> and DMA control <b>208</b> executes an operation to write the input activations into narrow memory <b>210</b>. Likewise, compute tile <b>200</b> receives parameters (weights) via data path <b>202</b> and DMA control <b>208</b> executes an operation to write the parameters into wide memory <b>212</b>. In some implementations, narrow memory <b>210</b> can include a memory arbiter typically used in shared memory systems to decide, for each memory cycle, which control device (e.g., TensorOp control <b>206</b> or DMAOp control <b>208</b>) will be allowed to access that shared memory units of narrow memory <b>210</b>.</p><p id="p-0057" num="0056">Compute tile <b>200</b> further includes an input activation bus <b>216</b> and a MAC array <b>214</b> including multiple cells that each include a MAC operator <b>215</b> and a sum register <b>220</b>. In general, MAC array <b>214</b> executes, using MAC operators <b>215</b> and sum registers <b>220</b> across multiple cells, tensor computations that include arithmetic operations relating to dot product computations. Input activation bus <b>216</b> provides a data path in which input activations are provided, by narrow memory <b>210</b>, one-by-one for respective access by each MAC operator <b>215</b> of MAC array <b>214</b>. Hence, based on the one-by-one broadcast of an input activation, a single MAC operator <b>215</b> of a particular cell will each receive an input activation. Arithmetic operations performed by the MAC operators of the MAC array <b>214</b> generally include multiplying an input activation provided by narrow memory <b>210</b> with a parameter accessed from wide memory <b>212</b> to produce a single output activation value.</p><p id="p-0058" num="0057">During arithmetic operations, partial sums may be accumulated and stored in a corresponding, e.g., sum register <b>220</b>, or written to wide memory <b>212</b> and re-accessed by a particular cell of MAC array <b>214</b> to complete follow-on multiply operations. The tensor computations can be described as having a first portion and second portion. The first portion is complete when multiply operations produce an output activation, for example, by completing a multiplication of an input activation and a parameter to generate the output activation. The second portion includes application of a non-linear function to an output activation and the second portion is complete when the output activation is written to narrow memory <b>210</b> after application of the function.</p><p id="p-0059" num="0058">Compute tile <b>200</b> further includes an output activation bus <b>218</b>, a non-linear unit (NLU) <b>222</b> comprising an output activation pipeline <b>224</b>, an NLU control <b>238</b>, and a reference map <b>230</b> that indicates a core attribute of a component in compute tile <b>200</b>. Reference map <b>230</b> is shown for clarity, but is not included in the compute tile <b>200</b>. Core attributes include whether a particular component is a unit, a storage device, an operator, a control device or a data path. In general, upon completion of the first portion of the tensor computations, output activations are provided from MAC array <b>214</b> to NLU <b>222</b> via output activation bus <b>218</b>. After arrival at NLU <b>222</b>, data specifying an activation function, received via activation pipeline <b>224</b> is applied to the output activations and the output activations are then written to narrow memory <b>210</b>. In some implementations, output activation bus <b>218</b> includes at least one pipelined shift register <b>236</b> and completing the second portion of the tensor computations includes using a shift register <b>236</b> of activation bus <b>218</b> to shift output activations toward narrow memory <b>210</b>.</p><p id="p-0060" num="0059">With regard to dot product computations of, for example, two multi-dimensional data arrays, for a single compute tile <b>200</b>, MAC array <b>214</b> provides robust single instruction multiple data (SIMD) functionality. SIMD generally means that all parallel units (multiple MAC operators <b>215</b>) share the same instruction (based on the deep loop nest), but each MAC operator <b>215</b> executes the instruction on different data elements. In one basic example, adding the arrays [1,2,3,4] and [5,6,7,8] element-wise to obtain the array [6,8,10,12] in one cycle will typically require four arithmetic units to execute the operation on each element. By using SIMD, the four units can share the same instruction (e.g., &#x201c;add&#x201d;) and perform computations in parallel. Thus, system <b>100</b> and compute tile <b>200</b> provides enhanced acceleration and parallelism in tensor computations over prior methods.</p><p id="p-0061" num="0060">In one example, and as described in more detail below, a single instruction can be provided by controller <b>102</b> to multiple compute tiles <b>200</b> (see tile sets <b>112</b>, <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) for consumption by multiple MAC arrays <b>214</b>. In general, neural network layers can include multiple output neurons and the output neurons can be partitioned such that tensor computations associated with a subset of output neurons can be assigned to a particular tile of tile sets <b>112</b>, <b>114</b>. Each tile of tile sets <b>112</b>, <b>114</b> can then perform related tensor computations on different groups of neurons for a given layer. Compute tile <b>200</b> can therefore provide at least two forms of parallelism: 1) one form includes partitioning the output activations (corresponding to the subset of output neurons) amongst the multiple tiles of tile set <b>112</b>, <b>114</b>; and 2) another form includes simultaneous computation (with a single instruction) of multiple subsets of output neurons based on the partitioning amongst the tiles of tile sets <b>112</b>, <b>114</b>.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example Tensor Traversal Unit (TTU) structure <b>300</b> comprising four tensors to track each having a depth of eight. TTU <b>300</b> generally includes a counters tensor <b>302</b>, a stride tensor <b>304</b>, an init tensor <b>306</b>, and a limit tensor <b>308</b>. TTU <b>300</b> further includes an adder bank <b>310</b> and a tensor address index <b>312</b>. As described above, a tensor is a multi-dimensional geometric object and to access an element of the tensor, an index of each dimension must be provided. Because the tensor is stored in narrow memory <b>210</b> and wide memory <b>212</b>, a set of tensor indices must be translated to a set of memory addresses. In some implementations, translation of the indices to memory addresses is done by making the memory addresses a linear combination of the indices and reflecting addresses via tensor address index <b>312</b>.</p><p id="p-0063" num="0062">There is a TTU per control thread and there is a control thread per instruction type (TensorOP and DMAOp) in compute tile <b>200</b>. Accordingly, as discussed above, there are two sets of TTUs in compute tile <b>200</b>: 1) TensorOp TTU <b>226</b>; and 2) DMAOp TTU <b>228</b>. In various implementations, TensorOp control <b>206</b> will cause TTU <b>300</b> to load TensorOp TTU counter <b>302</b>, limit <b>308</b>, and stride values <b>304</b> at the beginning of a particular tensor operation and will not change the register values before the instruction is retired. Each of the two TTUs will need to generate an address for the following memory address ports in compute tile <b>200</b>: 1) wide memory <b>212</b> address ports, and 2) narrow memory <b>210</b> which has four independently arbitrated banks that are presented as four address ports.</p><p id="p-0064" num="0063">As discussed above, in some implementations, narrow memory <b>210</b> can include a memory arbiter typically used in shared memory systems to decide, for each memory cycle, which control device (e.g., TensorOp control <b>206</b> or DMAOp control <b>208</b>) will be allowed to access shared memory resources of narrow memory <b>210</b>. In one example, the different instruction types (TensorOp and DMAOp) are independent control threads that request for memory access that need to be arbitrated. When a particular control thread commits a tensor element to memory, the control thread increments the counters <b>302</b> of the tensor reference that was committed to memory.</p><p id="p-0065" num="0064">In one example, when TensorOp control <b>206</b> executes an instruction for accessing a particular element of a tensor, TTU <b>300</b> can determine the address of the particular element of the tensor, such that the control <b>206</b> may access the storage, for example, narrow memory <b>210</b> to read data representing an activation value of the particular element. In some implementations, a program may include a nested loop and control <b>206</b> may execute an instruction to access an element of a two-dimensional array variable within the nested loop according to current index variable values associated with the nested loop.</p><p id="p-0066" num="0065">TTU <b>300</b> may hold traversal state for up to X number of TTU rows for a given tensor(s) at the same time. Each tensor that resides concurrently in TTU <b>300</b> occupies a dedicated hardware tensor control descriptor. The hardware control descriptor can consist of X number TTU counters <b>302</b> per row position, stride <b>304</b>, and limit registers <b>308</b> that support tensors having up to X number TTU counters per row dimensions. In some implementations, the number of rows and the number of counters per row can be different.</p><p id="p-0067" num="0066">For a given position register, the final memory address is computed from an addition operation that includes adding position registers together. The base address is incorporated into counter <b>302</b>. One or more adders are shared for tensor references that reside in the same memory. In one implementation, because there can only be a single load/store on any given port in a cycle, it will be a function of the loop nest control to ensure that multiple tensor references that reside in the same narrow or wide memory do not have their counters incremented on any given cycle. The use of registers for computing memory access address values including the determination of offset values are described in greater detail in patent application Ser. No. 15/014,265 titled &#x201c;Matrix Processing Apparatus,&#x201d; filed on Feb. 3, 2016, the entire disclosure of which is hereby expressly incorporated by reference in its entirety herein.</p><p id="p-0068" num="0067">The following provides template parameters that may be used to instantiate a specialized TTU <b>300</b>: 1) X Number of TTU Rows; 2) X Number of TTU Counters per Row; 3) X number of TTU Adder Units; 4) per TTU Row indicate shared Adder Reference; and 5) per Counter indicate X Counter Size [TTU][Row][Depth]. All TTU registers are architecturally visible. An address of a particular tensor element (i.e., tensor address <b>312</b>) that needs to be accessed for the computation is the result of the addition of the counters. When an increment signal is issued from the control thread to a row of the TTU, TTU <b>300</b> executes a single cycle operation and increments an innermost dimension by a stride <b>304</b> of that dimension and propagates the rollover through all the depths.</p><p id="p-0069" num="0068">In general, TTU <b>300</b> determines a status associated with one or more tensors. The status can include loop bound values, current loop index variable values, dimension multipliers for computing a memory address value, and/or program counter values for handling branch loop bounds. TTU <b>300</b> can include one or more tensor status elements and an arithmetic logic unit. Each of the tensor status elements may be a storage element, for example a register or any other suitable storage circuitry. In some implementations, the tensor status elements may be physically or logically arranged into different groups, as described in more detail in patent application Ser. No. 15/014,265.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example architecture that includes a narrow memory <b>210</b> broadcasting activations <b>404</b> via input bus <b>216</b> to one or more multiply accumulate (MAC) operators. Shift register <b>402</b> provides a shift functionality whereby activations <b>404</b> are sent out one at a time unto input bus <b>216</b> for receipt by one or more MAC operators <b>215</b> in a MAC cell <b>410</b>. In general, MAC cells <b>410</b>, including MAC operators <b>215</b>, can be defined as compute cells that calculate a partial sum and, in some implementations, are configured to write a partial sum datum to output bus <b>218</b>. As shown, cells <b>410</b> may consist of one or more MAC operators. In one implementation, the number of MAC operators <b>215</b> in a MAC cell <b>410</b> is referred to as the issue width of the cell. As an example, a dual issue cell refers to a cell with two MAC operators that can compute the multiplication of two activation values (from narrow memory <b>210</b>) with two parameters (from wide memory <b>212</b>) and perform an addition between the results of the two multipliers and the current partial sum.</p><p id="p-0071" num="0070">As described above, input bus <b>216</b> is a broadcast bus that provides input activations to MAC operators <b>215</b> of the linear unit (i.e., MAC array <b>214</b>). In some implementations, the same input is shared between all MAC operators <b>215</b>. The width of input bus <b>216</b> must be wide enough to supply the broadcast inputs to the corresponding number of cells for a given MAC array <b>214</b>. Consider the following example to illustrate the structure of input bus <b>216</b>. When the number of cells in the linear unit equals four and the activation width equals eight bits, input bus <b>216</b> can be configured to provide up to four input activations every cycle. In this example, every cell in MAC array <b>214</b> will only access one out the four activations that are broadcast.</p><p id="p-0072" num="0071">Based on TensorOp field settings of the instruction received by compute tile <b>200</b>, cells of MAC array <b>214</b> may need to perform computations using the same input activation. This may be referred to as Zout partitioning within a cell of MAC array <b>214</b>. Likewise, Zin partitioning within a cell occurs when cells of MAC array <b>214</b> need different activations to perform computations. In the former case, the single input activation is replicated four times and four activations read from narrow memory <b>210</b> are broadcast over four cycles. In the latter case, a read of narrow memory <b>210</b> is required every cycle. For the aforementioned example, TensorOp control <b>206</b> orchestrates this broadcast methodology based on execution of instructions received from controller <b>102</b>.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example architecture that includes an output bus <b>218</b> for providing output activations to a narrow memory unit <b>210</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In general, every MAC cell <b>215</b> of MAC array <b>214</b> in compute tile <b>200</b> computes a different output activation. However, with regard to an output feature array, in cases where output feature depth is less than the number of MAC cells <b>215</b> in a compute tile <b>200</b>, cells may be grouped to form one or more cell groups. All MAC cells <b>215</b> in a cell group compute the same output (i.e., for an output feature map), however each cell only computes a subset of the outputs, corresponding to a subset of the Zin dimension. As a result, the output of a MAC cell <b>215</b> is now a partial sum, not the final linear output. In some implementation, NLU <b>222</b> will aggregate these partial sums into the final linear output based on a control signal provided to NLU <b>222</b> by NLU control <b>238</b>.</p><p id="p-0074" num="0073">As discussed above, output bus <b>218</b> is a pipelined shift register. In various implementations, when a first portion of the tensor computations end and TensorOp control <b>206</b> indicates (by executing an instruction) that a partial sum needs to be written out, there will be a parallel load of partial sums that are provided to output bus <b>218</b>. The number of parallel loads will correspond to the number of MAC cells in compute tile <b>200</b>. TensorOp control <b>206</b> will then cause the partial sum quantities to be shifted out and sent through the non-linear pipeline. In some implementations, there may be circumstances in which not all MAC cells in a tile are actually utilized to perform computations. In such a circumstance, not all partial sums shifted onto the output bus will be valid. In this example, TensorOp control <b>206</b> may provide a control signal to MAC array <b>214</b> to indicate the number of valid cells that should be shifted out. The parallel load quantities loaded to output bus <b>218</b> will still correspond to the number MAC cells in the compute tile, however, only valid values will be shifted out and committed to narrow memory <b>210</b>.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an example flow chart of process <b>600</b> for performing tensor computations using the neural network compute tile <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Process <b>600</b> begins at block <b>602</b> and narrow memory <b>210</b> of compute tile <b>200</b> sends (i.e., broadcasts) activations one-by-one on to input activation data bus <b>216</b>. Activation values are stored in a narrow memory <b>210</b>. Narrow memory <b>210</b> can be a collection of static random access memory (SRAM) banks that permit addressing to particular memory locations for accessing input quantities. The activations read from the narrow memory <b>210</b> are broadcast, via input activation bus <b>216</b>, to linear cells of MAC array <b>214</b> (i.e., Linear Unit) that comprise multiple MAC operators <b>215</b> and sum registers <b>220</b>. At block <b>604</b> of process <b>600</b>, MAC operators <b>215</b> of compute tile <b>200</b> each receive two inputs&#x2014;one input (an activation) is received from input activation bus <b>216</b>; and another input (a parameter) is received from wide memory <b>212</b>. Accordingly, the activations feed one of the inputs of each MAC operator <b>215</b> and each MAC operator <b>215</b> in the cells of MAC array <b>214</b> get their second multiplier input from wide memory <b>212</b>.</p><p id="p-0076" num="0075">At block <b>606</b> of process <b>600</b>, MAC array <b>214</b> of compute tile <b>200</b> performs tensor computations comprising dot product computations based on elements of a data array structure accessed from memory. Wide memory <b>212</b> has a width in bits that is equal to the width of the linear unit (32-bits). The linear unit (LU) is thus a SIMD vector arithmetic logic unit (ALU) unit that receives data from a vector memory (i.e., wide memory <b>212</b>). In some implementations, MAC operators <b>215</b> may also get the accumulator inputs (partial sums) from wide memory <b>212</b> as well. In some implementations, there is time sharing relative to the wide memory <b>212</b> port for reads and/or writes relating to the two different operands (parameters and partial sum). In general, to optimize area, wide memory <b>212</b> may have a limited number of ports. As a result, when there is a need to read an operand (e.g., a parameter) from wide memory <b>212</b> and write an operand (e.g., a partial sum) to wide memory <b>212</b> at the same time, a pipeline associated with a particular operand can be stalled.</p><p id="p-0077" num="0076">At block <b>608</b>, a compute cell (having MAC operator <b>215</b> and sum register <b>220</b>) of compute tile <b>200</b> produces at least one output activation based on multiply operations performed by the MAC/compute cell. The result of MAC cell operations include either partial sums that are written back to wide memory (during partial sum arithmetic operations) or output activations that are sent to output bus <b>218</b>. At block <b>610</b>, NLU <b>222</b> of compute tile <b>200</b> applies a non-linear activation function to the output activations and writes then the activations to narrow memory <b>210</b>. In some implementations, output bus <b>218</b> is a shift register and may accumulate a parallel load of results/output activations from the MAC operator <b>215</b>, but shifts them out one at a time for application of the non-linear function and the write operation to narrow memory <b>210</b> of the same tile.</p><p id="p-0078" num="0077">Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory program carrier for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, which is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.</p><p id="p-0079" num="0078">The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output(s). The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array), an ASIC (application specific integrated circuit), or a GPGPU (General purpose graphics processing unit).</p><p id="p-0080" num="0079">Computers suitable for the execution of a computer program include, by way of example, can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices.</p><p id="p-0081" num="0080">Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.</p><p id="p-0082" num="0081">While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.</p><p id="p-0083" num="0082">Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.</p><p id="p-0084" num="0083">Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In certain implementations, multitasking and parallel processing may be advantageous.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. (canceled)</claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. A system comprising a plurality of computing tiles, wherein each computing tile of the plurality of computing tiles comprises:<claim-text>a first memory bank having a first data width;</claim-text><claim-text>a second memory bank having a second data width that is larger than the first data width;</claim-text><claim-text>a first computing unit comprising a first register, the first computing unit being configured to<claim-text>store instructions according to a first instruction type in the first register, and</claim-text><claim-text>execute the instructions according to the first instruction type; and</claim-text></claim-text><claim-text>a second computing unit comprising a second register, the second computing unit being configured to<claim-text>store instructions according to a second instruction type in the second register, and</claim-text><claim-text>execute the instructions according to the second instruction type.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the first memory bank comprises a memory arbiter configured to determine which of the first computing unit and the second computing unit can access memory resources of the first memory bank.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein each of the first register and the second register comprises a first-in first-out control scheme that prioritizes execution of instructions stored in the first register and the second register, respectively.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the first computing unit is configured to write input activation parameters to the first memory bank, and<claim-text>wherein the second computing unit is configured to write weight parameters to the second memory bank.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising an arithmetic computation unit, wherein the arithmetic computation unit comprises an array of arithmetic computation cells.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein each arithmetic computation cell of the array of arithmetic computation cells is configured to perform a multiple-accumulate operation.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the first computing unit is configured to load data parameters from a plurality of tensor traversal units at the beginning of a tensor operation.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the data parameters comprise counter values, limit values, and stride values.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A method for operating a computing tile, wherein the computing tile comprises a first memory bank having a first data width, a second memory bank having a second data width that is larger than the first data width, a first computing unit comprising a first register, a second computing unit comprising a second register, wherein the method comprises:<claim-text>storing instructions according to a first instruction type in the first register;</claim-text><claim-text>execute the instructions according to the first instruction type;</claim-text><claim-text>storing instructions according to a second instruction type in the second register, and</claim-text><claim-text>execute the instructions according to the second instruction type.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the first memory bank comprises a memory arbiter, and the method comprises determining, by the memory arbiter, which of the first computing unit and the second computing unit can access memory resources of the first memory bank.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein each of the first register and the second register comprises a first-in first-out control scheme that prioritizes execution of instructions stored in the first register and the second register, respectively.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, comprising:<claim-text>writing input activation parameters to the first memory bank; and</claim-text><claim-text>write weight parameters to the second memory bank.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the computing tile comprises an arithmetic computation unit, wherein the arithmetic computation unit comprises an array of arithmetic computation cells.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, comprising performing, by at least one arithmetic computation cell of the array of arithmetic computation cells, a multiple-accumulate operation.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, comprising loading, by the first computing unit, data parameters from a plurality of tensor traversal units at the beginning of a tensor operation.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the data parameters comprise counter values, limit values, and stride values.</claim-text></claim></claims></us-patent-application>