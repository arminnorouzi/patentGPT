<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004279A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004279</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17783100</doc-number><date>20201201</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-225720</doc-number><date>20191213</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04842</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04842</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>011</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION PROCESSING DEVICE, PROGRAM, AND INFORMATION PROVIDING SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>JAPAN TOBACCO INC.</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KURISU</last-name><first-name>Toshiharu</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>OKAMURA</last-name><first-name>Yusuke</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>HABASHIMA</last-name><first-name>Yoshiyuki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>JAPAN TOBACCO INC.</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/044639</doc-number><date>20201201</date></document-id><us-371c12-date><date>20220607</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The relationship between the smell or taste of an object and the expression of the smell or taste can be grasped, and the user can grasp what smell or taste they prefer. In the single-sample display mode, display control means displays a group of expressions relating to the olfactory sense stimulated by sample identified by sample identifying means, and if any expression is selected by the user, displays a relationship image indicating the relationship between the selected expression and other samples corresponding to the olfactory sense associated with the expression. In addition, in the multiple-sample display mode, display control means displays, for each of plural samples specific by sample identifying means, a group of expressions relating to the sense of smell stimulated by each sample, and displays the expression common to plural samples among the group of expressions to be distinguishable from the expression common to plural samples.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="64.52mm" wi="158.75mm" file="US20230004279A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="241.13mm" wi="171.87mm" file="US20230004279A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="248.75mm" wi="142.66mm" orientation="landscape" file="US20230004279A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="96.60mm" wi="111.34mm" file="US20230004279A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="250.11mm" wi="153.25mm" file="US20230004279A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="203.03mm" wi="153.16mm" file="US20230004279A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="189.99mm" wi="150.96mm" file="US20230004279A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="189.99mm" wi="152.48mm" file="US20230004279A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to a technique for providing information on the smell or taste of an object.</p><heading id="h-0002" level="1">RELATED ART</heading><p id="p-0003" num="0002">Smell is known to exert various effects on humans by acting on the cerebral limbic system that controls human emotion, behavior, and storage. For example, Patent Document 1 describes that a subject who smells a smell is asked questions about a mental event recalled by sniffing the smell, and the answer is acquired as a numerical value.</p><heading id="h-0003" level="1">PRIOR ART</heading><heading id="h-0004" level="1">Patent Document</heading><p id="p-0004" num="0003">Patent document 1: JP 2016-180830</p><heading id="h-0005" level="1">SUMMARY</heading><heading id="h-0006" level="1">Problem to be Solved</heading><p id="p-0005" num="0004">Incidentally, it is not so easy to accurately express what kind of smell a certain smell is and what kind of smell the user himself/herself prefers, and to convey it to others. This is probably due to the fact that most people have little chance to express the smell, so they have little vocabulary to express the smell, and they have not been able to accurately recognize the correspondence between the smell and the expression of the smell. Such a situation is not limited to the smell of the object, and the same applies to the taste of the object.</p><p id="p-0006" num="0005">Therefore, the present invention provides a mechanism capable of grasping the relationship between the smell or taste of an object and the expression of the smell or taste, and grasping what smell or taste the user himself/herself likes.</p><heading id="h-0007" level="1">Solution</heading><p id="p-0007" num="0006">According to one aspect of the invention, there is provided an information processing device including: an identifying means that identifies an object selected by the user among a plurality of objects that stimulate the user's sense of smell or taste; a display control means that displays a group of expressions relating to the sense of smell or taste stimulated by the identified object; a switching means that switches between the first display mode and the second display mode, wherein the display control means is configured: to display, in the first display mode, the expression group relating to one object identified by the identifying means, and to display, in the second display mode, the expression group relating to each of the plurality of objects identified by the identifying means, in addition to the expression common to the plurality of objects among the expression group so as to be distinguishable from the expression not common to the plurality of objects.</p><p id="p-0008" num="0007">The display control means may be configured to display, in the first display mode, the expression group relating to one object identified by the identifying means, and to display a relationship image, which is an image indicating a relationship between the selected expression and other object corresponding to the sense of smell or taste associated with the expression, in a case that any one of the displayed expression groups is selected by the user.</p><p id="p-0009" num="0008">A plurality of objects corresponding to the sense of smell or taste may be placed on the user interface device. The identifying means may be configured to identify an object selected by the user among a plurality of objects that stimulate the user's sense of smell or taste. The display control means may be configured to control to: display, in the first display mode, the expression group relating to one object identified by the identifying means, around the predetermined position where the object is placed, display on the user interface device, an image indicating the relationship between the selected expression and another object corresponding to the sense of smell or taste associated with the expression, which is an image corresponding to the position where the other object is placed, in a case that any one of the displayed expression groups is selected by the user.</p><p id="p-0010" num="0009">According to another aspect of the invention, there is provided a program to implement functions to a computer, the functions including: an identifying means that identifies an object selected by the user among a plurality of objects that stimulate the user's sense of smell or taste;</p><p id="p-0011" num="0010">a display control means that displays a group of expressions relating to the sense of smell or taste stimulated by the identified object; a switching means that switches between the first display mode and the second display mode, wherein the display control means is configured to control to: display, in the first display mode, the expression group relating to one object identified by the identifying means, and display, in the second display mode, the expression group relating to each of the plurality of objects identified by the identifying means, in addition to the expression common to the plurality of objects among the expression group so as to be distinguishable from the expression not common to the plurality of objects.</p><p id="p-0012" num="0011">According to yet another aspect of the invention, there is provided an information providing system including: a user interface device; and an information processing device including: an identifying means that identifies an object selected by the user among a plurality of objects that stimulate the user's sense of smell or taste; a display control means that displays a group of expressions relating to the sense of smell or taste stimulated by the identified object; a switching means that switches between the first display mode and the second display mode, wherein the display control means is configured: to display, in the first display mode, the expression group relating to one object identified by the identifying means, and to display, in the second display mode, the expression group relating to each of the plurality of objects identified by the identifying means, in addition to the expression common to the plurality of objects among the expression group so as to be distinguishable from the expression not common to the plurality of objects.</p><p id="p-0013" num="0012">The information provision system may further include: an object tag, which is a tag provided in each of the objects and storing identification information, a tray tag which is a tag provided on a tray on which the object is placed and which stores identification information, wherein the identifying means is configured to identify the objects by the identifying means reading the object tag provided on the object to identify the object in a case that any of the plurality of objects is placed at a predetermined position of the user interface device, and identify, in a case that the tray is placed in a predetermined position of the user interface device, the switching means reads the tray tag provided in the tray to switch between the first display mode and the second display mode.</p><heading id="h-0008" level="1">Advantageous Effects</heading><p id="p-0014" num="0013">According to the present invention, the relationship between the smell or taste of an object and the expression of the smell or taste can be grasped, and the user himself/herself can grasp what smell or taste he/she prefers.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating an overall configuration of an information providing system according to an embodiment of the present invention.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a side view illustrating the structure of the information providing system.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram showing an example of a hardware configuration of an information processing device according to an embodiment of the present invention.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram showing an example of an expression DB stored in the information processing device.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram showing an example of a functional configuration of an information processing device.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart showing an example of the operation of the information processing device.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a plan view illustrating a state when the user interface surface of the user interface device is viewed from above, during the information providing system is used.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a plan view illustrating a state when the user interface surface of the user interface device is viewed from above, during the information providing system is used.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a plan view illustrating a state when the user interface surface of the user interface device is viewed from above during the information providing system is used.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a plan view illustrating a state when the user interface surface of the user interface device is viewed from above, during the information providing system is used.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a plan view illustrating a state when the user interface surface of the user interface device is viewed from above, during the information providing system is used.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a plan view illustrating a state when the user interface surface of the user interface device is viewed from above, during the information providing system is used.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>13</b></figref> When the information providing system is used, it is a plan view illustrating a state when the user interface surface of the user interface device is viewed from above.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DETAILED DESCRIPTION</heading><heading id="h-0011" level="1">Configuration</heading><p id="p-0028" num="0027">First, an overall configuration of information providing system <b>1</b> according to an embodiment of the present invention will be described. Information providing system <b>1</b> is a system which enables a user to visually grasp the relationship between smell of an object and an expression of the smell, and to provide information for grasping what kind of smell the user himself/herself prefers. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, information providing system <b>1</b> includes information processing device <b>10</b>, display device <b>20</b>, sensor device <b>40</b>, first tray <b>50</b><i>a</i>, second tray <b>50</b><i>b</i>, and plural samples <b>60</b>. Information processing device <b>10</b>, display device <b>20</b> and sensor device <b>40</b> are connected each other to communicate. Information processing device <b>10</b> is an example of an information processing device according to the present invention, and is a device for central control in information providing system <b>1</b>. Display device <b>20</b> is an example of a display device for displaying information to a user. Sensor device <b>40</b> is an example of an operation device that receives an input operation of a user. Display device <b>20</b> and sensor device <b>40</b> constitute user interface device <b>30</b> that receives information provision to the user and instructions from the user.</p><p id="p-0029" num="0028">Each of plural samples <b>60</b> is an object that emits a smell that stimulates the user's sense of smell. For example, sample <b>60</b> may be a smelled natural product (e.g., a plant itself such as a lavender) or an artificial product containing its smell (e.g., a volatile liquid from which the smell of a lavender has been extracted, a sheet impregnated with the liquid, or the like) In this embodiment, as sample <b>60</b>, a cylindrical small bottle with a lid containing a liquid called an aroma oil containing a smell extracted from a natural product emitting a smell is used. Each sample <b>60</b> is provided with a seal or tag bearing the name or ID of the sample (e.g., the plant name &#x201c;Lavender&#x201d; or the number &#x201c;1&#x201d;). The user can visually identify each sample by referring to this name or ID.</p><p id="p-0030" num="0029">First tray <b>50</b><i>a </i>and second tray <b>50</b><i>b </i>function as an operating tool for switching the display mode of display device <b>20</b>. Both first tray <b>50</b><i>a </i>and second tray <b>50</b><i>b </i>have a size and a shape that allow sample <b>60</b> to be placed thereon. Both first tray <b>50</b><i>a </i>and second tray <b>50</b><i>b </i>have a size and shape so as to be superposed on each other.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a side view illustrating a structure of information providing system <b>1</b>, and more specifically, a positional relationship when display device <b>20</b>, sensor device <b>40</b>, first tray <b>50</b><i>a</i>, and sample <b>60</b> are viewed from the horizontal direction. Display device <b>20</b> has, for example, a thin rectangular plate shape, the upper surface of which has a horizontal display surface. Sensor device <b>40</b> is incorporated in a portion of display device <b>20</b>. In first tray <b>50</b><i>a </i>and second tray <b>50</b><i>b</i>, tag <b>51</b><i>a </i>(first tray tag) and tag <b>51</b><i>b </i>(second tray tag) are provided respectively, each of which is a storage medium storing identification information (called a tray ID) for identifying each tray. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, if first tray <b>50</b><i>a </i>is placed on the display surface (that is, a user interface surface corresponding to a predetermined position of the user interface device <b>30</b>, hereafter referred to as &#x201c;sensing surface&#x201d;) of display device <b>20</b> of the area in which sensor device <b>40</b> can sense the tray, for example, sensor device <b>40</b> reads the tray ID stored in tag <b>51</b><i>a </i>of first tray <b>50</b><i>a </i>using a short-distance wireless communication standard called NFC (Near Field Communication) and identifies that first tray <b>50</b><i>a </i>has been placed. In this case, the display mode of display device <b>20</b> is switched to a first display mode called a &#x201c;single-sample display mode&#x201d;. The single-sample display mode is a display mode on the assumption that only one sample <b>60</b> is placed on the sensing surface. In addition, second tray <b>50</b><i>b </i>may be stacked on first tray <b>50</b><i>a</i>. If first tray <b>50</b><i>a </i>and second tray <b>50</b><i>b </i>are placed on the sensing surface, sensor device <b>40</b> reads the two tray IDs stored in tag <b>51</b><i>a </i>of first tray <b>50</b><i>a </i>and tag <b>51</b><i>b </i>of second tray <b>50</b><i>b</i>, thereby identifying that first tray <b>50</b><i>a </i>and second tray <b>50</b><i>b </i>are placed. In this case, the display mode of display device <b>20</b> is switched to a second display mode called a &#x201c;multi-sample display mode&#x201d;. The multiple sample display mode is a display mode on the assumption that plural samples <b>60</b> are placed on the sensing surface.</p><p id="p-0032" num="0031">Sample <b>60</b> is provided with a tag <b>61</b> (object tag) which is a storage medium storing identification information (referred to as a sample ID) for identifying each sample. When sample <b>60</b> is placed on the sensing surface, sensor device <b>40</b> identifies the placed sample <b>60</b> by reading the sample ID stored in the tag <b>61</b> of sample <b>60</b>.</p><p id="p-0033" num="0032">Display device <b>20</b> also functions as a so-called touch screen, and detects a touched position on the display surface if, for example, a user touches the display surface with his/her finger or a predetermined device. The position touched by the user is expressed as X and Y coordinates in a two-dimensional coordinate plane with a certain position of the display surface as the origin.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>3</b></figref> exemplary shows a hardware configuration of information processing device <b>10</b>. Information processing device <b>10</b> is a computer that includes CPU<b>101</b>(Central Processing Unit), ROM (Read Only Memory) <b>102</b>, RAM (Random Access Memory) <b>103</b>, auxiliary storage device <b>104</b>, and communication IF (Interface) <b>105</b>.</p><p id="p-0035" num="0034">CPU<b>101</b> is a processor that performs various operations. ROM<b>102</b> is, for example, a non-volatile memory that stores a program and data used for starting information processing device <b>10</b>. RAM<b>103</b> is a volatile memory that functions as a work area while CPU<b>101</b> executes a program. The secondary storage device <b>104</b> is a nonvolatile storage device such as an HDD (Hard Disk Drive) or an SSD (Solid State Drive), and stores programs and data used in information processing device <b>10</b>. CPU<b>101</b> executes this program to implement the functions described later, and also executes the operations described later. The communication IF<b>105</b> is an interface for performing communication according to a predetermined communication standard. The communication standard may be a standard of wired communication or a standard of wireless communication. In addition to the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, information processing device <b>10</b> may include other elements such as a display device (for example, as a liquid crystal display) or an input device (for example, a keyboard).</p><p id="p-0036" num="0035">The auxiliary storage device <b>104</b> stores an expression database (hereinafter, the term &#x201c;database&#x201d; is referred to as a DB) as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In this expression DB, a sample ID and one or more expressions (that is, one or more expressions which expressed the smell of sample <b>60</b> by the character strings) about the smell stimulated by sample <b>60</b> corresponding to the sample ID, are recorded with their correspondence. That is, the expression is used as a means for communicating the smell of sample <b>60</b> to others when the user smells the smell. This expression may be an expression using any part of speech, such as a noun or adjective, and covers from direct to indirect expression of the smell. Here, the direct expression refers to an expression commonly used to recall an idea of a smell, for example, &#x201c;sweet&#x201d; or &#x201c;fruity&#x201d;, while the indirect expression refers to an expression that is not commonly used to recall an expression of a smell as compared with the direct expression described above, for example, &#x201c;spring&#x201d;, &#x201c;morning&#x201d; or &#x201c;walking&#x201d;. The indirect expression is a secondary expression that is recalled from a direct expression with respect to the direct expression, and may be an expression that abstractly represents a smell compared to the direct expression.</p><p id="p-0037" num="0036">Further, the expression DB includes information of an appearance for displaying each expression. The appearance includes, for example, a position where the expression is displayed, a color with which the expression is displayed, a size of the displayed expression, a font of the expression, a modification for displaying the expression, a time when the expression is displayed, a time to display an expression, a time period when the expression is displayed, a motion (including a spatial or temporal change in an expression) of the expression, a language used for the expression, and the like.</p><p id="p-0038" num="0037">This appearance changes depending on the relationship between sample <b>60</b> and the expression. The relationship between sample <b>60</b> and the expression is the intensity or amount of the smell (more strictly, the component contained in the smell of the sample) represented by the expression thereof in sample <b>60</b>, or the abstraction degree of the expression thereof with respect to the smell, and the like. For example, for sample <b>60</b> having strong sweet smell, the expression &#x201c;sweet&#x201d; is displayed near sample <b>60</b>, with a large font or a conspicuous color, or with a motion in which the expression is vibrating. These are examples of the appearance that varies depending on the relationship between sample <b>60</b> and the expression changes. In addition, for example, for a sample having a strong smell of &#x201c;sweet&#x201d; and a weak smell of &#x201c;fruity&#x201d;, an expression of &#x201c;sweet&#x201d; is displayed near the sample and the expression of &#x201c;fruity&#x201d; is displayed far from the sample. This is another example of an appearance which changes in accordance with the relationship between sample <b>60</b> and the expression. Further, in the case where a direct expression and an indirect expression are associated with a certain sample, the distance between sample <b>60</b> and the expression changes in accordance with the abstraction of the expression, for example, a direct expression is displayed near the sample and an indirect expression is displayed far from the sample. This is yet another example of an appearance which changes in accordance with the relationship between sample <b>60</b> and the expression. In short, the expression is displayed with an appearance such that the content of the relationship between sample <b>60</b> and the expression (specifically, the strength and weakness of the relationship and the manner of the relationship) can be visually recognized.</p><p id="p-0039" num="0038">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, for sample <b>60</b> having the sample ID &#x201c;ID001,&#x201d; the record has the first expression &#x201c;sweet,&#x201d; the second expression &#x201c;fresh,&#x201d; and the third expression &#x201c;flower.&#x201d; Among these expressions, the appearance of the first expression, which is &#x201c;sweet,&#x201d; corresponds to the position (X, Y coordinates) at which the expression is displayed is (X1, Y1), the color of the character is &#x201c;red&#x201d;, the font of the character is &#x201c;gothic&#x201d;, the size of the character is &#x201c;25 points&#x201d;, and the motion is &#x201c;flashing.&#x201d; It is of note that the appearance shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> is merely an example, and this appearance is arbitrarily determined by the system designer.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram showing an example of the functional configuration of information processing device <b>10</b>. Information processing device <b>10</b> includes functions of sample identifying means <b>11</b>, switching means <b>12</b>, expression DB storage means <b>13</b>, and display control means <b>14</b>.</p><p id="p-0041" num="0040">If sample <b>60</b> selected by the user is placed on the sensing surface of sensor device <b>40</b>, sample identifying means <b>11</b> specifics which sample <b>60</b> is placed on the sensing surface based on the sample ID stored in the tag <b>61</b> of sample <b>60</b> and read by the sensor device <b>40</b>.</p><p id="p-0042" num="0041">If first tray <b>50</b><i>a </i>or second tray <b>50</b><i>b </i>is placed on the sensing surface of sensor device <b>40</b>, switching means <b>12</b> mutually switches the single-sample display mode and the multi-sample display mode on the basis of the tray ID stored in tag <b>51</b><i>a </i>of first tray <b>50</b><i>a </i>or tag <b>51</b><i>b </i>of second tray <b>50</b><i>b </i>and read by the sensor device <b>40</b>.</p><p id="p-0043" num="0042">In the single-sample display mode and the multi-sample display mode, display control means <b>14</b> controls display device <b>20</b> to display a group of expressions relating to the olfactory sense stimulated by sample <b>60</b> specific by sample identifying means <b>11</b> in the display region corresponding to the periphery of the position where sample <b>60</b> is placed, i.e., the sensing surface, in the user interface device <b>30</b>. At this time, display control means <b>14</b> displays the expression with an appearance corresponding to sample <b>60</b> specific by sample identifying means <b>11</b> in the expression DB (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) stored in expression DB storage means <b>13</b>. The display area corresponding to the periphery of the position where sample <b>60</b> is placed is, for example, a circular area having a radius of 30 cm or less with respect to the sensing surface on which sample <b>60</b> is placed.</p><p id="p-0044" num="0043">In the single-sample display mode, if the user selects one of the expressions in the displayed expression group, display control means <b>14</b> displays a relationship image, which is an image indicating the relationship between the selected expression and other samples <b>60</b> corresponding to the sense of smell associated with the expression. In addition, in the multi-sample display mode, display control means <b>14</b> controls to display, for each of plural samples <b>60</b> identified by sample identifying means <b>11</b>, a group of expressions relating to the sense of smell stimulated by each of the samples <b>60</b>, and to display the expression common to plural samples <b>60</b> among the group of expressions so as to be distinguishable from the expression common to plural samples <b>60</b>. The display control by display control means <b>14</b> will be described in detail later with reference to <figref idref="DRAWINGS">FIGS. <b>7</b> to <b>13</b></figref>.</p><heading id="h-0012" level="1">Operation</heading><p id="p-0045" num="0044">Next, the operation of the present embodiment will be described with reference to the flowchart shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a plan view showing the user interface surface of the user interface device <b>30</b> viewed from above. Plural samples <b>60</b> (nine samples <b>60</b><i>a </i>to <b>60</b><i>i </i>are shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>) are placed at predetermined positions on the user interface surface. Which sample <b>60</b> is to be placed at which position is determined in advance, and an image indicating the position where each sample <b>60</b> is to be placed (e.g., an image indicating the same ID as the ID of sample <b>60</b>) is displayed on the user interface surface (i.e., the display surface). The user places each sample <b>60</b><i>a</i>-<b>60</b><i>i </i>at the position indicated by the images. It is assumed that the correspondence between each sample <b>60</b> and the position where sample <b>60</b> is placed is stored in advance in expression DB storage means <b>13</b>.</p><p id="p-0046" num="0045">If first tray <b>50</b><i>a </i>is placed on the sensing surface SA as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, switching means <b>12</b> of information processing device <b>10</b> determines (in step S<b>11</b>) that first tray <b>50</b><i>a </i>is placed on the basis of the reading result of the tray ID by sensor device <b>40</b>, and switches (in step S<b>12</b>) the display mode to the single-sample display mode. Although the position, shape, and size of the sensing surface by sensor device <b>40</b> are arbitrarily determined, the user knows in advance where the sensing surface is located in the user interface device <b>30</b>, or informs the user of the method by means of display, voice guidance, or the like.</p><p id="p-0047" num="0046">The user may select any of the samples <b>60</b><i>a</i>-<b>60</b><i>i </i>to smell the smell and place sample <b>60</b> of the smell he or she prefers on first tray <b>50</b><i>a </i>on the sensing surface SA. Sample identifying means <b>11</b> of information processing device <b>10</b> identifies (in step S<b>13</b>) which sample <b>60</b> is placed on the sensing surface, based on the reading result (in step S<b>13</b>) of the sample ID by sensor device <b>40</b>.</p><p id="p-0048" num="0047">In step S<b>14</b>, display control means <b>14</b> searches (in step S<b>14</b>) the expression DB for an expression corresponding to the sample ID using the identified sample ID as a key. In step S<b>15</b>, display control means <b>14</b> determines whether the display mode is the single-sample display mode or the multi-sample display mode and the display mode corresponding to each of the searched expressions with reference to the expression DB. Then, display control means <b>14</b> controls display device <b>20</b>, the display area around the sensing surface SA, the expression retrieved in step S<b>14</b>, and displays (in step S<b>16</b>) the display mode determined in step S<b>15</b>.</p><p id="p-0049" num="0048">For example, if sample <b>60</b><i>a </i>is placed on first tray <b>50</b><i>a</i>, as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a expression of the smell of sample <b>60</b><i>a </i>is displayed within a fan shape of an arbitrary size centered on, for example, the sensing surface, i.e., the position where sample <b>60</b><i>a </i>is placed. The appearance of each expression at this time is an appearance corresponding to the relationship between the expression and sample <b>60</b><i>g</i>. The user can know how to express the smell of sample <b>60</b><i>a </i>while watching these expressions. At the same time, the user can also know the relationship between the smell of sample <b>60</b><i>g </i>and each expression with reference to the appearance of each expression. For example, in the example of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the user can recognize that the smell of sample <b>60</b><i>a </i>is typically a smell expressed as &#x201c;sweet&#x201d; or &#x201c;relaxation&#x201d;, but also a component of the smell expressed as &#x201c;flower&#x201d;, &#x201c;flower&#x201d; or &#x201c;fruit&#x201d;, and further an abstract event such as &#x201c;spring&#x201d; is associated from the smell.</p><p id="p-0050" num="0049">Further, if there is any expression of the smell which the user is concerned about in the displayed expression group, the user selects the expression by performing an operation of touching the expression. If such a touch operation is performed, display control means <b>14</b> identifies the expression selected by the user based on the position at which the touch operation is performed and the display position of each expression. In this case, the expression is surrounded by some graphic image, the background of the expression is displayed in a specific color, or the expression is highlighted so that the user can know which expression selection is accepted.</p><p id="p-0051" num="0050">Then, display control means <b>14</b> searches the expression DB for the sample ID associated with the selected expression. As a result of the search, if there is a sample ID associated with the selected expression, display device <b>20</b> is controlled to display a relationship image in step S<b>16</b>. The relational image is, for example, an image showing the relationship between the expression selected by the user in the user interface plane and other sample <b>60</b> corresponding to the retrieved sample ID, and is an image corresponding to the position where sample <b>60</b> is placed.</p><p id="p-0052" num="0051">For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, if the expression &#x201c;sweet&#x201d; is selected by the user, an annular image surrounding the expression &#x201c;sweet&#x201d; is displayed, and an annular image surrounding the position of other samples (here, samples <b>60</b><i>b</i>, <b>60</b><i>d</i>, and <b>60</b><i>g</i>) represented by the expression &#x201c;sweet&#x201d; is displayed. By displaying such an annular image (related image), the user can know that there are samples <b>60</b><i>b</i>, <b>60</b><i>d</i>, and <b>60</b><i>g </i>in addition to sample <b>60</b><i>a </i>as the smell expressed as &#x201c;sweet&#x201d;.</p><p id="p-0053" num="0052">The appearance such as the color, thickness, size, and motion of the relationship image may correspond to the relation between the expression selected by the user and other samples represented by the expression. For example, if the relationship between the expression selected by the user and the other samples represented by the expression is strong, the display mode includes a color, a thickness, a size, a motion, or the like in which the relationship image becomes more conspicuous, and if the relationship is weak, the appearance becomes the opposite.</p><p id="p-0054" num="0053">If the user wants to know more about the expression, by performing an operation of touching the expression, the process described above is then performed for another expression.</p><p id="p-0055" num="0054">Here, from the state of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, it is assumed that the user removes sample <b>60</b><i>a </i>from the top of first tray <b>50</b><i>a</i>, smells the smell of another sample related to &#x201c;sweet&#x201d;, and places, for example, sample <b>60</b><i>b </i>on first tray <b>50</b><i>a </i>on the sensing surface SA as a sample of the smell that he/she prefers. Sample identifying means <b>11</b> identifies (in step S<b>13</b>) that sample <b>60</b><i>b </i>is placed on the sensing surface based on the result (in step S<b>11</b>) of reading the sample ID by sensor device <b>40</b>.</p><p id="p-0056" num="0055">Thus, as shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the expression relating to the smell of sample <b>60</b><i>b </i>is displayed inside the fan figure centered on the sensing surface (step S<b>16</b>). The user can know how to express the smell of sample <b>60</b><i>b </i>while viewing these expressions. Further, when the user performs an operation of touching the &#x201c;fruit&#x201d; as the expression of the smell which the user is concerned about in the displayed expression group, as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a ring image surrounding the expression of the &#x201c;fruit&#x201d; is displayed, and a ring image surrounding the position of another sample (here, samples <b>60</b><i>g </i>and <b>60</b><i>i</i>) represented by the expression of the &#x201c;fruit&#x201d; is displayed. The user can know that samples <b>60</b><i>g </i>and <b>60</b><i>i </i>are present in addition to sample <b>60</b><i>b </i>as the smell expressed as &#x201c;fruit&#x201d;. In <figref idref="DRAWINGS">FIG. <b>11</b></figref>, an annular image surrounding the position of sample <b>60</b><i>a </i>which is a sample represented by the expression &#x201c;fruit&#x201d; is not displayed. In this regard, in a series of operations, sample <b>60</b> placed on the sensing surface in the past is stored in the expression control unit <b>14</b>, and sample <b>60</b> placed in the past may not be displayed as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, or sample <b>60</b> placed in the past may also be displayed as an annular image.</p><p id="p-0057" num="0056">In addition, from the state of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a user removes sample <b>60</b><i>b </i>from above the first tray to sniff the smell of other samples associated with the &#x201c;fruit&#x201d; and as a sample of the smell he likes, e.g., <b>60</b><i>g </i>of sample, on top of first tray <b>50</b><i>a </i>of sensing surface SA. Sample identifying means <b>11</b> of information processing device <b>10</b>, based on the reading result (in step S<b>11</b>) of the sample ID by sensor device <b>40</b>, identifies (in step S<b>13</b>) that sample <b>60</b><i>g </i>is placed on the sensing surface.</p><p id="p-0058" num="0057">As a result, as shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the expression relating to the smell of sample <b>60</b><i>g </i>is displayed inside the fan figure centered on the sensing surface in step S<b>16</b>. The user can know how to express the smell of sample <b>60</b><i>g </i>while viewing these expressions. By passing the display processing in the single-sample display mode as described above a plurality of times, the user can know that the samples <b>60</b><i>a</i>, <b>60</b><i>b</i>, and <b>60</b><i>g </i>are samples of the smell that he/she prefers.</p><p id="p-0059" num="0058">Next, the user lays second tray <b>50</b><i>b </i>on first tray <b>50</b><i>a </i>on the sensing surface SA. Switching means <b>12</b> of information processing device <b>10</b> determines that first tray <b>50</b><i>a </i>and second tray <b>50</b><i>b </i>are placed on the basis of the reading result (in step S<b>11</b>) of the tray ID by sensor device <b>40</b>, and switches (in step S<b>12</b>) the display mode to the plurality of sample display modes.</p><p id="p-0060" num="0059">Next, the user places the selected samples <b>60</b><i>a</i>, <b>60</b><i>b</i>, and <b>60</b><i>g </i>as a sample of the smell that they prefer in the single-sample display mode over first tray <b>50</b><i>a </i>and second tray <b>50</b><i>b </i>on the sensing surface SA. Sample identifying means <b>11</b> of information processing device <b>10</b>, based on the reading result (in step S<b>11</b>) of the sample ID by sensor device <b>40</b>, sample <b>60</b><i>a</i>, <b>60</b><i>b</i>, to identify (in step S<b>13</b>) that <b>60</b><i>g </i>is placed on the sensing surface.</p><p id="p-0061" num="0060">In step S<b>14</b>, display control means <b>14</b> searches the expression DB for the expression corresponding to the sample ID by using the identified sample IDs of the samples <b>60</b><i>a</i>, <b>60</b><i>b</i>, and <b>60</b><i>g </i>as keys. In step S<b>15</b>, display control means <b>14</b> determines whether the display mode is the single-sample display mode or the multi-sample display mode and the display mode corresponding to each of the searched expressions with reference to the expression DB. In step S<b>16</b>, display control means <b>14</b> controls display device <b>20</b> to display the expression retrieved in step S<b>14</b> for each of the samples <b>60</b><i>a</i>, <b>60</b><i>b</i>, and <b>60</b><i>g </i>in the display area around the sensing surface SA in the display mode determined in step S<b>15</b>. Further, at this time, display control means <b>14</b> controls display device <b>20</b> to display a expression common to the samples <b>60</b><i>a</i>, <b>60</b><i>b</i>, and <b>60</b><i>g </i>so as to be distinguishable from a expression not common to the samples <b>60</b><i>a</i>, <b>60</b><i>b</i>, and <b>60</b><i>g</i>. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, if the expression &#x201c;fruit&#x201d; is a common expression in the samples <b>60</b><i>a</i>, <b>60</b><i>b</i>, and <b>60</b><i>g</i>, a triplet circular ring image surrounding the expression &#x201c;fruit&#x201d; is displayed. If the expressions &#x201c;sweet&#x201d;, &#x201c;flower&#x201d; and &#x201c;relax&#x201d; are all common expressions in the samples <b>60</b><i>a </i>and <b>60</b><i>b</i>, a double ring image surrounding the expressions &#x201c;sweet&#x201d;, &#x201c;flower&#x201d; and &#x201c;relax&#x201d; is displayed. The other expressions &#x201c;fresh,&#x201d; &#x201c;sea,&#x201d; &#x201c;pink,&#x201d; &#x201c;refreshment,&#x201d; &#x201c;flower,&#x201d; &#x201c;transparency,&#x201d; &#x201c;sea,&#x201d; &#x201c;vacation,&#x201d; &#x201c;spring,&#x201d; &#x201c;summer,&#x201d; and &#x201c;grassland&#x201d; are not common expressions in any two or more of the samples <b>60</b><i>a</i>, <b>60</b><i>b</i>, and <b>60</b><i>g</i>, and therefore, the above-mentioned ring image is not displayed. By confirming the common expression of plural samples <b>60</b> selected by such a user, it is possible to more clearly understand what smell the user himself/herself prefers.</p><p id="p-0062" num="0061">According to the embodiment described above, the user can visually grasp the relationship between the smell of the object and the expression of the smell. In addition, users can visually understand what kind of smell they prefer.</p><heading id="h-0013" level="1">MODIFICATION</heading><p id="p-0063" num="0062">The present invention is not limited to the embodiments described above. The embodiment described above may be modified as follows. Further, the following two or more modified examples may be implemented in combination.</p><heading id="h-0014" level="1">First Modification</heading><p id="p-0064" num="0063">The present invention is applicable not only to the smell sense but also to the taste sense (e.g., wine, sake, spice, seasoning, etc.). That is, the present invention may be implemented by replacing the &#x201c;smell&#x201d; in the embodiment with the &#x201c;taste&#x201d;.</p><heading id="h-0015" level="1">Second Modification</heading><p id="p-0065" num="0064">The appearance to display each expression may correspond to the relationship between the expression and the user who sees the display of the expression. The relationship between the expression and the user includes, for example, the degree of agreement between the smell of the smell expression by the expression and the user's preference related to the smell, and a history in which the user uses the expression as an expression of the smell. For example, for a user who prefers a &#x201c;sweet&#x201d; smell, an example may be considered in which the expression &#x201c;sweet&#x201d; is displayed near sample <b>60</b> placed on the sensing surface of sensor device <b>40</b>, the expression &#x201c;sweet&#x201d; is displayed in a large or noticeable color, or the expression &#x201c;sweet&#x201d; is displayed so as to be noticeable by moving in a vibrating manner. In this case, the user's preferences related to smell is stored in advance collected and database in the auxiliary storage device <b>104</b> or the like, display control means <b>14</b> shall refer to it.</p><p id="p-0066" num="0065">Further, for example, for a user who has used the expression &#x201c;sweet&#x201d; in many cases in the past, there may be considered an example in which the expression &#x201c;sweet&#x201d; is displayed near sample <b>60</b> placed on the sensing surface of sensor device <b>40</b>, the expression &#x201c;sweet&#x201d; is displayed in a large or conspicuous color, or the expression &#x201c;sweet&#x201d; is displayed so as to be conspicuous by moving so as to vibrate. In this case, the history of the expression used by the user for the smell is stored in advance in the auxiliary storage device <b>104</b> and the like is collected and stored in a database, display control means <b>14</b> shall refer to it.</p><heading id="h-0016" level="1">Third Modification</heading><p id="p-0067" num="0066">The appearance at the time of displaying each expression may correspond to the attribute of the smell expression by the expression. Attributes of the smell include, for example, whether it is a top note, a middle note, or a last note, intensity/weakness of stimulus of the smell, attractiveness, peculiarity, or scarcity of the smell. The top note, middle note, and initial note are the first perceived smell in time, the next perceived smell, and the next perceived smell as the smell changes. For example, examples of displaying expressions corresponding to top notes, middle notes, and last notes in order from near to far of sample <b>60</b> placed on the sensing surface of sensor device <b>40</b>, and examples of switching the display of expressions corresponding to top notes, middle notes, and last notes in chronological order are conceivable. In addition, examples of expressions relating to a smell with a strong stimulus or a rare smell may be displayed in a specific color, font, or motion.</p><heading id="h-0017" level="1">Fourth Modification</heading><p id="p-0068" num="0067">The appearance at the time of displaying each expression may correspond to the attribute of the expression. The expression attributes include, for example, an image to be received from an expression, an expression of a part of speech, a type of character to be used for expression (hiragana, katakana, kanji, alphabet, etc.), the number of characters/the number of words to be used in the expression, and the like. For example, an example in which the expression &#x201c;sweet&#x201d; is displayed in a warm color is conceivable.</p><heading id="h-0018" level="1">Fifth Modification</heading><p id="p-0069" num="0068">If displaying the above-mentioned related image (see <figref idref="DRAWINGS">FIG. <b>9</b></figref>), display control means <b>14</b> may display the related image in a display manner according to the user's preference for the smell expression by the expression selected by the user. For example, in a relational image emphasizing the expression &#x201c;sweet&#x201d; selected by the user and other samples represented by the expression, the appearance such as a color, a thickness, a size, a motion, and the like is changed in accordance with the user's preference for the &#x201c;sweet&#x201d; smell. Specifically, in the case where the user prefers a &#x201c;sweet&#x201d; smell, an example in which an appearance such as a color, a thickness, a size, a motion, or the like in which a relationship image becomes more conspicuous is conceivable. In this case, the user's preferences related to smell is stored in advance collected and database in the auxiliary storage device <b>104</b> or the like, display control means <b>14</b> shall refer to it. Note that the relationship image is not limited to the image illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, and any image may be used as long as it can be understood that the expression selected by the user and the sample represented by the expression are related to each other.</p><heading id="h-0019" level="1">Sixth Modification</heading><p id="p-0070" num="0069">Display control means <b>14</b> may display expressions having good compatibility in a display manner such that expressions having good compatibility can be identified by color, motion, or the like when displaying expressions for each of plural samples <b>60</b>. For example, with respect to sample <b>60</b> corresponding to the expression &#x201c;sweet&#x201d; and sample <b>60</b> corresponding to the expression &#x201c;marana&#x201d;, on the assumption that &#x201c;sweet&#x201d; and &#x201c;marana&#x201d; are compatible, the expressions &#x201c;sweet&#x201d; and &#x201c;marana&#x201d; are displayed in the same color, and the expressions having no good relationship with each other are displayed in different colors. Similarly, display control means <b>14</b> may display expressions that are incompatible with each other in a display manner that can be identified by color or motion. For example, with respect to sample <b>60</b> corresponding to the expression &#x201c;sweet&#x201d; and sample <b>60</b> corresponding to the expression &#x201c;bitter&#x201d;, on the assumption that &#x201c;sweet&#x201d; and &#x201c;bitter&#x201d; are incompatible with each other, an example may be considered in which the expressions &#x201c;sweet&#x201d; and &#x201c;bitter&#x201d; are displayed in the same violent movement, and the expression having no incompatible relationship is displayed without movement. In this manner, display control means <b>14</b> may display the expression having a predetermined relationship among the expression relating to the first object and the expression relating to the second object in a display manner distinguishable from the expression not having the predetermined relationship. The expression in which a plurality of expressions are in a predetermined relationship referred to herein may be, in addition to the relationship of good/bad compatibility as described above, an example in which any of the plurality of expressions matches/does not match the preference of the user or belongs/does not belong to the expression group of similar smells. In this case, the user's preferences and similar smell expression group about the smell is stored in advance in the collection and database in the auxiliary storage device <b>104</b> or the like, display control means <b>14</b> shall refer to it.</p><heading id="h-0020" level="1">Seventh Modification</heading><p id="p-0071" num="0070">In the above embodiment, switching means <b>12</b> switches to the &#x201c;single-sample display mode&#x201d; if first tray <b>50</b><i>a </i>is placed on the sensing surface, and switches to the &#x201c;multi-sample display mode&#x201d; if first tray <b>50</b><i>a </i>and second tray <b>50</b><i>b </i>are placed on the sensing surface. The method of using the tray as the operation tool for switching the display mode is not limited to the example of the above embodiment. For example, only one tray provided with a tag storing a tray ID may be prepared, and switching means <b>12</b> may switch to the &#x201c;single-sample display mode&#x201d; if the tray is not placed on the sensing surface, and may switch to the &#x201c;multi-sample display mode&#x201d; if the tray is placed on the sensing surface. Conversely, switching means <b>12</b> may switch to the &#x201c;single-sample display mode&#x201d; if the tray is placed on the sensing surface, and switch to the &#x201c;multi-sample display mode&#x201d; if the tray is not placed on the sensing surface. As described above, if the tray is placed at a predetermined position of the user interface device <b>30</b>, switching means <b>12</b> reads a tag provided in the tray, and switches between the first display mode called a single-sample display mode and the second display mode called a multi-sample display mode.</p><heading id="h-0021" level="1">Eighth Modification</heading><p id="p-0072" num="0071">The display device and the operation device is not limited to display device <b>20</b> and sensor device <b>40</b> illustrated in FIG. For example, the display device may project an image onto a certain display surface. Further, the display device may be intended to display an image on the wall surface (including the case of projecting an image on the wall surface or the wall itself is a display device). The display device may also be a display device that realizes so-called augmented reality. Specifically, if sample <b>60</b> is imaged by an imaging device of a smart phone, tablet, or glass type wearable device, a corresponding expression group may be displayed around sample <b>60</b> in the imaged image. The operation device may be a device that detects a user's operation using an image recognition technique.</p><heading id="h-0022" level="1">Ninth Modification</heading><p id="p-0073" num="0072">In the embodiment, the expression relating to sample <b>60</b> is displayed if sample <b>60</b> is placed on the sensing surface of sensor device <b>40</b>, but the expression relating to sample <b>60</b> may be displayed if the user opens the lid of the capped vial containing aroma oil corresponding to sample <b>60</b>, for example. Similarly, for example, a transparent cover may be put on a dish with a natural object itself, and if the user removes the transparent cover, a expression relating to the smell of the natural object may be displayed. Such user actions can be detected, for example, using well-known image recognition techniques.</p><heading id="h-0023" level="1">Tenth Modification</heading><p id="p-0074" num="0073">The display form of the expression may be a two-dimensional display or a three-dimensional display. The displayed &#x201c;expression&#x201d; is not limited to characters, and may be a color, an abstract image, an image such as a person/scene, or the like.</p><p id="p-0075" num="0074">The present invention may be provided as an information processing method including steps of processing performed in information processing device <b>10</b>. The present invention may also be provided as a program executed in information processing device <b>10</b>. Such a program can be provided in the form of being recorded on a recording medium such as an optical disk, or in the form of being downloaded to a computer via a network such as the Internet and installed and made available.</p><p id="p-0076" num="0075">While the present invention has been described in detail above, it is apparent to those skilled in the art that the present invention is not limited to the embodiments described herein. The present invention may be practiced as modifications and modifies without departing from the spirit and scope of the invention as defined by the appended claims. Accordingly, the description herein is for illustrative purposes and does not have any limiting meaning on the present invention.</p><heading id="h-0024" level="1">DESCRIPTION OF REFERENCE NUMERALS</heading><p id="p-0077" num="0076"><b>1</b> . . . Information-providing systems, <b>20</b>. display devices, <b>30</b>. user-interface devices, <b>40</b>. sensor devices, <b>50</b><i>a</i>. first tray, <b>51</b><i>a</i>. tag, <b>50</b><i>b</i>. second tray, <b>51</b><i>b</i>. tag, <b>60</b>, <b>60</b><i>a</i>-<b>60</b><i>i</i>. sample, <b>61</b>. tag, <b>101</b> . . . CPU, <b>102</b> . . . ROM, <b>103</b> . . . RAM, <b>104</b>. secondary storage device, <b>105</b>. communication IF, <b>11</b>. sample identifying means, <b>12</b>. switching means, <b>13</b>. expression DB storage unit, <b>14</b>. display control means.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing device comprising:<claim-text>an identifying means that identifies an object selected by the user among a plurality of objects that stimulate the user's sense of smell or taste;</claim-text><claim-text>a display control means that displays a group of expressions relating to the sense of smell or taste stimulated by the identified object;</claim-text><claim-text>a switching means that switches between the first display mode and the second display mode, wherein</claim-text><claim-text>the display control means is configured:<claim-text>to display, in the first display mode, the expression group relating to one object identified by the identifying means, and</claim-text><claim-text>to display, in the second display mode, the expression group relating to each of the plurality of objects identified by the identifying means, in addition to the expression common to the plurality of objects among the expression group so as to be distinguishable from the expression not common to the plurality of objects.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the display control means is configured:<claim-text>to display, in the first display mode, the expression group relating to one object identified by the identifying means, and</claim-text><claim-text>to display a relationship image, which is an image indicating a relationship between the selected expression and other object corresponding to the sense of smell or taste associated with the expression, in a case that any one of the displayed expression groups is selected by the user.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>a plurality of objects corresponding to the sense of smell or taste are placed on the user interface device,</claim-text><claim-text>the identifying means is configured to identify an object selected by the user among a plurality of objects that stimulate the user's sense of smell or taste;</claim-text><claim-text>the display control means is configured to control to:<claim-text>display, in the first display mode, the expression group relating to one object identified by the identifying means, around the predetermined position where the object is placed,</claim-text><claim-text>display on the user interface device, an image indicating the relationship between the selected expression and another object corresponding to the sense of smell or taste associated with the expression, which is an image corresponding to the position where the other object is placed, in a case that any one of the displayed expression groups is selected by the user.</claim-text></claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. A program to implement functions to a computer, the functions comprising:<claim-text>an identifying means that identifies an object selected by the user among a plurality of objects that stimulate the user's sense of smell or taste;</claim-text><claim-text>a display control means that displays a group of expressions relating to the sense of smell or taste stimulated by the identified object;</claim-text><claim-text>a switching means that switches between the first display mode and the second display mode, wherein</claim-text><claim-text>the display control means is configured to control to:<claim-text>display, in the first display mode, the expression group relating to one object identified by the identifying means, and</claim-text><claim-text>display, in the second display mode, the expression group relating to each of the plurality of objects identified by the identifying means, in addition to the expression common to the plurality of objects among the expression group so as to be distinguishable from the expression not common to the plurality of objects.</claim-text></claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. An information providing system comprising:<claim-text>a user interface device; and</claim-text><claim-text>an information processing device including: an identifying means that identifies an object selected by the user among a plurality of objects that stimulate the user's sense of smell or taste; a display control means that displays a group of expressions relating to the sense of smell or taste stimulated by the identified object; a switching means that switches between the first display mode and the second display mode, wherein the display control means is configured: to display, in the first display mode, the expression group relating to one object identified by the identifying means, and to display, in the second display mode, the expression group relating to each of the plurality of objects identified by the identifying means, in addition to the expression common to the plurality of objects among the expression group so as to be distinguishable from the expression not common to the plurality of objects.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information provision system according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:<claim-text>an object tag, which is a tag provided in each of the objects and storing identification information,</claim-text><claim-text>a tray tag which is a tag provided on a tray on which the object is placed and which stores identification information, wherein</claim-text><claim-text>the identifying means is configured to identify the objects by the identifying means reading the object tag provided on the object to identify the object in a case that any of the plurality of objects is placed at a predetermined position of the user interface device, and</claim-text><claim-text>identify, in a case that the tray is placed in a predetermined position of the user interface device, the switching means reads the tray tag provided in the tray to switch between the first display mode and the second display mode.</claim-text></claim-text></claim></claims></us-patent-application>