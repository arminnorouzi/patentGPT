<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005463A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005463</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17651022</doc-number><date>20220214</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>H</subclass><main-group>1</main-group><subgroup>36</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>H</subclass><main-group>1</main-group><subgroup>366</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>H</subclass><main-group>2240</main-group><subgroup>056</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>H</subclass><main-group>2250</main-group><subgroup>021</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>H</subclass><main-group>2210</main-group><subgroup>331</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>H</subclass><main-group>2250</main-group><subgroup>015</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">CROWD-SOURCED TECHNIQUE FOR PITCH TRACK GENERATION</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16665611</doc-number><date>20191028</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11250826</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17651022</doc-number></document-id></child-doc></relation></continuation><division><relation><parent-doc><document-id><country>US</country><doc-number>15649040</doc-number><date>20170713</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10460711</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16665611</doc-number></document-id></child-doc></relation></division><us-provisional-application><document-id><country>US</country><doc-number>62361789</doc-number><date>20160713</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SMULE, INC.</orgname><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SULLIVAN</last-name><first-name>Stefan</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SHIMMIN</last-name><first-name>John</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SCHAFFER</last-name><first-name>Dean</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>COOK</last-name><first-name>Perry R.</first-name><address><city>Jacksonville</city><state>OR</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Digital signal processing and machine learning techniques can be employed in a vocal capture and performance social network to computationally generate vocal pitch tracks from a collection of vocal performances captured against a common temporal baseline such as a backing track or an original performance by a popularizing artist. In this way, crowd-sourced pitch tracks may be generated and distributed for use in subsequent karaoke-style vocal audio captures or other applications. Large numbers of performances of a song can be used to generate a pitch track. Computationally determined pitch trackings from individual audio signal encodings of the crowd-sourced vocal performance set are aggregated and processed as an observation sequence of a trained Hidden Markov Model (HMM) or other statistical model to produce an output pitch track.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="112.10mm" wi="158.75mm" file="US20230005463A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="250.19mm" wi="177.29mm" orientation="landscape" file="US20230005463A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="201.85mm" wi="113.54mm" file="US20230005463A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="244.43mm" wi="159.85mm" orientation="landscape" file="US20230005463A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">The present application is a continuation patent application of Ser. No. 16/665,611, filed Oct. 28, 2019, which is a divisional patent application of U.S. Nonprovisional application Ser. No. 15/649,040, filed Jul. 13, 2017, which claims priority of U.S. Provisional Application No. 62/361,789, filed Jul. 13, 2016, which are hereby incorporated by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Field of the Invention</heading><p id="p-0003" num="0002">The invention relates generally to processing of audio performances and, in particular, to computational techniques suitable for generating a pitch track from vocal audio performances sourced from a plurality of performers and captured at a respective plurality of vocal capture platforms.</p><heading id="h-0004" level="1">Description of the Related Art</heading><p id="p-0004" num="0003">The installed base of mobile phones, personal media players, and portable computing devices, together with media streamers and television set-top boxes, grows in sheer number and computational power each day. Hyper-ubiquitous and deeply entrenched in the lifestyles of people around the world, many of these devices transcend cultural and economic barriers. Computationally, these computing devices offer speed and storage capabilities comparable to engineering workstation or workgroup computers from less than ten years ago, and typically include powerful media processors, rendering them suitable for real-time sound synthesis and other musical applications. Partly as a result, some modern devices, such as iPhone&#xae;, iPad&#xae;, iPod <sup>Touch</sup>&#xae; and other iOS&#xae; or Android devices, support audio and video processing quite capably, while at the same time providing platforms suitable for advanced user interfaces. Indeed, applications such as the Smule Ocarina&#x2122;, Leaf Trombone&#xae;, I Am T-Pain&#x2122;, AutoRap&#xae;, Sing! Karaoke&#x2122;, Guitar! By Smule&#xae;, and Magic Piano&#xae; apps available from Smule, Inc. have shown that advanced digital acoustic techniques may be delivered using such devices in ways that provide compelling musical experiences.</p><p id="p-0005" num="0004">One application domain in which exploitations of digital acoustic techniques have proven particularly successful is audiovisual performance capture, including karaoke-style capture of vocal audio. For vocal capture applications designed to appeal to a mass-market and for at least some user demographics, an important contributor to user experience can be the availability of a large catalog of high-quality vocal scores, including vocal pitch tracks for the very latest musical performances popularized by a currently popular set of vocal artists. Because the set of currently popular vocalists and performances is constantly changing, it can be a daunting task to generate and maintain a content library that includes vocal pitch tracks for an ever changing set of titles.</p><p id="p-0006" num="0005">As a result, many karaoke-style applications omit features that might otherwise be desirable if suitable content, including vocal pitch tracks, were readily available for new music releases and works for which vocal scores are not widely published. In contrast, some features of advanced karaoke-style vocal capture implementations and, indeed, some compelling aspects of the user experience thereof, including provision of performance-synchronized (or synchronizable) vocal pitch cues, real-time continuous pitch correction of captured vocal performances, auto-harmony generation, user performance grading, competitions etc., can depend upon availability of high-quality musical scores, including pitch tracks.</p><p id="p-0007" num="0006">To support these and other features, automated and/or semi-automated techniques are desired for production of musical scoring content, including pitch tracks. In particular, automated and/or semi-automated techniques are desired for production of vocal pitch tracks for use in mass-market, karaoke-style vocal capture applications.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0008" num="0007">It has been discovered that digital signal processing and machine learning techniques can be employed in a vocal capture and performance social network to computationally generate vocal pitch tracks from a collection of vocal performances captured against a common temporal baseline such as a backing track. In this way, crowd-sourced pitch tracks may be generated and distributed for use in subsequent karaoke-style vocal audio captures or other applications.</p><p id="p-0009" num="0008">In some embodiments in accordance with the present invention(s), a method includes receiving a plurality of audio signal encodings for respective vocal performances captured in correspondence with a backing track, processing the audio signal encodings to computationally estimate, for each of the vocal performances, a time-varying sequence of vocal pitches and aggregating the time-varying sequences of vocal pitches computationally estimated from the vocal performances. The method includes supplying, based at least in part on the aggregation, a computer-readable encoding of a resultant pitch track for use as either or both of (i) vocal pitch cues and (ii) pitch correction note targets in connection with karaoke-style vocal captures in correspondence with the backing track.</p><p id="p-0010" num="0009">In some embodiments, the method further includes crowd-sourcing the received audio signal encodings from a geographically distributed set of network-connected vocal capture devices. In some embodiments, the method further includes time-aligning the received audio signal encodings to account for differing audio pipeline delays at respective vocal capture devices. In some embodiments, the aggregating includes, on a per-frame basis, a weighted distribution of pitch estimates from respective of the vocal performances. In some embodiments, the weighting of individual ones of the pitch estimates is based at least in part on confidence ratings determined as part of the computational estimation of vocal pitch.</p><p id="p-0011" num="0010">In some embodiments, the method further includes processing the aggregated time-varying sequences of vocal pitches in accordance with a statistically-based, predictive model for vocal pitch transitions typical of a musical style or genre with which the backing track is associated. In some embodiments, the method further includes supplying the resultant pitch track to network-connected vocal capture devices as part of data structure that encodes temporal correspondence of lyrics with the backing track.</p><p id="p-0012" num="0011">In some embodiments in accordance with the present invention(s), a pitch track generation system includes a first geographically distributed set of network-connected devices and a service platform. The first geographically distributed set of network-connected devices is configured to capture audio signal encodings for respective vocal performances in correspondence with a backing track. The service platform is configured to receive and process the audio signal encodings to computationally estimate, for each of the vocal performances, a time-varying sequence of vocal pitches and to aggregate the time-varying sequences of vocal pitches in preparation of a crowd-sourced pitch track.</p><p id="p-0013" num="0012">In some embodiments, the system further includes a second geographically distributed set of the network-connected devices communicatively coupled to receive the crowd-sourced pitch track for use in correspondence with the backing track as either or both of (i) vocal pitch cues and (ii) pitch correction note targets in connection with karaoke-style vocal captures at respective ones of the network-connected devices. In some embodiments, the service platform is further configured to time-align the received audio signal encodings to account for differing audio pipeline delays at respective of ones the network-connected devices.</p><p id="p-0014" num="0013">In some embodiments, the aggregating includes determining at the service platform, on a per-frame basis, a weighted distribution of pitch estimates from respective ones of the vocal performances. In some embodiments, the weighting of individual ones of the pitch estimates is based at least in part on confidence ratings determined as part of the computational estimation of vocal pitch. In some embodiments, the service platform is further configured to process the aggregated time-varying sequences of vocal pitches in accordance with a statistically-based, predictive model for vocal pitch transitions. In some cases or embodiments, the statistically-based, predictive model for vocal pitch transitions typical of a musical style or genre with which the backing track is associated.</p><p id="p-0015" num="0014">In some embodiments in accordance with the present invention(s), a method of preparing a computer readable encoding of a pitch track includes receiving, from respective geographically-distributed, network-connected, portable computing devices configured for vocal capture, respective audio signal encodings of respective vocal audio performances separately captured at the respective network-connected portable computing devices against a same backing track, computationally estimating both a pitch and a confidence rating for corresponding frames of the respective audio signal encodings, aggregating results of the estimating on a per-frame basis as a weighted histogram of the pitch estimates using the confidence ratings as weights, and using a Viterbi-type dynamic programming algorithm to compute at least a precursor for the pitch track based on a trained Hidden Markov Model (HMM) and the aggregated histogram as an observation sequence of the trained HMM.</p><p id="p-0016" num="0015">In some embodiments, the method further includes time-aligning the respective audio signal encodings prior to the pitch estimating. In some cases or embodiments, the time-aligning is based, at least in part, on audio-signal path metadata particular to the respective geographically-distributed, network-connected, portable computing devices on which the respective vocal audio performances were captured. In some cases or embodiments, the time-aligning is based, at least in part, on digital signal processing that identifies corresponding audio features in the respective audio signal encodings. In some cases or embodiments, the per-frame computational estimation of pitch is based on a YIN pitch-tracking algorithm.</p><p id="p-0017" num="0016">In some embodiments, the method further includes selecting, for use in the pitch estimating, a subset of the vocal audio performances separately captured against the same backing track, wherein the selection is based on correspondence of computationally-defined audio features. In some cases or embodiments, the computationally-defined audio features include either or both of spectral peaks and frame-wise autocorrelation maxima. In some cases or embodiments, the selection is based on either or both of spectral clustering of the performances and a thresholded distance from a calculated mean in audio feature space.</p><p id="p-0018" num="0017">In some embodiments, the method further includes training the HMM. In some cases or embodiments, the training includes, for a selection of vocal performances and corresponding preexisting pitch track data: sampling both the pitch track and audio encodings of the vocal performances at a frame-rate; computing transition probabilities for (i) silence to each note, (ii) each note to silence, (iii) each note to each other note and (iv) each note to a same note; and computing emission probabilities based on an aggregation of pitch estimates computed for the selection of vocal performances. In some cases or embodiments, the training employs a non-parametric descent algorithm to computationally minimize mean error over successive iterations of pitch tracking using HMM parameters on a selection of vocal performances.</p><p id="p-0019" num="0018">In some embodiments, the method further includes (i) post-processing the HMM outputs by high-pass filtering and decimating to identify note transitions; (ii) based on timing of the identified note transitions, parsing samples of the HMM outputs into discrete MIDI events; and (iii) outputting the MIDI events as the pitch track. In some embodiments, the method further includes evaluating and optionally accepting the pitch track, wherein an error criterion for pitch track evaluation and acceptance normalizes for octave error. In some embodiments, the method further includes supplying the pitch track, as an automatically computed, crowd-sourced data artifact, to plural geographically-distributed, network-connected, portable computing devices for use in subsequent karaoke-type audio captures thereon.</p><p id="p-0020" num="0019">In some embodiments, the method is performed, at least in part, on a content server or service platform to which the geographically-distributed, network-connected, portable computing devices are communicatively coupled. In some embodiments, the method is embodied, at least in part, as a computer program product encoding of instructions executable on a content server or service platform to which the geographically-distributed, network-connected, portable computing devices are communicatively coupled.</p><p id="p-0021" num="0020">In some embodiments, the method further includes using the prepared pitch track in the course subsequent karaoke-type audio capture to (i) provide computationally determined performance-synchronized vocal pitch cues and (ii) drive real-time continuous pitch correction of captured vocal performances.</p><p id="p-0022" num="0021">In some embodiments, the method further includes computationally evaluating correspondence of the audio signal encodings of respective vocal audio performances with the prepared pitch track and, based on the evaluated correspondence, selecting one or more of the respective vocal audio performances for use as a vocal preview track.</p><p id="p-0023" num="0022">These and other embodiments in accordance with the present invention(s) will be understood with reference to the description and appended claims which follow.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0024" num="0023">The present invention(s) are illustrated by way of examples and not limitation with reference to the accompanying figures, in which like references generally indicate similar elements or features.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts information flows amongst illustrative mobile phone-type portable computing devices and a content server in accordance with some embodiments of the present invention.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depict a functional flow for an exemplary pitch track generation process that employs a Hidden Markov Model in accordance with some embodiments of the present invention.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref> depict exemplary training flows for a Hidden Markov Model computation employed in accordance with some embodiments of the present invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0028" num="0027">Skilled artisans will appreciate that elements or features in the figures are illustrated for simplicity and clarity and have not necessarily been drawn to scale. For example, the dimensions or prominence of some of the illustrated elements or features may be exaggerated relative to other elements or features in an effort to help to improve understanding of embodiments of the present invention.</p><heading id="h-0007" level="1">DESCRIPTION</heading><p id="p-0029" num="0028">Pitch track generating systems in accordance with some embodiments of the present invention leverage large numbers performances of a song (10s, 100s or more) to generate a pitch track. Such systems computationally estimate a temporal sequence of pitches from audio signal encodings of many performances captured against a common temporal baseline (typically an audio backing track for a popular song) and typically perform an aggregation of the estimated pitch tracks for the given song. A variety of pitch estimation algorithms may be employed to estimate vocal pitch including time-domain techniques such as algorithms based on average magnitude difference functions (AMDF) or autocorrelation, frequency-domain techniques and even algorithms that combine spectral and temporal approaches. Without loss of generality, techniques based a YIN estimator are described herein.</p><p id="p-0030" num="0029">Aggregation of time-varying sequences of pitches estimated from respective vocal performances (e.g., aggregation of crowd sourced pitch tracks) can be based on factors such as pitch estimation confidences (e.g., for a given performance and frame) and/or other weighting or selection factors including factors based on performer proficiency metadata or computationally determined figures of merit for particular performances. In some embodiments, a pitch track generation system may employ statistically-based predictive models that seek to constrain frame-to-frame pitch transitions in a resultant aggregated pitch track based on pitch transitions that are typical of a training corpus of songs. For example, in an embodiment described herein, a system treats aggregated data as an observation sequence of a Hidden Markov Model (HMM). The HMM encodes constrained transition and emission probabilities that are trained into the model by performing transition and emission statistics calculations on a corpus of songs, e.g., using a song catalog that already includes score coded data such as MIDI-type pitch tracks. In general, the training corpus may be specialized to a particular musical genre or style and/or to a region, if desired.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts information flows amongst illustrative mobile phone-type portable computing devices (<b>101</b>, <b>101</b>A, <b>101</b>B . . . <b>101</b>N) employed for vocal audio (or in some cases, audiovisual) capture and a content server <b>110</b> in accordance with some embodiments of the present invention. Content server <b>110</b> may be implemented as one or more physical servers, as virtualized, hosted and/or distributed application and data services, or using any other suitable service platform. Vocal audio captured from multiple performers and devices is processed using pitch tracking digital signal processing techniques (<b>112</b>) implemented as part of such a service platform and respective pitch tracks are aggregated (<b>113</b>). In some embodiments, the aggregation is represented as a histogram or other weighted distribution and is used as an observation sequence for a trained Hidden Markov Model (HMM <b>114</b>) which, in turn, generates a pitch track as its output. A resultant pitch track (and in some cases or embodiments, derived harmony cues) may then be employed in subsequent vocal audio captures to support (e.g., at a mobile phone-type portable computing device <b>101</b> or a media streaming device or set-top box hosting a Sing! Karaoke&#x2122; application) real-time continuous pitch correction, visually-supplied vocal pitch cues, real-time user performance grading, competitions etc.</p><p id="p-0032" num="0031">In some exemplary implementations of these techniques, a process flow optionally includes selection of particular vocal performances and/or preprocessing (e.g., time-alignment to account for differing audio pipeline delays in the vocal capture devices from which a crowd-sourced set of audio signal encodings is obtained), followed by pitch tracking of the individual performances, aggregation of the resulting pitch tracking data and processing of the aggregated data using the HMM or other statistical model of pitch transitions. <figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an exemplary functional flow for a portion of a pitch track generation process that employs an HMM in accordance with some embodiments of the present invention. Particular steps of the functional flow (including the computational estimation of vocal pitch from audio signal encodings of crowd sourced vocal performances [pitch tracking <b>232</b>], aggregation <b>233</b> of pitch estimates, and statistical techniques such the use of HMM <b>234</b>) are described in greater detail with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><heading id="h-0008" level="1">Optional Selection of Audio Encodings</heading><p id="p-0033" num="0032">In general, a set, database or collection <b>231</b> of captured audio signal encodings of vocal performances (or audio files) is stored at, received by, or otherwise available to a content server or other service platform and individual captured vocal performances are, or can be, associated with a backing track against which they were captured. Depending on design conditions and/or available datasets, pitch tracking (<b>232</b>) may be performed for some or all performances captures against a given backing track. While some embodiments rely on the statistical convergence of a large and generally representative sample, there are several options for selecting from the set of performances the recordings best suited for pitch tracking and/or further processing.</p><p id="p-0034" num="0033">In some cases or embodiments, performance or performer metadata may be used to identify particular audio signal encodings that are likely to contribute musically-consistent voicing data to a crowd-sourced set of samples. Similarly, performance or performer metadata may be used to identify audio signal encodings that may be less desirable in, and therefore excluded from, the crowd-sourced set of samples. In some cases or embodiments, it is possible to use one or more computationally-determined audio features extracted from the audio signal encodings themselves to select particular performances that are likely to contribute useful data to a crowd-sourced set of samples. As discussed elsewhere herein relative of aggregation <b>233</b>, some pitch estimation algorithms produce confidence metrics, and these confidence metrics may be thresholded and be used in selection as well as for aggregation. Additional exemplary audio features that may be employed in some cases or embodiment include:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0034">spectrogram peaks (time-frequency locations) and</li>        <li id="ul0002-0002" num="0035">frame-wise autocorrelation maxima.<br/>In general, selection is optional and may be employed at various stages of processing.</li>    </ul>    </li></ul></p><heading id="h-0009" level="1">Option 1&#x2014;No Selection</heading><p id="p-0035" num="0036">In some cases or embodiments, selection of a subset of performances is not necessary and/or may be omitted for simplicity. For example, when a sufficient number of performances are available to generate a confident pitch track for a song without filtering of outlier performances, selection may be unnecessary.</p><heading id="h-0010" level="1">Option 2&#x2014;Clustering</heading><p id="p-0036" num="0037">In some cases or embodiments, clustering techniques may be employed by performing audio feature extraction and clustering the performances using a spectral clustering algorithm to place audio signal encodings for vocal performances into 2 (or more) classes. A cluster that sits closest to the mean may be taken as the cluster that represents better pitch-trackable performances and may define the crowd-sources subset of vocal performances selected for use in subsequent processing.</p><heading id="h-0011" level="1">Option 3&#x2014;Mean Distance</heading><p id="p-0037" num="0038">In some cases or embodiments, feature extraction may be performed on some or all of the crowd-sourced audio signal encodings of vocal performances, and a mean and variance (or other measure of &#x201c;distance&#x201d;) for each feature vector can be computed. In this way, a multi-dimensional distance from the mean weighted by the variance of each feature can be calculated for each vocal performance, and a threshold can be applied to select certain audio signal encodings for subsequent processing. In some cases or embodiments, a suitable threshold is the root-mean-square (RMS) of the standard deviation of all features.</p><p id="p-0038" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mi>threshold</mi>   <mo>=</mo>   <msqrt>    <mrow>     <mfrac>      <mn>1</mn>      <mi>N</mi>     </mfrac>     <mo>&#x2062;</mo>     <mrow>      <munderover>       <mo>&#x2211;</mo>       <mrow>        <mi>n</mi>        <mo>=</mo>        <mn>1</mn>       </mrow>       <mi>N</mi>      </munderover>      <msubsup>       <mi>&#x3c3;</mi>       <mi>n</mi>       <mn>2</mn>      </msubsup>     </mrow>    </mrow>   </msqrt>  </mrow>  <mo>,</mo>  <mrow>   <mi>for</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>the</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>set</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>of</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>N</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>features</mi>  </mrow> </mrow></math></maths></p><p id="p-0039" num="0039">Persons of skill in the art having benefit of the present disclosure will appreciate a wide variety of selection criteria (whether metadata-based, audio-feature based, both metadata- and audio-feature based, or otherwise).</p><heading id="h-0012" level="1">Preprocessing</heading><p id="p-0040" num="0040">In some cases or embodiments, individual audio signal encodings (or audio files) of set, database or collection <b>231</b> are preprocessed by (i) time-aligning the crowd-sourced audio performances based on latency metadata that characterizes the differing audio pipeline delays at respective vocal capture devices or using computationally-distinguishable alignment features in the audio signals and (ii) normalizing the audio signals, e.g., to have a maximum peak-to-peak amplitude on the range [&#x2212;1 1]. After preprocessing, the audio signals are resampled at a sampling rate of 48 kHz.</p><p id="p-0041" num="0041">In general, latency metadata may be sourced from respective vocal capture devices or a crowd-sourced device/configuration latency database may be employed. Commonly-owned, co-pending U.S. patent application Ser. No. 15/178,234, filed Jun. 9, 2016, entitled &#x201c;CROWD-SOURCED DEVICE LATENCY ESTIMATION FOR SYNCHRONIZATION OF RECORDINGS IN VOCAL CAPTURE APPLICATIONS,&#x201d; and naming Chaudhary, Steinwedel, Shimmin, Jabr and Leistikow as describes suitable techniques for crowd-sourcing latency metadata. Commonly-owned, co-pending U.S. patent application Ser. No. 14/216,136, filed Mar. 14, 2016, entitled &#x201c;AUTOMATIC ESTIMATION OF LATENCY FOR SYNCHRONIZATION OF RECORDINGS IN VOCAL CAPTURE APPLICATIONS,&#x201d; and naming Chaudhary as inventor describes additional techniques based on roundtrip device latency measurements. Each of the foregoing applications is incorporated herein by reference. In some cases or embodiments, time alignment may be performed using signal processing techniques to identify computationally-distinguishable alignment features such as vocal onsets or rhythmic features in the audio signal encodings themselves.</p><heading id="h-0013" level="1">Pitch Tracking</heading><p id="p-0042" num="0042">In some cases or embodiments, vocal pitch estimation (pitch tracking <b>232</b>) is performed by windowing the resampled audio with a window size of 1024 samples at a hop size of 512 samples using a Hanning window. Pitch-tracking is then performed on a per-frame basis using a YIN pitch-tracking algorithm. See Cheveign&#xe9; and Kawahara, <i>YIN, A Fundamental Frequency Estimator for Speech and Music, </i>Journal of the Acoustical Society of America, 111:1917-30 (2002). Such a pitch tracker will return an estimated pitch between DC and Nyquist and a confidence rating between 0 and 1 for each frame. YIN pitch-tracking is merely an example technique. More generally, persons of skill in the art having benefit of the present disclosure will appreciate a variety of suitable pitch tracking algorithms that may be employed, including time-domain techniques such as algorithms based on average magnitude difference functions (AMDF), autocorrelation, etc., frequency-domain techniques, statistical techniques, and even algorithms that combine spectral and temporal approaches.</p><heading id="h-0014" level="1">Aggregation</heading><p id="p-0043" num="0043">In some cases or embodiments, temporal sequences of pitch estimates (e.g., pitch tracks) calculated using a YIN technique are aggregated (<b>233</b>) by taking weighted histograms of pitch estimates across the performances per-frame, where the weights are, or are derived from, confidence ratings for the pitch estimates. In general, the pitch tracking algorithm may have a predefined minimum and maximum frequency of possible tracked notes (or pitches). In some implementations, notes (or pitches) outside the valid frequency range are treated as if they had zero or negligible confidence and thus do not meaningfully contribute to the information content of the histograms or to the aggregation.</p><p id="p-0044" num="0044">As a practical matter, some crowd-sourced vocal performances may have audio files of different lengths. In such case, a maximum or full-length signal will typically-dictate the length of the entire aggregate. For individual performances whose audio signal encoding (or audio file) does not include a complete set of audio frames, e.g., an audio signal encoding missing the final or latter portion of frames, missing frames may be treated as if they had zero or negligible confidence and likewise do not meaningfully contribute any confidence to the information content of the histograms or to the aggregation. Aggregate pitches are typically quantized to discrete frequencies on a log-frequency scale.</p><p id="p-0045" num="0045">Although aggregation based on confidence weighted histograms is described herein, other aggregations of crowd-sourced vocal pitch estimates may be employed in other embodiments including equal weight aggregations, and aggregations based on weightings other than those derived from the pitch estimating process itself, aggregations based on metadata weightings, etc. In general, persons of skill in the art having benefit of the present disclosure will appreciate a wide variety of techniques for aggregating frame-by-frame pitch estimates from crowd-sourced or other sets of vocal performances.</p><p id="p-0046" num="0046">While some embodiments (such as described below) employ statistically-based techniques to operate on aggregated pitch estimates and thereby produce a resultant pitch track, it will be appreciated by persons of skill in the art having benefit of the present disclosure that, in some cases or embodiments, an aggregation of frame-by-frame pitch estimates from crowd-sourced or other sets of vocal performances may itself provide a suitable resultant pitch track, even without the use of statistical techniques that consider pitch transition probabilities.</p><heading id="h-0015" level="1">Hidden Markov Model</heading><p id="p-0047" num="0047">In some cases or embodiments, a temporal sequence of confidence-weighted aggregate histograms is treated as an observation sequence of a Hidden Markov Model (HMM) <b>234</b>. HMM <b>234</b> uses parameters for transition and emission probability matrices that are based on a constrained training phase. Typically, the transition probability matrix encodes the probability of transitioning between notes and silence, and transition from any note to any other note without encoding potential musical grammar. That is, all note transition probabilities are encoded with the same value. The emission probability matrix encodes the probability of observing a given note given a true hidden state. With this model, the system uses a Viterbi algorithm to find the path through the sequence of observations that optimally transitions between hidden-state notes and rests. The optimal sequence as computed by the Viterbi algorithm is taken as the output pitch track <b>235</b>.</p><heading id="h-0016" level="1">Training</heading><p id="p-0048" num="0048"><figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref> depict exemplary training flows for a Hidden Markov Model employed in accordance with some embodiments of the present invention. Training the HMM typically involves use of a database of songs with some coding of vocal pitch sequences (such as MIDI-type files containing vocal pitch track information) and a set of vocal audio performances for each such song. Training is performed by making observations on the vocal pitch sequence data. Typically, training is based a wide cross-section of songs from the database, including songs from different genres and countries of origin. In this way, HMM training may avoid learning overly genre- or region-specific musical tendencies. Nonetheless, in some cases or embodiments, it may be desirable to specialize the training corpus to a particular musical genre or style and/or to a country or region.</p><p id="p-0049" num="0049">Whatever the stylistic or regional scope of the training corpus, it will be generally desirable, for each given song represented in the training corpus, to include multiple performances of the given song and to aggregate data in a manner analogous to that described above with respect to the observation sequences supplied to the trained HMM. Persons of skill in the art having benefit of the present disclosure will appreciate a variety of suitable variations on the training techniques detailed herein.</p><heading id="h-0017" level="1">Option 1&#x2014;Observing MIDI Data</heading><p id="p-0050" num="0050">In some variation of the described techniques, the training of transition probabilities is performed on symbolic MIDI data by computing (<b>313</b>, <b>323</b>) a percentage of notes that transition (1) from silence to any particular note, (2) from any particular note to silence, (3) from any particular note to any other particular note, and (4) from any particular note to the same note. Referring to <figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref>, MIDI data <b>311</b> is first parsed and sampled (<b>312</b>) at the same rate as the frame-rate of the note histograms computed from audio data (<b>321</b>, <b>322</b>). Preferably, these transition probabilities are computed on the frame-by-frame samples (see <b>323</b>), not on a note-by-note basis. This HMM training approach is described in greater detail, below.</p><p id="p-0051" num="0051">Emission probabilities of the HMM are computing by performing on sets of performances for each song pitch tracking and aggregation (<b>314</b>) in a manner analogous to that described above with respect to crowd-sourced vocal performances. Error probabilities are computed (<b>313</b>, <b>323</b>) on the basis of observing:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0052">1. the weighted aggregate probability of observing silence in each frame of each song for all performances of that song where the MIDI pitch information for the given song indicates silence in the vocal pitch information for the given frames weighted by the number of silence frames,</li>        <li id="ul0004-0002" num="0053">2. the weighted aggregate probability of observing a given note in each frame for all performances of the given song where the MIDI pitch information for the given song indicates the given note,</li>        <li id="ul0004-0003" num="0054">3. the weighted aggregate probability of observing any other note in each frame for all performances of the given song where the MIDI pitch information for the given song indicates a given note,</li>        <li id="ul0004-0004" num="0055">4. the weighted aggregate probability of observing a silence in each frame of each song for all performances of that song where the MIDI pitch information for the given song indicates any note for all performances of that songs, and</li>        <li id="ul0004-0005" num="0056">5. the weighted aggregate probability of observing any note in each frame of each song for all performances of that song where the MIDI pitch information for the given song indicates silence for all performances of that song.</li>    </ul>    </li></ul></p><heading id="h-0018" level="1">Option 2&#x2014;Minibatch Descent</heading><p id="p-0052" num="0057">Since there is no parametric form of error as a function of the system parameters, a traditional gradient descent algorithm cannot generally be performed. However, there are non-parametric descent algorithms that can be used to optimize the HMM parameters, such as Markov chain Monte Carlo (MCMC), simulated annealing, and random walk techniques. For each of these cases, pitch tracking (or estimation) is performed using techniques such as described above, with HMM parameters initialized to reasonable values, in order that the optimization technique does not start at a local/global maximum. The descent algorithm follows the following procedure:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0058">1. Pitch tracking with the given parameters is performed on a (sufficiently large) subset of songs (each using a corpus of performance recordings);</li>        <li id="ul0006-0002" num="0059">2. The mean error is computed on the subset of songs;</li>        <li id="ul0006-0003" num="0060">3. The parameters are updated randomly (within a reasonable range for their starting position);</li>        <li id="ul0006-0004" num="0061">4. Pitch tracking with the new parameters is performed on another subset of songs;</li>        <li id="ul0006-0005" num="0062">5. The mean error is computed; and</li>        <li id="ul0006-0006" num="0063">6. The difference between the mean error and the previous mean error is computed.        <ul id="ul0007" list-style="none">            <li id="ul0007-0001" num="0064">a. If the difference is below a certain threshold and the mean error is below a certain threshold, descent is finished and the final parameters are recorded.</li>            <li id="ul0007-0002" num="0065">b. Otherwise, the parameters are updated as a function of the change in error and the algorithm continues from step 4.</li>        </ul>        </li>    </ul>    </li></ul></p><heading id="h-0019" level="1">Option 3&#x2014;Grid</heading><p id="p-0053" num="0066">An optimal transition matrix may be computed by partitioning the parameter space discretely and computing the mean error on a large batch of songs for each permutation of parameters. The mean error across all songs tracked is recorded along with the parameters used. The parameters which generate the minimum mean error are recorded.</p><heading id="h-0020" level="1">Post-Processing</heading><p id="p-0054" num="0067">Referring back to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in some embodiments, HMM <b>234</b> outputs a series of smooth sample vectors indicating the pitch represented as MIDI note numbers as a function of time. These smooth sample vectors are high-pass filtered and decimated such that only the note transitions (onset, offset, and change) are captured, along with their original timing. These samples are then parsed into discrete MIDI events and written to a new MIDI file (pitch track <b>235</b>) containing vocal pitch information for the given song. Note that typically, a pitch track is discarded from the results if it (1) fails to meet certain acceptance criteria and/or (2) fails to converge given the number of available performances.</p><heading id="h-0021" level="1">Acceptance Criteria</heading><p id="p-0055" num="0068">In some cases, the pitch tracking algorithm fails to produce acceptable results. During post-processing the system decides if a pitch track (e.g., pitch track <b>235</b>) should be outputted or not by taking measurements on the note histograms and the internal state of the HMM. In some cases or embodiments, decision thresholds are trained against an error criterion using the database of songs with MIDI vocal pitch information and an error metric described below. In some cases or embodiments, the decision boundary is trained using a simple Bayesian decision maximum likelihood estimation.</p><heading id="h-0022" level="1">Convergence</heading><p id="p-0056" num="0069">Each song will have a set of performances on which to track pitch. In order to determine that the best possible pitch track has results from the set of performances, several metrics are computed from the rejection metrics by increasing the number of performances used in pitch tracking and computing the slopes of each of these metrics, as well as a mean-square distance between one generated pitch track and the previous. A generated pitch track for a song (e.g., pitch track <b>235</b>) is not considered correct if the slope of the metrics never converges to certain pre-defined thresholds.</p><heading id="h-0023" level="1">Error Estimation</heading><p id="p-0057" num="0070">Certain types of errors are easily tolerated (e.g. the entire pitch track being offset by an octave). In order to best represent pitch tracks that seem disturbing from a music theoretic perspective, certain classes of errors are computed.<ul id="ul0008" list-style="none">    <li id="ul0008-0001" num="0000">    <ul id="ul0009" list-style="none">        <li id="ul0009-0001" num="0071">1. For each frame where the MIDI indicates silence, but the generated pitch track has non-silence, the error is considered 1;</li>        <li id="ul0009-0002" num="0072">2. For each frame where the MIDI indicates a note, but the generated pitch track has silence, the error is considered 1; and</li>        <li id="ul0009-0003" num="0073">3. For all other frames, the error is computed as a simple magnitude distance<br/>These three types of errors are combined with weights to produce an overall error metric.</li>    </ul>    </li></ul></p><p id="p-0058" num="0074">The generated MIDI track goes through a relative pre-processing before computing the above 3 error metrics, where a regional octave error (relative to the reference MIDI pitch information) is computed by taking a median-filtered frame-based octave error with median window of several seconds of duration. The purpose of this is to eliminate octave errors on a phrase-by-phrase basis, so that pitch tracks that are exactly correct, but shifted by octaves (within a particular region) are considered relatively more correct than pitch tracks with many notes that are incorrect, but always in the right octave.</p><heading id="h-0024" level="1">Representative (or Preview) Performances</heading><p id="p-0059" num="0075">Based on the foregoing description, it will be appreciated that certain performances of a given song used as crowd-sourced samples may more closely correspond to the HMM-generated pitch track (<b>235</b>) for the given song than other crowd-sourced samples. In some cases or embodiments, it may be desirable to computationally evaluate correspondence of individual ones of the crowd-sourced vocal audio performances with the HMM-generated pitch track. In general, correspondence metrics can be established as a post-process step or as a byproduct of the aggregation and HMM observation sequence computations. Based on evaluated correspondence, one or more of the respective vocal audio performances may be selected for use as a vocal preview track or as vocals (lead, duet part A/B, etc.) against which subsequent vocalists will sing in a Karaoke-style vocal capture. In some cases or embodiments, a single &#x201c;best match&#x201d; (based on any suitable statistical measure) may be employed. In some cases or embodiments, a set of top matches may be employed, either as a rotating set or as montage, group performance, duet, etc.</p><heading id="h-0025" level="1">VARIATIONS AND OTHER EMBODIMENTS</heading><p id="p-0060" num="0076">While the invention(s) is (are) described with reference to various embodiments, it will be understood that these embodiments are illustrative and that the scope of the invention(s) is not limited to them. Many variations, modifications, additions, and improvements are possible. For example, while pitch tracks generated from crowd-sourced vocal performances captured in accord with a karaoke-style interface have been described, other variations will be appreciated by persons of skill having benefit of the present disclosure. In some cases or embodiments, crowd-sourcing may be from a subset of the performers and/or devices that constitute a larger user base for pitch tracks generated using the inventive techniques. In some cases or embodiments, vocal captures from a set of power users or semi-professional vocalists (possibly including studio captures) may form, or be included in, the set of vocal performances from which pitches are estimated and aggregated. While some embodiments employ statistically-based techniques to constrain pitch transitions and to thereby produce a resultant pitch track, others may more directly resolve a weighted aggregate of frame-by-frame pitch estimates as a resultant pitch track.</p><p id="p-0061" num="0077">While certain illustrative signal processing techniques have been described in the context of certain illustrative applications, persons of ordinary skill in the art will recognize that it is straightforward to modify the described techniques to accommodate other suitable signal processing techniques and effects. Likewise, references to particular sampling techniques, pitch estimation algorithms, audio features for extraction, score coding formats, statistical classifiers, dynamical programming techniques and/or machine learning techniques are merely illustrative. Persons of skill in the art having benefit of the present disclosure and its teachings will appreciate a range of alternatives to those expressly described.</p><p id="p-0062" num="0078">Embodiments in accordance with the present invention may take the form of, and/or be provided as, one or more computer program products encoded in machine-readable media as instruction sequences and/or other functional constructs of software, which may in turn include components (particularly vocal capture, latency determination and, in some cases, pitch estimation code) executable on a computational system such as an iPhone handheld, mobile or portable computing device, media application platform or set-top box or (in the case of pitch estimation, aggregation, statistical modelling and audiovisual content storage and retrieval code) on a content server or other service platform to perform methods described herein. In general, a machine readable medium can include tangible articles that encode information in a form (e.g., as applications, source or object code, functionally descriptive information, etc.) readable by a machine (e.g., a computer, a server whether physical or virtual, computational facilities of a mobile or portable computing device, media device or streamer, etc.) as well as non-transitory storage incident to transmission of such applications, source or object code, functionally descriptive information. A machine-readable medium may include, but need not be limited to, magnetic storage medium (e.g., disks and/or tape storage); optical storage medium (e.g., CD-ROM, DVD, etc.); magneto-optical storage medium; read only memory (ROM); random access memory (RAM); erasable programmable memory (e.g., EPROM and EEPROM); flash memory; or other types of medium suitable for storing electronic instructions, operation sequences, functionally descriptive information encodings, etc.</p><p id="p-0063" num="0079">In general, plural instances may be provided for components, operations or structures described herein as a single instance. Boundaries between various components, operations and data stores are somewhat arbitrary, and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention(s). In general, structures and functionality presented as separate components in the exemplary configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements may fall within the scope of the invention(s).</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005463A1-20230105-M00001.NB"><img id="EMI-M00001" he="9.14mm" wi="76.20mm" file="US20230005463A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. (canceled)</claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. A method comprising:<claim-text>receiving a plurality of audio signal encodings for respective vocal performances captured in correspondence with a backing track;</claim-text><claim-text>identifying, from the plurality of audio signal encodings, a subset of audio signal encodings;</claim-text><claim-text>processing the subset of audio signal encodings to computationally estimate, for each of the vocal performances corresponding to the subset of audio signal encodings, a time-varying sequence of vocal pitches;</claim-text><claim-text>aggregating the time-varying sequences of vocal pitches computationally estimated from the vocal performances; and</claim-text><claim-text>based at least in part on the aggregation, supplying a computer-readable encoding of a resultant pitch track for use as either or both of (i) vocal pitch cues and (ii) pitch correction note targets in connection with karaoke-style vocal captures in correspondence with the backing track.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the step of identifying further comprises utilizing metadata associated with the plurality of audio signal encodings to identify the subset of audio signal encodings.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the step of identifying further comprises extracting one or more audio features from each of the plurality of audio signal encodings to identify the subset of audio signal encodings.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:<claim-text>identifying the subset of audio signal encodings based on a clustering technique applied to the extracted audio features.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:<claim-text>identifying the subset of audio signal encodings based on a distance measure calculated from the extracted audio features.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising preprocessing the plurality of audio signal encodings for respective vocal performances, wherein the preprocessing comprises one or more of: time-aligning the audio signal encodings for respective vocal performances based on latency metadata; and normalizing the audio signal encodings for respective vocal performances.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the aggregating is based at least in part on the confidence ratings determined as part of the computational estimation of vocal pitch.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A computer program product encoded in one or more non-transitory machine-readable media, the computer program product including instructions executable on a processor of a service platform to cause the service platform to:<claim-text>receive a plurality of audio signal encodings for respective vocal performances captured in correspondence with a backing track;</claim-text><claim-text>identify, from the plurality of audio signal encodings, a subset of audio signal encodings;</claim-text><claim-text>process the subset of audio signal encodings to computationally estimate, for each of the vocal performances corresponding to the subset of audio signal encodings, a time-varying sequence of vocal pitches;</claim-text><claim-text>aggregate the time-varying sequences of vocal pitches computationally estimated from the vocal performances; and</claim-text><claim-text>based at least in part on the aggregation, supply a computer-readable encoding of a resultant pitch track for use as either or both of (i) vocal pitch cues and (ii) pitch correction note targets in connection with karaoke-style vocal captures in correspondence with the backing track.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computer program product of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising instructions to cause the service platform to utilize metadata associated with the plurality of audio signal encodings to identify the subset of audio signal encodings.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computer program product of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising instructions to cause the service platform to extract one or more audio features from each of the plurality of audio signal encodings to identify the subset of audio signal encodings.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer program product of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising instructions to cause the service platform to identify the subset of audio signal encodings based on a clustering technique applied to the extracted audio features.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computer program product of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising instructions to cause the service platform to identify the subset of audio signal encodings based on a distance measure calculated from the extracted audio features.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer program product of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising instructions to cause the service platform to preprocess the plurality of audio signal encodings for respective vocal performances, wherein the preprocessing comprises one or more of: time-aligning the audio signal encodings for respective vocal performances based on latency metadata; and normalizing the audio signal encodings for respective vocal performances.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer program product of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising instructions to cause the service platform to aggregate based at least in part on the confidence ratings determined as part of the computational estimation of vocal pitch.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A pitch track generation system comprising:<claim-text>a content server configured to:<claim-text>receive a plurality of audio signal encodings for respective vocal performances captured in correspondence with a backing track;</claim-text><claim-text>identify, from the plurality of audio signal encodings, a subset of audio signal encodings;</claim-text><claim-text>process the subset of audio signal encodings to computationally estimate, for each of the vocal performances corresponding to the subset of audio signal encodings, a time-varying sequence of vocal pitches;</claim-text><claim-text>aggregate the time-varying sequences of vocal pitches computationally estimated from the vocal performances; and</claim-text><claim-text>based at least in part on the aggregation, supply a computer-readable encoding of a resultant pitch track for use as either or both of (i) vocal pitch cues and (ii) pitch correction note targets in connection with karaoke-style vocal captures in correspondence with the backing track.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the content server is further configured to utilize metadata associated with the plurality of audio signal encodings to identify the subset of audio signal encodings.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the content server is further configured to extract one or more audio features from each of the plurality of audio signal encodings to identify the subset of audio signal encodings.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the content server is further configured to identify the subset of audio signal encodings based on a clustering technique applied to the extracted audio features.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the content server is further configured to identify the subset of audio signal encodings based on a distance measure calculated from the extracted audio features.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the content server is further configured to aggregate based at least in part on the confidence ratings determined as part of the computational estimation of vocal pitch.</claim-text></claim></claims></us-patent-application>