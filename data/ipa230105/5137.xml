<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005138A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005138</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17854230</doc-number><date>20220630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0012</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>267</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>4007</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10088</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30012</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>033</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">LUMBAR SPINE ANNATOMICAL ANNOTATION BASED ON MAGNETIC RESONANCE IMAGES USING ARTIFICIAL INTELLIGENCE</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217157</doc-number><date>20210630</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>The University of Hong Kong</orgname><address><city>Hong Kong</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KUANG</last-name><first-name>Xihe</first-name><address><city>Hong Kong</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>CHEUNG</last-name><first-name>Jason PY</first-name><address><city>Hong Kong</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Teng</first-name><address><city>Hong Kong</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>The University of Hong Kong</orgname><role>03</role><address><city>Hong Kong</city><country>CN</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system for automated comprehensive assessment of clinical lumbar MRIs includes a MRI standardization component that reads MRI data from raw lumbar MRI files, uses an artificial intelligence (AI) model to convert the raw MRI data into a standardized format. A core assessment component automatically generates MRI assessment results, including multi-tissue anatomical annotation, multi-pathology detection and multi-pathology progression prediction based on the structured MRI data package. The core assessment component contains a semantic segmentation module that utilizes a deep learning artificial intelligence (AI) model to generate an MRI assessment results that contains multi-tissue anatomical annotation, a pathology detection module to generate multi-pathology detection, and a pathology progression prediction module to generate multi-pathology progression prediction. A model optimization component archives clinical MRI data and MRI assessment results based on comments provided by a specialist, and periodically optimizes the AI deep learning model of the core assessment component.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="69.26mm" wi="158.75mm" file="US20230005138A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="219.79mm" wi="183.81mm" file="US20230005138A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="224.87mm" wi="171.87mm" file="US20230005138A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="139.95mm" wi="81.45mm" file="US20230005138A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="140.72mm" wi="129.62mm" file="US20230005138A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="221.83mm" wi="168.66mm" file="US20230005138A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="111.42mm" wi="174.67mm" file="US20230005138A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="220.22mm" wi="182.37mm" file="US20230005138A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="211.67mm" wi="182.63mm" file="US20230005138A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="233.43mm" wi="168.91mm" file="US20230005138A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="183.73mm" wi="167.05mm" file="US20230005138A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="222.84mm" wi="195.92mm" file="US20230005138A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">The present application claims the benefit of priority to U.S. patent application Ser. No. 63/217,157, filed Jun. 30, 2021, which is hereby incorporated by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present invention relates to a system and device for lumbar spine anatomical annotation, pathology detection and progression prediction based on magnetic resonance images using artificial intelligence.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Low back pain (LBP) is a common health problem worldwide, with a lifetime incidence of 80%. LBP is associated with reduced quality of life, work disability, potential psychological distress, and increased health-care costs. Chronic LBP may affect 1 to 1.4 billion individuals worldwide, which causes an enormous global socioeconomic burden.</p><p id="p-0005" num="0004">Magnetic resonance images (MRIs) of the lumbar spine are widely used in the orthopedic clinic for the diagnosis of LBP. MRIs can illustrate the 3D structures and potential pathologies of multiple soft tissues in the lumbar spine, including intervertebral discs, the spinal canal, etc. Thus, the lumbar MRI is the gold standard for the assessment of LBP.</p><p id="p-0006" num="0005">Currently, the clinical assessment of lumbar MRIs relies heavily on the experience and subjective judgment of radiologists and spine specialists. Due to the high complexity of the assessment process, the manual assessment of lumbar MRI usually takes a relatively long time, which reduces the efficiency of clinical diagnosis. Besides, due to the subjective factor of manual assessment, the interrater variations are inevitable, which may make the clinical diagnostic results inconsistent and influence the success rate of treatment. Moreover, due to the limitation of human visual patterns, it is difficult for human specialists to utilize some non-perceptible image features within the MRI and to perform some quantitative and mathematical analysis.</p><p id="p-0007" num="0006">Therefore, a fully automated and objective lumbar MRI assessment system, which can perform the quantitative assessment on the MRI, is highly desired to improve the efficiency and consistency of clinical diagnosis and provide an important reference for treatment planning and disease management.</p><heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0008" num="0007">In order to address the problems involved in diagnosing LBP based on MRI, the present invention utilizes a novel artificial intelligence (AI) system and device for fully automated comprehensive assessment of clinical lumbar magnetic resonance images (MRIs). More specifically, the AI system and device perform AI deep learning based MRI assessment operations on the clinical lumbar MRIs and generate the MRI assessment results including multi-tissue anatomical annotation, multi-pathology detection, and multi-pathology progression prediction.</p><p id="p-0009" num="0008">According to the invention the clinical lumbar MRIs from multiple sources are converted into a standard form, and three MRI assessment operations, including semantic segmentation, pathology detection, and pathology progression prediction, are applied to the standardized lumbar MRIs. For each MRI assessment operation, a novel MRI assessment module is developed, which is equipped with a novel AI deep learning assessment pipeline powered by a parallel computing processer. The AI system and device significantly improve the efficiency and consistency of clinical diagnosis for better treatment planning and disease management, which also have multiple deployment schemes for the application scenarios with different scales.</p><p id="p-0010" num="0009">The multi-tissue anatomical annotation contains pixel-level segmentation masks for multiple spinal tissues, including vertebra, intervertebral disc, nerve, blood vessel, and muscle, which indicate not only the location and rough area, but also the pixel-level shape details of the corresponding tissues in the lumbar MRI. The multi-tissue anatomical annotation has great significance in the 3D reconstruction of multiple spinal tissues for biomechanical simulations and 3D printing. Besides, the multi-tissue anatomical annotation may also facilitate clinical decision making, surgical planning and tissue engineering. Moreover, the multi-tissue anatomical annotation is also used in the AI system and device to provide anatomical information for further pathology detection and progression prediction.</p><p id="p-0011" num="0010">The multi-pathology detection contains the classification and grading of multiple pathologies associated with LBP, including Schneiderman score, disc bulging, Pfirrmann grading, Schmorl's nodes, Marrow changes, etc. For some pathologies, which have only two statuses, positive and negative, the pathology detection is the binary classification of the corresponding pathologies. For some pathologies, which have more than two statuses, the pathology detection is the multi-class classification/grading of the corresponding pathologies. The multi-pathology detection may help clinicians improve the efficiency and accuracy of diagnosis.</p><p id="p-0012" num="0011">The multi-pathology progression prediction contains the prediction of multiple pathologies associated with LBP regarding whether the exiting pathologies will worsen, or new pathologies will emerge in the future. The multi-pathology progression prediction has great significance for better treatment planning and disease management.</p><p id="p-0013" num="0012">The AI system and device contain three major functional components, including: (1) the MRI standardization component, which converts the MRI data from clinical lumbar MRI files with different file formats to a standardized and structured form; (2) the core assessment component, which applies several AI deep learning based MRI assessment operations to the standardized and structured MRI data and generates the MRI assessment results (multi-tissue anatomical annotation, multi-pathology detection, and multi-pathology progression prediction); and (3) the model optimization component, which archives the clinical MRI data based on the specialist's comments and optimizes the AI deep learning models of the core assessment component with the archived data.</p><p id="p-0014" num="0013">The MRI standardization component is adopted to read the MRI data from raw lumbar MRI files, standardize the MRI data, and organize the standardized MRI data to a structured MRI data package. The raw lumbar MRI files from different MRI sources usually have different file formats, which save the MRI data with different protocols and rules. The MRI standardization component utilizes multiple MRI file reading units to read the clinical MRI files with the most of common file formats, which enables the AI system and device to be compatible with most of the common MRI sources.</p><p id="p-0015" num="0014">The raw lumbar MRI file usually saves different kinds of MRI data, including the MRI image data and MRI metadata. The MRI metadata contains multiple items of descriptive information about the attributes of the MRI file. The MRI data from different MRI files may be saved in different forms (different shape, value range, unit, etc.), which are standardized by the MRI standardization component into a standard form. The standardized MRI data is further organized into a structured MRI data package by the MRI standardization component. The structured MRI data package adopts a hierarchical tree structure to organize and save different kinds of MRI data. **</p><p id="p-0016" num="0015">The core assessment component is adopted to automatically generate the MRI assessment results based on the structured MRI data package. The core assessment component contains a semantic segmentation module to generate multi-tissue anatomical annotation, a pathology detection module to generate multi-pathology detection, and a pathology progression prediction module to generate multi-pathology progression prediction.</p><p id="p-0017" num="0016">The semantic segmentation module is equipped with an original anatomical knowledge driven pipeline, which integrates the anatomical a priori knowledge and state-of-the-art AI deep learning technology to generate pixel-level segmentation masks for multiple spinal tissues in lumbar MRI. The AI deep learning model of the semantic segmentation module can be automatically finetuned for each MRI case without relying on any human intervention and manually labelling work. *</p><p id="p-0018" num="0017">The semantic segmentation module consists of three major parts: 1) suboptimal region of interest (ROI) detection, 2) voting based automated supervision generation, and 3) AI deep learning based accurate segmentation. The suboptimal ROIs of multiple spinal tissues are identified first from the lumbar MRI with an original knowledge-based method, which utilizes the anatomical characteristics of spinal tissues. Further, the suboptimal ROIs are processed by a unique voting operation, which automatically generates high-quality supervision for fine-tuning the AI deep learning model. With the voting operation, the pipeline avoids the time-consuming and expensive manual annotation process, which significantly reduces the cost of model fine-tuning. Further, a pretrained AI deep learning model, which has learned the general MRI features and patterns of different spinal tissues, is fine-tuned for each specific MRI case by the automatically generated supervision. Further, the fine-tuned AI deep learning model generates pixel-level semantic segmentation masks for multiple spinal tissues, which serve as the multi-tissue anatomical annotation.</p><p id="p-0019" num="0018">The pathology detection module is equipped with a AI deep learning based pipeline, which contains a AI deep learning model to generate the classification/grading results for multiple pathologies associated with LBP. The multi-tissue anatomical annotation is adopted in the pathology classifying/grading process to provide anatomical information.</p><p id="p-0020" num="0019">The pathology detection module consists of two major parts: 1) intervertebral disc region (IVD region) extraction, and 2) AI deep learning based pathology detection. The IVD regions are determined and extracted from the lumbar MRI based on the multi-tissue anatomical annotation. Each IVD region is a 3D cuboid area containing a whole intervertebral disc, adjacent upper and lower vertebral endplates, and the nearby spinal canal. The IVD region covers most of the area where pathologies associated with LBP may occur. Different IVD regions may have different shapes and sizes, and they are resized to a standard size before being assessed by a AI deep learning model. Further, the IVD region is input to a AI deep learning model that will determine which pathologies are present in the input IVD region, and for some pathologies, the AI deep learning model will further determine the specific grades.</p><p id="p-0021" num="0020">The pathology progression prediction module has a similar AI deep learning based assessment pipeline to that of the pathology detection module, except that the specific task of the AI deep learning model changes from classifying/grading the present pathology to predicting a future pathology. The IVD region is extracted from lumbar MRI and resized in the same way as in the pathology detection module. Further, the IVD region is input to a AI deep learning model, which will provide the prediction of the future status of multiple pathologies associated with LBP. Referring to the multi-pathology detection result generated by the pathology detection module, the progression status of each pathology can be determined.</p><p id="p-0022" num="0021">The model optimization component is adapted to archive the clinical MRI data and MRI assessment results based on comments provided by the specialist or clinicians, and to periodically optimize the AI deep learning models of the core assessment component based on the archived MRI data with clinical labels under the control of the control signal.</p><p id="p-0023" num="0022">The MRI data and assessment results of each MRI case are recorded by the model optimization component during the clinical test. In some cases, clinicians may disagree with some automated assessment results provided by AI system and device, such as, believing that the anatomical annotation is not accurate enough, or the AI system and device have misjudged the grading of some pathologies, etc. In such a case these assessment results can be manually corrected based on the specialist's comments. The corrected assessment results served as the clinical labels and they are archived by the model optimization component. In some cases, the clinicians may believe that some of the MRI cases are more important than others, perhaps because these MRI cases are rarer or more representative. Then the clinicians can assign a higher importance score to them based on the specialist's comments on these cases.</p><p id="p-0024" num="0023">Further, the AI deep learning models of the core assessment component are optimized based on the archived MRI data with clinical labels. The AI deep learning optimizing operation adopts the basic strategy of transfer learning. For each optimization, part of the archived MRI data is involved, which consists of all newly archived MRI cases (not being used for optimization) and randomly selected old MRI cases (have been used for optimization). An old MRI case with a higher importance score has a higher probability of being selected. The AI deep learning optimizing operation is triggered by the control signal, which may be generated manually by the administrator or automatically by external timing devices for periodically optimizing.</p><p id="p-0025" num="0024">The hardware of the AI system and device contains the logical computing processer (CPU), parallel computing processer (GPU), random-access memory, and data storage.</p><p id="p-0026" num="0025">The functional components of the AI system and device are driven by the CPU processor and GPU processor. The CPU processor performs multiple logically complex tasks of MRI standardizing, assessing, and archiving processes, as well as AI deep learning optimizing processes. The GPU processor performs the computationally intensive tasks, including the AI deep learning based MRI assessing and training processes for AI deep learning optimization. The CPU processor and GPU processor here refer to a set of devices that perform the same functions, which can be configured in the AI system and device according to the specific demand of computational power in the application scenarios with different scales. For example, in the personal level application, the CPU and GPU processors can be integrated as separate chips on the motherboard, while in the institutional level application, the AI system and device may need to assess the MRI data of a large institution simultaneously, thus the CPU and GPU processors may upgrade to the CPU and GPU servers for increasing demand of computational power.</p><p id="p-0027" num="0026">The AI system and device contains the random-access memory to save the program codes, which instruct the operations of the CPU and GPU processors, the intermediate data generated during the MRI standardization, MRI assessment, and AI deep learning optimization, as well as the AI deep learning models for multiple specific MRI assessment tasks. The specific hardware configuration of the memory of AI system and device can be alternates for different application scenarios. The data storage of the AI system and device is adopted to store the clinical MRI data with labels for AI deep learning optimization, and back-up AI deep learning models. The specific hardware configuration of the data storage of the AI system and device can be alternates for different demand on the data storage capability in the application scenarios with different scales.</p><p id="p-0028" num="0027">Besides, the AI system and device also contains a set of input/output (I/O) interfaces for interacting with other external devices and MRI sources.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF SUMMARY OF THE DRAWINGS</heading><p id="p-0029" num="0028">This patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication with color drawing(s) will be provided by the Office upon request and payment of the necessary fee.</p><p id="p-0030" num="0029">The foregoing and other objects and advantages of the present invention will become more apparent when considered in connection with the following detailed description and appended drawings in which like designations denote like elements in the various views, and wherein:</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram illustrating the overall structure of the AI system for automated lumbar MRI assessment according to the present invention;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram illustrating the MRI standardization component of the AI system of the present invention;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating the structure and content of structured MRI data packages according to the present invention;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating the semantic segmentation module of the core assessment component according to the present invention;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of the suboptimal region of interest (ROI) detection process of the semantic segmentation module of the present invention;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is flowchart of the voting process of the semantic segmentation module of the present invention;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is flowchart of the pixel division process of the semantic segmentation module of the present invention;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is schematic diagram of the pathology detection module of the core assessment component of the present invention;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram of the semantic segmentation module of the core assessment component of the present invention;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating the model optimization component of the present invention;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flow chart of the clinical MRI data archiving process of the model optimization component of the present invention;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating an example of the present invention for single clinical scale (personal level) application scenarios;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram illustrating an example of the present invention for small scale (institutional level) application scenarios;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating an example of the present invention for medium-large scale (regional level) application scenarios;</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>15</b>A</figref> is the rule-based ROI detection component of the MRI sequential flow pipeline of the present invention, <figref idref="DRAWINGS">FIGS. <b>15</b>B and <b>15</b>C</figref> represent the volume of interest (VOI) generation and triaging, and <figref idref="DRAWINGS">FIGS. <b>15</b>D and <b>15</b>E</figref> represent the network training and vertebral segmentation components of the present invention;</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>16</b>A</figref> illustrates the rule-based ROI detection on an original image, <figref idref="DRAWINGS">FIG. <b>16</b>B</figref> is the preprocessed image thereof, <figref idref="DRAWINGS">FIG. <b>16</b>C</figref> is the edge enhanced and threshold image thereof and <figref idref="DRAWINGS">FIG. <b>16</b>D</figref> is the region of interest image thereof;</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>17</b>A</figref> illustrates the image from the voting process of the present invention for the volume of interest (VOI) and <figref idref="DRAWINGS">FIG. <b>17</b>B</figref> illustrates the three pixel categories of VOI in one MRI slice: positive pixels (green), ambiguous pixels (blue), and negative pixels (red);</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>18</b>A</figref> illustrates the original MRI image in the pipeline of the deep learning-based vertebra segmentation according to the present invention, <figref idref="DRAWINGS">FIG. <b>18</b>B</figref> and <figref idref="DRAWINGS">FIG. <b>18</b>C</figref> represent the local normalized image and the edge detection result, respectively, <figref idref="DRAWINGS">FIG. <b>18</b>D</figref> shows the coordinate channels, <figref idref="DRAWINGS">FIG. <b>18</b>E</figref> shows input to the network processed by convolution blocks, <figref idref="DRAWINGS">FIG. <b>18</b>F</figref> shows the output for all pixels in the VOI, which produces <figref idref="DRAWINGS">FIG. <b>18</b>G</figref>, the vertebral segmentation result; and</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>19</b>A</figref> illustrates a row of examples 1-5 of vertebral segmentation results produced by the rule-based method and <figref idref="DRAWINGS">FIG. <b>19</b>B</figref> shows a row of examples 1-5 of vertebral segmentation results produced by the MRI Sequence Flow pipeline.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating the overall framework of the AI system and device of the present invention. Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the system contains three major functional components, including MRI standardization component (<b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), core assessment component (<b>200</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), and model optimization component (<b>300</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). Each functional component is a combination of hardware and software designed for a specific task.</p><p id="p-0051" num="0050">The MRI standardization component <b>100</b> receives the raw MRI files (S<b>1</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), which may have different file formats, and converts the received raw MRI files S<b>1</b> to structured MRI data packages (S<b>2</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), which are transmitted to the core assessment component <b>200</b> and the model optimization component <b>300</b> for further automated MRI assessment and AI deep learning optimization.</p><p id="p-0052" num="0051">The MRI standardization component <b>100</b> enables the system and device of the present invention to be compatible with different MRI sources, which transmit raw MRI files S<b>1</b> with different file formats to the system and device through corresponding I/O interfaces. The common MRI sources include the MRI equipment, a compact disc read-only memory (CD-ROM), a universal serial bus (USB) device, a wireless communication system, etc. The common file formats of raw MRI files S<b>1</b> include DICOM, NIFTI, IMA, TIFF, PNG, JPEG, etc.</p><p id="p-0053" num="0052">The core assessment component <b>200</b> receives the structured MRI data packages S<b>2</b> and generates the automated MRI assessment results, which include the multi-tissue anatomical annotation (S<b>3</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), the multi-pathology detection (S<b>4</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), and the multi-pathology progression prediction (S<b>5</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). The automated MRI assessment results S<b>3</b>, S<b>4</b>, and S<b>5</b>, are transmitted to the output devices <b>340</b> through corresponding I/O interfaces <b>330</b> and the model optimization component <b>300</b> for further AI deep learning optimization. The common output devices <b>340</b> include a display, a USB device, a data storage server, a wireless communication system, etc.</p><p id="p-0054" num="0053">The core assessment component <b>200</b> contains three major modules, including the semantic segmentation module (<b>210</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), pathology detection module (<b>220</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), and pathology progression prediction module (<b>230</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0055" num="0054">The semantic segmentation module <b>210</b> receives the structured MRI data packages S<b>2</b> and utilizes a AI deep learning model to generate pixel-level segmentation masks for multiple spinal tissues, which are served as multi-tissue anatomical annotation S<b>3</b>. The multi-tissue anatomical annotation S<b>3</b> indicates not only locations or rough areas but also detailed shape features of corresponding spinal tissues.</p><p id="p-0056" num="0055">The pathology detection module <b>220</b> receives the structured MRI data packages S<b>2</b> and multi-tissue anatomical annotation S<b>3</b>, and utilizes a AI deep learning model to generate the classification and grading results of multiple pathologies associated with LPB, which are output as a multi-pathology detection S<b>4</b>.</p><p id="p-0057" num="0056">The pathology progression prediction module <b>230</b> receives the structured MRI data packages S<b>2</b>, multi-tissue anatomical annotation S<b>3</b>, and multi-pathology detection S<b>4</b>, and utilizes a AI deep learning model to generate the multi-pathology progression prediction S<b>5</b>.</p><p id="p-0058" num="0057">The AI deep learning models of the semantic segmentation module <b>210</b>, pathology detection module <b>220</b>, and pathology progression prediction module <b>230</b>, have different architectures and are trained for different specific tasks. The AI deep learning models are optimized via corresponding AI deep learning optimizing operations (O<b>1</b>, O<b>2</b>, and O<b>3</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), which are performed by the model optimization component <b>300</b>.</p><p id="p-0059" num="0058">The model optimization component <b>300</b> receives the structured MRI data packages S<b>2</b>, the automated MRI assessment results (multi-tissue anatomical annotation S<b>3</b>, multi-pathology detection S<b>4</b>, and multi-pathology progression prediction S<b>5</b>), the specialist's comment (S<b>6</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), and the control signal (S<b>7</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) and optimizes the AI deep learning models of three major modules (semantic segmentation module <b>210</b>, pathology detection module <b>220</b>, and pathology progression prediction module <b>230</b>) of the core assessment component <b>200</b></p><p id="p-0060" num="0059">The specialist's comment S<b>6</b> is input by clinicians though input devices <b>320</b> and corresponding I/O interfaces <b>310</b>. The common input devices <b>320</b> include the keyboard, the pointing device, the wireless communication system, etc. The specialist's comment S<b>6</b> may contain a correction of automated MRI assessment results S<b>3</b>, S<b>4</b>, and S<b>5</b>, when the clinicians disagree with the assessment results generated by the AI system and device, and the importance score of each MRI case, which represents the importance and significance of a corresponding MRI case. The model optimization component <b>300</b> will correct the automated MRI assessment results S<b>3</b>, S<b>4</b>, and S<b>5</b>, based on the specialist's comment S<b>6</b> and archive the MRI data S<b>2</b>, S<b>3</b>, S<b>4</b>, and S<b>5</b>. The AI deep learning optimizing operations O<b>1</b>, O<b>2</b>, and O<b>3</b>, are performed based on the archived MRI data S<b>2</b>, S<b>3</b>, S<b>4</b>, and S<b>5</b>.</p><p id="p-0061" num="0060">The input control signal S<b>7</b> triggers the AI deep learning optimizing operations O<b>1</b>, O<b>2</b>, and O<b>3</b>, and may be generated manually by the administrator or automatically by external timing devices for periodically optimizing.</p><p id="p-0062" num="0061">MRI Standardization Component and Structured MRI Data Package</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating the MRI standardization component <b>100</b> of the present invention. The MRI standardization component is adapted to use an AI system to convert raw MRI files (S<b>1</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>) with different file formats into the structured MRI data packages (S<b>2</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>). Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the MRI standardization component contains an MRI file discriminating unit (<b>101</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>), a set of MRI file reading units (<b>111</b>, <b>112</b>, . . . , <b>11</b><i>n </i>of <figref idref="DRAWINGS">FIG. <b>2</b></figref>), a set of data standardizing units (<b>121</b>, <b>122</b>, . . . , <b>12</b><i>n </i>of <figref idref="DRAWINGS">FIG. <b>2</b></figref>), a data organizing unit (<b>131</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>), and a memory (<b>141</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0064" num="0063">The MRI file discriminating unit <b>101</b> receives the raw MRI files S<b>1</b> and transmits each received raw MRI file S<b>1</b> to the corresponding MRI file reading unit <b>111</b>/<b>112</b>/ . . . /<b>11</b><i>n</i>, based on the file format. The MRI file discriminating unit <b>101</b> discriminates the file formats of raw MRI files S<b>1</b> based on the filename suffixes. Raw MRI files S<b>1</b> with different file formats usually have different corresponding filename suffixes. For example, the DICOM file usually has the filename suffix of &#x2018;.dicom&#x2019; or &#x2018;.dcm&#x2019;, the NIFTI file usually has the filename suffix of &#x2018;.nii&#x2019;, and the TIFF file usually has the filename suffix of &#x2018;.tiff&#x2019; or &#x2018;.tif&#x2019;.</p><p id="p-0065" num="0064">The MRI file reading units <b>111</b>, <b>112</b>, . . . <b>11</b><i>n</i>, receive the raw MRI files S<b>1</b> and read the MRI image data (S<b>411</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>) and MRI metadata (S<b>412</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>) from the received raw MRI files S<b>1</b>, which are transmitted to the corresponding data standardizing unit <b>121</b>/<b>122</b>/ . . . /<b>12</b><i>n</i>. The MRI metadata S<b>412</b> usually contains different kinds of descriptive information about the MRI case, including the pixel size which represents the actual size of each pixel in the MRI image, the slice thickness which represents the distance between two adjacent MRI slices, the MRI acquisition date, the subject ID, etc. The raw MRI files S<b>1</b> with different file formats usually save the MRI data S<b>411</b> and S<b>412</b> with different protocols and rules, which makes each file format readable for only one or a few specific reading units. A set of different MRI file reading units <b>111</b>, <b>112</b>, . . . <b>11</b><i>n</i>, are adapted in the MRI standardization component to read the raw MRI files S<b>1</b> with all commonly used file formats. The MRI file discriminating unit <b>101</b> selects one from the set of MRI file reading units <b>111</b>, <b>112</b>, . . . <b>11</b><i>n</i>, to read the MRI data S<b>411</b> and S<b>412</b> from received raw MRI file S<b>1</b>, based on the file format (in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the <b>111</b> is selected).</p><p id="p-0066" num="0065">The data standardizing units <b>121</b>, <b>122</b>, . . . , <b>12</b><i>n</i>, receive the MRI image data S<b>411</b> and MRI metadata S<b>412</b>, and convert the received MRI data S<b>411</b> and S<b>412</b> to the standardized MRI data (S<b>413</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>), which are transmitted to the data organizing unit <b>131</b>. The MRI data S<b>411</b> and S<b>412</b> from different raw MRI files S<b>1</b> may be saved in different forms. For example, in some cases the MRI image data S<b>411</b> is saved as a series of 2D arrays, each of which represents a MRI slice, and in other cases the MRI image data S<b>411</b> is saved as a 3D array directly.</p><p id="p-0067" num="0066">Further, the pixel value range of the MRI image may also be different in different raw MRI files S<b>1</b>. Moreover, the MRI metadata S<b>411</b>, such as pixel size, slice thickness, etc., may be saved with different units. A set of different data standardizing units <b>121</b>, <b>122</b>, . . . , <b>12</b><i>n</i>, are adapted in the MRI standardization component to convert the MRI data S<b>411</b> and S<b>412</b> of different raw MRI files S<b>1</b> to a standardized form, which will be described in detail below referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Since each data standardizing unit is connected to only one MRI file reading unit, for each raw MRI file S<b>1</b>, only one data standardizing unit <b>121</b>/<b>122</b>/ . . . /<b>12</b><i>n </i>will be used to standardize the MRI data. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the Data Standardizing Unit <b>121</b> is used.</p><p id="p-0068" num="0067">The data organizing unit <b>131</b> receives the standardized MRI data S<b>413</b>, and organizes the standardized MRI data S<b>413</b> into a structured MRI data package S<b>2</b>, which is the final output of the MRI standardization component. The data organizing unit <b>131</b> organizes different kinds of data based on a predefined hierarchical tree structure, which will be described in detail below referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0069" num="0068">The MRI file discriminating unit <b>101</b>, MRI file reading units <b>111</b>, <b>112</b>, . . . , <b>11</b><i>n</i>, data standardizing units <b>121</b>, <b>122</b>, . . . , <b>12</b><i>n</i>, and data organizing unit <b>131</b> of the MRI standardization component are driven by a CPU processor (not shown), which performs all algebraic operations, logical operations, control tasks, and communication tasks in the MRI file discriminating and reading, as well as MRI data standardizing and organizing process. All units in the MRI standardization component mentioned above perform the high-intensity data interaction (S<b>401</b>, S<b>402</b>, S<b>403</b>, and S<b>404</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>) with the memory <b>141</b>, which saves the program codes instructing the CPU operations and also saves the intermediate data during the processing. The program codes can be written with multiple programming languages including C, C++, Java, Python, etc.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating the structure and content of the structured MRI data package generated by the MRI standardization component of the present invention. The structured MRI data package adopts a hierarchical tree structure to organize different kinds of standardized data associated with the MRI case. Each structured MRI data package contains two kinds of programming objects, namely items and groups. The item represents a data element, which may be a scalar, vector, array, or character string. The group is a container, which may contain several items or other groups.</p><p id="p-0071" num="0070">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref> the structured MRI data package contains the MRI image array (item), MRI description (group), and MRI assessment result (group). The MRI image array is the standardized MRI image data from the raw MRI file, which is a 3D array with the shape of (H,W,N), where H, W represent the height and width of each MRI slice, and N is the slice number of the MRI sequence. The pixel value range of the MRI image array is normalized to [0,1). The data type of the MRI image array is float32.</p><p id="p-0072" num="0071">The MRI description contains the standardized MRI metadata including the pixel size (item), image size (item), slice number (item), slice thickness (item), subject ID (item), date (item), and importance score (item). The pixel size and slice thickness are scalars with the data type of float32, and in the unit of millimeter (mm). The slice number is a scalar with the data type of int8. The image size is a vector with the shape of (2,) and the data type of int16. The first and second elements of image size represent the height and width of each MRI slice. The subject ID and date are both character strings with the data type of uint8, and the date has the fixed length of 8 representing the year-month-date. The importance score is a scalar with the data type of int8, which can be one of {1,2,3,4,5}. The higher importance score, the more important the corresponding MRI case is.</p><p id="p-0073" num="0072">The MRI assessment result contains the multi-tissue anatomical annotation (group), multi-pathology detection (item), and multi-pathology progression prediction (item). The multi-tissue anatomical annotation contains the pixel-level segmentation masks (item) of vertebra, intervertebral disc, nerve, blood vessel, and muscle. Each pixel-level segmentation mask is a 3D array with the same shape as the MRI image array and has the data type int8. The multi-pathology detection and multi-pathology progression prediction are vectors with the shape of (M,) and data type of int8, where M represents the number of pathologies to be assessed.</p><p id="p-0074" num="0073">The importance score of the MRI description and MRI assessment result are not determined when the structured MRI data package is generated by the MRI standardization component, and 0 is used to fill in all undetermined items, which will be edited with determined values latter.</p><p id="p-0075" num="0074">The structured MRI data package is the major form of MRI data in the communication, processing, and storage of the AI system of the present invention. All functional components and modules of the system can read and edit the structured MRI data package. The structured MRI data package can be implemented with multiple existing structured data formats, such as JavaScript Object Notation (JSON), Hierarchical Data Format (HDF), etc.</p><p id="p-0076" num="0075">Assessment: Multi-Tissue Anatomical Annotation</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating the semantic segmentation module <b>210</b> of the core assessment component <b>200</b>, which generates the multi-tissue anatomical annotation (S<b>3</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) based on the structured MRI data package (S<b>2</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>). The multi-tissue anatomical annotation S<b>3</b> consists of the pixel-level segmentation masks of multiple spinal tissues including vertebra, intervertebral disc, nerve, blood vessel, and muscle. Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the semantic segmentation module contains a ROI detecting unit (<b>211</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), a voting unit (<b>212</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), a pixel dividing unit (<b>213</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), an MRI feature enhancing unit (<b>214</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), a AI deep learning fine-tuning unit (<b>215</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), a AI deep learning segmenting unit (<b>216</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), a post-processing unit (<b>217</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), and a memory (<b>218</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0078" num="0077">The ROI detecting unit <b>211</b> receives the structured MRI data package S<b>2</b> and generates the suboptimal regions of interest (ROIs) (S<b>111</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) of multiple spinal tissues, which are transmitted to the voting unit <b>212</b>. The suboptimal ROI S<b>111</b> of each spinal tissue is a 3D array with the same shape of the MRI image array. The ROI detecting unit <b>211</b> applies a set of knowledge-based image processing operations on the MRI image array. The knowledge-based image processing operations are designed based on the anatomical a priori knowledge (such as relative positions, average sizes, shapes, etc.) about the corresponding tissues, and configurated based on the MRI description, which will be described in detail below referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0079" num="0078">The suboptimal ROI S<b>111</b> may contain mistakes, including missing objects, false positive, shape distortion, etc., which may be caused by the underlying pathologies and/or inconsistent image quality (including shape distortion, low pixel intensity, low contrast, unclear edges, and noise). The mistakes are acceptable in the suboptimal ROI S<b>111</b> and will not affect the final assessment result.</p><p id="p-0080" num="0079">The voting unit <b>212</b> receives the suboptimal ROIs S<b>111</b> of multiple spinal tissues and generates the confidence coefficient maps (S<b>121</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) of the corresponding spinal tissues, which are transmitted to the pixel dividing unit <b>213</b>. The confidence coefficient map S<b>121</b> of each spinal tissue is a 3D array with the same shape of MRI image array, and indicates the probabilities that pixels in the MRI image array may belong to the corresponding spinal tissue. The confidence coefficient map S<b>121</b> of each spinal tissue is calculated by applying a unique voting operation on the corresponding suboptimal ROI S<b>111</b>. The voting operation is based on an anatomical a priori knowledge, which is that each spinal tissue has a relatively fixed position and no drastic shape variation in adjacent MRI slices. The voting operation will be described in detail below referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0081" num="0080">The pixel dividing unit <b>213</b> receives the confidence coefficient maps S<b>121</b> of multiple spinal tissues and divides the pixels in the MRI image array into three categories, including the positive pixels (S<b>131</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) of each corresponding spinal tissue, the negative pixels (S<b>132</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), and the ambiguous pixels. Only the positive pixels S<b>131</b> and negative pixels S<b>132</b> are transmitted to the AI deep learning fine-tuning unit <b>215</b>.</p><p id="p-0082" num="0081">The positive pixels S<b>131</b> of each spinal tissue represents pixels in the MRI image array that most likely belong to the corresponding spinal tissue. The negative pixels S<b>132</b> represent pixels in MRI image array that most likely do not belong to any spinal tissues. Pixels in MRI image array that do not belong to either positive pixels or negative pixels are defined as ambiguous pixels, which cover most of the mistakes in the suboptimal ROI, and are not transmitted to the AI deep learning fine-tuning unit <b>215</b>.</p><p id="p-0083" num="0082">The positive pixels S<b>131</b> of each spinal tissue and negative pixels S<b>132</b> are represented as the 3D array with the same shape as the MRI image array. The pixel dividing unit <b>213</b> applies the thresholding and anatomical spatial constraint on the confidence coefficient maps S<b>121</b> to divide the pixels in MRI image array, which will be described in detail below referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0084" num="0083">The MRI feature enhancing unit <b>214</b> receives the structured MRI data package S<b>2</b> and generates the enhanced MRI feature arrays (S<b>141</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), which are transmitted to the AI deep learning fine-tuning unit <b>215</b> and AI deep learning segmenting unit <b>216</b>. The MRI feature enhancing unit <b>214</b> applies two different image feature enhancement operations to the MRI image array of the structured MRI data package S<b>2</b>, including pixel value local normalization and edge enhancement. The pixel value local normalization eliminates the variation of pixel value in different spinal tissues and enhances the texture features of MRI, which are calculated as:</p><p id="p-0085" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <mi>L</mi>    <mo>&#x2062;</mo>    <mi>o</mi>    <mo>&#x2062;</mo>    <mi>c</mi>    <mo>&#x2062;</mo>    <mrow>     <mi>N</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mrow>      <mi>x</mi>      <mo>,</mo>      <mi>y</mi>      <mo>,</mo>      <mi>z</mi>     </mrow>     <mo>)</mo>    </mrow>   </mrow>   <mo>=</mo>   <mfrac>    <mrow>     <msup>      <mrow>       <mo>(</mo>       <mrow>        <mrow>         <mn>2</mn>         <mo>&#x2062;</mo>         <mi>n</mi>        </mrow>        <mo>+</mo>        <mn>1</mn>       </mrow>       <mo>)</mo>      </mrow>      <mn>2</mn>     </msup>     <mo>&#xd7;</mo>     <mrow>      <mi>I</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>       <mo>,</mo>       <mi>z</mi>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mrow>     <msubsup>      <mo>&#x2211;</mo>      <mrow>       <mi>i</mi>       <mo>=</mo>       <mrow>        <mo>-</mo>        <mi>n</mi>       </mrow>      </mrow>      <mi>n</mi>     </msubsup>     <mrow>      <msubsup>       <mo>&#x2211;</mo>       <mrow>        <mi>j</mi>        <mo>=</mo>        <mrow>         <mo>-</mo>         <mi>n</mi>        </mrow>       </mrow>       <mi>n</mi>      </msubsup>      <mrow>       <msubsup>        <mo>&#x2211;</mo>        <mrow>         <mi>k</mi>         <mo>=</mo>         <mrow>          <mo>-</mo>          <mi>n</mi>         </mrow>        </mrow>        <mi>n</mi>       </msubsup>       <mrow>        <mi>I</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mrow>          <mi>x</mi>          <mo>+</mo>          <mi>i</mi>         </mrow>         <mo>,</mo>         <mrow>          <mi>y</mi>          <mo>+</mo>          <mi>j</mi>         </mrow>         <mo>,</mo>         <mrow>          <mi>z</mi>          <mo>+</mo>          <mi>k</mi>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mfrac>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0086" num="0084">where I(x,y,z) denotes the pixel value of the MRI image array, 2n+1 is the scale of a 3D local neighbourhood, and the LocN(x,y,z) represents the result of pixel value local normalization. The edge enhancement detects and enhances the edge between different spinal tissues using Sobel edge detectors, which are two orthogonal convolution kernels for detecting the gradient of pixel value in different directions. The original MRI image array, together with the pixel value local normalization and edge enhancement results, are served as the enhanced MRI feature arrays S<b>141</b>, which is a 4D array with the shape of (H, W, N, 3), where 3 represents the 3 feature channels.</p><p id="p-0087" num="0085">The AI deep learning fine-tuning unit <b>215</b> receives the enhanced MRI feature arrays S<b>141</b>, the positive pixels S<b>131</b> of each spinal tissue, and the negative pixels S<b>132</b>, and fine-tunes the AI deep learning model saved in memory <b>218</b>. The AI deep learning model is developed to generate the pixel-level segmentation masks of multiple spinal tissues based on the enhanced MRI feature arrays S<b>141</b>. The AI deep learning model has learned the general MRI image features and patterns of different spinal tissues though a pretraining process. The AI deep learning fine-tuning unit <b>215</b> fine-tunes the AI deep learning model to adapt to the variation of image features and patterns in different MRI cases, which may be caused by the underlying pathologies and/or inconsistent image quality. The AI deep learning fine-tuning process is a standard transfer learning process. The pretrained AI deep learning model is further trained with the supervision provided by positive pixels S<b>131</b> (provided by the label of the pixels in each spinal tissue) and negative pixels S<b>132</b> (provided by the label of the pixels in the background). Since the positive pixels S<b>131</b> and negative pixels S<b>132</b> are generated with the fully automated operations including ROI detection, voting, and pixel dividing, the AI deep learning model can be fine-tuned for each MRI case without requiring any manual annotation or human intervention.</p><p id="p-0088" num="0086">The AI deep learning segmenting unit <b>216</b> receives the enhanced MRI feature arrays S<b>141</b> and generates the soft segmentation result (S<b>151</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) for multiple spinal tissues, which is transmitted to the post-processing unit <b>217</b>. The soft segmentation result S<b>151</b> is a 4D array with the shape of (H, W, N, 5+1), where the last dimension 5+1 represents the probabilities that pixels in the MRI image array belong to vertebra, intervertebral disc, nerve, blood vessel, muscle, and background.</p><p id="p-0089" num="0087">The AI deep learning segmenting unit <b>216</b> utilizes the AI deep learning model fine-tuned by the AI deep learning fine-tuning unit <b>215</b>, to analysis the enhanced MRI feature array S<b>141</b>. The AI deep learning model generates the soft segmentation result S<b>151</b> based on the local MRI features and pattern of each pixel in the MRI image array. Since the fine-tuned AI deep learning model adapts to the variation of image features and patterns in different MRI cases, the soft segmentation result S<b>151</b> is much more accurate and robust than the confidence coefficient maps S<b>121</b>.</p><p id="p-0090" num="0088">The post-processing unit <b>217</b> receives the soft segmentation result S<b>151</b> and generates the multi-tissue anatomical annotation S<b>3</b>, which is the final output of the semantic segmentation module <b>210</b> of the core assessment component <b>200</b>. The post-processing unit <b>217</b> splits the soft segmentation result S<b>151</b> into six 3D arrays with the shape of (H, W, N) according to the last dimension, which are the probability arrays of corresponding spinal tissues and background. Then the post-processing unit <b>217</b> applies thresholds on each probability array with the threshold value of 0.5, which converts the probability arrays to the binary masks. Finally, the post-processing unit <b>217</b> applies the morphology operations, including opening operation and closing operation, on binary masks to remove the small noise points. The denoised binary masks are the pixel-level segmentation masks of multiple spinal tissues, which are served as the multi-tissue anatomical annotation S<b>3</b>.</p><p id="p-0091" num="0089">The ROI detecting unit <b>211</b>, voting unit <b>212</b>, pixel dividing unit <b>213</b>, MRI feature enhancing unit <b>214</b> and post-processing unit <b>217</b> are driven by or are modules of the CPU processor, which performs the algebraic operations, logic operations, control tasks, and communication tasks in the ROI detecting, voting, pixel dividing, MRI feature enhancing and post-processing process. Vectoral computation is adopted, which significantly accelerates the CPU operations through single instruction, multiple data (SIMD) strategy.</p><p id="p-0092" num="0090">The AI deep learning fine-tuning unit <b>215</b> and AI deep learning segmenting unit <b>216</b> are driven by the CPU and a graphic processing unit (GPU) processor. The CPU performs the control tasks, communication tasks, and a very few logic operations in the AI deep learning fine-tuning and segmenting process. The GPU performs the major computational tasks in the AI deep learning fine-tuning and segmenting process, which requires intensive algebraic operations.</p><p id="p-0093" num="0091">All units of the semantic segmentation module mentioned above perform the high-intensity data interaction (S<b>101</b>, S<b>102</b>, S<b>103</b>, S<b>104</b>, S<b>105</b>, S<b>106</b> and S<b>107</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) with the memory <b>218</b>, which saves the program codes instructing the CPU and GPU operations and the intermediate data during the processing. The program codes can be written with multiple programming languages including C, C++, Java, Python, etc. Further, the memory <b>218</b> also saves the AI deep learning model for the segmentation of multiple spinal tissues. For the assessment of each MRI case, the AI deep learning fine-tuning unit <b>215</b> first loads the pretrained AI deep learning model from the memory <b>218</b>, and then fine-tunes the pretrained AI deep learning model to adapt to the specific MRI case. The fine-tuned AI deep learning model is transmitted back to the memory <b>218</b> and loaded by the AI deep learning segmenting unit <b>216</b> for the segmentation task. The AI deep learning model can be developed with multiple existing frameworks including Convolutional Architecture for Fast Feature Embedding (CAFFE), TensorFlow, PyTorch, etc., and is periodically updated with a AI deep learning optimizing operation O<b>1</b>.</p><p id="p-0094" num="0092"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart illustrating the knowledge-based suboptimal ROI detecting process. Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the anchor locations are determined first in the MRI image array (P<b>211</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>). The anchor locations should be obvious in the MRI and have anatomical meaning. For example, in the sagittal T2 lumbar MRI, the upper and lower endplates of vertebral bodies are clearly displayed in mid-sagittal MRI slice, and can be distinguished and located using the Sobel edge detector. Therefore, the locations of vertebral body endplates can serve as anchor locations.</p><p id="p-0095" num="0093">Further, the locations of multiple spinal tissues are determined (P<b>212</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>) based on the anchor locations and the anatomical a priori knowledge about relative locations between spinal structures, such as, each intervertebral disc is located between a pair of vertebral endplates, each vertebra is located between two adjacent intervertebral discs, etc. Further, the boundaries of multiple spinal tissues are detected in MRI image array (P<b>213</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>). There are several existing edge detection operators can be adopted for the boundary detection, including Sobel operator, Prewitt operator, Robert operator, Canny operator, Marr-Hildreth operator, etc.</p><p id="p-0096" num="0094">Further, the suboptimal ROI of each spinal tissue is generated by combining the locations and boundaries of corresponding tissue (P<b>214</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>). More specifically, the suboptimal ROI can be generated by iteratively applying the dilation morphology operation from the location points of corresponding spinal tissue with the constraint of boundaries. The iterations and structuring element of the dilation are determined based on the a priori knowledge about the shape and size of each spinal tissue.</p><p id="p-0097" num="0095">The spatial scale information adopted in the suboptimal ROI detection process, including the distance between spinal tissues and the size of each spinal tissue, is provided in pixel (px) units and is determined based on the anatomical prior knowledge and MRI metadata. For example, the height of intervertebral disc in an MRI image array (px) is calculated by the average height of intervertebral disc (mm) divided by the pixel size (mm/px) of the MRI. Thus, for each MRI case the suboptimal ROI detection process will be reconfigured based on the MRI metadata.</p><p id="p-0098" num="0096"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating the voting process, which generates the confidence coefficient map of each spinal tissue based on the corresponding suboptimal ROI. Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, an empty 3D voting array is generated first, which has the same shape of the suboptimal ROI (P<b>221</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>). Further, the first pixel in the suboptimal ROI is selected (P<b>222</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>) and the average pixel value of the suboptimal ROI within the 3D neighborhood around the selected pixel is calculated (P<b>223</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The pixel value of the suboptimal ROI can be 1 or 0. If the pixel is in the tissue (foreground of suboptimal ROI), the pixel value is 1; otherwise, the pixel value is 0. The average pixel value is assigned to the pixel in the voting array (P<b>224</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>), which has the same location as the selected pixel in suboptimal ROI.</p><p id="p-0099" num="0097">Further, the next pixel in the suboptimal ROI is selected (P<b>226</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>), and the operations of P<b>223</b> and P<b>224</b> are applied on the selected pixel. The operations of P<b>223</b>, P<b>224</b>, and P<b>226</b> are repeated until the last pixel of suboptimal ROI is selected (P<b>225</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>) and the average pixel values are assigned to all empty positions of the voting array. The filled voting array is output as the confidence coefficient map of a specific spinal tissue (P<b>227</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The voting process illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is performed 5 times by the voting unit of semantic segmentation module for each MRI case in order to generate the confidence coefficient maps of vertebra, intervertebral disc, nerve, blood vessel, and muscle, respectively.</p><p id="p-0100" num="0098"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating the pixel dividing process, which generates the supervision, containing the positive pixels for each spinal tissue and the negative pixels, for AI deep learning fine-tuning. Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the thresholding and inverse thresholding are first applied on the confidence coefficient maps of vertebra (C<sub>1</sub>), intervertebral disc (C<sub>2</sub>), nerve (C<sub>3</sub>), blood vessel (C<sub>4</sub>), and muscle (C<sub>5</sub>), respectively (P<b>231</b>, P<b>232</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>). The thresholding process P<b>231</b> is defined as:</p><p id="p-0101" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <mi>B</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mrow>    <mi>x</mi>    <mo>,</mo>    <mi>y</mi>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mtable>    <mtr>     <mtd>      <mrow>       <mn>1</mn>       <mo>,</mo>       <mtext>&#x205f;</mtext>       <mrow>        <mrow>         <mi>C</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>x</mi>          <mo>,</mo>          <mi>y</mi>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#x3e;</mo>        <mi>T</mi>        <mtext>  </mtext>       </mrow>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mrow>       <mrow>        <mn>0</mn>        <mo>,</mo>        <mtext>&#x205f;</mtext>        <mrow>         <mrow>          <mi>C</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mi>y</mi>          </mrow>          <mo>)</mo>         </mrow>         <mo>&#x2264;</mo>         <mi>T</mi>        </mrow>       </mrow>       <mo>&#x2019;</mo>      </mrow>     </mtd>    </mtr>   </mtable>  </mrow> </mrow></math></maths></p><p id="p-0102" num="0099">and the inverse thresholding process P<b>232</b> is defined as:</p><p id="p-0103" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mrow>   <mi>B</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mrow>    <mi>x</mi>    <mo>,</mo>    <mi>y</mi>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mtable>    <mtr>     <mtd>      <mrow>       <mn>1</mn>       <mo>,</mo>       <mtext>&#x205f;</mtext>       <mrow>        <mrow>         <mi>C</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>x</mi>          <mo>,</mo>          <mi>y</mi>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#x3c;</mo>        <mi>T</mi>        <mtext>    </mtext>       </mrow>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mrow>       <mrow>        <mn>0</mn>        <mo>,</mo>        <mtext>&#x205f;</mtext>        <mrow>         <mrow>          <mi>C</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mi>y</mi>          </mrow>          <mo>)</mo>         </mrow>         <mo>&#x2265;</mo>         <mi>T</mi>        </mrow>       </mrow>       <mo>&#x2019;</mo>      </mrow>     </mtd>    </mtr>   </mtable>  </mrow> </mrow></math></maths></p><p id="p-0104" num="0100">where C(x,y) and B(x,y) represent the input confidence coefficient map and the output binary mask, and T is the threshold value. The thresholding process P<b>231</b> generates the binary arrays P<sub>1</sub>, . . . , P<sub>5</sub>, which correspond to different confidence coefficient maps, with threshold values T<sub>p1</sub>, . . . , T<sub>p5</sub>, and the inverse thresholding process P<b>232</b> generates the binary arrays N<sub>1</sub>, . . . , N<sub>5 </sub>with threshold values T<sub>n1</sub>, . . . , T<sub>n5</sub>.</p><p id="p-0105" num="0101">For each spinal tissue, the threshold value for thresholding (T<sub>pi</sub>) should be larger than the threshold value for inverse thresholding (T<sub>ni</sub>). Further, the overlaps between P<sub>1</sub>, . . . , P<sub>5 </sub>are removed (P<b>233</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>) to ensure that each pixel will not belong to more than one spinal tissue in the final supervision, and the union of N<sub>1</sub>, . . . , N<sub>5 </sub>is generated (P<b>234</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>), which is denoted as N. Further, the anatomical spatial constraint is applied to the binary arrays P<sub>1</sub>, . . . , P<sub>5 </sub>and N<sub>u </sub>(P<b>235</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>). The anatomical a priori knowledge, including the shapes, sizes, and relative locations of each spinal tissue, is adopted to constrain the foreground (pixel with the pixel value of 1) in P<sub>1</sub>, . . . , P<sub>5 </sub>to remove the noise and potential mistakes. Also, the pixels in the borders of the foregrounds in P<sub>1</sub>, . . . , P<sub>5 </sub>and N<sub>u</sub>, which may contain the potential mistakes, are also removed. Further, the supervision for AI deep learning model fine-tuning is generated (P<b>236</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>). The processed P<sub>1</sub>, . . . , P<sub>5 </sub>are served as the positive pixels of each spinal tissue, and the processed N<sub>u </sub>serves as negative pixels.</p><p id="p-0106" num="0102">Assessment: Multi-Pathology Detection</p><p id="p-0107" num="0103"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram illustrating the pathology detection module <b>220</b> of the core assessment component <b>200</b>, which is adopted in the present invention to generate the multi-pathology detection (S<b>4</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) based on the structured MRI data package (S<b>2</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) and the multi-tissue anatomical annotation (S<b>3</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>). The multi-pathology detection S<b>4</b> contains the classification and grading of multiple pathologies associated with LBP, including Schneiderman score, disc bulging, Pfirrmann grading, Schmorl's nodes, Marrow changes, etc. Referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the pathology detection module contains an intervertebral disc (IVD) region locating unit (<b>221</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>), an IVD region measuring unit (<b>222</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>), an IVD region extracting unit (<b>223</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>), an IVD region standardizing unit (<b>224</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>), a pathology detection unit (<b>225</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>), and a memory (<b>226</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>). The IVD region locating unit <b>221</b> and IVD region measuring unit <b>222</b> receive the multi-tissue anatomical annotation S<b>3</b> and determine the position (S<b>211</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) and size (S<b>212</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) of each IVD region respectively, which are transmitted to the IVD region extracting unit <b>223</b>.</p><p id="p-0108" num="0104">The IVD region of each intervertebral disc is a cuboid region, which centers at and is parallel to the corresponding intervertebral disc. The IVD region locating unit <b>221</b> determines the position of each IVD region S<b>211</b> by identifying the location and rotation of each intervertebral disc based on the multi-tissue anatomical annotation S<b>3</b>. The IVD region has the shape of (h+0.6 h<sub>vb</sub>, d+2d<sub>sc</sub>, n) where h and d represent the height and diameter of the corresponding intervertebral disc, h<sub>vb </sub>is the average height of two adjacent vertebrae bodies, and d<sub>sc </sub>is the average diameter of the spinal canal within the posterior region of the corresponding intervertebral disc. The IVD region measuring unit <b>222</b> determines the size of each IVD region S<b>212</b> by measuring the dimensions of spinal tissues mentioned above based on the multi-tissue anatomical annotation S<b>3</b>.</p><p id="p-0109" num="0105">The IVD region extracting unit <b>223</b> receives the position S<b>211</b> and size S<b>212</b> of each IVD region, as well as the structured MRI data package S<b>2</b>, and extracts the 3D IVD region array of each IVD (S<b>221</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) from MRI image array and transmits them to the IVD region standardizing unit <b>224</b>. Each IVD region array S<b>221</b> contains the whole corresponding IVD, two adjacent vertebral endplates, and the spinal canal within the posterior region of the IVD. The IVD region array S<b>221</b> covers most of the areas where the pathologies associated with LBP may occur.</p><p id="p-0110" num="0106">The IVD region standardizing unit <b>224</b> receives the 3D IVD region array of each intervertebral disc S<b>221</b> and converts the received IVD region arrays S<b>221</b> to standardized IVD region arrays (S<b>231</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>), which are transmitted to the pathology detection unit <b>225</b>. The IVD region arrays S<b>221</b> extracted by the IVD region extracting unit <b>223</b> usually have different sizes, which depend on the actual anatomical dimensions. The IVD region standardizing unit <b>224</b> adopts several interpolation technologies, including nearest interpolation, linear interpolation, bilinear interpolation, etc., to convert the sizes of received IVD region arrays S<b>221</b> to a standard size.</p><p id="p-0111" num="0107">The pathology detection unit <b>225</b> receives the standardized IVD region arrays S<b>231</b> and provides the multi-pathology detection S<b>4</b>, which is the final output of the pathology detection module of the core assessment component <b>200</b>. The multi-pathology detection S<b>4</b> includes the classification and grading results for multiple pathologies within each IVD region array.</p><p id="p-0112" num="0108">The pathology detection unit <b>225</b> adopts a AI deep learning model for the assessment of pathologies. Some pathologies have only two statuses, positive and negative, such as Schmorl's nodes, Marrow changes, etc., and the AI deep learning model will perform the binary classification on these pathologies. Some pathologies have more than two statuses, such as Schneiderman score, disc bulging, Pfirrmann grading, etc., and the AI deep learning model will perform the multi-class classification/grading on these pathologies.</p><p id="p-0113" num="0109">The IVD region locating unit <b>221</b>, IVD region measuring unit <b>222</b>, IVD region extracting unit <b>223</b>, and IVD region standardizing unit <b>224</b> are driven by the CPU processor, which performs all algebraic operations, logical operations, control tasks, and communication tasks in the IVD region locating, measuring, extracting, and standardizing process.</p><p id="p-0114" num="0110">The pathology detection unit <b>225</b> is driven by the CPU and the GPU processor. The CPU performs the control, communication, and logic operation tasks. The GPU performs the major computational tasks of AI deep learning based pathology detection process, which requires intensive algebraic operations.</p><p id="p-0115" num="0111">All units of the pathology detection module mentioned above (<b>221</b>, <b>222</b>, <b>223</b>, <b>224</b>, and <b>225</b>) perform the high-intensity data interaction (S<b>201</b>, S<b>202</b>, S<b>203</b>, S<b>204</b> and S<b>205</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) with the memory <b>226</b>. The memory <b>226</b> saves the program codes instructing the CPU and GPU operations, the intermediate data during the processing, and the AI deep learning model for the detection of multiple pathologies. The program codes can be written with multiple programming languages including C, C++, Java, Python, etc. The AI deep learning model can be developed with multiple existing frameworks including CAFFE, TensorFlow, PyTorch, etc., and periodically updated with AI deep learning optimizing operation O<b>2</b>.</p><p id="p-0116" num="0112">Assessment: Multi-Pathology Progression Prediction</p><p id="p-0117" num="0113"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating the pathology progression prediction module <b>230</b> of the core assessment component <b>200</b>, which is adopted in the AI system of the present invention to generate the multi-pathology progression prediction S<b>5</b> based on the structured MRI data package S<b>2</b>, multi-tissue anatomical annotation S<b>3</b>, and multi-pathology detection S<b>4</b>. Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the pathology progression prediction module <b>230</b> contains an IVD region locating unit (<b>231</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>), an IVD region measuring unit (<b>232</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>), an IVD region extracting unit (<b>233</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>), an IVD region standardizing unit (<b>234</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>), a pathology predicting unit (<b>235</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>), a pathology progression determining unit (<b>236</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>), and a memory (<b>237</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>).</p><p id="p-0118" num="0114">The IVD region locating unit <b>231</b>, IVD region measuring unit <b>232</b>, IVD region extracting unit <b>233</b>, and IVD region standardizing unit <b>234</b> of the pathology progression prediction module <b>230</b> are the same as the corresponding units of the pathology detection module (<b>221</b>, <b>222</b>, <b>223</b>, <b>224</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>), which receive the structured MRI data package S<b>2</b> and multi-tissue anatomical annotation S<b>3</b>, and transmit the standardized IVD region arrays (S<b>331</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>) to the pathology predicting unit <b>235</b>.</p><p id="p-0119" num="0115">The pathology predicting unit <b>235</b> receives the standardized IVD region arrays S<b>331</b> and generates multi-pathology status prediction S<b>341</b>, which are transmitted to the pathology progression determining unit <b>236</b>. The pathology predicting unit <b>235</b> adopts a AI deep learning model for the predicting task. For the pathologies with only two statuses, the AI deep learning model will predict whether the pathologies will be positive or negative in about 5 years. For the pathologies with more than two statuses, the AI deep learning model will predict the future grades of the pathologies in about 5 years.</p><p id="p-0120" num="0116">The pathology progression determining unit <b>236</b> receives the multi-pathology status prediction S<b>341</b> and multi-pathology detection S<b>4</b>, and generates the multi-pathology progression prediction S<b>5</b>, which is the final output of the pathology progression prediction module <b>230</b> of the core assessment component <b>200</b>. The pathology progression status, including progression and non-progression, is determined based on the future status and detection result (current status) of each pathology. More specifically, for a pathology with two statuses, if the current status is negative and the future status is positive, the pathology is determined as progression; otherwise, the pathology is determined as non-progression. For the pathology with more than two statuses, if the future grade is larger than current grade, the pathology is determined as progression, otherwise the pathology is determined as non-progression.</p><p id="p-0121" num="0117">The IVD region locating unit <b>231</b>, IVD region measuring unit <b>232</b>, IVD region extracting unit <b>233</b>, IVD region standardizing unit <b>234</b>, and pathology progression determining unit <b>236</b>, are driven by the CPU processor, which performs all algebraic operations, logic operations, control tasks, and communication tasks in the IVD region locating, measuring, extracting, and standardizing units, as well as the pathology progression determining unit.</p><p id="p-0122" num="0118">The pathology predicting unit <b>235</b> is driven by the CPU and the GPU processor. The CPU performs the control, communication, and logic operation tasks, and the GPU performs the major computational tasks of AI deep learning based pathology future status prediction, which requires intensive algebraic operations.</p><p id="p-0123" num="0119">All units of the pathology progression prediction module <b>239</b> mentioned above (<b>231</b>, <b>232</b>, <b>233</b>, <b>234</b>, <b>235</b> and <b>226</b>) perform the high-intensity data interaction (S<b>301</b>, S<b>302</b>, S<b>303</b>, S<b>304</b>, S<b>305</b> and S<b>306</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) with the memory <b>237</b>. The memory <b>237</b> saves the program codes instructing the CPU and the GPU operations, the intermediate data during the processing, and the AI deep learning model for the prediction of the future status of multiple pathologies. The program codes can be written with multiple programming languages including C, C++, Java, Python, etc. The AI deep learning model can be developed with multiple existing frameworks including CAFFE, TensorFlow, PyTorch, etc., and periodically updated with AI deep learning optimizing operation O<b>3</b>.</p><p id="p-0124" num="0120">AI Deep Learning Optimization</p><p id="p-0125" num="0121"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating the model optimization component <b>300</b>, which is adopted in the AI system of the present invention to periodically optimize different AI deep learning models used in different modules of the core assessment component <b>200</b> via the corresponding AI deep learning optimizing operations O<b>1</b>, O<b>2</b>, O<b>3</b>, based on the structured MRI data package S<b>2</b>, MRI assessment results (includes multi-tissue anatomical annotation S<b>3</b>, multi-pathology detection S<b>4</b>, and multi-pathology progression prediction S<b>5</b>), specialist's comment S<b>6</b>, and control signal S<b>7</b>. Referring to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the model optimization component <b>300</b> contains a data recording unit (<b>301</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>), a data archiving unit (<b>311</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>), three AI deep learning optimizing units (<b>321</b>, <b>322</b>, and <b>323</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>), a data storage system (<b>331</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>), and a memory (<b>341</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>).</p><p id="p-0126" num="0122">The data recording unit <b>301</b> merges the received structured MRI data package S<b>2</b> and MRI assessment results S<b>3</b>, S<b>4</b>, S<b>5</b> together, and transmits the structured MRI data package with assessment results (S<b>511</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>) to the data archiving unit <b>311</b>. The structured MRI data package S<b>2</b> is generated by the MRI standardization component. The importance score of the MRI description and items of the MRI assessment result in the structured MRI data package S<b>2</b> are undetermined, so they are filled by 0. The data recording unit <b>301</b> merges the received data by assigning the MRI assessment results S<b>3</b>, S<b>4</b>, S<b>5</b> generated by the core assessment component <b>200</b> to the corresponding undermined items of the structured MRI data package S<b>2</b>.</p><p id="p-0127" num="0123">The data archiving unit <b>311</b> receives the structured MRI data package with assessment results S<b>511</b> and specialist's comment S<b>6</b>, and archives the MRI data with clinical label (S<b>512</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>) in the data storage system <b>331</b>. The data archiving unit <b>311</b> may correct the automated assessment results generated by the AI system and device, and assign the importance score to each MRI case, based on the specialist's comment S<b>6</b>. The corrected assessment results serve as the clinical label for the optimization of the AI deep learning model. The data archiving process will be described in detail below referring to <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0128" num="0124">The AI deep learning optimizing units <b>321</b>, <b>322</b>, <b>323</b> optimize the AI deep learning models of the semantic segmentation module <b>210</b>, pathology detection module <b>220</b>, and pathology progression prediction module <b>230</b>, via the AI deep learning optimizing operations O<b>1</b>, O<b>2</b>, O<b>3</b>, respectively, which are based on the accumulated MRI data with clinical labels (S<b>513</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>) loaded from the data storage system <b>331</b>. The AI deep learning optimizing units <b>321</b>, <b>322</b>, <b>323</b> receive the control signal S<b>7</b> to trigger the AI deep learning optimizing operations O<b>1</b>, O<b>2</b>, O<b>3</b>, which adopt the standard transfer learning technology.</p><p id="p-0129" num="0125">The AI deep learning optimizing unit <b>1</b> <b>321</b> loads the back-up AI deep learning model for semantic segmentation module <b>210</b> (S<b>514</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>) and accumulated MRI data with clinical label S<b>513</b> from the data storage system <b>331</b>, and further trains the loaded AI deep learning model with the MRI image array and multi-tissue anatomical annotation results within the loaded MRI data. The AI deep learning optimizing operation O<b>1</b> updates the AI deep learning model of the semantic segmentation module with the trained (optimized) AI deep learning model, which is also transmitted back to the data storage system <b>331</b> to update the back-up AI deep learning model for semantic segmentation module S<b>514</b>. The AI deep learning optimizing unit <b>2</b> <b>322</b> and unit <b>3</b> <b>323</b> adopt the same process for the AI deep learning optimizing operations of the pathology detection module O<b>2</b> and the pathology progression prediction module O<b>3</b>, except that the AI deep learning optimizing unit <b>2</b> <b>322</b> utilizes the MRI image array and multi-pathology detection results for training, and the AI deep learning optimizing unit <b>3</b> <b>323</b> utilizes the baseline MRI image array and follow-up multi-pathology detection results for training.</p><p id="p-0130" num="0126">The MRI case with a higher importance score will be assigned more weight in training. The AI deep learning optimizing operations O<b>1</b>, O<b>2</b>, O<b>3</b> should be performed when a certain amount of new clinic MRI data (usually about 500 cases) is accumulated. The AI deep learning optimizing operations O<b>1</b>, O<b>2</b>, O<b>3</b> are triggered by the control signal S<b>7</b>, which can be generated manually by an administrator of the AI system or automatically by an external timing devices for periodic optimization.</p><p id="p-0131" num="0127">The data storage system <b>331</b> adopts the hard disk drive (HDD) or solid-state drive (SSD) for the long-term storage of clinic MRI data and back-up AI deep learning models. In the data storage system <b>331</b>, the MRI data with clinical label is grouped by subject ID and sorted by date, and the AI deep learning models are saved with the structured data formats, such as HDF (HDF4, HDF5), JSON, etc. For the AI deep learning optimizing operations O<b>1</b>, O<b>2</b>, O<b>3</b>, the data storage system <b>331</b> will feed a part of the archived MRI data, which contains all newly accumulated MRI cases (i.e., cases that have not yet been used for optimization) and randomly selected old MRI cases, to the AI deep learning optimizing units <b>321</b>, <b>322</b>, <b>323</b>. For each old MRI case, a higher importance score means a higher probability of being selected. After AI deep learning optimizing operations O<b>1</b>, O<b>2</b>, O<b>3</b>, the trained (optimized) AI deep learning models will be validated on all MRI cases used for optimization, and for each MRI case, if the assessment result generated by the trained (optimized) AI deep learning models has improved, the importance score will be reduced, otherwise, the importance score will be increased.</p><p id="p-0132" num="0128">The data recording unit <b>301</b> and data archiving unit <b>311</b> are driven by the CPU processor, which performs all algebraic operations, logic operations, control tasks, and communication tasks in the data recording and archiving process.</p><p id="p-0133" num="0129">The AI deep learning optimizing units <b>321</b>, <b>322</b>, <b>323</b> are driven by the CPU and the GPU processor. The CPU performs the control, communication, and logic operation tasks, and the GPU performs the major computational task of AI deep learning training (optimizing) process, which requires intensive algebraic operations.</p><p id="p-0134" num="0130">All units of the model optimization component mentioned above (<b>301</b>, <b>311</b>, <b>321</b>, <b>322</b>, and <b>323</b>) perform the high-intensity data interaction (S<b>501</b>, S<b>502</b>, S<b>503</b>, S<b>504</b> and S<b>505</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>) with the memory <b>341</b>. The memory <b>341</b> saves the program codes instructing the CPU and GPU operations and the intermediate data during the processing. The program codes can be written with multiple programming languages including C, C++, Java, Python, etc.</p><p id="p-0135" num="0131"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating the data archiving process, which corrects the automated assessment result and assigns the importance score of each MRI case based on the specialist's comment. Referring to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the structured MRI data package with automated assessment results and specialist's comment are received first (P<b>311</b> and P<b>312</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>). Further, a determination is made as to whether there is any disagreement with the automated assessment results in the specialist's comments (P<b>313</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>). If there is any disagreement, the automated assessment results will be corrected accordingly (P<b>315</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>), and the structured MRI data package will be assigned the highest importance score (P<b>316</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>). Otherwise, a determination is made as to whether the importance score is specified in the specialist's comment (P<b>314</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>). If the importance score is specified, the specified importance score is assigned to the structured MRI data package (P<b>317</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>), otherwise, the lowest importance score is assigned to the structured MRI data package (P<b>318</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>). Further, the processed structured MRI data package is saved in the data storage system.</p><p id="p-0136" num="0132">Examples of AI System and Device Deployment</p><p id="p-0137" num="0133">The AI system and device of the present invention is able to perform the same MRI assessment tasks in the application scenarios with different scales (different number of clinicians, patients, institutions, etc.), without adjusting the overall structure, workflow, and functional component division. All that is needed is to adopt different hardware (CPU processor, GPU processor, memory, data storage device, etc.) for the different computational power and data storage requirements. <figref idref="DRAWINGS">FIG. <b>12</b></figref>, <figref idref="DRAWINGS">FIG. <b>13</b></figref>, and <figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrate different deployment schemes of the AI system in a single clinic (personal level), small scale (institutional level), and medium-large scale (regional level) application scenarios.</p><p id="p-0138" num="0134"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example of the AI system and device for single clinic scale (personal level) application scenarios, such as a primary health care institution, single clinic, etc. All functional components, including the MRI standardization component <b>405</b>, core assessment component <b>410</b>, and model optimization component <b>415</b>, are integrated into a portable AI device <b>400</b>, which is the size of a personal laptop computer.</p><p id="p-0139" num="0135">The CPU processor <b>420</b>, GPU processor <b>422</b>, and memory <b>424</b> are integrated into one single motherboard, which can be reused in different functional components, i.e. different modules of the CPU and GPU may perform the functions of the different components. As an alternative, each component may have its own separate CPU, GPU and memory. In either case the GPU is a specialized processor designed to rapidly manipulate and alter memory, and has a highly parallel structure for large scale algebraic operations. The memory <b>424</b> and data storage <b>426</b>, can be HDD or SSD devices, which are also integrated into the AI device, and can be further extended with an external data storage device. Since all components are integrated together, no special device is required for the intra-system communication.</p><p id="p-0140" num="0136">The AI device contains several common I/O interfaces <b>430</b>, <b>432</b>, including USB (type-A and type-C), Video Graphics Adapter (VGA), Digital Visual Interface (DVI), High-Definition Multimedia Interface (HDMI), etc., for communicating with external devices, <b>438</b>. Besides, the AI device may contain special I/O interfaces <b>434</b> for communicating with MRI equipment <b>436</b>. Moreover, the AI device may contain wireless communication device, such as a local area network (LAN), 5G network, etc., to assess the remote data. Several input and output devices <b>437</b>, <b>438</b> including keyboard, display, etc., are also integrated into the AI device.</p><p id="p-0141" num="0137"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an example of the AI system and device for small scale (institutional level) application scenarios, such as a large general hospital, large specialized hospital, etc. A set of personal computers (PC) serve as the MRI standardization component <b>450</b> of the AI system, which PCs are distributed in different clinics to collect and standardize the raw MRI files, and provide the interactions between the system and clinicians. Each PC is connected to a set of MRI File Sources and Input/output devices <b>460</b> through individual I/O interfaces <b>470</b>. The MRI standardizing process does not require a large amount of computational power and data storage, which can be supported by the CPU processor and memory within common office PCs. Besides, the PC usually contains multiple I/O interfaces, wireless communication device, and external devices, which are sufficient to support the data interaction between the AI system and MRI sources/clinicians.</p><p id="p-0142" num="0138">The core assessment and model optimization components <b>500</b> of the AI system are driven by a CPU server <b>502</b>, a GPU server <b>504</b>, and a data storage server <b>506</b>. The CPU server <b>502</b> and GPU server <b>504</b> are hardware upgrades of the CPU processor and GPU processor previously discussed, which can provide much more computational power to handle the peak MRI assessment demand of the whole institution. The data storage server <b>506</b> is a hardware upgrade of the single HDD or SSD device, which can provide much more data storage capacity to archive clinic MRI data generated by the whole institution.</p><p id="p-0143" num="0139">The CPU server, GPU server, and data storage server require a dedicated space, such as a central machine room or server room, which has a relatively high demand for environmental refrigeration and cleanliness. The communication between different functional components of the AI system requires special communication devices, such as LAN, 5G network, optical fiber.</p><p id="p-0144" num="0140"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates an example of the AI system and device for medium-large scale (regional level) application scenarios, such as multiple large hospitals in the same area, etc. Similar to the deployment strategy in the small scale (institutional level) application scenarios, MRI standardization components <b>450</b>, <b>450</b>&#x2032; also consists of a set of PCs, and each PC is adopted for the MRI standardization and data interaction between the AI system and MRI sources/clinicians <b>460</b>, <b>460</b>&#x2032; of each clinic through interfaces <b>470</b>, <b>470</b>&#x2032;.</p><p id="p-0145" num="0141">For each institution, the MRI data received from different clinics will be transmitted to a core assessment component <b>500</b>, <b>500</b>&#x2032; supported by a CPU server and a GPU server, which will assess all MRI data from this institution. MRI data from different institutions within the region will be aggregated and archived to a data center <b>550</b>, which is the model optimization component of the AI system and device and contains a more powerful CPU server <b>560</b>, GPU server <b>570</b> and data storage server <b>580</b> for storing all MRI data of this region, and optimizing the deep learning models based on the MRI data of this region. The communication between different functional components of the AI system requires the wireless communication device, such as LAN, 5G network, etc.</p><p id="p-0146" num="0142">The AI system and device for the application scenarios with different scales are compatible with each other. For example, the single clinic scale (personal level) AI device can be used as the MRI standardization component of a small scale (institutional level) or medium-large scale (regional level) AI system, and the small scale AI system can be merged into a medium-large scale AI system by uploading the MRI data to the regional data center and sharing the MRI data and AI deep learning models of this region.</p><p id="p-0147" num="0143">Unsupervised Deed Learning Pipeline</p><p id="p-0148" num="0144">As noted above, MRI is widely used in the diagnosis of intervertebral disc degeneration. [1] Most deep learning based vertebral segmentation methods require laborious manual labelling tasks. The present invention permits an unsupervised deep learning pipeline for vertebral segmentation of MR images by integrating the sub-optimal segmentation results produced by a rule-based method with a unique voting mechanism to provide supervision in the training process for the deep learning model. The present invention has been shown to have a high segmentation accuracy without relying on any manual labelling. Potential applications are in automated pathology detection and vertebral 3D reconstructions for biomechanical simulations and 3D printing, facilitating clinical decision making, surgical planning and tissue engineering.</p><p id="p-0149" num="0145">Vertebral segmentation is an essential part of many lumbar MRI automated analysis tasks, such as pathology detection and vertebral 3D reconstruction for further mechanical simulations or 3D printing [2].</p><p id="p-0150" num="0146">Prior art unsupervised segmentation methods are based on geometrical characteristics of the vertebrae [3-8], with specific templates, detectors or rules designed to identify and segment these. However, due to the complexity of the MR images, the accuracy and robustness of these unsupervised methods are low. Thus, they cannot be used for clinical practice. Supervised methods typically train a segmentation model using manually masked vertebrae, and apply it on new images [9-11]. Using a convolutional neural network (CNN) [12], supervised methods can achieve high segmentation accuracy [10-11]. However, the laborious manual segmentation task is still required for the training process of supervised algorithms.</p><p id="p-0151" num="0147">Original spine MR images contain more than the required information for vertebral segmentation, which may increase the complexity of the learning task and reduce the accuracy of the output. Therefore, the present invention uses a regional convolutional neural network (CNN) strategy. The training and predicting process of the network are restrained to a specific vertebral region to filter out the irrelevant context. A voting mechanism integrating a set of sub-optimal segmentation results can provide satisfactory supervision for the training of a deep learning network. Thus, manual labels are not be required.</p><p id="p-0152" num="0148">As indicated above, the present invention has the following steps: 1) selecting the region of interest (ROI) of each slice via a rule-based method; 2) integrating the ROIs via a voting mechanism to identify a volume of interest (VOI); and 3) training a CNN model to segment vertebrae within the identified VOI.</p><p id="p-0153" num="0149">Dataset and Pipeline</p><p id="p-0154" num="0150">A trial was set up to prove the features of the present invention. The trial included 243 volunteers who were recruited by open advertisement if they were above 18 years old without any cancers and previous spinal surgeries. Their lumbar MR images were collected and used for the testing of the present invention. The resolution of these images was 448&#xd7;448. There were more than 15 slices in each MRI series. The average vertebral area in the MR images was 1650 pixels (range 800-2200), the average vertebral height was 40 pixels (range 30-55), and the average vertebral perimeter was 140 pixels (range 100-200), indicating the diversified nature of the dataset.</p><p id="p-0155" num="0151">The test procedure consisted of 3 major components, 1) the ROI detection (<figref idref="DRAWINGS">FIG. <b>15</b>A</figref>: blue), 2) the voting mechanism (<figref idref="DRAWINGS">FIGS. <b>15</b>B</figref> and C: pink), and 3) the CNN (<figref idref="DRAWINGS">FIGS. <b>15</b>D</figref> and E: green). The ROI detection is a rule-based vertebral detection process used to identify the ROI in each MRI slice. During the voting mechanism, ROIs of all slices were integrated to generate the VOI of the series (<figref idref="DRAWINGS">FIG. <b>15</b>B</figref>), and then the pixels in the VOI were classified as 1) the vertebrae (positive pixels), 2) likely to be the vertebrae (ambiguous pixels) and 3) not the vertebrae (negative pixels). <figref idref="DRAWINGS">FIG. <b>15</b>C</figref>. In the last component of the CNN-based vertebral segmentation (<figref idref="DRAWINGS">FIG. <b>15</b>D</figref>), the network was trained by positive pixels and negative pixels to segment all the vertebrae in a VOI. <figref idref="DRAWINGS">FIG. <b>15</b>E</figref>.</p><p id="p-0156" num="0152">Rule-Based ROI Detection</p><p id="p-0157" num="0153">The ROI of each MRI slice was defined as the vertebral regions identified by the rule-based detecting process (<figref idref="DRAWINGS">FIG. <b>16</b>A</figref>). The local normalization [13] was first adopted to eliminate the pixel intensity variation in different tissues and to highlight vertebral edges (<figref idref="DRAWINGS">FIG. <b>16</b>B</figref>). The vertebral edges were then further enhanced with a Sobel filter, and after local thresholding, we obtained a binary mask in which the vertebrae were separated (<figref idref="DRAWINGS">FIG. <b>16</b>C</figref>). ROIs were all connected components in the mask having an area and perimeter within the range of vertebrae (<figref idref="DRAWINGS">FIG. <b>16</b>D</figref>).</p><p id="p-0158" num="0154">Voting Mechanism</p><p id="p-0159" num="0155">The VOI of each MRI series consisted of all pixels that might belong to the vertebrae. Due to the consistent location of the vertebrae on each MRI slice, the VOI cross-section in different slices was the same. Due to spinal pathologies and the limited identification ability of the rule-based method, there were some errors in the ROIs, such as missing vertebrae and mistaken selection of non-vertebral regions <figref idref="DRAWINGS">FIG. <b>16</b>D</figref>). Therefore, a novel voting mechanism was used to integrate the sub-optimal ROIs and identify the VOI of a MRI series.</p><p id="p-0160" num="0156">In the following statements, the ROI of slice i and the VOI are denoted as ROIi (x, y) and VOI(x, y). ROIi (x, y)=1, if the pixel at (x, y) is in the ROI of slice i; ROIi (x, y)=0, if it is not. A similar definition holds for VOI (x, y).</p><p id="p-0161" num="0157">The process of voting included:</p><p id="p-0162" num="0158">Step 1: Assign weights to all pixels in ROIs and create a weight map for each MRI slice. Since the pixels at the edge of ROIs were more likely to be misjudged than those at the center, the central pixels were assigned more weight. The weight map of slice i, denoted as Wi (x, y), is defined as:</p><p id="p-0163" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>W</mi>       <mi>i</mi>      </msub>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mrow>       <mtable>        <mtr>         <mtd>          <mrow>           <mrow>            <msub>             <mi>w</mi>             <mi>c</mi>            </msub>            <mo>&#x2062;</mo>            <mrow>             <msub>              <mi>ROI</mi>              <mi>i</mi>             </msub>             <mo>(</mo>             <mrow>              <mi>x</mi>              <mo>,</mo>              <mi>y</mi>             </mrow>             <mo>)</mo>            </mrow>           </mrow>           <mo>,</mo>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mrow>            <msub>             <mi>w</mi>             <mi>e</mi>            </msub>            <mo>&#x2062;</mo>            <msub>             <mi>ROI</mi>             <mi>i</mi>            </msub>            <mo>&#x2062;</mo>            <mrow>             <mo>(</mo>             <mrow>              <mi>x</mi>              <mo>,</mo>              <mi>y</mi>             </mrow>             <mo>)</mo>            </mrow>           </mrow>           <mo>,</mo>          </mrow>         </mtd>        </mtr>       </mtable>       <mo>&#x2062;</mo>       <mtext>  </mtext>       <mtable>        <mtr>         <mtd>          <mrow>           <mrow>            <mi>d</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mrow>             <mi>x</mi>             <mo>,</mo>             <mi>y</mi>             <mo>,</mo>             <msub>              <mi>B</mi>              <mn>1</mn>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mo>&#x2265;</mo>           <msub>            <mi>d</mi>            <mi>e</mi>           </msub>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mrow>            <mi>d</mi>            <mo>&#x2062;</mo>            <mrow>             <mo>(</mo>             <mrow>              <mi>x</mi>              <mo>,</mo>              <mi>y</mi>              <mo>,</mo>              <msub>               <mi>B</mi>               <mn>1</mn>              </msub>             </mrow>             <mo>)</mo>            </mrow>           </mrow>           <mo>&#x3c;</mo>           <msub>            <mi>d</mi>            <mi>e</mi>           </msub>          </mrow>         </mtd>        </mtr>       </mtable>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0164" num="0000">where W<sub>c </sub>and W<sub>e </sub>represent the weight assigned to central and edge pixels, d<sub>e </sub>is the cut-off for the separation of ROI center and edge, B<sub>1</sub>={(x, y)|ROI<sub>i</sub>(x, y)=0}, and the d(x, y, B<sub>1</sub>) represents the distance from (x, y) to B<sub>1</sub>, which is defined as:</p><p id="p-0165" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>d</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>       <mo>,</mo>       <msub>        <mi>B</mi>        <mn>1</mn>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <munder>       <mi>min</mi>       <mrow>        <mrow>         <mo>(</mo>         <mrow>          <mi>a</mi>          <mo>.</mo>          <mi>b</mi>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#x2208;</mo>        <msub>         <mi>B</mi>         <mn>1</mn>        </msub>       </mrow>      </munder>      <msqrt>       <mrow>        <msup>         <mrow>          <mo>(</mo>          <mrow>           <mi>x</mi>           <mo>-</mo>           <mi>a</mi>          </mrow>          <mo>)</mo>         </mrow>         <mn>2</mn>        </msup>        <mo>+</mo>        <msup>         <mrow>          <mo>(</mo>          <mrow>           <mi>y</mi>           <mo>-</mo>           <mi>b</mi>          </mrow>          <mo>)</mo>         </mrow>         <mn>2</mn>        </msup>       </mrow>      </msqrt>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0166" num="0159">Step 2: Calculate the voting result V(x, y) as:</p><p id="p-0167" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>V</i>(<i>x,y</i>)=&#x3a3;<sub>i</sub><sup>N</sup><i>W</i><sub>i</sub>(<i>x,y</i>)&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0168" num="0000">where N represents the number of MRI slices.</p><p id="p-0169" num="0160">Step 3: Define the vertebra central region Rvc in the VOI as:</p><p id="p-0170" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>vc</sub>={(<i>x,y</i>)|<i>V</i>(<i>x,y</i>)&#x2265;<i>T</i><sub>h</sub><i>,d</i>(<i>x,y,B</i><sub>2</sub>)&#x2265;<i>h</i><sub>ave</sub>/8}&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0171" num="0000">where T<sub>h </sub>is the higher voting cut-off, h<sub>ave </sub>is the average vertebral height and B<sub>2</sub>={(x, y)|V(x, y)&#x3c;T<sub>h</sub>}</p><p id="p-0172" num="0161">Step 4: Define the potential vertebral region R<sub>pv </sub>in the VOI as:</p><p id="p-0173" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Rpv</i>={(<i>x,y</i>)|<i>V</i>(<i>x,y</i>)&#x2265;<i>T</i><sub>l</sub>}&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0174" num="0000">where T<sub>l </sub>represents the lower voting cut-off.</p><p id="p-0175" num="0162">Step 5: Check each connected component Ci in Rpv, and</p><p id="p-0176" num="0000">select it if Ci&#x2229;Rvc&#x2260;&#xd8;. The determined vertebral region Rdv in the VOI consisted of all the selected connected components.</p><p id="p-0177" num="0163">Step 6: In case some vertebral pixels were not identified in any ROI, the neighborhood of Rdv was also merged in the VOI. Therefore, the VOI is defined as (<figref idref="DRAWINGS">FIG. <b>17</b>A</figref>):</p><p id="p-0178" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>VOI</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mtable>      <mtr>       <mtd>        <mrow>         <mo>{</mo>         <mrow>          <mn>1</mn>          <mo>,</mo>          <mrow>           <mrow>            <mi>d</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mrow>             <mi>x</mi>             <mo>,</mo>             <mi>y</mi>             <mo>,</mo>             <mi>Rdv</mi>            </mrow>            <mo>)</mo>           </mrow>           <mo>&#x2264;</mo>           <msub>            <mi>d</mi>            <mi>n</mi>           </msub>          </mrow>         </mrow>        </mrow>       </mtd>      </mtr>      <mtr>       <mtd>        <mrow>         <mo>{</mo>         <mrow>          <mn>0</mn>          <mo>,</mo>          <mrow>           <mrow>            <mi>d</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mrow>             <mi>x</mi>             <mo>,</mo>             <mi>y</mi>             <mo>,</mo>             <mi>Rdv</mi>            </mrow>            <mo>)</mo>           </mrow>           <mo>&#x3e;</mo>           <msub>            <mi>d</mi>            <mi>n</mi>           </msub>          </mrow>         </mrow>        </mrow>       </mtd>      </mtr>     </mtable>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0179" num="0000">where d<sub>n </sub>represents the maximum neighborhood distance.</p><p id="p-0180" num="0164">Based on the voting result and the ROIs, the pixels in a VOI could be further classified into 3 categories: positive pixels, ambiguous pixels and negative pixels. These three categories for slice i (<figref idref="DRAWINGS">FIG. <b>17</b>B</figref>), denoted as Pos<sub>i</sub>, Amb<sub>i </sub>and Negi, are defined as follows:</p><p id="p-0181" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Posi={(<i>x,y</i>)|<i>ROI</i><sub>i</sub>(<i>x,y</i>)=1,<i>V</i>(<i>x,y</i>)&#x2265;<i>T</i><sub>p</sub>}&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0182" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Negi={(<i>x,y</i>)|<i>V</i>(<i>x,y</i>)=0}&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0183" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Ambi={(<i>x,y</i>)|(<i>x,y</i>)&#x2208;Pos<sub>i</sub>&#x222a;Neg<sub>i</sub>}&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0184" num="0000">where the T<sub>p </sub>represents the positive voting cut-off, and (x, y) satisfies VOI(x, y)=1.</p><p id="p-0185" num="0165">The positive pixels were identified in most ROIs and had a high probability of belonging to vertebrae, while the negative pixels were in the VOI but not included in any ROI.</p><p id="p-0186" num="0166">Deep Learning Based Vertebrae Segmentation</p><p id="p-0187" num="0167">A convolutional neural network (CNN) was trained to further segment vertebrae from the VOI. Unlike the conventional network training process, in the pipeline of the present invention no manual labels were involved. Instead, the system relies on the positive and negative pixels in the VOIs to provide supervision.</p><p id="p-0188" num="0168">The input of the CNN was a combination of the original MRI image (<figref idref="DRAWINGS">FIG. <b>18</b>A</figref>), the local normalized image (<figref idref="DRAWINGS">FIG. <b>18</b>B</figref>) and the edge detection results (<figref idref="DRAWINGS">FIG. <b>18</b>C</figref>). Since the local context was enough for segmentation, the network received a small patch of input images of size 24&#xd7;24, one at a time, and could sufficiently determine whether the center pixel of the patch belonged to the vertebrae. To improve the localization sensitivity of the model, coordinate channels [14] were introduced, which were two constant matrices of the same size as the input patches. <figref idref="DRAWINGS">FIG. <b>18</b>D</figref> The coordinate channels indicated the relative position of each pixel in the input patch, which could help the network to better understand the spatial distribution of features. Input to the network was processed by 3 successive convolution (Cony) blocks, each containing two convolutional layers with a kernel size of 3&#xd7;3, and one max-pooling layer of kernel size 2&#xd7;2. <figref idref="DRAWINGS">FIG. <b>18</b>E</figref> After each Cony block, the channels were doubled. Two fully connected layers followed the Cony blocks with <b>1024</b> hidden units. All convolutional layers and fully-connected layers were activated via Rectified Linear Unit (ReLU) (<figref idref="DRAWINGS">FIG. <b>18</b>F</figref>), and the output layer was activated by softmax [12]. <figref idref="DRAWINGS">FIG. <b>18</b>G</figref> The output of the network was the probability that the center pixel of the input patch belonged to the vertebrae. After inferring all pixels in the VOI, the vertebral segmentation result was obtained.</p><p id="p-0189" num="0169">The training process of the network could be divided into pre-training and fine-tuning. In the pre-training process, the same number of positive and negative pixels are selected randomly from all MRI series to train the network. Then the pre-trained model is fine-tuned on each MRI series to deal with individual differences. All positive and negative pixels from the MRI series were used in the fine-tuning. The optimized model was used to segment all vertebrae in the VOI of the MRI series.</p><p id="p-0190" num="0170">Sagittal lumbar MRIs obtained from the 234 patients were used for the validation of the present invention pipeline, with the mid-9 slices selected for segmentation. In the voting process, the weight of center pixels W<sub>c </sub>and edge pixels we were set to 2 and 1, respectively. The d<sub>e </sub>and d<sub>n </sub>were both set to 5. The T<sub>h</sub>, T<sub>i </sub>and T<sub>p </sub>were set to 8, 1 and 4, respectively.</p><p id="p-0191" num="0171">In the pre-training process, 100,000 positive pixels and 100,000 negative pixels were selected randomly from all MRI series. The pre-training took 5 epochs, and the fine-tuning generally took 1-3 epochs, depending on the different MRI images. Stochastic gradient descent (SGD) was used to optimize the model in both the pre-training and fine-tune stages.</p><p id="p-0192" num="0172">Preliminary test results of the pipeline (<figref idref="DRAWINGS">FIG. <b>19</b>B</figref>) were visually compared with the results of the rule-based method (<figref idref="DRAWINGS">FIG. <b>19</b>A</figref>). By comparing the present invention with the prior results, it can be seen that the present invention achieves significantly higher segmentation accuracy by eliminating any missing vertebrae and image distortion.</p><p id="p-0193" num="0173">Thus, the present invention represents a novel unsupervised vertebral segmentation pipeline for sagittal lumbar MR images. It demonstrates high segmentation accuracy without relying on manual labels by adopting a regional CNN strategy in the pipeline. An ROI is determined first in each slice by a rule-based method and then it is integrated through a unique voting mechanism to produce a VOI. The parameters in the rule-based ROI detection are empirically selected, but may need to be fine-tuned on another dataset. Since the ROI can be suboptimal, the fine-tuning process is simple. The pixels within the VOI are further divided into positive pixels, ambiguous pixels and negative pixels. The model is trained by the positive and negative pixels to segment all vertebrae. The training and segmenting process are both within the predefined VOI, with any information not necessary for the segmentation being filtered out.</p><p id="p-0194" num="0174">Compared with previously reported supervised pipeline segmentation approaches [9-12] the present invention has the advantage of eliminating laborious manual labelling, while achieving comparable segmentation accuracy. In comparison with unsupervised methods [3-8], the present invention solves the issues of missing vertebrae and image distortion. These outcomes are due to the integrated CNN that is able to learn and detect vertebrae having a wide range of feature variations caused not only by underlying disease, but by inconsistencies in image quality since the MRIs were acquired by different machines and human operators. This system and process can be used with other medical imaging modalities, or even to segment other anatomical structures in MRIs.</p><p id="p-0195" num="0175">The cited references in this application are incorporated herein by reference in their entirety and are as follows:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0176">[1] C. W. Pfirrmann, A. Metzdorf, M. Zanetti, J. Hodler, and N. Boos, &#x201c;Magnetic resonance classification of lumbar intervertebral disc degeneration,&#x201d; Spine, vol. 26, no. 17, pp. 1873-1878, 2001.</li>    <li id="ul0001-0002" num="0177">[2] J. S. Ramos, M. T. Cazzolato, B. S. Fai&#xe7;al, M. H. Nogueira-Barbosa, C. Traina, and A. J. Traina, &#x201c;3DBGrowth: volumetric vertebrae segmentation and reconstruction in magnetic resonance imaging,&#x201d; in 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS), 2019, pp. 435-440: IEEE.</li>    <li id="ul0001-0003" num="0178">[3] S. Benameur, M. Mignotte, S. Parent, H. Labelle, W. Skalli, and J. de Guise, &#x201c;3D/2D registration and segmentation of scoliotic vertebrae using statistical models,&#x201d; Computerized Medical Imaging Graphics, vol. 27, no. 5, pp. 321-337, 2003.</li>    <li id="ul0001-0004" num="0179">[4] Z. Peng, J. Zhong, W. Wee, and J.-h. Lee, &#x201c;Automated vertebra detection and segmentation from the whole spine MR images,&#x201d; in 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference, 2006, pp. 2527-2530: IEEE</li>    <li id="ul0001-0005" num="0180">[5] M. S. Asian, A. Ali, D. Chen, B. Arnold, A. A. Farag, and P. Xiang, &#x201c;3D vertebrae segmentation using graph cuts with shape prior constraints,&#x201d; in 2010 IEEE International Conference on Image Processing, 2010, pp. 2193-2196: IEEE.</li>    <li id="ul0001-0006" num="0181">[6] P. H. Lim, U. Bagci, and L. Bai, &#x201c;Introducing Willmore flow into level set segmentation of spinal vertebrae,&#x201d; IEEE Transactions on Biomedical Engineering, vol. 60, no. 1, pp. 115-122, 2012.</li>    <li id="ul0001-0007" num="0182">[7] A. Rasoulian, R. Rohling, and P. Abolmaesumi, &#x201c;Lumbar spine segmentation using a statistical multi-vertebrae anatomical shape+ pose model,&#x201d; IEEE Transactions on Medical Imaging, vol. 32, no. 10, pp. 1890-1900, 2013.</li>    <li id="ul0001-0008" num="0183">[8] R. Korez, B. Ibragimov, B. Likar, F. Pernu&#x161;, and T. Vrtovec, &#x201c;A framework for automated spine and vertebrae interpolation-based detection and model-based segmentation,&#x201d; IEEE Transactions on Medical Imaging, vol. 34, no. 8, pp. 1649-1662, 2015.</li>    <li id="ul0001-0009" num="0184">[9] S. M. R. A I Arif, K. Knapp, and G. Slabaugh, &#x201c;Fully automatic cervical vertebrae segmentation framework for X-ray images,&#x201d; Computer methods programs in biomedicine, vol. 157, pp. 95-111, 2018.</li>    <li id="ul0001-0010" num="0185">[10] R. Janssens, G. Zeng, and G. Zheng, &#x201c;Fully automatic segmentation of lumbar vertebrae from CT images using cascaded 3D fully convolutional networks,&#x201d; in 2018 IEEE 15<sup>th </sup>International Symposium on Biomedical Imaging (ISBI 2018), 2018, pp. 893-897: IEEE.</li>    <li id="ul0001-0011" num="0186">[11] J.-T. Lu et al., &#x201c;Deepspine: Automated lumbar vertebral segmentation, disc-level designation, and spinal stenosis grading using deep learning,&#x201d; arXiv preprint arXiv:10215, 2018.</li>    <li id="ul0001-0012" num="0187">[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton, &#x201c;Imagenet classification with deep convolutional neural networks,&#x201d; in Advances in neural information processing systems, 2012, pp. 1097-1105.</li>    <li id="ul0001-0013" num="0188">[13] M. Foracchia, E. Grisan, and A. Ruggeri, &#x201c;Luminosity and contrast normalization in retinal images,&#x201d; Medical image analysis, vol. 9, no. 3, pp. 179-190, 2005.</li>    <li id="ul0001-0014" num="0189">[14] R. Liu et al., &#x201c;An intriguing failing of convolutional neural networks and the coordconv solution,&#x201d; in Advances in Neural Information Processing Systems, 2018, pp. 9605-9616.</li></ul></p><p id="p-0196" num="0190">While the invention is explained in relation to certain embodiments, it is to be understood that various modifications thereof will become apparent to those skilled in the art upon reading the specification. Therefore, it is to be understood that the invention disclosed herein is intended to cover such modifications as fall within the scope of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005138A1-20230105-M00001.NB"><img id="EMI-M00001" he="8.47mm" wi="76.20mm" file="US20230005138A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005138A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US20230005138A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005138A1-20230105-M00003.NB"><img id="EMI-M00003" he="5.67mm" wi="76.20mm" file="US20230005138A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230005138A1-20230105-M00004.NB"><img id="EMI-M00004" he="5.67mm" wi="76.20mm" file="US20230005138A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230005138A1-20230105-M00005.NB"><img id="EMI-M00005" he="5.67mm" wi="76.20mm" file="US20230005138A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230005138A1-20230105-M00006.NB"><img id="EMI-M00006" he="5.67mm" wi="76.20mm" file="US20230005138A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for fully automated comprehensive assessment of clinical lumbar magnetic resonance images, comprising:<claim-text>a MRI standardization component that reads MRI data from raw lumbar MRI files with different formats, and converts the raw MRI data into standardized MRI data, and organizes the standardized MRI data into a structured MRI data package;</claim-text><claim-text>a core assessment component automatically generates MRI assessment results which include multi-tissue anatomical annotation, multi-pathology detection and multi-pathology progression prediction based on the structured MRI data package, said core assessment component contains a semantic segmentation module that utilizes an unsupervised AI deep learning pipeline to generate a multi-tissue anatomical annotation, a pathology detection module utilizes an AI deep learning pipeline to generate multi-pathology detection, and a pathology progression prediction module utilizes an AI deep learning pipeline to generate multi-pathology progression prediction; and</claim-text><claim-text>a model optimization component that archives clinical MRI data and MRI assessment results based on comments provided by a specialist, and periodically optimizes the AI deep learning pipelines of the core assessment component.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the standardization component comprises:<claim-text>an MRI discriminating unit that discriminates the file formats of received raw MRI files;</claim-text><claim-text>MRI file reading units for different file formats that receive the raw MRI files from the MRI discriminating unit based on the file format of the raw MRI files, and read the raw MRI image data and MRI metadata from the received raw MRI files;</claim-text><claim-text>data standardizing units for each file reading unit that receive the raw MRI image data and MRI metadata from the respective file reading unit and convert the received MRI data to standardized MRI data; and</claim-text><claim-text>a data organizing unit that receives the standardized MRI data from the different data standardization units and organizes different kinds of data of the standardized MRI data into a structured MRI data package.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein in the MRI discriminating unit discriminates the file format of received MRI file and sends the received MRI file to the appropriate file reading unit based on its filename suffix.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the data organizing units organize the standardized MRI data into structured MRI data packages based on a predefined hierarchical tree structure that is delivered to the core assessment component.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the multi-tissue anatomical annotation of the core assessment component comprises pixel-level segmentation masks of multiple spinal tissues and wherein the semantic segmentation module comprises:<claim-text>a memory for saving an AI deep learning model that generates the pixel-level segmentation masks of multiple spinal tissues based on the enhanced MRI feature arrays by learning general MRI image features and patterns of different spinal tissues through a training process, including pretraining and fine-tuning;</claim-text><claim-text>a suboptimal region of interest (ROI) detector that receives the structured MRI data package and generates suboptimal regions of interest (ROIs) of multiple spinal tissues;</claim-text><claim-text>a voting unit that receives the suboptimal ROIs of multiple spinal tissues and generates confidence coefficient maps of the corresponding spinal tissues;</claim-text><claim-text>a pixel dividing unit receives the confidence coefficient maps and divides the pixels in the MRI image array into three categories, including positive pixels of each corresponding spinal tissue, negative pixels and ambiguous pixels, where the positive pixels represent pixels in the MRI image array that most likely belong to the corresponding spinal tissue and the negative pixels represent the pixels that most likely do not belong to any spinal tissues;</claim-text><claim-text>an MRI feature enhancing unit receives the structured MRI data package and generates enhanced MRI feature arrays by applying two different image feature enhancement operations to the MRI image array of the structured MRI data package including pixel value local normalization and edge enhancement;</claim-text><claim-text>an AI deep learning fine-tuning unit receives the enhanced MRI feature arrays as well as the positive and negative pixels and fine-tunes the AI deep learning model saved in memory;</claim-text><claim-text>a deep learning segmenting unit receives the enhanced MRI feature arrays and utilizes the AI deep learning model fine-tuned by the AI deep learning fine-tuning unit to analyze the enhance MRI feature array and to generates a soft segmentation result for multiple spinal tissues based on the local MRI features and the pattern of each pixel in the MRI image array, where the soft segmentation result is a 4D array with a last dimension representing the probabilities that pixels in the MRI image array belong to vertebra, intervertebral disc, nerve, blood vessel, muscle, and background; and</claim-text><claim-text>a post-processing unit receives the soft segmentation result for multiple spinal tissues and generates the multi-tissue anatomical annotation, which is the final output of the semantic segmentation module.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the ROI detecting unit applies a set of knowledge based image processing operations on the MRI image array, where the operations are designed based on the anatomical a priori knowledge about the corresponding tissues.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the confidence coefficient map of each spinal tissue in the voting unit are a 3D array with the same shape as the MRI image array, and indicates the probabilities that pixels in the MRI image array may belong to the corresponding spinal tissue.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein, in the pixel dividing unit, the positive pixels of each spinal tissue and negative pixels are represented as a 3D array with the same shape as the MRI image array, and the pixel dividing unit applies thresholding and anatomical spatial constraint on the confidence coefficient maps to divide the pixels in MRI image array.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the AI deep learning model learned of the general MRI image features and patterns of different spinal tissues though a pretraining process and the AI deep learning fine-tuning process is a standard transfer learning process, the pretrained AI deep learning model is further trained with the supervision provided by the positive pixels and the negative pixels, whereby because the process is fully automated, (including ROI detection, voting, and pixel dividing) the AI deep learning model can be fine-tuned for each MRI case without requiring any manual annotation or human intervention.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the post-processing unit splits the soft segmentation result into six 3D arrays with the shape of (H,W,N) according to the last dimension, which are the probability arrays of corresponding spinal tissues and background, applies thresholds on each probability array with the threshold value of 0.5, which converts the probability arrays to binary masks, and applies morphology operations, including opening operation and closing operation, on the binary masks to remove small noise points, where the denoised binary masks are the pixel-level segmentation masks of multiple spinal tissues, which serve as the multi-tissue anatomical annotation.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the pathology detection module comprises:<claim-text>an intervertebral disc (IVD) region locating unit that receives the multi-tissue anatomical annotation and determine the position of each IVD region, respectively;</claim-text><claim-text>an IVD region measuring unit that receives the multi-tissue anatomical annotation and determine the size of each IVD region, respectively;</claim-text><claim-text>an IVD region extracting unit that receives the position and size of each IVD region, respectively, as well as the structured MRI data package, and extracts the 3D IVD region array of each IVD from the MRI image array, each IVD region array contains the whole corresponding IVD, two adjacent vertebral endplates, and the spinal canal within the posterior region of the IVD;</claim-text><claim-text>an IVD region standardizing unit that receives the 3D IVD region array of each IVD from the MRI image array and converts the received IVD region arrays to standardized IVD region arrays, the received IVD region arrays have different sizes depending on the actual anatomical dimensions of IVD and the IVD region standardizing unit adopts an interpolation technology to convert the sizes of received IVD region arrays to a standard size; and</claim-text><claim-text>a pathology detection unit receives the standardized IVD region arrays and uses an AI deep learning model for the assessment of pathologies and provides the multi-pathology detection, the multi-pathology detection includes the classification and grading results for multiple pathologies within each IVD region array and the multi-pathology detection is the final output of the pathology detection module,</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the interpolation technology is one or more of nearest interpolation, linear interpolation, and bilinear interpolation.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein the IVD region of each intervertebral disc is a cuboid region, which is centered at and is parallel to the corresponding intervertebral disc, and wherein the IVD region locating unit determines the position of each IVD region by identifying the location and rotation of each intervertebral disc based on the multi-tissue anatomical annotation; and<claim-text>wherein the IVD region measuring unit determines the size of each IVD region by measuring the dimensions of the spinal tissues based on the multi-tissue anatomical annotation.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the pathology progression prediction module generates the multi-pathology progression prediction based on the structured MRI data package, multi-tissue anatomical annotation, and multi-pathology detection, and comprises:<claim-text>an IVD region locating unit that receives the multi-tissue anatomical annotation and determine the position of each IVD region, respectively;</claim-text><claim-text>an IVD region measuring unit that receives the multi-tissue anatomical annotation and determine the size of each IVD region, respectively;</claim-text><claim-text>an IVD region extracting unit that receives the position and size of each IVD region, respectively, as well as the structured MRI data package, and extracts the 3D IVD region array of each IVD from the MRI image array, each IVD region array contains the whole corresponding IVD, two adjacent vertebral endplates, and the spinal canal within the posterior region of the IVD;</claim-text><claim-text>an IVD region standardizing unit that receives the 3D IVD region array of each IVD from the MRI image array and converts the received IVD region arrays to standardized IVD region arrays, the received IVD region arrays have different sizes depending on the actual anatomical dimensions of VID and the IVD region standardizing unit adopts an interpolation technology to convert the sizes of received IVD region arrays to a standard size;</claim-text><claim-text>a pathology predicting unit that receives the standardized IVD region arrays and adopts an AI deep learning model to generate multi-pathology future status prediction; and</claim-text><claim-text>a pathology progression determining unit receives the multi-pathology future status prediction and multi-pathology detection, and generates the multi-pathology progression prediction, the pathology progression status, includes progression and non-progression, and is based on the future status and detection result (current status) of each pathology and the multi-pathology progression prediction is the final output of the pathology progression prediction module.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the interpolation technology is one or more of nearest interpolation, linear interpolation, and bilinear interpolation.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein, for a pathology with two statuses, if the current status is negative and the future status is positive, the pathology is determined as progression; otherwise, the pathology is determined as non-progression, for the pathology with more than two statuses, if the future grade is larger than current grade, the pathology is determined as progression, otherwise the pathology is determined as non-progression.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the model optimization component, which periodically optimizes the different AI deep learning models used in different modules of the core assessment component via corresponding AI deep learning optimizing operations that are based on the structured MRI data package, MRI assessment results that include multi-tissue anatomical annotation, multi-pathology detection, and multi-pathology progression prediction as well as specialist's comments and a control signal, wherein the model optimization component comprises:<claim-text>a data recording unit that merges the received structured MRI data package and MRI assessment results together by assigning the MRI assessment results generated by the core assessment component to the corresponding undermined items of the structured MRI data package;</claim-text><claim-text>a data storage system;</claim-text><claim-text>a data archiving unit receiving the structured MRI data package with assessment results and specialist's comment, and archives the MRI data with a clinical label in the data storage system, the data archiving unit may correct the automated assessment results generated by the AI system and assign an importance score to each MRI case based on the specialist's comment, wherein the corrected assessment results serve as the clinical label for the optimization of the AI deep learning model; and</claim-text><claim-text>three AI deep learning optimizing units optimize the AI deep learning models of the semantic segmentation module, pathology detection module, and pathology progression prediction module via the respective AI deep learning optimizing operations, which are based on the accumulated MRI data with clinical labels loaded from the data storage system, the AI deep learning optimizing units receive the control signal to trigger the AI deep learning optimizing operations, which adopt standard transfer learning technology.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref> wherein:<claim-text>a first AI deep learning optimizing unit loads a back-up AI deep learning model for the semantic segmentation module and accumulated MRI data with clinical labels from the data storage system, and further trains the loaded AI deep learning model with the MRI image array and multi-tissue anatomical annotation results within the loaded MRI data, updates the AI deep learning model of the semantic segmentation module with the trained (optimized) AI deep learning model, and transmits it back to the data storage system to update the back-up AI deep learning model for semantic segmentation module;</claim-text><claim-text>a second AI deep learning optimizing unit and a third AI deep learning optimizing unit adopt the same process for the AI deep learning optimizing operations of the pathology detection module and the pathology progression prediction module, except that the second AI deep learning optimizing unit utilizes the MRI image array and multi-pathology detection results for training, and the third AI deep learning optimizing unit utilizes the baseline MRI image array and follow-up multi-pathology detection results for training.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A method for fully automated comprehensive assessment of clinical lumbar magnetic resonance images, comprising the steps of:<claim-text>detecting the region of interest (ROI) of each magnetic resonance image (MRI) slice, which is defined as the vertebral regions identified by a rule-based detecting process;</claim-text><claim-text>locally normalizing the ROI results to eliminate pixel intensity variation in different tissues and to highlight vertebral edges;</claim-text><claim-text>enhancing the images with a Sobel filter;</claim-text><claim-text>locally thresholding the images to obtain a binary mask in which the vertebrae are separated, where the ROIs are all connected components in the mask having an area and perimeter within the range of vertebrae;</claim-text><claim-text>voting to integrate the sub-optimal ROIs and to identify the volume of interest (VOI) of a MRI series by<claim-text>assigning weights to all pixels in the ROIs and creating a weight map for each MRI slice where the center pixels are assigned more weight</claim-text><claim-text>calculating the voting result,</claim-text><claim-text>defining the vertebra central region in the VOI,</claim-text><claim-text>defining the potential vertebral regions in the VOI, and</claim-text><claim-text>based on the voting result and the ROIs, classifying the pixels in a VOI into positive pixels, ambiguous pixels and negative pixels; and</claim-text></claim-text><claim-text>training a convolutional neural network (CNN) to further segment vertebrae from the VOI based on a combination of the original MRI image, the local normalized image and the edge detection results, wherein the positive and negative pixels in the VOIs provide supervision.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein to improve the localization sensitivity of the model, further including the steps of introducing coordinate channels in the form of two constant matrices of the same size as input patches, the coordinate channels indicating the relative position of each pixel in the input patch;<claim-text>processing the input to the network with 3 successive convolution (Cony) blocks, each containing two convolutional layers with a kernel size of 3&#xd7;3, and one maxpooling layer of kernel size 2&#xd7;2,</claim-text><claim-text>doubling the channels after each Cony block, and</claim-text><claim-text>providing two fully connected layers following the Cony blocks with hidden units.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The method according to <claim-ref idref="CLM-00018">claim 18</claim-ref> further including the steps of activating all convolutional layers and fully-connected layers via a Rectified Linear Unit (ReLU) (<figref idref="DRAWINGS">FIG. <b>18</b>F</figref>), and activating the output layer by softmax, where the output of the network is the probability that the center pixel of the input patch belongs to the vertebrae.</claim-text></claim></claims></us-patent-application>