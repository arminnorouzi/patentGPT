<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005293A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005293</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17781450</doc-number><date>20191217</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>16</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>165</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>267</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">OBJECT DETECTION METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NEC Corporation</orgname><address><city>Minato-ku, Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>TAKAHASHI</last-name><first-name>Yusuke</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>NEC Corporation</orgname><role>03</role><address><city>Minato-ku, Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2019/049334</doc-number><date>20191217</date></document-id><us-371c12-date><date>20220601</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An object detection apparatus according to the present invention includes an object detecting unit configured to perform, for each region in an image set based on a size of a specific object detected in the image, a process of detecting the specific object on a new image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="97.28mm" wi="158.75mm" file="US20230005293A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="195.66mm" wi="156.80mm" orientation="landscape" file="US20230005293A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="196.77mm" wi="140.29mm" orientation="landscape" file="US20230005293A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="190.08mm" wi="156.63mm" orientation="landscape" file="US20230005293A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="220.30mm" wi="156.63mm" orientation="landscape" file="US20230005293A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="220.22mm" wi="156.80mm" orientation="landscape" file="US20230005293A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="191.01mm" wi="121.58mm" orientation="landscape" file="US20230005293A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="194.39mm" wi="156.80mm" orientation="landscape" file="US20230005293A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="203.20mm" wi="156.80mm" orientation="landscape" file="US20230005293A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="228.52mm" wi="128.35mm" orientation="landscape" file="US20230005293A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="219.12mm" wi="118.87mm" orientation="landscape" file="US20230005293A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="228.52mm" wi="115.40mm" orientation="landscape" file="US20230005293A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="185.76mm" wi="79.25mm" file="US20230005293A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="173.06mm" wi="146.39mm" orientation="landscape" file="US20230005293A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="79.08mm" wi="126.15mm" file="US20230005293A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="82.21mm" wi="80.60mm" file="US20230005293A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to an object detection method for detecting an object in an image, an object detection apparatus, and a program.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">In recent years, with the progress of image processing technology, security cameras are installed in various places to detect persons from images captured by the security cameras. For example, a security camera is installed in a place where many persons gather such as an airport, a station, a commercial facility and an event venue, and detection of a person is performed for a purpose such as checking the number of persons and the degree of congestion and performing a process of matching with previously registered persons such as criminals.</p><p id="p-0004" num="0003">An example of a process of detecting a person from an image is described in Patent Document 1. In Patent Document 1, the image size of an input image is changed, and a face of a preset detection face size is detected.</p><p id="p-0005" num="0004">Patent Document 1: Japanese Unexamined Patent Application Publication No. JP-A 2011-008704</p><p id="p-0006" num="0005">However, the abovementioned technique described in Patent Document 1 needs a face detection process on the entire region of an input image whose image size has been changed, and has a problem that the speed of such a process cannot be increased. For example, in a case where an input image contains faces of a plurality of sizes as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, there is a need to change the image size of the input image a plurality of times and repeatedly perform the face detection process on the entire region of each input image whose image size has been changed. Moreover, not only in the case of detecting a face in an image but also in the case of detecting any object from an image, there arises a problem that the speed of a detection process cannot be increased.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0007" num="0006">Accordingly, an object of the present invention is to provide an object detection method, an object detection apparatus and a program that can solve the abovementioned problem that the speed of a process of detecting an object in an image cannot be increased.</p><p id="p-0008" num="0007">An object detection method as an aspect of the present invention includes performing, for each region in an image set based on a size of a specific object detected in the image, a process of detecting the specific object on a new image.</p><p id="p-0009" num="0008">Further, an object detection apparatus as an aspect of the present invention includes an object detecting unit configured to perform, for each region in an image set based on a size of a specific object detected in the image, a process of detecting the specific object on a new image.</p><p id="p-0010" num="0009">Further, a computer program as an aspect of the present invention includes instructions for causing a processor of an information processing apparatus to execute performing, for each region in an image set based on a size of a specific object detected in the image, a process of detecting the specific object on a new image.</p><p id="p-0011" num="0010">With the configurations as described above, the present invention can increase the speed of a process of detecting an object in an image.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a view showing an entire configuration of an information processing system in a first example embodiment of the present invention;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram showing a configuration of a detection apparatus disclosed in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a view showing an example of processing by the detection apparatus disclosed in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a view showing an example of processing by the detection apparatus disclosed in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a view showing an example of processing by the detection apparatus disclosed in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a view showing an example of processing by the detection apparatus disclosed in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a view showing an example of processing by the detection apparatus disclosed in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a view showing an example of processing by the detection apparatus disclosed in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a view showing an example of processing by the detection apparatus disclosed in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a view showing an example of processing by the detection apparatus disclosed in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a view showing an example of processing by the detection apparatus disclosed in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart showing a processing operation by the detection apparatus disclosed in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram showing a hardware configuration of an object detection apparatus in a second example embodiment of the present invention;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a block diagram showing a configuration of the object detection apparatus in the second example embodiment of the present invention; and</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart showing an operation of the object detection apparatus in the second example embodiment of the present invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">EXAMPLE EMBODIMENTS</heading><heading id="h-0006" level="1">First Example Embodiment</heading><p id="p-0027" num="0026">A first example embodiment of the present invention will be described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>12</b></figref>. <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>2</b></figref> are views for describing a configuration of an information processing system, and <figref idref="DRAWINGS">FIGS. <b>3</b> to <b>12</b></figref> are views for describing a processing operation of the information processing system.</p><p id="p-0028" num="0027">An information processing system according to the present invention is used for detecting the face of a person P who is in a place where many persons gather such as an airport, a station, a commercial facility and an event venue. For example, the information processing system detects the face of a person P who is in a target place to check the number of persons P and the degree of congestion in the place and to perform a process of matching with previously registered persons such as criminals. However, the information processing system according to the present invention is not limited to detecting the face of a person P for the abovementioned purpose, and may be used for detecting the face of a person P for any purpose. Moreover, the information processing system according to the present invention is not limited to detecting the face of a person P, and may detect any object.</p><p id="p-0029" num="0028">[Configuration]</p><p id="p-0030" num="0029">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the information processing system in this example embodiment includes a camera C for capturing an image of a space to be a target place, and a detection apparatus <b>10</b> (an object detection apparatus) that detects the face of a person P in the captured image. The detection apparatus <b>10</b> is configured by one or a plurality of information processing apparatuses including an arithmetic logic unit (a processor) and a storage unit.</p><p id="p-0031" num="0030">The detection apparatus <b>10</b> includes, as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an image acquiring unit <b>11</b>, a size detecting unit <b>12</b>, a region setting unit <b>13</b> and an object detecting unit <b>14</b> that are structured by execution of a program by the arithmetic logic unit. The detection apparatus <b>10</b> also includes an image storing unit <b>15</b> and a region information storing unit <b>16</b> that are formed in the storage unit. The respective components will be described in detail below.</p><p id="p-0032" num="0031">The image acquiring unit <b>11</b> first accepts a captured image of a target place captured by the camera C at regular time intervals. For example, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the image acquiring unit <b>11</b> accepts a captured image including a plurality of faces of persons P and temporarily stores into the image storing unit <b>15</b>. Although only one camera C is connected to the detection apparatus <b>10</b> in this example embodiment, a plurality of cameras C may be connected and processing as will be described later may be performed on captured images captured by the respective cameras C.</p><p id="p-0033" num="0032">It is desirable for a target place captured by the camera C to be a place where, for each region in the captured range, that is, in the captured image, the position and size of the face of an appearing person are within certain ranges. For example, it is desirable to capture a place where many persons whose face sizes are within a certain range appear in the upper right region of the captured image and many persons whose face sizes are within a different certain range appear in the lower left region of the captured image. However, a target place captured by the camera C may be a place where the position and size of the face of an appearing person in each region are not known in advance.</p><p id="p-0034" num="0033">The size detecting unit <b>12</b> (a size detecting unit) extracts a person P in a captured image based on the movement, shape, color and so on of an object shown in the captured image, and also detects the size of the face (a specific object) of the extracted person P. Specifically, in this example embodiment, the size detecting unit <b>12</b> detects the eye distance of a person P as the face size of the person P. For example, as mentioned above, the size detecting unit <b>12</b> detects an eye of a person P based on the movement, shape, color and so on of an object in a captured image, and detects the distance between the two eyes of a single person. As one example, the size detecting unit <b>12</b> calculates, for each of persons Pa, Pb and Pc shown in a captured image, an eye distance on image of each of the persons as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a case where the eye distances of the two persons Pa located on the upper side in the captured image are 100 pix (pixels), the eye distance of the person Pb located on the left side in the captured image is 140 pix (pixel), and the eye distance of the person Pc located on the right front side in the captured image is 200 pix (pixel).</p><p id="p-0035" num="0034">Then, the size detecting unit <b>12</b> stores so as to associate the detected eye distance of the person P and the position of the detected eyes of the person P on the captured image in the image storing unit <b>15</b>. At this time, the size detecting unit <b>12</b> sets division regions r obtained by dividing an entire captured image G into a plurality of regions as indicated by dotted line in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, and stores so as to associate the eye distance of the eyes of a person P located in a division region r and the division region r with each other. That is to say, the position of the face of a person P is represented by the position of a division region r in this example embodiment. However, the size detecting unit <b>12</b> may represent by another method, for example, represent the position of the face of a person P by the coordinates on a captured image.</p><p id="p-0036" num="0035">The size detecting unit <b>12</b> detects the eye distance of a person P and stores so as to associate with a division region r in the same manner as described above for a plurality of captured images. Therefore, for each division region r, the eye distance of a person P located in the division region r is associated and stored in the image storing unit <b>15</b>. As a result, no eye distance is associated with a division region r where no person P is detected, and a plurality of eye distances are associated with a division region r where a plurality of persons P are detected.</p><p id="p-0037" num="0036">The region setting unit <b>13</b> (a region setting unit) sets a detection region R that is a region where a process of detecting the face of a person P is detected as will be described later on a captured image G, by using the eye distance of the person P detected as described above. At this time, the region setting unit <b>13</b> sets, for each reference eye distance, a detection region R so as to include a position where an eye distance determined to be identical to the value of the reference eye distance is detected. For example, the region setting unit <b>13</b> in this example embodiment sets reference eye distances to 100 pix, 150 pix and 200 pix, and sets detection regions Ra, Rb and Rc corresponding to the respective reference eye distances. The eye distances determined to be identical to the reference eye distances each have a range. As an example, the reference eye distance 100 pix has a range of 75 to 124 pix, the reference eye distance 150 pix has a range of 125 to 174 pix, for example, and the reference eye distance 200 pix has a range of 175 to 224 pix.</p><p id="p-0038" num="0037">Specifically, the region setting unit <b>13</b> sets a detection region R in the following manner. First, the region setting unit <b>13</b> generates, for each division region r obtained by dividing a captured image G, a distribution d of eye distance associated with the division region r. For example, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the region setting unit <b>13</b> generates a distribution d of eye distance in association with each division region r so as to represent an eye distance detected in the division region r by a rod-shaped body extending from the minimum value to the maximum value on the vertical axis.</p><p id="p-0039" num="0038">Then, the region setting unit <b>13</b> sets, for the respective reference eye distances 100 pix, 150 pix and 200 pix, planes Fa, Fb and Fc representing the height positions of the respective eye distances as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Then, the region setting unit <b>13</b> sets a detection region R for each reference eye distance in accordance with a positional relation between each of the planes Fa, Fb and Fc and a rod-shaped body representing a distribution d of eye distance. In a case where a rod-shaped body representing a distribution d of eye distance passes through any of the planes Fa, Fb and Fc, the region setting unit <b>13</b> includes a division region r where the distribution d represented by the rod-shaped body is located as a detection region R of a reference eye distance corresponding to the plane. At this time, values within ranges determined to be identical to the values of the reference eye distances are also considered to pass through the respective planes Fa, Fb and Fc. For example, in a case where a reference eye distance is 100 pix, a distribution d of eye distance from 75 pix to 124 pix is also considered to pass through the plane Fa where the reference eye distance is 100 pix.</p><p id="p-0040" num="0039">Then, the region setting unit <b>13</b> includes a division region r determined to pass through each of the planes Fa, Fb and Fc into a detection region R of a reference eye distance corresponding to each of the planes Fa, Fb and Fc. As an example, for a predetermined reference eye distance, a division region r where an eye distance within a range determined to be an identical value to the given reference eye distance is located is indicated by a gray region in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In this case, the region setting unit <b>13</b> sets a connection region where the division regions r indicated by the gray regions are connected, and sets the connection region as a detection region R corresponding to a predetermined reference eye distance. Thus, the region setting unit <b>13</b> sets, for each the reference eye distances (for example, 100 pix, 150 pix and 200 pix), a region where the face of a person P having an eye distance determined to be identical to the value of the reference eye distance (for example, 75 to 124 pix, 125 to 174 pix, and 175 to 224 pix) is located, as each of the detection regions Ra, Rb and Rc.</p><p id="p-0041" num="0040">An example of the detection regions Ra, Rb and Rc set by the region setting unit <b>13</b> so as to correspond to the respective cases where a reference eye distance is 100 pix, 150 pix and 200 pix is shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. For example, the detection region Ra set so as to correspond to a reference eye distance of 100 pix is a region where the face of a person Pa having an eye distance of 75 to 124 pix is detected, the detection region Rb set so as to correspond to a reference eye distance of 150 pix is a region where the face of a person Pb having an eye distance of 125 to 174 pix is detected, and the detection region Rc set so as to correspond to a reference eye distance of 200 pix is a region where the face of a person Pc having an eye distance of 175 to 224 pix is detected.</p><p id="p-0042" num="0041">The region setting unit <b>13</b> may exclude a no-detection region R&#x2032; where the face of a person P, that is, an eye distance is not detected in a captured image from the detection regions Ra, Rb and Rc set as described above and then set final detection regions Ra, Rb and Rc. At this time, the region setting unit <b>13</b> may exclude not a unit of a division region r but a predetermined range around coordinates at which an eye distance is not detected in a captured image as a no-detection region and then set detection regions Ra, Rb and Rc.</p><p id="p-0043" num="0042">Then, the region setting unit <b>13</b> stores region information that specify positions on captured image of the detection regions Ra, Rb and Rc set as described above into the region information storing unit <b>16</b>. At this time, the region setting unit <b>13</b> stores so as to associate each of the region information of the detection regions Ra, Rb and Rc and information that specifies the image size of a new captured image to be the target of a process of detecting the face of a person P as will be described later with each other. The later face detection process is performed by detecting a face having a preset eye distance (for example, 100 pix). For this, information that specifies the image size of a new captured image is determined in accordance with a preset magnification of an eye distance at the time of performing the later face detection process with reference to the reference eye distance corresponding to each of the detection regions Ra, Rb and Rc.</p><p id="p-0044" num="0043">For example, a case where detection of a face having an eye distance of 100 pix in a later face detection process is previously set will be described. In the case of an eye distance at the face detection process of 100 pix with reference to the reference eye distance of 100 pix, the magnification is 1 time, so that the face detection process can be performed by determining the image size of a new captured image to be 1 time as well. Therefore, information of &#x201c;1 time&#x201d; is associated with the detection region Ra corresponding to the reference eye distance of 100 pix as information that specifies the image size of a new captured image. Moreover, in the case of an eye distance at the face detection process of 100 pix with reference to the reference eye distance of 150 pix, the magnification is about 0.7 times, so that the face detection process can be performed by determining the image size of a new captured image to be 0.7 times as well. Therefore, information of &#x201c;0.7 times&#x201d; is associated with the detection region Rb corresponding to the reference eye distance of 150 pix as information that specifies the image size of a new captured image. Moreover, in the case of an eye distance at the face detection process of 100 pix with reference to the reference eye distance of 200 pix, the magnification is 0.5 times, so that the face detection process can be performed by determining the image size of a new captured image to be 0.5 times as well. Therefore, information of &#x201c;0.5 times&#x201d; is associated with the detection region Rc corresponding to the reference eye distance of 200 pix as information that specifies the image size of a new captured image.</p><p id="p-0045" num="0044">The region setting unit <b>13</b> is not limited to associating the magnification of a new captured image as described above as information that specifies the image size of the new captured image to be associated with each of the region information of the detection regions Ra, Rb and Rc. For example, the region setting unit <b>13</b> may associate the image size of a new captured image, or may associate any other information.</p><p id="p-0046" num="0045">The region setting unit <b>13</b> is not necessarily limited to setting the detection region R by the abovementioned method, and may set the detection region R by any method. Moreover, the abovementioned detection region R is not necessarily limited to being set by the region setting unit <b>13</b>, and may be set beforehand by any method and stored into the region information storing unit <b>16</b>.</p><p id="p-0047" num="0046">The object detecting unit <b>14</b> (an object detecting unit) performs a process of detecting the face of a person P on a new captured image after a detection region R is set as described above. Specifically, the object detecting unit <b>14</b> acquires a new captured image captured by the camera C from the image acquiring unit <b>11</b>, and changes the overall image size of the new captured image to a predetermined image size. For example, the object detecting unit <b>14</b> sequentially changes the overall image size of the new captured image to 1 time, 0.7 times, and 0.5 times. Then, the object detecting unit <b>14</b> acquires information of a detection region R associated with each of the magnifications of the changed overall image sizes of the new captured image from the region information storing unit <b>16</b>. Furthermore, the object detecting unit <b>14</b> performs, on the new captured image of the changed image size, a process of detecting a face having an eye distance of 100 pix in the detection region R associated with the image size. The face detection process is performed by detecting an eye of a person in a captured image based on the movement, shape, color and so on of an object shown in the captured image, detecting the two eyes of a single person having a distance of 100 pix, and detecting a face region based on the position of the eyes. In the face detection process, a face region of an eye distance within a predetermined range before and after the eye distance of 100 pix is also detected. For example, a face region of an eye distance from 75 to 124 pix is also detected in a captured image.</p><p id="p-0048" num="0047">Here, an example of the face detection process by the object detecting unit <b>14</b> will be described with reference to <figref idref="DRAWINGS">FIGS. <b>9</b> to <b>11</b></figref>. First, the object detecting unit <b>14</b> changes the overall image size of a new captured image G to 1 time as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. Since the magnification is 1 time, the acquired new captured image G may be used without change. Then, the object detecting unit <b>14</b> sets a detection region Ra in a new captured image G<b>1</b> after the image size has been changed to 1 time, and performs detection of a face having an eye distance of 100 pix only in the detection region Ra. Consequently, the face region of a person Pa enclosed by a rectangle appearing in the detection region Ra can be detected.</p><p id="p-0049" num="0048">Further, the object detecting unit <b>14</b> changes the overall image size of the new captured image G to 0.7 times as shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. Then, the object detecting unit <b>14</b> sets a detection region Rb in a new captured image G<b>2</b> after the image size has been changed to 0.7 times, and performs detection of a face having an eye distance of 100 pix only in the detection region Rb. Consequently, the face region of a person Pb enclosed by a rectangle appearing in the detection region Rb can be detected. Further, the object detecting unit <b>14</b> changes the overall image size of the new captured image G to 0.5 times as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. Then, the object detecting unit <b>14</b> sets a detection region Rc in a new captured image G3 after the image size has been changed to 0.5 times, and performs detection of a face having an eye distance of 100 pix only in the detection region Rc. Consequently, the face region of a person Pc enclosed by a rectangle appearing in the detection region Rc can be detected. The object detecting unit <b>14</b> may change the overall image size of the new captured image G to 0.7 times and thereafter change this image size to 0.7 times to make the first image size to approximately 0.5 times.</p><p id="p-0050" num="0049">Although the object detecting unit <b>14</b> performs a detection process on a new captured image by using a detection region R set by the region setting unit <b>13</b> as described above, a detection region R is not necessarily limited to being set by the region setting unit <b>13</b>. For example, information of a detection region R as described above set by any means may be previously stored in the region information storing unit <b>16</b>, and the object detecting unit <b>14</b> may retrieve and use the information of the detection region R to perform the detection process in the detection region R on a new image.</p><heading id="h-0007" level="2">[Operation]</heading><p id="p-0051" num="0050">Next, an operation of the above information processing system will be described majorly with reference to a flowchart of <figref idref="DRAWINGS">FIG. <b>12</b></figref>. A process at steps S<b>1</b> to step S<b>4</b> described below is a process performed in advance as a process of setting a detection region R, and a process at steps S<b>5</b> to S<b>7</b> is a detection process performed on a new image acquired after the detection region R is set. However, the process of setting a detection region R at steps S<b>1</b> to S<b>4</b> does not necessarily need to be executed, and a detection region R set by any method may be stored in the information processing system in advance.</p><p id="p-0052" num="0051">First, the information processing system captures an image of a target place by a camera C, and the detection apparatus <b>10</b> acquires the captured image (step S<b>1</b>). Then, the detection apparatus <b>10</b> detects the size of the face of a person P in the captured image (step S<b>2</b>). Here, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the detection apparatus <b>10</b> detects the eye distance of the person P, and stores so as to associate the detected eye distance of the person P with a division region r, which is information representing the position of the detected face of the person P on the captured image. The detection apparatus <b>10</b> performs detection of the eye distance of the person P on a plurality of captured images, and stores the position.</p><p id="p-0053" num="0052">Subsequently, the detection apparatus <b>10</b> generates the distribution of the detected eye distance of the face of the person P (step S<b>3</b>). For example, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the detection apparatus <b>10</b> generates, for each of the division regions r obtained by dividing the captured image, a distribution d of the eye distance associated with the division region r. Then, by using the distribution of the detected eye distance of the person P, the detection apparatus <b>10</b> sets, for each reference eye distance, a detection region R so that a position where an eye distance determined to be identical to the value of the reference eye distance has been detected is included (step S<b>4</b>). For example, the detection apparatus <b>10</b> assumes 100 pix, 150 pix and 200 pix to be reference eye distances, and sets detection regions Ra, Rb and Rc corresponding to the respective reference eye distances as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. Then, the detection apparatus <b>10</b> stores so as to associate region information that specifies a position on the captured image of each of the set detection regions Ra, Rb and Rc with information that specifies the image size of a new captured image.</p><p id="p-0054" num="0053">After that, every time a new captured image is captured by the camera C, the detection apparatus <b>10</b> acquires the new captured image (step S<b>5</b>) and performs a person detection process on the new captured image. At this time, the detection apparatus <b>10</b> changes the overall image size of the new captured image to 1 time, 0.7 times, and 0.5 times (step S<b>6</b>). Then, the detection apparatus <b>10</b> performs, in the captured image whose magnification has been changed to each of the magnifications, detection of a face having a specific eye distance (for example, 100 pix) only in any of the detection regions Ra, Rb and Rc associated with the magnification (step S<b>7</b>).</p><p id="p-0055" num="0054">For example, as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the detection apparatus <b>10</b> performs, in a new captured image G<b>1</b> whose magnification has been changed to 1 time, detection of a face Pa having an eye distance of 100 pix only in a detection region Ra in the captured image G<b>1</b>. Moreover, as shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the detection apparatus <b>10</b> performs, in a new captured image G<b>2</b> whose magnification has been changed to 0.7 times, detection of a face Pb having an eye distance of 100 pix only in a detection region Rb in the captured image G<b>2</b>. Moreover, as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the detection apparatus <b>10</b> performs, in a new captured image G<b>3</b> whose magnification has been changed to 0.5 times, detection of a face Pc having an eye distance of 100 pix only in a detection region Rc in the captured image G<b>3</b>.</p><p id="p-0056" num="0055">As described above, in this example embodiment, the eye distance of a person P appearing in a captured image is detected first, and a detection region R is set for each eye distance in accordance with the appearing position. Then, at the time of a face detection process on a new captured image, the overall image size is changed, and the face detection process is performed only in a detection region R set in accordance with the changed image size. Consequently, there is no need to repeatedly perform the face detection process on the entire region of each new captured image whose image size has been changed. That is to say, it is sufficient to perform the face detection process only in one detection region for each changed image size of a new captured image. As a result, it is possible to increase the speed of the face detection process and reduce the load.</p><p id="p-0057" num="0056">Although a case where the detection apparatus <b>10</b> detects the face of a person P in a captured image is illustrated above, a target to be detected may be any object. In this case, the detection apparatus <b>10</b> detects a predetermined size of an object to be a detection target instead of the eye distance of a person P, and sets a region in an image for each reference size in accordance with the predetermined size. Then, the detection apparatus <b>10</b> changes the magnification of a new image, and performs an object detection process only in a region set for each image whose magnification has been changed.</p><heading id="h-0008" level="2">&#x3c;Second Example Embodiment&#x3e;</heading><p id="p-0058" num="0057">Next, a second example embodiment of the present invention will be described with reference to <figref idref="DRAWINGS">FIGS. <b>13</b> to <b>15</b></figref>. <figref idref="DRAWINGS">FIGS. <b>13</b> to <b>14</b></figref> are block diagrams showing a configuration of an object detection apparatus in the second example embodiment, and <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart showing an operation of an image processing apparatus. In this example embodiment, the overview of the configurations of the detection apparatus <b>10</b> described in the first example embodiment and an object detection method is shown.</p><p id="p-0059" num="0058">First, a hardware configuration of an object detection apparatus <b>100</b> in this example embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref>. The object detection apparatus <b>100</b> includes one or a plurality of generally-used information processing apparatuses, and includes the following hardware configuration as shown below;<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0059">a CPU (Central Processing Unit) <b>101</b> (an arithmetic logic unit),</li>        <li id="ul0002-0002" num="0060">a ROM (Read Only Memory) <b>102</b> (a storage unit),</li>        <li id="ul0002-0003" num="0061">a RAM (Random Access Memory) <b>103</b> (the storage unit),</li>        <li id="ul0002-0004" num="0062">programs <b>104</b> loaded to the RAM <b>103</b>,</li>        <li id="ul0002-0005" num="0063">a storage device <b>105</b> for storing the programs <b>104</b>,</li>        <li id="ul0002-0006" num="0064">a drive device <b>106</b> that reads from and writes into a storage medium <b>110</b> outside the information processing apparatus,</li>        <li id="ul0002-0007" num="0065">a communication interface <b>107</b> connected to a communication network <b>111</b> outside the information processing apparatus,</li>        <li id="ul0002-0008" num="0066">an input/output interface <b>108</b> that inputs and outputs data, and</li>        <li id="ul0002-0009" num="0067">a bus <b>109</b> that connects the components.</li>    </ul>    </li></ul></p><p id="p-0060" num="0068">Then, the object detection apparatus <b>100</b> can structure and include an object detecting unit <b>121</b> shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> by acquisition and execution of the programs <b>104</b> by the CPU <b>101</b>. The programs <b>104</b> are, for example, stored in the storage device <b>105</b> or the ROM <b>102</b> in advance, and are loaded to the RAM <b>103</b> and executed by the CPU <b>101</b> as necessary. The programs <b>104</b> may also be supplied to the CPU <b>101</b> via the communication network <b>101</b>, or may be stored in the storage medium <b>110</b> in advance and retrieved by the drive device <b>106</b> and supplied to the CPU <b>101</b>. However, the abovementioned object detecting unit <b>121</b> may be structured by an electronic circuit.</p><p id="p-0061" num="0069"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows an example of the hardware configuration of the object detection apparatus <b>100</b>, and the hardware configuration of the object detection apparatus is not limited to the above case. For example, the object detection apparatus <b>100</b> may include part of the abovementioned configuration, for example, excluding the drive device <b>106</b>.</p><p id="p-0062" num="0070">Then, the object detection apparatus <b>100</b> executes an object detection method shown in the flowchart of <figref idref="DRAWINGS">FIG. <b>15</b></figref> by the function of the object detecting unit <b>121</b> structured by the programs as described above.</p><p id="p-0063" num="0071">As shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, the object detection apparatus <b>100</b> performs, for each region in an image set based on a size of a specific object detected in the image, a process of detecting the specific object on a new image (step S<b>11</b>).</p><p id="p-0064" num="0072">In this example embodiment, with the configuration as described above, the size of an object appearing in a captured image is detected first, and a region is set for each size in accordance with the appearing position. Then, a process of detecting the object is performed only in the set region on a new image. Consequently, there is no need to repeatedly performing the object detection process on the entire region of a new image. As a result, it is possible to increase the speed of the object detection process and reduce the load.</p><heading id="h-0009" level="2">&#x3c;Supplementary Notes&#x3e;</heading><p id="p-0065" num="0073">The whole or part of the example embodiments disclosed above can be described as the following supplementary notes. The outline of the configurations of an object detection apparatus, an object detection method and a program according to the present invention will be described below. However, the present invention is not limited to the following configurations.</p><heading id="h-0010" level="2">(Supplementary Note 1)</heading><p id="p-0066" num="0074">An object detection method comprising<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0075">performing, for each region in an image set based on a size of a specific object detected in the image, a process of detecting the specific object on a new image.</li>    </ul>    </li></ul></p><heading id="h-0011" level="2">(Supplementary Note 2)</heading><p id="p-0067" num="0076">The object detection method according to Supplementary Note 1, comprising<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0077">changing an overall size of the new image and performing, for each overall size of the new image, the process of detecting the specific object in the region in the new image.</li>    </ul>    </li></ul></p><heading id="h-0012" level="2">(Supplementary Note 3)</heading><p id="p-0068" num="0078">The object detection method according to Supplementary Note 2, comprising<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0079">performing the process of detecting the specific object in one region for each overall size of the new image.</li>    </ul>    </li></ul></p><heading id="h-0013" level="2">(Supplementary Note 4)</heading><p id="p-0069" num="0080">The object detection method according to Supplementary Note 2 or 3, comprising<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0081">performing, for each region in the new image, the process of detecting the specific object having a specific size.</li>    </ul>    </li></ul></p><heading id="h-0014" level="2">(Supplementary Note 5)</heading><p id="p-0070" num="0082">The object detection method according to any of Supplementary Notes 1 to 4, comprising:<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0000">    <ul id="ul0012" list-style="none">        <li id="ul0012-0001" num="0083">associating the region set based on the size of the specific object with information specifying an image size of the new image to be a processing target; and</li>        <li id="ul0012-0002" num="0084">changing an overall size of the new image and performing, for each overall size of the new image, the process of detecting the specific object in the region in the new image associated with the information specifying the image size identical to the overall size.</li>    </ul>    </li></ul></p><heading id="h-0015" level="2">(Supplementary Note 6)</heading><p id="p-0071" num="0085">The object detection method according to any of Supplementary Notes 1 to 5, comprising:<ul id="ul0013" list-style="none">    <li id="ul0013-0001" num="0000">    <ul id="ul0014" list-style="none">        <li id="ul0014-0001" num="0086">detecting the size of the specific object in the image and setting the region in the image based on the size of the specific object; and</li>        <li id="ul0014-0002" num="0087">thereafter performing, on the new image, the process of detecting the specific object for each region.</li>    </ul>    </li></ul></p><heading id="h-0016" level="2">(Supplementary Note 7)</heading><p id="p-0072" num="0088">The object detection method according to Supplementary Note 6, comprising<ul id="ul0015" list-style="none">    <li id="ul0015-0001" num="0000">    <ul id="ul0016" list-style="none">        <li id="ul0016-0001" num="0089">setting, for each size determined to be identical to the size of the specific object according to a preset criterion, the region in the image including the specific object having the size determined to be identical.</li>    </ul>    </li></ul></p><heading id="h-0017" level="2">(Supplementary Note 8)</heading><p id="p-0073" num="0090">The object detection method according to Supplementary Note 6 or 7, comprising<ul id="ul0017" list-style="none">    <li id="ul0017-0001" num="0000">    <ul id="ul0018" list-style="none">        <li id="ul0018-0001" num="0091">generating, for each division region obtained by dividing the image into a plurality of regions, a distribution of the size of the specific object located in the division region, and setting the region based on the distribution.</li>    </ul>    </li></ul></p><heading id="h-0018" level="2">(Supplementary Note 9)</heading><p id="p-0074" num="0092">The object detection method according to Supplementary Note 8, comprising<ul id="ul0019" list-style="none">    <li id="ul0019-0001" num="0000">    <ul id="ul0020" list-style="none">        <li id="ul0020-0001" num="0093">generating, based on the distribution, a connection region obtained by connecting the division regions including the specific object having a size determined to be identical according to a preset criterion, and setting the region based on the connection region.</li>    </ul>    </li></ul></p><heading id="h-0019" level="2">(Supplementary Note 10)</heading><p id="p-0075" num="0094">The object detection method according to any of Supplementary Notes 6 to 9, comprising<ul id="ul0021" list-style="none">    <li id="ul0021-0001" num="0000">    <ul id="ul0022" list-style="none">        <li id="ul0022-0001" num="0095">setting the region so as to exclude a non-detection region that is a region where the specific object is not detected in the image.</li>    </ul>    </li></ul></p><heading id="h-0020" level="2">(Supplementary Note 11)</heading><p id="p-0076" num="0096">The object detection method according to any of Supplementary Notes 6 to 10, comprising:<ul id="ul0023" list-style="none">    <li id="ul0023-0001" num="0000">    <ul id="ul0024" list-style="none">        <li id="ul0024-0001" num="0097">performing detection of the size of the specific object in the image on a plurality of images; and</li>        <li id="ul0024-0002" num="0098">setting the region in the image based on the size of the specific object detected from the plurality of images.</li>    </ul>    </li></ul></p><heading id="h-0021" level="2">(Supplementary Note 12)</heading><p id="p-0077" num="0099">The object detection method according to any of Supplementary Notes 6 to 11, comprising<ul id="ul0025" list-style="none">    <li id="ul0025-0001" num="0000">    <ul id="ul0026" list-style="none">        <li id="ul0026-0001" num="0100">detecting a size of a face of a person as the specific object.</li>    </ul>    </li></ul></p><heading id="h-0022" level="2">(Supplementary Note 13)</heading><p id="p-0078" num="0101">An object detection apparatus comprising<ul id="ul0027" list-style="none">    <li id="ul0027-0001" num="0000">    <ul id="ul0028" list-style="none">        <li id="ul0028-0001" num="0102">an object detecting unit configured to perform, for each region in an image set based on a size of a specific object detected in the image, a process of detecting the specific object on a new image.</li>    </ul>    </li></ul></p><heading id="h-0023" level="2">(Supplementary Note 14)</heading><p id="p-0079" num="0103">The object detection apparatus according to Supplementary Note 13, wherein<ul id="ul0029" list-style="none">    <li id="ul0029-0001" num="0000">    <ul id="ul0030" list-style="none">        <li id="ul0030-0001" num="0104">the object detecting unit is configured to change an overall size of the new image and perform, for each overall size of the new image, the process of detecting the specific object in the region in the new image.</li>    </ul>    </li></ul></p><heading id="h-0024" level="2">(Supplementary Note 14.1)</heading><p id="p-0080" num="0105">The object detection apparatus according to Supplementary Note 14, wherein<ul id="ul0031" list-style="none">    <li id="ul0031-0001" num="0000">    <ul id="ul0032" list-style="none">        <li id="ul0032-0001" num="0106">the object detecting unit is configured to perform the process of detecting the specific object in one region for each overall size of the new image.</li>    </ul>    </li></ul></p><heading id="h-0025" level="2">(Supplementary Note 14.2)</heading><p id="p-0081" num="0107">The object detection apparatus according to Supplementary Note 14 or 14.1, wherein<ul id="ul0033" list-style="none">    <li id="ul0033-0001" num="0000">    <ul id="ul0034" list-style="none">        <li id="ul0034-0001" num="0108">the object detecting unit is configured to perform, for each region in the new image, the process of detecting the specific object having a specific size.</li>    </ul>    </li></ul></p><heading id="h-0026" level="2">(Supplementary Note 15)</heading><p id="p-0082" num="0109">The object detection apparatus according to any of Supplementary Note 13 to 14.2, comprising<ul id="ul0035" list-style="none">    <li id="ul0035-0001" num="0000">    <ul id="ul0036" list-style="none">        <li id="ul0036-0001" num="0110">a size detecting unit configured to detect the size of the specific object in the image; and</li>        <li id="ul0036-0002" num="0111">a region setting unit configured to set the region in the image based on the size of the specific object,</li>        <li id="ul0036-0003" num="0112">wherein the object detecting unit is configured to, after the region is set, perform the process of detecting the specific object for each region on the new image.</li>    </ul>    </li></ul></p><heading id="h-0027" level="2">(Supplementary Note 16)</heading><p id="p-0083" num="0113">The object detection apparatus according to Supplementary Note 15, wherein<ul id="ul0037" list-style="none">    <li id="ul0037-0001" num="0000">    <ul id="ul0038" list-style="none">        <li id="ul0038-0001" num="0114">the region setting unit is configured to set, for each size determined to be identical to the size of the specific object according to a preset criterion, the region in the image including the specific object having the size determined to be identical.</li>    </ul>    </li></ul></p><heading id="h-0028" level="2">(Supplementary Note 17)</heading><p id="p-0084" num="0115">The object detection apparatus according to Supplementary Note 15 or 16, wherein<ul id="ul0039" list-style="none">    <li id="ul0039-0001" num="0000">    <ul id="ul0040" list-style="none">        <li id="ul0040-0001" num="0116">the region setting unit is configured to generate, for each division region obtained by dividing the image into a plurality of regions, a distribution of the size of the specific object located in the division region, and set the region based on the distribution.</li>    </ul>    </li></ul></p><heading id="h-0029" level="2">(Supplementary Note 17.1)</heading><p id="p-0085" num="0117">The object detection apparatus according to Supplementary Note 17, wherein<ul id="ul0041" list-style="none">    <li id="ul0041-0001" num="0000">    <ul id="ul0042" list-style="none">        <li id="ul0042-0001" num="0118">the region setting unit is configured to generate, based on the distribution, a connection region obtained by connecting the division regions including the specific object having a size determined to be identical according to a preset criterion, and setting the region based on the connection region.</li>    </ul>    </li></ul></p><heading id="h-0030" level="2">(Supplementary Note 18)</heading><p id="p-0086" num="0119">The object detection apparatus according to any of Supplementary Notes 15 to 17.1, wherein:<ul id="ul0043" list-style="none">    <li id="ul0043-0001" num="0000">    <ul id="ul0044" list-style="none">        <li id="ul0044-0001" num="0120">the region setting unit is configured to associate the region set based on the size of the specific object with information specifying an image size of the new image to be a processing target; and</li>        <li id="ul0044-0002" num="0121">the object detecting unit is configured to change an overall size of the new image and perform, for each overall size of the new image, the process of detecting the specific object in the region in the new image associated with the information specifying the image size identical to the overall size.</li>    </ul>    </li></ul></p><heading id="h-0031" level="2">(Supplementary Note 19)</heading><p id="p-0087" num="0122">The object detection apparatus according to any of Supplementary Notes 15 to 18, wherein<ul id="ul0045" list-style="none">    <li id="ul0045-0001" num="0000">    <ul id="ul0046" list-style="none">        <li id="ul0046-0001" num="0123">the region setting unit is configured to set the region so as to exclude a non-detection region that is a region where the specific object is not detected in the image.</li>    </ul>    </li></ul></p><heading id="h-0032" level="2">(Supplementary Note 20)</heading><p id="p-0088" num="0124">A computer program comprising instructions for causing a processor of an information processing apparatus to execute<ul id="ul0047" list-style="none">    <li id="ul0047-0001" num="0000">    <ul id="ul0048" list-style="none">        <li id="ul0048-0001" num="0125">performing, for each region in an image set based on a size of a specific object detected in the image, a process of detecting the specific object on a new image.</li>    </ul>    </li></ul></p><heading id="h-0033" level="2">(Supplementary Note 21)</heading><p id="p-0089" num="0126">The computer program according to Supplementary Note 20, comprising instructions for causing the processor of the information processing apparatus to further execute:<ul id="ul0049" list-style="none">    <li id="ul0049-0001" num="0000">    <ul id="ul0050" list-style="none">        <li id="ul0050-0001" num="0127">detecting the size of the specific object in the image and setting the region in the image based on the size of the specific object; and</li>        <li id="ul0050-0002" num="0128">thereafter performing, on the new image, the process of detecting the specific object for each region.</li>    </ul>    </li></ul></p><p id="p-0090" num="0129">The abovementioned programs can be stored by using various types of non-transitory computer-readable mediums and supplied to a computer. The non-transitory computer-readable mediums include various types of tangible storage mediums. Examples of the non-transitory computer-readable mediums include a magnetic recording medium (for example, a flexible disk, a magnetic tape, a hard disk drive), a magnetooptical recording medium (for example, a magnetooptical disk), a CD-ROM (Read Only Memory), a CD-R, a CD-R/W, and a semiconductor memory (for example, a mask ROM, a PROM (Programmable ROM), an EPROM (Erasable PROM), a flash ROM, a RAM (Random Access Memory). The programs may also be supplied to a computer by various types of transitory computer-readable mediums. Examples of the transitory computer-readable mediums include an electric signal, an optical signal, and an electromagnetic wave. The transitory computer-readable mediums can supply the programs to a computer via a wired communication path such as an electric wire and an optical fiber or via a wireless communication path.</p><p id="p-0091" num="0130">Although the present invention has been described above with reference to the example embodiments and the like, the present invention is not limited to the above example embodiments.</p><p id="p-0092" num="0131">The configurations and details of the present invention can be changed in various manners that can be understood by one skilled in the art within the scope of the present invention.</p><heading id="h-0034" level="1">DESCRIPTION OF NUMERALS</heading><p id="p-0093" num="0132"><b>10</b> detection apparatus</p><p id="p-0094" num="0133"><b>11</b> image acquiring unit</p><p id="p-0095" num="0134"><b>12</b> size detecting unit</p><p id="p-0096" num="0135"><b>13</b> region setting unit</p><p id="p-0097" num="0136"><b>14</b> object detecting unit</p><p id="p-0098" num="0137"><b>15</b> image storing unit</p><p id="p-0099" num="0138"><b>16</b> region information storing unit</p><p id="p-0100" num="0139">C camera</p><p id="p-0101" num="0140">P person</p><p id="p-0102" num="0141"><b>100</b> image processing apparatus</p><p id="p-0103" num="0142"><b>101</b> CPU</p><p id="p-0104" num="0143"><b>102</b> ROM</p><p id="p-0105" num="0144"><b>103</b> RAM</p><p id="p-0106" num="0145"><b>104</b> programs</p><p id="p-0107" num="0146"><b>105</b> storage device</p><p id="p-0108" num="0147"><b>106</b> drive device</p><p id="p-0109" num="0148"><b>107</b> communication interface</p><p id="p-0110" num="0149"><b>108</b> input/output interface</p><p id="p-0111" num="0150"><b>109</b> bus</p><p id="p-0112" num="0151"><b>110</b> storage medium</p><p id="p-0113" num="0152"><b>111</b> communication network</p><p id="p-0114" num="0153"><b>121</b> object detecting unit</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An object detection method comprising<claim-text>performing, for each region in an image set based on a size of a specific object detected in the image, a process of detecting the specific object on a new image.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The object detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising<claim-text>changing an overall size of the new image and performing, for each overall size of the new image, the process of detecting the specific object in the region in the new image.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The object detection method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, comprising performing the process of detecting the specific object in one region for each overall size of the new image.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The object detection method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, comprising<claim-text>performing, for each region in the new image, the process of detecting the specific object having a specific size.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The object detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising:<claim-text>associating the region set based on the size of the specific object with information specifying an image size of the new image to be a processing target; and</claim-text><claim-text>changing an overall size of the new image and performing, for each overall size of the new image, the process of detecting the specific object in the region in the new image associated with the information specifying the image size identical to the overall size.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The object detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising:<claim-text>detecting the size of the specific object in the image and setting the region in the image based on the size of the specific object; and</claim-text><claim-text>thereafter performing, on the new image, the process of detecting the specific object for each region.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The object detection method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, comprising<claim-text>setting, for each size determined to be identical to the size of the specific object according to a preset criterion, the region in the image including the specific object having the size determined to be identical.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The object detection method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, comprising<claim-text>generating, for each division region obtained by dividing the image into a plurality of regions, a distribution of the size of the specific object located in the division region, and setting the region based on the distribution.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The object detection method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, comprising<claim-text>generating, based on the distribution, a connection region obtained by connecting the division regions including the specific object having a size determined to be identical according to a preset criterion, and setting the region based on the connection region.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The object detection method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, comprising<claim-text>setting the region so as to exclude a non-detection region that is a region where the specific object is not detected in the image.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The object detection method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, comprising:<claim-text>performing detection of the size of the specific object in the image on a plurality of images; and</claim-text><claim-text>setting the region in the image based on the size of the specific object detected from the plurality of images.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The object detection method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, comprising<claim-text>detecting a size of a face of a person as the specific object.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An object detection apparatus comprising:<claim-text>at least one memory configured to store instructions; and</claim-text><claim-text>at least one processor configured to execute the instructions to</claim-text><claim-text>perform, for each region in an image set based on a size of a specific object detected in the image, a process of detecting the specific object on a new image.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The object detection apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the at least one processor is configured to execute the instructions to<claim-text>change an overall size of the new image and perform, for each overall size of the new image, the process of detecting the specific object in the region in the new image.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The object detection apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the at least one processor is configured to execute the instructions to:<claim-text>detect the size of the specific object in the image;</claim-text><claim-text>set the region in the image based on the size of the specific object; and</claim-text><claim-text>after the region is set, perform the process of detecting the specific object for each region on the new image.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The object detection apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the at least one processor is configured to execute the instructions to<claim-text>set, for each size determined to be identical to the size of the specific object according to a preset criterion, the region in the image including the specific object having the size determined to be identical.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The object detection apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the at least one processor is configured to execute the instructions to<claim-text>generate, for each division region obtained by dividing the image into a plurality of regions, a distribution of the size of the specific object located in the division region, and set the region based on the distribution.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The object detection apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the at least one processor is configured to execute the instructions to:<claim-text>associate the region set based on the size of the specific object with information specifying an image size of the new image to be a processing target; and</claim-text><claim-text>change an overall size of the new image and perform, for each overall size of the new image, the process of detecting the specific object in the region in the new image associated with the information specifying the image size identical to the overall size.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The object detection apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the at least one processor is configured to execute the instructions to<claim-text>set the region so as to exclude a non-detection region that is a region where the specific object is not detected in the image.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer-readable medium having a computer program stored thereon, the computer program comprising instructions for causing a processor of an information processing apparatus to execute<claim-text>performing, for each region in an image set based on a size of a specific object detected in the image, a process of detecting the specific object on a new image.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. (canceled)</claim-text></claim></claims></us-patent-application>