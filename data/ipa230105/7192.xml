<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007193A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007193</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364858</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>365</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>378</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>931</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>89</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>3651</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>378</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>931</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>89</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR GENERATING A CORRECTED IMAGE OUTPUT BY A CAMERA HAVING A GLOBAL SHUTTER</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>GM Cruise Holdings LLC</orgname><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WU</last-name><first-name>Qihong</first-name><address><city>Dublin</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Technologies are described herein that are configured to generate a corrected image by addressing photo response nonuniformity (PRNU) in a camera having a global shutter. A calibration procedure is described, where correction factors for each pixel in an image sensor are computed and subsequently employed to generate improved images.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="79.08mm" wi="158.75mm" file="US20230007193A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="230.63mm" wi="142.75mm" orientation="landscape" file="US20230007193A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="241.72mm" wi="148.67mm" file="US20230007193A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="193.72mm" wi="146.56mm" orientation="landscape" file="US20230007193A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="208.96mm" wi="139.62mm" orientation="landscape" file="US20230007193A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="223.27mm" wi="140.21mm" file="US20230007193A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="212.34mm" wi="153.67mm" orientation="landscape" file="US20230007193A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="266.28mm" wi="153.25mm" file="US20230007193A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="233.17mm" wi="161.29mm" orientation="landscape" file="US20230007193A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="217.93mm" wi="131.83mm" file="US20230007193A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="234.36mm" wi="145.97mm" orientation="landscape" file="US20230007193A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="177.46mm" wi="127.42mm" file="US20230007193A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">A camera having a global shutter exposes an entire scene to each pixel in an image sensor at the same time and for the same amount of time. Therefore, unlike a camera with a rolling shutter, which has pixels switch on and off from one side of the image sensor to another (e.g., from top to bottom) like a scan, a camera with a global shutter captures a scene using all of the pixels of the image sensor at once. Cameras with global shutters are particularly advantageous when capturing images of objects moving at high speeds; an issue with cameras with global shutters, however, is photo response nonuniformity (PRNU).</p><p id="p-0003" num="0002">With more particularly, when a fixed, uniform field of light falls across pixels of an image sensor in a camera having a global shutter, values readout from pixels in the image sensor should be identical. In actuality, however, and due to a variety of factors including variations in pixel, parasitic light sensitivity, amplification gain, and further due to values from the pixels not being readout at the same time, the pixels will have different values when readout. This variance is referred to as PRNU.</p><p id="p-0004" num="0003">With still more detail, as referenced above, in a camera with a global shutter, pixels in the image sensor are exposed to a scene at the same time and for the same duration of time. Readout electronics of cameras with global shutters, however, readout values from pixels row by row. Therefore, in an example, a pixel in a first row of the image sensor has a first value readout therefrom prior to a second value being readout from a second pixel in a tenth row of the image sensor, despite both the first and second values being employed to generate an image of the scene. In the time between when the first value of the first pixel is readout from the image sensor and when the second value of the second pixel is readout from the image sensor, the second pixel accumulates additional charge (e.g., caused by ambient light, electronics of the image sensor, etc.). Therefore, even when the first pixel and the second pixel are exposed to light of the same intensity for the same amount of time, the second value readout from the second pixel tends to be higher than the first value readout from the first pixel. Depending upon the exposure time used to capture the scene, the resultant image may be suboptimal. As exposure time decreases, the PRNU tends to become worse.</p><p id="p-0005" num="0004">Pixels of digital cameras with electronic shutters include sensor nodes and memory nodes, where a sensor node accumulates charge when &#x201c;on&#x201d; and a memory node retains the charge until its value is readout from the memory node. Conventional approaches for addressing PRNU include physically shielding memory nodes to minimize light reaching such memory nodes. Shielding, however, can be expensive, and may significantly increase the cost of a camera. High cost renders many cameras having global shutters unsuitable for certain types of applications. Contrarily, use of a camera with a global shutter that outputs suboptimal images renders such a camera unsuitable for applications where high-quality images are desired.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0006" num="0005">The following is a brief summary of subject matter that is described in greater detail herein. This summary is not intended to be limiting as to scope of the claims.</p><p id="p-0007" num="0006">Described herein are various technologies pertaining to a camera having a global shutter, where correction factors are applied to pixel values readout from an image sensor to form a corrected image, where photo response nonuniformity (PRNU) is addressed by applying the correction factors to the pixel values. With more specificity, during a calibration procedure, a matrix of correction factors is computed, where each pixel in the image sensor has a respective correction factor assigned thereto from the matrix of correction factors. With respect to an individual pixel, a correction factor for the pixel addresses a dark signal portion of a value readout from the pixel, where the dark signal portion is a portion of the value readout from the pixel that is caused by a &#x201c;dark signal&#x201d; (a signal that is independent of light that was incident upon the pixel during an exposure). The correction factor for the pixel additionally addresses parasitic light sensitivity of the pixel and gain associated with the image sensor. As noted above, the camera has a global shutter, such that when the camera generates an image, the pixels of the image sensor are exposed to a scene at the same time and for the same amount of time; however, pixels in different rows are readout at different times, thereby contributing to nonuniformity of outputs across the pixels, where PRNU is caused by different pixels having different parasitic light sensitivity and gain.</p><p id="p-0008" num="0007">In connection with computing a correction factor for a pixel in the image sensor of the camera, a calibration process described herein is undertaken. The image sensor is placed in an environment that includes no light (or as little light as possible). In other words, the flat field intensity of light to which the pixel is exposed is set to zero. Thereafter, the camera is operated to generate several data frames (images), where the data frames are captured when the camera has different integration times (exposure times). Thus, for example, the camera is configured to generate six data frames: 1) a first data frame when a first integration time is employed; 2) a second data frame when a second integration time is employed; 3) a third data frame when a third integration time is employed, etc. Continuing with this example, the result is six different values readout from the pixel for six different integration times, where the flat field intensity of light to which the pixel is exposed is zero. The values for the pixel can be linearly fitted to identify a slope that represents an increase in intensity with respect to integration time, and a dark signal constant value that represents the dark signal when the integration time is set to zero (the intercept of the linear fitting). Accordingly, for a given integration time, the correction factor accounts for a dark signal portion of the value readout from the pixel. When the camera is operated, and based upon the integration time used to capture a data frame, the dark signal portion in the correction factor is subtracted from the value readout from the pixel.</p><p id="p-0009" num="0008">To account for gain and parasitic light sensitivity, the image sensor is exposed to a flat light field of uniform (predefined) intensity, where the intensity is non-zero. While the image sensor is exposed to the uniform flat light field, the camera is caused to generate several data frames, each with a different integration time. Therefore, with respect to the pixel, several values will be readout from the pixel, with each value corresponding to a different integration time. Contrary to conventional approaches, during the calibration process, each value readout from the pixel is normalized based upon a value readout from a second pixel, where the second pixel is in a row that is readout first by readout electronics of the camera. For instance, when the pixel is in a tenth row of the image sensor and pixels in a first row of the image sensor are readout first by the readout electronics, the value readout for the pixel is normalized by at least one value readout from at least one pixel in the first row. In an example, an average of values readout from pixels in the first row is computed, and the pixel value is normalized by such average. This process is repeated for each value of the pixel readout with the different integration times, thereby creating several normalized values. These normalized values are subjected to a nonlinear fitting, such that a formula that defines a curve that is representative of influence of system gain and parasitic light intensity with respect to integration time is determined. The correction factor includes the formula that defines the curve, and therefore during operation of the camera and given an integration time, the correction value for the pixel is employed to account for a portion of the value caused by system gain and parasitic light sensitivity of the pixel. Hence, in an output image, PRNU is addressed by the correction factor. Correction factors, as indicated above, are computed for each pixel in the image sensor.</p><p id="p-0010" num="0009">In an embodiment, the camera with the global shutter can be included in an autonomous vehicle (AV). The AV includes the camera with a global shutter, a lidar system, and/or a radar system. A computing system of the AV receives an image output by the camera (e.g., corrected through use of the correction factors), and additionally receives a point cloud output by the lidar system and/or radar system. The AV performs a driving maneuver based upon the image and the point cloud.</p><p id="p-0011" num="0010">The above summary presents a simplified summary in order to provide a basic understanding of some aspects of the systems and/or methods discussed herein. This summary is not an extensive overview of the systems and/or methods discussed herein. It is not intended to identify key/critical elements or to delineate the scope of such systems and/or methods. Its sole purpose is to present some concepts in a simplified form as a prelude to the more detailed description that is presented later.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic that depicts a camera having a global shutter,</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a raw data frame output by a camera having a global shutter.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a data frame output by a camera having a global shutter, where the data frame illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> has been subject to correction for the raw image shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a timing diagram that illustrates times when different pixel values are readout from pixels in different rows of an image sensor in a camera having a global shutter.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a functional block diagram of a computing system that is configured to compute a matrix of correction factors for use when correcting images generated by a camera having a global shutter.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow diagram illustrating a methodology for outputting a matrix of dark signal correction factors for a camera having a global shutter.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a plot that depicts a linear fitting of values readout from a pixel in a camera having a global shutter, where the values correspond to different integration times.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow diagram of a methodology for outputting correction factors for pixels of a camera having a global shutter, where the correction factors are configured to address PRNU caused by parasitic light sensitivity and system gain with respect to pixels in a camera having a global shutter.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a plot that depicts a nonlinear fitting of values assigned to a pixel in an image sensor of a camera having a global shutter.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flow diagram of a methodology for generating a corrected image by applying correction factors to pixels of the image.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic that illustrates an autonomous vehicle (AV) that includes a camera having a global shutter.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an example computing system.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0024" num="0023">Various technologies pertaining to generating improved images in cameras with global shutters are now described with reference to the drawings, where like reference numerals are used to refer to like elements throughout. In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of one or more aspects. It may be evident, however, that such aspect(s) may be practiced without these specific details. In other instances, well-known structures and devices are shown in block diagram form in order to facilitate describing one or more aspects. Further, it is to be understood that functionality that is described as being carried out by certain system modules may be performed by multiple modules. Similarly, for instance, a module may be configured to perform functionality that is described as being carried out by multiple modules.</p><p id="p-0025" num="0024">Moreover, the term &#x201c;or&#x201d; is intended to mean an inclusive &#x201c;or&#x201d; rather than an exclusive &#x201c;or.&#x201d; That is, unless specified otherwise, or clear from the context, the phrase &#x201c;X employs A or B&#x201d; is intended to mean any of the natural inclusive permutations. That is, the phrase &#x201c;X employs A or B&#x201d; is satisfied by any of the following instances: X employs A; X employs B; or :X employs both A and B. In addition, the articles &#x201c;a&#x201d; and &#x201c;an&#x201d; as used in this application and the appended claims should generally be construed to mean &#x201c;one or more&#x201d; unless specified otherwise or clear from the context to be directed to a singular form.</p><p id="p-0026" num="0025">Described herein are various technologies pertaining to using correction factors to address photo response nonuniformity (PRNU) with respect to pixels in a camera that uses a global shutter to generate images. In an example, such a camera can be included in an autonomous vehicle (AV), and the AV can autonomously maneuver based upon images output by the camera. As will be described in greater detail herein, the camera employs correction factors in a correction matrix to address PRNU associated with pixels in an image sensor of the camera. The correction factors are computed to address PRNU associated with dark signals of the pixels and are further configured to address parasitic light sensitivity of the pixels, as well as system gain. With respect to the parasitic light sensitivity and system gain, the correction factors are computed by capturing several data frames (Images) when a uniform flat light field is applied over pixels of the image sensor of the camera throughout an integration time. Multiple data frames are captured, with each data frame captured using a different integration (exposure) time. With respect to a single pixel, and for each integration time, a value readout from the pixel is normalized based upon at least one value readout from at least one pixel in a predefined row of pixels, where pixels in the predefined row are readout first by readout electronics of the camera, and further where the value readout from the at least one pixel in the predefined row corresponds to the same integration time. Value(s) from pixels in a row that is readout first by readout electronics are used to normalize value(s) readout from the pixel due to the small amount of time between when the pixels in the row cease being exposed to light and when values are readout from such pixels. Additional detail is set forth below.</p><p id="p-0027" num="0026">With reference now to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a camera <b>100</b> that uses a global shutter in connection with generating images is illustrated. The camera <b>100</b> includes an image sensor <b>102</b>. The image sensor <b>102</b> includes several pixels (shown as multiple rows and columns of pixels), where each pixel includes a sensor node and a memory node. The sensor node is configured to convert light that is incident upon the sensor node into electrical charge, and the memory node is configured to retain such charge until readout from the memory node. Ideally, the memory node does not contribute to the charge retained therein; however, in actuality, due to physical properties of the memory node, electronics of the image sensor <b>102</b>, sensitivity to parasitic light, etc., after the charge is retained in the memory node, additional charge is accumulated over time, where an amount of the additional charge is dependent upon an integration time and an amount of time that the charge is stored in the memory node before being readout. This accumulation of additional charge is problematic in cameras with global shutters (such as the camera <b>100</b>), as values are read from memory nodes of different pixels at different times, despite the sensor nodes being exposed to a scene at the same time and for the same duration of time. For instance, values are readout pixels in the first row of pixels, followed by values being readout from pixels in the second row of pixels, followed by values being readout from pixels in the third row of pixels, and so forth. Therefore, even though the sensor nodes of the pixels are exposed to the scene at the same time and for the same amount of time, values are readout from different pixels at different times, causing PRNU.</p><p id="p-0028" num="0027">The camera <b>100</b> further includes readout electronics <b>104</b> that are configured to readout values from the memory nodes of the pixels in the image sensor <b>102</b>, where the readout electronics <b>104</b> reads out values from pixels row by row (e.g., from top to bottom).</p><p id="p-0029" num="0028">The camera <b>100</b> also includes memory <b>106</b> and processing circuitry <b>108</b> that is operably coupled to the memory <b>106</b>, where the memory <b>106</b> is configured to store data that is accessible to the processing circuitry <b>108</b> and instructions that are executed by the processing circuitry <b>108</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the memory <b>106</b> includes an image <b>110</b> (also referred to as a data frame), where the image <b>110</b> includes a matrix of values that respectively correspond to the matrix of pixels in the image sensor <b>102</b>. Thus, the image <b>110</b> includes values readout from the pixels of the image sensor <b>102</b> by the readout electronics <b>104</b>.</p><p id="p-0030" num="0029">The memory <b>106</b> further includes a correction module <b>112</b> that is executed by the processing circuitry <b>108</b>, where the correction module <b>112</b> is configured to apply correction factors to the values of the image <b>110</b>.</p><p id="p-0031" num="0030">The memory <b>106</b> also includes a correction matrix <b>114</b> that includes correction factors that respectively correspond to the pixels of the image sensor <b>102</b>. Therefore, each pixel of the image sensor <b>102</b> has a correction factor assigned thereto in the correction matrix <b>114</b>. A correction factor for a pixel in the image sensor <b>102</b>. addresses two portions of a value readout from the pixel that are not caused by light incident upon the pixel during an exposure of the pixel. These two portions are: 1) a dark signal portion; and 2) and a PRNU portion. The dark signal portion is a portion of the value readout from the pixel that is independent of the light incident upon the sensor node of the pixel during exposure of the pixel. In other words, the dark signal portion is caused by electronics of the camera <b>100</b> (e.g., caused by background noise in the camera). Therefore, if no light was incident upon the sensor node of the pixel during an exposure, electronics of the camera <b>100</b> nevertheless cause the memory node to accumulate charge that is subsequently readout by the readout electronics <b>104</b>. The correction factor in the correction matrix <b>114</b> for the pixel addresses the dark signal portion, such that when the correction module <b>112</b> applies the correction factor to the value readout from the memory node of the pixel, the dark signal portion of the readout value is removed from the readout value.</p><p id="p-0032" num="0031">The PRNU portion of the readout value is caused by parasitic light sensitivity of the memory node of the pixel and system gain. As will be described in greater detail herein, the correction factor for the pixel is determined during a calibration phase and is based upon at least one value readout from a second pixel during the calibration phase, where the second pixel is included in a row of the image sensor <b>102</b> that is readout by the readout electronics <b>104</b> first. The calibration phase that is employed in connection with computing correction factors of the correction matrix <b>114</b> will be described in greater detail below.</p><p id="p-0033" num="0032">Upon the correction module <b>110</b> respectively applying the correction factors of the correction matrix <b>114</b> to values in the image <b>110</b>, the correction module <b>112</b> outputs a corrected image <b>116</b>. Therefore, in the corrected image <b>116</b>, PRNU has been addressed by the processing circuitry <b>108</b> through utilization of the correction factors in the correction matrix <b>114</b>. In an example, the corrected image <b>116</b> can be provided to a computing system of an AV, and the AV performs a driving maneuver based upon the corrected image <b>116</b>. The driving maneuver may be a turn, deceleration, acceleration, or other suitable driving maneuver.</p><p id="p-0034" num="0033">Referring now to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an example depiction of the image <b>110</b> is set forth. The image <b>110</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> represents values readout from the memory nodes of pixels of the image sensor <b>102</b> by the readout electronics <b>104</b> prior to the image <b>110</b> being subjected to correction by the correction module <b>112</b>. The image <b>110</b> depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref> was constructed based upon the pixels of the image sensor <b>102</b> being exposed to a uniform flat field of light during an integration window. Hence, intensity values of the pixels of the image <b>110</b> should be uniform. As can be ascertained from viewing the image <b>110</b> depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, however, intensities of values of pixels in the image <b>110</b> are not uniform. Rather, the intensities increase from top to bottom, which corresponds to times when pixels of the image sensor <b>102</b> had values read therefrom.</p><p id="p-0035" num="0034">Referring now to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, an example depiction of the corrected image <b>116</b> is presented. Put differently, the image <b>116</b> depicted in <figref idref="DRAWINGS">FIG. <b>3</b></figref> is the image <b>110</b> depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref> subsequent to such image <b>110</b> being corrected by the correction module <b>112</b>. From reviewing <figref idref="DRAWINGS">FIG. <b>3</b></figref>, it can be ascertained that the corrected image <b>116</b> has improved uniform intensity across pixels when compared to the image illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, which corresponds to the uniform flat field of light applied across the image sensor <b>102</b> during exposure.</p><p id="p-0036" num="0035">With reference now to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a timing diagram <b>400</b> that illustrates an integration time for pixels in the image sensor <b>102</b> as well as an amount of time subsequent to the integration time when values are read from (memory nodes of) the pixels. As illustrated in the schematic of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the image sensor <b>102</b> includes X rows of pixels. With respect to an image generated by the camera <b>100</b>, the camera <b>100</b> has an integration time of t<sub>2</sub>, which includes an amount of time that the pixels are exposed to an environment and amount of time t<sub>2</sub>&#x2212;t<sub>3</sub>) for transferring charge from the sensor nodes of the pixels to the memory nodes of the pixels. As the image sensor <b>102</b> captures an image of an entirety of a scene at the same time, charges are transferred from the sensor nodes of the pixels to the memory nodes of the pixels at the same time.</p><p id="p-0037" num="0036">Subsequently, the readout electronics <b>104</b> begins to readout values from the memory nodes of the pixels row by row. Hence, at time t<sub>2 </sub>(or immediately after t<sub>2</sub>), the readout electronics <b>104</b> reads out values from memory nodes of pixels in the first row of the image sensor <b>102</b>. Subsequent to reading out the values from the memory nodes of the pixels in the first row, the readout electronics <b>104</b> reads out values from memory nodes of pixels in the second row, where the additional time is represented by reference numeral <b>402</b>. The time from when the charge is stored in the memory node of the pixel increases as the row number increases and is represented by dt. This increase in time, for example, contributes to nonuniformity between values read from memory nodes of pixels in the first row and values readout from memory nodes of pixels in later rows (e.g., the Xth row).</p><p id="p-0038" num="0037">Turning now to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a functional block diagram of a computing system <b>500</b> that is configured to generate the correction matrix <b>114</b> is illustrated. The computing system <b>500</b> includes a processor <b>502</b> and memory <b>504</b>, where the memory <b>504</b> includes data that is accessible to the processor and instructions that are executed by the processor <b>502</b>. The computing system <b>500</b> is in communication with the camera <b>100</b> and is configured to generate the correction matrix <b>114</b> based upon data frames generated by the camera <b>100</b>.</p><p id="p-0039" num="0038">For example, the camera <b>100</b> can be configured to generate dark field images (where, when the dark field images are captured, a flat field intensity of light applied across pixels of the image sensor <b>102</b> is zero). The camera <b>100</b> is further configured to generate the dark field images such that each dark field image is captured with a different exposure time. The data store <b>506</b> includes a first dark field image <b>508</b> through an Nth dark field image <b>510</b>, where the first dark field image was generated when the camera <b>100</b> used a first exposure time, and the Nth dark field image was generated when the camera <b>100</b> used an Nth exposure time.</p><p id="p-0040" num="0039">The data store <b>506</b> further includes uniform intensity images <b>512</b>-<b>514</b> generated by the camera <b>100</b>, where each image in the uniform intensity images <b>512</b>-<b>514</b> was generated with a different exposure time, and further where the uniform intensity images <b>512</b>-<b>514</b> were generated by the camera <b>100</b> when a flat field of light with uniform intensity was applied across the image sensor <b>102</b>. Hence, the first uniform intensity image <b>512</b> was generated by the camera <b>100</b> with a first exposure time when a flat field of light of uniform intensity was applied across the image sensor <b>102</b>, and the uniform intensity image N <b>514</b> the generated by the camera <b>100</b> with an Nth exposure time when the fiat field of light of uniform intensity was applied across the image sensor <b>102</b>.</p><p id="p-0041" num="0040">The memory <b>504</b> includes a calibration system <b>516</b> that, when executed by the processor <b>502</b>, generates the correction matrix <b>114</b> based upon the images <b>508</b>-<b>514</b> in the data store <b>506</b>. As described above, a value read from a memory node of a pixel in the image sensor <b>102</b> by the readout electronics <b>104</b> represents is an aggregate of three different portions: 1) an intensity of light incident upon the pixel over the exposure time (the portion of the value that should match the input); 2) a dark signal portion (the portion of the readout value that is attributable to background noise), and 3) a PRNU portion caused by parasitic light sensitivity of the memory node of the pixel and system gain. A correction factor for the pixel in the correction matrix <b>114</b>, when applied to the value readout from the memory node of the pixel, is configured to account for the second and third portion in the value, thereby leaving the desired output. The calibration system <b>516</b> is configured to compute a correction factor for each pixel in the image sensor <b>102</b> based upon the images <b>508</b>-<b>514</b>.</p><p id="p-0042" num="0041">To that end, the calibration system <b>516</b> includes a dark field module <b>518</b> that is configured to compute, for each pixel in the image sensor <b>102</b>, a dark field signal correction factor based upon the dark field images <b>508</b>-<b>510</b>. It has further been observed that the dark field signal value increases linearly with integration time. Hence, the dark field module <b>518</b>, with respect to the pixel, can perform a linear fitting to compute a slope for the dark field signal value, where the slope represents rate of change of the dark field signal value with respect to integration time. A correction factor in the correction matrix <b>114</b> for the pixel includes a dark field constant value and the slope. The dark field constant value can be set as the intercept of the linear fitting (where the integration time is set to zero). Subsequent to calibration, when the camera <b>100</b> is employed to generate an image with a certain integration time, the dark field constant value and the slope multiplied by the integration time can be subtracted from the value from the memory node of the pixel readout by the readout electronics <b>104</b>, thereby removing the dark field signal portion from the readout value.</p><p id="p-0043" num="0042">The calibration system <b>516</b> additionally includes an AC matrix module <b>520</b> that is configured to compute, for each pixel, a correction factor that addresses parasitic light sensitivity of the pixel and system gain associated with the pixel, wherein the AC matrix module <b>520</b> computes such correction factor based upon the uniform intensity images <b>512</b>-<b>514</b>. Contrary to conventional approaches, the AC matrix module <b>520</b> computes the correction factor for a pixel by normalizing values for the pixel in the uniform intensity images <b>512</b>-<b>514</b> based upon values for at least one pixel in the images <b>512</b>-<b>514</b>, where the at least one pixel is in a row that is readout first by the readout electronics <b>104</b>.</p><p id="p-0044" num="0043">In a more specific example, in the first uniform intensity image <b>512</b>, the AC matrix module <b>520</b> can identify a row in such image <b>512</b> that corresponds to a row of pixels in the image sensor <b>102</b> that was readout first (before any other rows in the image sensor <b>102</b>) by the readout electronics <b>104</b>. The AC matrix module <b>520</b> may then compute a mean value for such row. When computing a correction factor for a pixel in the image sensor <b>102</b>, the AC matrix module <b>520</b>. for example, identifies a value that corresponds to the pixel in the first uniform intensity image <b>512</b> and then normalizes that value through use of an average of values in a row of the first uniform intensity image <b>512</b> that were readout first by the readout electronics <b>104</b>. The first row of pixels is selected due to the input to the image sensor being uniform (a uniform flat field of light intensity), and therefore the output should also be uniform and, except for the constant dark field signal portion of values in the first row, should be approximately equivalent to the values in the first row of pixels.</p><p id="p-0045" num="0044">Referring again to a single pixel, a respective normalized value is computed for the pixel for each of the uniform intensity images <b>512</b>-<b>514</b> (e.g., N normalized values). These N values form a non-linear curve, and the AC matrix module <b>520</b> can perform a nonlinear fitting to identify a function that fits the N normalized values. The correction factor for the pixel can thus be the dark signal constant value, the slope of the dark signal value, and the function computed by the AC matrix module <b>520</b>. When the camera <b>100</b> is subsequently employed to capture an image, the value read from a pixel by the readout electronics <b>104</b> can be corrected through utilization of the corresponding correction factor for the pixel in the correction matrix <b>114</b>. As noted above, the correction matrix <b>114</b> includes a correction factor for each pixel in the image sensor <b>102</b>.</p><p id="p-0046" num="0045">Referring to <figref idref="DRAWINGS">FIGS. <b>6</b>, <b>8</b>, and <b>10</b></figref>, methodologies that are performed by the computing system <b>500</b> and/or the camera <b>100</b> are illustrated. While the methodologies are shown and described as being a series of acts that are performed in a sequence, it is to be understood and appreciated that the methodology is not limited by the order of the sequence. For example, some acts can occur in a different order than what is described herein. In addition, an act can occur concurrently with another act. Further, in some instances, not all acts may be required to implement the methodology described herein.</p><p id="p-0047" num="0046">Moreover, some of the acts described herein may be computer-executable instructions that can be implemented by one or more processors and/or stored on a computer-readable medium or media. The computer-executable instructions can include a routine, a sub-routine, programs, a thread of execution, and/or the like. Still further, results of acts of the methodologies can be stored in a computer-readable medium, displayed on a display device, and/or the like.</p><p id="p-0048" num="0047">Referring now solely to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a flow diagram illustrating an example methodology <b>600</b> computing dark signal constant values and rates of change of dark signal values for the pixels in the image sensor <b>102</b> is illustrated. The methodology <b>600</b> starts at <b>602</b>, and at <b>604</b> an image sensor is positioned in a lightless environment, such that a flat field light intensity applied across pixels of the image sensor <b>102</b> is zero. At <b>606</b>, data frames (images) are captured using different integration times. It is to be understood that any suitable number of images can be generated, each with a different integration time. For instance, four images, eight images, twelve images, etc.</p><p id="p-0049" num="0048">At <b>608</b>, pixel i is selected (where i initially equals one). At <b>610</b>, a dark signal constant value and dark signal value increase rate (rate of change) are determined for pixel i, as described above. At <b>612</b>, a determination is made as to whether there are additional pixels for which dark signal constant values and dark signal value increase rates are to be computed. If it is determined at <b>612</b> that there are more pixels (e.g., i is less than the number of pixels in the image sensor <b>102</b>), then the methodology <b>600</b> proceeds to <b>614</b>, where i is incremented. The methodology <b>600</b> then returns to <b>608</b>, where the next pixel is selected. When it is determined at <b>612</b> that there are no more pixels, at <b>616</b> a matrix of dark signal constant values and dark signal value increase rates is output for pixels of the image sensor <b>102</b>. The methodology completes at <b>618</b>.</p><p id="p-0050" num="0049">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a plot <b>700</b> that depicts values readout from a pixel in the image sensor <b>102</b> with respect to data frames generated in a dark field with different integration times is depicted. As can be ascertained, in the example plot <b>700</b>, six different values are illustrated, where the increase in magnitude of the values in the data frames with respect to integration time is approximately linear. A linear fitting can be performed on such values to determine the rate of change of dark signal values with respect to the global shutter integration time, as well as the dark signal constant value.</p><p id="p-0051" num="0050">Now referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a flow diagram illustrating a methodology <b>800</b> for computing an &#x201c;A+C&#x201d; matrix is illustrated. The methodology <b>800</b> starts at <b>802</b>, and at <b>804</b> a fiat field intensity to be applied across the image sensor <b>102</b> is set to a desired value (non-zero). For instance, the value can be somewhere in the operating range of the camera <b>100</b> (to avoid over or under saturation). At <b>806</b>, data frames are captured using different integration times. At <b>808</b>, pixel i is selected (where i is initially set to one).</p><p id="p-0052" num="0051">At <b>810</b>, data frame q associated with a qth integration time is selected. At <b>812</b>, a readout value for pixel i in the qth data frame is normalized based upon a value of a second pixel in the qth data frame, where the second pixel in the qth data frame corresponds to a row of pixels in the image sensor <b>102</b> that was readout first by the readout electronics <b>104</b>. In a more specific example, as indicated above, the readout value for pixel i in data frame q is normalized at <b>812</b> based upon an average value of values in a row in data frame q that correspond to the row of pixels in the image sensor <b>102</b> that were first readout by the readout electronics <b>104</b> when generating data frame q. At <b>814</b>, a determination is made as to whether there are additional data frames to consider. When it is determined at <b>814</b> that there are additional data frames to consider, the methodology <b>800</b> proceeds to <b>816</b>, where q is incremented, and the next data frame is selected <b>810</b>.</p><p id="p-0053" num="0052">When it is determined at <b>814</b> that there are no more data frames to consider, then at <b>818</b> a nonlinear fitting is performed on the normalized readout values for pixel i to identify a function that fits the curve formed by the normalized values.</p><p id="p-0054" num="0053">At <b>820</b>, a determination is made as to whether there are additional pixels to consider (e.g., whether i is less than the number of pixels in the image sensor <b>102</b>). When it is determined that there are additional pixels to consider, the methodology proceeds to <b>822</b> where i is incremented. Thereafter, the methodology <b>800</b> returns to <b>808</b>. When it is determined at <b>820</b> that there are no more pixels to consider, the methodology <b>800</b> proceeds to <b>824</b>, where functions for pixels of the image sensor <b>102</b> are output for inclusion in the correction matrix <b>114</b>. The methodology <b>800</b> completes at <b>826</b>.</p><p id="p-0055" num="0054">With brief reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a plot that illustrates a nonlinear fitting of normalized values for a pixel is presented. The plot <b>900</b> depicts values for a single pixel; it is again emphasized that a function can be computed for each pixel in the image sensor <b>102</b>.</p><p id="p-0056" num="0055">Referring now to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, a flow diagram illustrating a methodology <b>1000</b> for generating an image using a camera with a global shutter (and addressing PRNU) is illustrated. The methodology <b>1000</b> starts at <b>1002</b>, and at <b>1004</b> a readout value for a pixel in an image sensor is received. At <b>1006</b>, a correction factor is applied to the readout value to form a corrected readout value, where the correction factor is applied to the readout value to address PRNU of the pixel. The correction factor is computed based upon a second readout value of a second pixel in a row from amongst several rows of the image sensor, where the second readout value was previously obtained when a uniform light field was applied over pixels of the image sensor, and further wherein the values from pixels in the row were readout prior to reading of values from pixels in any other row in the several rows of the image sensor. At <b>1008</b>, an image is generated based upon the corrected readout value. The methodology <b>1000</b> completes at <b>1010</b>.</p><p id="p-0057" num="0056">A mathematical description of application of a correction factor and computation of the correction factor is now set forth. With respect to a single pixel in the image sensor <b>102</b>:</p><p id="p-0058" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>r=g</i>(<i>It+pI&#x394;t</i>)<i>+d, </i>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0059" num="0000">where r is a value readout from the pixel, g is the system gain, I is the intensity of light incident upon the pixel (the input), t is the integration time, p is the parasitic light sensitivity, &#x394;t is time added to readout the signal from the pixel after the global shutter is closed, and d is the dark signal.</p><p id="p-0060" num="0057">The dark signal can be represented as follows:</p><p id="p-0061" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d=dst+d</i><sub>0</sub>, &#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0062" num="0000">where ds is the increase rate of the dark signal value and d<sub>0 </sub>is the dark signal constant value, which can be determined by linearly fitting r=ds t+d<sub>0</sub>, with I=0. A noiseless reading from the pixel can be obtained by subtracting d from r:</p><p id="p-0063" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>m=r&#x2212;d=</i>(1+<i>p&#x394;t/t</i>)<i>gIt. </i>&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0064" num="0000">Eq. (1) can be modified to represent all pixels of the image sensor <b>102</b> as follows:</p><p id="p-0065" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>R[x,y]=G[x,y]</i>(<i>I[x,y]t+P[x,y]I[x,y]&#x394;T [x,y]</i>)<i>+D[x,y], </i>&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0066" num="0000">where capital letters denote matrices, and where [x, y] are coordinates for a pixel located at [x, y] on the image sensor <b>102</b>. The above can be simplified by omitting the [x, y] coordinates annotation as follows:</p><p id="p-0067" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>R=G</i>(<i>It+PI&#x394;T</i>)<i>+D. </i>&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0068" num="0000">Equation (3), written using matrix annotation, is as follows:</p><p id="p-0069" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>M=</i>(1+<i>P&#x394;t/t</i>)<i>GIt. </i>&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0070" num="0000">As M represents the noiseless output values of pixels in the image sensor, the correction module <b>112</b> can correct for PRNU by way of the following algorithm:</p><p id="p-0071" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Y=KM, &#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0072" num="0000">where Y is the corrected image (e.g., the image <b>116</b>) output by the correction module <b>112</b> and K represents corrections factors for pixels in the raw image <b>110</b>.</p><p id="p-0073" num="0058">K can be ascertained during the calibration procedure described herein. As described previously, a uniform flat field I<sup>0 </sup>is applied across the image sensor <b>102</b>. Since I<sup>0 </sup>is uniform, when a correction factor is applied to the noiseless output values, the output should also be uniform. Hence:</p><p id="p-0074" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msup>      <mi>Y</mi>      <mn>0</mn>     </msup>     <mo>=</mo>     <mrow>      <msup>       <mi>KM</mi>       <mn>0</mn>      </msup>      <mo>=</mo>      <mrow>       <msup>        <mi>KGI</mi>        <mn>0</mn>       </msup>       <mo>&#x2062;</mo>       <mrow>        <mrow>         <mi>t</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mn>1</mn>          <mo>+</mo>          <mrow>           <mi>P</mi>           <mo>&#x2062;</mo>           <mfrac>            <mrow>             <mi>&#x394;</mi>             <mo>&#x2062;</mo>             <mi>T</mi>            </mrow>            <mi>t</mi>           </mfrac>          </mrow>         </mrow>         <mo>)</mo>        </mrow>        <mo>.</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0075" num="0000">As values of pixels readout first by the readout electronics <b>104</b> have negligible parasitic light sensitivity, Y<sup>0 </sup>can be presumed to be equivalent to the mean of the values of the aforementioned pixels (e.g., values of pixels in the first row that are readout immediately after the global shutter is closed: <u style="single">M<sub>[0,:]</sub><sup>0</sup></u>). Therefore, <u style="single">M<sub>[0,:]</sub><sup>0</sup></u>=K M<sup>0</sup>, and thus K=<u style="single">M<sub>[0,:]</sub><sup>0</sup></u>/M<sup>0</sup>.</p><p id="p-0076" num="0059">Because the parasitic light sensitivity is negligible for values readout from pixels in the first row, <o ostyle="single">M<sub>[0,:]</sub><sup>0</sup></o>&#x2dc;<o ostyle="single">G<sub>[0,:]</sub></o>I<sup>0</sup>t. Given such assumption, the following derivation can be undertaken:</p><p id="p-0077" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msup>       <mi>M</mi>       <mn>0</mn>      </msup>      <mo>=</mo>      <mrow>       <msup>        <mi>GI</mi>        <mn>0</mn>       </msup>       <mo>&#x2062;</mo>       <mrow>        <mi>t</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>+</mo>         <mrow>          <mi>P</mi>          <mo>&#x2062;</mo>          <mfrac>           <mrow>            <mi>&#x394;</mi>            <mo>&#x2062;</mo>            <mi>T</mi>           </mrow>           <mi>t</mi>          </mfrac>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>     <mo>&#x2062;</mo>     <mtext></mtext>     <mrow>      <mi>K</mi>      <mo>=</mo>      <mrow>       <mrow>        <mover>         <msup>          <mi>M</mi>          <mn>0</mn>         </msup>         <mo>_</mo>        </mover>        <mo>/</mo>        <msup>         <mi>M</mi>         <mn>0</mn>        </msup>       </mrow>       <mo>=</mo>       <mrow>        <mover>         <msub>          <mi>G</mi>          <mrow>           <mo>[</mo>           <mrow>            <mn>0</mn>            <mo>,</mo>            <mo>:</mo>           </mrow>           <mo>]</mo>          </mrow>         </msub>         <mo>_</mo>        </mover>        <mo>/</mo>        <mrow>         <mo>(</mo>         <mrow>          <mi>G</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mn>1</mn>           <mo>+</mo>           <mrow>            <mi>P</mi>            <mo>&#x2062;</mo>            <mi>&#x394;</mi>            <mo>&#x2062;</mo>            <mi>T</mi>            <mo>/</mo>            <mi>t</mi>           </mrow>          </mrow>          <mo>)</mo>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mrow>     </mrow>     <mo>&#x2062;</mo>     <mtext></mtext>     <mrow>      <mi>K</mi>      <mo>=</mo>      <mfrac>       <mn>1</mn>       <mrow>        <mi>A</mi>        <mo>+</mo>        <mfrac>         <mi>C</mi>         <mi>t</mi>        </mfrac>       </mrow>      </mfrac>     </mrow>     <mo>&#x2062;</mo>     <mtext></mtext>     <mrow>      <mi>A</mi>      <mo>=</mo>      <mrow>       <mi>G</mi>       <mo>/</mo>       <mrow>        <mover>         <msub>          <mi>G</mi>          <mrow>           <mo>[</mo>           <mrow>            <mn>0</mn>            <mo>,</mo>            <mo>:</mo>           </mrow>           <mo>]</mo>          </mrow>         </msub>         <mo>_</mo>        </mover>        <mo>(</mo>        <mrow>         <mi>the</mi>         <mo>&#x2062;</mo>         <mtext>   </mtext>         <mi>A</mi>         <mo>&#x2062;</mo>         <mtext>   </mtext>         <mi>matrix</mi>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>     <mo>&#x2062;</mo>     <mtext></mtext>     <mrow>      <mi>C</mi>      <mo>=</mo>      <mrow>       <mi>P</mi>       <mo>&#x2062;</mo>       <mi>&#x394;</mi>       <mo>&#x2062;</mo>       <mi>TG</mi>       <mo>/</mo>       <mrow>        <mover>         <msub>          <mi>G</mi>          <mrow>           <mo>[</mo>           <mrow>            <mn>0</mn>            <mo>,</mo>            <mo>:</mo>           </mrow>           <mo>]</mo>          </mrow>         </msub>         <mo>_</mo>        </mover>        <mo>(</mo>        <mrow>         <mi>the</mi>         <mo>&#x2062;</mo>         <mtext>   </mtext>         <mi>C</mi>         <mo>&#x2062;</mo>         <mtext>   </mtext>         <mi>matrix</mi>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>     <mo>&#x2062;</mo>     <mtext></mtext>     <mrow>      <mi>Y</mi>      <mo>=</mo>      <mfrac>       <mi>M</mi>       <mrow>        <mi>A</mi>        <mo>+</mo>        <mfrac>         <mi>C</mi>         <mi>t</mi>        </mfrac>       </mrow>      </mfrac>     </mrow>     <mo>&#x2062;</mo>     <mtext></mtext>     <mrow>      <msup>       <mi>Y</mi>       <mn>0</mn>      </msup>      <mo>=</mo>      <mfrac>       <msup>        <mi>M</mi>        <mn>0</mn>       </msup>       <mrow>        <mi>A</mi>        <mo>+</mo>        <mfrac>         <mi>C</mi>         <mi>t</mi>        </mfrac>       </mrow>      </mfrac>     </mrow>     <mo>&#x2062;</mo>     <mtext></mtext>     <mrow>      <mrow>       <mrow>        <mi>A</mi>        <mo>+</mo>        <mrow>         <mi>C</mi>         <mo>/</mo>         <mi>t</mi>        </mrow>       </mrow>       <mo>=</mo>       <mrow>        <mrow>         <msup>          <mi>M</mi>          <mn>0</mn>         </msup>         <mo>/</mo>         <msup>          <mi>Y</mi>          <mn>0</mn>         </msup>        </mrow>        <mo>=</mo>        <mrow>         <msup>          <mi>M</mi>          <mn>0</mn>         </msup>         <mo>/</mo>         <mover>          <msubsup>           <mi>M</mi>           <mrow>            <mo>[</mo>            <mrow>             <mn>0</mn>             <mo>,</mo>             <mo>:</mo>            </mrow>            <mo>]</mo>           </mrow>           <mn>0</mn>          </msubsup>          <mo>_</mo>         </mover>        </mrow>       </mrow>      </mrow>      <mo>,</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>9</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0078" num="0000">where matrices A and C can be nonlinearly fitted by sweeping different integration times with uniform flat field I<sup>0</sup>.</p><p id="p-0079" num="0060">Once D<sub>s</sub>t, D<sub>0</sub>, and (A+C/t) are identified with the integration time t, the correction module <b>112</b> can correct a raw image R to obtain a corrected image Y by way of the following algorithm:</p><p id="p-0080" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Y=</i>(<i>R&#x2212;D</i><sub>s</sub><i>t&#x2212;D</i><sub>0</sub>)/(<i>A+C/t</i>) &#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0081" num="0061">Referring now to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, an AV <b>1100</b> is illustrated. As depicted in callout <b>1102</b>. the AV <b>1100</b> includes the camera <b>100</b>, a lidar system <b>1104</b>, and a computing system <b>1106</b>. The computing system <b>1106</b> is in communication with the camera <b>100</b> and the lidar system <b>1104</b>. The computing system <b>1106</b> is configured to receive a corrected image output by the camera <b>100</b> and is further configured to receive a point cloud that is representative of distances to objects in proximity to the AV <b>1100</b> from the lidar system <b>1104</b>. The computing system <b>1106</b> causes the AV <b>1100</b> to perform a driving maneuver based upon the corrected image output by the camera <b>100</b> and the point cloud output by the lidar system <b>1104</b>. For instance, the computing system <b>1106</b> can be configured to control a propulsion system of the AV <b>1100</b>, where the propulsion system can be or include an electric motor, a combustion engine, etc. In another example, the computing system <b>1106</b> is configured to control a steering mechanism of the AV <b>1100</b> based upon the corrected image output by the camera <b>100</b> and the point cloud output by the lidar system <b>1104</b>. In still yet another example, the computing system <b>106</b> is configured to control a braking system of the AV <b>1100</b> based upon the corrected image output by the camera <b>100</b> and the point cloud output by the lidar system <b>1104</b>.</p><p id="p-0082" num="0062">Referring now to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, a high-level illustration of an exemplary computing device <b>1200</b> that can be used in accordance with the systems and methodologies disclosed herein is illustrated. For instance, the computing device <b>1200</b> may be used in a system that is configured to correct an image created by a camera having a global shutter. By way of another example, the computing device <b>1200</b> can be used in a system that is configured to compute correction factors to use when correcting an image. The computing device <b>1200</b> includes at least one processor <b>1202</b> that executes instructions that are stored in a memory <b>1204</b>. The instructions may be, for instance, instructions for implementing functionality described as being carried out by one or more components discussed above or instructions for implementing one or more of the methods described above. The processor <b>1202</b> may access the memory <b>1204</b> by way of a system bus <b>1206</b>. In addition to storing executable instructions, the memory <b>1204</b> may also store pixel values, correction factors, images, calibration settings, etc.</p><p id="p-0083" num="0063">The computing device <b>1200</b> additionally includes a data store <b>1208</b> that is accessible by the processor <b>1202</b> by way of the system bus <b>1206</b>. The data store <b>1208</b> may include executable instructions, images, correction factors, etc. The computing device <b>1200</b> also includes an input interface <b>1210</b> that allows external devices to communicate with the computing device <b>1200</b>. For instance, the input interface <b>1210</b> may be used to receive instructions from an external computer device, from a user, etc. The computing device <b>1200</b> also includes an output interface <b>1212</b> that interfaces the computing device <b>1200</b> with one or more external devices. For example, the computing device <b>1200</b> may display text, images, etc. by way of the output interface <b>1212</b>.</p><p id="p-0084" num="0064">Additionally, while illustrated as a single system, it is to be understood that the computing device <b>1200</b> may be a distributed system. Thus, for instance, several devices may be in communication by way of a network connection and may collectively perform tasks described as being performed by the computing device <b>1200</b>.</p><p id="p-0085" num="0065">Various functions described herein can be implemented in hardware, software, or any combination thereof. If implemented in software, the functions can be stored on or transmitted over as one or more instructions or code on a computer-readable medium. Computer-readable media includes computer-readable storage media. A computer-readable storage media can be any available storage media that can be accessed by a computer. By way of example, and not limitation, such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Disk and disc, as used herein, include compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk, and Blu-ray disc (BD), where disks usually reproduce data magnetically and discs usually reproduce data optically with lasers. Further, a propagated signal is not included within the scope of computer-readable storage media. Computer-readable media also includes communication media including any medium that facilitates transfer of a computer program from one place to another. A connection, for instance, can be a communication medium. For example, if the software is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio and microwave are included in the definition of communication medium. Combinations of the above should also be included within the scope of computer-readable media.</p><p id="p-0086" num="0066">Alternatively, or in addition, the functionally described herein can be performed, at least in part, by one or more hardware logic components. For example, and without limitation, illustrative types of hardware logic components that can be used include field-programmable Gate Arrays (FPGAs), Program-specific Integrated Circuits (ASICs), Program-specific Standard Products (ASSPs), System-on-a-chip systems (SOCs), Complex Programmable Logic Devices (CPLDs), etc.</p><p id="p-0087" num="0067">The features described herein relate to generating an improved image as output by a camera having a global shutter according to at least the examples provided below.</p><p id="p-0088" num="0068">(A1) In one aspect, some embodiments include a method performed by processing circuitry of a camera having a global shutter, where the processing circuitry is in communication with an image sensor of the camera. The method includes receiving a readout value for a pixel in the image sensor. The method also includes applying a correction factor to the readout value to form a corrected readout value. The correction factor is applied to the readout value to address PRNU of the pixel. The correction factor is computed during a calibration process based upon a second readout value of a second pixel in a row from amongst the several rows, where the second readout value was previously obtained during the calibration process when a uniform light field was applied over the image sensor, wherein the processing circuitry reads out values from pixels in the row prior to reading out values from pixels in any other row in the several rows. The method further includes generating an image based upon the corrected readout value.</p><p id="p-0089" num="0069">(A2) In some embodiments of the method of A1, the method also includes receiving a third readout value for a third pixel of the image captured by the camera, where the pixel is in a second row in the several rows and the third pixel is in a third row in the several rows, the third row being different from the second row, The method additionally includes applying a second correction factor to the third readout value to form a second corrected readout value. The second correction factor is applied to the third readout value to address PRNU of the third pixel. The second correction factor is computed during the calibration process based upon an average of the readout values of pixels in the row from amongst the several rows when the uniform light field was applied over the image sensor, where the image is generated based further upon the second corrected readout value.</p><p id="p-0090" num="0070">(A3) in some embodiments of any of the methods of A1-A2, the correction factor is further computed based upon a dark signal value for the pixel, wherein the dark signal value is representative of a portion of the readout signal that is not caused by light that is incident upon the pixel.</p><p id="p-0091" num="0071">(A4) in some embodiments of the method of A3, the dark signal value is computed based upon an integration time for the camera.</p><p id="p-0092" num="0072">(A5) in some embodiments of any of the methods of A1-A4, the correction factor is computed based upon a first value readout from the pixel when the uniform light field was applied over the pixel when the camera captured a first image using a first exposure time. In addition, the correction factor is computed based upon a second value readout from the pixel when the camera was placed in an environment with no light and captured a second image using a second exposure time.</p><p id="p-0093" num="0073">(A6) In some embodiments of any of the methods of A1-A5, an autonomous vehicle comprises the camera and performs a driving maneuver based upon the image generated by the camera.</p><p id="p-0094" num="0074">(A7) In some embodiments of any of the methods of A1-A6, the calibration process includes: a) applying a uniform flat field intensity of zero across the several rows of pixels; b) while the uniform flat field of the intensity of zero is applied across the several rows of pixels, capturing images with different respective integration times; c) for each of the images captured, reading out respective values of the pixel; and d) linearly fitting the respective values of the pixel to determine a slope, wherein the correction factor is computed based upon the slope.</p><p id="p-0095" num="0075">(A8) In some embodiments of the method of A7, the calibration process further includes: e) applying a second uniform flat field intensity across the several rows of pixels, the second uniform flat field intensity being non-zero; f) while the second uniform flat field intensity is applied across the several rows of pixels, capturing second images with different respective integration times; g) for each of the second images captured, reading out second respective values of the pixel; h) normalizing the second respective values of the pixel based upon the readout value of the second pixel; and i) non-linearly fitting the second respective values of the pixel to determine a function, wherein the correction factor is computed based upon the function.</p><p id="p-0096" num="0076">(B1) In another aspect, some embodiments include a method performed by processing circuitry of a camera having a global shutter. The method includes receiving a readout value for a pixel in an image sensor of the camera. The method further includes applying a correction factor to the readout value to form a corrected readout value. The correction factor is applied to the readout value to address PRNU of the pixel. The correction factor is computed based upon a second readout value of a second pixel in a row from amongst the several rows, where the second readout value was previously obtained when a uniform light field was applied over the image sensor, and further where the processing circuitry reads out values from pixels in the row prior to reading out values from pixels in any other row in the several rows. The method additionally includes generating an image based upon the corrected readout value.</p><p id="p-0097" num="0077">(B2) In some embodiments of the method of B1, the method further includes receiving a third readout value for a third pixel of the image captured by the camera, wherein the pixel is in a second row in the several rows and the third pixel is in a third row in the several rows, the third row being different from the second row. The method also includes applying a second correction factor to the third readout value to form a second corrected readout value, wherein the second correction factor is applied to the third readout value to address PRNU of the third pixel, and further wherein the second correction factor is computed based upon an average of the readout values of pixels in the row from amongst the several rows when the uniform light field was applied over the image sensor, wherein the image is generated based further upon the second corrected readout value.</p><p id="p-0098" num="0078">(B3) In some embodiments of any of the methods of B1-B2, the correction factor is further computed based upon a dark signal value for the pixel, wherein the dark signal value is representative of a portion of the readout signal that is not caused by light that is incident upon the pixel.</p><p id="p-0099" num="0079">(B4) In some embodiments of the method of B3, the dark signal value is computed based upon an integration time for the camera.</p><p id="p-0100" num="0080">(B5) In some embodiments of any of the methods of B1-B4, the correction factor is computed based upon: 1) a first value readout from the pixel when the uniform light field was applied over the pixel when the camera captured a first image using a first exposure time; and 2) a second value readout from the pixel when the camera was placed in an environment with no light and captured a second image using a second exposure time.</p><p id="p-0101" num="0081">(B6) In some embodiments of any of the methods of B1-B5, each pixel of the camera has a respective correction factor assigned thereto.</p><p id="p-0102" num="0082">(B7) In some embodiments of any of the methods of B1-B6, an autonomous vehicle comprises the camera and performs a driving maneuver based upon the image generated by the camera.</p><p id="p-0103" num="0083">(B8) In some embodiments of any of the methods of B1-B7, the correction factor is computed based upon a calibration procedure, wherein the calibration procedure includes: a) applying a flat field intensity of zero across the several rows of pixels; b) while the uniform flat field of the intensity of zero is applied. across the several rows of pixels, capturing images with different respective integration times; c) for each of the images captured, reading out respective values of the pixel; and d) linearly fitting the respective values of the pixel to determine a slope, wherein the correction factor is computed based upon the slope.</p><p id="p-0104" num="0084">(B9) In some embodiments of the method of B8, the calibration procedure further comprises: e) applying a uniform flat field intensity across the several rows of pixels, the uniform flat field intensity being non-zero, f) while the uniform flat field intensity is applied across the several rows of pixels, capturing second images with different respective integration times; g) for each of the second. images captured, reading out second respective values of the pixel; h) normalizing the second respective values of the pixel based upon the readout value of the second pixel; and i) non-linearly fitting the second respective values of the pixel to determine a function that defines a curve of the second respective values, wherein the correction factor is computed based upon the function.</p><p id="p-0105" num="0085">(B10) In some embodiments of the method of B9, the second respective values of the pixel are normalized based upon several readout values from pixels in the row.</p><p id="p-0106" num="0086">(B11) In some embodiments of any of the methods of B1-B10, the camera is included in an autonomous vehicle, where the autonomous vehicle includes a lidar system and a computing system, where the computing system detects an object in a scene based upon the image and a point cloud generated by the lidar system, and further wherein the computing system causes the autonomous vehicle to perform the driving maneuver based upon the image and the point cloud.</p><p id="p-0107" num="0087">(C1) In yet another aspect, some embodiments include a camera having a global shutter, where the camera includes an image sensor that includes several rows of pixels, where each row of pixels includes several pixels. The camera further includes processing circuitry that is electrically coupled to the image sensor, where the processing circuitry is configured to perform any of the methods described herein (e.g., any of A1-A8 or B1-B11).</p><p id="p-0108" num="0088">(D1) In still yet another aspect, some embodiments include a computer-readable storage medium comprising instructions that, when executed by processing circuitry of a camera with a global shutter, cause the processing circuitry to perform any of the methods described herein (e.g., any of A1-A8 or B1-B11).</p><p id="p-0109" num="0089">What has been described above includes examples of one or more embodiments. It is, of course, not possible to describe every conceivable modification and alteration of the above devices or methodologies for purposes of describing the aforementioned aspects, but one of ordinary skill in the art can recognize that many further modifications and permutations of various aspects are possible. Accordingly, the described aspects are intended to embrace all such alterations, modifications, and variations that fall within the spirit and scope of the appended claims. Furthermore, to the extent that the term &#x201c;includes&#x201d; is used in either the details description or the claims, such term is intended to be inclusive in a manner similar to the term &#x201c;comprising&#x201d; as &#x201c;comprising&#x201d; is interpreted when employed as a transitional word in a claim.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007193A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.67mm" wi="76.20mm" file="US20230007193A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230007193A1-20230105-M00002.NB"><img id="EMI-M00002" he="55.71mm" wi="76.20mm" file="US20230007193A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A camera having a global shutter, the camera comprising:<claim-text>an image sensor that includes several rows of pixels, where each row of pixels includes several pixels; and</claim-text><claim-text>processing circuitry that is electrically coupled to the image sensor, wherein the processing circuitry is configured to perform acts comprising:<claim-text>receiving a readout value for a pixel in the image sensor;</claim-text><claim-text>applying a correction factor to the readout value to form a corrected readout value, wherein the correction factor is applied to the readout value to address photo response non-uniformity (PRNU) of the pixel, wherein the correction factor is computed based upon a second readout value of a second pixel in a row from amongst the several rows, the second readout value previously obtained when a uniform light field was applied over the image sensor, and further wherein the processing circuitry reads out values from pixels in the row prior to reading out values from pixels in any other row in the several rows; and</claim-text><claim-text>generating an image based upon the corrected readout value.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The camera of <claim-ref idref="CLM-00001">claim 1</claim-ref>, the acts further comprising:<claim-text>receiving a third readout value for a third pixel of the image captured by the camera, wherein the pixel is in a second row in the several rows and the third pixel is in a third row in the several rows, the third row being different from the second row; and</claim-text><claim-text>applying a second correction factor to the third readout value to form a second corrected readout value, wherein the second correction factor is applied to the third readout value to address PRNU of the third pixel, and further wherein the second correction factor is computed based upon an average of the readout values of pixels in the row from amongst the several rows when the uniform light field was applied over the image sensor, wherein the image is generated based further upon the second corrected readout value.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The camera of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the correction factor is further computed based upon a dark signal value for the pixel, wherein the dark signal value is representative of a portion of the readout signal that is not caused by light that is incident upon the pixel.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The camera of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the dark signal value is computed based upon an integration time for the camera.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The camera of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the correction factor is computed based upon:<claim-text>a first value readout from the pixel when the uniform light field was applied over the pixel when the camera captured a first image using a first exposure time; and</claim-text><claim-text>a second value readout from the pixel when the camera was placed in an environment with no light and captured a second image using a second exposure time.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The camera of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each pixel of the camera has a respective correction factor assigned thereto.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The camera of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein an autonomous vehicle comprises the camera and performs a driving maneuver based upon the image generated by the camera.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The camera of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the correction factor is computed based upon a calibration procedure, wherein the calibration procedure comprises:<claim-text>applying a flat field intensity of zero across the several rows of pixels;</claim-text><claim-text>while the uniform flat field of the intensity of zero is applied across the several rows of pixels, capturing images with different respective integration times;</claim-text><claim-text>for each of the images captured, reading out respective values of the pixel; and</claim-text><claim-text>linearly fitting the respective values of the pixel to determine a slope, wherein the correction factor is computed based upon the slope.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The camera of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the calibration procedure further comprises:<claim-text>applying a uniform flat field intensity across the several rows of pixels, the uniform flat field intensity being non-zero;</claim-text><claim-text>while the uniform flat field intensity is applied across the several rows of pixels, capturing second images with different respective integration times;</claim-text><claim-text>for each of the second images captured, reading out second respective values of the pixel;</claim-text><claim-text>normalizing the second respective values of the pixel based upon the readout value of the second pixel; and</claim-text><claim-text>non-linearly fitting the second respective values of the pixel to determine a function that defines a curve of the second respective values, wherein the correction factor is computed based upon the function.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The camera of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the second respective values of the pixel are normalized based upon several readout values from pixels in the row.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The camera of <claim-ref idref="CLM-00001">claim 1</claim-ref> included in an autonomous vehicle (AV), wherein the AV comprises:<claim-text>a lidar system; and</claim-text><claim-text>a computing system, wherein the computing system detects an object in a scene based upon the image and a point cloud generated by the lidar system, and further wherein the computing system causes the AV to perform the driving; maneuver based upon the image and the point cloud.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A method performed by processing circuitry of a camera having a global shutter, wherein the processing circuitry is in communication with an image sensor of the camera, the method comprising:<claim-text>receiving a readout value for a pixel in the image sensor;</claim-text><claim-text>applying a correction factor to the readout value to form a corrected readout value, wherein the correction factor is applied to the readout value to address photo response non-uniformity (PRNU) of the pixel, wherein the correction factor is computed during a calibration process based upon a second readout value of a second pixel in a row from amongst the several rows, the second readout value previously obtained during the calibration process when a uniform light field was applied over the image sensor, and further wherein the processing circuitry reads out, values from pixels in the row prior to reading out values from pixels in any other row in the several rows; and</claim-text><claim-text>generating an image based upon the corrected readout value.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>receiving a third readout value for a third pixel of the image captured by the camera, wherein the pixel is in a second row in the several rows and the third pixel is in a third row in the several rows, the third row being different from the second row; and</claim-text><claim-text>applying a second correction factor to the third readout value to form a second corrected readout value, wherein the second correction factor is applied to the third readout value to address PRNU of the third pixel, and further wherein the second correction factor is computed during the calibration process based upon an average of the readout values of pixels in the row from amongst the several rows when the uniform light field was applied over the image sensor, wherein the image is generated based further upon the second corrected readout value.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the correction factor is further computed based upon a dark signal value for the pixel, wherein the dark signal value is representative of a portion of the readout signal that is not caused by light that is incident upon the pixel.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the dark signal value is computed based upon an integration time for the camera.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the correction factor is computed based upon:<claim-text>a first value readout from the pixel when the uniform light field was applied over the pixel when the camera captured a first image using a first exposure time; and</claim-text><claim-text>a second value readout from the pixel when the camera was placed in an environment with no light and captured a second image using a second exposure time.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein an autonomous vehicle comprises the camera and performs a driving maneuver based upon the image generated by the camera.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the calibration process comprises:<claim-text>applying a uniform flat field intensity of zero across the several rows of pixels;</claim-text><claim-text>while the uniform flat field of the intensity of zero is applied across the several rows of pixels, capturing images with different respective integration times;</claim-text><claim-text>for each of the images captured, reading out respective values of the pixel; and</claim-text><claim-text>linearly fitting the respective values of the pixel to determine a slope, wherein the correction factor is computed based upon the slope.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the calibration process further comprises:<claim-text>applying a second uniform flat field intensity across the several rows of pixels, the second uniform flat field intensity being non-zero;</claim-text><claim-text>while the second uniform flat field intensity is applied across the several rows of pixels, capturing second images with different respective integration times;</claim-text><claim-text>for each of the second images captured, reading out second respective values of the pixel;</claim-text><claim-text>normalizing the second respective values of the pixel based upon the readout value of the second pixel; and</claim-text><claim-text>non-linearly fitting the second respective values of the pixel to determine a function, wherein the correction factor is computed based upon the function.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A computer-readable storage medium comprising instructions that, when executed by processing circuitry of a camera with a global shutter, cause the processing circuitry to perform acts comprising:<claim-text>receiving a readout value for a pixel in an image sensor of the camera;</claim-text><claim-text>applying a correction factor to the readout value to form a corrected readout value, wherein the correction factor is applied to the readout value to address photo response non-uniformity (PRNU) of the pixel, wherein the correction factor is computed based upon a second readout value of a second pixel in a row from amongst the several rows, the second readout value previously obtained when a uniform light field was applied over the image sensor, and further wherein the processing circuitry reads out, values from pixels in the row prior to reading out values from pixels in any other row in the several rows; and</claim-text><claim-text>generating an image based upon the corrected readout value.</claim-text></claim-text></claim></claims></us-patent-application>