<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005075A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005075</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17854329</doc-number><date>20220630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>Q</subclass><main-group>40</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>412</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>416</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20131203</date></cpc-version-indicator><section>G</section><class>06</class><subclass>Q</subclass><main-group>40</main-group><subgroup>12</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>412</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>416</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>041</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">AI-AUGMENTED AUDITING PLATFORM INCLUDING TECHNIQUES FOR AUTOMATED ASSESSMENT OF VOUCHING EVIDENCE</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217119</doc-number><date>20210630</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63217123</doc-number><date>20210630</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63217127</doc-number><date>20210630</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63217131</doc-number><date>20210630</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63217134</doc-number><date>20210630</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>PricewaterhouseCoopers LLP</orgname><address><city>New York</city><state>NY</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LI</last-name><first-name>Chung-Sheng</first-name><address><city>Scarsdale</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>CHENG</last-name><first-name>Winnie</first-name><address><city>West New York</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>FLAVELL</last-name><first-name>Mark John</first-name><address><city>Madison</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>HALLMARK</last-name><first-name>Lori Marie</first-name><address><city>Xenia</city><state>OH</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>LIZOTTE</last-name><first-name>Nancy Alayne</first-name><address><city>Saline</city><state>MI</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>LEONG</last-name><first-name>Kevin Ma</first-name><address><city>Randolph</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>ZHU</last-name><first-name>Di</first-name><address><city>Jersey City</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="07" designation="us-only"><addressbook><last-name>O'ROURKE</last-name><first-name>Kevin Michael</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="08" designation="us-only"><addressbook><last-name>KWON</last-name><first-name>Eun Kyung</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="09" designation="us-only"><addressbook><last-name>NARULA</last-name><first-name>Vandit</first-name><address><city>Monroe Township</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="10" designation="us-only"><addressbook><last-name>CHEN</last-name><first-name>Weichao</first-name><address><city>Secaucus</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="11" designation="us-only"><addressbook><last-name>RAMIREZ</last-name><first-name>Maria Jesus Perez</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>PricewaterhouseCoopers LLP</orgname><role>02</role><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for determining whether an electronic document constitutes vouching evidence is provided. The system may receive ERP item data and generate hypothesis data based thereon, and may receive electronic document data and extract ERP information therefrom. The system may then apply one or more models to compare the hypothesis data to the extracted ERP information to determine whether the electronic document constitutes vouching evidence for the ERP item. Systems and methods for verifying an assertion against a source document are provided. The system may receive first data indicating an unverified assertion and second data comprising a plurality of source documents. The system may apply one or more extraction models to extract a set of key data from the plurality of source documents and may apply one or more matching models to compare the first data to the set of key data to determine whether vouching criteria are met.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="158.33mm" wi="158.75mm" file="US20230005075A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="216.32mm" wi="153.59mm" orientation="landscape" file="US20230005075A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="237.24mm" wi="162.64mm" orientation="landscape" file="US20230005075A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="172.04mm" wi="162.73mm" file="US20230005075A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="233.17mm" wi="163.32mm" file="US20230005075A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="230.55mm" wi="159.60mm" orientation="landscape" file="US20230005075A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="211.24mm" wi="174.92mm" orientation="landscape" file="US20230005075A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="199.64mm" wi="101.52mm" orientation="landscape" file="US20230005075A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="123.87mm" wi="143.34mm" orientation="landscape" file="US20230005075A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Application No. 63/217,119 filed Jun. 30, 2021; U.S. Provisional Application No. 63/217,123 filed Jun. 30, 2021; U.S. Provisional Application No. 63/217,127 filed Jun. 30, 2021; U.S. Provisional Application No. 63/217,131 filed Jun. 30, 2021; and U.S. Provisional Application No. 63/217,134, filed Jun. 30, 2021, the entire contents of each of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">This relates generally to automated data processing and validation of data, and more specifically to AI-augmented auditing platforms including techniques for assessment of vouching evidence.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">When performing audits, or when otherwise ingesting, reviewing, and analyzing documents or other data, there is often a need to establish that one or more statements, assertions, or other representations of fact are sufficiently substantiated by documentary evidence. In the context of performing audits, establishing that one or more statements (e.g., a financial statement line item (FSLI)) is sufficiently supported by documentary evidence is referred to as vouching.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">When performing audits, or when otherwise ingesting, reviewing, and analyzing documents or other data, there is often a need to establish that one or more statements, assertions, or other representations of fact are sufficiently substantiated by documentary evidence. In the context of performing audits, establishing that one or more statements (e.g., a financial statement line item (FSLI)) is sufficiently supported by documentary evidence is referred to as vouching.</p><p id="p-0006" num="0005">In automated auditing systems that seek to ingest and understand documentary evidence in order to vouch for one or more statements (e.g., FSLI's), known document-understanding techniques are sensitive to the structure of the documents that are ingested and analyzed. Accordingly, known document-understanding techniques may fail to correctly recognize and identify certain entities referenced in documents, due for example to a misinterpretation of the structure or layout of one or more ingested documents. Accordingly, there is a need for improved document-understanding (e.g., document ingestion and analysis) techniques that are more robust to various document structures and layouts and that provide higher accuracy for entity recognition in documents. There is a need for such improved document-understanding techniques configured to be able to be applied in automated auditing systems in order to determine whether one or more documents constitutes sufficient vouching evidence to substantiate one or more assertions (e.g., FSLI's).</p><p id="p-0007" num="0006">Disclosed herein are improved document-understanding techniques that may address one or more of the above-identified needs. In some embodiments, as explained herein, the document-understanding techniques disclosed herein may leverage a priori knowledge (e.g., information available from a data source separate from the document(s) being assessed for sufficiency for vouching purposes) of one or more entities in extracting and/or analyzing information from one or more documents. In some embodiments, the document-understanding techniques may analyze the spatial configuration of words, paragraphs, or other content in a document in extracting and/or analyzing information from one or more documents.</p><p id="p-0008" num="0007">Furthermore, pursuant to the need to perform automated vouching, there is a need for improved systems and methods for vouching ERP entries against bank statement data in order to verify payment.</p><p id="p-0009" num="0008">In some embodiments, a system is configured vouch payment data against evidence data. More specifically, a system may be configured to provide a framework that performs ERP payment activities vouching against physical bank statement. The system may include a pipeline that perform information extraction and characteristics extraction from bank statements, and the system may leverage one or more advanced data structures and matching algorithms to perform one-to-many matching between ERP data and bank statement data. The payment vouching systems provided herein may thus automate the process of finding material evidence such as remittance advice or bank statements to corroborate ERP payment entries.</p><p id="p-0010" num="0009">In some embodiments, a first system is provided, the first system being for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, the first system comprising one or more processors configured to cause the first system to: receive data representing an ERP item; generate hypothesis data based on the received data represent an ERP item; receive an electronic document; extract ERP information from the document; apply one or more models to the hypothesis data and to extracted ERP information in order to generate output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</p><p id="p-0011" num="0010">In some embodiments of the first system, extracting the instance of ERP information comprises generating first data representing information content of the instance of ERP information and second data representing a document location for the instance of ERP information</p><p id="p-0012" num="0011">In some embodiments of the first system, the ERP information comprises one or more of: a purchase order number, a customer name, a date, a delivery term, a shipping term, a unit price, and a quantity.</p><p id="p-0013" num="0012">In some embodiments of the first system, applying the one or more models to generate output data is based on preexisting information regarding spatial relationships amongst instances of ERP information in documents.</p><p id="p-0014" num="0013">In some embodiments of the first system, the preexisting information comprises a graph representing spatial relationships amongst instances of ERP information in documents.</p><p id="p-0015" num="0014">In some embodiments of the first system, the one or more processors are configured to cause the system to augment the hypothesis data based on one or more models representing contextual data.</p><p id="p-0016" num="0015">In some embodiments of the first system, the contextual data comprises information regarding one or more synonyms for the information content of the instance of ERP information.</p><p id="p-0017" num="0016">In some embodiments of the first system, the instance of ERP information comprises a single word in the document.</p><p id="p-0018" num="0017">In some embodiments of the first system, the instance of ERP information comprises a plurality of words in the document.</p><p id="p-0019" num="0018">In some embodiments of the first system, the one or more processors are configured to determine whether the ERP information vouches for the ERP item.</p><p id="p-0020" num="0019">In some embodiments of the first system, determining whether the ERP information vouches for the ERP item comprises generating and evaluating a similarity score representing a comparison of the ERP information and the ERP item.</p><p id="p-0021" num="0020">In some embodiments of the first system, the similarity generated by comparing an entity graph associated with the ERP information to an entity graph associated with the ERP item.</p><p id="p-0022" num="0021">In some embodiments of the first system, extracting the ERP information from the document comprises applying a fingerprinting operation to determine, based on the receive data representing an ERP item, a characteristic of a data extraction operation to be applied to the electronic document.</p><p id="p-0023" num="0022">In some embodiments, a first non-transitory computer-readable storage medium is provided, the first non-transitory computer-readable storage medium storing instructions for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, the instructions configured to be executed by a system comprising one or more processors to cause the system to: receive data representing an ERP item; generate hypothesis data based on the received data represent an ERP item; receive an electronic document; extract ERP information from the document; apply one or more models to the hypothesis data and to extracted ERP information in order to generate output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</p><p id="p-0024" num="0023">In some embodiments, a first method is provided, the first method being for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, wherein the first method is performed by a system comprising one or more processors, the first method comprising: receiving data representing an ERP item; generating hypothesis data based on the received data represent an ERP item; receiving an electronic document; extracting ERP information from the document; applying one or more models to the hypothesis data and to extracted ERP information in order to generate output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</p><p id="p-0025" num="0024">In some embodiments, a second system is provided, the second system being for verifying an assertion against a source document, the second system comprising one or processors configured to cause the second system to: receive first data indicating an unverified assertion; receive second data comprising a plurality of source documents; apply one or more extraction models to extract a set of key data from the plurality of source documents; and apply one or more matching models to compare the first data to the set of key data to generate an output indicating whether one or more of the plurality of source documents satisfies one or more verification criteria for verifying the unverified assertion.</p><p id="p-0026" num="0025">In some embodiments of the second system, the one or more extraction models comprise one or more machine learning models.</p><p id="p-0027" num="0026">In some embodiments of the second system, the one or more matching models comprises one or more approximation models.</p><p id="p-0028" num="0027">In some embodiments of the second system, the one or more matching models are configured to perform one-to-many matching between the first data and the set of key data.</p><p id="p-0029" num="0028">In some embodiments of the second system, the one of more processors are configured to cause the system to modify one or more of the extraction models without modification of one or more of the matching models.</p><p id="p-0030" num="0029">In some embodiments of the second system, the one of more processors are configured to cause the system to modify one or more of the matching models without modification of one or more of the extraction models.</p><p id="p-0031" num="0030">In some embodiments of the second system, the unverified assertion comprises an ERP payment entry.</p><p id="p-0032" num="0031">In some embodiments of the second system, the plurality of source documents comprises a bank statement.</p><p id="p-0033" num="0032">In some embodiments of the second system, applying one or more matching models comprises generating a match score and generating a confidence score.</p><p id="p-0034" num="0033">In some embodiments of the second system, applying one or more matching models comprises: applying a first matching model; if a match is indicated by the first matching model, generating a match score and a confidence score based on the first matching model; if a match is not indicated by the second matching model: applying a second matching model; if a match is indicated by the second matching model, generating a match score and a confidence score based on the second matching mode; and if a match is not indicated by the second matching model, generating a match score of 0.</p><p id="p-0035" num="0034">In some embodiments, a second non-transitory computer-readable storage medium is provided, the second non-transitory computer-readable storage medium storing instructions for verifying an assertion against a source document, the instructions configured to be executed by a system comprising one or processors to cause the system to: receive first data indicating an unverified assertion; receive second data comprising a plurality of source documents; apply one or more extraction models to extract a set of key data from the plurality of source documents; and apply one or more matching models to compare the first data to the set of key data to generate an output indicating whether one or more of the plurality of source documents satisfies one or more verification criteria for verifying the unverified assertion.</p><p id="p-0036" num="0035">In some embodiments, a second method is provided, the second method being for verifying an assertion against a source document, wherein the second method is executed by a system comprising one or processors, the second method comprising: receiving first data indicating an unverified assertion; receiving second data comprising a plurality of source documents; applying one or more extraction models to extract a set of key data from the plurality of source documents; and applying one or more matching models to compare the first data to the set of key data to generate an output indicating whether one or more of the plurality of source documents satisfies one or more verification criteria for verifying the unverified assertion.</p><p id="p-0037" num="0036">In some embodiments, a third system, for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, is provided, the third system comprising one or more processors configured to cause the third system to: receive data representing an ERP item; generate hypothesis data based on the received data represent an ERP item; receive an electronic document; extract ERP information from the document; apply a first set of one or more models to the hypothesis data and to extracted ERP information in order to generate first output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; apply a second set of one or more models to the extracted ERP information in order to generate second output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; generate combined determination data, based on the first output data and the second output data, indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</p><p id="p-0038" num="0037">In some embodiments, a third non-transitory computer-readable storage medium is provided, the third non-transitory computer-readable storage medium storing instructions for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, the instructions configured to be executed by a system comprising one or more processors to cause the system to: receive data representing an ERP item; generate hypothesis data based on the received data represent an ERP item; receive an electronic document; extract ERP information from the document; apply a first set of one or more models to the hypothesis data and to extracted ERP information in order to generate first output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; apply a second set of one or more models to the extracted ERP information in order to generate second output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; generate combined determination data, based on the first output data and the second output data, indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</p><p id="p-0039" num="0038">In some embodiments, a third method, for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, is provided, wherein the third method is performed by a system comprising one or more processors, the third method comprising: receiving data representing an ERP item; generating hypothesis data based on the received data represent an ERP item; receiving an electronic document; extracting ERP information from the document; applying a first set of one or more models to the hypothesis data and to extracted ERP information in order to generate first output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; applying a second set of one or more models to the extracted ERP information in order to generate second output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; generating combined determination data, based on the first output data and the second output data, indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</p><p id="p-0040" num="0039">In some embodiments, any one or more of the features, characteristics, or aspects of any one or more of the above systems, methods, or non-transitory computer-readable storage media may be combined, in whole or in part, with one another and/or with any one or more of the features, characteristics, or aspects (in whole or in part) of any other embodiment or disclosure herein.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading><p id="p-0041" num="0040">Various embodiments are described with reference to the accompanying figures, in which:</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows two examples of extracting entities from documents, in accordance with some embodiments.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a system for data processing for an AI-augmented auditing platform, in accordance with some embodiments.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref> depict a diagram of how a fingerprinting algorithm may be used as part of a process to render a decision about whether purchase order is vouched, in accordance with some embodiments.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a diagram of a fingerprinting algorithm, document-understanding, and vouching algorithm, in accordance with some embodiments.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> show a diagram of a payment vouching method, in accordance with some embodiments.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example of a computer, according to some embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0048" num="0047">Active Document Comprehension for Assurance</p><p id="p-0049" num="0048">When performing audits, or when otherwise ingesting, reviewing, and analyzing documents or other data, there is often a need to establish that one or more statements, assertions, or other representations of fact are sufficiently substantiated by documentary evidence. In the context of performing audits, establishing that one or more statements (e.g., a financial statement line item (FSLI)) is sufficiently supported by documentary evidence is referred to as vouching.</p><p id="p-0050" num="0049">In automated auditing systems that seek to ingest and understand documentary evidence in order to vouch for one or more statements (e.g., FSLI's), known document-understanding techniques are sensitive to the structure of the documents that are ingested and analyzed. Accordingly, known document-understanding techniques may fail to correctly recognize and identify certain entities referenced in documents, due for example to a misinterpretation of the structure or layout of one or more ingested documents. Accordingly, there is a need for improved document-understanding (e.g., document ingestion and analysis) techniques that are more robust to various document structures and layouts and that provide higher accuracy for entity recognition in documents. There is a need for such improved document-understanding techniques configured to be able to be applied in automated auditing systems in order to determine whether one or more documents constitutes sufficient vouching evidence to substantiate one or more assertions (e.g., FSLI's).</p><p id="p-0051" num="0050">Disclosed herein are improved document-understanding techniques that may address one or more of the above-identified needs. In some embodiments, as explained herein, the document-understanding techniques disclosed herein may leverage a priori knowledge (e.g., information available from a data source separate from the document(s) being assessed for sufficiency for vouching purposes) of one or more entities in extracting and/or analyzing information from one or more documents. In some embodiments, the document-understanding techniques may analyze the spatial configuration of words, paragraphs, or other content in a document in extracting and/or analyzing information from one or more documents.</p><p id="p-0052" num="0051">In some embodiments, a document-understanding system is configured to perform automated hypothesis generation based on one or more data sets. The data sets on which hypothesis generation is based may include one or more sets of ingested documents, for example documents ingested in accordance with one or more document-understanding techniques described herein. In some embodiments, the data sets on which hypothesis generation is based may include enterprise resource planning (ERP) data. In some embodiments, the data (e.g., ERP data) may indicate one or more entities, for example a PO #, a customer name, a date, a delivery term, a shipping term, a unit price, and/or a quantity. The system may be configured to apply a priori knowledge (e.g., information available from a data source separate from the document(s) being assessed for sufficiency for vouching purposes) regarding one or more of the entities indicated in the data. The hypothesis generation techniques disclosed herein may enable more accurate vouching of ERP data with evidence from unstructured documents and other evidence sources.</p><p id="p-0053" num="0052">The system may be configured to analyze spatial relationships and constellation among entities indicated in the data. For example, the position at which entities are indicated in a document (e.g., a unit price and a quantity indicated on a same line of a document versus on a different line of a document) may be analyzed. In some embodiments, the system may be configured to generate, store, and/or analyze a data structure, such as a graph data structure, that represents spatial relationships amongst a plurality of entities in one or more documents.</p><p id="p-0054" num="0053">The system may be configured to apply one or more AI models to comprehend documents to identify and assess evidence to vouch for the validity of financial information reported in ERPs. The system may use the ERP data to weakly label and provide hypotheses to documents that are candidates for possible evidence. The system may further apply one or more name entity extraction models to provide additional bias-free information to overlay on top of these documents. The combination of these features may enable the system to validate whether candidate evidence is indeed vouching evidence (e.g., whether it meets vouching criteria) for a given ERP entry, including by providing a quantification/score of the system's confidence in the conclusion that the candidate evidence does or does not constitute vouching evidence.</p><p id="p-0055" num="0054">In some embodiments, the system may be configured to receive ERP data and to apply one or more data processing operations (e.g., AI models) to the received data in order to generate hypothesis data. (Any data processing operation referenced herein may include application of one or more models trained by machine-learning.) The hypothesis data may consist of one or more content entities that the system hypothesizes to be indicated in the received data, for example: PO #, customer name, date, delivery term, shipping term, unit price, and/or quantity. The system may assess one or more of the following in generating hypothesis data and/or in assessing hypothesis data once it is generated: a priori knowledge (e.g., knowledge from one or more data sources aside from the ERP data source); spatial relationships amongst words, paragraphs, or other indications of entities within the ERP data (e.g., spatial relationships of words within a document), and/or constellations amongst entities (e.g., unit price &#x26; quantity appearing on the same line).</p><p id="p-0056" num="0055">Following hypothesis generation, the system may apply one or more data processing operations (e.g., AI models) in order to augment one or more of the generated hypotheses. In some embodiments, the system may augment (or otherwise modify) a generated hypothesis on the basis of context data available to the system. In some embodiments, context data may include synonym data, such that the system may augment a hypothesis in accordance with synonym data. For example, hypothesis data that includes the word &#x201c;IBM&#x201d; may be augmented to additionally include the term &#x201c;International Business Machines&#x201d;.</p><p id="p-0057" num="0056">The system may be configured to perform spatial entity extraction. In some embodiments, spatial entity extraction includes extracting entities (at the word-level and at the multi-word level) from a document to generate information regarding (a) the entity content/identity and (b) information regarding a spatial location of the entity (e.g., an absolute spatial location within a document and/or a spatial location/proximity/alignment/orientation with respect to one or more other entities within the document).</p><p id="p-0058" num="0057">The system may be configured to perform one or more hypothesis testing operations in order to evaluate the likelihood of a match, for example based on calculating a similarity score. The likelihood of a match may be evaluated between ERP data on one hand and a plurality of documents on the other hand. In some embodiments, the likelihood of a match may be based on calculating a similarity score between the entity (or entities) representing the hypothesis and the entity (or entity graph) representing components within the documents.</p><p id="p-0059" num="0058">The systems and methods provided herein may provide improvements over existing approaches, including by providing the ability to use contextual information guided by an audit process to aid in comprehension, to use contextual information to form hypotheses on the expected information to be extracted from documents, to allow the testing of these hypotheses to guide document comprehension, and/or to apply methods to mitigate and account for the possibility of biases introduced by contextual information (e.g., by adjusting a confidence score accordingly).</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts two examples of extracting entities from documents, in accordance with some embodiments.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a system <b>200</b> for data processing for an AI-augmented auditing platform, in accordance with some embodiments. The components labeled &#x201c;hypothesis generation&#x201d; and &#x201c;active vouching&#x201d; may, in some embodiments, include any one or more of the systems (and/or may apply any one or more of the methods) described herein.</p><p id="p-0062" num="0061">In some embodiments each of the schematic blocks shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may represent a distinct module (e.g., each distinct module comprising one or more distinct computer systems including storage devices and/or one or more physical and/or virtual processors) configured to perform associated functionality. In some embodiments, any one or more of the schematic blocks shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may represent functionalities performed by a same module (e.g., by a same computer system).</p><p id="p-0063" num="0062">As described below, system <b>200</b> may be configured to perform any one or processes for active vouching; passive vouching and tracing; and/or data integrity integration, for example as described herein.</p><p id="p-0064" num="0063">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, system <b>100</b> may include documents source <b>202</b>, which may include any one or more computer storage devices such as databases, data stores, data repositories, live data feeds, or the like. Documents source <b>202</b> may be communicatively coupled to one or more other components of system <b>200</b> and configured to provide a plurality of document to system <b>200</b>, such that the documents can be assessed to determine whether one or more data integrity criteria are met, e.g., whether the documents sufficiently vouch for one or more representations made by a set of ERP data. In some embodiments, system <b>200</b> may receive documents from documents source <b>202</b> on a scheduled basis, in response to a user input, in response to one or more trigger conditions being met, and/or in response to the documents being manually sent. Documents received from documents source <b>202</b> may be provided in any suitable electronic data format, for example as structured, unstructured, and/or semi-structured data. The documents may include, for example, spreadsheets, word processing documents, and/or PDFs.</p><p id="p-0065" num="0064">System <b>200</b> may include OCR module <b>204</b>, which may include any one or more processors configured to perform OCR analysis and/or any other text or character recognition/extraction based on documents received from documents source <b>202</b>. OCR module <b>204</b> may generate data representing characters recognized in the received documents.</p><p id="p-0066" num="0065">System <b>200</b> may include document classification module <b>206</b>, which may include one or more processors configured to perform document classification of documents received from documents source <b>202</b> and/or from OCR module <b>204</b>. Document classification module <b>206</b> may receive document data from documents source <b>202</b> and/or may receive data representing characters in documents from OCR module <b>204</b>, and may apply one or more classification algorithms to the received data to apply one or more classifications to the documents received from documents source <b>202</b>. Data representing the determined classifications may be stored as metadata in association with the documents themselves and/or may be used to store the documents in a manner according to their determined respective classification(s).</p><p id="p-0067" num="0066">System <b>200</b> may include ERP data source <b>208</b>, which may include any one or more computer storage devices such as databases, data stores, data repositories, live data feeds, or the like. Documents source <b>202</b> may be communicatively coupled to one or more other components of system <b>200</b> and configured to provide ERP data to system <b>200</b>, such that the ERP data can be assessed to determine whether one or more data integrity criteria are met, e.g., whether the ERP data is sufficiently vouched by one or more documents (e.g., the documents provided by documents source <b>202</b>). In some embodiments, one or more components of system <b>200</b> may receive ERP data from ERP data source <b>208</b> on a scheduled basis, in response to a user input, in response to one or more trigger conditions being met, and/or in response to the data being manually sent. ERP data received from ERP data source <b>208</b> may be provided in any suitable electronic data format. In some embodiments, ERP data may be provided in a tabular data format, including a data model that defines the structure of the data.</p><p id="p-0068" num="0067">System <b>200</b> may include knowledge substrate <b>210</b>, which may include any one or more data sources such as master data source <b>210</b><i>a</i>, ontology data source <b>210</b><i>b</i>, and exogenous knowledge data source <b>210</b><i>c</i>. The data sources included in knowledge substrate <b>210</b> may be provided as part of a single computer system, multiple computer systems, a single network, or multiple networks. The data sources included in knowledge substrate <b>210</b> may be configured to provide data to one or more components of system <b>200</b> (e.g., hypothesis generation module <b>212</b>, normalization and contextualization module <b>222</b>, and/or passive vouching and tracing module <b>224</b>). In some embodiments, one or more components of system <b>200</b> may receive data from knowledge substrate <b>210</b> on a scheduled basis, in response to a user input, in response to one or more trigger conditions being met, and/or in response to the data being manually sent. Data received from knowledge substrate <b>210</b> may be provided in any suitable data format.</p><p id="p-0069" num="0068">In some embodiments, interaction with knowledge substrate <b>210</b> may be query based. Interaction with knowledge substrate <b>210</b> may be in one or more of the following forms: question answering, information retrieval, query into knowledge graph engine, and/or inferencing engine (e.g., against inferencing rules).</p><p id="p-0070" num="0069">Knowledge substrate <b>210</b> may include data such as ontology/taxonomy data, knowledge graph data, and/or inferencing rules data. Master data received from master data source <b>210</b><i>a </i>may include, for example, master customer data, master vendor data, and/or master product data. Ontology data received from ontology data source <b>210</b><i>b </i>may include, for example, IncoTerms data for international commercial terms that define the cost, liability, and/or insurance among the sell side, buy side, and shipper for shipping a product. Exogenous knowledge data source received from exogenous knowledge data source <b>210</b><i>c </i>may include, for example, knowledge external to a specific audit client. This knowledge could be related to the industry of the client, the geographic area of a client, and/or the entire economy.</p><p id="p-0071" num="0070">System <b>200</b> may include hypothesis generation module <b>212</b>, which may include one or more processors configured to generate hypothesis data. Hypothesis generation module <b>212</b> may receive input data from any one or more of: (a) document classification module <b>206</b>, (b) ERP data source <b>208</b>, and (c) knowledge substrate <b>210</b>. Hypothesis generation module <b>212</b> may apply one or more hypothesis generation algorithms to some or all of the received data and may thereby generate hypothesis data. Hypothesis generation may be based on any one of, and/or a combination of: (1) ERP data, (2) document type data, (3) data regarding prior understanding of one or more documents. A generated hypothesis may represent where and what is expected to be found in documents data, based on previous exposure to similar documents. Document classification data (e.g., from document classification module <b>206</b>), for one document and/or for a group of documents, maybe used to determine, augment, and/or weight hypothesis data generated by hypothesis generation module <b>212</b>. In some embodiments, document content itself (e.g., document data received from documents source <b>202</b>), as distinct from document classification data (e.g., as generated by document classification module <b>206</b>) may not be used for hypothesis generation. In some embodiments, document content itself may be used, in addition to document classification data, for hypothesis generation. The hypothesis data generated by hypothesis generation module <b>212</b> may be provided in any suitable data format. In some embodiments, hypothesis data in the context of document understanding may be represented as sets of tuples (e.g., representing entity, location, and value), each of which represent what is expected to be found from the documents data.</p><p id="p-0072" num="0071">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, system <b>200</b> may provide for an &#x201c;active vouching&#x201d; pipeline and for a &#x201c;passive vouching&#x201d; pipeline that may each be applied, using some or all of the same underlying data, in parallel to one another. The two pipelines may be applied at the same time or one after the other. Below, the active vouching pipeline is described with respect to element <b>214</b>, while the passive vouching pipeline is described with respect to elements <b>216</b>-<b>224</b>.</p><p id="p-0073" num="0072">System <b>200</b> may include active vouching module <b>214</b>, which may include one or more processors configured to apply any one or more active vouching analysis operations. Active vouching module <b>214</b> may receive input data from one or more of: OCR module <b>204</b>, document classification module <b>206</b>, and hypothesis generation module <b>212</b>. Active vouching module <b>214</b> may apply one or more active vouching analysis operations to some or all of the received data and may thereby generate active vouching output data. In some embodiments, an active vouching analysis operation may include a &#x201c;fingerprinting&#x201d; analysis operation. In some embodiments, active vouching or fingerprinting may include data processing operations configured to determine whether there exist one (or more) tuples (e.g., representing entity, location, and value) extracted from documents data that can match hypothesis data. Some embodiments of a fingerprinting analysis operation are described below with respect to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>. In some embodiments, the active vouching output data generated by active vouching module <b>214</b> may be provided in any suitable data format. In some embodiments, the active vouching output may include data indicating one or more of the following: a confidence score indicating a confidence level as to whether there is a match (e.g., whether vouching criteria are met, whether there is a match for a hypothesis); a binary indication as to whether there is any match for a hypothesis, which may feedback iteratively into the fingerprinting process; and/or a location within a document corresponding to a hypothesis for which a confidence and/or a binary indication are generated. In some embodiments, the active vouching output may include four values: an entity name, an entity value, a location (indicating an exact or relative location of the entity), and a confidence value indicating a confidence value of the determined match.</p><p id="p-0074" num="0073">In some embodiments, the active vouching operations performed by module <b>214</b> may leverage contextual knowledge to inform what information is sought in an underlying document. In some embodiments, the active vouching operations performed by module <b>214</b> may be considered &#x201c;context aware&#x201d; because they are able to draw on contextual information that is injected via hypothesis generation module <b>212</b> drawing on data received from knowledge substrate <b>210</b>.</p><p id="p-0075" num="0074">In some embodiments, the active vouching operations may include one or more deductive reasoning operations, which may include application of one or more rules-based approaches to evaluate document information (e.g., information received from OCR module <b>204</b>). For example, a rules based approach may be used to determine that, if a document is a certain document type, then the document will be known to include certain associated data fields. In some embodiments, the deductive reasoning operation(s) may be used to calculate and/or adjust an overall weighting. In some embodiments, weighting may be used in integrating results from multiple approaches (e.g., an inductive approach and a deductive approach). A weighting may be trained using various machine learning methods.</p><p id="p-0076" num="0075">In some embodiments, the active vouching operations may include one or more inductive reasoning operations that may be based on a previous calculation or determination, historical information, or one or more additional insights. In some embodiments, inductive reasoning operations may based on learning from previous instances of similar data (e.g., sample documents) to determine what may be expected from future data.</p><p id="p-0077" num="0076">In some embodiments, active vouching module <b>214</b> may apply context awareness, deductive reasoning, and inductive reasoning together for hypothesis testing.</p><p id="p-0078" num="0077">Turning now to the passive vouching pipeline (elements <b>216</b>-<b>224</b>), system <b>200</b> may include three parallel pipelines within the passive vouching pipeline, as represented by template-based pipeline <b>216</b>, templateless pipeline <b>218</b>, and specialized pipeline <b>220</b>. Each of pipelines <b>216</b>-<b>220</b> may comprise one or more processors configured to receive input data from OCR module <b>204</b> and/or from document classification module <b>206</b> and to process the received input data. Each of the pipelines <b>216</b>-<b>220</b> may apply respective data analysis operations to the received input data and may generate respective output data.</p><p id="p-0079" num="0078">Template-based pipeline <b>216</b> may be configured to apply any one or more template-based analysis operations to the received document data and/or document classification data and to generate output data representing document contents, such as one or more tuples representing entity, location, and value for content extracted from the document. Template-based pipeline <b>216</b> may be configured to apply one or more document understanding models that are trained for a specific known format. Abbyy Flexicapture is an example of such template-based tool.</p><p id="p-0080" num="0079">Templateless pipeline <b>218</b> may be configured to apply any one or more analysis operations to the received document data and/or document classification data and to generate output data representing document contents, such as one or more tuples representing entity, location, and value for content extracted from the document. Templateless pipeline <b>218</b> may be configured which to operate without any assumption that documents being analyzed have a presumed &#x201c;template&#x201d; for document understanding. In some embodiments, a templateless approach may be less accurate than a template-based tool, and may require more training against a larger training set as compared to a template-based tool.</p><p id="p-0081" num="0080">Specialized pipeline <b>220</b> may be configured to apply any one or more analysis operations to the received document data and/or document classification data and to generate output data representing document contents. In some embodiments, specialized pipeline <b>220</b> may be configured to apply a signature analysis. In some embodiments, signature analysis may include signature detection, for example using a machine-learning algorithm configured to determine whether or not a signature is present. In some embodiments, additionally or alternatively to signature detection, signature analysis may include signature matching, for example using one or more data processing operations to determine a person whose signature matches a detected signature (for example by leveraging comparison to a library of known signatures).</p><p id="p-0082" num="0081">In some embodiments, specialized pipeline <b>220</b> may be used when system <b>200</b> has access to outside information, such as information in addition to information from documents source <b>202</b> and from ERP data source <b>208</b>. For example, specialized pipeline may be configured to use information from knowledge substrate <b>210</b> in analyzing the received data and generating output data.</p><p id="p-0083" num="0082">In some embodiments, pipeline <b>220</b> may be configured to extract data from documents that includes additional data (or data in a different format) as compared to data that is extracted by pipelines <b>216</b> and <b>218</b>. For example, pipeline <b>220</b> may extract data other than (or in addition to) a tuple representing entity, location, and value). The extracted data may include logo data, signature data (e.g., an image or other representation of the signature, an indication as to whether there is a signature, etc.), figures, drawings, or the like. For an extracted logo, output data may include the logo itself (e.g., an image or other representation of the signature), a location within the document, and/or a customer name matched to the logo. For an extracted signature, output data may include the signature itself (e.g., an image or other representation of the signature), a location within the document, and/or a customer name matched to the signature. For extracted handwriting, output data may include the handwriting itself (e.g., an image or other representation of the handwriting), a location within the document, a customer name matched to the handwriting, and/or text extracted from the handwriting. For an extracted figure, output data may include the figure itself (e.g., an image or other representation of the figure), a location within the document, and/or a bounding box for the figure.</p><p id="p-0084" num="0083">System <b>200</b> may include normalization and contextualization module <b>222</b>, which may include one or more processors configured to perform one or more data normalization and/or contextualization operations. Normalization and contextualization module <b>222</b> may receive input data from any one or more of: (a) template-based pipeline <b>216</b>, (b) templateless pipeline <b>218</b>, (c) specialized pipeline <b>220</b>; and knowledge substrate <b>210</b>. Normalization and contextualization module <b>222</b> may apply one or more normalization and contextualization operations to some or all of the received data and may thereby generate normalized and/or contextualized output data.</p><p id="p-0085" num="0084">A normalization and contextualization data processing operation may determine context of an entity and/or may normalize an entity value so that it can be used for subsequent comparison or classification. Examples include (but are not limited to) the following: normalization of customer name data (such as alias, abbreviations, and potentially including parent/sibling/subsidiary when the name is used in the context of payment) based on master customer/vendor data; normalization of address data (e.g., based on geocoding, based on standardized addresses from a postal office, and/or based on customer/vendor data); normalization of product name and SKU based on master product data; normalization of shipping and payment terms based on terms (e.g., based on International Commerce Terms); and/or normalization of currency exchange code (e.g., based on ISO 4217).</p><p id="p-0086" num="0085">The normalized and/or contextualized output data generated by normalization and contextualization module <b>222</b> may be provided in any suitable data format, for example as a set of tuples representing entity, entity location, normalized entity value, and confidence score.</p><p id="p-0087" num="0086">System <b>200</b> may include passive vouching and tracing module <b>224</b>, which may include one or more processors configured to perform one or more passive vouching and tracing operations. Passive vouching and tracing module <b>224</b> may receive input data from any one or more of: (a) normalization and contextualization module <b>222</b>, (b) knowledge substrate <b>210</b>, and (c) ERP data source <b>208</b>. Passive vouching and tracing module <b>224</b> may apply one or more passive vouching and/or tracing operations to some or all of the received data and may thereby generate passive vouching and tracing output data. Passive vouching may comprise comparing values from a given transaction record (e.g., as represented in ERP data) with entity values extracted from documents data (which may be assumed to be the evidence that is associated with the transaction record). Passive tracing may comprise comparing values from a given document with a corresponding transaction record, e.g., from in the ERP. Comparison of entity values may be precise, such that the generated result indicates either a match or a mismatch, or the comparison may be fuzzy, such that the generated result comprises a similarity score.</p><p id="p-0088" num="0087">The passive vouching and tracing output data generated by passive vouching and tracing module <b>224</b> may be provided in any suitable data format. The passive vouching and tracing operations performed by module <b>224</b> may be considered &#x201c;context aware&#x201d; because they are able to draw on contextual information received from knowledge substrate <b>210</b>. In some embodiments, the passive vouching output may include four values: an entity name, an entity value, a location (indicating an exact or relative location of the entity), and a confidence value indicating a confidence value of the determined match.</p><p id="p-0089" num="0088">Downstream of both the active vouching pipeline and the passive vouching pipeline, system <b>200</b> may be configured to combine the results of the active vouching and the passive vouching pipelines in order to generate a combined result.</p><p id="p-0090" num="0089">System <b>200</b> may include data integrity integration module <b>226</b>, which may include one or more processors configured to perform one or more data integrity integration operations. Data integrity integration module <b>226</b> may receive input data from any one or more of: (a) active vouching module <b>214</b> and (b) passive vouching and tracing module <b>224</b>. Data integrity integration module <b>226</b> may apply one or more data integrity integration operations to some or all of the received data and may thereby data integrity integration output data. The data integrity integration output data generated by data integrity integration module <b>226</b> may be provided in any suitable data format, and may for example include a combined confidence score indicating a confidence level (e.g., a percentage confidence) by which system <b>200</b> has determined that the underlying documents vouch for the ERP information. In some embodiments, the data integrity integration output data may comprise a set of tuples&#x2014;e.g., representing entity, match score, and confidence&#x2014;for each of the entities that have been analyzed. A decision (e.g., a preliminary decision) on whether the evidence is considered to support the existence and accuracy of a record (e.g., an ERP record) may be rendered as part of the data integrity integration output data.</p><p id="p-0091" num="0090">In some embodiments, the one or more data integrity integration operations applied by module <b>226</b> may process the input data from active vouching module <b>214</b> and passive vouching module <b>224</b> in accordance with one of the following four scenarios:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0091">Scenario 1&#x2014;in embodiments in which active vouching module <b>214</b> and passive vouching module <b>224</b> each confirm an entity, the two confidence values associated with the two vouching methods may be combined with one another (e.g., through averaging and/or through a multiplication operation), including optionally by being used to boost one another, to generate an overall confidence level, or the higher of the two confidence levels may be chosen as the overall confidence level;</li>        <li id="ul0002-0002" num="0092">Scenario 2&#x2014;in embodiments in which active vouching module <b>214</b> confirms an entity but passive vouching module <b>224</b> does not confirm an entity, the confidence level from active vouching module <b>214</b> may be used as an overall confidence level (with or without downward adjustment to reflect the lack of confirmation by passive vouching module <b>224</b>);</li>        <li id="ul0002-0003" num="0093">Scenario 3&#x2014;in embodiments in which passive vouching module <b>224</b> confirms an entity but active vouching module <b>214</b> does not confirm an entity, the confidence level from passive vouching module <b>224</b> may be used as an overall confidence level (with or without downward adjustment to reflect the lack of confirmation by active vouching module <b>214</b>);</li>        <li id="ul0002-0004" num="0094">Scenario 4&#x2014;in embodiments in which active vouching module <b>214</b> and passive vouching module <b>224</b> generate conflicting results, the system may apply one or more operations to reconcile the conflicting results. In some embodiments, integrating result from passive and active vouching may comprise resolving an entity value, e.g., based on confidence level(s) obtained from passive and active approaches. This resolution may be performed for each individual entity.</li>    </ul>    </li></ul></p><p id="p-0092" num="0095"><figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref> depict a diagram of how a fingerprinting algorithm may be used as part of a process to render a decision (e.g., a confidence value) about whether purchase order is vouched, in some embodiments, by the systems disclosed herein. <figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref> depict how two evidence sets may be used to generate an overall result indicating a vouching confidence level. In the example of <figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref>, &#x201c;evidence set 1&#x201d; may comprise output data generated by an active vouching algorithm, and may share any one or more characteristics in common with the output data generated by active vouching module <b>214</b> in system <b>200</b>. In the example of <figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref>, &#x201c;evidence set 2&#x201d; may comprise output data generated by one or more document processing pipelines, and may share any one or more characteristics in common with the output data generated by pipelines <b>216</b>, <b>218</b>, and/or <b>220</b> in system <b>200</b>. In some embodiments, the combination of evidence set 1 and evidence set 2, as shown in <figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref>, to generate a vouching decision and/or a confidence value (as shown, for example, in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>), may correspond to any one or more of modules <b>222</b>, <b>224</b>, and <b>226</b> in system <b>200</b>.</p><p id="p-0093" num="0096">Fingerprinting is a technique that may leverage ERP data to aid document understanding and vouching. Fingerprinting uses the context from ERP as a fingerprint for how the system searches an unstructured document for evidence of a match. By knowing what PO characteristics to look for from the ERP entry (e.g., specific PO #, set of item numbers associated with this PO, total amount of this PO, etc.), the system may look for those evidences in the attached PO (unstructured document).</p><p id="p-0094" num="0097">One advantage of fingerprinting is that it may provide important context that allows an AI algorithm to make better judgement of what it is seeing on a document, such that the system can achieve higher extraction accuracy and match rates. One drawback of fingerprinting is that, if not used carefully, it may introduce bias&#x2014;e.g., causing the system to see &#x201c;only what you want to see.&#x201d; For example, there may be additional attachments (POs, transactions, statements) that bear no relationships to the ERP but yet should be carefully reviewed. Thus, in some embodiments fingerprinting should not be used alone, but rather should be combined with other vouching logic and algorithms to ensure accuracy and effectiveness.</p><p id="p-0095" num="0098">In some embodiments, fingerprinting can include a simple search for an expected value, such as a particular PO number. As PO number is very unique, this may work well in most cases, giving the system confidence that if it found PBC2145XC01, it did indeed match on the expected PO number. However, other fields might not be as simple, for example, the field Quantity. Searching for a value of &#x2018;1&#x2019; could return a number of matches on a single document and even more across an entire set of documents, giving the system little confident that it has indeed matched on Quantity. Thus, it is important to include the ability to measure the system's confidence, as well as to design additional algorithms and ML models to help improve confidence and hone in on the right match. For example, if the system that the Item #, Unit Price for the PO line with that Quantity are located nearby or resides on the same PO line, this gives the match higher confidence and can remove other spurious matches of the value &#x201c;1&#x201d;. Confidence in fingerprinting may be refined by combining what is learned from <b>1</b>) template-based extraction, 2) template-less extraction, and 3) additional ML models and algorithms on top of search findings, to remove spurious matches and increase confidence in matches.</p><p id="p-0096" num="0099"><figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref> show how various document-understanding components function together with fingerprinting, in accordance with some embodiments. The combination of functions shown in <figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref> may enable improved overall goals, including an increased percent of vouched entries and an increased confidence on vouched entries.</p><p id="p-0097" num="0100"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a diagram of a fingerprinting algorithm, in accordance with some embodiments.</p><p id="p-0098" num="0101">In some embodiments, a fingerprinting algorithm may generate output for PO Headers and/or PO Lines. The algorithm may support exact match (fuzzy=1.0) and fuzzy matches. The algorithm may use Elasticsearch to index OCR text extraction of unstructured documents for search and/or lookup. The algorithm may use entity extraction to identify and normalize dates. The algorithm may use one or more spatial models to identify PO Lines to reduce spurious matches. The algorithm may support derived total amount search. The algorithm may support delivery terms synonyms.</p><p id="p-0099" num="0102">In some embodiments, the fingerprinting algorithm may include one or more of the following steps, sub-steps, and/or features:</p><p id="p-0100" num="0000">1) Prepare the ERP data for search (prepare_master.ipynb).<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0103">a) This puts it in a standard format for searching field content against unstructured document. If one follows same format, this can be applied to other ERP entries (invoices, shipment tracking number, etc.)</li>        <li id="ul0004-0002" num="0104">b) Also, computes the total amount from the PO lines and will look for this derived total amount while going through the &#x201c;PO Headers&#x201d; in Step <b>6</b>.<br/>2) Perform text extraction of PDFs using Abbyy Finereader FRE.</li>        <li id="ul0004-0003" num="0105">a) This produces a_basic.XML that has all the text blocks.<br/>3) Create concatenated text document from these text blocks<br/>4) Perform entity extraction on text document<br/>5) Index text document into Elasticsearch (text plus entities and some metadata)</li>        <li id="ul0004-0004" num="0106">a) Incorporate document classification model results so the system knows which ones are POs        <ul id="ul0005" list-style="none">            <li id="ul0005-0001" num="0107">i) Optional whether the system excludes indexing non-POs or marks it in elasticsearch<br/>6) Run fingerprinting search on PO headers</li>        </ul>        </li>        <li id="ul0004-0005" num="0108">a) For each field, analyze expected ERP data and generate text value candidates        <ul id="ul0006" list-style="none">            <li id="ul0006-0001" num="0109">i) For example, delivery terms will have a set of synonyms to the one in ERP as search candidates</li>            <li id="ul0006-0002" num="0110">ii) For example, date will be normalized to search against the date entities of documents</li>        </ul>        </li>        <li id="ul0004-0006" num="0111">b) Issue appropriate query against elasticsearch        <ul id="ul0007" list-style="none">            <li id="ul0007-0001" num="0112">i) Target at documents with same SO</li>            <li id="ul0007-0002" num="0113">ii) If non-POs were included, optionally limit to docclass=PO</li>        </ul>        </li>        <li id="ul0004-0007" num="0114">c) Evaluate elasticsearch results        <ul id="ul0008" list-style="none">            <li id="ul0008-0001" num="0115">i) Interpret and find fuzzy matches from elasticsearch highlighted text</li>            <li id="ul0008-0002" num="0116">ii) Compute fuzzy scores with search candidates</li>            <li id="ul0008-0003" num="0117">iii) Match if fuzzy score equal or above configured threshold</li>            <li id="ul0008-0004" num="0118">iv) compute confidence (1/number of matches)<br/>7) Run fingerprinting search on PO lines</li>        </ul>        </li>        <li id="ul0004-0008" num="0119">a) The PO lines search is run separately from the PO headers</li>        <li id="ul0004-0009" num="0120">b) Run algorithm to identify PO lines        <ul id="ul0009" list-style="none">            <li id="ul0009-0001" num="0121">i) For each SO,            <ul id="ul0010" list-style="none">                <li id="ul0010-0001" num="0122">(1) From ERP, find all the item numbers, this is used as anchor</li>                <li id="ul0010-0002" num="0123">(2) Find all POs (document classification results) for this SO, and for each document                <ul id="ul0011" list-style="none">                    <li id="ul0011-0001" num="0124">(a) Identify locations in text of all anchor values (i.e. in item numbers)</li>                    <li id="ul0011-0002" num="0125">(b) Calculate spacing between anchor values (number of word token part)</li>                    <li id="ul0011-0003" num="0126">(c) Calculate average of these spacing as line window width</li>                </ul>                </li>                <li id="ul0010-0003" num="0127">(3) With the line window width and the location of the anchors, the system know the vicinity of values for a given PO line</li>            </ul>            </li>        </ul>        </li>        <li id="ul0004-0010" num="0128">c) Run search for each ERP PO line, limited to the PO line window of text identified in previous step        <ul id="ul0012" list-style="none">            <li id="ul0012-0001" num="0129">i) For each PO line in ERP, look for the line values (e.g., Item #, Unit Price, Quantity, etc.) in the corresponding PO line window            <ul id="ul0013" list-style="none">                <li id="ul0013-0001" num="0130">(1) The window may be defined as: (location of anchor&#x2212;window size, location of anchor+window size)</li>                <li id="ul0013-0002" num="0131">(2) This may be refined with more experiments</li>                <li id="ul0013-0003" num="0132">(3) Match if fuzzy score equal or above configured threshold</li>                <li id="ul0013-0004" num="0133">(4) Compute confidence (1/number of matches)</li>            </ul>            </li>        </ul>        </li>    </ul>    </li></ul></p><heading id="h-0007" level="2">Payment Vouching for Assurance</heading><p id="p-0101" num="0134">Pursuant to the need to perform automated vouching, there is a need for improved systems and methods for vouching ERP entries against bank statement data in order to verify payment.</p><p id="p-0102" num="0135">In some embodiments, a system is configured vouch payment data against evidence data. More specifically, a system may be configured to provide a framework that performs ERP payment activities vouching against physical bank statement. The system may include a pipeline that perform information extraction and characteristics extraction from bank statements, and the system may leverage one or more advanced data structures and matching algorithms to perform one-to-many matching between ERP data and bank statement data. The payment vouching systems provided herein may thus automate the process of finding material evidence such as remittance advice or bank statements to corroborate ERP payment entries.</p><p id="p-0103" num="0136">The system may be configured to receive a data set comprising bank statement data, wherein the bank statement data may be provided, for example, in the form of PDF files or JPG files of bank statements. The system may apply one or more data processing operations (e.g., AI models) to the received bank statement data in order to extract information (e.g., key content and characteristics) from said data. The extracted information may be stored in any suitable output format, and/or may be used to generate one or more feature vectors representing one or more bank statements in the bank statement data.</p><p id="p-0104" num="0137">The system may be configured to receive a data set comprising ERP data, wherein the ERP data may be comprise one or more ERP entries. The system may apply one or more data processing operations (e.g., AI models) to the received ERP data in order to extract information (e.g., key content and characteristics) from said data. The extracted information may be stored in any suitable output format, and/or may be used to generate one or more feature vectors representing one or more ERP entries in the ERP data.</p><p id="p-0105" num="0138">The system may be configured to apply one or more algorithms (e.g., matching algorithms) to compare the information extracted from the bank statements against the information extracted from the ERP entries, and to thereby determine whether the bank statements sufficiently vouch the ERP entries. In some embodiments, performing the comparison may comprise applying an approximation algorithm configured to achieve better matching rates between ERP records and bank statements with minor numeric discrepancies, which may be caused, for example, due to currency conversion, rather than being indicative of substantive discrepancies. The system may determine, based on the similarity or dissimilarity of the information indicated by the two information sets, whether one or more vouching criteria are satisfied. The system may generate an output that indicates a level of matching between the bank statements and ERP entries (e.g., a similarity score), an indication of whether one or more vouching criteria (e.g., a threshold similarity score and/or threshold confidence level) are met, an indication of any discrepancies identified, and/or a level of confidence (e.g., a confidence score) in one or more conclusions reached by the system. In some embodiments, output data may be stored, transmitted, presented to a user, used to generate one or more visualizations, and/or used to trigger one or more automated system actions.</p><p id="p-0106" num="0139">In some embodiments, the system may be configured in a modular manner, such that one or more data processing operations may be modified without modification or one or more feature engineering and/or data comparison operations, and vice versa. This may allow for the system to be configured and fine-tuned in accordance with changes in business priorities, requested new features, or evolution of legal or regulatory requirements.</p><p id="p-0107" num="0140"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> show a diagram of a payment vouching method <b>500</b>, in accordance with some embodiments. In some embodiments, all or part of the method depicted in <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> may be applied by the systems described herein (e.g., system <b>200</b>). In some embodiments, a payment vouching method may seek to match data representing one or more of the following: date, amount, customer name, and invoice number. As shown in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, the system may accept ERP payment journal data and bank statement data as inputs (optionally following data pre-processing and formatting). The bank statement data may be subject to one or more AI information extraction models to extract information regarding transaction category, customer name, and invoices. The system may then apply a first matching algorithm, for example a fuzzy matching algorithm, to compare the ERP data to the data extracted from the bank statements. If a match is detected, then the system may, among one or more other operations, apply one or more comparison and/or scoring operations in order to generate overall match score data and overall confidence data. If no match is detected, then the system may apply a second matching algorithm, for example an optimization algorithm that has been proposed to solve the Knapsack problem. If no match is detected by the second algorithm, then an overall match score of 0 may be generated. If a match is detected by the second algorithm, then the system may select an optimal subset candidate and may, among one or more other operations, apply one or more comparison and/or scoring operations in order to generate an overall match score and an overall confidence score. A more detailed description follows.</p><p id="p-0108" num="0141">At block <b>502</b>, in some embodiments, the system may receive data representing ERP information, for example by receiving data from an ERP payment journal data source. The data representing ERP information may be received automatically, according to a predefined schedule, in response to one or more trigger conditions being met, as part of a scraping method, and/or in response to a user input. The system may receive the ERP data in any acceptable format. In some embodiments, ERP data may be provided in a tabular data format, including a data model that defines the structure of the data. ERP data may be received from &#x201c;account receivable&#x201d; data or from &#x201c;cash received&#x201d; data. ERP data may be in tabular format including customer name, invoice data, and invoice amount.</p><p id="p-0109" num="0142">At block <b>504</b>, in some embodiments, the system may receive data representing one or more bank statements. The data representing the bank statements may be received automatically, according to a predefined schedule, in response to one or more trigger conditions being met, as part of a scraping method, and/or in response to a user input. The system may receive the bank statement data in any acceptable format, for example as a structured and/or unstructured document, including for example a PDF document. In some embodiments, the system may receive bank statement data in PDF format and/or CSV format. In some embodiments, the system may download electronic bank statement data (such as BAI/BAI2, Multicash, MT940). In some embodiments, the system may receive bank statement data via EDI and/or ISO 20022. In some embodiments, the system may receive bank statement data through one or more API aggregators such as Plaid and Yodlee.</p><p id="p-0110" num="0143">At block <b>506</b>, in some embodiments, the system may apply one or more information extraction models to the data representing the one or more bank statements. The one information extraction models may generate transaction category data <b>508</b>, customer name data <b>510</b>, and/or invoice data <b>512</b>. The extracted information may be stored, displayed to a user, transmitted, and/or used for further processing for example as disclosed herein.</p><p id="p-0111" num="0144">At block <b>514</b>, in some embodiments, the system may apply one or more fuzzy matching algorithms. The one or more fuzzy matching algorithms may accept input data including (but not limited to) data representing ERP information from block <b>502</b>, transaction category data <b>508</b>, customer name data <b>510</b>, and/or invoice data <b>512</b>. The one or more fuzzy matching algorithms may compare data in a many-to-many manner. The one or more fuzzy matching algorithms may process the received input data in order to determine whether there is a match or a near match (e.g., a &#x201c;fuzzy match&#x201d;) between the data representing ERP information and the transaction category data <b>508</b>, customer name data <b>510</b>, and/or invoice data <b>512</b>. The one or more fuzzy matching algorithms may generate data representing an indication as to whether or not a match has been determined. The indication may comprise a binary indication as to whether or not a match has been determined and/or may comprise a confidence score representing a confidence level that a match has been determined.</p><p id="p-0112" num="0145">At block <b>516</b>, in some embodiments, the system may determine whether a match was determined at block <b>514</b>. In some embodiments, the system may reference output data generated by the one or more fuzzy matching algorithms to determine whether a match was determined, for example by referencing whether a match is indicated by the output data on a binary basis. In some embodiments, the system may determine whether a match score generated at block <b>514</b> exceeds one or more predetermined or dynamically-determined threshold values in order to determine whether match criteria are met and thus whether a match is determined. In accordance with a determination that a match was determined, method <b>500</b> may proceed to blocks <b>518</b>-<b>538</b>. In accordance with a determination that a match was not determined, method <b>500</b> may proceed to block <b>540</b> and onward.</p><p id="p-0113" num="0146">Turning first to cases in which it is determined at block <b>516</b> that a match was determined, attention is drawn to block <b>518</b>. At block <b>518</b>, the system may determine whether the match that was determined is a one-to-one match. In some embodiments, the system may reference output data generated by the one or more fuzzy matching algorithms to determine whether the match that was determined is a one-to-one match. In accordance with a determination that the match that was determined is a one-to-one match, the method may proceed to one or both of blocks <b>510</b> and <b>524</b>.</p><p id="p-0114" num="0147">At block <b>520</b>, in some embodiments, the system may apply a fuzzy comparison algorithm to data representing customer name information. In some embodiments, the system may compare customer name data in the data representing ERP information (received at block <b>502</b>) to customer name data in the data representing one or more bank statements (received at block <b>504</b>). The comparison of customer name data may generate output data comprising customer name match score <b>522</b>, which may indicate an extent to which and/or a confidence with which the compared customer name data matches.</p><p id="p-0115" num="0148">At block <b>524</b>, in some embodiments, the system may apply a fuzzy comparison algorithm to data representing invoice information. In some embodiments, the system may compare invoice data in the data representing ERP information (received at block <b>502</b>) to invoice data in the data representing one or more bank statements (received at block <b>504</b>). The comparison of invoice data may generate output data comprising invoice match score <b>526</b>, which may indicate an extent to which and/or a confidence with which the compared invoice data matches.</p><p id="p-0116" num="0149">In some embodiments, the processes represented by blocks <b>518</b>, <b>520</b>, and <b>524</b> may be performed as follows. The system may test whether there is a match between data extracted from bank statements and ERP data for the following three attributes: we will need to test whether there is a match between the data extracted from the back statement and the ERP data for the following three attributes: fuzzy date comparison, where small deviations of date data between bank statements and ERP data may be considered acceptable; fuzzy customer name comparison, which may allow comparing normalized customer name data from bank statements (if present) with customer name data from ERP data; and invoice number comparison, where fuzzy invoice number comparison allows comparing invoice numbers between bank statement (if present). It should be noted that customer name and invoice number might not always be available in the bank statement data.</p><p id="p-0117" num="0150">In some embodiments, one or more other component scores, aside from or in addition to a customer name match score and an invoice match score, may be computed.</p><p id="p-0118" num="0151">In addition to or alternatively to customer name match score <b>522</b> and invoice match score <b>526</b>, the system may generate data comprising temporal match score <b>528</b>, for example by performing a fuzzy comparison of date data as shown at block <b>527</b>. Temporal match score <b>528</b> may be computed based on a temporal difference (e.g., a number of days difference) in compared data. For example, the system may compare a date indicated in the data representing ERP information (received at block <b>502</b>) to a date indicated in the data representing one or more bank statements (received at block <b>504</b>), and may generate temporal match score <b>528</b> based on the difference between the two compared dates.</p><p id="p-0119" num="0152">Following generation of component scores including for example customer name match score <b>522</b>, invoice match score <b>526</b>, and/or temporal match score <b>528</b>, the system may generate an overall match score and/or an overall confidence score based on the component scores.</p><p id="p-0120" num="0153">At block <b>532</b>, in some embodiments, the system may compute overall match score <b>534</b>. Computation of overall match score <b>534</b> may comprise applying an averaging algorithm (e.g., averaging non-zero component scores), for example by computing a weighted or unweighted average of one or more underlying component scores. In some embodiments, overall match score <b>534</b> may be computed as a the sum of three terms: a weighted fuzzy date comparison score (e.g., weighted <b>528</b>), a weighted fuzzy customer name comparison score (e.g., weighted <b>522</b>), and a weighted fuzzy invoice number comparison score (e.g., weighted <b>526</b>). Computing an additive overall match score <b>534</b> may mean the overall match score <b>534</b> is higher when it is based on a comparison of more (e.g., all three) underlying terms than when it is not.</p><p id="p-0121" num="0154">At block <b>536</b>, in some embodiments, the system may compute overall confidence score <b>538</b>. Computation of overall confidence score <b>538</b> may comprise applying an algorithm based one or more underlying confidence scores, such as confidence scores associated with one or more of underlying component scores. In some embodiments, a highest underlying confidence score may be selected as overall confidence score <b>538</b>. In some embodiments, a lowest underlying confidence score may be selected as overall confidence score <b>538</b>. In some embodiments, a weighted or unweighted average of underlying confidence scores may be computed as overall confidence score <b>538</b>. In some embodiments, a product based on underlying confidence scores may be computed as overall confidence score <b>538</b>.</p><p id="p-0122" num="0155">Overall match score <b>534</b> and/or overall confidence score <b>538</b> may be stored, transmitted, presented to a user, used to generate one or more visualizations, and/or used to trigger one or more automated system actions.</p><p id="p-0123" num="0156">Turning now to cases in which it is determined at block <b>516</b> that a match was not determined, attention drawn to block <b>540</b>. At block <b>540</b>, in some embodiments, the system may apply one or more amount matching algorithms, for example including one or more optimization algorithms that have been proposed to solve the Knapsack problem. The one or more amount matching algorithms may accept input data including (but not limited to) data representing ERP information from block <b>502</b>, transaction category data <b>508</b>, customer name data <b>510</b>, and/or invoice data <b>512</b>. The one or more amount matching algorithms may compare data in a one to many manner. The one or more amount matching algorithms may compare data from one bank transaction (e.g., data received at block <b>504</b>) to data for many vouchers (e.g., data received at block <b>502</b>). The one or more amount matching algorithms may process the received input data in order to determine whether there is a match between the data representing ERP information and the transaction category data <b>508</b>, customer name data <b>510</b>, and/or invoice data <b>512</b>. The one or more amount matching algorithms may generate data representing an indication as to whether or not a match has been determined. The indication may comprise a binary indication as to whether or not a match has been determined and/or may comprise a confidence score representing a confidence level that a match has been determined.</p><p id="p-0124" num="0157">At block <b>542</b>, in some embodiments, the system may determine whether a match was determined at block <b>540</b>. In some embodiments, the system may reference output data generated by the one or more amount matching algorithms to determine whether a match was determined, for example by referencing whether a match is indicated by the output data on a binary basis. In some embodiments, the system may determine whether a match score generated at block <b>540</b> exceeds one or more predetermined or dynamically-determined threshold values in order to determine whether match criteria are met and thus whether a match is determined. In accordance with a determination that a match was determined, method <b>500</b> may proceed to blocks <b>544</b>-<b>564</b>. In accordance with a determination that a match was not determined, method <b>500</b> may proceed to block <b>566</b> and onward.</p><p id="p-0125" num="0158">At block <b>544</b>, in some embodiments, the system may select a candidate subset of data from the data received at block <b>502</b> and/or the data received at block <b>504</b>. The analysis performed at blocks <b>546</b>-<b>564</b> may be performed with respect to the selected candidate subset of data. In some embodiments, to perform candidate subset selection, the system may identify a set of bank transactions that may be a match, and may then assess each item in the subset to determine which is the best match. In some embodiments, candidate subsets may include different numbers of items in the candidate subset. For example, one candidate subsets may be &#x201c;three transactions that may match to a voucher,&#x201d; while another candidate subset may be &#x201c;two transactions that may match to a voucher.&#x201d;</p><p id="p-0126" num="0159">In some embodiments, candidate subset selection may proceed as follows: candidates may be sorted from largest to smallest; then those items in the sorted list that are already larger than the target may be eliminated, and only those which are smaller than or equal to the target amount are retained; then, a total amount from all of the remaining items may be computed, and those that match the target may be identified. In some embodiments, an overall objective may include determining whether the amount C from payment is a match to two or more elements among {A1, A2, A3}. If A1, A2, A3, have been sorted from largest to smallest, then it may be necessary to test whether</p><p id="p-0127" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>C=A</i>1+<i>A</i>2; or<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0128" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>C=A</i>2+<i>A</i>3; or<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0129" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>C=A</i>1+<i>A</i>2+<i>A</i>3.<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0130" num="0000">Thus, if A1 is known to be larger than C, then other additive combinations that include A1 may be known to be larger than C, and thus may not need to be tested, and the only remaining possibility that may need to be tested is whether C=A2+A3.</p><p id="p-0131" num="0160">Based on the selected candidate subset, the system may generate one or more component scores, such as component scores <b>548</b>, <b>552</b>, and/or <b>556</b> described below.</p><p id="p-0132" num="0161">At block <b>546</b>, in some embodiments, the system may apply one or more subset match score algorithms to the selected candidate subset of data, thereby generating subset match score <b>548</b>, which may indicate an extent to which and or a confidence by which two or more components (e.g., data points) of the selected subset match with one another. Block <b>546</b> may compare a voucher amount to a bank amount. Block <b>546</b> may compare an amount appearing in the data received at block <b>502</b> to an amount appearing in the data received at block <b>504</b>.</p><p id="p-0133" num="0162">At block <b>550</b>, in some embodiments, the system may apply one or more fuzzy name comparison algorithms to the selected candidate subset of data, thereby generating customer name match score <b>552</b>, which may indicate an extent to which and or a confidence by which two or more customer names in the selected subset match with one another. Block <b>550</b> may compare a customer name in voucher data with a customer name in statement data. Block <b>550</b> may compare a customer name appearing in the data received at block <b>502</b> to a customer name appearing in the data received at block <b>504</b>.</p><p id="p-0134" num="0163">At block <b>554</b>, in some embodiments, the system may apply one or more fuzzy invoice comparison algorithms to the selected candidate subset of data, thereby generating invoice match score <b>556</b>, which may indicate an extent to which and or a confidence by which two or more invoices in the selected subset match with one another. Block <b>554</b> may compare two instances of invoice data to one another. Block <b>550</b> may compare invoice data appearing in the data received at block <b>502</b> to invoice data appearing in the data received at block <b>504</b>.</p><p id="p-0135" num="0164">Following generation of component scores including for example subset match score <b>548</b>, customer name match score <b>552</b>, and/or invoice match score <b>556</b>, the system may generate an overall match score and/or an overall confidence score based on the component scores.</p><p id="p-0136" num="0165">At block <b>558</b>, in some embodiments, the system may compute overall match score <b>560</b>. Computation of overall match score <b>560</b> may comprise applying an averaging algorithm (e.g., averaging non-zero component scores), for example by computing a weighted or unweighted average of one or more underlying component scores.</p><p id="p-0137" num="0166">At block <b>562</b>, in some embodiments, the system may compute overall confidence score <b>564</b>. Computation of overall confidence score <b>564</b> may comprise applying an algorithm based one or more underlying confidence scores, such as confidence scores associated with one or more of underlying component scores. In some embodiments, a highest underlying confidence score may be selected as overall confidence score <b>564</b>. In some embodiments, a lowest underlying confidence score may be selected as overall confidence score <b>564</b>. In some embodiments, a weighted or unweighted average of underlying confidence scores may be computed as overall confidence score <b>564</b>. In some embodiments, a product based on underlying confidence scores may be computed as overall confidence score <b>564</b>.</p><p id="p-0138" num="0167">Overall match score <b>560</b> and/or overall confidence score <b>564</b> may be stored, transmitted, presented to a user, used to generate one or more visualizations, and/or used to trigger one or more automated system actions.</p><p id="p-0139" num="0168">Turning now to cases in which it is determined at block <b>542</b> that a match was not determined, attention drawn to block <b>564</b>. At block <b>564</b>, in some embodiments, the system may determine that an overall match score is 0. The overall match score of 0 may be stored, transmitted, presented to a user, used to generate one or more visualizations, and/or used to trigger one or more automated system actions.</p><p id="p-0140" num="0169">In some embodiments, the system may be configured to apply a plurality of different algorithms (e.g., two different algorithms, three different algorithms, etc.) as part of a payment vouching process. In some embodiments, the algorithms may be applied in parallel. In some embodiments, the algorithms may be applied in series. In some embodiments, the algorithms may be applied selectively dependent on the outcome of one another; for example, the system may first apply one algorithm and then may apply another algorithm selectively dependent on the outcome of the first algorithm (e.g., whether or not a match was indicated by the first algorithm). In some embodiments the system may be configured to apply a waterfall algorithm, a fuzzy date-amount algorithm, and an optimization algorithm that has been proposed to solve the Knapsack problem.</p><heading id="h-0008" level="2">Computer</heading><p id="p-0141" num="0170"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example of a computer, according to some embodiments. Computer <b>600</b> can be a component of a system for providing an AI-augmented auditing platform including techniques for providing AI-explainability for processing data through multiple layers. In some embodiments, computer <b>600</b> may execute any one or more of the methods described herein.</p><p id="p-0142" num="0171">Computer <b>600</b> can be a host computer connected to a network. Computer <b>600</b> can be a client computer or a server. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, computer <b>600</b> can be any suitable type of microprocessor-based device, such as a personal computer, workstation, server, or handheld computing device, such as a phone or tablet. The computer can include, for example, one or more of processor <b>610</b>, input device <b>620</b>, output device <b>630</b>, storage <b>640</b>, and communication device <b>660</b>. Input device <b>620</b> and output device <b>630</b> can correspond to those described above and can either be connectable or integrated with the computer.</p><p id="p-0143" num="0172">Input device <b>620</b> can be any suitable device that provides input, such as a touch screen or monitor, keyboard, mouse, or voice-recognition device. Output device <b>630</b> can be any suitable device that provides an output, such as a touch screen, monitor, printer, disk drive, or speaker.</p><p id="p-0144" num="0173">Storage <b>640</b> can be any suitable device that provides storage, such as an electrical, magnetic, or optical memory, including a random access memory (RAM), cache, hard drive, CD-ROM drive, tape drive, or removable storage disk. Communication device <b>660</b> can include any suitable device capable of transmitting and receiving signals over a network, such as a network interface chip or card. The components of the computer can be connected in any suitable manner, such as via a physical bus or wirelessly. Storage <b>640</b> can be a non-transitory computer-readable storage medium comprising one or more programs, which, when executed by one or more processors, such as processor <b>610</b>, cause the one or more processors to execute methods described herein.</p><p id="p-0145" num="0174">Software <b>650</b>, which can be stored in storage <b>640</b> and executed by processor <b>610</b>, can include, for example, the programming that embodies the functionality of the present disclosure (e.g., as embodied in the systems, computers, servers, and/or devices as described above). In some embodiments, software <b>650</b> can include a combination of servers such as application servers and database servers.</p><p id="p-0146" num="0175">Software <b>650</b> can also be stored and/or transported within any computer-readable storage medium for use by or in connection with an instruction execution system, apparatus, or device, such as those described above, that can fetch and execute instructions associated with the software from the instruction execution system, apparatus, or device. In the context of this disclosure, a computer-readable storage medium can be any medium, such as storage <b>640</b>, that can contain or store programming for use by or in connection with an instruction execution system, apparatus, or device.</p><p id="p-0147" num="0176">Software <b>650</b> can also be propagated within any transport medium for use by or in connection with an instruction execution system, apparatus, or device, such as those described above, that can fetch and execute instructions associated with the software from the instruction execution system, apparatus, or device. In the context of this disclosure, a transport medium can be any medium that can communicate, propagate, or transport programming for use by or in connection with an instruction execution system, apparatus, or device. The transport-readable medium can include but is not limited to, an electronic, magnetic, optical, electromagnetic, or infrared wired or wireless propagation medium.</p><p id="p-0148" num="0177">Computer <b>600</b> may be connected to a network, which can be any suitable type of interconnected communication system. The network can implement any suitable communications protocol and can be secured by any suitable security protocol. The network can comprise network links of any suitable arrangement that can implement the transmission and reception of network signals, such as wireless network connections, T1 or T3 lines, cable networks, DSL, or telephone lines.</p><p id="p-0149" num="0178">Computer <b>600</b> can implement any operating system suitable for operating on the network. Software <b>650</b> can be written in any suitable programming language, such as C, C++, Java, or Python. In various embodiments, application software embodying the functionality of the present disclosure can be deployed in different configurations, such as in a client/server arrangement or through a Web browser as a Web-based application or Web service, for example.</p><p id="p-0150" num="0179">Following is a list of enumerated embodiments:<ul id="ul0014" list-style="none">    <li id="ul0014-0001" num="0000">    <ul id="ul0015" list-style="none">        <li id="ul0015-0001" num="0180">Embodiment 1. A system for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, the system comprising one or more processors configured to cause the system to:</li>        <li id="ul0015-0002" num="0181">receive data representing an ERP item;</li>        <li id="ul0015-0003" num="0182">generate hypothesis data based on the received data represent an ERP item;</li>        <li id="ul0015-0004" num="0183">receive an electronic document;</li>        <li id="ul0015-0005" num="0184">extract ERP information from the document;</li>        <li id="ul0015-0006" num="0185">apply a first set of one or more models to the hypothesis data and to extracted ERP information in order to generate first output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item;</li>        <li id="ul0015-0007" num="0186">apply a second set of one or more models to the extracted ERP information in order to generate second output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; and</li>        <li id="ul0015-0008" num="0187">generate combined determination data, based on the first output data and the second output data, indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</li>        <li id="ul0015-0009" num="0188">Embodiment 2. The system of embodiment 1, wherein extracting the ERP information comprises generating first data representing information content of the ERP information and second data representing a document location for the ERP information</li>        <li id="ul0015-0010" num="0189">Embodiment 3. The system of any one of embodiments 1-2, wherein the ERP information comprises one or more of: a purchase order number, a customer name, a date, a delivery term, a shipping term, a unit price, and a quantity.</li>        <li id="ul0015-0011" num="0190">Embodiment 4. The system of any one of embodiments 1-3, wherein applying the first set of one or more models to generate output data is based on preexisting information regarding spatial relationships amongst instances of ERP information in documents.</li>        <li id="ul0015-0012" num="0191">Embodiment 5. The system of embodiment 4, wherein the preexisting information comprises a graph representing spatial relationships amongst instances of ERP information in documents.</li>        <li id="ul0015-0013" num="0192">Embodiment 6. The system of any one of embodiments 1-5, wherein the one or more processors are configured to cause the system to augment the hypothesis data based on one or more models representing contextual data.</li>        <li id="ul0015-0014" num="0193">Embodiment 7. The system of embodiment 6, wherein the contextual data comprises information regarding one or more synonyms for the information content of the ERP information.</li>        <li id="ul0015-0015" num="0194">Embodiment 8. The system of any one of embodiments 1-7, wherein the ERP information comprises a single word in the document.</li>        <li id="ul0015-0016" num="0195">Embodiment 9. The system of any one of embodiments 1-8, wherein the ERP information comprises a plurality of words in the document.</li>        <li id="ul0015-0017" num="0196">Embodiment 10. The system of any one of embodiments 1-9, wherein the second output data comprises one or more of:</li>        <li id="ul0015-0018" num="0197">a confidence score indicating a confidence level as to whether the extracted ERP information constitutes vouching evidence for the ERP item;</li>        <li id="ul0015-0019" num="0198">a binary indication as to whether the extracted ERP information constitutes vouching evidence for the ERP item; and</li>        <li id="ul0015-0020" num="0199">a location within the electronic document corresponding to the determination as to whether the extracted ERP information constitutes vouching evidence for the ERP item.</li>        <li id="ul0015-0021" num="0200">Embodiment 11. The system of embodiment 1, wherein generating the second output data comprises generating a similarity score representing a comparison of the ERP information and the ERP item.</li>        <li id="ul0015-0022" num="0201">Embodiment 12. The system of embodiment 11, wherein the similarity score is generated based on an entity graph representing contextual data.</li>        <li id="ul0015-0023" num="0202">Embodiment 13. The system of any one of embodiments 1-12, wherein extracting the ERP information from the document comprises applying a fingerprinting operation to determine, based on the receive data representing an ERP item, a characteristic of a data extraction operation to be applied to the electronic document.</li>        <li id="ul0015-0024" num="0203">Embodiment 14. The system of any one of embodiments 1-13, wherein applying the second set of one or more models is based at least in part on contextual data.</li>        <li id="ul0015-0025" num="0204">Embodiment 15. The system of any one of embodiments 1-14, wherein applying the second set of one or more models comprises:</li>        <li id="ul0015-0026" num="0205">applying a set of document processing pipelines in parallel to generate a plurality of processing pipeline output data;</li>        <li id="ul0015-0027" num="0206">applying one or more data normalization operations to the plurality of processing pipeline output data to generate normalized data; and</li>        <li id="ul0015-0028" num="0207">generating the second output data based on the normalized data.</li>        <li id="ul0015-0029" num="0208">Embodiment 16. A non-transitory computer-readable storage medium storing instructions for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, the instructions configured to be executed by a system comprising one or more processors to cause the system to:</li>        <li id="ul0015-0030" num="0209">receive data representing an ERP item;</li>        <li id="ul0015-0031" num="0210">generate hypothesis data based on the received data represent an ERP item;</li>        <li id="ul0015-0032" num="0211">receive an electronic document;</li>        <li id="ul0015-0033" num="0212">extract ERP information from the document;</li>        <li id="ul0015-0034" num="0213">apply a first set of one or more models to the hypothesis data and to extracted ERP information in order to generate first output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item;</li>        <li id="ul0015-0035" num="0214">apply a second set of one or more models to the extracted ERP information in order to generate second output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; and</li>        <li id="ul0015-0036" num="0215">generate combined determination data, based on the first output data and the second output data, indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</li>        <li id="ul0015-0037" num="0216">Embodiment 17. A method for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, wherein the method is performed by a system comprising one or more processors, the method comprising:</li>        <li id="ul0015-0038" num="0217">receiving data representing an ERP item;</li>        <li id="ul0015-0039" num="0218">generating hypothesis data based on the received data represent an ERP item;</li>        <li id="ul0015-0040" num="0219">receiving an electronic document;</li>        <li id="ul0015-0041" num="0220">extracting ERP information from the document;</li>        <li id="ul0015-0042" num="0221">applying a first set of one or more models to the hypothesis data and to extracted</li>        <li id="ul0015-0043" num="0222">ERP information in order to generate first output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item;</li>        <li id="ul0015-0044" num="0223">applying a second set of one or more models to the extracted ERP information in order to generate second output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; and</li>        <li id="ul0015-0045" num="0224">generating combined determination data, based on the first output data and the second output data, indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</li>        <li id="ul0015-0046" num="0225">Embodiment 18. A system for verifying an assertion against a source document, the system comprising one or processors configured to cause the system to:        <ul id="ul0016" list-style="none">            <li id="ul0016-0001" num="0226">receive first data indicating an unverified assertion;</li>        </ul>        </li>        <li id="ul0015-0047" num="0227">receive second data comprising a plurality of source documents;</li>        <li id="ul0015-0048" num="0228">apply one or more extraction models to extract a set of key data from the plurality of source documents; and        <ul id="ul0017" list-style="none">            <li id="ul0017-0001" num="0229">apply one or more matching models to compare the first data to the set of key data to generate an output indicating whether one or more of the plurality of source documents satisfies one or more verification criteria for verifying the unverified assertion.</li>        </ul>        </li>        <li id="ul0015-0049" num="0230">Embodiment 19. The system of embodiment 18, wherein the one or more extraction models comprise one or more machine learning models.</li>        <li id="ul0015-0050" num="0231">Embodiment 20. The system of any one of embodiments 18-19, wherein the one or more matching models comprises one or more approximation models.</li>        <li id="ul0015-0051" num="0232">Embodiment 21. The system of any one of embodiments 18-20, wherein the one or more matching models are configured to perform one-to-many matching between the first data and the set of key data.</li>        <li id="ul0015-0052" num="0233">Embodiment 22. The system of any one of embodiments 16-21, wherein the one of more processors are configured to cause the system to modify one or more of the extraction models without modification of one or more of the matching models.</li>        <li id="ul0015-0053" num="0234">Embodiment 23. The system of any one of embodiments 18-22, wherein the one of more processors are configured to cause the system to modify one or more of the matching models without modification of one or more of the extraction models.</li>        <li id="ul0015-0054" num="0235">Embodiment 24. The system of any one of embodiments 18-23, wherein the unverified assertion comprises an ERP payment entry.</li>        <li id="ul0015-0055" num="0236">Embodiment 25. The system of any one of embodiments 18-24, wherein the plurality of source documents comprises a bank statement.</li>        <li id="ul0015-0056" num="0237">Embodiment 26. The system of any one of embodiments 18-25, wherein applying one or more matching models comprises generating a match score and generating a confidence score.</li>        <li id="ul0015-0057" num="0238">Embodiment 27. The system of any one of embodiments 18-26, wherein applying one or more matching models comprises: applying a first matching model;</li>        <li id="ul0015-0058" num="0239">if a match is indicated by the first matching model, generating a match score and a confidence score based on the first matching model;</li>        <li id="ul0015-0059" num="0240">if a match is not indicated by the second matching model:        <ul id="ul0018" list-style="none">            <li id="ul0018-0001" num="0241">applying a second matching model; and</li>            <li id="ul0018-0002" num="0242">if a match is indicated by the second matching model, generating a match score and a confidence score based on the second matching mode; and</li>            <li id="ul0018-0003" num="0243">if a match is not indicated by the second matching model, generating a match score of 0.</li>        </ul>        </li>        <li id="ul0015-0060" num="0244">Embodiment 28. A non-transitory computer-readable storage medium storing instructions for verifying an assertion against a source document, the instructions configured to be executed by a system comprising one or processors to cause the system to:        <ul id="ul0019" list-style="none">            <li id="ul0019-0001" num="0245">receive first data indicating an unverified assertion;</li>        </ul>        </li>        <li id="ul0015-0061" num="0246">receive second data comprising a plurality of source documents;</li>        <li id="ul0015-0062" num="0247">apply one or more extraction models to extract a set of key data from the plurality of source documents; and        <ul id="ul0020" list-style="none">            <li id="ul0020-0001" num="0248">apply one or more matching models to compare the first data to the set of key data to generate an output indicating whether one or more of the plurality of source documents satisfies one or more verification criteria for verifying the unverified assertion.</li>        </ul>        </li>        <li id="ul0015-0063" num="0249">Embodiment 29. A method for verifying an assertion against a source document, wherein the method is executed by a system comprising one or processors, the method comprising:        <ul id="ul0021" list-style="none">            <li id="ul0021-0001" num="0250">receiving first data indicating an unverified assertion;</li>        </ul>        </li>        <li id="ul0015-0064" num="0251">receiving second data comprising a plurality of source documents;</li>        <li id="ul0015-0065" num="0252">applying one or more extraction models to extract a set of key data from the plurality of source documents; and        <ul id="ul0022" list-style="none">            <li id="ul0022-0001" num="0253">applying one or more matching models to compare the first data to the set of key data to generate an output indicating whether one or more of the plurality of source documents satisfies one or more verification criteria for verifying the unverified assertion.</li>        </ul>        </li>    </ul>    </li></ul></p><p id="p-0151" num="0254">This application incorporates by reference the entire contents of the U.S. patent application titled &#x201c;AI-AUGMENTED AUDITING PLATFORM INCLUDING TECHNIQUES FOR AUTOMATED ADJUDICATION OF COMMERCIAL SUBSTANCE, RELATED PARTIES, AND COLLECTABILITY&#x201d;, filed Jun. 30, 2022, Attorney Docket no. 13574-20069.00.</p><p id="p-0152" num="0255">This application incorporates by reference the entire contents of the U.S. patent application titled &#x201c;AI-AUGMENTED AUDITING PLATFORM INCLUDING TECHNIQUES FOR APPLYING A COMPOSABLE ASSURANCE INTEGRITY FRAMEWORK&#x201d;, filed Jun. 30, 2022, Attorney Docket no. 13574-20070.00.</p><p id="p-0153" num="0256">This application incorporates by reference the entire contents of the U.S. patent application titled &#x201c;AI-AUGMENTED AUDITING PLATFORM INCLUDING TECHNIQUES FOR AUTOMATED DOCUMENT PROCESSING&#x201d;, filed Jun. 30, 2022, Attorney Docket no. 13574-20071.00.</p><p id="p-0154" num="0257">This application incorporates by reference the entire contents of the U.S. patent application titled &#x201c;AI-AUGMENTED AUDITING PLATFORM INCLUDING TECHNIQUES FOR PROVIDING AI-EXPLAINABILITY FOR PROCESSING DATA THROUGH MULTIPLE LAYERS&#x201d;, filed Jun. 30, 2022, Attorney Docket no. 13574-20072.00.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, the system comprising one or more processors configured to cause the system to:<claim-text>receive data representing an ERP item;</claim-text><claim-text>generate hypothesis data based on the received data represent an ERP item;</claim-text><claim-text>receive an electronic document;</claim-text><claim-text>extract ERP information from the document;</claim-text><claim-text>apply a first set of one or more models to the hypothesis data and to extracted ERP information in order to generate first output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item;</claim-text><claim-text>apply a second set of one or more models to the extracted ERP information in order to generate second output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; and</claim-text><claim-text>generate combined determination data, based on the first output data and the second output data, indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein extracting the ERP information comprises generating first data representing information content of the ERP information and second data representing a document location for the ERP information</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ERP information comprises one or more of: a purchase order number, a customer name, a date, a delivery term, a shipping term, a unit price, and a quantity.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein applying the first set of one or more models to generate output data is based on preexisting information regarding spatial relationships amongst instances of ERP information in documents.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the preexisting information comprises a graph representing spatial relationships amongst instances of ERP information in documents.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors are configured to cause the system to augment the hypothesis data based on one or more models representing contextual data.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the contextual data comprises information regarding one or more synonyms for the information content of the ERP information.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ERP information comprises a single word in the document.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ERP information comprises a plurality of words in the document.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second output data comprises one or more of:<claim-text>a confidence score indicating a confidence level as to whether the extracted ERP information constitutes vouching evidence for the ERP item;</claim-text><claim-text>a binary indication as to whether the extracted ERP information constitutes vouching evidence for the ERP item; and</claim-text><claim-text>a location within the electronic document corresponding to the determination as to whether the extracted ERP information constitutes vouching evidence for the ERP item.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the second output data comprises generating a similarity score representing a comparison of the ERP information and the ERP item.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the similarity score is generated based on an entity graph representing contextual data.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein extracting the ERP information from the document comprises applying a fingerprinting operation to determine, based on the receive data representing an ERP item, a characteristic of a data extraction operation to be applied to the electronic document.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein applying the second set of one or more models is based at least in part on contextual data.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein applying the second set of one or more models comprises:<claim-text>applying a set of document processing pipelines in parallel to generate a plurality of processing pipeline output data;</claim-text><claim-text>applying one or more data normalization operations to the plurality of processing pipeline output data to generate normalized data; and</claim-text><claim-text>generating the second output data based on the normalized data.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A non-transitory computer-readable storage medium storing instructions for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, the instructions configured to be executed by a system comprising one or more processors to cause the system to:<claim-text>receive data representing an ERP item;</claim-text><claim-text>generate hypothesis data based on the received data represent an ERP item;</claim-text><claim-text>receive an electronic document;</claim-text><claim-text>extract ERP information from the document;</claim-text><claim-text>apply a first set of one or more models to the hypothesis data and to extracted ERP information in order to generate first output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item;</claim-text><claim-text>apply a second set of one or more models to the extracted ERP information in order to generate second output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; and</claim-text><claim-text>generate combined determination data, based on the first output data and the second output data, indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A method for determining whether data within an electronic document constitutes vouching evidence for an enterprise resource planning (ERP) item, wherein the method is performed by a system comprising one or more processors, the method comprising:<claim-text>receiving data representing an ERP item;</claim-text><claim-text>generating hypothesis data based on the received data represent an ERP item;</claim-text><claim-text>receiving an electronic document;</claim-text><claim-text>extracting ERP information from the document;</claim-text><claim-text>applying a first set of one or more models to the hypothesis data and to extracted ERP information in order to generate first output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item;</claim-text><claim-text>applying a second set of one or more models to the extracted ERP information in order to generate second output data indicating whether the extracted ERP information constitutes vouching evidence for the ERP item; and</claim-text><claim-text>generating combined determination data, based on the first output data and the second output data, indicating whether the extracted ERP information constitutes vouching evidence for the ERP item.</claim-text></claim-text></claim></claims></us-patent-application>