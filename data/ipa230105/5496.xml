<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005497A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005497</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17855637</doc-number><date>20220630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>63</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>63</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR GENERATING TRAILERS FOR AUDIO CONTENT</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217603</doc-number><date>20210701</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Spotify AB</orgname><address><city>Stockholm</city><country>SE</country></address></addressbook><residence><country>SE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ZHU</last-name><first-name>Xingran</first-name><address><city>Stockholm</city><country>SE</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>KARLGREN</last-name><first-name>Jussi Jerker</first-name><address><city>Stockholm</city><country>SE</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>TANVEER</last-name><first-name>Md. Iftekhar</first-name><address><city>Revere</city><state>MA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An electronic device receives an audio file and divides the audio file into a plurality of segments. The electronic device, automatically, without user input, determines, for each segment, a descriptor from a plurality of descriptors and a value of the descriptor for the segment. The electronic device selects one or more segments of the plurality of segments, based on a comparison of the respective values of respective descriptors for respective segments and genre-specific criteria selected based on a genre of the audio file. The electronic device generates a trailer for the audio file using the selected one or more segments.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="99.14mm" wi="158.75mm" file="US20230005497A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="248.67mm" wi="163.83mm" orientation="landscape" file="US20230005497A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="224.20mm" wi="162.05mm" file="US20230005497A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="246.13mm" wi="158.83mm" file="US20230005497A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="253.15mm" wi="169.76mm" orientation="landscape" file="US20230005497A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="260.52mm" wi="168.74mm" orientation="landscape" file="US20230005497A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="245.11mm" wi="153.84mm" file="US20230005497A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="250.44mm" wi="153.84mm" file="US20230005497A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">PRIORITY APPLICATION</heading><p id="p-0002" num="0001">This application claims priority to U.S. Prov. Appl. No. 63/217,603, filed Jul. 1, 2021, which is hereby incorporated by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The disclosed embodiments relate generally to media provider systems, and, in particular, to automatically generating a trailer for a media item based on an evaluation of characteristics of the media item, including a targeted genre of the media item.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Recent years have shown a remarkable growth in consumption of digital goods such as digital music, movies, books, and podcasts, among many others. The overwhelmingly large number of these goods often makes the choice of consumers an extremely difficult task. To cope with the constantly growing complexity of making such a choice, the users typically rely on summaries or trailers of the content. Typically, generation of trailers is performed manually and provides an introduction or a summary of the content, rather than showcasing examples from the content that best represent a mood or energy of the content as a whole.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">While many trailer generation systems for media items require manual input from a user and/or provide a simple summary of the content of the media item, it is difficult for these systems to capture samples of the media item that best represent the stylistic qualities of the media item as a whole, and to include those samples in the trailer. Thus, there is a need for a trailer generation system that automatically, without requiring user input, determines portions from a media item to include in a trailer and arranges the portions in such a way that the overall stylistic qualities (e.g., emotion or feeling) of the media item is reflected in the trailer.</p><p id="p-0006" num="0005">In the disclosed embodiments, systems and methods are provided for automatically generating a trailer for an audio item, such as a podcast. The system takes the audio file for the audio item, and, using a parallel neural network, determines segments from the audio file that best capture a vibe (e.g., emotion, mood, and/or energy) of the podcast (e.g., based on the genre of the podcast). For example, for a &#x201c;true crime&#x201d; genre podcast, the system identifies segments of the podcast episode that best capture a &#x201c;fear&#x201d; and/or &#x201c;anger&#x201d; vibe. The system optionally combines the identified segments that capture a vibe of the podcast with one or more additional portions of the podcast (e.g., an introductory segment, musical segments, etc.) to generate a podcast trailer for the podcast.</p><p id="p-0007" num="0006">In accordance with some embodiments, a method is provided. The method includes receiving an audio file and dividing the audio file into a plurality of segments. The method further includes, automatically, without user input, determining, for each segment, a descriptor from a plurality of descriptors and a value of the descriptor for the segment. The method includes selecting one or more segments of the plurality of segments, based on a comparison of the respective values of respective descriptors for respective segments and genre-specific criteria selected based on a genre of the audio file. The method further includes generating a trailer for the audio file using the selected one or more segments.</p><p id="p-0008" num="0007">In accordance with some embodiments, a computer system is provided. The computer system includes one or more processors and memory storing one or more programs. The one or more programs include instructions for performing any of the methods described herein.</p><p id="p-0009" num="0008">In accordance with some embodiments, a non-transitory computer-readable storage medium is provided. The non-transitory computer-readable storage medium stores one or more programs for execution by a computer system with one or more processors. The one or more programs comprising instructions for performing any of the methods described herein.</p><p id="p-0010" num="0009">Thus, systems are provided with improved methods for generating trailers for media items.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010">The embodiments disclosed herein are illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings. Like reference numerals refer to corresponding parts throughout the drawings and specification.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a media content delivery system, in accordance with some embodiments.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating an electronic device, in accordance with some embodiments.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating a media content server, in accordance with some embodiments.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b>A-<b>4</b>B</figref> are block diagrams illustrating systems for generating trailers for an audio file using hotspot detection, in accordance with some embodiments.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> are flow diagrams illustrating a method for generating trailers for an audio file, in accordance with some embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0017" num="0016">Reference will now be made to embodiments, examples of which are illustrated in the accompanying drawings. In the following description, numerous specific details are set forth in order to provide an understanding of the various described embodiments. However, it will be apparent to one of ordinary skill in the art that the various described embodiments may be practiced without these specific details. In other instances, well-known methods, procedures, components, circuits, and networks have not been described in detail so as not to unnecessarily obscure aspects of the embodiments.</p><p id="p-0018" num="0017">It will also be understood that, although the terms first, second, etc. are, in some instances, used herein to describe various elements, these elements should not be limited by these terms. These terms are used only to distinguish one element from another. For example, a first electronic device could be termed a second electronic device, and, similarly, a second electronic device could be termed a first electronic device, without departing from the scope of the various described embodiments. The first electronic device and the second electronic device are both electronic devices, but they are not the same electronic device.</p><p id="p-0019" num="0018">The terminology used in the description of the various embodiments described herein is for the purpose of describing particular embodiments only and is not intended to be limiting. As used in the description of the various described embodiments and the appended claims, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will also be understood that the term &#x201c;and/or&#x201d; as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items. It will be further understood that the terms &#x201c;includes,&#x201d; &#x201c;including,&#x201d; &#x201c;comprises,&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.</p><p id="p-0020" num="0019">As used herein, the term &#x201c;if&#x201d; is, optionally, construed to mean &#x201c;when&#x201d; or &#x201c;upon&#x201d; or &#x201c;in response to determining&#x201d; or &#x201c;in response to detecting&#x201d; or &#x201c;in accordance with a determination that,&#x201d; depending on the context. Similarly, the phrase &#x201c;if it is determined&#x201d; or &#x201c;if [a stated condition or event] is detected&#x201d; is, optionally, construed to mean &#x201c;upon determining&#x201d; or &#x201c;in response to determining&#x201d; or &#x201c;upon detecting [the stated condition or event]&#x201d; or &#x201c;in response to detecting [the stated condition or event]&#x201d; or &#x201c;in accordance with a determination that [a stated condition or event] is detected,&#x201d; depending on the context.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a media content delivery system <b>100</b>, in accordance with some embodiments. The media content delivery system <b>100</b> includes one or more electronic devices <b>102</b> (e.g., electronic device <b>102</b>-<b>1</b> to electronic device <b>102</b>-<i>m</i>, where m is an integer greater than one), one or more media content servers <b>104</b>, and/or one or more content distribution networks (CDNs) <b>106</b>. The one or more media content servers <b>104</b> are associated with (e.g., at least partially compose) a media-providing service. The one or more CDNs <b>106</b> store and/or provide one or more content items (e.g., to electronic devices <b>102</b>). In some embodiments, the CDNs <b>106</b> are included in the media content servers <b>104</b>. One or more networks <b>112</b> communicably couple the components of the media content delivery system <b>100</b>. In some embodiments, the one or more networks <b>112</b> include public communication networks, private communication networks, or a combination of both public and private communication networks. For example, the one or more networks <b>112</b> can be any network (or combination of networks) such as the Internet, other wide area networks (WAN), local area networks (LAN), virtual private networks (VPN), metropolitan area networks (MAN), peer-to-peer networks, and/or ad-hoc connections.</p><p id="p-0022" num="0021">In some embodiments, an electronic device <b>102</b> is associated with one or more users. In some embodiments, an electronic device <b>102</b> is a personal computer, mobile electronic device, wearable computing device, laptop computer, tablet computer, mobile phone, feature phone, smart phone, an infotainment system, digital media player, a speaker, television (TV), digital versatile disk (DVD) player, and/or any other electronic device capable of presenting media content (e.g., controlling playback of media items, such as music tracks, videos, etc.). Electronic devices <b>102</b> may connect to each other wirelessly and/or through a wired connection (e.g., directly through an interface, such as an HDMI interface). In some embodiments, electronic devices <b>102</b>-<b>1</b> and <b>102</b>-<i>m </i>are the same type of device (e.g., electronic device <b>102</b>-<b>1</b> and electronic device <b>102</b>-<i>m </i>are both speakers). Alternatively, electronic device <b>102</b>-<b>1</b> and electronic device <b>102</b>-<i>m </i>include two or more different types of devices.</p><p id="p-0023" num="0022">In some embodiments, electronic devices <b>102</b>-<b>1</b> and <b>102</b>-<i>m </i>send and receive media-control information through network(s) <b>112</b>. For example, electronic devices <b>102</b>-<b>1</b> and <b>102</b>-<i>m </i>send media control requests (e.g., requests to play music, movies, videos, or other media items, or playlists thereof) to media content server <b>104</b> through network(s) <b>112</b>. Additionally, electronic devices <b>102</b>-<b>1</b> and <b>102</b>-<i>m</i>, in some embodiments, also send indications of media content items to media content server <b>104</b> through network(s) <b>112</b>. In some embodiments, the media content items are uploaded to electronic devices <b>102</b>-<b>1</b> and <b>102</b>-<i>m </i>before the electronic devices forward the media content items to media content server <b>104</b>.</p><p id="p-0024" num="0023">In some embodiments, electronic device <b>102</b>-<b>1</b> communicates directly with electronic device <b>102</b>-<i>m </i>(e.g., as illustrated by the dotted-line arrow), or any other electronic device <b>102</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, electronic device <b>102</b>-<b>1</b> is able to communicate directly (e.g., through a wired connection and/or through a short-range wireless signal, such as those associated with personal-area-network (e.g., BLUETOOTH/BLE) communication technologies, radio-frequency-based near-field communication technologies, infrared communication technologies, etc.) with electronic device <b>102</b>-<i>m</i>. In some embodiments, electronic device <b>102</b>-<b>1</b> communicates with electronic device <b>102</b>-<i>m </i>through network(s) <b>112</b>. In some embodiments, electronic device <b>102</b>-<b>1</b> uses the direct connection with electronic device <b>102</b>-<i>m </i>to stream content (e.g., data for media items) for playback on the electronic device <b>102</b>-<i>m. </i></p><p id="p-0025" num="0024">In some embodiments, electronic device <b>102</b>-<b>1</b> and/or electronic device <b>102</b>-<i>m </i>include a media application <b>222</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) that allows a respective user of the respective electronic device to upload (e.g., to media content server <b>104</b>), browse, request (e.g., for playback at the electronic device <b>102</b>), and/or present media content (e.g., control playback of music tracks, videos, etc.). In some embodiments, one or more media content items are stored locally by an electronic device <b>102</b> (e.g., in memory <b>212</b> of the electronic device <b>102</b>, <figref idref="DRAWINGS">FIG. <b>2</b></figref>). In some embodiments, one or more media content items are received by an electronic device <b>102</b> in a data stream (e.g., from the CDN <b>106</b> and/or from the media content server <b>104</b>). The electronic device(s) <b>102</b> are capable of receiving media content (e.g., from the CDN <b>106</b>) and presenting the received media content. For example, electronic device <b>102</b>-<b>1</b> may be a component of a network-connected audio/video system (e.g., a home entertainment system, a radio/alarm clock with a digital display, or an infotainment system of a vehicle). In some embodiments, the CDN <b>106</b> sends media content to the electronic device(s) <b>102</b>.</p><p id="p-0026" num="0025">In some embodiments, the CDN <b>106</b> stores and provides media content (e.g., media content requested by the media application <b>222</b> of electronic device <b>102</b>) to electronic device <b>102</b> via the network(s) <b>112</b>. Content (also referred to herein as &#x201c;media items,&#x201d; &#x201c;media content items,&#x201d; and &#x201c;content items&#x201d;) is received, stored, and/or served by the CDN <b>106</b>. In some embodiments, content includes audio (e.g., music, spoken word, podcasts, etc.), video (e.g., short-form videos, music videos, television shows, movies, clips, previews, etc.), text (e.g., articles, blog posts, emails, etc.), image data (e.g., image files, photographs, drawings, renderings, etc.), games (e.g., 2- or 3-dimensional graphics-based computer games, etc.), or any combination of content types (e.g., web pages that include any combination of the foregoing types of content or other content not explicitly listed). In some embodiments, content includes one or more audio media items (also referred to herein as &#x201c;audio items,&#x201d; &#x201c;tracks,&#x201d; and/or &#x201c;audio tracks&#x201d;).</p><p id="p-0027" num="0026">In some embodiments, media content server <b>104</b> receives media requests (e.g., commands) from electronic devices <b>102</b>. In some embodiments, media content server <b>104</b> includes a voice API, a connect API, and/or key service. In some embodiments, media content server <b>104</b> validates (e.g., using key service) electronic devices <b>102</b> by exchanging one or more keys (e.g., tokens) with electronic device(s) <b>102</b>.</p><p id="p-0028" num="0027">In some embodiments, media content server <b>104</b> and/or CDN <b>106</b> stores one or more playlists (e.g., information indicating a set of media content items). For example, a playlist is a set of media content items defined by a user and/or defined by an editor associated with a media-providing service. The description of the media content server <b>104</b> as a &#x201c;server&#x201d; is intended as a functional description of the devices, systems, processor cores, and/or other components that provide the functionality attributed to the media content server <b>104</b>. It will be understood that the media content server <b>104</b> may be a single server computer, or may be multiple server computers. Moreover, the media content server <b>104</b> may be coupled to CDN <b>106</b> and/or other servers and/or server systems, or other devices, such as other client devices, databases, content delivery networks (e.g., peer-to-peer networks), network caches, and the like. In some embodiments, the media content server <b>104</b> is implemented by multiple computing devices working together to perform the actions of a server system (e.g., cloud computing).</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating an electronic device <b>102</b> (e.g., electronic device <b>102</b>-<b>1</b> and/or electronic device <b>102</b>-<i>m</i>, <figref idref="DRAWINGS">FIG. <b>1</b></figref>), in accordance with some embodiments. The electronic device <b>102</b> includes one or more central processing units (CPU(s), i.e., processors or cores) <b>202</b>, one or more network (or other communications) interfaces <b>210</b>, memory <b>212</b>, and one or more communication buses <b>214</b> for interconnecting these components. The communication buses <b>214</b> optionally include circuitry (sometimes called a chipset) that interconnects and controls communications between system components.</p><p id="p-0030" num="0029">In some embodiments, the electronic device <b>102</b> includes a user interface <b>204</b>, including output device(s) <b>206</b> and/or input device(s) <b>208</b>. In some embodiments, the input devices <b>208</b> include a keyboard, mouse, or track pad. Alternatively, or in addition, in some embodiments, the user interface <b>204</b> includes a display device that includes a touch-sensitive surface, in which case the display device is a touch-sensitive display. In electronic devices that have a touch-sensitive display, a physical keyboard is optional (e.g., a soft keyboard may be displayed when keyboard entry is needed). In some embodiments, the output devices (e.g., output device(s) <b>206</b>) include a speaker <b>252</b> (e.g., speakerphone device) and/or an audio jack <b>250</b> (or other physical output connection port) for connecting to speakers, earphones, headphones, or other external listening devices. Furthermore, some electronic devices <b>102</b> use a microphone and voice recognition device to supplement or replace the keyboard. Optionally, the electronic device <b>102</b> includes an audio input device (e.g., a microphone) to capture audio (e.g., speech from a user).</p><p id="p-0031" num="0030">Optionally, the electronic device <b>102</b> includes a location-detection device <b>240</b>, such as a global navigation satellite system (GNSS) (e.g., GPS (global positioning system), GLONASS, Galileo, BeiDou) or other geo-location receiver, and/or location-detection software for determining the location of the electronic device <b>102</b> (e.g., module for finding a position of the electronic device <b>102</b> using trilateration of measured signal strengths for nearby devices).</p><p id="p-0032" num="0031">In some embodiments, the one or more network interfaces <b>210</b> include wireless and/or wired interfaces for receiving data from and/or transmitting data to other electronic devices <b>102</b>, a media content server <b>104</b>, a CDN <b>106</b>, and/or other devices or systems. In some embodiments, data communications are carried out using any of a variety of custom or standard wireless protocols (e.g., NFC, RFID, IEEE 802.15.4, Wi-Fi, ZigBee, 6LoWPAN, Thread, Z-Wave, Bluetooth, ISA100.11a, WirelessHART, MiWi, etc.). Furthermore, in some embodiments, data communications are carried out using any of a variety of custom or standard wired protocols (e.g., USB, Firewire, Ethernet, etc.). For example, the one or more network interfaces <b>210</b> include a wireless interface <b>260</b> for enabling wireless data communications with other electronic devices <b>102</b>, media presentations systems <b>108</b>, and/or or other wireless (e.g., Bluetooth-compatible) devices (e.g., for streaming audio data to the media presentations system <b>108</b> of an automobile). Furthermore, in some embodiments, the wireless interface <b>260</b> (or a different communications interface of the one or more network interfaces <b>210</b>) enables data communications with other WLAN-compatible devices (e.g., a media presentations system <b>108</b>) and/or the media content server <b>104</b> (via the one or more network(s) <b>112</b>, <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0033" num="0032">In some embodiments, electronic device <b>102</b> includes one or more sensors including, but not limited to, accelerometers, gyroscopes, compasses, magnetometer, light sensors, near field communication transceivers, barometers, humidity sensors, temperature sensors, proximity sensors, range finders, and/or other sensors/devices for sensing and measuring various environmental conditions.</p><p id="p-0034" num="0033">Memory <b>212</b> includes high-speed random-access memory, such as DRAM, SRAM, DDR RAM, or other random-access solid-state memory devices; and may include non-volatile memory, such as one or more magnetic disk storage devices, optical disk storage devices, flash memory devices, or other non-volatile solid-state storage devices. Memory <b>212</b> may optionally include one or more storage devices remotely located from the CPU(s) <b>202</b>. Memory <b>212</b>, or alternately, the non-volatile memory solid-state storage devices within memory <b>212</b>, includes a non-transitory computer-readable storage medium. In some embodiments, memory <b>212</b> or the non-transitory computer-readable storage medium of memory <b>212</b> stores the following programs, modules, and data structures, or a subset or superset thereof:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0034">an operating system <b>216</b> that includes procedures for handling various basic system services and for performing hardware-dependent tasks;</li>        <li id="ul0002-0002" num="0035">network communication module(s) <b>218</b> for connecting the client device <b>102</b> to other computing devices (e.g., media presentation system(s) <b>108</b>, media content server <b>104</b>, and/or other client devices) via the one or more network interface(s) <b>210</b> (wired or wireless) connected to one or more network(s) <b>112</b>;</li>        <li id="ul0002-0003" num="0036">a user interface module <b>220</b> that receives commands and/or inputs from a user via the user interface <b>204</b> (e.g., from the input devices <b>208</b>) and provides outputs for playback and/or display on the user interface <b>204</b> (e.g., the output devices <b>206</b>);</li>        <li id="ul0002-0004" num="0037">a media application <b>222</b> (e.g., an application for accessing a media-providing service of a media content provider associated with media content server <b>104</b>) for uploading, browsing, receiving, processing, presenting, and/or requesting playback of media (e.g., media items). In some embodiments, media application <b>222</b> includes a media player, a streaming media application, and/or any other appropriate application or component of an application. In some embodiments, media application <b>222</b> is used to monitor, store, and/or transmit (e.g., to media content server <b>104</b>) data associated with user behavior. In some embodiments, media application <b>222</b> also includes the following modules (or sets of instructions), or a subset or superset thereof:        <ul id="ul0003" list-style="none">            <li id="ul0003-0001" num="0038">a trailer generation module <b>224</b> for selecting one or more segments of a media content item to include in a trailer and/or compiling one or more segments of the media content item to generate a trailer for respective media items;</li>            <li id="ul0003-0002" num="0039">a genre criteria module <b>226</b> for identifying and/or storing genre information for respective media items, including genre-specific criteria used to select segments to be included in a trailer generated using trailer generation module <b>224</b>;</li>            <li id="ul0003-0003" num="0040">a content items module <b>228</b> for storing media items, including audio items such as podcasts, for playback and/or for forwarding requests for media content items to the media content server. In some embodiments, content items module <b>228</b> associates (e.g., stores) a trailer associated with a respective media item and provides the trailer for playback;</li>        </ul>        </li>        <li id="ul0002-0005" num="0041">a web browser application <b>234</b> for accessing, viewing, and interacting with web sites; and</li>        <li id="ul0002-0006" num="0042">other applications <b>236</b>, such as applications for word processing, calendaring, mapping, weather, stocks, time keeping, virtual digital assistant, presenting, number crunching (spreadsheets), drawing, instant messaging, e-mail, telephony, video conferencing, photo management, video management, a digital music player, a digital video player, 2D gaming, 3D (e.g., virtual reality) gaming, electronic book reader, and/or workout support.</li>    </ul>    </li></ul></p><p id="p-0035" num="0043"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating a media content server <b>104</b>, in accordance with some embodiments. The media content server <b>104</b> typically includes one or more central processing units/cores (CPUs) <b>302</b>, one or more network interfaces <b>304</b>, memory <b>306</b>, and one or more communication buses <b>308</b> for interconnecting these components.</p><p id="p-0036" num="0044">Memory <b>306</b> includes high-speed random access memory, such as DRAM, SRAM, DDR RAM, or other random access solid-state memory devices; and may include non-volatile memory, such as one or more magnetic disk storage devices, optical disk storage devices, flash memory devices, or other non-volatile solid-state storage devices. Memory <b>306</b> optionally includes one or more storage devices remotely located from one or more CPUs <b>302</b>. Memory <b>306</b>, or, alternatively, the non-volatile solid-state memory device(s) within memory <b>306</b>, includes a non-transitory computer-readable storage medium. In some embodiments, memory <b>306</b>, or the non-transitory computer-readable storage medium of memory <b>306</b>, stores the following programs, modules and data structures, or a subset or superset thereof:<ul id="ul0004" list-style="none">    <li id="ul0004-0001" num="0000">    <ul id="ul0005" list-style="none">        <li id="ul0005-0001" num="0045">an operating system <b>310</b> that includes procedures for handling various basic system services and for performing hardware-dependent tasks;</li>        <li id="ul0005-0002" num="0046">a network communication module <b>312</b> that is used for connecting the media content server <b>104</b> to other computing devices via one or more network interfaces <b>304</b> (wired or wireless) connected to one or more networks <b>112</b>;</li>        <li id="ul0005-0003" num="0047">one or more server application modules <b>314</b> for performing various functions with respect to providing and managing a content service, the server application modules <b>314</b> including, but not limited to, one or more of:        <ul id="ul0006" list-style="none">            <li id="ul0006-0001" num="0048">a media content module <b>316</b> for storing one or more media content items and/or sending (e.g., streaming), to the electronic device, one or more requested media content item(s);</li>            <li id="ul0006-0002" num="0049">trailer generation module <b>318</b> for generating and/or providing (e.g., streaming) trailers of media content items, including but not limited to, one or more of:            <ul id="ul0007" list-style="none">                <li id="ul0007-0001" num="0050">segmentation module <b>320</b> for dividing a media content item into a plurality of segments (e.g., audio segments and/or textual segments);</li>                <li id="ul0007-0002" num="0051">detection module(s) <b>322</b>, including one or more of a hotspot detection module, a topical center detection module, a music detection module, and a predefined audio type detection module for detecting features of the segments and selecting which segments, from the plurality of segments generated by segmentation module <b>320</b>, are enabled to be included in a generated trailer;</li>                <li id="ul0007-0003" num="0052">genre criteria module <b>324</b> for storing and/or updating criteria, such as descriptors selected based on a genre of the media content item, to be applied to the segments in order to select which segment(s) to include in the generated trailer; and</li>            </ul>            </li>        </ul>        </li>        <li id="ul0005-0004" num="0053">one or more server data module(s) <b>330</b> for handling the storage of and/or access to media items and/or metadata relating to the media items; in some embodiments, the one or more server data module(s) <b>330</b> include:        <ul id="ul0008" list-style="none">            <li id="ul0008-0001" num="0054">a media content database <b>332</b> for storing media items; and</li>            <li id="ul0008-0002" num="0055">a metadata database <b>334</b> for storing metadata relating to the media items, including a genre associated with the respective media items.</li>        </ul>        </li>    </ul>    </li></ul></p><p id="p-0037" num="0056">In some embodiments, the media content server <b>104</b> includes web or Hypertext Transfer Protocol (HTTP) servers, File Transfer Protocol (FTP) servers, as well as web pages and applications implemented using Common Gateway Interface (CGI) script, PHP Hyper-text Preprocessor (PHP), Active Server Pages (ASP), Hyper Text Markup Language (HTML), Extensible Markup Language (XML), Java, JavaScript, Asynchronous JavaScript and XML (AJAX), XHP, Javelin, Wireless Universal Resource File (WURFL), and the like.</p><p id="p-0038" num="0057">Each of the above identified modules stored in memory <b>212</b> and <b>306</b> corresponds to a set of instructions for performing a function described herein. The above identified modules or programs (i.e., sets of instructions) need not be implemented as separate software programs, procedures, or modules, and thus various subsets of these modules may be combined or otherwise re-arranged in various embodiments. In some embodiments, memory <b>212</b> and <b>306</b> optionally store a subset or superset of the respective modules and data structures identified above. Furthermore, memory <b>212</b> and <b>306</b> optionally store additional modules and data structures not described above.</p><p id="p-0039" num="0058">Although <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the media content server <b>104</b> in accordance with some embodiments, <figref idref="DRAWINGS">FIG. <b>3</b></figref> is intended more as a functional description of the various features that may be present in one or more media content servers than as a structural schematic of the embodiments described herein. In practice, and as recognized by those of ordinary skill in the art, items shown separately could be combined and some items could be separated. For example, some items shown separately in <figref idref="DRAWINGS">FIG. <b>3</b></figref> could be implemented on single servers and single items could be implemented by one or more servers. In some embodiments, media content database <b>332</b> and/or metadata database <b>334</b> are stored on devices (e.g., CDN <b>106</b>) that are accessed by media content server <b>104</b>. The actual number of servers used to implement the media content server <b>104</b>, and how features are allocated among them, will vary from one implementation to another and, optionally, depends in part on the amount of data traffic that the server system handles during peak usage periods as well as during average usage periods.</p><p id="p-0040" num="0059"><figref idref="DRAWINGS">FIGS. <b>4</b>A-<b>4</b>B</figref> are block diagrams illustrating generation of a trailer for an audio item. <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates a hotspot detection system <b>400</b>. <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates a trailer generation system <b>450</b>, a portion of which includes the hotspot detection system described with reference to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>. Accordingly, <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates additional details for the system detecting hotspots, and <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates the hotspot detection system implemented with additional detection systems (e.g., topical center detection <b>452</b>, music detection <b>454</b>, and predefined audio type detection <b>456</b>).</p><p id="p-0041" num="0060">The hotspot detection system <b>400</b> divides a media content item (e.g., audio file <b>401</b>) into a plurality of segments <b>402</b> (e.g., segment <b>402</b>-<b>1</b>, segment <b>402</b>-<b>2</b>, segment <b>402</b>-<b>3</b>, etc.). In some embodiments, the plurality of segments <b>402</b> includes a plurality of textual segments (e.g., each textual segment corresponding a transcript of the audio for the segment) and/or a plurality of audio segments for the entirety of the media content item.</p><p id="p-0042" num="0061">In some embodiments, the system obtains a transcript of the audio file as an input. In some embodiments, the system transcribes the media content item (e.g., using an internal tool). For example, audio file <b>401</b> is a podcast and the system <b>400</b> obtains the transcript and the audio for the podcast. In some embodiments, the system divides the media item into textual segments and/or audio segments based on the transcript of the audio file. For example, each segment corresponds to a length of a sentence of the transcript. In some embodiments, the system produces textual segments and audio segments for the audio file <b>401</b>. For example, respective textual segments and respective audio segments are synchronized (e.g., based on timing of the audio file <b>401</b>). In some embodiments, each segment is a predefined length (e.g., or within a range of predefined lengths). In the example described below, segments <b>402</b>-<b>1</b>, <b>402</b>-<b>2</b> and <b>402</b>-<b>3</b> are audio segments.</p><p id="p-0043" num="0062">In some embodiments, for each segment (e.g., each audio segment), the system determines (<b>403</b>) a descriptor and a value of a strength of the descriptor (<b>410</b>, &#x201c;descriptor1, 0.86&#x201d;) for the segment. It will be understood that the processes described in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> may be performed on audio segments and/or textual segments of the audio file <b>401</b>.</p><p id="p-0044" num="0063">In some embodiments, determining the descriptor and the value of the strength of the descriptor includes determining a descriptor and a value of the strength of the descriptor for a plurality of time windows (e.g., a rolling time window) within the segment. In some embodiments, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, a rolling time window <b>405</b> (e.g., of predefined length, such as a 3-second rolling time window) is applied to a first segment <b>402</b>-<b>1</b>. In some embodiments, for each portion (e.g., comprising a window of time) of the first audio segment in the rolling time window, a representation of the audio segment (e.g., a Mel spectrogram or other two-dimensional array) is generated, and the representation of the audio segment is provided to a parallel neural network, which includes providing the representation of the audio segment, in parallel, to a sequenced convolutional neural network (CNN) <b>404</b> and a transformer neural network <b>406</b>. In some embodiments, the sequenced CNN <b>404</b> performs max pooling, a series of convolutions (e.g., <b>4</b> convolutions) and flattens the result, and the transformer neural network <b>406</b> comprises a series of encoders used to determine a time-domain mean. In some embodiments, the outputs of the sequenced CNN <b>404</b> and the transformer neural network <b>406</b> are then concatenated (e.g., as represented by the &#x201c;+&#x201d; in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>).</p><p id="p-0045" num="0064">In some embodiments, the concatenated outputs are then used to determine (e.g., using softmax), for each rolling time window <b>408</b> (e.g., <b>408</b>-<b>1</b>, <b>408</b>-<b>2</b>, <b>408</b>-<b>3</b>, <b>408</b>-<b>4</b>, etc), a descriptor selected from a set of descriptors and a value for the descriptor (e.g., a value for the strength of the descriptor). For example, values are calculated for each descriptor in the set of descriptors for the rolling time window and the descriptor with the greatest value is determined as the descriptor for the respective rolling time window. In some embodiments, the set of descriptors are predefined by a user. In some embodiments, the set of descriptors comprises a set of emotions. For example, the set of descriptors comprises two or more of: joy, surprise, anger, fear, disgust, sad, and neutral. It will be understood that additional or alternative descriptors may also be used.</p><p id="p-0046" num="0065">For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, segment <b>402</b>-<b>1</b> is divided into sub-portions (e.g., windows of time) <b>408</b> as the rolling time windows <b>405</b> are applied to the segment <b>402</b>-<b>1</b>. Each sub-portion <b>408</b> is assigned a descriptor from the set of descriptors (and, in some embodiments, a value of the descriptor of the sub-portion describing how strongly that descriptor correlates to the sub-portion). For example, sub-portion <b>408</b>-<b>1</b> is assigned a first descriptor (e.g., fear), sub-portion <b>408</b>-<b>2</b> is assigned a second descriptor (which can be the same descriptor or a different descriptor than the first descriptor) (e.g., fear), sub-portion <b>408</b>-<b>3</b> is assigned a third descriptor (which can be the same descriptor or a different descriptor as the first and/or second descriptors) (e.g., anger) and sub-portion <b>408</b>-<b>4</b> is assigned a fourth descriptor (which can be the same descriptor or a different descriptor as the first, second and/or third descriptors) (e.g., disgust), and so on. Although the sub-portions <b>408</b> illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> do not appear to overlap, it will be understood that in some embodiments, the rolling time window <b>405</b> is moved along segment <b>402</b>-<b>1</b> at intervals shorter than the sub-portions (e.g., windows of time) <b>408</b> such that the sub-portions overlap with each other (e.g., the rolling window is 3-seconds and a new window is established at 1-second intervals such that the a subsequent sub-portion overlaps a current sub-portion by 2-seconds).</p><p id="p-0047" num="0066">After determining a descriptor (and value of the descriptor) for each sub-portion (time window) <b>408</b> for the segment <b>402</b>-<b>1</b>, a single descriptor is determined for the overall segment (e.g., segment <b>402</b>-<b>1</b>). For example, the descriptor for the segment is determined based on the descriptor that was assigned most often to the sub-portions of the segment <b>402</b>-<b>1</b>.</p><p id="p-0048" num="0067">In some embodiments, a value for the descriptor for the overall segment <b>402</b>-<b>1</b> is also determined (e.g., as an average of the values of the descriptor determined for each sub-portion of the segment). In some embodiments, the values of the descriptors are determined using the neural network (e.g., by generating a linear combination of the outputs of CNN <b>404</b> and transformer NN <b>406</b> and applying an activation function, such as Softmax). For example, descriptor1 with a value of 0.86 is determined (<b>410</b>) for segment <b>402</b>-<b>1</b>. In some embodiments, the value of the descriptor indicates how strongly that descriptor represents the segment (e.g., a higher value corresponds to a stronger representation of the descriptor for the segment).</p><p id="p-0049" num="0068">In some embodiments, set of labeled segments <b>411</b> includes the plurality of segments <b>402</b>-<b>1</b> through <b>402</b>-<i>n </i>(where n is an integer greater than 2) for the audio file <b>401</b> (e.g., each segment of the audio file <b>401</b> is labeled with a descriptor and a value of the descriptor via process <b>403</b>). For example, process <b>403</b> is repeated for each segment in the set of segments <b>402</b>. In some embodiments, for each segment in the set of labeled segments <b>411</b>, a descriptor and a value of the descriptor is assigned to the segment.</p><p id="p-0050" num="0069">After assigning a respective descriptor and a respective value of the descriptor to each of the segments, the system applies genre criteria <b>412</b> to the segments. In some embodiments, genre criteria <b>412</b> define target descriptor(s) that correlate to a particular genre (e.g., wherein the target descriptor(s) are used to identify hotspots for the media item). For example, for a &#x201c;true crime&#x201d; genre, the genre criteria <b>412</b> define that &#x201c;fear&#x201d; is the target descriptor. In some embodiments, a user is enabled to select the genre (and/or genre criteria) to be applied to the audio file <b>401</b>. In some embodiments, the genre (and genre criteria) of the audio file <b>401</b> are automatically determined by the system based on a classification scheme (e.g., without user input). In some embodiments, a user is enabled to update the genre and/or genre criteria in order to change the target descriptor(s). For example, a content creator is enabled to update the genre criteria such that the system identifies hotspots that have been tagged with a &#x201c;joy&#x201d; descriptor even though the genre of the audio file is identified as a &#x201c;true crime&#x201d; genre (instead of using the default target descriptor(s) for the genre, such as &#x201c;fear&#x201d;).</p><p id="p-0051" num="0070">After applying the genre criteria <b>412</b> to the set of labeled segments <b>411</b>, the system identifies a set of hotspot segments <b>413</b>, which includes a subset (e.g., less than all) of the set of labeled segments <b>411</b> that have descriptor(s) that match the genre criteria <b>412</b>. For example, the target descriptor is &#x201c;descriptor1&#x201d; and the set of hotspot segments <b>413</b> only includes segments that have been assigned &#x201c;descriptor1&#x201d; as the descriptor. In some embodiments, the set of hotspot segments <b>413</b> are ordered based on the value of the target descriptor (e.g., instead of being ordered based on timing of the segment within the audio file). For example, segment <b>402</b>-<b>8</b> has the greatest value of descriptor1 of 1.95, while segment <b>402</b>-<b>5</b> has the second greatest value of descriptor1 of 1.67, etc. It will be understood that in some embodiments, the set of hotspot segments <b>413</b> are ordered based on the timing of the segment within the audio file (e.g., the segments are presented in the order in which they appear within the audio file).</p><p id="p-0052" num="0071">In some embodiments, the set of hotspot segments <b>413</b> are further filtered (not shown) to remove segments that are unwanted in the generated trailer for the audio file. For example, in addition to applying genre criteria <b>412</b>, one or more additional filters are applied to the set of labeled segments <b>411</b> and/or to the set of segments <b>413</b> (e.g., the one or more additional filters can be applied to either set to concurrently apply the filters with the genre criteria <b>412</b> or to apply the one or more additional filters after applying the genre criteria <b>412</b>). In some embodiments, the filters remove segments that include spoilers, inappropriate language, or other content that a user does not want to include in the generated trailer from the set of hotspot segments <b>413</b>. In some embodiments, a user is enabled to control types of segments to be removed from the set of hotspot segments (e.g., a content creator is enabled to exclude profanity from the trailer). For example, the user is enabled to select various filters to apply to the set of segments.</p><p id="p-0053" num="0072">In some embodiments, after detecting the set of hotspot segments <b>413</b>, the system selects one or more hotspot segments to include in the trailer. For example, the system selects the hotspot segments with the greatest value of the target descriptor (or other genre criteria).</p><p id="p-0054" num="0073"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates trailer generation system <b>450</b>, which determines one or more additional components (in addition to the hotspot detection described in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>) of the audio file to include in a generated trailer <b>460</b>. For example, in addition to the hotspot detection described above with reference to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the system is also enabled to determine a topic (e.g., summary) of the audio file (e.g., topical center detection <b>452</b>). For example, the system identifies a segment from the audio file <b>401</b> that best represents a topic of the audio file to be included as an introduction (or summary) of the audio file <b>401</b> within the trailer <b>460</b> for the audio file.</p><p id="p-0055" num="0074">In some embodiments, the topical center detection <b>452</b> is performed by generating embeddings for each segment (e.g., segments <b>402</b>), where the segments comprise audio segments or textual segments (e.g., from a transcript corresponding to the audio file), as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>. In some embodiments, the embeddings for each segments are compared with embeddings of a document (e.g., such as a description of the audio file (e.g., a podcast episode description)) that is received as an input to the system for topical center detection <b>452</b>. In some embodiments, a similarity score is calculated for each segment (e.g., each segment corresponding to a sentence) as compared with the input document. In some embodiments, the segment that has the greatest similarity score is selected as the most topically representative for the audio file <b>401</b>. This selected segment is included in the trailer <b>460</b> as the &#x201c;intro&#x201d; segment.</p><p id="p-0056" num="0075">In some embodiments, the system also determines one or more musical portions of the audio file to be included in the trailer <b>460</b> via music detection <b>454</b>. In some embodiments, music detection <b>454</b> identifies a plurality of music segments from audio file <b>401</b> using a neural network and arranges the music segments according to length (e.g., from the longest music segments to the shortest music segments). In some embodiments, the music segments selected to be included in the trailer <b>460</b> comprise the longest music segments. For example, the two &#x201c;music&#x201d; segments illustrated in trailer <b>460</b> comprise the longest two music segments identified from audio file <b>401</b>.</p><p id="p-0057" num="0076">In some embodiments, the system optionally determines one or more portions of a predefined audio type that occurs in the audio file <b>401</b> via predefined audio type detection <b>456</b>. In some embodiments, the predefined audio type (e.g., laughter, applause, explosions, or other types of audio) that is detected for a particular audio file depends on the genre of the audio file <b>401</b>. For example, in accordance with a determination that the audio file is a first genre type, the system detects a first type of predefined audio. For example, for a comedy genre, the system detects laughter and for a sports genre, the system detects applause. In some embodiments, segments that are detected using predefined audio type detection <b>456</b> are used in place of hotspot segments in the trailer <b>460</b> (not shown). For example, instead of including hotspot segments detected using the hotspot detection process described with reference to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the system places segments of the audio file that include the predefined audio type (e.g., laughter, applause, etc.) in trailer <b>460</b>. In some embodiments, the trailer generation system <b>450</b> includes both hotspot segments and segments identified using the predefined audio type detection <b>456</b>.</p><p id="p-0058" num="0077">In some embodiments, trailer generation system <b>450</b> includes time length controller <b>458</b> that analyzes the segments identified from the topical center detection <b>452</b>, hotspot detection (e.g., hotspot segments <b>413</b>), music detection <b>454</b> and/or predefined audio type detection <b>456</b>, and selects one or more of the identified segments to include in the trailer <b>460</b> based on time constraints (e.g., a predefined length of the trailer <b>460</b>). For example, time length controller <b>458</b> determines how many of each type of segment (e.g., hotspot segment, music segment, etc.) to include in trailer <b>460</b> based on the lengths of the segments and the target length for the trailer. In some embodiments, time length controller <b>458</b> arranges the selected segments to generate trailer <b>460</b>. In some embodiments, the arrangement of segments is predefined (e.g., a music segment, then an introductory segment, then a transition segment, then a hotspot segment, etc.). In some embodiments, the transition audio segment is an additional audio segment that is not from the audio file <b>401</b>. For example, the transition audio segments included in trailer <b>460</b> are selected from a group of transition audio segments (e.g., based on a genre of audio file <b>401</b>) to be placed between the selected segments identified by the trailer generation system in order to provide a smooth transition between the selected segments.</p><p id="p-0059" num="0078">It will be understood that the trailer generation system <b>450</b> is enabled to include any combination of segments identified using the system above. For example, the trailer <b>460</b> does not have to include each type of segment identified (e.g., the trailer <b>460</b> may include hotspot segments without including an intro segment).</p><p id="p-0060" num="0079"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> are flow diagrams illustrating a method <b>500</b> of automatically generating a trailer for an audio file, in accordance with some embodiments. Method <b>500</b> may be performed at an electronic device (e.g., media content server <b>104</b> and/or electronic device(s) <b>102</b>) having one or more processors and memory storing instructions for execution by the one or more processors. In some embodiments, the method <b>500</b> is performed by executing instructions stored in the memory (e.g., memory <b>212</b>, <figref idref="DRAWINGS">FIG. <b>2</b></figref>, memory <b>306</b>, <figref idref="DRAWINGS">FIG. <b>3</b></figref>) of the electronic device. In some embodiments, the method <b>500</b> is performed by a combination of the server system (e.g., including media content server <b>104</b> and CDN <b>106</b>) and a client device.</p><p id="p-0061" num="0080">Referring now to <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, in performing the method <b>500</b>, the electronic device receives (<b>502</b>) an audio file (e.g., audio file <b>401</b> illustrated in <figref idref="DRAWINGS">FIGS. <b>4</b>A-<b>4</b>B</figref>).</p><p id="p-0062" num="0081">In some embodiments, the audio file comprises (<b>504</b>) spoken word audio content (e.g., a podcast, an audiobook, etc.). In some embodiments, the audio file is the audio content of a media content item that includes an audio portion and an additional portion (e.g., a video). In some embodiments, the audio file comprises music.</p><p id="p-0063" num="0082">The electronic device divides (<b>506</b>) the audio file into a plurality of segments. For example, as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, audio file <b>401</b> is divided into a plurality of segments <b>402</b> (e.g., segment <b>402</b>-<b>1</b>, segment <b>402</b>-<b>2</b>, segment <b>402</b>-<b>3</b>, etc.). In some embodiments, the entirety of the audio file is divided into segments (e.g., every portion of the audio file is in a segment). In some embodiments, a portion, less than all, of the audio file is divided into segments (e.g., to exclude portions of the audio file that correspond to commercials, introductions, credits, etc.).</p><p id="p-0064" num="0083">In some embodiments, each segment of the plurality of segments corresponds to (<b>508</b>) a sentence in the audio file. For example, the audio file <b>401</b> is mapped to a transcript of the audio file, and dividing the audio file <b>401</b> into segments comprises determining the start (e.g., and/or end) of a sentence (e.g., using the transcript) such that each segment comprises a sentence of the audio file. In some embodiments, the electronic device receives a transcript (e.g., as an additional input) corresponding to the audio file. In some embodiments, the segments <b>402</b> comprise portions of the transcript (e.g., the segments are textual segments). In some embodiments, the segments <b>402</b> comprise portions of audio from the audio file <b>401</b> (e.g., the segments are audio segments) corresponding to sentences. It will be understood that the system is enabled to use audio segments and/or textual segments to determine hotspots, as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>. In some embodiments, the type of segment used to determine the hotspots is based on the type of audio file. For example, a podcast uses audio segments to determine hotspots while an audiobook uses textual segments to determine the hotspots. In some embodiments, the segments <b>402</b> are a predefined length (e.g., or within a predefined range of lengths). For example, each segment has a minimum and/or maximum length (e.g., between 10 seconds and 50 seconds).</p><p id="p-0065" num="0084">The electronic device, automatically, without user input, determines (<b>510</b>), for each segment, a descriptor from a plurality of descriptors and a value of the descriptor for the segment. For example, the descriptors are not assigned to the segments based on user input (e.g., or user feedback). For example, the descriptors are selected using a neural network or other automated process that does not require input from a user. For example, as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the descriptor descriptor1 and a value of 0.86 is determined for segment <b>402</b>-<b>1</b> without user input.</p><p id="p-0066" num="0085">In some embodiments, determining (<b>512</b>) the descriptor and the value of the descriptor comprises using a parallel neural network. For example, as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the parallel neural network comprises sequenced CNN <b>404</b> and transformer neural network <b>406</b> to determine the descriptor and value of the descriptor of the segment (e.g., [descriptor1, 0.86] for segment <b>402</b>-<b>1</b>).</p><p id="p-0067" num="0086">In some embodiments, each descriptor of the plurality of descriptors comprises (<b>514</b>) an emotion selected from a group of emotions. For example, &#x201c;descriptor1&#x201d; corresponds to an emotion, such as &#x201c;fear&#x201d; selected from a group of emotions. In some embodiments, the group of emotions is predefined. For example, as described above, the set of descriptors comprises two or more of: joy, surprise, anger, fear, disgust, sad, and neutral. It will be understood that additional or alternative descriptors may also be used.</p><p id="p-0068" num="0087">In some embodiments, respective values of respective descriptors for respective segments are (<b>516</b>) based on the audio of the audio file (e.g., not the transcription). For example, the descriptor and the value of a segment is based on an audio segment (not a textual segment). For example, while the textual segment corresponding to a transcript of the segment includes particular words, the tone and/or other characteristic of the audio is important to be considered in order to generate a more accurate descriptor. For example, using only the text of a true crime podcast, a sentence may state &#x201c;she arrived at the party&#x201d; which, in text, may produce a descriptor of &#x201c;joy&#x201d; because of the word &#x201c;party,&#x201d; but with the tone of the speaker, this sentence may sound more ominous and be assigned a different descriptor, such as &#x201c;fear.&#x201d; Accordingly, it is important that in some embodiments, only textual segments or only audio segments are used to predict (e.g., determine) the descriptors (and values of the descriptors) for segments. In some embodiments, both the textual segments and audio segments are used to predict the descriptors. In some embodiments, the value of the first descriptor is based on the text file corresponding to the transcript.</p><p id="p-0069" num="0088">In some embodiments, determining the descriptor for each segment comprises (<b>518</b>) applying a rolling time window (e.g., of a predefined length (3 seconds)) to the segment to generate a set of descriptors, each descriptor in the set of descriptors corresponding to a respective time window of the segment. For example, as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, a rolling time window <b>405</b> is applied to segment <b>402</b>-<b>1</b>, and a set of descriptors is determined for each sub-portion <b>408</b> of the segment <b>402</b>-<b>1</b>. In some embodiments, the sub-portions overlap each other. In some embodiments, the rolling time window is shorter than a length of the segment (e.g., the rolling time window is a 3-second window).</p><p id="p-0070" num="0089">The electronic device selects (<b>520</b>) one or more segments (e.g., hotspot segments) of the plurality of segments, based on a comparison of the respective values of respective descriptors for respective segments and genre-specific criteria selected based on a genre of the audio file. (Note that, although the term &#x201c;criteria&#x201d; is used, it should be understood that the genre-specific criteria may include a single criterion, such as a single descriptor, or may include a plurality of criteria). For example, as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the device applies (e.g., compares) the set of labeled segments <b>411</b> with genre criteria <b>412</b> and selects the set of hotspot segments <b>413</b> based on the segments in the set of labeled segments <b>411</b> that match (e.g., have the descriptor(s) of) the genre criteria <b>412</b>.</p><p id="p-0071" num="0090">In some embodiments, the genre-specific criteria comprise (<b>522</b>) one or more descriptors of the plurality of descriptors selected based on a genre of the audio file. For example, the genre-specific criteria comprise an emotion selected from the set of emotions based on the genre of the audio file. For example, different genres of audio files are assigned to different descriptors as the genre-specific criteria (e.g., true crime is assigned &#x201c;fear&#x201d;, while comedy is assigned &#x201c;joy&#x201d;). In some embodiments, the genre-specific criteria comprise a recipe that defines particular characteristics of the segments. In some embodiments, the descriptor is a vector that represents characteristics of the audio, and are matched to corresponding vectors that represent the genre-specific criteria.</p><p id="p-0072" num="0091">In some embodiments, selecting (<b>524</b>) the one or more segments comprises selecting the one or more segments with the highest values of the genre-specific criteria. For example, as described with reference to the set of hotspot segments <b>413</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the set of hotspot segments are ordered based on the value of descriptor1 such that the selected segments are the first predefined number of hotspot segments that have the highest values of descriptor1. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, only segments <b>402</b>-<b>8</b> and <b>402</b>-<b>5</b> from the set of hotspot segments <b>413</b> are selected to be included in trailer <b>460</b>. In some embodiments, the number of segments selected to be included in the trailer <b>460</b> is a predefined number (e.g., 2 segments, 3 segments, etc.). In some embodiments, the number of segments selected to be in the trailer <b>460</b> is based on the length of the selected segments. For example, where the trailer cannot be more than a predefined length (e.g., 1 minute long), the device determines a number of hotspot segments that can be included in the trailer without exceeding the predefined length (e.g., which is further dependent on the length of the hotspot segments).</p><p id="p-0073" num="0092">In some embodiments, the genre-specific criteria are defined (<b>526</b>) (or updated) by a user. For example, a user (e.g., a creator of the audio file <b>401</b>) selects a different recipe, or a different set of genre-specific criteria to generate the trailer. For example, the genre-specific criteria are not automatically selected based on a genre of audio file <b>401</b>. In some embodiments, the genre-specific criteria are automatically selected (e.g., as default criteria) based on the genre of audio file <b>401</b>, but the user is enabled to change the genre-specific criteria (e.g., to select a different descriptor as the criteria).</p><p id="p-0074" num="0093">In some embodiments, selecting the one or more segments is (<b>528</b>) further based on the one or more segments satisfying user-defined criteria. For example, in addition to the genre-specific criteria <b>412</b>, the user is enabled to select additional filters to apply to the set of segments before selecting the segments to include in trailer <b>460</b>. For example, the additional filters include selecting a particular speaker that is identified as speaking in the segment. In some embodiments, the electronic device provides a set of controls that allows the user to implement additional criteria (e.g., or to replace/update the genre-specific criteria).</p><p id="p-0075" num="0094">The electronic device generates (<b>530</b>) a trailer for the audio file using the selected one or more segments. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, trailer <b>460</b> is generated using hotspot segment <b>402</b>-<b>8</b> and hotspot segment <b>402</b>-<b>5</b> (e.g., selected from the set of hotspot segments <b>413</b>). In some embodiments, the term &#x201c;trailer&#x201d; refers to a compilation of segments that capture the overall stylistic qualities (e.g., a vibe, emotion or feeling) of the audio item. For example, the hotspot segments are selected and arranged (e.g., compiled) to generate a &#x201c;vibe clip&#x201d; (e.g., trailer) for the audio item. As such, a user that listens to the trailer is provided with a representative vibe or emotion associated with the audio item as a whole.</p><p id="p-0076" num="0095">In some embodiments, generating the trailer comprises (<b>532</b>) combining the selected one or more segments and one or more additional segments (e.g., one or more music segments, one or more intro segments, and/or one or more transition audio segments, etc.). For example, as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the trailer generation system <b>450</b> includes performing topical center detection <b>452</b> (e.g., to determine an introductory segment), music detection <b>454</b>, and optionally, predefined audio type detection <b>456</b>.</p><p id="p-0077" num="0096">In some embodiments, the trailer is (<b>534</b>) a predefined length (e.g., or range of lengths). For example, the predefined length is 1-minute (e.g., or between 50 seconds and 1-minute). For example, time length controller <b>458</b>, shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, determines (e.g., stores) the predefined length and selects the one or more segments (e.g., hotspot segments, music segments, intro segment, and/or transition audio segments) to include in the generated trailer <b>460</b>.</p><p id="p-0078" num="0097">In some embodiments, the generated trailer <b>460</b> is provided to a user for playback. For example, a user is enabled to select the trailer <b>460</b> and playback the trailer. In some embodiments, after generating trailer <b>460</b>, the user is enabled to change (e.g., update) the genre criteria and/or other user-specified criteria and the device repeats the process described in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> by applying additional criteria and/or alternative genre criteria <b>412</b> to generate an additional trailer. In some embodiments, the electronic device is enabled to generate a plurality of trailers for the audio file <b>401</b> and the user is enabled to select which trailer of the plurality of trailers to be stored in association with the audio file <b>401</b>. For example, a content creator (e.g., user) is enabled to select, from the plurality of generated trailers, which trailer to publish for the public (e.g., other users of the media-providing service) in association with audio file <b>401</b>.</p><p id="p-0079" num="0098">Although <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> illustrate a number of logical stages in a particular order, stages which are not order dependent may be reordered and other stages may be combined or broken out. Some reordering or other groupings not specifically mentioned will be apparent to those of ordinary skill in the art, so the ordering and groupings presented herein are not exhaustive. Moreover, it should be recognized that the stages could be implemented in hardware, firmware, software, or any combination thereof.</p><p id="p-0080" num="0099">The foregoing description, for purpose of explanation, has been described with reference to specific embodiments. However, the illustrative discussions above are not intended to be exhaustive or to limit the embodiments to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles and their practical applications, to thereby enable others skilled in the art to best utilize the embodiments and various embodiments with various modifications as are suited to the particular use contemplated.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>at an electronic device:<claim-text>receiving an audio file;</claim-text><claim-text>dividing the audio file into a plurality of segments;</claim-text><claim-text>automatically, without user input, determining, for each segment, a descriptor from a plurality of descriptors and a value of the descriptor for the segment;</claim-text><claim-text>selecting one or more segments of the plurality of segments, based on a comparison of the respective values of respective descriptors for respective segments and genre-specific criteria selected based on a genre of the audio file; and</claim-text><claim-text>generating a trailer for the audio file using the selected one or more segments.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the genre-specific criteria comprise one or more descriptors of the plurality of descriptors selected based on a genre of the audio file.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the descriptor and the value of the descriptor comprises using a parallel neural network.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each descriptor of the plurality of descriptors comprises an emotion selected from a group of emotions.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the audio file comprises spoken word audio content.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein selecting the one or more segments comprises selecting the one or more segments with the highest values of the genre-specific criteria.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein respective values of respective descriptors for respective segments are based on the audio of the audio file.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the genre-specific criteria are defined by a user.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein selecting the one or more segments is further based on the one or more segments satisfying user-defined criteria.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the trailer comprises combining the selected one or more segments and one or more additional segments.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each segment of the plurality of segments corresponds to a sentence in the audio file.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the descriptor for each segment comprises:<claim-text>applying a rolling time window to the segment to generate a set of descriptors, each descriptor in the set of descriptors corresponding to a respective time window of the segment.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the trailer is a predefined length.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A computer system, comprising:<claim-text>one or more processors; and</claim-text><claim-text>memory storing one or more programs, the one or more programs including instructions for:<claim-text>receiving an audio file;</claim-text><claim-text>dividing the audio file into a plurality of segments;</claim-text><claim-text>automatically, without user input, determining, for each segment, a descriptor from a plurality of descriptors and a value of the descriptor for the segment;</claim-text><claim-text>selecting one or more segments of the plurality of segments, based on a comparison of the respective values of respective descriptors for respective segments and genre-specific criteria selected based on a genre of the audio file; and</claim-text><claim-text>generating a trailer for the audio file using the selected one or more segments.</claim-text></claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable storage medium storing one or more programs for execution by a computer system with one or more processors, the one or more programs comprising instructions for:<claim-text>receiving an audio file;</claim-text><claim-text>dividing the audio file into a plurality of segments;</claim-text><claim-text>automatically, without user input, determining, for each segment, a descriptor from a plurality of descriptors and a value of the descriptor for the segment;</claim-text><claim-text>selecting one or more segments of the plurality of segments, based on a comparison of the respective values of respective descriptors for respective segments and genre-specific criteria selected based on a genre of the audio file; and</claim-text><claim-text>generating a trailer for the audio file using the selected one or more segments.</claim-text></claim-text></claim></claims></us-patent-application>