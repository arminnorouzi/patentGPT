<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007169A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007169</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943935</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-006219</doc-number><date>20200117</date></priority-claim><priority-claim sequence="02" kind="national"><country>JP</country><doc-number>2020-069279</doc-number><date>20200407</date></priority-claim><priority-claim sequence="03" kind="national"><country>JP</country><doc-number>2020-186751</doc-number><date>20201109</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23222</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23206</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23229</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e98">INFORMATION PROCESSING APPARATUS, IMAGE PROCESSING APPARATUS, AND METHOD OF CONTROLLING THE SAME</invention-title><us-related-documents><division><relation><parent-doc><document-id><country>US</country><doc-number>17151655</doc-number><date>20210118</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11477367</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17943935</doc-number></document-id></child-doc></relation></division></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>CANON KABUSHIKI KAISHA</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Honda</last-name><first-name>Yoshiaki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Ogino</last-name><first-name>Hiroshi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">According to an aspect of the disclosure, an information processing apparatus comprising a communication circuit and a control circuit is provided. The control circuit transmits reduced image data of captured image data, to a first external apparatus through the communication circuit; receives, from the first external apparatus, a result of evaluation processing applied to the reduced image data; on the basis of the result of the evaluation processing, determines whether to apply image processing to the captured image data corresponding to the reduced image data, by a second external apparatus; transmits the captured image data that is determined to be applied the image processing, to the second external apparatus through the communication circuit; and receives, from the second external apparatus, a result of the image processing applied to the captured image data.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="33.10mm" wi="115.23mm" file="US20230007169A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="243.50mm" wi="156.55mm" orientation="landscape" file="US20230007169A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="112.95mm" wi="151.89mm" file="US20230007169A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="244.43mm" wi="158.41mm" file="US20230007169A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="244.69mm" wi="159.17mm" file="US20230007169A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="245.11mm" wi="161.97mm" orientation="landscape" file="US20230007169A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="246.72mm" wi="161.37mm" orientation="landscape" file="US20230007169A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="243.33mm" wi="113.45mm" file="US20230007169A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="248.92mm" wi="161.12mm" orientation="landscape" file="US20230007169A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="234.19mm" wi="143.59mm" file="US20230007169A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="228.09mm" wi="149.69mm" orientation="landscape" file="US20230007169A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="235.37mm" wi="159.94mm" orientation="landscape" file="US20230007169A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="247.06mm" wi="161.04mm" orientation="landscape" file="US20230007169A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="248.67mm" wi="161.04mm" orientation="landscape" file="US20230007169A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="241.81mm" wi="123.36mm" orientation="landscape" file="US20230007169A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="233.34mm" wi="160.53mm" file="US20230007169A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="249.17mm" wi="154.52mm" orientation="landscape" file="US20230007169A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="249.17mm" wi="160.61mm" orientation="landscape" file="US20230007169A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="248.16mm" wi="160.95mm" orientation="landscape" file="US20230007169A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="241.13mm" wi="152.99mm" orientation="landscape" file="US20230007169A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="119.97mm" wi="58.00mm" file="US20230007169A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="248.16mm" wi="160.87mm" orientation="landscape" file="US20230007169A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="248.41mm" wi="161.63mm" orientation="landscape" file="US20230007169A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="226.06mm" wi="160.44mm" file="US20230007169A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="243.67mm" wi="161.54mm" file="US20230007169A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="124.21mm" wi="159.77mm" file="US20230007169A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="250.36mm" wi="162.31mm" file="US20230007169A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="251.12mm" wi="162.31mm" file="US20230007169A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="240.37mm" wi="159.26mm" file="US20230007169A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a divisional of application Ser. No. 17/151,655, filed Jan. 18, 2021, the entire disclosure of which is hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><heading id="h-0003" level="1">Field of the Invention</heading><p id="p-0003" num="0002">The present invention relates to an information processing apparatus, an image processing apparatus, and a method of controlling the same.</p><heading id="h-0004" level="1">Description of the Related Art</heading><p id="p-0004" num="0003">Image capture apparatuses such as digital cameras and digital video cameras have hardware and software that execute various types of image processing to convert image data (RAW data) read from an image sensor into image data in a generic format such as JPEG, MPEG, etc., that can be used in other devices.</p><p id="p-0005" num="0004">With the exception of updates for bug fixes, the software included in an image capture apparatus basically stay the same as when the apparatus was first sold, and the apparatus typically cannot use functions which are provided in newer models sold at a later date. This is because, for example, the hardware of older models does not meet the specifications required to implement the functions provided in newer models.</p><p id="p-0006" num="0005">Meanwhile, any smartphone, tablet computer, or the like which has imported image data from an image capture apparatus can utilize functions developed at a later date by uploading an application. However, for functions that are very computationally advanced or computationally intensive, the provided hardware may not be capable of handling those functions, making the functions unusable even by a smartphone, a tablet computer, or the like.</p><p id="p-0007" num="0006">The same problem therefore arises not only in digital cameras, digital video cameras, and the like, but also in all information processing apparatuses, including smartphones, tablet computers, and the like.</p><p id="p-0008" num="0007">Japanese Patent Laid-Open No. 2003-259281 discloses a configuration in which a RAW data processing program is output along with RAW format image data, and image processing is executed by an external apparatus. According to this configuration, image processing which requires a higher level of performance than the information processing apparatus is capable of providing can be applied to RAW data as well.</p><p id="p-0009" num="0008">However, the size of RAW data is extremely large. Furthermore, processing programs also transmit data to external apparatuses, and thus the configuration described in Japanese Patent Laid-Open No. 2003-259281 also takes time for data transmission. There is an additional problem in that when the data transmission costs money, the communication costs will rise. This problem becomes particularly serious when transmitting all of the RAW data which has been captured to the external apparatus.</p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0010" num="0009">The present invention has been conceived to alleviate at least these problems with the past techniques.</p><p id="p-0011" num="0010">One aspect of the present invention provides an information processing apparatus that enables image processing in an external apparatus to be used efficiently, as well as a method of controlling the information processing apparatus.</p><p id="p-0012" num="0011">Another aspect of the present invention provides an image processing apparatus suitable as an external apparatus for such an information processing apparatus, and a method of controlling the image processing apparatus, as well as an image processing system including the information processing apparatus and the image processing apparatus.</p><p id="p-0013" num="0012">According to an aspect of the present invention, there is provided an information processing apparatus comprising a communication circuit and a control circuit, wherein the control circuit: transmits reduced image data of captured image data, to a first external apparatus through the communication circuit, receives, from the first external apparatus, a result of evaluation processing applied to the reduced image data, on the basis of the result of the evaluation processing, determines whether to apply image processing to the captured image data corresponding to the reduced image data, by a second external apparatus, transmits the captured image data that is determined to be applied the image processing, to the second external apparatus through the communication circuit, and receives, from the second external apparatus, a result of the image processing applied to the captured image data.</p><p id="p-0014" num="0013">According to another aspect of the present invention, there is provided an image processing apparatus comprising a communication circuit and a control circuit, wherein the control circuit: receives reduced image data from an information processing apparatus through the communication circuit, applies evaluation processing to the reduced image data, transmits a result of the evaluation processing to the information processing apparatus through the communication circuit, receives image data having a higher resolution than the reduced image data from the information processing apparatus through the communication circuit, applies predetermined image processing to the image data, and transmits a result of the image processing to the information processing apparatus through the communication circuit.</p><p id="p-0015" num="0014">According to a further aspect of the present invention, there is provided an image processing apparatus comprising a communication circuit and a control circuit, wherein the control circuit: receives reduced image data from an information processing apparatus through the communication circuit, applies evaluation processing to the reduced image data, on the basis of a result of the evaluation processing, requests the information processing apparatus to transmit original image data of the reduced image data, receives the original image data from the information processing apparatus through the communication circuit, applies predetermined image processing to the original image data, and transmits a result of the image processing to the information processing apparatus through the communication circuit.</p><p id="p-0016" num="0015">According to another aspect of the present invention, there is provided an image processing system in which the information processing apparatus according to the present invention and the image processing apparatus according to the present invention are communicatively connected.</p><p id="p-0017" num="0016">According to a further aspect of the present invention, there is provided a control method for an information processing apparatus, the method comprising: transmitting reduced image data of captured image data to a first external apparatus through a communication circuit of the information processing apparatus; receiving, from the first external apparatus, a result of evaluation processing applied to the reduced image data; on the basis of the result of the evaluation processing, determining whether to apply image processing to the captured image data corresponding to the reduced image data, by a second external apparatus; transmitting the captured image data that is determined to be applied the image processing, to the second external apparatus through the communication circuit; and receiving, from the second external apparatus, a result of the image processing applied to the captured image data.</p><p id="p-0018" num="0017">According to another aspect of the present invention, there is provided a control method for an image processing apparatus, the method comprising: receiving reduced image data from an information processing apparatus through a communication circuit of the image processing apparatus; applying evaluation processing to the reduced image data; transmitting a result of the evaluation processing to the information processing apparatus through the communication circuit; receiving image data having a higher resolution than the reduced image data from the information processing apparatus through the communication circuit; applying predetermined image processing to the image data; and transmitting a result of applying the image processing to the information processing apparatus through the communication circuit.</p><p id="p-0019" num="0018">According to a further aspect of the present invention, there is provided a control method for an image processing apparatus, the method comprising: receiving reduced image data from an information processing apparatus through a communication circuit of the image processing apparatus; applying evaluation processing to the reduced image data; on the basis of a result of the evaluation processing, requesting the information processing apparatus to transmit original image data of the reduced image data; receiving the original image data from the information processing apparatus through the communication circuit; applying predetermined image processing to the original image data; and transmitting a result of the image processing to the information processing apparatus through the communication circuit.</p><p id="p-0020" num="0019">According to another aspect of the present invention, there is provided an information processing apparatus capable of communicating with a first external apparatus and a second external apparatus, the information processing apparatus comprising: a storage device that stores image data; and a control circuit that controls an operation of the information processing apparatus, wherein the control circuit: transmits, to the first external apparatus, information for selecting image data stored in the storage device, receives, from the first external apparatus, information about selected image data, transmits the selected image data to the second external apparatus, and receives, from the second external apparatus, a result of image processing applied to the image data that has been transmitted.</p><p id="p-0021" num="0020">According to a further aspect of the present invention, there is provided an information processing apparatus capable of communicating with a first external apparatus and a second external apparatus, the information processing apparatus comprising: a storage device that stores image data; and a control circuit that controls an operation of the information processing apparatus, wherein the control circuit: transmits, to the first external apparatus, information for selecting image data stored in the storage device, receives information that identifies selected image data and a partial region of the selected image data, from the first external apparatus, as information about the selected image data, transmits data of the partial region of the selected image data to the second external apparatus, transmits, to the second external apparatus, image data, among the selected image data, for which a request has been made by the first external apparatus, and receives, from the second external apparatus, a result of image processing applied to the image data requested by the first external apparatus.</p><p id="p-0022" num="0021">According to another aspect of the present invention, there is provided an image processing apparatus comprising a control circuit, wherein the control circuit: receives information pertaining to image data stored in an information processing apparatus from the information processing apparatus, displays, on the basis of the information, a screen for selecting image data stored in the information processing apparatus, determines a partial region of the image data selected through the screen, and transmits, to the information processing apparatus, information pertaining to the selected image data and the partial region.</p><p id="p-0023" num="0022">According to a further aspect of the present invention, there is provided an image processing apparatus comprising a control circuit, wherein the control circuit: receives proxy image data based on image data stored in an information processing apparatus from the information processing apparatus, calculates an evaluation value for the proxy image data, generates, on the basis of the evaluation value, information indicating a priority order of the proxy image data, and transmits, to the information processing apparatus, information indicating the priority order.</p><p id="p-0024" num="0023">According to an aspect of the present invention, there is provided a non-transitory computer-readable medium storing a program for causing a computer to function as the information processing apparatus according to the present invention.</p><p id="p-0025" num="0024">According to a further aspect of the present invention, there is provided a non-transitory computer-readable medium storing a program for causing a computer to function as the image processing apparatus according to the present invention.</p><p id="p-0026" num="0025">According to another aspect of the present invention, there is provided a control method for an information processing apparatus, the information processing apparatus including a storage device which stores image data and being capable of communicating with a first external apparatus and a second external apparatus, the method comprising: transmitting, to the first external apparatus, information for selecting image data stored in the storage device; receiving, from the first external apparatus, information about selected image data; transmitting the selected image data to the second external apparatus; and receiving, from the second external apparatus, a result of image processing applied to the image data that has been transmitted.</p><p id="p-0027" num="0026">According to a further aspect of the present invention, there is provided a control method for an information processing apparatus, the information processing apparatus including a storage device which stores image data and being capable of communicating with a first external apparatus and a second external apparatus, the method comprising: transmitting, to the first external apparatus, information for selecting image data stored in the storage device; receiving information that identifies selected image data and a partial region of the selected image data, from the first external apparatus, as information about the selected image data; transmitting data of the partial region of the selected image data to the second external apparatus; transmitting, to the second external apparatus, image data, among the selected image data, for which a request has been made by the first external apparatus; and receiving, from the second external apparatus, a result of image processing applied to the image data requested by the first external apparatus.</p><p id="p-0028" num="0027">According to another aspect of the present invention, there is provided a control method for an image processing apparatus, the method comprising: receiving information pertaining to image data stored in an information processing apparatus from the information processing apparatus; displaying, on the basis of the information, a screen for selecting image data stored in the information processing apparatus; determining a partial region of the image data selected through the screen; and transmitting, to the information processing apparatus, information pertaining to the selected image data and the partial region.</p><p id="p-0029" num="0028">According to a further aspect of the present invention, there is provided a control method for an image processing apparatus, the method comprising: receiving proxy image data based on image data stored in an information processing apparatus from the information processing apparatus; calculating an evaluation value for the proxy image data; generating, on the basis of the evaluation value, information indicating a priority order of the proxy image data; and transmitting, to the information processing apparatus, information indicating the priority order.</p><p id="p-0030" num="0029">Further features of the present invention will become apparent from the following description of exemplary embodiments (with reference to the attached drawings).</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIGS. <b>1</b>A to <b>1</b>C</figref> are diagrams illustrating an image processing system, an information processing apparatus, and a server according to an embodiment.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart pertaining to a first embodiment.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating the first embodiment.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart pertaining to the first embodiment.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are diagrams pertaining to an image processing system and an edge device according to second to fifth embodiments.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart pertaining to the second embodiment.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram pertaining to an edge device according to the third embodiment.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart pertaining to the third embodiment.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart pertaining to the third and fourth embodiments.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating the third and fourth embodiments.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIGS. <b>11</b>A and <b>11</b>B</figref> are diagrams illustrating the third and fourth embodiments.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram pertaining to a server according to the fourth embodiment.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart pertaining to the fourth embodiment.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart pertaining to the fifth embodiment.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a block diagram illustrating an example of the functional configuration of an information processing apparatus according to a sixth embodiment.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIGS. <b>16</b>A and <b>16</b>B</figref> are block diagrams illustrating examples of the functional configurations of a server apparatus and an edge device according to the sixth embodiment.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart pertaining to the sixth embodiment.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a flowchart pertaining to a variation on the sixth embodiment.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIGS. <b>19</b>A and <b>19</b>B</figref> are flowcharts pertaining to a seventh embodiment.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram illustrating an example of a partial region according to the seventh embodiment.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIGS. <b>21</b>A and <b>21</b>B</figref> are flowcharts pertaining to an eighth embodiment.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a flowchart pertaining to a ninth embodiment.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a flowchart pertaining to a tenth embodiment.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flowchart pertaining to the tenth embodiment.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a flowchart pertaining to an eleventh embodiment.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart pertaining to a twelfth embodiment.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a flowchart pertaining to a thirteenth embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0058" num="0057">Hereinafter, embodiments will be described in detail with reference to the attached drawings. Note, the following embodiments are not intended to limit the scope of the claimed invention. Multiple features are described in the embodiments, but limitation is not made to an invention that requires all such features, and multiple such features may be combined as appropriate. Furthermore, in the attached drawings, the same reference numerals are given to the same or similar configurations, and redundant description thereof is omitted.</p><p id="p-0059" num="0058">The following embodiments will describe a case where the present invention is applied in an image capture apparatus, which is an example of an information processing apparatus. However, the present invention can be applied in any electronic device capable of handling image data. Examples of such an electronic device include video cameras, computer devices (personal computers, tablet computers, media players, PDAs, and the like), mobile phones, smartphones, game consoles, robots, drones, and dashboard cameras. These are merely examples, however, and the present invention can be applied in other electronic devices as well.</p><heading id="h-0008" level="1">First Embodiment</heading><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a diagram schematically illustrating an example of the configuration of an image processing system according to a first embodiment of the present invention. The image processing system has a configuration in which an image capture apparatus <b>100</b> and a server apparatus <b>200</b> serving as an external apparatus (first and second external apparatuses) are communicatively connected over a network <b>300</b>. Note that the image capture apparatus <b>100</b> may be connected to a plurality of networks <b>300</b>. The image capture apparatus <b>100</b> and the server apparatus <b>200</b> can communicate with each other using a communication protocol based on the type of the network <b>300</b>. For example, if the network <b>300</b> is the Internet, the apparatuses can communicate using the TCP/IP, UDP, or similar protocols. Here, it is assumed that the network <b>300</b> is a public network such as the Internet, or a network for which usage or communication amounts are charged, for example.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a block diagram illustrating an example of the functional configuration of the image capture apparatus <b>100</b>. Typically, the image capture apparatus <b>100</b> is a digital (video) camera, but may be any electronic device that has an image capture function and is capable of communicating with the server apparatus <b>200</b> (also called simply the &#x201c;server <b>200</b>&#x201d; hereinafter) over the network <b>300</b>.</p><p id="p-0062" num="0061">A control circuit <b>101</b> is, for example, a CPU (also called an &#x201c;MPU&#x201d; or a &#x201c;microprocessor&#x201d;), and implements various functions of the image capture apparatus <b>100</b> by loading programs stored in ROM <b>102</b> into RAM <b>103</b>, executing the programs, and controlling the operations of the respective elements of the image capture apparatus <b>100</b>. Note that when an optical system <b>104</b> is an interchangeable type, the control circuit <b>101</b> controls operations of the optical system <b>104</b> by communicating with a controller of the optical system <b>104</b>.</p><p id="p-0063" num="0062">The ROM <b>102</b> stores programs executed by the control circuit <b>101</b>, various types of setting values and GUI data of the image capture apparatus <b>100</b>, and the like. The ROM <b>102</b> may be electrically rewriteable. The RAM <b>103</b> is main memory used when the control circuit <b>101</b> executes programs. The RAM <b>103</b> may also be used as buffer memory for image data, video memory for a display circuit <b>109</b>, and the like.</p><p id="p-0064" num="0063">The optical system <b>104</b> includes mobile lenses such as a zoom lens and a focus lens, and forms a subject image on an image capturing surface of an image capturing circuit <b>105</b>. The control circuit <b>101</b> controls driving of the mobile lenses of the optical system <b>104</b>.</p><p id="p-0065" num="0064">The image capturing circuit <b>105</b> is an image sensor such as a CCD image sensor, a CMOS image sensor, or the like, for example. A plurality of photoelectric conversion circuits are arranged two-dimensionally in the image sensor, and the subject image formed on the image capturing surface of the optical system <b>104</b> is converted into an analog image signal by the photoelectric conversion circuits. An A/D conversion circuit <b>106</b> converts the analog image signal input from the image capturing circuit <b>105</b> into digital image data. Note that the A/D conversion circuit <b>106</b> may be included in the image capturing circuit <b>105</b>.</p><p id="p-0066" num="0065">The digital image data is temporarily stored in the RAM <b>103</b>. An image processing circuit <b>107</b> generates signals and image data, obtains and/or generates various types of information, and so on by applying predetermined image processing to the image data stored in the RAM <b>103</b>. The image processing circuit <b>107</b> may, for example, be a dedicated hardware circuit, such as an ASIC, designed to implement specific functions, or may be realized by a programmable processor, such as a DSP, which is configured to implement specific functions by executing software.</p><p id="p-0067" num="0066">Here, the image processing applied by the image processing circuit <b>107</b> includes preprocessing, color interpolation processing, correction processing, detection processing, data processing, evaluation value calculation processing, special effect processing, and so on.</p><p id="p-0068" num="0067">The pre-processing includes signal amplification, reference level adjustment, missing pixel correction, and the like. The color interpolation processing is processing for interpolating the values of color components not included in the image data read out from the pixels, and is also called &#x201c;demosaicing&#x201d; or &#x201c;synchronization&#x201d;.</p><p id="p-0069" num="0068">The correction processing includes white balance adjustment, tone correction (gamma processing), processing for correcting the effects of optical aberration, vignetting, and so on of the optical system <b>104</b>, color correction processing, and so on.</p><p id="p-0070" num="0069">The detection processing includes detecting a feature region (e.g., a facial region or a human body region) or movement in such a region, processing for recognizing a person, or the like.</p><p id="p-0071" num="0070">The data processing includes compositing processing, scaling processing, encoding and decoding processing, header information generation processing, and the like.</p><p id="p-0072" num="0071">The evaluation value calculation processing includes generating signals, evaluation values, and the like used in automatic focus detection (AF), processing for calculating evaluation values used in automatic exposure control (AE), and the like.</p><p id="p-0073" num="0072">The special effect processing includes adding blur, changing color tones, relighting processing, and the like.</p><p id="p-0074" num="0073">Note that these are merely examples of the image processing that can be applied by the image processing circuit <b>107</b>, and the image processing applied by the image processing circuit <b>107</b> is not limited thereto.</p><p id="p-0075" num="0074">Pixel data constituting the digital image data supplied from the A/D conversion circuit <b>106</b> to the image processing circuit <b>107</b> in the above-described image processing has a single color component value based on a color arrangement in color filters of the image sensor. Such image data is called &#x201c;RAW data&#x201d;. Of the image processing applied to the RAW data by the image processing circuit <b>107</b>, a series of image processing for converting the RAW data into image data of a generic format which can be used to display or print photographic data is called &#x201c;development processing&#x201d;. The development processing typically includes white balance processing and color interpolation processing, and can also include lens aberration correction processing, noise reduction (NR) processing, gamma (tone conversion) processing, and the like.</p><p id="p-0076" num="0075">A recording circuit <b>108</b> records data of captured images into a recording medium, reads out data of captured images recorded in the recording medium, and so on. The recording medium may be a removable memory card, for example. The recording circuit <b>108</b> records, into the recording medium, captured image data for recording (moving image or still image data) and/or RAW data, stored in the RAM <b>103</b> and to which the development processing has been applied by the image processing circuit <b>107</b>, as well as proxy image data.</p><p id="p-0077" num="0076">The proxy image data is image data having a lower data amount than the captured image data for recording and the RAW data, and for a still image, may be a thumbnail image. The thumbnail image is a reduced-size image of the original image after the development processing, and the thumbnail image data is reduced image data. The proxy image data may be image data that, for example, has a lower resolution, and/or a lower bit depth, than the captured image data for recording and the RAW data. For still images, the proxy image data may be, for example, image data (thumbnail image data) which has been subjected to the development processing (developed), and that has a lower resolution than the RAW data. For moving images, the proxy image data may be, for example, developed moving image data having a lower bitrate than the RAW data. Note that although the proxy image data does not absolutely have to be developed, using developed data makes it possible to reduce the data amount.</p><p id="p-0078" num="0077">The display circuit <b>109</b> is a display device such as an LCD, and performs displays corresponding to the image data stored in a video memory region of the RAM <b>103</b>. The display circuit <b>109</b> displays captured images, information obtained from captured images (e.g., a luminance histogram), setting values of the image capture apparatus <b>100</b>, and GUI elements (icons, marks, and the like overlaid on menu screens, captured images, and the like).</p><p id="p-0079" num="0078">A communication circuit <b>110</b> includes a communication interface compliant with one or more publicly known wireless or wired communication standards, for example, and implements data communication between an external device and the image capture apparatus <b>100</b>. The communication circuit <b>110</b> can support data communication using a mobile phone network compliant with standards such as LTE and 5G, data communication compliant with standards such as wireless LAN and Bluetooth (registered trademark), and the like. The communication circuit <b>110</b> can also support communication standards such as USB, Ethernet (registered trademark), and the like. In the present embodiment, the communication circuit <b>110</b> implements communication between the image capture apparatus <b>100</b> and the server <b>200</b> over the network <b>300</b>. The control circuit <b>101</b> exchanges image data and the like with the server <b>200</b> through the communication circuit <b>110</b>.</p><p id="p-0080" num="0079">An operating unit <b>111</b> includes input devices, such as buttons and switches, with which a user inputs various types of instructions to the image capture apparatus <b>100</b>. If the display circuit <b>109</b> is a touch screen, the touch panel is also included in the input devices. The input devices constituting the operating unit <b>111</b> are named according to the functions assigned thereto. For example, the operating unit <b>111</b> includes a release switch for instructing the start of shooting preparation operations and the start of shooting, a shooting mode selection switch for selecting a shooting mode, a menu button, a directional key, an OK key, and the like. A plurality of functions may be assigned to the same input device. Additionally, the input device may include a software button/key. Input devices through which instructions are input in a non-contact manner, such as voice input or gaze-directed input, may also be included in the operating unit <b>111</b>.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>1</b>C</figref> is a block diagram illustrating an example of the functional configuration of the server <b>200</b>. Although typically a generic computer located on a network, the server <b>200</b> may be any electronic device that has server functionality and is capable of communicating with the image capture apparatus <b>100</b> over the network <b>300</b>.</p><p id="p-0082" num="0081">A control circuit <b>201</b> is, for example, a CPU (also called an &#x201c;MPU&#x201d; or a &#x201c;microprocessor&#x201d;), and implements various server functions by loading programs stored in ROM <b>203</b> or a storage device <b>205</b> into RAM <b>202</b> and executing the programs. Note that an auxiliary processor suited to the execution of image processing, such as a GPU, may be used in the server <b>200</b> in addition to the CPU. The ROM <b>203</b> stores application programs for implementing image processing functions provided by the server <b>200</b>, programs executed during startup, various types of setting values, and the like.</p><p id="p-0083" num="0082">Here, as examples of the application programs for implementing the image processing functions, an image recognition program <b>210</b> and an image processing program <b>211</b> are stored in the ROM <b>203</b>. The image processing functions provided by the server <b>200</b> are typically functions which cannot be applied by the image capture apparatus <b>100</b>, or are superior to functions provided by the image capture apparatus <b>100</b> in terms of at least one of processing time and processing quality. The server <b>200</b> can, as needed, obtain a program based on the latest image processing technology from another device capable of communicating over the network <b>300</b>, through, for example, a communication circuit <b>204</b>. This makes it possible to more easily reap the benefits of technological progress than with the image capture apparatus <b>100</b>, for which the software cannot be updated easily. For example, image recognition programs using machine learning techniques such as deep learning, which are being put into practical use at a dramatic rate in recent years, can be used with ease.</p><p id="p-0084" num="0083">The RAM <b>202</b> is main memory used when the control circuit <b>201</b> executes programs. The RAM <b>202</b> may also be used as buffer memory for image data, video memory for a display circuit <b>207</b>, and the like.</p><p id="p-0085" num="0084">The communication circuit <b>204</b> includes a communication interface compliant with one or more publicly known wireless or wired communication standards, for example, and implements data communication between an external device and the server <b>200</b>. In the present embodiment, the communication circuit <b>204</b> implements communication between the server <b>200</b> and the image capture apparatus <b>100</b> over the network <b>300</b>. The control circuit <b>201</b> exchanges image data and the like with the image capture apparatus <b>100</b> through the communication circuit <b>204</b>.</p><p id="p-0086" num="0085">The storage device <b>205</b> may be a large-capacity storage device such as a hard disk drive (HDD), a solid-state drive (SSD), or the like. The storage device <b>205</b> is used to store, for example, an OS, application programs, user data, and the like. The ROM <b>203</b> may be part of the storage device <b>205</b>.</p><p id="p-0087" num="0086">An operating unit <b>206</b> includes input devices with which a user inputs instructions to the server <b>200</b>, and is typically a keyboard, a mouse, a touch pad, or the like.</p><p id="p-0088" num="0087">The display circuit <b>207</b> is a display device such as an LCD, and performs displays corresponding to the image data stored in a video memory region of the RAM <b>202</b>. The display circuit <b>207</b> displays user interface elements provided by the OS, information pertaining to applications, and the like.</p><p id="p-0089" num="0088">Note that the network <b>300</b> may have any configuration capable of supporting data communication between the image capture apparatus <b>100</b> and the server <b>200</b>, and because any configuration can be used, detailed descriptions thereof will not be given.</p><p id="p-0090" num="0089">Interactive operations between the image capture apparatus <b>100</b> and the server <b>200</b> will be described next with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In the following, the control circuit <b>101</b> (CPU) is the main entity actually performing operations described as being executed by the image capture apparatus <b>100</b>, and the control circuit <b>201</b> (CPU) is the main entity actually performing operations described as being executed by the server <b>200</b>. Additionally, in the image capture apparatus <b>100</b>, when generating image data for recording from image data obtained from shooting, the image processing circuit <b>107</b> also generates the corresponding proxy image data. It is further assumed that the image data for recording and proxy image data are recorded in association with each other by the recording circuit <b>108</b>.</p><p id="p-0091" num="0090">In step S<b>401</b>, the image capture apparatus <b>100</b> transmits the proxy image data recorded by the recording circuit <b>108</b> from the communication circuit <b>110</b> to the server <b>200</b> over the network <b>300</b>. It is assumed that settings, information, and the like necessary for communication with the server <b>200</b> are registered in advance and stored in the ROM <b>102</b>. The timing at which step S<b>401</b> is executed is not particularly limited. For example, the step may be started upon confirmation that unprocessed RAW data is present in the recording circuit <b>108</b>, or may be started in response to a user instruction. The transmitted proxy image data may be data which corresponds to the RAW data, data which corresponds to developed image data, or data selected by the user. Alternatively, all of the proxy image data may be transmitted.</p><p id="p-0092" num="0091">Furthermore, proxy image data corresponding to RAW data which has not been subjected to the development processing (unprocessed RAW data) may be transmitted. This makes it possible, for example, to distinguish RAW data for which corresponding image data in a generic format (e.g., the JPEG format) is not recorded as unprocessed RAW data. Alternatively, a filename that makes it possible to distinguish whether or not the RAW data is unprocessed may be added, and the unprocessed RAW data may be determined from the filename.</p><p id="p-0093" num="0092">In step S<b>402</b>, the server <b>200</b> receives the proxy image data from the image capture apparatus <b>100</b> through the communication circuit <b>204</b>. The server <b>200</b> stores the received proxy image data in the RAM <b>202</b>.</p><p id="p-0094" num="0093">In step S<b>403</b>, the server <b>200</b> loads the image recognition program <b>210</b> stored in the ROM <b>203</b> into the RAM <b>202</b> and executes the program. The image recognition program <b>210</b> provides a function for detecting a facial region by applying publicly known facial recognition processing to the image data, and a subject recognition function for recognizing a person in the detected facial region by comparing features of the detected facial region with features of facial regions of people registered in advance. By executing the image recognition program <b>210</b>, the control circuit <b>201</b> applies, to the proxy image data, evaluation processing that generates information serving as a determination standard for the image capture apparatus <b>100</b> to determine image data to which image processing is to be applied by the external apparatus. Subject recognition processing is an example of the evaluation processing, and other information may be generated on the basis of the proxy image data.</p><p id="p-0095" num="0094">It is assumed that the facial region of a person to be detected, or feature information thereof, is registered in the server <b>200</b> in advance. This registration can be implemented by, for example, transmitting data of an image showing the face of the person to be detected from the image capture apparatus <b>100</b> to the server <b>200</b>, and the server <b>200</b> registering feature information extracted from the facial region using the image recognition program <b>210</b>. Note that the person to be detected may be registered through a different method instead. Here, it is assumed that a person <b>501</b> and a person <b>502</b> are registered as people to be detected using an image such as that illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The server <b>200</b> adds unique identification information (ID) to the feature information of each person to be detected and stores that information in the storage device <b>205</b>, for example. It is assumed here that &#x201c;ID<b>1</b>&#x201d; is assigned to the person <b>501</b> and &#x201c;ID<b>2</b>&#x201d; is assigned to the person <b>502</b>.</p><p id="p-0096" num="0095">The subject recognition processing performed in step S<b>403</b> will be described in detail with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. This processing is implemented by the control circuit <b>201</b> (CPU) executing the image recognition program <b>210</b>.</p><p id="p-0097" num="0096">In step S<b>601</b>, the control circuit <b>201</b> applies facial authentication processing to one of the instances of proxy image data stored in the RAM <b>202</b>. The facial authentication processing is processing for detecting a facial region from the proxy image data and processing for determining whether or not the detected facial region is that of a registered person. If a feature of the detected facial region is determined to be identical or highly similar to a feature of a registered person, the control circuit <b>201</b> can determine that the detected facial region is the facial region of a registered person. Note that when a plurality of facial regions are detected from the proxy image data, the control circuit <b>201</b> determines whether or not each of the detected facial regions are facial regions of a registered person.</p><p id="p-0098" num="0097">In steps S<b>602</b> to S<b>603</b> and S<b>605</b> to S<b>606</b>, the control circuit <b>201</b> adds a tag to the proxy image data on the basis of the result of the facial authentication processing performed in step S<b>601</b>.</p><p id="p-0099" num="0098">In step S<b>602</b>, the control circuit <b>201</b> determines whether the facial authentication processing performed in step S<b>601</b> indicates that the facial region detected from the proxy image data is a facial region of a person registered as &#x201c;ID<b>1</b>&#x201d;. If so, the sequence moves to step S<b>606</b>, and if not, the sequence moves to step S<b>603</b>.</p><p id="p-0100" num="0099">In step S<b>603</b>, the control circuit <b>201</b> determines whether the facial authentication processing performed in step S<b>601</b> indicates that the facial region detected from the proxy image data is a facial region of a person registered as &#x201c;ID<b>2</b>&#x201d;. If so, the sequence moves to step S<b>605</b>, and if not, the sequence moves to step S<b>604</b>.</p><p id="p-0101" num="0100">In step S<b>605</b>, the control circuit <b>201</b> adds a tag of &#x201c;ID<b>2</b>&#x201d; to the proxy image data. In step S<b>606</b>, the control circuit <b>201</b> adds a tag of &#x201c;ID<b>1</b>&#x201d; to the proxy image data. Note that when a plurality of facial regions have been detected from the proxy image data, the control circuit <b>201</b> executes steps S<b>602</b> and S<b>603</b> for each of the facial regions.</p><p id="p-0102" num="0101">In step S<b>604</b>, the control circuit <b>201</b> adds, to the proxy image data, a tag indicating that it has been determined that no registered person appears in the image (a &#x201c;no match&#x201d; tag).</p><p id="p-0103" num="0102">In step S<b>607</b>, the control circuit <b>201</b> determines whether the subject recognition processing has been applied to all of the received proxy image data. If so, the subject recognition processing ends, and if not, the processing from step S<b>601</b> is applied to the proxy image data to which the subject recognition processing has not yet been applied.</p><p id="p-0104" num="0103">Although an example in which there are two registered people, having the ID<b>1</b> and ID<b>2</b>, has been described here, the control circuit <b>201</b> performs the same tagging processing regardless of the number of registered people. In this manner, through the subject recognition processing, a tag for the ID of a registered person determined to be present in the proxy image, or a tag indicating that no registered person is present, is added to the proxy image data received from the image capture apparatus <b>100</b>.</p><p id="p-0105" num="0104">Returning to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in step S<b>404</b>, the server <b>200</b> transmits information of the tags added in step S<b>403</b> (a subject recognition result) to the image capture apparatus <b>100</b> over the network <b>300</b>. The server <b>200</b> transmits, for example, data in which information specifying the proxy image data and information of the tags added to that proxy image data are associated with each other as tag information. Not transmitting the proxy image data makes it possible to reduce the amount of transmitted data and cut down on processing delay. The tag information may be information indicating, for example, one or more person IDs, or no match.</p><p id="p-0106" num="0105">In step S<b>405</b>, the image capture apparatus <b>100</b> receives the tag information from the server <b>200</b> and stores that information in the RAM <b>103</b>.</p><p id="p-0107" num="0106">In step S<b>406</b>, the image capture apparatus <b>100</b> determines the image data to which image processing is to be applied by the server <b>200</b> (here, the RAW data) on the basis of the received tag information for the people. Here, for example, it is assumed that the original data of a proxy image associated with one or more person IDs is determined to be the image data to which the image processing is to be applied by the server <b>200</b>. Note that the &#x201c;original data of a proxy image&#x201d; is the image data serving as the basis of the proxy image, and may be RAW data or developed image data.</p><p id="p-0108" num="0107">In this case, in step S<b>406</b>, the image capture apparatus <b>100</b> determines, for one instance of the proxy image data transmitted in step S<b>401</b>, whether or not one or more person IDs are associated with the tag information received from the server <b>200</b>. If it is determined that one or more person IDs are associated, the image capture apparatus <b>100</b> determines that the corresponding RAW data is RAW data to which the image processing is to be applied by the server <b>200</b>, and executes step S<b>407</b>. On the other hand, if it is not determined that one or more person IDs are associated, the image capture apparatus <b>100</b> executes step S<b>413</b> without executing step S<b>407</b>.</p><p id="p-0109" num="0108">In step S<b>407</b>, the image capture apparatus <b>100</b> transmits the RAW data to the server <b>200</b> and stands by to receive a developing result.</p><p id="p-0110" num="0109">In step S<b>408</b>, upon receiving the RAW data from the image capture apparatus <b>100</b>, the server <b>200</b> stores the RAW data in the RAM <b>202</b>, the storage device <b>205</b>, or the like.</p><p id="p-0111" num="0110">In step S<b>409</b>, the server <b>200</b> loads the image processing program <b>211</b> from the ROM <b>203</b> into the RAM <b>202</b> and executes the program. The image processing program <b>211</b> applies the development processing to the RAW data.</p><p id="p-0112" num="0111">The development processing includes, for example, white balance processing, lens aberration correction, noise reduction (NR) processing, color interpolation processing, gamma processing, and the like. Additionally, an image data file in a predetermined format such as the JPEG format may be generated by applying encoding processing after the development processing.</p><p id="p-0113" num="0112">The development processing applied by the server <b>200</b> is processing based on a newer and/or more complex method than the processing performed by the image processing circuit <b>107</b> of the image capture apparatus <b>100</b>, and can therefore achieve a better processing result than development processing applied by the image capture apparatus <b>100</b>. This is due to the processing capabilities of the server <b>200</b> being higher than the processing capabilities of the image capture apparatus <b>100</b>, and the image processing program <b>211</b> executed by the server <b>200</b> being based on newer technology than the image processing circuit <b>107</b> of the image capture apparatus <b>100</b>.</p><p id="p-0114" num="0113">In step S<b>410</b>, the server <b>200</b> transmits the image data, to which the development processing has been applied in step S<b>409</b>, to the image capture apparatus <b>100</b> over the network <b>300</b>.</p><p id="p-0115" num="0114">In step S<b>411</b>, the image capture apparatus <b>100</b> receives the post-development processing image data from the server <b>200</b>.</p><p id="p-0116" num="0115">In step S<b>412</b>, the image capture apparatus <b>100</b> supplies the post-development processing image data to the image processing circuit <b>107</b>, and causes image data for recording and image data for display to be generated. The image capture apparatus <b>100</b> then causes the display circuit <b>109</b> to display the image data for display, and causes the recording circuit <b>108</b> to record the image data for recording.</p><p id="p-0117" num="0116">In step S<b>413</b>, the image capture apparatus <b>100</b> determines whether or not all of the proxy image data transmitted in step S<b>401</b> has been processed, and if it is determined that all of the proxy image data has been processed, the RAW data development processing ends. On the other hand, if it is not determined that all of the proxy image data has been processed, the processing from step S<b>406</b> is applied to the unprocessed proxy image data.</p><p id="p-0118" num="0117">The image capture apparatus according to the present embodiment determines, on the basis of a result of applying predetermined image processing to the proxy image data of recorded image data using an external apparatus, whether or not to execute image processing on the original data of the proxy image data using the external apparatus. Accordingly, whether or not the image processing should be applied using the external apparatus can be determined on the basis of a result of image processing which cannot be applied, or can only be applied with a lower level of accuracy, using the image capture apparatus.</p><p id="p-0119" num="0118">For example, when the image capture apparatus cannot apply accurate subject recognition processing, using a result of the external apparatus applying accurate subject recognition processing to the proxy image data makes it possible to more accurately determine image data that should be processed by the external apparatus. The present embodiment has described an example of using a result of determining whether or not a registered person is present using an external apparatus. However, a result of other image processing can also be used, e.g., a result of recognition processing for recognizing a subject aside from a person, scene determination processing, and so on.</p><p id="p-0120" num="0119">The present embodiment has described a configuration in which the user cannot be involved in the determination, made by the image capture apparatus <b>100</b>, that image data should be processed by the external apparatus. However, the configuration may be such that a list of image data determined to be processed by the external apparatus is displayed in a selectable manner in the display circuit <b>109</b>, and only image data selected by the user from the list is transmitted to the external apparatus. Additionally, the image processing ultimately applied by the external apparatus is not limited to RAW data development processing, and may be any image processing. Furthermore, the image data to which the external apparatus applies the image processing is not limited to RAW data, and may be developed image data for recording.</p><heading id="h-0009" level="1">Second Embodiment</heading><p id="p-0121" num="0120">A second embodiment of the present invention will be described next. <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a diagram schematically illustrating an example of the configuration of an image processing system according to the present embodiment. Constituent elements which are the same as in the first embodiment will be given the same reference numerals as in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, and will not be described. In the present embodiment, the image processing performed on the proxy image data is performed by an edge device <b>400</b>, which is an external apparatus positioned before the network <b>300</b> with respect to the image capture apparatus <b>100</b>, rather than being performed by the server <b>200</b> on the network <b>300</b>. Accordingly, in the present embodiment, the edge device <b>400</b> is a first external apparatus and the server <b>200</b> is a second external apparatus.</p><p id="p-0122" num="0121">The edge device <b>400</b> is an image processing apparatus that is capable of communicating with the image capture apparatus <b>100</b>, and is also capable of communicating with the server <b>200</b> over the network <b>300</b>. Here, a configuration in which a computer device having a communication function, such as a smartphone or a tablet computer, is used as the edge device <b>400</b> will be described as an example. Note that when a smartphone is used instead of the image capture apparatus <b>100</b>, it is conceivable to use, for example, a desktop computer that is more powerful than a smartphone as the edge device <b>400</b>.</p><p id="p-0123" num="0122">In recent years, more and more smartphones and tablet computers are being equipped with processors for executing high-speed image processing using machine learning technology. Such devices are therefore suited to use as edge devices. Note that the same kind of generic computer as the server <b>200</b> may be used as the edge device <b>400</b>.</p><p id="p-0124" num="0123">The method of communication between the image capture apparatus <b>100</b> and the edge device <b>400</b> is not particularly limited, and may be wired communication or wireless communication. The image capture apparatus <b>100</b> may use the same or different communication methods for communication with the server <b>200</b> and the edge device <b>400</b>. When different communication methods are used, the communication circuit <b>110</b> has a communication interface corresponding to each of the communication methods (see <figref idref="DRAWINGS">FIG. <b>15</b></figref>). A plurality of communication circuits <b>110</b> may be provided in accordance with the communication methods. Here, it is assumed that the communication between the image capture apparatus <b>100</b> and the edge device <b>400</b> is charged.</p><p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a block diagram illustrating an example of the functional configuration of the edge device <b>400</b>. A control circuit <b>401</b> is, for example, a CPU (also called an &#x201c;MPU&#x201d; or a &#x201c;microprocessor&#x201d;), and implements various functions of the edge device <b>400</b> by loading programs stored in ROM <b>402</b> or a recording circuit <b>407</b> into RAM <b>403</b> and executing the programs.</p><p id="p-0126" num="0125">The ROM <b>402</b> stores programs executed by the control circuit <b>401</b>, various types of setting values and GUI data of the edge device <b>400</b>, and the like. The ROM <b>402</b> may be electrically rewriteable. The RAM <b>403</b> is main memory used when the control circuit <b>401</b> executes programs. The RAM <b>403</b> may also be used as buffer memory for data, video memory for a display circuit <b>408</b>, and the like.</p><p id="p-0127" num="0126">An image recognition circuit <b>404</b> executes subject recognition processing for determining whether or not a person, registered in advance, is present in an image. The subject recognition processing performed by the image recognition circuit <b>404</b> may be the same subject recognition processing as that executed by the server <b>200</b> in the first embodiment.</p><p id="p-0128" num="0127">An image processing circuit <b>405</b> generates signals and image data, obtains and/or generates various types of information, and so on by applying predetermined image processing to the image data stored in the RAM <b>403</b>. The image processing that can be applied by the image processing circuit <b>405</b> may be the same as the image processing applied by the image processing circuit <b>107</b> of the image capture apparatus <b>100</b>, the image processing applied by the server <b>200</b> using the image processing program <b>211</b>, and so on.</p><p id="p-0129" num="0128">An operating unit <b>406</b> includes input devices with which a user inputs instructions to the edge device <b>400</b>, and is typically a keyboard, a mouse, a touch pad, or the like. When the display circuit <b>408</b> is a touch screen, the touch panel provided in the touch screen is included in the operating unit <b>406</b>.</p><p id="p-0130" num="0129">The recording circuit <b>407</b> records data into a recording medium, reads out data recorded in the recording medium, and so on. The recording medium may be a removable memory card, or may be a built-in storage device, for example.</p><p id="p-0131" num="0130">The display circuit <b>408</b> is a display device such as an LCD, and performs displays corresponding to the image data stored in a video memory region of the RAM <b>403</b>. The display circuit <b>408</b> displays data stored in the recording circuit <b>407</b>, GUI elements of applications and an OS, and the like.</p><p id="p-0132" num="0131">The edge device <b>400</b> according to the present embodiment includes two communication circuits <b>409</b> and <b>410</b>. It is assumed here that the communication circuit <b>410</b> is used in communication with an external device on the network <b>300</b> (e.g., the server <b>200</b>) and the communication circuit <b>409</b> is used in communication with the image capture apparatus <b>100</b>. Note that a single communication circuit that supports a plurality of communication methods may be provided.</p><p id="p-0133" num="0132">Alternatively, the communication with both the external device on the network <b>300</b> and the image capture apparatus <b>100</b> may be carried out using only one of the communication circuits <b>409</b> and <b>410</b>.</p><p id="p-0134" num="0133">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>, and the server <b>200</b> will be described next with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In the following, the control circuit <b>101</b> (CPU) is the main entity actually performing operations described as being executed by the image capture apparatus <b>100</b>, the control circuit <b>201</b> (CPU) is the main entity actually performing operations described as being executed by the server <b>200</b>, and the control circuit <b>401</b> (CPU) is the main entity actually performing operations described as being executed by the edge device <b>400</b>. In <figref idref="DRAWINGS">FIG. <b>6</b></figref>, steps in which the same operations as those described in the first embodiment are performed have been given the same reference numerals as those in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. It is assumed that settings, information, and the like necessary for communication with the server <b>200</b> and the edge device <b>400</b> are registered in advance and stored in the ROM <b>102</b>. Additionally, if it is necessary to register the edge device <b>400</b> as a communication partner prior to communicating with the edge device <b>400</b>, it is assumed that such registration has already been performed in advance.</p><p id="p-0135" num="0134">Steps S<b>401</b> and S<b>405</b> are the same as in the first embodiment, except that the communication partner is the edge device <b>400</b> instead of the server <b>200</b>.</p><p id="p-0136" num="0135">In steps S<b>402</b> to S<b>404</b>, the edge device <b>400</b> executes the same processing as that performed by the server in the first embodiment.</p><p id="p-0137" num="0136">In step S<b>402</b>, the edge device <b>400</b> receives the proxy image data from the image capture apparatus <b>100</b> through the communication circuit <b>409</b>. The edge device <b>400</b> stores the received proxy image data in the RAM <b>403</b>.</p><p id="p-0138" num="0137">In step S<b>403</b>, the edge device <b>400</b> executes the above-described subject recognition processing on the image data stored in the RAM <b>403</b> using the image recognition circuit <b>404</b>. The method of the subject recognition processing need not be the same as the method used by the server <b>200</b>.</p><p id="p-0139" num="0138">In step S<b>404</b>, the edge device <b>400</b> transmits the tag information (subject recognition result) added in step S<b>403</b> to the image capture apparatus <b>100</b> through the communication circuit <b>409</b>.</p><p id="p-0140" num="0139">In steps S<b>405</b> and S<b>406</b>, the image capture apparatus <b>100</b> determines the RAW data, among the RAW data corresponding to the proxy image data transmitted in step S<b>401</b>, to which the external apparatus (here, the server <b>200</b>) is to apply the image processing, in the same manner as in the first embodiment.</p><p id="p-0141" num="0140">Then, in step S<b>407</b>, the image capture apparatus <b>100</b> transmits the RAW data which has been determined to the server <b>200</b>.</p><p id="p-0142" num="0141">The operations of the server <b>200</b> in steps S<b>408</b> to S<b>410</b> are the same as in the first embodiment.</p><p id="p-0143" num="0142">Note that in step S<b>410</b>, the server <b>200</b> may transmit the image data which has been subjected to the development processing to the edge device <b>400</b> instead of, or in addition to, transmitting that image data to the image capture apparatus <b>100</b> over the network <b>300</b>. In this case, information necessary for transmitting the data to the edge device <b>400</b> (a destination address or the like) is registered in the server <b>200</b> in advance, or is communicated to the server <b>200</b> when the image capture apparatus <b>100</b> transmits the RAW data.</p><p id="p-0144" num="0143">The operations of the image capture apparatus <b>100</b> in steps S<b>411</b> to S<b>413</b> are the same as in the first embodiment.</p><p id="p-0145" num="0144">When the edge device <b>400</b> receives the image data which has been subjected to the development processing from the server <b>200</b>, in step S<b>1011</b>, the edge device <b>400</b> stores the received image data in the RAM <b>403</b>. Then, in step S<b>1012</b>, the edge device <b>400</b> displays the image data stored in the RAM <b>403</b> in the display circuit <b>408</b>, and records the image data into the recording circuit <b>407</b>.</p><p id="p-0146" num="0145">In the present embodiment, the image processing for the proxy image data is performed by the edge device <b>400</b> instead of the server <b>200</b>. According to the present embodiment, the amount of communication data between the image capture apparatus <b>100</b> and the server <b>200</b> can be reduced compared to the first embodiment, and thus communication costs can be reduced, the effects of congestion and a drop in quality in the network <b>300</b> can be alleviated, and so on, in addition to the effects described in the first embodiment.</p><p id="p-0147" num="0146">As in the first embodiment, the image processing applied to the proxy image data, the image processing applied to the RAW data by the server <b>200</b>, and the like are merely examples, and may be other kinds of image processing, in the present embodiment as well. Additionally, the image data to which the image processing is ultimately applied by the server <b>200</b> is not limited to RAW data, and may be developed image data for recording.</p><p id="p-0148" num="0147">Furthermore, the image capture apparatus <b>100</b> may allow the user to confirm the image data determined to be the image data to which the image processing will ultimately be applied by the server <b>200</b> before transmitting that image data to the server <b>200</b>. At this time, the user may change one or more instances of the image data determined by the image capture apparatus.</p><heading id="h-0010" level="1">Third Embodiment</heading><p id="p-0149" num="0148">A third embodiment of the present invention will be described next. An image processing system according to the present embodiment differs from the second embodiment in terms of the configuration and operations of the edge device. The descriptions will therefore focus on the differences from the second embodiment.</p><p id="p-0150" num="0149"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram illustrating an example of the functional configuration of an edge device <b>400</b>&#x2032; according to the present embodiment, obtained by adding an evaluation circuit <b>411</b> to the edge device <b>400</b> of the second embodiment. The other function blocks are the same as in the second embodiment and will therefore not be described.</p><p id="p-0151" num="0150">In the present embodiment, the image capture apparatus <b>100</b> transmits a plurality of frames' worth of RAW data to the edge device <b>400</b>&#x2032; rather than the server <b>200</b>. Then, the edge device <b>400</b>&#x2032; selects RAW data on which it is thought that the development processing by the server <b>200</b> will have a great effect, and transfers that RAW data to the server <b>200</b>. The evaluation circuit <b>411</b> is an element for determining the RAW data to be transmitted. Although illustrated as an independent constituent element in <figref idref="DRAWINGS">FIG. <b>7</b></figref> for the sake of simplicity, the evaluation circuit <b>411</b> may be implemented by the control circuit <b>401</b> (CPU) executing an image processing system application stored in the ROM <b>402</b>.</p><p id="p-0152" num="0151">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>&#x2032;, and the server <b>200</b> will be described next with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In the following, the control circuit <b>101</b> (CPU) is the main entity actually performing operations described as being executed by the image capture apparatus <b>100</b>, the control circuit <b>201</b> (CPU) is the main entity actually performing operations described as being executed by the server <b>200</b>, and the control circuit <b>401</b> (CPU) is the main entity actually performing operations described as being executed by the edge device <b>400</b>&#x2032;. In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, steps in which the same operations as those described in the first and second embodiments are performed have been given the same reference numerals as those in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>6</b></figref>. It is assumed that settings, information, and the like necessary for communication with the server <b>200</b> and the edge device <b>400</b>&#x2032; are registered in advance and stored in the ROM <b>102</b>. Additionally, if it is necessary to register the edge device <b>400</b>&#x2032; as a communication partner prior to communicating with the edge device <b>400</b>&#x2032;, it is assumed that such registration has already been performed in advance.</p><p id="p-0153" num="0152">In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the operations of steps S<b>401</b> to S<b>406</b> are the same as in the second embodiment and will therefore not be described. In step S<b>407</b>, the image capture apparatus <b>100</b> transmits the RAW data to which the image processing is to be applied by the server <b>200</b> to the edge device <b>400</b>&#x2032;.</p><p id="p-0154" num="0153">In step S<b>1208</b>, the edge device <b>400</b>&#x2032; stores the received RAW data in the RAM <b>403</b> or the recording circuit <b>407</b>.</p><p id="p-0155" num="0154">In step S<b>1209</b>, the edge device <b>400</b>&#x2032; uses the evaluation circuit <b>411</b> to calculate an evaluation value for each instance of the RAW data which has been received.</p><p id="p-0156" num="0155">An example of operations performed by the evaluation circuit <b>411</b> to calculate the evaluation value will be described here with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The evaluation circuit <b>411</b> calculates an evaluation value serving as an indicator of the magnitude of the effect of the image processing applied by the server <b>200</b>, for each piece of image data received from the image capture apparatus <b>100</b>.</p><p id="p-0157" num="0156">A method of calculating the evaluation value in which the effect of the image processing applied by the server <b>200</b> is greater for images having a higher number of edges will be described here as an example. Note, however, that the method for calculating the evaluation value is not particularly limited, and can be changed as appropriate by, for example, changing the aforementioned image processing system application. The evaluation value depends on the image processing applied by the server <b>200</b>, and thus the evaluation value calculated by the evaluation circuit <b>411</b> may be changed in accordance with updates made to the image processing program <b>211</b> in the server <b>200</b>, for example.</p><p id="p-0158" num="0157">In step S<b>1601</b>, the evaluation circuit <b>411</b> reads out one frame's worth of the RAW data and generates a luminance signal. It is assumed here that the image sensor of the image capture apparatus <b>100</b> has color filters configured as a primary color Bayer array. In this case, the RAW data is data in which each pixel has a single color component value, as indicated in a of <figref idref="DRAWINGS">FIG. <b>10</b></figref>. Note that <figref idref="DRAWINGS">FIG. <b>10</b></figref> only illustrates an image region of the RAW data that has four pixels each in the horizontal and vertical directions. R stands for red, G for green, and B for blue.</p><p id="p-0159" num="0158">First, the evaluation circuit <b>411</b> generates color plane image data (b in <figref idref="DRAWINGS">FIG. <b>10</b></figref>) by separating the RAW data into the respective color components and inserting zeros at pixel locations where no color components are present. Then, the evaluation circuit <b>411</b> applies publicly known interpolation processing to the color plane image data to interpolate the values of pixels where zeros have been inserted (c in <figref idref="DRAWINGS">FIG. <b>10</b></figref>).</p><p id="p-0160" num="0159">Next, using each instance of the post-interpolation color plane image data, the evaluation circuit <b>411</b> finds a luminance value Y at each pixel location through the following Equation <b>1</b>, and generates luminance data (d in <figref idref="DRAWINGS">FIG. <b>10</b></figref>).</p><p id="p-0161" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Y=</i>0.3&#xd7;<i>R+</i>0.6&#xd7;<i>G+</i>0.1&#xd7;<i>B </i>&#x2003;&#x2003;(Equation 1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0162" num="0160">Here, R, G, and B are values in each instance of the color plane image data.</p><p id="p-0163" num="0161">In step S<b>1602</b>, the evaluation circuit <b>411</b> applies edge detection processing, using one pixel in the luminance data generated in step S<b>1601</b> as a target pixel. The edge detection may be, for example, spatial filter processing using a Sobel filter, as illustrated in <figref idref="DRAWINGS">FIGS. <b>11</b>A and <b>11</b>B</figref>. <figref idref="DRAWINGS">FIG. <b>11</b>A</figref> illustrates a Sobel filter for calculating a vertical direction edge strength E<sub>y </sub>of the target pixel, and <figref idref="DRAWINGS">FIG. <b>11</b>B</figref> illustrates a Sobel filter for calculating a horizontal direction edge strength E<sub>x </sub>of the target pixel. The values obtained by applying the filters to the values of the eight pixels surrounding the target pixel are the vertical direction edge strength E<sub>y </sub>and the horizontal direction edge strength E. The spatial filter processing is a sum-of-products operation for the target pixel, the value of each of the pixels surrounding the target pixel, and a coefficient of a position corresponding to the spatial filter. Note that spatial filter coefficient and size illustrated in <figref idref="DRAWINGS">FIGS. <b>11</b>A and <b>11</b>B</figref> are merely examples. The edge strengths may be detected using other filters or methods.</p><p id="p-0164" num="0162">The evaluation circuit <b>411</b> finds an edge strength E (change amount) of the target pixel through the following Equation 2, from the vertical direction edge strength E<sub>y </sub>and the horizontal direction edge strength E<sub>x </sub>calculated for the target pixel.</p><p id="p-0165" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>E</i>={(<i>Ex</i><sup>2</sup><i>+Ey</i><sup>2</sup>)}<sup>1/2 </sup>&#x2003;&#x2003;(Equation 2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0166" num="0163">In step S<b>1603</b>, the evaluation circuit <b>411</b> determines whether or not the edge strength E found in step S<b>1602</b> is greater than or equal to a threshold. If it is determined that the edge strength E is greater than or equal to the threshold, an edge flag is added to the target pixel in step S<b>1604</b>, after which step S<b>1605</b> is executed. On the other hand, if the evaluation circuit <b>411</b> does not determine that the edge strength E is greater than or equal to the threshold in step S<b>1603</b>, step S<b>1604</b> is skipped, and step S<b>1605</b> is then executed.</p><p id="p-0167" num="0164">In step S<b>1605</b>, the evaluation circuit <b>411</b> determines whether or not the edge detection processing has been applied to all of the pixels in the luminance data. If it is determined that the edge detection processing has been applied to all of the pixels, in step S<b>1606</b>, the evaluation circuit <b>411</b> calculates a number of pixels to which the edge flag has been added as the evaluation value of the RAW data. On the other hand, if it is not determined that the edge detection processing has been applied to all of the pixels, the processing from step S<b>1602</b> is applied to the pixels to which the edge detection processing has not yet been applied.</p><p id="p-0168" num="0165">The evaluation circuit <b>411</b> calculates the evaluation value for each instance of RAW data which is stored.</p><p id="p-0169" num="0166">Returning to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, in step S<b>1210</b>, the edge device <b>400</b>&#x2032; determines the RAW data to transmit to the server <b>200</b> on the basis of the evaluation value calculated in step S<b>1209</b>. Specifically, RAW data for which the evaluation value is greater than or equal to a pre-set threshold is transmitted to the server <b>200</b>. The configuration may be such that the threshold can be adjusted in response to user instructions obtained via a user interface.</p><p id="p-0170" num="0167">In step S<b>1211</b>, the edge device <b>400</b>&#x2032; transmits the RAW data determined in step S<b>1210</b> to the server <b>200</b> over the network <b>300</b>.</p><p id="p-0171" num="0168">The operations of steps S<b>408</b> to S<b>410</b>, performed by the server <b>200</b>, the operations of steps S<b>1011</b> and S<b>1012</b>, performed by the edge device <b>400</b>&#x2032;, and the operations of steps S<b>411</b> to S<b>413</b>, performed by the image capture apparatus <b>100</b>, are the same as in the second embodiment and will therefore not be described here.</p><p id="p-0172" num="0169">According to the present embodiment, the image data to which image processing is to be applied by the server <b>200</b> is evaluated by the edge device <b>400</b>&#x2032; for transition to the server <b>200</b>, and RAW data on which the image processing by the server <b>200</b> is considered to have a great effect is transmitted to the server <b>200</b>. Accordingly, the image capture apparatus <b>100</b> can receive a processing result of an image for which the image processing applied by the server <b>200</b> has a greater effect.</p><p id="p-0173" num="0170">Note that the image data to be transmitted may be presented to the user before the edge device <b>400</b>&#x2032; transmits the image data to the server <b>200</b>, and the image data may be transmitted after obtaining confirmation from the user. At this time, the configuration may be such that the user can select image data not to be transmitted.</p><heading id="h-0011" level="1">Fourth Embodiment</heading><p id="p-0174" num="0171">A fourth embodiment of the present invention will be described next. An image processing system according to the present embodiment is configured so that the image data evaluation executed by the edge device <b>400</b>&#x2032; in the third embodiment is performed by the server <b>200</b>, and the RAW data to which the image processing is applied is been determined. The descriptions will therefore focus on the differences from the third embodiment.</p><p id="p-0175" num="0172"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a block diagram illustrating an example of the functional configuration of a server <b>200</b>&#x2032; according to the present embodiment, which is configured by adding an effect determination program <b>212</b> to the ROM <b>203</b> of the server <b>200</b> described in the first embodiment. The other function blocks are the same as in the first embodiment and will therefore not be described.</p><p id="p-0176" num="0173">By being executed by the control circuit <b>201</b>, the effect determination program <b>212</b> implements the same functions as the evaluation circuit <b>411</b> of the edge device <b>400</b>&#x2032; according to the third embodiment.</p><p id="p-0177" num="0174">The control circuit <b>201</b> evaluates image data (RAW data) received from the image capture apparatus <b>100</b> by executing the effect determination program <b>212</b>. Then, the RAW data to which processing is to be applied by the image processing program <b>211</b> is determined on the basis of a result of the evaluation.</p><p id="p-0178" num="0175">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>, and the server <b>200</b>&#x2032; will be described next with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>13</b></figref>. In the following, the control circuit <b>101</b> (CPU) is the main entity actually performing operations described as being executed by the image capture apparatus <b>100</b>, the control circuit <b>201</b> (CPU) is the main entity actually performing operations described as being executed by the server <b>200</b>&#x2032;, and the control circuit <b>401</b> (CPU) is the main entity actually performing operations described as being executed by the edge device <b>400</b>. In <figref idref="DRAWINGS">FIG. <b>13</b></figref>, steps in which the same operations as those described in the first to third embodiments are performed have been given the same reference numerals as those in <figref idref="DRAWINGS">FIGS. <b>2</b>, <b>6</b></figref>, and <b>8</b>. Additionally, if it is necessary to register the edge device <b>400</b>&#x2032; as a communication partner prior to communicating with the edge device <b>400</b>&#x2032;, it is assumed that such registration has already been performed in advance.</p><p id="p-0179" num="0176">In <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the operations of steps S<b>401</b> to <b>5406</b> are the same as in the second embodiment and will therefore not be described. In step S<b>407</b>, the image capture apparatus <b>100</b> transmits the RAW data to which the image processing is to be applied by the server <b>200</b>&#x2032; to the server <b>200</b>&#x2032; in the same manner as in the first embodiment.</p><p id="p-0180" num="0177">In step S<b>408</b>, upon receiving the RAW data from the image capture apparatus <b>100</b>, the server <b>200</b>&#x2032; stores the RAW data in the RAM <b>202</b>, the storage device <b>205</b>, or the like.</p><p id="p-0181" num="0178">In step S<b>1209</b>, the control circuit <b>201</b> executes the effect determination program <b>212</b> stored in the ROM <b>203</b>, and calculates an evaluation value for the RAW data received from the image capture apparatus <b>100</b>. This processing may be the same as that described in the third embodiment.</p><p id="p-0182" num="0179">In step S<b>1210</b>, on the basis of the evaluation value calculated in step S<b>1209</b>, the control circuit <b>201</b> determines whether or not to apply the image processing by the image processing program <b>211</b> to the RAW data received from the image capture apparatus <b>100</b>. Specifically, the control circuit <b>201</b> determines to apply the image processing only to RAW data for which the image processing by the image processing program <b>211</b> is considered to have a great effect.</p><p id="p-0183" num="0180">In step S<b>409</b>, the control circuit <b>201</b> executes the image processing program <b>211</b>, and applies the image processing (here, development processing) to the RAW data determined in step S<b>1210</b>.</p><p id="p-0184" num="0181">The subsequent operations of step S<b>410</b>, performed by the server <b>200</b>&#x2032;, the operations of steps S<b>1011</b> and S<b>1012</b>, performed by the edge device <b>400</b>, and the operations of steps S<b>411</b> to S<b>413</b>, performed by the image capture apparatus <b>100</b>, are the same as in the second embodiment and will therefore not be described here.</p><p id="p-0185" num="0182">According to the present embodiment, the server <b>200</b>&#x2032; evaluates the image data to which the server <b>200</b>&#x2032; is to apply image processing, and the image processing is applied only to RAW data for which the image processing by the server <b>200</b>&#x2032; is considered to have a great effect. Accordingly, the image capture apparatus <b>100</b> can receive a processing result only for an image for which the image processing applied by the server <b>200</b> has a greater effect.</p><p id="p-0186" num="0183">Note that the configuration may be such that the image data is transmitted to the image capture apparatus <b>100</b>, and presented to the user through the display circuit <b>109</b> of the image capture apparatus <b>100</b>, before the image processing is applied by the server <b>200</b>&#x2032;. The image processing may then be applied after confirmation has been received from the image capture apparatus <b>100</b>. At this time, the configuration may be such that the user can use the image capture apparatus <b>100</b> to select image data to which the image processing is not to be applied.</p><heading id="h-0012" level="1">Fifth Embodiment</heading><p id="p-0187" num="0184">A fifth embodiment of the present invention will be described next. An image processing system according to the present embodiment is configured so that the processing performed by the server <b>200</b> upon receiving the RAW data according to the first embodiment is performed by the edge device <b>400</b>. The configuration of the image processing system, as well as the functional configurations of the image capture apparatus <b>100</b>, the server <b>200</b>, and the edge device <b>400</b> according to the present embodiment, may be the same as in the second embodiment.</p><p id="p-0188" num="0185">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>, and the server <b>200</b> will be described next with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>14</b></figref>. In the following, the control circuit <b>101</b> (CPU) is the main entity actually performing operations described as being executed by the image capture apparatus <b>100</b>, the control circuit <b>201</b> (CPU) is the main entity actually performing operations described as being executed by the server <b>200</b>, and the control circuit <b>401</b> (CPU) is the main entity actually performing operations described as being executed by the edge device <b>400</b>. In <figref idref="DRAWINGS">FIG. <b>14</b></figref>, steps in which the same operations as those described in the first embodiment are performed have been given the same reference numerals as those in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Additionally, if it is necessary to register the edge device <b>400</b> as a communication partner prior to communicating with the edge device <b>400</b>, it is assumed that such registration has already been performed in advance.</p><p id="p-0189" num="0186">In <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the operations of steps S<b>401</b> to S<b>406</b> are the same as in the first embodiment and will therefore not be described. In step S<b>407</b>, the image capture apparatus <b>100</b> transmits the RAW data to the edge device <b>400</b> instead of the server <b>200</b>.</p><p id="p-0190" num="0187">In step S<b>408</b>, upon receiving the RAW data from the image capture apparatus <b>100</b>, the edge device <b>400</b> stores the data in the RAM <b>403</b>, the recording circuit <b>407</b>, or the like.</p><p id="p-0191" num="0188">In step S<b>409</b>, the edge device <b>400</b> uses the image processing circuit <b>405</b> to apply the same image processing (development processing) to the RAW data as that applied by the server <b>200</b> executing the image processing program <b>211</b> in the first embodiment.</p><p id="p-0192" num="0189">In step S<b>1510</b>, the edge device <b>400</b> displays the post-development processing image data in the display circuit <b>408</b>.</p><p id="p-0193" num="0190">Then, in step S<b>410</b>, the edge device <b>400</b> transmits the post-development processing image data to the image capture apparatus <b>100</b>.</p><p id="p-0194" num="0191">The operations of the image capture apparatus <b>100</b> in steps S<b>411</b> to S<b>413</b> are the same as in the first embodiment and therefore will not be described here.</p><p id="p-0195" num="0192">In the present embodiment, the image processing for the original data of the proxy image data is performed by the edge device <b>400</b> instead of the server <b>200</b>. The original data of the proxy image data has a large size, and has a particularly large size if the data is RAW data. Accordingly, having the edge device <b>400</b> apply the image processing instead of the server <b>200</b> makes it possible to greatly reduce the amount of image data exchanged with the server <b>200</b> over the network <b>300</b>. This makes it possible to greatly reduce the communication costs for using the network <b>300</b>, server costs, and the like.</p><p id="p-0196" num="0193">Variation</p><p id="p-0197" num="0194">The foregoing embodiment describes a configuration in which the image capture apparatus <b>100</b> determines the image data to which the image processing is applied by the external apparatus. In this case, as described in the embodiment, it is necessary to provide the image capture apparatus with a function (e.g., a program, a determination standard, or the like) for the image capture apparatus to determine, on the basis of a result of processing the proxy image data, the image data to which the image processing is applied by the external apparatus.</p><p id="p-0198" num="0195">As described earlier, the hardware resources of the image capture apparatus are much more limited, both in terms of capacity and performance, than an edge device or a server, and it is therefore not easy to update the provided functions. Thus when an image processing program in the external apparatus has been updated, it is not always possible to update the function for the image capture apparatus for determining the image data to which the image processing is applied by the external apparatus, even when it is desirable to do so. In this case, the image capture apparatus can no longer appropriately determine the image data to be transmitted to the external apparatus.</p><p id="p-0199" num="0196">As such, it is conceivable to have the server <b>200</b> or the edge device <b>400</b> perform the processing for determining the image data to which the image processing is applied by the external apparatus, executed by the image capture apparatus <b>100</b> in steps S<b>405</b> and S<b>406</b> in the foregoing embodiment. In this case, the server <b>200</b> or the edge device <b>400</b> requests the image capture apparatus <b>100</b> to transmit the determined image data, and the image capture apparatus transmits the requested image data to the server <b>200</b> or the edge device <b>400</b>. For example, in the first embodiment, after performing the subject recognition processing in step S<b>403</b>, the server <b>200</b> makes, in step S<b>404</b>, a request, to the image capture apparatus <b>100</b>, for the original data (which may be developed image data or RAW data) of the proxy image data in which a registered person has been detected. Then, in step S<b>407</b>, the image capture apparatus <b>100</b> transmits the original data to the server <b>200</b>. Likewise, in the other embodiments as well, the apparatus that performs the image processing on the proxy image data can make a request for the original data of the proxy image data to the image capture apparatus. Alternatively, the apparatus that performs the image processing on the proxy image data (e.g., the edge device) may request the image capture apparatus to transmit the original data of the proxy image data to another apparatus (e.g., the server).</p><p id="p-0200" num="0197">Note that the edge device <b>400</b> is closer to the server <b>200</b> than the image capture apparatus <b>100</b> in terms of performance, and it is also easy to update the software of the edge device <b>400</b>. As such, even if the image processing program <b>211</b> of the server <b>200</b> has been updated and the determination standard for determining the image data that should be processed by the server <b>200</b> has changed, such a situation can be handled with ease. For example, a determination standard suited to the updated image processing program <b>211</b> may be transmitted from the server <b>200</b> to the edge device <b>400</b>, and the determination standard used by the edge device <b>400</b> may be updated. Alternatively, the determination standard may be updated by updating the image processing system application of the edge device <b>400</b>.</p><p id="p-0201" num="0198">Furthermore, although the foregoing embodiments have been described using the image capture apparatus <b>100</b> as an example, the present invention can be applied in any information processing apparatus capable of handling image data.</p><heading id="h-0013" level="1">Sixth Embodiment</heading><p id="p-0202" num="0199">The present embodiment relates to an image processing system having a configuration in which the image capture apparatus <b>100</b>, the edge device <b>400</b> (first external apparatus), and the server <b>200</b> (second external apparatus) are connected to the network <b>300</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. However, parts of the functional configurations of the edge device <b>400</b> and the server <b>200</b> are different, and these will therefore be referred to as an edge device <b>400</b>&#x2033; and a server <b>200</b>&#x2033;.</p><p id="p-0203" num="0200">As illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, in the present embodiment, it is assumed that the communication circuit <b>110</b> of the image capture apparatus <b>100</b> includes a communication interface for the edge device <b>400</b>&#x2033; in addition to a communication interface for the network <b>300</b> (the server <b>200</b>&#x2033;). It is also assumed in the present embodiment that the communication between the image capture apparatus <b>100</b> and the edge device <b>400</b>&#x2033; is charged.</p><p id="p-0204" num="0201"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a block diagram illustrating an example of the functional configuration of the image capture apparatus <b>100</b>. With the exception of the configuration of the communication circuit <b>110</b>, the configuration may be the same as in the first embodiment, and thus function blocks aside from the communication circuit <b>110</b> will not be described.</p><p id="p-0205" num="0202">In the present embodiment, the communication circuit <b>110</b> implements communication between the image capture apparatus <b>100</b> and the server <b>200</b>&#x2033; over the network <b>300</b>, and wireless communication between the image capture apparatus <b>100</b> and the edge device <b>400</b>&#x2033;. The control circuit <b>101</b> exchanges image data and the like with the server <b>200</b>&#x2033;, the edge device <b>400</b>&#x2033;, and the like through the communication circuit <b>110</b>.</p><p id="p-0206" num="0203"><figref idref="DRAWINGS">FIG. <b>16</b>A</figref> is a block diagram illustrating an example of the functional configuration of the server <b>200</b>&#x2033;. The server <b>200</b>&#x2033; may have the same configuration as the server <b>200</b> described in the first embodiment with reference to <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>, except that it is not necessary to store the image recognition program in ROM <b>203</b>&#x2032;. Accordingly, matters which are common to the server <b>200</b> will not be described. Although the following will describe a case in which an image processing function provided by the server <b>200</b>&#x2033; is RAW data development processing, a different image processing function may be provided instead.</p><p id="p-0207" num="0204">In the present embodiment, the communication circuit <b>204</b> implements communication between the server <b>200</b>&#x2033; and the image capture apparatus <b>100</b> over the network <b>300</b>, and communication between the server <b>200</b>&#x2033; and the edge device <b>400</b>&#x2033; over the network <b>300</b>. The control circuit <b>201</b> exchanges image data and the like with the image capture apparatus <b>100</b>, the edge device <b>400</b>&#x2033;, and the like through the communication circuit <b>204</b>.</p><p id="p-0208" num="0205">Note that the network <b>300</b> may be any publicly known network that supports data communication between the image capture apparatus <b>100</b> and edge device <b>400</b>&#x2033; and the server <b>200</b>&#x2033;, and thus the specific configuration thereof will not be described here.</p><p id="p-0209" num="0206"><figref idref="DRAWINGS">FIG. <b>16</b>B</figref> is a block diagram illustrating an example of the functional configuration of the edge device <b>400</b>&#x2033;. Aside from including the evaluation circuit <b>411</b> instead of the image recognition circuit <b>404</b>, the edge device <b>400</b>&#x2033; may have the same configuration as the edge device <b>400</b> described in the second embodiment with reference to <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>. Accordingly, matters which are common to the edge device <b>400</b> will not be described.</p><p id="p-0210" num="0207">In some embodiments, the evaluation circuit <b>411</b> calculates an evaluation value from image data. The evaluation value is used, for example, to determine an order of the transmission of RAW data subjected to image processing by an external apparatus with respect to the image capture apparatus <b>100</b>. The evaluation circuit <b>411</b> may be the same as that described in the third embodiment with reference to <figref idref="DRAWINGS">FIGS. <b>9</b> to <b>11</b>B</figref>. In an embodiment which does not use the evaluation value, the evaluation circuit <b>411</b> is not necessary.</p><p id="p-0211" num="0208">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>&#x2033;, and the server <b>200</b>&#x2033; will be described next with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>17</b></figref>. In the following descriptions in the present specification, the operations described as being primarily performed by the image capture apparatus <b>100</b> are realized by the control circuit <b>101</b> (CPU) controlling other constituent elements of the image capture apparatus <b>100</b>. Additionally, operations described as being primarily performed by the server <b>200</b>&#x2033; are realized by the control circuit <b>201</b> (CPU) controlling other constituent elements of the server <b>200</b>&#x2033;. Furthermore, operations described as being primarily performed by the edge device <b>400</b>&#x2033; are realized by the control circuit <b>401</b> (CPU) controlling other constituent elements of the edge device <b>400</b>&#x2033;. Unless otherwise noted in the following descriptions, when specific information, data, and the like is communicated between devices, other information, data, and the like not explicitly mentioned can also be communicated.</p><p id="p-0212" num="0209">It is assumed that settings, information, and the like necessary for the image capture apparatus <b>100</b> to communicate with the server <b>200</b>&#x2033; and the edge device <b>400</b>&#x2033; are stored in the ROM <b>102</b> in advance. Additionally, if it is necessary to register the edge device <b>400</b>&#x2033; as a communication partner prior to the image capture apparatus <b>100</b> communicating with the edge device <b>400</b>&#x2033;, it is assumed that such registration has already been performed in advance.</p><p id="p-0213" num="0210">In step S<b>1701</b>, the image capture apparatus <b>100</b> transmits the proxy image data recorded by the recording circuit <b>108</b> from the communication circuit <b>110</b> to the edge device <b>400</b>&#x2033;. This proxy image data is an example of information pertaining to image data recorded by the recording circuit <b>108</b>. The timing at which step S<b>1701</b> is executed is not particularly limited. For example, the step may be started upon confirmation that unprocessed RAW data is present in the recording circuit <b>108</b>, or may be started in response to a user instruction. The transmitted proxy image data may be data which corresponds to the RAW data, data which corresponds to developed image data, or data selected by the user. Alternatively, all of the proxy image data may be transmitted.</p><p id="p-0214" num="0211">Furthermore, proxy image data corresponding to RAW data which has not been subjected to the development processing (unprocessed RAW data) may be transmitted. This makes it possible, for example, to distinguish RAW data for which corresponding image data in a generic format (e.g., the JPEG format) is not recorded as unprocessed RAW data. Alternatively, a filename that makes it possible to distinguish whether or not the RAW data is unprocessed may be added, and the unprocessed RAW data may be determined from the filename. It is assumed that in step S<b>1701</b>, at least two pieces (frames) of the proxy image data are transmitted to the edge device <b>400</b>&#x2033;.</p><p id="p-0215" num="0212">In step S<b>1702</b>, the edge device <b>400</b>&#x2033; receives the proxy image data from the image capture apparatus <b>100</b> through the communication circuit <b>204</b>. The edge device <b>400</b>&#x2033; stores the received proxy image data in the RAM <b>403</b>.</p><p id="p-0216" num="0213">In step S<b>1703</b>, the edge device <b>400</b>&#x2033; displays a list of images based on the proxy image data stored in the RAM <b>403</b> in a selectable format in the display circuit <b>408</b>. For example, the edge device <b>400</b>&#x2033; displays an image selection screen, in which a list of images based on the proxy image data (proxy images) is displayed, in the display circuit <b>408</b>, and makes a notification prompting the user to select an image on which the development processing is to be executed. Note that the number of proxy images displayed by the edge device <b>400</b>&#x2033; at one time in the display circuit <b>408</b>, the display size of the proxy images, and the display order are not particularly limited. Reducing the display size enables a larger number of proxy images to be displayed at the same time and therefore increases the usefulness of the list, whereas increasing the display size makes it easier to see the details of the images. Note that it is not necessary to display all of the proxy images based on the proxy image data stored in the RAM <b>403</b> in a single screen. A publicly known method such as scrolling, turning pages, or the like may be used to split the list of proxy images across a plurality of screens.</p><p id="p-0217" num="0214">By operating the operating unit <b>111</b> of the edge device <b>400</b>&#x2033;, a user of the edge device <b>400</b>&#x2033; (who may or may not be the same as the user of the image capture apparatus <b>100</b>) selects one or more of the proxy images. For example, if the display circuit <b>408</b> is a touch screen, the user can select an image by touching the image. The user may also select an image by operating a key, a button, or the like of the operating unit <b>111</b>. The image selection performed here corresponds to selecting RAW data suited to the development processing by the server <b>200</b>&#x2033;.</p><p id="p-0218" num="0215">In step S<b>1704</b>, the edge device <b>400</b>&#x2033; transmits, to the image capture apparatus <b>100</b>, selected file information (e.g., a filename) specifying the proxy image data corresponding to the proxy image selected by the user. Note that the selected file information transmitted from the edge device <b>400</b>&#x2033; to the image capture apparatus <b>100</b> in step S<b>1704</b> can vary in accordance with information received from the image capture apparatus <b>100</b> along with the proxy image data. For example, when information specifying the RAW data corresponding to the proxy image data has been obtained, the edge device <b>400</b>&#x2033; may transmit information specifying the RAW data corresponding to the selected proxy image (e.g., a filename) to the image capture apparatus <b>100</b> as the selected file information. Any other information that enables the image capture apparatus <b>100</b> to specify the RAW data selected by the user to undergo development processing by the server <b>200</b>&#x2033; can be transmitted from the edge device <b>400</b>&#x2033; to the image capture apparatus <b>100</b> as the selected file information.</p><p id="p-0219" num="0216">In step S<b>1705</b>, the image capture apparatus <b>100</b> stores the information received from the edge device <b>400</b>&#x2033; (the selected file information) in the RAM <b>103</b>.</p><p id="p-0220" num="0217">In step S<b>1706</b>, the image capture apparatus <b>100</b> specifies the RAW data on the basis of the selected file information. The image capture apparatus <b>100</b> then reads out the specified RAW data from the recording circuit <b>108</b> and transmits the data to the server <b>200</b>&#x2033; over the network <b>300</b>.</p><p id="p-0221" num="0218">In step S<b>1707</b>, the server <b>200</b>&#x2033; receives the RAW data and stores the data in the RAM <b>202</b>, the storage device <b>205</b>, or the like.</p><p id="p-0222" num="0219">In step S<b>1708</b>, the server <b>200</b>&#x2033; loads the image processing program <b>211</b> from the ROM <b>203</b>&#x2032; into the RAM <b>202</b> and executes the program. The image processing program <b>211</b> applies the development processing to the RAW data. The development processing includes, for example, white balance processing, lens aberration correction processing, noise reduction (NR) processing, color interpolation processing, gamma processing, and the like. Additionally, an image data file in a predetermined format such as the JPEG format may be generated by applying encoding processing after the development processing. The server <b>200</b>&#x2033; may receive the RAW data in step S<b>1707</b> and execute the image processing program <b>211</b> (apply the development processing) in step S<b>1708</b> in parallel.</p><p id="p-0223" num="0220">The development processing applied by the server <b>200</b>&#x2033; is processing based on a newer and/or more complex method than the processing performed by the image processing circuit <b>107</b> of the image capture apparatus <b>100</b>, and can therefore achieve a better processing result than development processing applied by the image capture apparatus <b>100</b>. This is due to the processing capabilities of the server <b>200</b>&#x2033; being higher than the processing capabilities of the image capture apparatus <b>100</b>, and the image processing program <b>211</b> executed by the server <b>200</b>&#x2033; being based on newer technology than the image processing circuit <b>107</b> of the image capture apparatus <b>100</b>.</p><p id="p-0224" num="0221">When the reception of the RAW data in step S<b>1707</b> is complete, in step S<b>1709</b>, the server <b>200</b>&#x2033; transmits a notification that the reception of the RAW data is complete to the image capture apparatus <b>100</b> over the network <b>300</b>.</p><p id="p-0225" num="0222">In step S<b>1710</b>, the image capture apparatus <b>100</b> receive the notification that reception is complete from the server <b>200</b>&#x2033;.</p><p id="p-0226" num="0223">In step S<b>1711</b>, the image capture apparatus <b>100</b> starts transmitting the RAW data, of the RAW data stored in the recording circuit <b>108</b>, which was not transmitted in step S<b>1707</b>, to the server <b>200</b>&#x2033;. Note that the RAW data transmitted to the server <b>200</b>&#x2033; in step S<b>1711</b> does not necessarily have to be all of the RAW data not transmitted in step S<b>1707</b> (i.e., the RAW data not selected by the user). This is because, for example, the RAW data stored in the recording circuit <b>108</b> can include RAW data for which the proxy image data was not transmitted to the server <b>200</b>&#x2033; in step S<b>1701</b>. Accordingly, the RAW data to be transmitted in step S<b>1711</b> may be determined from the RAW data for which the proxy image data has been transmitted in step S<b>1701</b>, RAW data captured on or near the date on which the RAW data for which the proxy image data has been transmitted in step S<b>1701</b>, and so on.</p><p id="p-0227" num="0224">In step S<b>1712</b>, the server <b>200</b>&#x2033; starts receiving the RAW data and stores the data in the RAM <b>202</b>, the storage device <b>205</b>, or the like.</p><p id="p-0228" num="0225">In step S<b>1713</b>, the server <b>200</b>&#x2033; executes the same development processing as that executed in step S<b>1708</b>, on the RAW data received in step S<b>1712</b>. Note that the server <b>200</b>&#x2033; executes step S<b>1713</b> after the execution of step S<b>1708</b> is complete. The server <b>200</b>&#x2033; stores the developed image data, obtained from steps S<b>1708</b> and S<b>1713</b>, in the storage device <b>205</b>.</p><p id="p-0229" num="0226">In <figref idref="DRAWINGS">FIG. <b>17</b></figref>, the developed image data obtained by the server <b>200</b>&#x2033; applying the development processing is stored in the server <b>200</b>&#x2033;. This is because it is assumed that the communication between the image capture apparatus <b>100</b> and the server <b>200</b>&#x2033; is charged according to the amount of data communicated. In such a case, communication costs can be reduced by moving to an environment with fixed communication costs, such as a home, and then obtaining the developed image data from the server <b>200</b>&#x2033;. However, the user may wish to confirm the developing result immediately, and thus the server <b>200</b>&#x2033; may transmit the image data to the image capture apparatus <b>100</b> in order from the image data for which the development processing is complete. The RAW data to which the development processing is applied in step S<b>1708</b> is RAW data selected by the user, and thus the user can confirm the developing result of desired RAW data before the developing result of other RAW data.</p><p id="p-0230" num="0227">In the present embodiment, the edge device is used to query the user as to the RAW data, among the RAW data present in the image capture apparatus, to which the user wishes to apply the development processing preferentially. Then, the image capture apparatus transmits the RAW data selected by the user to the external apparatus that executes the development processing before the other RAW data. Thus even if a large amount of RAW data is present in the image capture apparatus, the development processing can be executed preferentially on the user's desired RAW data. Accordingly, the wait time for the user can be reduced when executing the development processing using an external apparatus external to the image capture apparatus. Additionally, because the development processing is executed by the external apparatus, development processing using processing which cannot be executed by the image capture apparatus, development processing which is faster than that performed by the image capture apparatus, and the like can be performed.</p><p id="p-0231" num="0228">Although the present embodiment describes a configuration in which the proxy images are transmitted directly from the image capture apparatus <b>100</b> to the edge device <b>400</b>&#x2033;, the proxy images may be transmitted via the server <b>200</b>&#x2033;. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, in step S<b>1701</b>, the proxy image data is transmitted from the image capture apparatus <b>100</b> to the server <b>200</b>&#x2033;, and in step S<b>1721</b>, the server <b>200</b>&#x2033; receives the proxy image data. Then, in step S<b>1722</b>, the server <b>200</b>&#x2033; transmits the proxy image data to the edge device <b>400</b>&#x2033;. Step S<b>1702</b> and on are the same as the sequence illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>. Note that rather than transmitting the selected file information directly from the edge device <b>400</b>&#x2033; to the image capture apparatus <b>100</b>, the selected file information may be transmitted to the image capture apparatus <b>100</b> via the server <b>200</b>&#x2033;. Employing such a configuration, in which the communication between the image capture apparatus <b>100</b> and the edge device <b>400</b>&#x2033; is performed via the server <b>200</b>&#x2033;, makes it possible to carry out the present invention even in situations where the image capture apparatus <b>100</b> and the edge device <b>400</b>&#x2033; cannot communicate with each other directly.</p><p id="p-0232" num="0229">Additionally, although the present embodiment describes the server <b>200</b>&#x2033; as executing the development processing, the development processing may be performed by the edge device <b>400</b>&#x2033;. In this case, the processing executed by the server <b>200</b>&#x2033;, illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, may be executed by the edge device <b>400</b>&#x2033;.</p><heading id="h-0014" level="1">Seventh Embodiment</heading><p id="p-0233" num="0230">A seventh embodiment of the present invention will be described next. In the present embodiment, a development processing result for part of the RAW data is presented to the user, and the development processing is executed for the entirety of RAW data for which the need to execute the processing has been confirmed. This makes it possible to shorten the time and amount of data communication required to obtain a developing result for a partial region.</p><p id="p-0234" num="0231">The present embodiment can be carried out by the same image processing system as that described in the sixth embodiment, and thus the configurations of the image capture apparatus <b>100</b>, the server <b>200</b>&#x2033;, and the edge device <b>400</b>&#x2033; will not be described.</p><p id="p-0235" num="0232">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>&#x2033;, and the server <b>200</b>&#x2033; according to the present embodiment will be described next with reference to the flowcharts in <figref idref="DRAWINGS">FIGS. <b>19</b>A and <b>19</b>B</figref>. Steps S<b>1901</b> to S<b>1903</b> are the same as steps S<b>1701</b> to S<b>1703</b> in the sixth embodiment and will therefore not be described here. Here, it is assumed that when one image is selected in step S<b>1903</b>, the edge device <b>400</b>&#x2033; executes step S<b>1904</b>.</p><p id="p-0236" num="0233">In step S<b>1904</b>, the edge device <b>400</b>&#x2033; selects a partial region of the proxy image selected by the user in step S<b>1903</b>. This selection will be described further with reference to <figref idref="DRAWINGS">FIG. <b>20</b></figref>. <figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates a proxy image <b>2000</b> selected by the user in step S<b>1903</b>. The control circuit <b>401</b> controls the image processing circuit <b>405</b> to execute feature region detection on the proxy image data corresponding to the proxy image selected by the user. The position and size of a region thought to be a person's face (a facial region) is assumed to be detected as the feature region. <figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates an example in which facial regions <b>2001</b> and <b>2002</b> have been detected through the detection processing.</p><p id="p-0237" num="0234">The edge device <b>400</b>&#x2033; can superimpose an indicator of the detected feature region (e.g., a rectangular indicator) over the proxy image selected in step S<b>1903</b> and present the resulting display to the user. The edge device <b>400</b>&#x2033; selects one feature region for the proxy image in which a plurality of feature regions have been detected by allowing the user to select the feature region, or by automatically selecting the feature region according to a predetermined condition. An example in which the partial region on which to perform the development processing is selected from the feature region in advance is described here. However, the image may be divided into a plurality of regions using one or more of color, luminance, and distance information, and the user may then be allowed to select one of the regions. The user may also be allowed to select a desired partial region within the image.</p><p id="p-0238" num="0235">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the user can select one of the two facial regions <b>2001</b> and <b>2002</b> by touching the display circuit <b>408</b>, for example. Assume here that the facial region <b>2001</b> has been selected.</p><p id="p-0239" num="0236">Returning to <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>, in step S<b>1905</b>, the edge device <b>400</b>&#x2033; transmits information specifying the partial region selected in the file (partial region information) to the image capture apparatus <b>100</b>, in addition to the selected file information. Although the partial region information is not particularly limited, the coordinates of the vertices of opposing corners of the partial region in the proxy image can be given as an example. At this time, using coordinate values normalized according to the size of the proxy image in the horizontal direction and the vertical direction makes it possible to easily specify the position of the partial region in the corresponding RAW data.</p><p id="p-0240" num="0237">In step S<b>1906</b>, the image capture apparatus <b>100</b> receives the selected file information and the partial region information, and stores that information in the RAM <b>103</b>.</p><p id="p-0241" num="0238">In step S<b>1907</b>, for the RAW data specified from the selected file information, the image capture apparatus <b>100</b> transmits the RAW data of the partial region based on the partial region information (partial RAW data) to the server <b>200</b>&#x2033; via the network <b>300</b>.</p><p id="p-0242" num="0239">In step S<b>1908</b>, the server <b>200</b>&#x2033; receives the partial RAW data.</p><p id="p-0243" num="0240">In step S<b>1909</b>, the server <b>200</b>&#x2033; applies the development processing to the partial RAW data received in step S<b>1908</b>. The development processing which is applied may be the same as that of the sixth embodiment.</p><p id="p-0244" num="0241">In step S<b>1910</b>, the server <b>200</b>&#x2033; transmits the result of developing the partial RAW data to the edge device <b>400</b>&#x2033; over the network <b>300</b>, along with the information specifying the RAW data.</p><p id="p-0245" num="0242">In step S<b>1911</b>, the edge device <b>400</b>&#x2033; receives the developing result and stores the developing result in the RAM <b>403</b>. The edge device <b>400</b>&#x2033; displays the image corresponding to the developing result in the display circuit <b>408</b>. The edge device <b>400</b>&#x2033; may display the developing result along with the entirety of the corresponding proxy image. Through the display circuit <b>408</b>, the user can confirm the result of developing the partial region for an image they themselves selected in step S<b>1903</b>.</p><p id="p-0246" num="0243">In the present embodiment, the edge device <b>400</b>&#x2033; displays the image that is the developing result so that the user can select one of an instruction to execute the development processing on the entire image, an instruction to reselect the partial region, and an instruction to cancel the development processing. The edge device <b>400</b>&#x2033; can, for example, display selectable icons corresponding to these instructions next to the image that is the developing result.</p><p id="p-0247" num="0244">In step S<b>1912</b>, the edge device <b>400</b>&#x2033; branches the sequence according to the user instruction which has been received. The edge device <b>400</b>&#x2033; executes step S<b>1904</b> if the instruction to reselect the partial region has been detected, and executes step S<b>1913</b> if the instruction to execute the development processing on the entire image has been detected. Although not illustrated in the drawings, the edge device <b>400</b>&#x2033; ends the display of the corresponding image if the cancel instruction is detected.</p><p id="p-0248" num="0245">When step S<b>1904</b> is executed in response to the instruction to reselect the partial region, the edge device <b>400</b>&#x2033; can display the indicators of the feature regions aside from the feature region which has already been selected. When the user selects a new partial region, the processing from step S<b>1905</b> is executed again for the target image.</p><p id="p-0249" num="0246">On the other hand, in step S<b>1913</b>, the edge device <b>400</b>&#x2033; transmits the information specifying the RAW data, along with a notification requesting that the entire image undergo development processing (a main development request), to the image capture apparatus <b>100</b>.</p><p id="p-0250" num="0247">In step S<b>1914</b>, the image capture apparatus <b>100</b> receives the main development request, and the information specifying the RAW data to be processed, and stores these items in the RAM <b>103</b>. The operations from step S<b>1915</b>, in which the entirety of the RAW data is transmitted, are the same as the operations from steps S<b>1706</b> to S<b>1713</b> in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, described in the sixth embodiment, and will therefore not be described in detail here.</p><p id="p-0251" num="0248">Here, to simplify the descriptions and facilitate understanding, the flow of operations for a single proxy image displayed in the edge device <b>400</b>&#x2033; has been described. When a plurality of proxy images are to be displayed in the display circuit <b>408</b>, the same processing as that executed for the image selected in step S<b>1903</b> is executed for the remaining proxy images. Note that a configuration is also possible in which the processing from steps S<b>1903</b> to S<b>1911</b> is executed for a plurality of proxy images at the same time. In other words, when a plurality of proxy images are selected in step S<b>1903</b>, the edge device <b>400</b>&#x2033; transmits the selected file information and the partial region information for the plurality of images to the image capture apparatus <b>100</b> at once. Then, the transmission of the partial RAW data, the development processing, and the transmission of the developing result are executed all together for the plurality of images, and the developing results for the plurality of images are ultimately displayed in the display circuit <b>408</b>. Thereafter, the requests for the overall development processing and the reselection of the partial regions can each be executed at the same time.</p><p id="p-0252" num="0249">According to the present embodiment, in addition to the effects of the sixth embodiment, the user can determine whether or not it is necessary to execute the development processing on the entirety of a selected image after first confirming the developing result for a partial region. Executing the development processing for the partial region first reduces the amount of data communication and reduces the load of the development processing, and thus the user can quickly confirm the developing result for the partial region. Then, if there is RAW data for which it is determined, on the basis of the developing result of the partial region, that the development processing need not be performed on the entirety of the data, the amount of data communication using the network <b>300</b> can be reduced. Note that destructive or non-destructive processing for reducing the data amount can also be applied to the partial RAW data, and in this case, the amount of data communication can be reduced even further.</p><p id="p-0253" num="0250">Furthermore, the development processing performed on the partial RAW data or the RAW data does not necessarily have to be executed by the server <b>200</b>&#x2033;, and may be at least partially executed by the edge device <b>400</b>&#x2033;.</p><heading id="h-0015" level="1">Eighth Embodiment</heading><p id="p-0254" num="0251">An eighth embodiment of the present invention will be described next. The present embodiment corresponds to a variation on the seventh embodiment. Specifically, the partial RAW data and the developing result thereof according to the seventh embodiment are not repeatedly transmitted between devices, which makes it possible to further reduce the amount of data communication.</p><p id="p-0255" num="0252">Like the seventh embodiment, the present embodiment can be carried out by the same image processing system as that described in the sixth embodiment, and thus the configurations of the image capture apparatus <b>100</b>, the server <b>200</b>&#x2033;, and the edge device <b>400</b>&#x2033; will not be described.</p><p id="p-0256" num="0253">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>&#x2033;, and the server <b>200</b>&#x2033; according to the present embodiment will be described next with reference to the flowcharts in <figref idref="DRAWINGS">FIGS. <b>21</b>A and <b>21</b>B</figref>. In <figref idref="DRAWINGS">FIGS. <b>21</b>A and <b>21</b>B</figref>, processes that are the same as in the seventh embodiment are given the same reference numerals. Steps S<b>1901</b> to S<b>1914</b> are the same as in the seventh embodiment and will therefore not be described here.</p><p id="p-0257" num="0254">Upon the main development request being received in step S<b>1914</b>, in step S<b>2115</b>, the image capture apparatus <b>100</b> transmits the RAW data to be subjected to the main development to the server <b>200</b>&#x2033;, with the exception of the partial RAW data transmitted to the server <b>200</b>&#x2033; in step S<b>1907</b>. For example, when partial RAW data corresponding to the facial region <b>2001</b> has been transmitted to the server <b>200</b>&#x2033; in step S<b>1907</b>, the image capture apparatus <b>100</b> transmits the RAW data corresponding to regions aside from the facial region <b>2001</b> (the remaining regions) to the server. By referring to the selected file information and the partial region information stored in the RAM <b>103</b>, the image capture apparatus <b>100</b> can generate RAW data which excludes the partial RAW data already transmitted.</p><p id="p-0258" num="0255">In step S<b>2116</b>, the server <b>200</b>&#x2033; receives the RAW data excluding the partial RAW data received in step S<b>1908</b>.</p><p id="p-0259" num="0256">In step S<b>2117</b>, the server <b>200</b>&#x2033; generates RAW data corresponding to the entire image by compositing the partial RAW data received in step S<b>1908</b> with the RAW data received in step S<b>2116</b>.</p><p id="p-0260" num="0257">The subsequent processing from step S<b>1917</b> is the same as that in the seventh embodiment and will therefore not be described.</p><p id="p-0261" num="0258">According to the present embodiment, when main development (the development processing for the entire image) is performed on the RAW data for which the development processing has already been performed on a partial region, the image capture apparatus <b>100</b> does not retransmit the partial RAW data which has already been transmitted to the server <b>200</b>&#x2033;. Thus in addition to the effects of the seventh embodiment, the amount of data communication from the image capture apparatus <b>100</b> to the server <b>200</b>&#x2033; can be reduced.</p><p id="p-0262" num="0259">In the present embodiment too, the development processing performed on the partial RAW data or the RAW data does not necessarily have to be executed by the server <b>200</b>&#x2033;, and may be at least partially executed by the edge device <b>400</b>&#x2033;.</p><heading id="h-0016" level="1">Ninth Embodiment</heading><p id="p-0263" num="0260">A ninth embodiment of the present invention will be described next. In the present embodiment, an order in which the RAW data is transmitted to the external apparatus is determined in accordance with a predetermined condition. The present embodiment, too, can be carried out by the same image processing system as that described in the sixth embodiment, and thus the configurations of the image capture apparatus <b>100</b>, the server <b>200</b>&#x2033;, and the edge device <b>400</b>&#x2033; will not be described.</p><p id="p-0264" num="0261">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>&#x2033;, and the server <b>200</b>&#x2033; according to the present embodiment will be described next with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>22</b></figref>. In <figref idref="DRAWINGS">FIG. <b>22</b></figref>, steps S<b>2201</b> and S<b>2202</b> are the same as steps S<b>1701</b> and S<b>1702</b> in the sixth embodiment, and will therefore not be described.</p><p id="p-0265" num="0262">In step S<b>2203</b>, the edge device <b>400</b>&#x2033; calculates a predetermined evaluation value for the proxy image data stored in the RAM <b>403</b>. This evaluation value is an evaluation value serving as an indicator of the magnitude of the effect achieved by the external apparatus (the server <b>200</b>&#x2033; or the edge device <b>400</b>&#x2033;) performing the development processing on the RAW image data corresponding to the proxy image data. Accordingly, the evaluation value of the proxy image data can be considered to be an evaluation value of the corresponding RAW data.</p><p id="p-0266" num="0263">Operations performed by the evaluation circuit <b>411</b> for calculating the evaluation value in the edge device <b>400</b>&#x2033; may be the same as those described in the third embodiment with reference to <figref idref="DRAWINGS">FIGS. <b>9</b> to <b>11</b>B</figref>.</p><p id="p-0267" num="0264">The evaluation circuit <b>411</b> calculates the evaluation value for each instance of proxy image data which is stored. The calculated evaluation values are stored in the RAM <b>403</b>.</p><p id="p-0268" num="0265">Returning to <figref idref="DRAWINGS">FIG. <b>22</b></figref>, in step S<b>2204</b>, the edge device <b>400</b>&#x2033; rearranges the evaluation values found in step S<b>2203</b> in order from the highest evaluation value, and generates rating information associated with the corresponding proxy image data. The edge device <b>400</b>&#x2033; then transmits the rating information to the image capture apparatus <b>100</b>. The rating information is information indicating a priority order of the proxy image data (and the image data on which the proxy image data is based).</p><p id="p-0269" num="0266">In step S<b>2205</b>, the image capture apparatus <b>100</b> receives the rating information and stores the rating information in the RAM <b>103</b>.</p><p id="p-0270" num="0267">In step S<b>2206</b>, in accordance with the rating information, the image capture apparatus <b>100</b> determines a transmission order for the RAW data to be transmitted to the server <b>200</b>&#x2033;, such that RAW data having a higher evaluation value is transmitted first. Note that when determining the transmission order, RAW data having an evaluation value lower than a pre-set threshold may be excluded from the transmission. Additionally, the configuration may be such that the threshold can be adjusted in response to user instructions obtained via a user interface.</p><p id="p-0271" num="0268">In step S<b>2207</b>, the image capture apparatus <b>100</b> transmits the RAW data sequentially to the server <b>200</b>&#x2033; according to the transmission order determined in step S<b>2206</b>.</p><p id="p-0272" num="0269">In step S<b>2208</b>, the server <b>200</b>&#x2033; receives the RAW data and stores the data in the RAM <b>202</b> or the storage device <b>205</b>.</p><p id="p-0273" num="0270">In step S<b>2209</b>, the server <b>200</b>&#x2033; executes the image processing program <b>211</b> and applies the development processing to the RAW data, in the same manner as in step S<b>1708</b> in the sixth embodiment. Note that the server <b>200</b>&#x2033; may receive the RAW data in step S<b>2208</b> and execute the image processing program <b>211</b> (apply the development processing) in step S<b>2209</b> in parallel.</p><p id="p-0274" num="0271">In step S<b>2210</b>, the server <b>200</b>&#x2033; transmits the developing result (the developed image data) to at least one of the image capture apparatus <b>100</b> and the edge device <b>400</b>&#x2033; over the network <b>300</b>, in order from the RAW data for which the development processing has finished being applied.</p><p id="p-0275" num="0272">When the edge device <b>400</b>&#x2033; receives the developed image data from the server <b>200</b>&#x2033;, in step S<b>2211</b>, the edge device <b>400</b>&#x2033; stores the received image data in the RAM <b>403</b>. Then, in step S<b>2212</b>, the edge device <b>400</b> displays the image data stored in the RAM <b>403</b> in the display circuit <b>408</b>, and records the image data into the recording circuit <b>407</b>.</p><p id="p-0276" num="0273">When the image capture apparatus <b>100</b> is to receive the developed image data from the server <b>200</b>&#x2033;, in step S<b>2213</b>, the image capture apparatus <b>100</b> receives the developed image data from the server <b>200</b>&#x2033; and stores that data in the RAM <b>103</b>. Then, in step S<b>2214</b>, the image capture apparatus <b>100</b> supplies the image data stored in the RAM <b>103</b> to the image processing circuit <b>107</b>, and causes image data for recording and image data for display to be generated. The image capture apparatus <b>100</b> then causes the display circuit <b>109</b> to display the image data for display, and causes the recording circuit <b>108</b> to record the image data for recording.</p><p id="p-0277" num="0274">According to the present embodiment, the magnitude of the effect of the external apparatus (e.g., the server <b>200</b>&#x2033;) applying image processing is evaluated by the edge device <b>400</b>&#x2033; for the RAW data in the image capture apparatus <b>100</b>, and the RAW data is transmitted to the server <b>200</b>&#x2033; in order from the RAW data for which the effect is considered to be the greatest. Accordingly, the image capture apparatus <b>100</b> can receive processing results in order from the RAW data for which the effect of the image processing by the external apparatus is greatest.</p><p id="p-0278" num="0275">The present embodiment can be combined with the sixth to eighth embodiments. Specifically, the calculation of the evaluation value, the determination of the transmission order, and the development processing may be performed only on image data selected by the user. In this case, in addition to the effects of the sixth to eighth embodiments, processing results can be received in order from the RAW data for which the effect of the image processing by the external apparatus is greatest, for the RAW data selected by the user.</p><p id="p-0279" num="0276">Although the evaluation circuit <b>411</b> calculates the evaluation value on the basis of an edge detection result in the present embodiment, the evaluation value may be calculated on the basis of another standard based on the properties of the development processing performed by the server <b>200</b>&#x2033;. Additionally, the evaluation value is not limited to an indicator of the magnitude of the effect of the development processing performed by the server <b>200</b>&#x2033;, and may be an indicator of image quality. For example, an evaluation value based on an aesthetic quality of a composition, an evaluation value based on a level of focus, an evaluation value based on the correctness of exposure, an evaluation value based on an expression of a subject, such as a degree of smiling, and the like may be calculated. A plurality of types of evaluation values may also be calculated, and the transmission order may then be determined on the basis of an average value, a maximum value, or the like thereof. Note that the evaluation value calculation is not limited to be performed by the edge device <b>400</b>&#x2033;, and may be performed by the image capture apparatus <b>100</b> or the server <b>200</b>&#x2033; instead.</p><heading id="h-0017" level="1">Tenth Embodiment</heading><p id="p-0280" num="0277">A tenth embodiment of the present invention will be described next. In the present embodiment, the method for finding the indicator of the magnitude of the effect of the external apparatus applying the image processing is different from that in the ninth embodiment. The present embodiment, too, can be carried out by the same image processing system as that described in the sixth embodiment, and thus the configurations of the image capture apparatus <b>100</b>, the server <b>200</b>&#x2033;, and the edge device <b>400</b>&#x2033; will not be described.</p><p id="p-0281" num="0278">In the present embodiment, the magnitude of the effect of the external apparatus applying the image processing is evaluated in more detail by evaluating an image taking in account a plurality of types of evaluation values, in light of the performance of the image processing provided by the external apparatus. This makes it possible to make evaluations which reflect performance improvements resulting from updating the image processing program <b>211</b> provided by the external apparatus.</p><p id="p-0282" num="0279">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>&#x2033;, and the server <b>200</b>&#x2033; according to the present embodiment will be described next with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>23</b></figref>. In <figref idref="DRAWINGS">FIG. <b>23</b></figref>, steps in which the same operations as in the ninth embodiment are carried out have been given the same reference numerals as those in <figref idref="DRAWINGS">FIG. <b>22</b></figref>. Operations that are the same as in the ninth embodiment will not be described, and the following will focus on operations characteristic to the present embodiment.</p><p id="p-0283" num="0280">In step S<b>2301</b>, the server <b>200</b>&#x2033; generates a development prediction flag and transmits that flag to the edge device <b>400</b>&#x2033;. The development prediction flag is information serving as an indicator of the performance of the various image processing functions provided by the image processing program <b>211</b> of the server <b>200</b>&#x2033;. For example, the development processing, which is an example of an image processing function provided by the image processing program <b>211</b>, includes a plurality of types of processing, such as white balance processing, lens aberration correction processing, noise reduction (NR) processing, color interpolation processing, and gamma processing. The development prediction flag may be information provided for each of these types of processing, having a value of &#x201c;1&#x201d; when the processing is capable of high performance and &#x201c;0&#x201d; for other processing. Note that the development prediction flag need not be generated each time step S<b>2301</b> is executed, and can be stored in advance in the ROM <b>203</b>&#x2032;. The necessary part is then updated when the image processing program <b>211</b> is updated.</p><p id="p-0284" num="0281">Although the method for evaluation high or low performance is not particularly limited, an application, module, or the like that realizes the processing having been updated within the past year, processing that has a higher load, and so on are considered to be highly likely to have higher performance. Alternatively, a vendor of the image processing program <b>211</b> may provide an update to the development prediction flag when updating the program.</p><p id="p-0285" num="0282">In step S<b>2302</b>, the edge device <b>400</b>&#x2033; receives the development prediction flag from the server <b>200</b>&#x2033;.</p><p id="p-0286" num="0283">In step S<b>2303</b>, the edge device <b>400</b>&#x2033; calculates the evaluation value for each instance of the proxy image data recorded in the RAM <b>403</b>.</p><p id="p-0287" num="0284">Operations performed by the evaluation circuit <b>411</b> in step S<b>2303</b> will be described with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>24</b></figref>. In the present embodiment, the evaluation circuit <b>411</b> calculates the evaluation values for the individual instances of proxy image data by weighting the plurality of types of evaluation values in accordance with the development prediction flags.</p><p id="p-0288" num="0285">In step S<b>2401</b>, the evaluation circuit <b>411</b> calculates a first evaluation value for the proxy image data, based on the same type of edge detection is that described in the ninth embodiment.</p><p id="p-0289" num="0286">In step S<b>2402</b>, the evaluation circuit <b>411</b> calculates a person's degree of smiling as a second evaluation value by applying a publicly known method to the proxy image data. The person's degree of smiling is a numerical value indicating the degree to which a facial region detected from the proxy image data is a smile. Applications for finding the degree of smiling from an image are publicly known, and for example, the second evaluation value can be obtained by providing the image processing circuit <b>405</b> with such an application. For a proxy image in which a plurality of facial regions are present, an average value, a maximum value, or the like of the degree of smiling is taken as the second evaluation value, for example. The first and second evaluation values are both normalized to a range of 0 to 1.</p><p id="p-0290" num="0287">In step S<b>2403</b>, the evaluation circuit <b>411</b> determines weights of the first and second evaluation values on the basis of the development prediction flags. Methods of calculating the weights based on the development prediction flags are registered in the evaluation circuit <b>411</b> in advance for each type of evaluation value. Then, after adjusting the proportion of the weights so that a total of the weight for the first evaluation value and the weight for the second evaluation value is 1, the evaluation circuit <b>411</b> performs weighted addition of the first and second evaluation values to calculate the final evaluation value.</p><p id="p-0291" num="0288">By calculating the evaluation values for the individual instances of proxy image data in this manner, evaluation values based on the properties of the image processing program <b>211</b> of the server <b>200</b>&#x2033; can be calculated, and the magnitude of the effect of the server <b>200</b>&#x2033; performing the development processing can be evaluated in more detail. Additionally, if the image processing program <b>211</b> has been updated and the prediction flight has changed, an evaluation value reflecting that change can be obtained. An appropriate evaluation can therefore always be performed.</p><p id="p-0292" num="0289">For example, assume that a method for noise reduction (NR) processing in the image processing program <b>211</b> has been changed so that noise reduction processing with a higher level of performance than before is possible, and a noise reduction processing flag in the development prediction flag has also changed from &#x201c;0&#x201d; to &#x201c;1&#x201d;. In this case, the improvement in the performance of the noise reduction processing has a major effect on an improvement in the result of development processing performed on images with many edges, and thus the weight on the first evaluation value becomes greater than the weight from before the image processing program <b>211</b> was updated.</p><p id="p-0293" num="0290">For example, when the first evaluation value is represented by Sedge and the second evaluation value is represented by S face, an evaluation value S obtained from weighted addition can be found as:</p><p id="p-0294" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i>=(<i>S</i>_edge&#xd7;&#x3b1;)+(<i>S</i>_face&#xd7;(1&#x2212;&#x3b1;)) &#x2003;&#x2003;(Equation 3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0295" num="0291">Then, for a weight a on the first evaluation value, the evaluation circuit <b>411</b> sets a weight &#x3b1;<sub>after</sub>, from after the image processing program <b>211</b> was updated, to be greater than a weight &#x3b1;<sub>before</sub>, from before the image processing program <b>211</b> was updated, in accordance with the change in the development prediction flag (&#x3b1;<sub>after</sub>&#x3e;&#x3b1;<sub>before</sub>). As a result, an evaluation value from after the weighted addition, which reflects the update in the image processing program <b>211</b>, is obtained.</p><p id="p-0296" num="0292">Returning to the flowchart in <figref idref="DRAWINGS">FIG. <b>23</b></figref>, in step S<b>2204</b>, the edge device <b>400</b>&#x2033; generates the rating information and transmits that information to the image capture apparatus <b>100</b> in the same manner as in the ninth embodiment, with the exception that the evaluation value found in step S<b>2303</b> is used. The subsequent processing is the same as in the ninth embodiment, and will therefore not be described here.</p><p id="p-0297" num="0293">According to the present embodiment, an evaluation value calculated by performing weighted addition of a plurality of evaluation values is used, and the weighting is determined so as to increase the weight of the evaluation values to better reflect the effect of the external apparatus applying the image processing. Accordingly, in addition to the same effects as those of the ninth embodiment, the image processing by the external apparatus can be applied in an order that reflects the latest image processing provided by the external apparatus.</p><p id="p-0298" num="0294">The present embodiment describes a case where a proportion (weight) is determined for the weighted addition of an evaluation value based on edge detection and an evaluation value based on a degree of smiling, on the basis of the development prediction flag. However, the types of evaluation values are not limited thereto. An evaluation value based on an aesthetic quality of a composition, an evaluation value based on a level of focus, an evaluation value based on the correctness of exposure, an evaluation value based on an expression of a subject, such as a degree of smiling, and the like may be calculated, in the same manner as in the ninth embodiment. Additionally, the weighted addition may be performed on three or more types of evaluation values. Note that the evaluation value calculation is not limited to be performed by the edge device <b>400</b>&#x2033;, and may be performed by the image capture apparatus <b>100</b> or the server <b>200</b>&#x2033; instead.</p><heading id="h-0018" level="1">Eleventh Embodiment</heading><p id="p-0299" num="0295">An eleventh embodiment of the present invention will be described next. The present embodiment differs from the ninth embodiment in that when transmitting the RAW data to the external apparatus in accordance with the transmission order determined on the basis of the evaluation values, a maximum number of instances of RAW data to be transmitted can be limited. The present embodiment, too, can be carried out by the same image processing system as that described in the sixth embodiment, and thus the configurations of the image capture apparatus <b>100</b>, the server <b>200</b>&#x2033;, and the edge device <b>400</b>&#x2033; will not be described.</p><p id="p-0300" num="0296">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>&#x2033;, and the server <b>200</b>&#x2033; according to the present embodiment will be described next with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>25</b></figref>. In <figref idref="DRAWINGS">FIG. <b>25</b></figref>, steps in which the same operations as in the ninth embodiment are carried out have been given the same reference numerals as those in <figref idref="DRAWINGS">FIG. <b>22</b></figref>. Operations that are the same as in the ninth embodiment will not be described, and the following will focus on operations characteristic to the present embodiment.</p><p id="p-0301" num="0297">A maximum number of instances of RAW data to be transmitted to the external apparatus is set in the ROM <b>102</b> of the image capture apparatus <b>100</b> according to the present embodiment. This setting may be changeable by the user. Individual maximum numbers may also be set in accordance with the destination or method of communication. For example, a maximum number is set for communication using a mobile phone communication network (communication that incurs a cost based on the amount of data communication), and a maximum number is not set (or is set to be unlimited) for communication using wireless LAN (communication for which no costs are incurred). Furthermore, a maximum number is set for communication with the server <b>200</b>&#x2033;, whereas a maximum number is not set for communication with the edge device <b>400</b>&#x2033;. The maximum number may be a number of RAW data files (a number of instances of data), or may be an amount of data transmitted. Note that a number of transmitted files, an amount of data communication, and so on to be compared with the maximum number are reset every predetermined period (e.g., every month, every week, every day, or the like).</p><p id="p-0302" num="0298">The operations performed up until the rating information is received and the transmission order is determined (i.e., up to step S<b>2206</b>) may be the same as in the ninth embodiment (or the tenth embodiment).</p><p id="p-0303" num="0299">In step S<b>2501</b>, the image capture apparatus <b>100</b> refers to the RAM <b>103</b>, for example, and determines whether or not a current transmission number is less than the maximum number. If the current transmission number is not less than the maximum number, the image capture apparatus <b>100</b> does not send anymore RAW data. On the other hand, if the current transmission number is less than the maximum number, in step S<b>2207</b>, the image capture apparatus <b>100</b> transmits the next RAW data to the server <b>200</b>&#x2033; in accordance with the transmission order. After the transmission, the image capture apparatus <b>100</b> applies the information of the RAW data transmitted most recently to the number of transmitted files, the amount of data communication, and so on stored in the RAM <b>103</b>. Then, if untransmitted RAW data remains, the image capture apparatus <b>100</b> executes step S<b>2501</b> again. Note that if the maximum number is a data amount, even if the current transmission number is less than the maximum number, the next RAW data is not transmitted in the event that the maximum number is exceeded during the transmission of the next RAW data.</p><p id="p-0304" num="0300">The operations from step S<b>2207</b> may be the same as in the ninth embodiment (or the tenth embodiment), and therefore will not be described.</p><p id="p-0305" num="0301">The same effects as those of the ninth embodiment (or the tenth embodiment) can be achieved by the present embodiment as well. Additionally, setting the maximum number to a value that takes into account, for example, a maximum monthly communication charge in a mobile communication contract makes it possible to prevent the transmission of RAW data in an environment where mobile communication is used from exceeding the maximum amount of communication.</p><heading id="h-0019" level="1">Twelfth Embodiment</heading><p id="p-0306" num="0302">A twelfth embodiment of the present invention will be described next. The present embodiment differs from the ninth embodiment in that when transmitting the RAW data to the external apparatus in accordance with the transmission order determined on the basis of the evaluation values, a remaining amount of data that can be transmitted to the server is taken into account. The present embodiment, too, can be carried out by the same image processing system as that described in the sixth embodiment, and thus the configurations of the image capture apparatus <b>100</b>, the server <b>200</b>&#x2033;, and the edge device <b>400</b>&#x2033; will not be described.</p><p id="p-0307" num="0303">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>&#x2033;, and the server <b>200</b>&#x2033; according to the present embodiment will be described next with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>26</b></figref>. In <figref idref="DRAWINGS">FIG. <b>26</b></figref>, steps in which the same operations as in the ninth embodiment are carried out have been given the same reference numerals as those in <figref idref="DRAWINGS">FIG. <b>22</b></figref>. Operations that are the same as in the ninth embodiment will not be described, and the following will focus on operations characteristic to the present embodiment.</p><p id="p-0308" num="0304">In the server <b>200</b>&#x2033; according to the present embodiment, a usable storage amount in the storage device <b>205</b> is set for each user, and a usage amount is managed for each user. The usage amount is managed in association with, for example, unique information of the user or the image capture apparatus <b>100</b>. The server <b>200</b>&#x2033; can manage the usage amount on the basis of the unique information of the image capture apparatus or the user from which the RAW data is received.</p><p id="p-0309" num="0305">The operations performed up until the rating information is received and the transmission order is determined (i.e., up to step S<b>2206</b>) may be the same as in the ninth embodiment (or the tenth embodiment).</p><p id="p-0310" num="0306">In step S<b>2601</b>, the server <b>200</b>&#x2033; calculates a difference between a maximum value of the usable storage amount set for the image capture apparatus <b>100</b> or the user and a current usage amount as the remaining data amount, and transmits the remaining data amount to the image capture apparatus <b>100</b>. Note that step S<b>2601</b> can be executed at any time between when the server <b>200</b>&#x2033; and the image capture apparatus <b>100</b> enter a communicative state and when the image capture apparatus <b>100</b> begins transmitting the RAW data.</p><p id="p-0311" num="0307">In step S<b>2602</b>, the image capture apparatus <b>100</b> receives the remaining data amount from the server <b>200</b>&#x2033; and stores the remaining data amount in the RAM <b>103</b>.</p><p id="p-0312" num="0308">In step S<b>2603</b>, the image capture apparatus <b>100</b> determines whether or not the received remaining data amount is greater than or equal to a data amount of the RAW data to be transmitted next. When the remaining data amount is not greater than or equal to the data amount of the RAW data to be transmitted next, the image capture apparatus <b>100</b> does not transmit any of the subsequent RAW data. On the other hand, when the remaining data amount is greater than or equal to the data amount of the RAW data to be transmitted next, the image capture apparatus <b>100</b> executes step S<b>2207</b> and transmits the RAW data to the server. After the transmission, the image capture apparatus <b>100</b> updates the remaining data amount by reducing the remaining data amount stored in the RAM <b>103</b> by the data amount of the RAW data transmitted most recently. Then, if untransmitted RAW data remains, the image capture apparatus <b>100</b> executes step S<b>2603</b> again.</p><p id="p-0313" num="0309">The operations from step S<b>2207</b> may be the same as in the ninth embodiment (or the tenth embodiment), and therefore will not be described.</p><p id="p-0314" num="0310">The same effects as those of the ninth embodiment (or the tenth embodiment) can be achieved by the present embodiment as well. Additionally, if there is a limit on the amount of data which can be stored in the server <b>200</b>&#x2033; (or the data amount to which the image processing can be applied), the image processing can be applied preferentially to RAW data for which the image processing performed by the server <b>200</b>&#x2033; has the greatest effect. Note that a value obtained by converting the remaining data amount into a number of RAW data files using a typical data size of the RAW data may be used instead of the remaining data amount.</p><heading id="h-0020" level="1">Thirteenth Embodiment</heading><p id="p-0315" num="0311">A thirteenth embodiment of the present invention will be described next. The present embodiment differs from the ninth embodiment in that the rating information generated by the edge device <b>400</b>&#x2033; is transmitted to the server <b>200</b>&#x2033;, and the rating information is then generated again taking into account the rating information for RAW data stored in the server <b>200</b>&#x2033;. The present embodiment, too, can be carried out by the same image processing system as that described in the sixth embodiment, and thus the configurations of the image capture apparatus <b>100</b>, the server <b>200</b>&#x2033;, and the edge device <b>400</b>&#x2033; will not be described.</p><p id="p-0316" num="0312">Interactive operations between the image capture apparatus <b>100</b>, the edge device <b>400</b>&#x2033;, and the server <b>200</b>&#x2033; according to the present embodiment will be described next with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>27</b></figref>. In <figref idref="DRAWINGS">FIG. <b>27</b></figref>, steps in which the same operations as in the ninth embodiment are carried out have been given the same reference numerals as those in <figref idref="DRAWINGS">FIG. <b>22</b></figref>. Operations that are the same as in the ninth embodiment will not be described, and the following will focus on operations characteristic to the present embodiment.</p><p id="p-0317" num="0313">The operations performed up until the rating information is generated by the edge device <b>400</b>&#x2033; (i.e., up to step S<b>2204</b>) may be the same as in the ninth embodiment (or the tenth embodiment). However, in the present embodiment, the edge device <b>400</b>&#x2033; transmits the rating information to the server <b>200</b>&#x2033; instead of the image capture apparatus <b>100</b> in step S<b>2204</b>.</p><p id="p-0318" num="0314">In step S<b>2701</b>, the server <b>200</b>&#x2033; receives the rating information from the edge device <b>400</b>&#x2033; and stores the rating information in the RAM <b>202</b>.</p><p id="p-0319" num="0315">The server <b>200</b>&#x2033; holds, in the storage device <b>205</b> of the server <b>200</b>&#x2033;, rating information for the RAW data associated with the image capture apparatus <b>100</b> or that user. This rating information is continuously updated on the basis of rating information received from the edge device <b>400</b>&#x2033; in the past, and includes a predetermined number of evaluation values from the highest value and information of the corresponding RAW data.</p><p id="p-0320" num="0316">In step S<b>2702</b>, the server <b>200</b>&#x2033; updates the rating information stored in the storage device <b>205</b> using the rating information received from the edge device <b>400</b>&#x2033; in step S<b>2701</b> (re-rating processing). Specifically, the server <b>200</b>&#x2033; rearranges a plurality of evaluation values included in two instances of the rating information in order from the highest value, and stores the information of the RAW data corresponding to a predetermined number of evaluation values from the highest value in the storage device <b>205</b> as the rating information. Note that the maximum number of evaluation values (instances of RAW data) included in the rating information stored in the storage device <b>205</b> can be set to the maximum number of instances of RAW data that can be stored by the user in the storage device <b>205</b>.</p><p id="p-0321" num="0317">Additionally, of the new rating information generated through the re-rating processing, the server <b>200</b>&#x2033; extracts only the information pertaining to RAW data not present in the server <b>200</b>&#x2033;, and generates rating information to be transmitted to the image capture apparatus <b>100</b>. Of the RAW data stored in the storage device <b>205</b>, RAW data which is no longer included in the rating information updated through the re-rating processing can be deleted to free up space in the storage device <b>205</b>.</p><p id="p-0322" num="0318">In step S<b>2703</b>, the server <b>200</b>&#x2033; transmits, to the image capture apparatus <b>100</b>, the rating information generated for the image capture apparatus.</p><p id="p-0323" num="0319">The operations from step S<b>2205</b> and on are the same as in the ninth embodiment, and will therefore not be described here.</p><p id="p-0324" num="0320">According to the present embodiment, when the number of instances, size, and so on of data that can be stored in the server <b>200</b>&#x2033; are limited, data for which the image processing by the server <b>200</b>&#x2033; will have a greater effect (data having a higher evaluation value) can be stored in the server <b>200</b>&#x2033;.</p><heading id="h-0021" level="1">Other Embodiments</heading><p id="p-0325" num="0321">Embodiment(s) of the present invention can also be realized by a computer of a system or apparatus that reads out and executes computer executable instructions (e.g., one or more programs) recorded on a storage medium (which may also be referred to more fully as a &#x2018;non-transitory computer-readable storage medium&#x2019;) to perform the functions of one or more of the above-described embodiment(s) and/or that includes one or more circuits (e.g., application specific integrated circuit (ASIC)) for performing the functions of one or more of the above-described embodiment(s), and by a method performed by the computer of the system or apparatus by, for example, reading out and executing the computer executable instructions from the storage medium to perform the functions of one or more of the above-described embodiment(s) and/or controlling the one or more circuits to perform the functions of one or more of the above-described embodiment(s). The computer may comprise one or more processors (e.g., central processing unit (CPU), micro processing unit (MPU)) and may include a network of separate computers or separate processors to read out and execute the computer executable instructions. The computer executable instructions may be provided to the computer, for example, from a network or the storage medium. The storage medium may include, for example, one or more of a hard disk, a random-access memory (RAM), a read only memory (ROM), a storage of distributed computing systems, an optical disk (such as a compact disc (CD), digital versatile disc (DVD), or Blu-ray Disc (BD)&#x2122;), a flash memory device, a memory card, and the like.</p><p id="p-0326" num="0322">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.</p><p id="p-0327" num="0323">This application claims the benefit of Japanese Patent Application No. 2020-006219, filed on Jan. 17, 2020, Japanese Patent Application No. 2020-069279, filed on Apr. 7, 2020, and Japanese Patent Application No. 2020-186751, filed on Nov. 9, 2020, which are hereby incorporated by reference herein in their entirety.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image processing apparatus comprising a communication circuit and a control circuit,<claim-text>wherein the control circuit:</claim-text><claim-text>receives reduced image data from an information processing apparatus through the communication circuit,</claim-text><claim-text>applies evaluation processing to the reduced image data,</claim-text><claim-text>transmits a result of the evaluation processing to the information processing apparatus through the communication circuit,</claim-text><claim-text>receives image data having a higher resolution than the reduced image data from the information processing apparatus through the communication circuit,</claim-text><claim-text>applies predetermined image processing to the image data, and</claim-text><claim-text>transmits a result of the image processing to the information processing apparatus through the communication circuit.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the image processing apparatus communicates with the information processing apparatus over a network that incurs a cost for use or a communication amount.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the control circuit further:</claim-text><claim-text>determines image data to which the image processing is to be applied from a plurality of frames' worth of image data received from the information processing apparatus, and</claim-text><claim-text>applies the image processing to the determined image data.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the control circuit further:</claim-text><claim-text>receives a plurality of frames' worth of image data from the information processing apparatus through the communication circuit,</claim-text><claim-text>determines image data to which predetermined image processing is to be applied from the plurality of frames' worth of image data, and</claim-text><claim-text>transmits the determined image data to an apparatus different from the information processing apparatus through the communication circuit.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. A control method for an image processing apparatus, the method comprising:<claim-text>receiving reduced image data from an information processing apparatus through a communication circuit of the image processing apparatus;</claim-text><claim-text>applying evaluation processing to the reduced image data;</claim-text><claim-text>transmitting a result of the evaluation processing to the information processing apparatus through the communication circuit;</claim-text><claim-text>receiving image data having a higher resolution than the reduced image data from the information processing apparatus through the communication circuit;</claim-text><claim-text>applying predetermined image processing to the image data; and</claim-text><claim-text>transmitting a result of applying the image processing to the information processing apparatus through the communication circuit.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. A non-transitory computer-readable medium storing a program for causing a computer to function as an image processing apparatus comprising:<claim-text>a communication unit; and</claim-text><claim-text>a control unit,</claim-text><claim-text>wherein the control unit:</claim-text><claim-text>receives reduced image data from an information processing apparatus through the communication unit,</claim-text><claim-text>applies evaluation processing to the reduced image data,</claim-text><claim-text>transmits a result of the evaluation processing to the information processing apparatus through the communication unit,</claim-text><claim-text>receives image data having a higher resolution than the reduced image data from the information processing apparatus through the communication unit,</claim-text><claim-text>applies predetermined image processing to the image data, and</claim-text><claim-text>transmits a result of the image processing to the information processing apparatus through the communication unit.</claim-text></claim-text></claim></claims></us-patent-application>