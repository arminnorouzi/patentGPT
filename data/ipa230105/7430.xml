<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007431A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007431</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17858987</doc-number><date>20220706</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>5</main-group><subgroup>033</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>W</subclass><main-group>4</main-group><subgroup>029</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>304</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>307</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>5</main-group><subgroup>033</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180201</date></cpc-version-indicator><section>H</section><class>04</class><subclass>W</subclass><main-group>4</main-group><subgroup>029</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>2420</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">LOCATION BASED AUDIO SIGNAL MESSAGE PROCESSING</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17242288</doc-number><date>20210427</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11451923</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17858987</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16425410</doc-number><date>20190529</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11032664</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17242288</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62677635</doc-number><date>20180529</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only"><addressbook><orgname>Staton Techiya LLC</orgname><address><city>Delray Beach</city><state>FL</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Walker</last-name><first-name>Bruce</first-name><address><city>Atlanta</city><state>GA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Usher</last-name><first-name>John</first-name><address><city>Beer</city><country>GB</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Keady</last-name><first-name>John P.</first-name><address><city>Fairfax Station</city><state>VA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Staton Techiya LLC</orgname><role>02</role><address><city>Delray Beach</city><state>FL</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of incorporating environmental acoustic sources into a virtual environment by measuring real environment acoustic sources and locations and incorporating them into a virtual environment with virtual acoustic sources.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="223.35mm" wi="134.03mm" file="US20230007431A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="101.77mm" wi="86.44mm" file="US20230007431A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="164.68mm" wi="140.80mm" file="US20230007431A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="135.89mm" wi="86.36mm" file="US20230007431A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="208.79mm" wi="149.94mm" file="US20230007431A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="225.38mm" wi="136.06mm" file="US20230007431A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="244.69mm" wi="133.43mm" file="US20230007431A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="118.53mm" wi="148.51mm" file="US20230007431A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="199.31mm" wi="149.18mm" file="US20230007431A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="162.31mm" wi="127.34mm" file="US20230007431A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of and claims priority to U.S. patent application Ser. No. 17/242,288, filed 27 Apr. 2021, which is a continuation of and claims priority to U.S. patent application Ser. No. 16/425,410, filed 29 May 2019, which is a non provisional of and claims priority to U.S. Pat. App. No. 62/677,635, filed 29 May 2018, the disclosure of all of which are incorporated herein by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE INVENTION</heading><p id="p-0003" num="0002">The present invention relates in general to methods for hardware and software components of an earphone for processing audio in an augmented reality sound scene, and in particular, though not exclusively, enhancing the perceived naturalness of the augmented reality experience.</p><heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0004" num="0003">Auditory display is the use of speech and non-speech audio to convey information or perceptualize data. Such auditorily displayed messages can be virtual, originating from a sound source that is not physically located in an immediate environment, although in Augmented Reality (AR) scenes the user may be presented with an image corresponding to the virtual sound source.</p><p id="p-0005" num="0004">To enhance the perceived naturalness of the AR experience, the spatial acoustic properties of the audio message can be consistent with the spatial acoustic properties of the scene the sound source is located within. For example, if the user is located outdoors then the virtual sound message can be processed with a spatial acoustics sound filter that matches the outdoor scene. Likewise indoors the spatial acoustics sound filter can be matched to an indoor scene.</p><p id="p-0006" num="0005">The present invention discloses a method to process a speech or non-speech audio content/message with a spatial acoustic filter where the filter matches the spatial acoustic properties of the physical environment the user is within. The processing is directed to earphone hardware platforms.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006">Exemplary embodiments of present invention will become more fully understood from the detailed description and the accompanying drawings, wherein:</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a general method using a spatial filter in accordance with an exemplary embodiment;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a method of determining whether a user is inside or outside to choose an appropriate impulse response in accordance with an exemplary embodiment;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a method of determining modifying an acoustic message with a spatial impulse in accordance with an exemplary embodiment;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a method of processing an audio signal with an environmental impulse response in accordance with an exemplary embodiment;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a method of monitoring the change in location of the user and modifying the environmental impulse response in accordance with an exemplary embodiment;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, and <figref idref="DRAWINGS">FIG. <b>6</b>C</figref> illustrate methods of incorporating environmental acoustic sources into a virtual environment;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a method of associating a geographical region with a particular SIR;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic diagram of a system for utilizing earphones according to an embodiment of the present disclosure; and</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram of a machine in the form of a computer system which a set of instructions, when executed, may cause the machine to perform any one or more of the methodologies or operations of the systems and methods for utilizing an earphone according to embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">ABBREVIATIONS</heading><p id="p-0017" num="0016">A2DP: Advanced Audio Distribution Profile. The Bluetooth 2.1 mode for uni-directional transfer of an audio stream in up to 2 channel stereo, either to or from the Bluetooth host, AKA &#x201c;music mode&#x201d;.</p><p id="p-0018" num="0017">ASM: Ambient Sound Microphone. Microphones configured to detect sound around the listener, not in the ear canal. There is one external microphone on each HearBud.</p><p id="p-0019" num="0018">BTLE: Bluetooth low energy, AKA Bluetooth 4.0 (i.e. non-audio low baud data transfer).</p><p id="p-0020" num="0019">CL: Cirrus Logic, the quad core DSP in the ButtonBox.</p><p id="p-0021" num="0020">CSR: Cambridge Silicon Radio Bluetooth module, containing the Bluetooth CSR 8670 chip, antennae, RAM etc.</p><p id="p-0022" num="0021">ECM: Ear Canal Microphone. Digital microphone for detecting sound in the occluded ear canal of the user. The ASM and ECM are the same component model.</p><p id="p-0023" num="0022">SPKR/ECR: Ear Canal Receiver. A &#x201c;receiver&#x201d; is another name for a loudspeaker: it is probably so-called due to Bells <b>1876</b> patent for &#x201c;apparatus for transmitting vocal or other sounds telegraphically&#x201d;, where the &#x201c;receiver&#x201d; was the loudspeaker transducer for receiving the telegraphic signal from the far-end party.</p><p id="p-0024" num="0023">SNR: Signal-to-noise ratio.</p><p id="p-0025" num="0024">SPKR: LoudSpeaker, this abbreviation is often used instead of ECR but refer to the same component.</p><p id="p-0026" num="0025">SIR: Spatial Impulse Response. An SIR is one or two signals corresponding (respectively) to a mono or stereo acoustic impulse response for an acoustic space&#x2014;as is familiar to those skilled in the art. The mono SIR is an actual or virtual acoustic IR from a sound source to a single location, e.g. a single microphone measurement location, and a stereo SIR is an actual or virtual acoustic IR from a sound source to two locations, where each location approximates the location of a left and right ear of an individual in that same acoustic space.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS</heading><p id="p-0027" num="0026">The following description of exemplary embodiment(s) is merely illustrative in nature and is in no way intended to limit the invention, its application, or uses.</p><p id="p-0028" num="0027">Exemplary embodiments are directed to or can be operatively used on various wired or wireless audio devices (e.g., hearing aids, ear monitors, earbuds, headphones, ear terminal, behind the ear devices or other acoustic devices as known by one of ordinary skill, and equivalents). For example, the earpieces can be without transducers (for a noise attenuation application in a hearing protective earplug) or one or more transducers (e.g. ambient sound microphone (ASM), ear canal microphone (ECM), ear canal receiver (ECR)) for monitoring/providing sound. In all the examples illustrated and discussed herein, any specific values should be interpreted to be illustrative only and non-limiting. Thus, other examples of the exemplary embodiments could have different values.</p><p id="p-0029" num="0028">Processes, techniques, apparatus, and materials as known by one of ordinary skill in the art may not be discussed in detail but are intended to be part of the enabling description where appropriate. For example, specific materials may not be listed for achieving each of the targeted properties discussed, however one of ordinary skill would be able, without undo experimentation, to determine the materials needed given the enabling disclosure herein.</p><p id="p-0030" num="0029">Notice that similar reference numerals and letters refer to similar items in the following figures, and thus once an item is defined in one figure, it may not be discussed or further defined in the following figures. Processes, techniques, apparatus, and materials as known by one of ordinary skill in the relevant art may not be discussed in detail but are intended to be part of the enabling description where appropriate.</p><p id="p-0031" num="0030">The present invention is directed towards processing audio signals for reproduction with a loudspeaker on an earphone or headphone. The audio signal can be a speech or non speech message or audio content that conveys information. The speech signal may be a continuous speech signal from a pre-recorded stored data file or a live speech signal from another individual.</p><p id="p-0032" num="0031">In the present invention, the received audio signal is processed with a spatial impulse response (SIR). An SIR is one or two signals corresponding (respectively) to a mono or stereo acoustic impulse response for an acoustic space&#x2014;as is familiar to those skilled in the art. The mono SIR is an actual or virtual acoustic IR from a sound source to a single location, e.g. a single microphone measurement location, and a stereo SIR is an actual or virtual acoustic IR from a sound source to two locations, where each location approximates the location of a left and right ear of an individual in that same acoustic space.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a general overview of at least one example of the present invention: A user location is determined in step <b>102</b> to determine if the user is inside a building or outside. Such a location can be determined using GPS coordinates, using a GPS transceiver and associated hardware and software processing on a mobile computing device or located with an earphone device. Note that inertial navigation systems can be used in conjunction with GPS systems. For example, the virtual reality device (e.g., goggles) can have INS chips imbedded that measure acceleration and velocity which can then be fed into a navigational model from a start position. Alternatively, the location can be determined using other methods such as based on triangulation from available wifi signals and associating the wifi network name with a known geographical location. For instance, if the mobile phone of the user has a strong signal connection strength to a wifi network, then a logic unit can determine the user is inside.</p><p id="p-0034" num="0033">A spatial filter (i.e. an SIR) is determined in step <b>104</b>: a nonexemplary embodiment is described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0035" num="0034">In step <b>106</b>, a received audio signal is processed with the determined spatial filter, a nonexemplary embodiment is described in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0036" num="0035">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an exemplary method is described to select either an &#x201c;outside&#x201d; SIR or an &#x201c;inside&#x201d; SIR.</p><p id="p-0037" num="0036">Based on the determined user location from step <b>202</b>, it is determined if the user is located outside or inside in step <b>204</b>. &#x201c;Outside&#x201d; here refers to meaning the user (who is assumed to have the same location as the location determining device, e.g. the GPS device) is situated outdoors&#x2014;i.e. with no substantial roofing material location directly above them. Inside here means the user is located within a building structure, with a substantial roof above them (a substantial roofing material being concrete, wood, roofing tiles etc).</p><p id="p-0038" num="0037">An exemplary embodiment that describes a method for determining if the user is outside or inside is described below:</p><p id="p-0039" num="0038">Method 1: based on an analysis of the GPS accuracy: if the accuracy of the GPS location estimate is below a pre-determined value (which may also be stated as when the error of the GPS location estimate is greater than a pre-determined value, e.g., 15 metres for any given direction) then the user is determined to be inside, as it is assumed that substantial roofing material will attenuate the GPS signals and give a lower accuracy (i.e., higher error) when the user (i.e., the GPS device) is inside. Accuracy can also be determined by the variation in GPS over time. The accuracy of the GPS determined location can be determined by comparison of GPS determined location with map placement location and comparison with Inertial Navigation System (INS) information. For example, if a user has identified that they have just left a building yet the map placement is 5 m inside the building, then the GPS accuracy can be determined. Additionally, GPS location can be enhanced. For example, accuracy is often dependent upon the number of satellites that can be seen, a minimum of 4 is needed to solve the unknowns, x, y, z, and t. If 3 satellites are only available because of signal degradation due to being inside a structure then it is common to eliminate a change in z so that there are three unknowns x, y, and t, which improves x, y, and t location. If 2 satellites are available solution is not typically attempted. Even if no GPS solution is available, INS data can be augmented with GPS data, even if only 2 or 1 satellite signal are acquired to improve the INS determined location, velocity, and acceleration.</p><p id="p-0040" num="0039">Method 2: based on the given GPS location, it is determined if the user is inside or outside by associating the given GPS location with the location on a map, and from the location on the map determining if the user is inside a building or outside. If the user is outside then the audio signal is processed with an outside impulse response function, step <b>206</b>. If the user is inside then the audio signal is processed with an inside impulse response function, step <b>208</b>.</p><p id="p-0041" num="0040">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, an exemplary method is described to process the received audio signal with an SIR and direct the processed signal to a loudspeaker in an earphone. If the SIR is a stereo SIR, then two processed output signals will be produced&#x2014;one sent to the left loudspeaker and the other to the right loudspeaker of the earphone. If the SIR is mono, then the same output signal is directed to both left and right loudspeakers of the earphone. The method of processing the received audio signal with the SIR is convolution&#x2014;a frequency or time domain signal operation that is familiar and well known to those skilled in the art. The steps includes receiving an audio signal <b>302</b>, receiving a spatial impulse response (SIR) <b>304</b>, convolving the received audio signal with the SIR <b>306</b>, and direct the processed audio message to the loudspeaker in the earphone <b>308</b>.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a method of SIR modification of the virtual audio environment. First the navigational information (location, velocity, acceleration, orientation) is obtained, e.g., via GPS <b>402</b>, INS <b>404</b>, combining multiple sensor data <b>410</b> into a navigational filter (e.g. Kalman Filter) to refine location, orientation, velocity and acceleration of the viewing vector VV of a user (e.g., a vector from the user's head through the VR visual display). In step <b>406</b> the velocity, orientation, and acceleration can be determined from the INS data, GPS data, or a data fusion of the sensor data. In step <b>408</b> the GPS signal can be used to determine the User's likely orientation, location and direction of travel. IN step <b>410</b> the data from available resources is placed into navigational filters (e.g., Kalman Filter) to refine orientation and location. The navigational information is then compared to maps <b>412</b> (e.g., google maps) and a virtual representation of the user placed upon the map and oriented according to the information (Note that this modified map can also be displayed if desired). If the user is located inside a building, a database is searched for that building's schematics, and the virtual representation placed within the building. In step <b>414</b>, the &#x201c;near user&#x201d; environment of the map, (e.g., within 1000 m of the user representation) is searched for sources (S<b>1</b>, S<b>2</b>, S<b>3</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>), which are then used to compile a SIR to apply to audio content so that the augmented or virtual environment's audio environment includes elements from the real environment in which a user is located, or virtual representations of the real acoustic environment (e.g., stylized or recorded audio mimicking elements of the real environment, e.g., bird calls). In step <b>416</b> the time and spatial environmental impulse response are derived. In step <b>418</b> the audio signal is modified the time and spatial environmental impulse response derived.</p><p id="p-0043" num="0042">Note that an additional feature is to feedback into the SIR measurements from the microphones on the virtual or augments system (V/A-S), or a device on the user that can record and transmit to the V/A-S or in the environment. For example microphones on devices in the environment (outside cameras, microphones, e.g., traffic poles with mics and cameras) if publicly available can be fed into the V/A-S to update the next SIR as the location varies. Note also that an alarm can be passed through and emphasized in the virtual or augmented reality environment.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a general process in accordance with at least one exemplary embodiment; in this embodiment when the navigational information changes an amount greater than a predetermined threshold the SIR is updated using all available environment data sources. The steps include:</p><p id="p-0045" num="0044">Step <b>502</b>: receive an audio signal;</p><p id="p-0046" num="0045">Step <b>504</b>: receive an environmental impulse response;</p><p id="p-0047" num="0046">Step <b>506</b>: the received audio signal is convolved using the environmental impulse response;</p><p id="p-0048" num="0047">Step <b>508</b>: send the convolved audio signal to a speaker;</p><p id="p-0049" num="0048">Step <b>510</b>: monitor the time, location and orientation of the user; and</p><p id="p-0050" num="0049">Step <b>512</b>: check to see if the monitored values in Step <b>510</b> exceed a threshold, and if so then proceed to Step <b>504</b>.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> illustrates a User in an environment outdoors. In <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> a user <b>600</b> wearing an AR or VR goggle set <b>610</b> walking along a path <b>680</b>. <figref idref="DRAWINGS">FIG. <b>6</b>C</figref> shows a user wearing an AR or VR set within a room. The AR set can be operationally connected with a computer processing device (S<b>4</b>), e.g. via a wired or wireless link. Alternatively, the VR or AR set can be directly connected with the cloud via wireless means. The goggle <b>610</b> has a coordinate system attached to it (e.g., <b>620</b>, <b>630</b>, <b>640</b>) the display <b>611</b> seen by the user is illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>. Sources (e.g., acoustic sources) in the real world (<figref idref="DRAWINGS">FIG. <b>6</b>A</figref>) can be virtually displayed as icons (e.g., <b>671</b>) and acoustically displayed (e.g., played binaurally into a headset) in the video display <b>611</b> so that the source S<b>3</b> is heard by the user in the approximate location as the real source location. Likewise sources S<b>1</b> and S<b>2</b> can be displayed virtually in display <b>611</b>. The critical real world boundaries <b>680</b> can be shown in the display <b>611</b> as edges <b>690</b> to aid the user in avoiding hazards.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>6</b>C</figref> illustrates a user wearing a goggle <b>613</b>, with various room sources S<b>5</b>, S<b>6</b>, and S<b>7</b> displayed in the users visual field and acoustically displayed binaurally so that the user hears the sources in their approximate location to the user.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a method to associate a geographical region with a particular SIR. The method comprises the steps of:</p><p id="p-0054" num="0053"><b>702</b>: Receiving a determined location of the user, e.g. as a latitude and longitude position.</p><p id="p-0055" num="0054"><b>706</b>: associating the determined region with a Spatial Impulse Response</p><p id="p-0056" num="0055"><b>708</b>: if the user location has changed, we repeat the process of associating the new location with an SIR (note the SIR may not necessarily change). Note that the methods herein can be implemented on various platforms, for example an earphone.</p><p id="p-0057" num="0056">As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a system <b>2400</b> and methods for utilizing eartips and/or earphone devices are disclosed. The system <b>2400</b> may be configured to support, but is not limited to supporting, data and content services, audio processing applications and services, audio output and/or input applications and services, applications and services for transmitting and receiving audio content, authentication applications and services, computing applications and services, cloud computing services, internet services, satellite services, telephone services, software as a service (SaaS) applications, platform-as-a-service (PaaS) applications, gaming applications and services, social media applications and services, productivity applications and services, voice-over-internet protocol (VoIP) applications and services, speech-to-text translation applications and services, interactive voice applications and services, mobile applications and services, and any other computing applications and services. The system may include a first user <b>2401</b>, who may utilize a first user device <b>2402</b> to access data, content, and applications, or to perform a variety of other tasks and functions. As an example, the first user <b>2401</b> may utilize first user device <b>2402</b> to access an application (e.g. a browser or a mobile application) executing on the first user device <b>2402</b> that may be utilized to access web pages, data, and content associated with the system <b>2400</b>. In certain embodiments, the first user <b>2401</b> may be any type of user that may potentially desire to listen to audio content, such as from, but not limited to, a music playlist accessible via the first user device <b>2402</b>, a telephone call that the first user <b>2401</b> is participating in, audio content occurring in an environment in proximity to the first user <b>2401</b>, any other type of audio content, or a combination thereof. For example, the first user <b>2401</b> may be an individual that may be participating in a telephone call with another user, such as second user <b>2420</b>.</p><p id="p-0058" num="0057">The first user device <b>2402</b> utilized by the first user <b>2401</b> may include a memory <b>2403</b> that includes instructions, and a processor <b>2404</b> that executes the instructions from the memory <b>2403</b> to perform the various operations that are performed by the first user device <b>2402</b>. In certain embodiments, the processor <b>2404</b> may be hardware, software, or a combination thereof. The first user device <b>2402</b> may also include an interface <b>2405</b> (e.g. screen, monitor, graphical user interface, etc.) that may enable the first user <b>2401</b> to interact with various applications executing on the first user device <b>2402</b>, to interact with various applications executing within the system <b>2400</b>, and to interact with the system <b>2400</b> itself. In certain embodiments, the first user device <b>2402</b> may include any number of transducers, such as, but not limited to, microphones, speakers, any type of audio-based transducer, any type of transducer, or a combination thereof. In certain embodiments, the first user device <b>2402</b> may be a computer, a laptop, a tablet device, a phablet, a server, a mobile device, a smartphone, a smart watch, and/or any other type of computing device. Illustratively, the first user device <b>2402</b> is shown as a mobile device in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The first user device <b>2402</b> may also include a global positioning system (GPS), which may include a GPS receiver and any other necessary components for enabling GPS functionality, accelerometers, gyroscopes, sensors, and any other componentry suitable for a mobile device.</p><p id="p-0059" num="0058">In addition to using first user device <b>2402</b>, the first user <b>2401</b> may also utilize and/or have access to a second user device <b>2406</b> and a third user device <b>2410</b>. As with first user device <b>2402</b>, the first user <b>2401</b> may utilize the second and third user devices <b>2406</b>, <b>2410</b> to transmit signals to access various online services and content. The second user device <b>2406</b> may include a memory <b>2407</b> that includes instructions, and a processor <b>2408</b> that executes the instructions from the memory <b>2407</b> to perform the various operations that are performed by the second user device <b>2406</b>. In certain embodiments, the processor <b>2408</b> may be hardware, software, or a combination thereof. The second user device <b>2406</b> may also include an interface <b>2409</b> that may enable the first user <b>2401</b> to interact with various applications executing on the second user device <b>2406</b> and to interact with the system <b>2400</b>. In certain embodiments, the second user device <b>2406</b> may include any number of transducers, such as, but not limited to, microphones, speakers, any type of audio-based transducer, any type of transducer, or a combination thereof. In certain embodiments, the second user device <b>2406</b> may be and/or may include a computer, any type of sensor, a laptop, a set-top-box, a tablet device, a phablet, a server, a mobile device, a smartphone, a smart watch, and/or any other type of computing device. Illustratively, the second user device <b>2402</b> is shown as a smart watch device in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0060" num="0059">The third user device <b>2410</b> may include a memory <b>2411</b> that includes instructions, and a processor <b>2412</b> that executes the instructions from the memory <b>2411</b> to perform the various operations that are performed by the third user device <b>2410</b>. In certain embodiments, the processor <b>2412</b> may be hardware, software, or a combination thereof. The third user device <b>2410</b> may also include an interface <b>2413</b> that may enable the first user <b>2401</b> to interact with various applications executing on the second user device <b>2406</b> and to interact with the system <b>2400</b>. In certain embodiments, the third user device <b>2410</b> may include any number of transducers, such as, but not limited to, microphones, speakers, any type of audio-based transducer, any type of transducer, or a combination thereof. In certain embodiments, the third user device <b>2410</b> may be and/or may include a computer, any type of sensor, a laptop, a set-top-box, a tablet device, a phablet, a server, a mobile device, a smartphone, a smart watch, and/or any other type of computing device. Illustratively, the third user device <b>2410</b> is shown as a smart watch device in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0061" num="0060">The first, second, and/or third user devices <b>2402</b>, <b>2406</b>, <b>2410</b> may belong to and/or form a communications network <b>2416</b>. In certain embodiments, the communications network <b>2416</b> may be a local, mesh, or other network that facilitates communications among the first, second, and/or third user devices <b>2402</b>, <b>2406</b>, <b>2410</b> and/or any other devices, programs, and/or networks of system <b>2400</b> or outside system <b>2400</b>. In certain embodiments, the communications network <b>2416</b> may be formed between the first, second, and third user devices <b>2402</b>, <b>2406</b>, <b>2410</b> through the use of any type of wireless or other protocol and/or technology. For example, the first, second, and third user devices <b>2402</b>, <b>2406</b>, <b>2410</b> may communicate with one another in the communications network <b>2416</b>, such as by utilizing Bluetooth Low Energy (BLE), classic Bluetooth, ZigBee, cellular, NFC, Wi-Fi, Z-Wave, ANT+, IEEE 802.15.4, IEEE 802.22, ISA100a, infrared, ISM band, RFID, UWB, Wireless HD, Wireless USB, any other protocol and/or wireless technology, satellite, fiber, or any combination thereof. Notably, the communications network <b>2416</b> may be configured to communicatively link with and/or communicate with any other network of the system <b>2400</b> and/or outside the system <b>2400</b>.</p><p id="p-0062" num="0061">The system <b>2400</b> may also include an earphone device <b>2415</b>, which the first user <b>2401</b> may utilize to hear and/or audition audio content, transmit audio content, receive audio content, experience any type of content, process audio content, adjust audio content, store audio content, perform any type of operation with respect to audio content, or a combination thereof. The earphone device <b>2415</b> may be an earpiece, a hearing aid, an ear monitor, an ear terminal, a behind-the-ear device, any type of acoustic device, or a combination thereof. The earphone device <b>2415</b> may include any type of component utilized for any type of earpiece. In certain embodiments, the earphone device <b>2415</b> may include any number of ambient sound microphones that may be configured to capture and/or measure ambient sounds and/or audio content occurring in an environment that the earphone device <b>2415</b> is present in and/or is proximate to. In certain embodiments, the ambient sound microphones may be placed at a location or locations on the earphone device <b>2415</b> that are conducive to capturing and measuring ambient sounds occurring in the environment. For example, the ambient sound microphones may be positioned in proximity to a distal end (e.g. the end of the earphone device <b>2415</b> that is not inserted into the first user's <b>2401</b> ear) of the earphone device <b>2415</b> such that the ambient sound microphones are in an optimal position to capture ambient or other sounds occurring in the environment. In certain embodiments, the earphone device <b>2415</b> may include any number of ear canal microphones, which may be configured to capture and/or measure sounds occurring in an ear canal of the first user <b>2401</b> or other user wearing the earphone device <b>2415</b>. In certain embodiments, the ear canal microphones may be positioned in proximity to a proximal end (e.g. the end of the earphone device <b>2415</b> that is inserted into the first user's <b>2401</b> ear) of the earphone device <b>2415</b> such that sounds occurring in the ear canal of the first user <b>2401</b> may be captured more readily.</p><p id="p-0063" num="0062">The earphone device <b>2415</b> may also include any number of transceivers, which may be configured to transmit signals to and/or receive signals from any of the devices in the system <b>2400</b>. In certain embodiments, a transceiver of the earphone device <b>2415</b> may facilitate wireless connections and/or transmissions between the earphone device <b>2415</b> and any device in the system <b>2400</b>, such as, but not limited to, the first user device <b>2402</b>, the second user device <b>2406</b>, the third user device <b>2410</b>, the fourth user device <b>2421</b>, the fifth user device <b>2425</b>, the earphone device <b>2430</b>, the servers <b>2440</b>, <b>2445</b>, <b>2450</b>, <b>2460</b>, and the database <b>2455</b>. The earphone device <b>2415</b> may also include any number of memories for storing content and/or instructions, processors that execute the instructions from the memories to perform the operations for the earphone device <b>2415</b>, and/or any type integrated circuit for facilitating the operation of the earphone device <b>2415</b>. In certain embodiments, the processors may comprise, hardware, software, or a combination of hardware and software. The earphone device <b>2415</b> may also include one or more ear canal receivers, which may be speakers for outputting sound into the ear canal of the first user <b>2401</b>. The ear canal receivers may output sounds obtained via the ear canal microphones, ambient sound microphones, any of the devices in the system <b>2400</b>, from a storage device of the earphone device <b>2415</b>, or any combination thereof.</p><p id="p-0064" num="0063">The ear canal receivers, ear canal microphones, transceivers, memories, processors, integrated circuits, and/or ear canal receivers may be affixed to an electronics package that includes a flexible electronics board. The earphone device <b>2415</b> may include an electronics packaging housing that may house the ambient sound microphones, ear canal microphones, ear canal receivers (i.e. speakers), electronics supporting the functionality of the microphones and/or receivers, transceivers for receiving and/or transmitting signals, power sources (e.g. batteries and the like), any circuitry facilitating the operation of the earphone device <b>2415</b>, or any combination thereof. The electronics package including the flexible electronics board may be housed within the electronics packaging housing to form an electronics packaging unit. The earphone device <b>2415</b> may further include an earphone housing, which may include receptacles, openings, and/or keyed recesses for connecting the earphone housing to the electronics packaging housing and/or the electronics package. For example, nozzles of the electronics packaging housing may be inserted into one or more keyed recesses of the earphone housing so as to connect and secure the earphone housing to the electronics packaging housing. When the earphone housing is connected to the electronics packaging housing, the combination of the earphone housing and the electronics packaging housing may form the earphone device <b>2415</b>. The earphone device <b>2415</b> may further include a cap for securing the electronics packaging housing, the earphone housing, and the electronics package together to form the earphone device <b>2415</b>.</p><p id="p-0065" num="0064">In certain embodiments, the earphone device <b>2415</b> may be configured to have any number of changeable tips, which may be utilized to facilitate the insertion of the earphone device <b>2415</b> into an ear aperture of an ear of the first user <b>2401</b>, secure the earphone device <b>2415</b> within the ear canal of an ear of the first user <b>2401</b>, and/or to isolate sound within the ear canal of the first user <b>2401</b>. The tips may be foam tips, which may be affixed onto an end of the earphone housing of the earphone device <b>2415</b>, such as onto a stent and/or attachment mechanism of the earphone housing. In certain embodiments, the tips may be any type of eartip as disclosed and described in the present disclosure. The eartips as disclosed in the present disclosure may be configured to facilitate distributed reduced contact force, sound isolation for sound in the ear canal of the first user <b>2401</b> (i.e. between the ambient environment and the ear canal environment within an ear of the first user <b>2401</b>), mold into a variety of forms and/or positions, encapsulate volumes upon insertion into an ear aperture of the first user <b>2401</b>, have a pressure adjusting design, facilitate notched stent retention (i.e. on a stent of the earphone housing), facilitate stent insertion into an ear canal of the first user <b>2401</b> via an ear aperture of the first user <b>2401</b>, or any combination thereof. In certain embodiments, the eartip may be designed to provide sound isolation capability that is at least as effective as conventional foam and/or flange tips. Notably, the eartips may be manufactured and configured to be made in any desired size specifications and/or materials, and may be tailored to each individual user, such as first user <b>2401</b>. In contrast to conventional foam or flange tips, an eartip according to the present disclosure may be adjusted for size without having to substitute the eartip with another eartip, may have an EPA NRR rating of NRR=18, may have a unique flatter high frequency attenuation profile so as to maintain audio quality, may have ease of manufacturability, and may be designed to distribute contact force and minimize radial force against a user's ear canal walls when positioned in a user's ear canal. Additionally, an eartip according to the present disclosure may be made of a non-porous material that is not closed cell foam or open cell foam.</p><p id="p-0066" num="0065">In certain embodiments, the eartip may be designed so that the earphone device's <b>2415</b> retention force on the ear canal walls of the first user <b>2401</b> may be distributed over a larger area than traditional foam or flange tips allow, thereby reducing the pressure on the ear canal walls of the first user <b>2401</b>. Unlike foam tips, which primarily provide a restoring radial force that exerts pressure against the ear canal walls of a user, the eartip is designed to move both radially and axially, which allows for more give and redistribution of contact over a larger area, and, thus, decreases the retention pressure. As a result, this allows for increased comfort for the user and allows the user to utilize the eartip for an extended period of time when compared to traditional foam and/or flange tips. In certain embodiments, the eartip utilized with the earphone device <b>2415</b> may be configured to encapsulate a volume of gas and/or liquid. In either case (i.e. gas or liquid), the bulk of sound isolation provided by the eartip is achieved through the reflection of ambient sound waves so that the encapsulated volume can be low mass. In certain embodiments, portions of the eartip may encapsulate a volume with the ability to release volume when pressed upon without having to incorporate complicated valves. The encapsulated volume may be achieved by the ear canal wall pressing radially and/or axially against the outer surfaces of the eartip, which may force the outer portion of the eartip to seal with the inner portion of the eartip. In certain embodiments, the inner portion of the eartip may be small than the outer diameter of the stent of the earphone housing upon which the eartip is placed so that upon insertion of the eartip on the stent, the inner portion stretches outward to meet the outer surface of the eartip, which further facilitates the sealing of the ear canal of the first user <b>2401</b>.</p><p id="p-0067" num="0066">In certain embodiments, the stent of the eartip, over which the eartip is placed, may be designed to have a smaller diameter front end and a larger diameter middle section to promote retention of the eartip on the stent itself. In certain embodiments, a portion of the eartip may have an inner core diameter that is smaller than the stent outer diameter so that the eartip provides radial compression upon the stent so as to enhance sealing and to add friction to prevent axial slippage within the ear canal of the first user <b>2401</b>. In certain embodiments, an increased mid-section inner core diameter of the eartip may be utilized (i.e. larger than the smaller inner core diameter of the eartip), which may be configured to line up with the mid-section outer diameter of the stent of the earphone housing of the earphone device <b>2415</b>. This may provide axial stability for the earphone device <b>2415</b>, while simultaneously preventing axial slippage from the ear canal of the first user <b>2401</b>. In certain embodiments, the eartip may have an insertion end that has a funnel shape, which aids in inserting the eartip onto the stent of the earphone housing of the earphone device <b>2415</b>.</p><p id="p-0068" num="0067">In certain embodiments, the eartip has a configuration that applies minimal force against the first user's <b>2401</b> ear canal. Additionally, the eartip can seal the first user's <b>2401</b> ear canal by providing at least 15 dB of attenuation across frequency. To facilitate manufacturability, the eartip may be molded inverted, thereby allowing inexpensive mass production. Lips of the eartip may then be folded to contact ledges for the eartip that may be utilized by the first user <b>2401</b>. Sealing and comfort depend upon an accurate fit within the first user's <b>2401</b> ear canal, and, as a result, eartips according to the present disclosure may be manufactured in several single sizes, and, because of the unique design of the eartips, a single eartip may be adjusted to fit multiple sizes, which minimizes manufacturing costs, while allowing for more flexibility, versatility, and for a greater number of sizes for the eartip. Notably, any of the features of any of the eartips described in the present disclosure may be combined and/or interchanged with any other eartips described in the present disclosure. Furthermore, the shape, size, features and/or functionality of any of the components of the earphone device and/or hearbud housing device described in the present disclosure may be modified for each particular user for the shape and size of each user's ear aperture and/or ear canal, or a combination thereof.</p><p id="p-0069" num="0068">Notably, in experiments conducted using the eartip, the experiments have shown that the eartip allows for similar levels of sound isolation when compared to conventional foam and/or flange tips. For example, experiments have shown that the eartips provided in the present disclosure provided a NRR of 18 with a generally flat high frequency profile. A flat attenuation profile maintains an ambient environment's frequency profile when level reduced by the attenuation, which can be useful in maintaining the quality of ambient speech and music (or other audio content) during the level reduction process.</p><p id="p-0070" num="0069">In further embodiments, the eartip may be configured to have an open configuration prior to insertion onto a stent of the earphone housing and/or the earphone device <b>2415</b> itself. By having an open configuration, the eartip may be mass produced using conventional molding techniques and/or by utilizing 3D commercial printers. The open configuration of the eartip also facilitates molding, and can be 3D printed, where the open configuration allows for resin removal. For example, resin removal may be achieved by utilizing commercial 3D printers that allow the use of lower durometer materials, such as Stratasys machines and the like. In certain embodiments, since the eartip has an open configuration, which is then sealed, any additional pressure can force encapsulated gas out of the eartip relieving the feedback pressure so as to keep the comfort level for the first user <b>2401</b> relatively stable.</p><p id="p-0071" num="0070">In addition to the first user <b>2401</b>, the system <b>2400</b> may include a second user <b>2420</b>, who may utilize a fourth user device <b>2421</b> to access data, content, and applications, or to perform a variety of other tasks and functions. Much like the first user <b>2401</b>, the second user <b>2420</b> may be may be any type of user that may potentially desire to listen to audio content, such as from, but not limited to, a storage device of the fourth user device <b>2421</b>, a telephone call that the second user <b>2420</b> is participating in, audio content occurring in an environment in proximity to the second user <b>2420</b>, any other type of audio content, or a combination thereof. For example, the second user <b>2420</b> may be an individual that may be listening to songs stored in a playlist that resides on the fourth user device <b>2421</b>. Also, much like the first user <b>2401</b>, the second user <b>2420</b> may utilize fourth user device <b>2421</b> to access an application (e.g. a browser or a mobile application) executing on the fourth user device <b>2421</b> that may be utilized to access web pages, data, and content associated with the system <b>2400</b>. The fourth user device <b>2421</b> may include a memory <b>2422</b> that includes instructions, and a processor <b>2423</b> that executes the instructions from the memory <b>2422</b> to perform the various operations that are performed by the fourth user device <b>2421</b>. In certain embodiments, the processor <b>2423</b> may be hardware, software, or a combination thereof. The fourth user device <b>2421</b> may also include an interface <b>2424</b> (e.g. a screen, a monitor, a graphical user interface, etc.) that may enable the second user <b>2420</b> to interact with various applications executing on the fourth user device <b>2421</b>, to interact with various applications executing in the system <b>2400</b>, and to interact with the system <b>2400</b>. In certain embodiments, the fourth user device <b>2421</b> may include any number of transducers, such as, but not limited to, microphones, speakers, any type of audio-based transducer, any type of transducer, or a combination thereof. In certain embodiments, the fourth user device <b>2421</b> may be a computer, a laptop, a tablet device, a phablet, a server, a mobile device, a smartphone, a smart watch, and/or any other type of computing device. Illustratively, the fourth user device <b>2421</b> may be a computing device in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The fourth user device <b>2421</b> may also include any of the componentry described for first user device <b>2402</b>, the second user device <b>2406</b>, and/or the third user device <b>2410</b>. In certain embodiments, the fourth user device <b>2421</b> may also include a global positioning system (GPS), which may include a GPS receiver and any other necessary components for enabling GPS functionality, accelerometers, gyroscopes, sensors, and any other componentry suitable for a computing device.</p><p id="p-0072" num="0071">In addition to using fourth user device <b>2421</b>, the second user <b>2420</b> may also utilize and/or have access to a fifth user device <b>2425</b>. As with fourth user device <b>2421</b>, the second user <b>2420</b> may utilize the fourth and fifth user devices <b>2421</b>, <b>2425</b> to transmit signals to access various online services and content. The fifth user device <b>2425</b> may include a memory <b>2426</b> that includes instructions, and a processor <b>2427</b> that executes the instructions from the memory <b>2426</b> to perform the various operations that are performed by the fifth user device <b>2425</b>. In certain embodiments, the processor <b>2427</b> may be hardware, software, or a combination thereof. The fifth user device <b>2425</b> may also include an interface <b>2428</b> that may enable the second user <b>2420</b> to interact with various applications executing on the fifth user device <b>2425</b> and to interact with the system <b>2400</b>. In certain embodiments, the fifth user device <b>2425</b> may include any number of transducers, such as, but not limited to, microphones, speakers, any type of audio-based transducer, any type of transducer, or a combination thereof. In certain embodiments, the fifth user device <b>2425</b> may be and/or may include a computer, any type of sensor, a laptop, a set-top-box, a tablet device, a phablet, a server, a mobile device, a smartphone, a smart watch, and/or any other type of computing device. Illustratively, the fifth user device <b>2425</b> is shown as a tablet device in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0073" num="0072">The fourth and fifth user devices <b>2421</b>, <b>2425</b> may belong to and/or form a communications network <b>2431</b>. In certain embodiments, the communications network <b>2431</b> may be a local, mesh, or other network that facilitates communications between the fourth and fifth user devices <b>2421</b>, <b>2425</b>, and/or any other devices, programs, and/or networks of system <b>2400</b> or outside system <b>2400</b>. In certain embodiments, the communications network <b>2431</b> may be formed between the fourth and fifth user devices <b>2421</b>, <b>2425</b> through the use of any type of wireless or other protocol and/or technology. For example, the fourth and fifth user devices <b>2421</b>, <b>2425</b> may communicate with one another in the communications network <b>2416</b>, such as by utilizing BLE, classic Bluetooth, ZigBee, cellular, NFC, Wi-Fi, Z-Wave, ANT+, IEEE 802.15.4, IEEE 802.22, ISA100a, infrared, ISM band, RFID, UWB, Wireless HD, Wireless USB, any other protocol and/or wireless technology, satellite, fiber, or any combination thereof. Notably, the communications network <b>2431</b> may be configured to communicatively link with and/or communicate with any other network of the system <b>2400</b> and/or outside the system <b>2400</b>.</p><p id="p-0074" num="0073">Much like first user <b>2401</b>, the second user <b>2420</b> may have his or her own earphone device <b>2430</b>. The earphone device <b>2430</b> may be utilized by the second user <b>2420</b> to hear and/or audition audio content, transmit audio content, receive audio content, experience any type of content, process audio content, adjust audio content, store audio content, perform any type of operation with respect to audio content, or a combination thereof. The earphone device <b>2430</b> may be an earpiece, a hearing aid, an ear monitor, an ear terminal, a behind-the-ear device, any type of acoustic device, or a combination thereof. The earphone device <b>2430</b> may include any type of component utilized for any type of earpiece, and may include any of the features, functionality and/or components described and/or usable with earphone device <b>2415</b>. For example, earphone device <b>2430</b> may include any number of transceivers, ear canal microphones, ambient sound microphones, processors, memories, housings, eartips, foam tips, flanges, any other component, or any combination thereof.</p><p id="p-0075" num="0074">In certain embodiments, the first, second, third, fourth, and/or fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or earphone devices <b>2415</b>, <b>2430</b> may have any number of software applications and/or application services stored and/or accessible thereon. For example, the first and second user devices <b>2402</b>, <b>2411</b> may include applications for processing audio content, applications for playing, editing, transmitting, and/or receiving audio content, streaming media applications, speech-to-text translation applications, cloud-based applications, search engine applications, natural language processing applications, database applications, algorithmic applications, phone-based applications, product-ordering applications, business applications, e-commerce applications, media streaming applications, content-based applications, database applications, gaming applications, internet-based applications, browser applications, mobile applications, service-based applications, productivity applications, video applications, music applications, social media applications, presentation applications, any other type of applications, any types of application services, or a combination thereof. In certain embodiments, the software applications and services may include one or more graphical user interfaces so as to enable the first and second users <b>2401</b>, <b>2420</b> to readily interact with the software applications. The software applications and services may also be utilized by the first and second users <b>2401</b>, <b>2420</b> to interact with any device in the system <b>2400</b>, any network in the system <b>2400</b> (e.g. communications networks <b>2416</b>, <b>2431</b>, <b>2435</b>), or any combination thereof. For example, the software applications executing on the first, second, third, fourth, and/or fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or earphone devices <b>2415</b>, <b>2430</b> may be applications for receiving data, applications for storing data, applications for auditioning, editing, storing and/or processing audio content, applications for receiving demographic and preference information, applications for transforming data, applications for executing mathematical algorithms, applications for generating and transmitting electronic messages, applications for generating and transmitting various types of content, any other type of applications, or a combination thereof. In certain embodiments, the first, second, third, fourth, and/or fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or earphone devices <b>2415</b>, <b>2430</b> may include associated telephone numbers, internet protocol addresses, device identities, or any other identifiers to uniquely identify the first, second, third, fourth, and/or fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or earphone devices <b>2415</b>, <b>2430</b> and/or the first and second users <b>2401</b>, <b>2420</b>. In certain embodiments, location information corresponding to the first, second, third, fourth, and/or fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or earphone devices <b>2415</b>, <b>2430</b> may be obtained based on the internet protocol addresses, by receiving a signal from the first, second, third, fourth, and/or fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or earphone devices <b>2415</b>, <b>2430</b> or based on profile information corresponding to the first, second, third, fourth, and/or fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or earphone devices <b>2415</b>, <b>2430</b>.</p><p id="p-0076" num="0075">The system <b>2400</b> may also include a communications network <b>2435</b>. The communications network <b>2435</b> may be under the control of a service provider, the first and/or second users <b>2401</b>, <b>2420</b>, any other designated user, or a combination thereof. The communications network <b>2435</b> of the system <b>2400</b> may be configured to link each of the devices in the system <b>2400</b> to one another. For example, the communications network <b>2435</b> may be utilized by the first user device <b>2402</b> to connect with other devices within or outside communications network <b>2435</b>. Additionally, the communications network <b>2435</b> may be configured to transmit, generate, and receive any information and data traversing the system <b>2400</b>. In certain embodiments, the communications network <b>2435</b> may include any number of servers, databases, or other componentry. The communications network <b>2435</b> may also include and be connected to a mesh network, a local network, a cloud-computing network, an IMS network, a VoIP network, a security network, a VoLTE network, a wireless network, an Ethernet network, a satellite network, a broadband network, a cellular network, a private network, a cable network, the Internet, an internet protocol network, MPLS network, a content distribution network, any network, or any combination thereof. Illustratively, servers <b>2440</b>, <b>2445</b>, and <b>2450</b> are shown as being included within communications network <b>2435</b>. In certain embodiments, the communications network <b>2435</b> may be part of a single autonomous system that is located in a particular geographic region, or be part of multiple autonomous systems that span several geographic regions.</p><p id="p-0077" num="0076">Notably, the functionality of the system <b>2400</b> may be supported and executed by using any combination of the servers <b>2440</b>, <b>2445</b>, <b>2450</b>, and <b>2460</b>. The servers <b>2440</b>, <b>2445</b>, and <b>2450</b> may reside in communications network <b>2435</b>, however, in certain embodiments, the servers <b>2440</b>, <b>2445</b>, <b>2450</b> may reside outside communications network <b>2435</b>. The servers <b>2440</b>, <b>2445</b>, and <b>2450</b> may provide and serve as a server service that performs the various operations and functions provided by the system <b>2400</b>. In certain embodiments, the server <b>2440</b> may include a memory <b>2441</b> that includes instructions, and a processor <b>2442</b> that executes the instructions from the memory <b>2441</b> to perform various operations that are performed by the server <b>2440</b>. The processor <b>2442</b> may be hardware, software, or a combination thereof. Similarly, the server <b>2445</b> may include a memory <b>2446</b> that includes instructions, and a processor <b>2447</b> that executes the instructions from the memory <b>2446</b> to perform the various operations that are performed by the server <b>2445</b>. Furthermore, the server <b>2450</b> may include a memory <b>2451</b> that includes instructions, and a processor <b>2452</b> that executes the instructions from the memory <b>2451</b> to perform the various operations that are performed by the server <b>2450</b>. In certain embodiments, the servers <b>2440</b>, <b>2445</b>, <b>2450</b>, and <b>2460</b> may be network servers, routers, gateways, switches, media distribution hubs, signal transfer points, service control points, service switching points, firewalls, routers, edge devices, nodes, computers, mobile devices, or any other suitable computing device, or any combination thereof. In certain embodiments, the servers <b>2440</b>, <b>2445</b>, <b>2450</b> may be communicatively linked to the communications network <b>2435</b>, the communications network <b>2416</b>, the communications network <b>2431</b>, any network, any device in the system <b>2400</b>, any program in the system <b>2400</b>, or any combination thereof.</p><p id="p-0078" num="0077">The database <b>2455</b> of the system <b>2400</b> may be utilized to store and relay information that traverses the system <b>2400</b>, cache content that traverses the system <b>2400</b>, store data about each of the devices in the system <b>2400</b> and perform any other typical functions of a database. In certain embodiments, the database <b>2455</b> may be connected to or reside within the communications network <b>2435</b>, the communications network <b>2416</b>, the communications network <b>2431</b>, any other network, or a combination thereof. In certain embodiments, the database <b>2455</b> may serve as a central repository for any information associated with any of the devices and information associated with the system <b>2400</b>. Furthermore, the database <b>2455</b> may include a processor and memory or be connected to a processor and memory to perform the various operation associated with the database <b>2455</b>. In certain embodiments, the database <b>2455</b> may be connected to the earphone devices <b>2415</b>, <b>2430</b>, the servers <b>2440</b>, <b>2445</b>, <b>2450</b>, <b>2460</b>, the first user device <b>2402</b>, the second user device <b>2406</b>, the third user device <b>2410</b>, the fourth user device <b>2421</b>, the fifth user device <b>2425</b>, any devices in the system <b>2400</b>, any other device, any network, or any combination thereof.</p><p id="p-0079" num="0078">The database <b>2455</b> may also store information and metadata obtained from the system <b>2400</b>, store metadata and other information associated with the first and second users <b>2401</b>, <b>2420</b>, store user profiles associated with the first and second users <b>2401</b>, <b>2420</b>, store device profiles associated with any device in the system <b>2400</b>, store communications traversing the system <b>2400</b>, store user preferences, store information associated with any device or signal in the system <b>2400</b>, store information relating to patterns of usage relating to the first, second, third, fourth, and fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b>, store audio content associated with the first, second, third, fourth, and fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or earphone devices <b>2415</b>, <b>2430</b>, store audio content and/or information associated with the audio content that is captured by the ambient sound microphones, store audio content and/or information associated with audio content that is captured by ear canal microphones, store any information obtained from any of the networks in the system <b>2400</b>, store audio content and/or information associated with audio content that is outputted by ear canal receivers of the system <b>2400</b>, store any information and/or signals transmitted and/or received by transceivers of the system <b>2400</b>, store any device and/or capability specifications relating to the earphone devices <b>2415</b>, <b>2430</b>, store historical data associated with the first and second users <b>2401</b>, <b>2415</b>, store information relating to the size (e.g. depth, height, width, curvatures, etc.) and/or shape of the first and/or second user's <b>2401</b>, <b>2420</b> ear canals and/or ears, store information identifying and or describing any eartip utilized with the earphone devices <b>2401</b>, <b>2415</b>, store device characteristics for any of the devices in the system <b>2400</b>, store information relating to any devices associated with the first and second users <b>2401</b>, <b>2420</b>, store any information associated with the earphone devices <b>2415</b>, <b>2430</b>, store log on sequences and/or authentication information for accessing any of the devices of the system <b>2400</b>, store information associated with the communications networks <b>2416</b>, <b>2431</b>, store any information generated and/or processed by the system <b>2400</b>, store any of the information disclosed for any of the operations and functions disclosed for the system <b>2400</b> herewith, store any information traversing the system <b>2400</b>, or any combination thereof. Furthermore, the database <b>2455</b> may be configured to process queries sent to it by any device in the system <b>2400</b>.</p><p id="p-0080" num="0079">The system <b>2400</b> may also include a software application, which may be configured to perform and support the operative functions of the system <b>2400</b>, such as the operative functions of the first, second, third, fourth, and fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or the earphone devices <b>2415</b>, <b>2430</b>. In certain embodiments, the application may be a website, a mobile application, a software application, or a combination thereof, which may be made accessible to users utilizing one or more computing devices, such as the first, second, third, fourth, and fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or the earphone devices <b>2415</b>, <b>2430</b>. The application of the system <b>2400</b> may be accessible via an internet connection established with a browser program or other application executing on the first, second, third, fourth, and fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or the earphone devices <b>2415</b>, <b>2430</b>, a mobile application executing on the first, second, third, fourth, and fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or the earphone devices <b>2415</b>, <b>2430</b>, or through other suitable means. Additionally, the application may allow users and computing devices to create accounts with the application and sign-in to the created accounts with authenticating username and password log-in combinations. The application may include a custom graphical user interface that the first user <b>2401</b> or second user <b>2420</b> may interact with by utilizing a browser executing on the first, second, third, fourth, and fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or the earphone devices <b>2415</b>, <b>2430</b>. In certain embodiments, the software application may execute directly as an installed program on the first, second, third, fourth, and fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or the earphone devices <b>2415</b>, <b>2430</b>.</p><heading id="h-0007" level="2">Computing System for Facilitating the Operation and Functionality of the System</heading><p id="p-0081" num="0080">Referring now also to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, at least a portion of the methodologies and techniques described with respect to the exemplary embodiments of the system <b>2400</b> can incorporate a machine, such as, but not limited to, computer system <b>2500</b>, or other computing device within which a set of instructions, when executed, may cause the machine to perform any one or more of the methodologies or functions discussed above. The machine may be configured to facilitate various operations conducted by the system <b>2400</b>. For example, the machine may be configured to, but is not limited to, assist the system <b>2400</b> by providing processing power to assist with processing loads experienced in the system <b>2400</b>, by providing storage capacity for storing instructions or data traversing the system <b>2400</b>, by providing functionality and/or programs for facilitating the operative functionality of the earphone devices <b>2415</b>, <b>2430</b>, and/or the first, second, third, fourth, and fifth user devices <b>2402</b>, <b>2406</b>, <b>2410</b>, <b>2421</b>, <b>2425</b> and/or the earphone devices <b>2415</b>, <b>2430</b>, by providing functionality and/or programs for facilitating operation of any of the components of the earphone devices <b>2415</b>, <b>2430</b> (e.g. ear canal receivers, transceivers, ear canal microphones, ambient sound microphones, or by assisting with any other operations conducted by or within the system <b>2400</b>.</p><p id="p-0082" num="0081">In some embodiments, the machine may operate as a standalone device. In some embodiments, the machine may be connected (e.g., using communications network <b>2435</b>, the communications network <b>2416</b>, the communications network <b>2431</b>, another network, or a combination thereof) to and assist with operations performed by other machines and systems, such as, but not limited to, the first user device <b>2402</b>, the second user device <b>2411</b>, the third user device <b>2410</b>, the fourth user device <b>2421</b>, the fifth user device <b>2425</b>, the earphone device <b>2415</b>, the earphone device <b>2430</b>, the server <b>2440</b>, the server <b>2450</b>, the database <b>2455</b>, the server <b>2460</b>, or any combination thereof. The machine may be connected with any component in the system <b>2400</b>. In a networked deployment, the machine may operate in the capacity of a server or a client user machine in a server-client user network environment, or as a peer machine in a peer-to-peer (or distributed) network environment. The machine may comprise a server computer, a client user computer, a personal computer (PC), a tablet PC, a laptop computer, a desktop computer, a control system, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine. Further, while a single machine is illustrated, the term &#x201c;machine&#x201d; shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein.</p><p id="p-0083" num="0082">The computer system <b>2500</b> may include a processor <b>2502</b> (e.g., a central processing unit (CPU), a graphics processing unit (GPU, or both), a main memory <b>2504</b> and a static memory <b>2506</b>, which communicate with each other via a bus <b>2508</b>. The computer system <b>2500</b> may further include a video display unit <b>2510</b>, which may be, but is not limited to, a liquid crystal display (LCD), a flat panel, a solid state display, or a cathode ray tube (CRT). The computer system <b>2500</b> may include an input device <b>2512</b>, such as, but not limited to, a keyboard, a cursor control device <b>2514</b>, such as, but not limited to, a mouse, a disk drive unit <b>2516</b>, a signal generation device <b>2518</b>, such as, but not limited to, a speaker or remote control, and a network interface device <b>2520</b>.</p><p id="p-0084" num="0083">The disk drive unit <b>2516</b> may include a machine-readable medium <b>2522</b> on which is stored one or more sets of instructions <b>2524</b>, such as, but not limited to, software embodying any one or more of the methodologies or functions described herein, including those methods illustrated above. The instructions <b>2524</b> may also reside, completely or at least partially, within the main memory <b>2504</b>, the static memory <b>2506</b>, or within the processor <b>2502</b>, or a combination thereof, during execution thereof by the computer system <b>2500</b>. The main memory <b>2504</b> and the processor <b>2502</b> also may constitute machine-readable media.</p><p id="p-0085" num="0084">Dedicated hardware implementations including, but not limited to, application specific integrated circuits, programmable logic arrays and other hardware devices can likewise be constructed to implement the methods described herein. Applications that may include the apparatus and systems of various embodiments broadly include a variety of electronic and computer systems. Some embodiments implement functions in two or more specific interconnected hardware modules or devices with related control and data signals communicated between and through the modules, or as portions of an application-specific integrated circuit. Thus, the example system is applicable to software, firmware, and hardware implementations.</p><p id="p-0086" num="0085">In accordance with various embodiments of the present disclosure, the methods described herein are intended for operation as software programs running on a computer processor. Furthermore, software implementations can include, but not limited to, distributed processing or component/object distributed processing, parallel processing, or virtual machine processing can also be constructed to implement the methods described herein.</p><p id="p-0087" num="0086">The present disclosure contemplates a machine-readable medium <b>2522</b> containing instructions <b>2524</b> so that a device connected to the communications network <b>2435</b>, the communications network <b>2416</b>, the communications network <b>2431</b>, another network, or a combination thereof, can send or receive voice, video or data, and communicate over the communications network <b>2435</b>, the communications network <b>2416</b>, the communications network <b>2431</b>, another network, or a combination thereof, using the instructions. The instructions <b>2524</b> may further be transmitted or received over the communications network <b>2435</b>, another network, or a combination thereof, via the network interface device <b>2520</b>.</p><p id="p-0088" num="0087">While the machine-readable medium <b>2522</b> is shown in an example embodiment to be a single medium, the term &#x201c;machine-readable medium&#x201d; should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions. The term &#x201c;machine-readable medium&#x201d; shall also be taken to include any medium that is capable of storing, encoding or carrying a set of instructions for execution by the machine and that causes the machine to perform any one or more of the methodologies of the present disclosure.</p><p id="p-0089" num="0088">The terms &#x201c;machine-readable medium,&#x201d; &#x201c;machine-readable device,&#x201d; or &#x201c;computer-readable device&#x201d; shall accordingly be taken to include, but not be limited to: memory devices, solid-state memories such as a memory card or other package that houses one or more read-only (non-volatile) memories, random access memories, or other re-writable (volatile) memories; magneto-optical or optical medium such as a disk or tape; or other self-contained information archive or set of archives is considered a distribution medium equivalent to a tangible storage medium. The &#x201c;machine-readable medium,&#x201d; &#x201c;machine-readable device,&#x201d; or &#x201c;computer-readable device&#x201d; may be non-transitory, and, in certain embodiments, may not include a wave or signal per se. Accordingly, the disclosure is considered to include any one or more of a machine-readable medium or a distribution medium, as listed herein and including art-recognized equivalents and successor media, in which the software implementations herein are stored.</p><p id="p-0090" num="0089">The illustrations of arrangements described herein are intended to provide a general understanding of the structure of various embodiments, and they are not intended to serve as a complete description of all the elements and features of apparatus and systems that might make use of the structures described herein. Other arrangements may be utilized and derived therefrom, such that structural and logical substitutions and changes may be made without departing from the scope of this disclosure. Figures are also merely representational and may not be drawn to scale. Certain proportions thereof may be exaggerated, while others may be minimized. Accordingly, the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.</p><p id="p-0091" num="0090">Thus, although specific arrangements have been illustrated and described herein, it should be appreciated that any arrangement calculated to achieve the same purpose may be substituted for the specific arrangement shown. This disclosure is intended to cover any and all adaptations or variations of various embodiments and arrangements of the invention. Combinations of the above arrangements, and other arrangements not specifically described herein, will be apparent to those of skill in the art upon reviewing the above description. Therefore, it is intended that the disclosure not be limited to the particular arrangement(s) disclosed as the best mode contemplated for carrying out this invention, but that the invention will include all embodiments and arrangements falling within the scope of the appended claims.</p><p id="p-0092" num="0091">The foregoing is provided for purposes of illustrating, explaining, and describing embodiments of this invention. Modifications and adaptations to these embodiments will be apparent to those skilled in the art and may be made without departing from the scope or spirit of this invention. Upon reviewing the aforementioned embodiments, it would be evident to an artisan with ordinary skill in the art that said embodiments can be modified, reduced, or enhanced without departing from the scope and spirit of the claims described below.</p><p id="p-0093" num="0092">In at least one embodiment the step of measuring the vocalization of the user with an ear canal microphone and an ambient sound microphone refers to the microphone measuring the acoustic environment to which it is exposed, where the acoustic environment can include the user's voice or another's voice, and where the system <b>2400</b> can be configured to separate the user's voice from another's by comparing the ECM pickup with the ASM. For example, the ECM will primarily pick-up the user's voice whose spectrum can be compared to the ASM pickup spectrum to separate out the user's voice in the ASM pickup from the ambient environment. For example, parsing the temporal signal from the ECM and ASM into blocks, e.g., <b>256</b>, and performing an FFT on the block, then looking at the amplitude and phase.</p><p id="p-0094" num="0093">In at least one embodiment determining whether the user is in a noisy or quiet environment refers to measuring the SPL levels of the acoustic environment sampled by the ECM and ASM, and comparing the SPL levels to NIOSH and EPA standards for noise exposure, for example, a threshold level of 85 dB can be used as a threshold above which can be referred to as noisy, whereas a different lower level can be used to determine quiet, for example levels below 60 dB can be referred to as quiet. Note those these threshold values are non-limiting examples.</p><p id="p-0095" num="0094">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation to encompass all modifications, equivalent structures and functions of the relevant exemplary embodiments. For example, if words such as &#x201c;orthogonal&#x201d;, &#x201c;perpendicular&#x201d; are used, the intended meaning is &#x201c;substantially orthogonal&#x201d; and &#x201c;substantially perpendicular&#x201d; respectively. Additionally, although specific numbers may be quoted in the claims, it is intended that a number close to the one stated is also within the intended scope, i.e. any stated number (e.g., 20 mils) should be interpreted to be &#x201c;about&#x201d; the value of the stated number (e.g., about 20 mils).</p><p id="p-0096" num="0095">Thus, the description of the invention is merely exemplary in nature and, thus, variations that do not depart from the gist of the invention are intended to be within the scope of the exemplary embodiments of the present invention. Such variations are not to be regarded as a departure from the spirit and scope of the present invention.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>.) A method of incorporating environmental acoustic sources into a virtual environment comprising:<claim-text>a first virtual sound source, wherein the first virtual sound source generates a first virtual signal, wherein the first virtual signal is received by a processor, wherein the processor is part of a virtual display system configured to be worn by a user;</claim-text><claim-text>a second virtual sound source signal received by the processor;</claim-text><claim-text>a first environmental sound source, wherein the first environmental sound source generates a first sound signal and a first location, wherein the processor receives the first sound signal and the first location;</claim-text><claim-text>a second environmental sound source, wherein the second environmental sound source generates a second sound signal and a second location, wherein the processor receives the second sound signal and the second location;</claim-text><claim-text>receiving a camera signal from a camera, wherein the camera is part of the virtual display system;</claim-text><claim-text>receiving a virtual visual environment from memory, wherein the memory is operatively connected to the processor;</claim-text><claim-text>converting the first sound signal to a first environmental virtual signal wherein the first environmental virtual signal is generated using the first location and a user location,</claim-text><claim-text>converting the second sound signal to a second environmental virtual signal wherein the second environmental virtual signal is generated using the second location and the user location;</claim-text><claim-text>generating a new virtual visual environment using the camera signal, the virtual visual environment, the first virtual signal, the second virtual signal, the first environmental virtual signal, the second environmental virtual signal, wherein the first environmental virtual signal is configured to appear to originate from a first direction in the new virtual visual environment; wherein the second environmental virtual signal is configured to appear to originate from a second direction in the new virtual visual environment; and</claim-text><claim-text>sending the new virtual visual environment to a display of the virtual display system.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>.) The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the new virtual environment is sent to the display.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>.) The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>analyzing the first sound signal and the second sound signal to detect an alarm.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>.) The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:<claim-text>sending a warning to the user when an alarm is detected.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>.) The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the virtual display system is a VR goggle.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>.) The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the virtual display system is an AR goggle.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>.) The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first environmental virtual signal is sent to a speaker in the virtual display system.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>.) The method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the second environmental virtual signal is sent to the speaker.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>.) The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the speaker is part of an earphone.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>.) The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the earphone includes an ambient sound microphone and an ear canal microphone.</claim-text></claim></claims></us-patent-application>