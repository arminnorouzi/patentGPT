<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005111A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005111</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363054</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>90</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>90</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00805</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10024</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">DETECTING HYBDRID-DISTANCE ADVERSARIAL PATCHES</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>International Business Machines Corporation</orgname><address><city>Armonk</city><state>NY</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Fan</last-name><first-name>Quanfu</first-name><address><city>Lexington</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Liu</last-name><first-name>Sijia</first-name><address><city>Somerville</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Chen</last-name><first-name>Richard</first-name><address><city>Baldwin Place</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Panda</last-name><first-name>Rameswar</first-name><address><city>Medford</city><state>MA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A hybrid-distance adversarial patch generator can be trained to generate a hybrid adversarial patch effective at multiple distances. The hybrid patch can be inserted into multiple sample images, each depicting an object, to simulate inclusion of the hybrid patch at multiple distances. The multiple sample images can then be used to train an object detection model to detect the objects.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="214.71mm" wi="148.17mm" file="US20230005111A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="231.90mm" wi="142.24mm" file="US20230005111A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="231.90mm" wi="150.20mm" file="US20230005111A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="229.19mm" wi="172.47mm" orientation="landscape" file="US20230005111A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="225.98mm" wi="136.57mm" orientation="landscape" file="US20230005111A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="235.12mm" wi="158.67mm" file="US20230005111A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="201.51mm" wi="159.43mm" orientation="landscape" file="US20230005111A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="231.90mm" wi="158.67mm" file="US20230005111A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="236.56mm" wi="172.30mm" file="US20230005111A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">The systems and methods of the present disclosure relate to object detection.</p><p id="p-0003" num="0002">Adversarial patches are an emerging technology designed to deceive automated object detection, particularly machine-learning-based recognition. As an example, a camera observing a space may transmit a video feed to a machine learning model trained to identify people seen by the camera. In response, a person may wear clothing with an adversarial patch (for example, an adversarial patch may be a pattern printed on a T-shirt). To a human observer, an adversarial patch may appear as a mass of seemingly random pixels, not unlike corrupted image data. However, the specific pattern of an adversarial patch, when analyzed by a machine learning model, may throw off results of the machine learning model (e.g., a system attempting to identify a person in an image may fail to do so simply due to the image including an adversarial patch). In this way, adversarial patches can serve as a digital form of camouflage.</p><p id="p-0004" num="0003">While adversarial patches may be useful to prevent unauthorized surveillance, malicious actors may use them to fool helpful object detection models. For example, adversarial patches have been placed on street signs in an apparent attempt to prevent automated vehicle safety systems from being able to recognize the signs.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0005" num="0004">Some embodiments of the present disclosure can be illustrated as a method. The method comprises training a hybrid adversarial patch generator. The method also comprises generating a hybrid adversarial patch via the hybrid adversarial patch generator. The method also comprises inserting the hybrid adversarial patch into a first image, resulting in a first modified image depicting the hybrid adversarial patch at a first distance and a first object. The method also comprises inserting the hybrid adversarial patch into a second image, resulting in a second modified image depicting the hybrid adversarial patch at a second distance and a second object.</p><p id="p-0006" num="0005">Some embodiments of the present disclosure can also be illustrated as a computer program product comprising a computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to perform the method discussed above.</p><p id="p-0007" num="0006">Some embodiments of the present disclosure can be illustrated as a system. The system may comprise memory and a central processing unit (CPU). The CPU may be configured to execute instructions to perform the method discussed above.</p><p id="p-0008" num="0007">The above summary is not intended to describe each illustrated embodiment or every implementation of the present disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008">The drawings included in the present application are incorporated into, and form part of, the specification. They illustrate embodiments of the present disclosure and, along with the description, serve to explain the principles of the disclosure. The drawings are only illustrative of certain embodiments and do not limit the disclosure. Features and advantages of various embodiments of the claimed subject matter will become apparent as the following</p><p id="p-0010" num="0009">Detailed Description proceeds, and upon reference to the drawings, in which like numerals indicate like parts, and in which:</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a high-level method for training a hybrid adversarial patch generator to defeat object detection models, consistent with several embodiments of the present disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a method for training an adversarial patch generator, consistent with several embodiments of the present disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of a system including adversarial patch generators and an object detection model, consistent with several embodiments of the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a diagram of a sample image including a blank patch target, consistent with several embodiments of the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a modified image including an abstract representation of a near adversarial patch at a near distance, consistent with several embodiments of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b>C</figref> is a modified image including an abstract representation of a hybrid adversarial patch at a near distance, consistent with several embodiments of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b>D</figref> is a modified image including an abstract representation of a hybrid adversarial patch at a far distance, consistent with several embodiments of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a detailed method of training a hybrid-distance adversarial patch generator, consistent with several embodiments of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts an example of merging a near-distance patch and a far-distance patch into a hybrid-distance patch, consistent with several embodiments of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a detailed method of training an object detection model to defeat hybrid-distance adversarial patches, consistent with several embodiments of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a high-level block diagram of an example computer system that may be used in implementing embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0022" num="0021">While the invention is amenable to various modifications and alternative forms, specifics thereof have been shown by way of example in the drawings and will be described in detail. It should be understood, however, that the intention is not to limit the invention to the particular embodiments described. On the contrary, the intention is to cover all modifications, equivalents, and alternatives falling within the spirit and scope of the invention.</p><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0023" num="0022">Aspects of the present disclosure relate to systems and methods to train a hybrid adversarial patch generator to defeat object detection models. More particular aspects relate to a system and a method to train a hybrid distance adversarial patch generator, generate a hybrid patch from the generator, modify images to include the hybrid patch, submit the images to an object detection model, receive an output from the model, and revise the patch generator based on the output.</p><p id="p-0024" num="0023">Adversarial patches are an emerging technology designed to deceive automated object detection, particularly deep-learning-based detection. As an example, a camera observing a space may transmit a video feed to a machine learning model trained to identify people seen by the camera. In response, a person may wear clothing with an adversarial patch (for example, an adversarial patch may be a pattern printed on a T-shirt). To a human observer, an adversarial patch may appear as a mass of seemingly random pixels, similar to corrupted image data. However, the specific pattern of an adversarial patch, when analyzed by a machine learning model, may throw off results of the machine learning model (e.g., a system attempting to identify a person in an image may fail to do so simply due to the image including an adversarial patch). In this way, adversarial patches can serve as a digital form of camouflage.</p><p id="p-0025" num="0024">As an illustrative example, a user may capture an image of a dog and a cat. The image may be input to an object detection model, which may detect, based on a trained machine learning model, that the image depicts a dog and a cat. A user may then put an adversarial patch on the dog and capture an otherwise-identical image. The second image may be input to the same object detection model. However, due to the adversarial patch included in the second image, the object detection model may incorrectly determine that the second image only depicts a cat (e.g., the system may fail to detect the dog at all). In other examples, the adversarial patch may cause the system to incorrectly determine that the second image depicts a cat and an avocado (e.g., the system may misidentify the dog as an avocado) or that the second image depicts a sunset (e.g., the system may misidentify both the dog and the cat). Notably, the adversarial patch need not simply be a picture of an avocado (or a blank covering) placed over the dog to obscure it; the patch may appear to the user to be an abstract image with no discernable pattern, and the majority of the identifying features of the dog (e.g., the dog's face, overall shape, markings, etc.) may still be visible in the second image. Nonetheless, the adversarial patch may still be able to disrupt the object detection model simply by being present in the image.</p><p id="p-0026" num="0025">A &#x201c;performance&#x201d; of an adversarial patch describes how consistently the patch is able to disrupt an object detection model (e.g., how much of an impact the patch has on the system's accuracy). Performance can vary wildly between patches, models, and images; an image including a first patch may completely fool a first object detector (e.g., YOLOv3) but be consistently defeated by a second object detector (e.g., RetinaNet). At the same time, an image including a second patch may be consistently defeated by the first system but completely fool the second system.</p><p id="p-0027" num="0026">Adversarial patch performance often varies with how the patch appears in a given image. For example, a person may be wearing a shirt with an adversarial patch printed on the shirt. A picture of the person may be captured, compressed, and input to an object detection model. The appearance of the patch in the compressed picture (i.e., what the detection system &#x201c;sees&#x201d;) may differ significantly from an uncompressed image file of the adversarial patch itself. For example, the captured picture may be poorly lit, resulting in portions of the patch appearing black or &#x201c;washed out.&#x201d; As another example, the person's shirt may be wrinkled, resulting in portions of the patch appearing distorted / shaded slightly differently. Further, the patch may not be directly facing the camera capturing the image, which may result in portions of the patch being omitted entirely from the picture, some portions appearing compressed, etc.</p><p id="p-0028" num="0027">One major impact on patch performance is distance from a camera. For example, two images may be captured of a person wearing a shirt including an adversarial patch: a first image where the person (and, more importantly, the patch) is relatively close to the camera capturing the image (e.g., within 3 meters), and a second image where the person (and patch) are relatively far from the camera (e.g., 9 meters or farther away). The appearance of the patch in the two images will likely be substantially different, as the patch in the second image will appear smaller, and the distance will result in some lost information (e.g., the patch will appear less detailed). Because appearance within an image is a major factor of performance, the patch can be expected to perform differently between the two images.</p><p id="p-0029" num="0028">In practice, adversarial patches are typically generated and checked for effectiveness at deceiving object detection models. A patch generated this way is typically more effective at closer distances (and thus it is referred to as a &#x201c;near&#x201d; patch). This is because features of the patch that enable it to mislead an object detection model (whatever those features may be) become blurred and/or distorted when viewed from longer distances. For example, a patch consisting of a regular grid of black and white squares, when viewed from a sufficient distance, may simply appear as a solid block of gray. The sharp contrast between the black squares and the white squares may be the feature primarily responsible for the patch's effectiveness. Thus, viewing the patch from a great distance can result in the patch losing effectiveness, as the object detection model will not &#x201c;see&#x201d; the grid.</p><p id="p-0030" num="0029">However, patches can be generated specifically to perform well at far distances (when they appear at lower levels of detail in a captured image). Patches that are designed to be effective at far distances (regardless of whether they actually are effective) are referred to as &#x201c;far&#x201d; patches or &#x201c;far distance&#x201d; patches for purposes of this disclosure.</p><p id="p-0031" num="0030">Far patches often have reduced performance at near distances. This is because far patches are generally designed to be viewed at lower levels of detail (e.g., from farther away), so the increased detail included in a near image can result in hindered performance of a far patch. For example, a far patch may include subtle features that might typically hinder the patch's effectiveness, but that are typically lost when the patch is viewed from a far distance.</p><p id="p-0032" num="0031">Notably, while the present disclosure refers extensively to &#x201c;distance,&#x201d; &#x201c;near,&#x201d; &#x201c;far,&#x201d; and the like, this is merely for purposes of illustration; similar effects can be achieved through different variables, particularly specific features of various cameras. For example, a particularly high-quality first camera can capture a first image including a &#x201c;near&#x201d; adversarial patch that is 150 meters from the camera with a high level of detail, while a low-quality second camera may capture a second image including the near patch from 3 meters away with a lower level of detail. As a &#x201c;near&#x201d; patch, the adversarial patch may generally be expected to be more effective at shorter distances (as the loss of detail associated with greater distances can reduce effectiveness). However, in this example, submitting both images to the same object detection model may result in the object detection model failing to identify an object in the first image but successfully identifying the object in the second image, even though the near patch is significantly closer to the camera in the second image. This is because the quality of the first camera results in the object detection model &#x201c;seeing&#x201d; the near patch in the first image with a high level of detail, resulting in the near patch being more effective. In contrast, the quality of the second camera results in the model seeing the near patch in the second image with a low level of detail, resulting in the near patch being less effective. Thus, while distance is generally a good analog for level of detail, other factors can also affect patch performance, such as lens types, motion blur, etc.; in the interest of brevity, &#x201c;distance&#x201d; is the primary descriptor used in this disclosure.</p><p id="p-0033" num="0032">A &#x201c;patch generator&#x201d; may be a machine learning model that can be trained to generate adversarial patches. As adversarial patches are typically most effective within a particular distance range, different generators may be utilized to generate patches effective at different distance ranges. For example, a first patch generator may generate adversarial patches that are effective at relatively short distances (but ineffective at longer distances) if near-view images are used for training, while with far-view training data, a second patch generator may generate adversarial patches that are effective at relatively long distances (but ineffective at shorter distances). Such generators can be referred to as &#x201c;near&#x201d; patch generators and &#x201c;far&#x201d; patch generators, respectively. A third adversarial patch generator, trained with both near-view and far-view data, may achieve a tradeoff near distance performance and far distance performance.</p><p id="p-0034" num="0033">Systems and methods consistent with the present disclosure enable training adversarial patch generators. However, in some instances, the patch generators themselves may be pre-trained or preexisting generators, and the systems described herein may not have access or permissions to modify the generators. Instead, in such instances, systems and methods consistent with the present disclosure may train modifiers to receive a generated patch (from a pretrained generator) and modify the received patch. In the interest of brevity, this is still referred to as &#x201c;training a patch generator,&#x201d;; the pretrained generator and the trained modifier are referred to collectively as a patch generator.</p><p id="p-0035" num="0034">Systems and methods consistent with the present disclosure enable generation of &#x201c;hybrid&#x201d; patches that are effective at multiple distances. In several embodiments, hybrid patches can be generated based on both near and far patches. As a simple example, a hybrid patch may simply be an average or merge of a near patch and a far patch. Hybrid patches may result in reduced performance at a given range than a &#x201c;dedicated&#x201d; near/far patch, but sometimes only slightly. For example, a near patch may have a performance of 95% at near distances and 3% at far distances, while a far patch may have a performance of 13% at near distances and 89% at far distances. A hybrid patch generated based on both the near patch and the far patch may have a performance of 90% at near distances and 82% at far distances.</p><p id="p-0036" num="0035">Hybrid patches can be used to train models to be increasingly robust and capable of defeating them. As used herein, &#x201c;defeating&#x201d; an adversarial patch refers to a system successfully identifying an object the patch is attempting to obfuscate (e.g., identifying a person wearing a shirt with an adversarial patch on it). In some instances, adversarial patches may be specifically detected and disregarded (e.g., removed from an image), allowing a system to detect objects without considering the patch. In other instances, adversarial patches may simply go unnoticed, and the system may detect objects in spite of the patch.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a high-level method <b>100</b> for training an object detection model to defeat hybrid-distance adversarial patches, consistent with several embodiments of the present disclosure. In order to defeat hybrid-distance patches, method <b>100</b> includes training patch generators. As described above, a patch generator may be a machine learning model that can generate adversarial patches. Different generators may be utilized to generate patches effective at different distance ranges. For example, a &#x201c;near&#x201d; patch generator may generate adversarial patches that are effective at relatively short distances (but may be ineffective at longer distances), while a &#x201c;far&#x201d; patch generator may generate adversarial patches that are effective at relatively long distances (but may be ineffective at shorter distances).</p><p id="p-0038" num="0037">Method <b>100</b> comprises training a &#x201c;near&#x201d; adversarial patch generator at operation <b>102</b>. An example of how patch generators may be trained is provided in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, discussed below with reference to method <b>200</b>. In general, operation <b>102</b> includes training a generator to generate near adversarial patches. As an explanatory example, a camera may capture a picture including an adversarial patch and an object. The patch may appear to be close to the camera (for example, the patch may appear to be relatively large and/or may be depicted at a high level of detail) and an object detection model may attempt to identify the object. If the object detection model is unable to detect the object, the patch may be considered an effective &#x201c;near patch.&#x201d; In some instances described in further detail below, a camera may capture a picture including an object and then a patch image modifier may digitally insert the patch into the picture to appear as though the patch is near to the camera, resulting in an image including the object and the patch.</p><p id="p-0039" num="0038">A patch generator can be trained as a machine learning model. For example, an initial iteration of a patch generator may generate patches randomly. The patches may then be tested against one or more object detection models to determine an effectiveness rating for the patch. Randomly generated adversarial patches are generally unlikely to be effective, so adjustments may be made to the generator, after which the generator may be used to generate additional patches. This training process can be repeated until the generator produces patches that satisfy given performance criteria.</p><p id="p-0040" num="0039">Method <b>100</b> further comprises training a &#x201c;far&#x201d; adversarial patch generator at operation <b>104</b>. Operation <b>104</b> may be performed in a similar manner to operation <b>102</b>, except the generator may be trained to generate effective &#x201c;far&#x201d; patches. As an explanatory example, a camera may capture a picture including an adversarial patch and an object. The patch may appear to be far from the camera (for example, the patch may appear to be relatively small and/or may be depicted at a low level of detail) and an object detection model may attempt to identify the object. If the object detection model is unable to identify the object, the patch may be considered an effective &#x201c;far patch.&#x201d; In some instances described in further detail below, a camera may capture a picture including an object and then a patch image modifier may digitally insert the patch into the picture to appear as though the patch is far from the camera, resulting in an image including the object and the patch.</p><p id="p-0041" num="0040">Method <b>100</b> further comprises training a &#x201c;hybrid&#x201d; adversarial patch generator at operation <b>106</b>. An example method <b>500</b> for training a hybrid adversarial patch generator is provided in <figref idref="DRAWINGS">FIG. <b>5</b></figref> and discussed in further detail below. As an overview, operation <b>106</b> may be performed based in part on patches generated by the near and far generators trained at operations <b>102</b> and <b>104</b>, respectively. For example, in some instances, the hybrid adversarial patch generator can be trained to generate patches based on a simple merge of a near patch and a far patch. An example of such a merge is provided in <figref idref="DRAWINGS">FIG. <b>6</b></figref> and discussed in further detail below. In more complex implementations, the hybrid generator may leverage different aspects of near patches and far patches. For instance, a hybrid patch may be generated with an outer portion based upon a far patch but an inner portion based on a near patch.</p><p id="p-0042" num="0041">As an illustrative example, a far patch may be a 30 pixel&#xd7;30 pixel square of a first pattern and a near patch may be 30 pixel&#xd7;30 pixel square of a second pattern; a hybrid patch based on the two may be a 30 pixel&#xd7;30 pixel square with both patterns. For example, a central 15 pixel&#xd7;15 pixel region of the hybrid patch might consist of the second (near) pattern while the surrounding 15 pixel-wide boundary region might consist of the first (far) pattern.</p><p id="p-0043" num="0042">In some instances, each pixel from a near and far patch may be included in the hybrid patch. This means the hybrid patch may be larger than any of the near or far patches. As an example, a 60 pixel&#xd7;60 pixel hybrid patch may be generated based upon four 30 pixel&#xd7;30 pixel patches (such as two 30 pixel&#xd7;30 pixel near patches, N1 and N2, and two 30 pixel&#xd7;30 pixel far patches, F1 and F2). While each of the input near and far patches may be grids of pixels, the hybrid patch may be considered a grid of &#x201c;superpixels,&#x201d; where each superpixel comprises 4 pixels (1 from each input patch). For example, a top left superpixel in the hybrid patch may include the top left pixel of each of the four input patches.</p><p id="p-0044" num="0043">In some instances, a near patch and a far patch may be &#x201c;striped&#x201d; to generate a hybrid patch. For example, if both the near and far patches are squares of equal size, a hybrid patch may also be a square of the same size. The hybrid patch may be comprised of pixels copied from either the near patch or the fear patch. For example, a leftmost column of pixels in the hybrid patch might be copied from a leftmost column of pixels in the near patch, a first-from-the-leftmost column of pixels in the hybrid patch might be copied from a first-from-the-leftmost column of pixels from the far patch, a second-from-the-leftmost column of pixels in the hybrid patch might be copied from a second-from-the-leftmost column of pixels in the near patch, and so on. Thus, columns of the hybrid patch may be copied from the two patches in an alternating fashion. Of course, this is not limited to vertical stripes (i.e., columns); rows can be striped, striping can be performed diagonally, etc.</p><p id="p-0045" num="0044">Pixels may be described in terms of &#x201c;relative position&#x201d; in multiple different images. For example, a first relative position may be a top-left position (e.g., in a top row and a leftmost column). To illustrate, a first image may have a blue pixel in a top-left position, while a second image may have a red pixel in a top-left position. This can be described as a blue pixel in a first relative position in the first image and a red pixel in the first relative position in the second image. This &#x201c;relative location&#x201d; terminology can also refer to patches (e.g., a near patch may have a blue pixel in a second relative location while a far patch may have a red pixel in the second relative location).</p><p id="p-0046" num="0045">In some instances, a hybrid patch generator may be trained by minimizing a total loss L on the training data. Loss L is a function of a hybrid patch P, a near patch P<sub>N</sub>, a far patch P<sub>F</sub>, and a set of images I. Loss L can be evaluated as a sum of component losses such as an attack loss L<sub>attack</sub>, a knowledge distillation loss L<sub>distill</sub>, and a sum of a total variation L<sub>tv</sub>. This is represented in Equation 1, below:</p><p id="p-0047" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>L</i>(<i>P, P</i><sub>N</sub><i>, P</i><sub>F</sub><i>, I</i>)=<i>L</i><sub>attack</sub>(<i>P, P</i><sub>N</sub><i>, P</i><sub>F</sub><i>, I</i>)+<i>L</i><sub>distill</sub>(<i>P, P</i><sub>N</sub><i>, P</i><sub>F</sub><i>, I</i>)+<i>L</i><sub>tv</sub>(<i>P, P</i><sub>N</sub><i>, P</i><sub>F</sub><i>, I</i>) &#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0048" num="0046">The attack loss L<sub>attack </sub>may represent a chance or rate at which an adversarial patch is defeated by an object detection model over a series of tests. For example, L<sub>attack</sub>=0 may indicate that the object detection model successfully identified an object in spite of the patch in 0% of tests (e.g., the patch consistently defeated the model), while L<sub>attack</sub>=1 may indicate that the object detection model successfully identified the object in spite of the patch in 100% of tests (e.g., the model consistently defeated the patch). The total attack loss utilized in Equation 1 is a sum of component attack losses for each patch, as shown in Equation 2:</p><p id="p-0049" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>L</i><sub>attack</sub>(<i>P, P</i><sub>N</sub><i>, P</i><sub>F</sub><i>, l</i>)=<i>L</i><sub>attack</sub>(<i>P, I</i>)+<i>L</i><sub>attack</sub>(<i>P</i><sub>N</sub><i>, I</i>)+<i>L</i><sub>attack</sub>(<i>P</i><sub>F</sub><i>, I</i>) &#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0050" num="0047">The near and far attack losses (P<sub>N </sub>and P<sub>F</sub>, respectively) can be determined via a sum of weighted losses l for each image f of the set I. The individual losses l can be weighted according to a distance weight &#x3b3;, as shown in Equations 3 and 4:</p><p id="p-0051" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>L</mi>       <mi>attack</mi>      </msub>      <mo>(</mo>      <mrow>       <msub>        <mi>P</mi>        <mi>N</mi>       </msub>       <mo>,</mo>       <mi>I</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mfrac>       <mn>1</mn>       <mrow>        <mo>&#xf605;</mo>        <mi>I</mi>        <mo>&#xf606;</mo>       </mrow>      </mfrac>      <mo>&#x2062;</mo>      <mrow>       <munder>        <mo>&#x2211;</mo>        <mrow>         <msub>          <mi>f</mi>          <mi>i</mi>         </msub>         <mo>&#x2208;</mo>         <mi>I</mi>        </mrow>       </munder>       <mrow>        <msub>         <mi>&#x3b3;</mi>         <mi>i</mi>        </msub>        <mo>&#x2062;</mo>        <mrow>         <mi>l</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>P</mi>          <mo>,</mo>          <msub>           <mi>P</mi>           <mi>N</mi>          </msub>          <mo>,</mo>          <msub>           <mi>P</mi>           <mi>F</mi>          </msub>          <mo>,</mo>          <msub>           <mi>f</mi>           <mi>j</mi>          </msub>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>L</mi>       <mi>attack</mi>      </msub>      <mo>&#x2062;</mo>      <mrow>       <mo>(</mo>       <mrow>        <msub>         <mi>P</mi>         <mi>F</mi>        </msub>        <mo>,</mo>        <mi>I</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>     <mo>=</mo>     <mrow>      <mfrac>       <mn>1</mn>       <mrow>        <mo>&#xf605;</mo>        <mi>I</mi>        <mo>&#xf606;</mo>       </mrow>      </mfrac>      <mo>&#x2062;</mo>      <mrow>       <munder>        <mo>&#x2211;</mo>        <mrow>         <msub>          <mi>f</mi>          <mi>i</mi>         </msub>         <mo>&#x2208;</mo>         <mi>I</mi>        </mrow>       </munder>       <mrow>        <mrow>         <mo>(</mo>         <mrow>          <mn>1</mn>          <mo>-</mo>          <msub>           <mi>&#x3b3;</mi>           <mi>i</mi>          </msub>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#x2062;</mo>        <mrow>         <mi>l</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>P</mi>          <mo>,</mo>          <msub>           <mi>P</mi>           <mi>N</mi>          </msub>          <mo>,</mo>          <msub>           <mi>P</mi>           <mi>F</mi>          </msub>          <mo>,</mo>          <msub>           <mi>f</mi>           <mi>j</mi>          </msub>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0052" num="0048">Distance weight &#x3b3; can enable control over whether patches are weighted to target near or far performances.</p><p id="p-0053" num="0049">The knowledge distillation loss Limn is defined according to Equation 5, below:</p><p id="p-0054" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>L</mi>      <mi>distill</mi>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <msub>        <mi>&#x3bb;</mi>        <mi>N</mi>       </msub>       <mo>&#x2062;</mo>       <mfrac>        <mn>1</mn>        <mrow>         <mo>&#xf605;</mo>         <mi>I</mi>         <mo>&#xf606;</mo>        </mrow>       </mfrac>       <mo>&#x2062;</mo>       <mrow>        <munder>         <mo>&#x2211;</mo>         <mrow>          <msub>           <mi>f</mi>           <mi>i</mi>          </msub>          <mo>&#x2208;</mo>          <mi>I</mi>         </mrow>        </munder>        <mrow>         <mrow>          <mo>(</mo>          <mrow>           <mn>1</mn>           <mo>-</mo>           <msub>            <mi>&#x3b3;</mi>            <mi>j</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>         <mo>&#x2062;</mo>         <mrow>          <msub>           <mi>cl</mi>           <mi>j</mi>          </msub>          <mo>(</mo>          <mrow>           <mi>P</mi>           <mo>,</mo>           <msub>            <mi>P</mi>            <mi>N</mi>           </msub>           <mo>,</mo>           <msub>            <mi>P</mi>            <mi>F</mi>           </msub>           <mo>,</mo>           <msub>            <mi>f</mi>            <mi>j</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </mrow>      </mrow>      <mo>+</mo>      <mrow>       <msub>        <mi>&#x3bb;</mi>        <mi>F</mi>       </msub>       <mo>&#x2062;</mo>       <mfrac>        <mn>1</mn>        <mrow>         <mo>&#xf605;</mo>         <mi>T</mi>         <mo>&#xf606;</mo>        </mrow>       </mfrac>       <mo>&#x2062;</mo>       <mrow>        <munder>         <mo>&#x2211;</mo>         <mrow>          <msub>           <mi>f</mi>           <mi>j</mi>          </msub>          <mo>&#x2208;</mo>          <mi>I</mi>         </mrow>        </munder>        <mrow>         <msub>          <mi>&#x3b3;</mi>          <mi>j</mi>         </msub>         <mo>&#x2062;</mo>         <mi>c</mi>         <mo>&#x2062;</mo>         <mrow>          <msub>           <mi>l</mi>           <mi>j</mi>          </msub>          <mo>(</mo>          <mrow>           <mi>P</mi>           <mo>,</mo>           <msub>            <mi>P</mi>            <mi>N</mi>           </msub>           <mo>,</mo>           <msub>            <mi>P</mi>            <mi>F</mi>           </msub>           <mo>,</mo>           <msub>            <mi>f</mi>            <mi>j</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0055" num="0050">Where &#x3bb;<sub>N </sub>and &#x3bb;<sub>F </sub>are teaching weights enabling preference for the near patch or the far patch in the knowledge distillation process, respectively. cl<sub>j </sub>is a &#x201c;contrast loss,&#x201d; which describes a similarity between the hybrid patch P and the near patch P<sub>N</sub>, a far patch P<sub>F</sub>, and a j<sup>th </sup>image f<sub>j </sub>of a set of images I. clj is defined according to Equation 6, below:</p><p id="p-0056" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>cl</i><sub>j</sub>(<i>P, P</i><sub>+</sub><i>, P</i><sub>&#x2212;</sub><i>, f</i><sub>j</sub>)=max(|<i>l</i>(<i>P, f</i><sub>j</sub>)&#x2212;<i>l</i>(<i>P</i><sub>+</sub><i>, f</i><sub>j</sub>)|&#x2212;|<i>l</i>(<i>P, f</i><sub>j</sub>)&#x2212;<i>l</i>(<i>P</i><sub>&#x2212;</sub><i>, f</i><sub>j</sub>)|+<i>m</i><sub>s</sub>, 0) &#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0057" num="0051">Where m<sub>s </sub>functions as a buffer value to prevent penalties that may otherwise result from minor discrepancies.</p><p id="p-0058" num="0052">The total variation L<sub>tv </sub>essentially represents a level of smoothness/continuity in the patch. For example, a patch of completely random pixels may be more likely to have a relatively high L<sub>tv</sub>, while a patch where colors are more localized into regions of pixels, having smoother transitions, may have a relatively low L<sub>tv</sub>. Patches with relatively high L<sub>tv </sub>typically have a more noisy/discontinuous appearance. Adversarial patches with higher L<sub>tv </sub>may be more readily apparent, making them easier to detect (and therefore defeat), as well as being generally conspicuous, which a user of the adversarial patch may wish to avoid. As reducing L<sub>tv </sub>may not have a significant impact on patch effectiveness, in some embodiments, patches with lower L<sub>tv </sub>may be preferred. However, in other embodiments, the L<sub>tv </sub>term may be omitted from the calculation of the total loss L.</p><p id="p-0059" num="0053">Minimizing the total loss L can result in a patch that is effective at both near distances and far distances, and control of weights &#x3b3; and &#x3bb; can enable a developer to prioritize towards a given distance or teacher, respectively.</p><p id="p-0060" num="0054">In some instances, the hybrid patch generator may be trained based upon the near and far generators themselves, rather than (or in addition to) the patches they generate. Further, different features can be utilized from different generators. As an illustrative example, near patch generators and far patch generators may tend to generate patches with different color trends; a far patch generator may generate patches with minimal amounts of red (e.g., red-green-blue (RGB) values of the pixels in far patches may typically have an &#x201c;R&#x201d; value below 50 out of an example maximum of 256), while a near patch generator may generate adversarial patches with sharply contrasting colors (e.g., a first group of pixels in a near patch may be predominantly blue, and a second group of pixels in the near patch adjacent to the first group of pixels may be predominantly red with minimal blue). A hybrid patch generator may combine these trends to generate patches with minimal amounts of red but otherwise sharply contrasting colors (e.g., a first group of pixels in the hybrid patch may be predominantly blue, and a second group of pixels in the hybrid patch adjacent to the first group of pixels may be predominantly green with minimal blue, but few or no pixels in the hybrid patch have an &#x201c;R&#x201d; value above 50). As a different example, a near patch generator may utilize regions of sharply contrasting colors while a far patch generator may utilize triangular regions of varying size; a hybrid patch generator may combine these styles and generate patches with triangular regions of sharply contrasting colors.</p><p id="p-0061" num="0055">In some instances, generation of hybrid patches can be weighted to &#x201c;favor&#x201d; a near patch or a far patch. As an example, a weight &#x201c;k&#x201d; may be used to describe a level of preference for near patches. A simple pixel-by-pixel average of a near patch and a far patch may constitute averaging an RGB value of a given pixel (e.g., top-left) in the near patch and an RGB value of a corresponding pixel (e.g., also top-left) in the far patch, resulting in an averaged RGB value utilized for the corresponding pixel in the hybrid patch. Without any weighting, the R (red) value for the hybrid patch (R_Hybrid) may be acquired by averaging the red value for the near patch (R_Near) and the red value for the far patch (R_Far), i.e., (R_Near+R_Far)/2. A weight &#x201c;k&#x201d; may be introduced to bias the merge in favor of the near or far patch; for example, R_Hybrid=(k*R_Near+R_Far)/(1+k). Thus, with k=1, the merging has no preference, but with k=7, R_Hybrid=(7*R_Near+R_Far)/8, strongly weighting the color of the hybrid pixel in favor of the color of the corresponding pixel of the near patch. In addition, 0&#x3c;k&#x3c;1 results in favoring the far patch; k=0 results in disregarding the near patch entirely.</p><p id="p-0062" num="0056">Weights can be implemented in other ways as well. For example, a &#x201c;striping&#x201d; of a near and far patch can be weighted such that hybrid pixels copied from a near patch are averaged with their corresponding pixels in a far patch, resulting in some hybrid pixels that are direct copies of far patch pixels and the remaining hybrid pixels being an average of far patch and near patch pixels (thus weighting the generation of the patch in favor of the far patch).</p><p id="p-0063" num="0057">Features of patch generators can also be weighted. For example, a near patch generator may utilize circular regions of sharply contrasting colors while a far patch generator may utilize triangular regions of predominantly blue colors. A hybrid patch generator combining these styles might utilize shapes more closely resembling triangles or circles, depending upon weight. Additionally or alternatively, the hybrid patch generator may utilize a mixture of circles and triangles, with a number of each depending upon weight.</p><p id="p-0064" num="0058">In some instances, the hybrid adversarial patch generator can be utilized to train an object detection model to defeat hybrid adversarial patches at multiple distances. A more detailed example of training such a model is provided as method <b>700</b>, discussed in further detail below with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. As an overview, this training may include acquiring at least two images including the same hybrid patch at different distances, determining whether the system can defeat the patch in both cases, and revising the system until it can.</p><p id="p-0065" num="0059">For example, systems and methods consistent with the present disclosure may receive a patch generated by a hybrid adversarial patch generator trained via operation <b>106</b>. An example system may then digitally alter a sample image by inserting the hybrid patch at a first distance into the sample image. A &#x201c;sample image&#x201d; may be an image captured by a camera depicting an object for an object detection model to classify. For example, the sample image may depict a first person standing 3 m away from the camera and a second person standing 15 m away from the camera. Both people may be wearing similar (or identical) shirts. The sample image may be submitted to an object detection model, which may identify that the image depicts two people.</p><p id="p-0066" num="0060">The system may digitally alter the sample image to appear as though the first person's shirt has the hybrid patch on it; this altered image is referred to as a first (near) test image. The system may further digitally alter the sample image to appear as though the second person's shirt has the hybrid patch on it, resulting in a second (far) test image.</p><p id="p-0067" num="0061">The test images may be submitted to an object detection model, which may output a result. If the object detection model successfully identified both people in both images, then the model has successfully defeated the hybrid patch. If the object detection model misidentified one or both people in either (or both) image(s), then the model may have failed to defeat the hybrid patch. If the model failed, the model can be revised (e.g., features/layers can be modified), and the test can be run again to determine if the revision was helpful. In some instances, the model may be revised until the most recent revision results in decreased performance (relative to the preceding revision).</p><p id="p-0068" num="0062"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a method <b>200</b> for training an adversarial patch generator, consistent with several embodiments of the present disclosure. Method <b>200</b> comprises receiving a sample image at operation <b>202</b>. As discussed above, a sample image may depict an object in order to evaluate performance of an object detection model and/or an adversarial patch. Operation <b>202</b> may include, for example, receiving an image from a database (such as database <b>320</b>, discussed below with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In some instances, the sample image received at operation <b>202</b> may include a marking to designate where an adversarial patch may be inserted. For example, the sample image may be an image of a user of a system performing method <b>100</b>, and the user may be wearing a shirt including scan targets designating boundaries of a patch region. In some instances, the sample image may include metadata tags designating where a patch should be inserted (for example, a set of four pixels defining a quadrilateral region in the image). While sometimes useful, such indicators may not be required; the sample image received at operation <b>202</b> may, for example, be a stock image or publicly available image without any metadata tags. In some instances, metadata tags may be manually added (for example, a user might designate areas in the image suitable for tags).</p><p id="p-0069" num="0063">Many examples described throughout the present disclosure refer to an image of a person wearing a shirt with an adversarial patch either depicted on the shirt or inserted into the image to appear on the shirt. However, many other examples simulating other use cases for adversarial patches are also considered. For example, adversarial patches may be tested or depicted on road signs, animals, bodies of text such as license plates, etc.</p><p id="p-0070" num="0064">Method <b>200</b> further comprises generating an adversarial patch via an adversarial patch generator at operation <b>204</b>. In general, operation <b>204</b> may include selecting a size of the patch and adding features to a &#x201c;template&#x201d; to generate a patch. For example, operation <b>204</b> may include selecting a 600&#xd7;600 pixel patch (based on a default value or user input). A trained patch generator might add specific features to a particular template in order to yield an effective adversarial patch; for example, a trained patch generator might add curves and edges according to a particular pattern, resulting in modifying the template image into the adversarial patch. However, in some instances, the adversarial patch generator may initially be untrained. For example, operation <b>204</b> may initially include generating a set of random pixels (for example, each pixel may have randomly determined red/green/blue (RGB) values) for use as a template. In other instances, the template may initially simply be a blank (i.e., white) image. Further, an untrained patch generator may modify the template in a random manner. For example, an artificial neural network (ANN) may be utilized, including various layers, connections, and weights; an untrained ANN may initially have random or default weights, which may be adjusted over time. In some instances, operation <b>204</b> may utilize an adversarial patch that is known to be effective (a &#x201c;known-good&#x201d; patch) as a template for generating a new patch.</p><p id="p-0071" num="0065">Operation <b>204</b> may include generating a patch based on a given distance. For example, operation <b>204</b> may include generating a patch designed to be effective within 5 meters (which may be considered a &#x201c;near&#x201d; patch/distance in some instances). In some instances, operation <b>204</b> may include generating a patch designed to be effective at 15 meters (considered a &#x201c;far&#x201d; patch/distance).</p><p id="p-0072" num="0066">Over time, generation at operation <b>204</b> may be refined. For example, certain features (such as concentrations of curves/edges, color gradients, etc.) may be learned to be particularly useful in fooling classifiers. Thus, even if initial iterations of operation <b>204</b> may be mostly random, in some instances operation <b>204</b> may include modifying a template based upon learned features to generate an effective adversarial patch. These refinements are often range-specific. For example, a first assortment of curves might be useful for deceiving detection models at near distances but useless at far distances. Therefore, modifying the patch generator to include the first assortment of curves in output patches might be useful for a near patch generator but unhelpful for a far patch generator. In view of this, the features implemented at operation <b>204</b> may be vary depending upon whether the generated patch is intended to be effective at a near distance or a far distance.</p><p id="p-0073" num="0067">Method <b>200</b> further comprises modifying, at operation <b>206</b>, the sample image acquired via operation <b>202</b> to insert the adversarial patch generated via operation <b>204</b> in the image. The modifications of operation <b>206</b> may result in an edited image which may appear as if the adversarial patch were a part of the sample image. For example, a sample image may depict a person wearing a shirt with a blank patch on it. Operation <b>206</b> may include editing the sample image to look as if the person's shirt had the adversarial patch on it rather than the blank patch. For example, operation <b>206</b> may include identifying a size and shape of the blank patch and modifying the adversarial patch to match. As an example, an adversarial patch may be a square having sides <b>600</b> pixels in length. Operation <b>206</b> may include detecting a blank patch on a person's shirt, identifying that the blank patch is in the shape of a parallelogram having sides that are 50 pixels in length. Operation <b>206</b> may further include converting the adversarial patch from a square shape into the parallelogram shape, and scaling the size by 50/600. The resulting modified adversarial patch may then be overlaid over the blank patch in the image, resulting in the adversarial patch appearing as though the image were captured of a person wearing a shirt bearing the adversarial patch. In some instances, operation <b>206</b> may account for lighting, a disconnect between digital colors of the generated patch and printed colors that might be produced by the manufacture of the shirt depicted in the image, etc. when modifying the image.</p><p id="p-0074" num="0068">Method <b>200</b> further comprises evaluating object detection model performance at operation <b>208</b>. Operation <b>208</b> may include, for example, submitting the modified image as input to an object detection model and receiving an output. For example, an object detection model such as YOLOv3 may receive images as input, analyze the image in an attempt to detect (and, in some instances, identify) objects depicted in the image, and output results of the analysis including identification of objects detected in the image. In some instances, the output may include a classification and a confidence value describing how confident the model is in its classification. For example, the model may output &#x201c;98% dog&#x201d; indicating that the model is 98% confident that the image depicts a dog. Operation <b>208</b> may include evaluating an accuracy of the output of the system based on annotations of the modified image. As an example, operation <b>208</b> may include determining whether an object detection model is able to correctly identify that a person wearing the patch in the modified image was a person.</p><p id="p-0075" num="0069">For example, a sample image may depict a person wearing a shirt. The sample image may be known to depict the person (either via metadata tags or via a user manually identifying the person). The sample image may be modified to include the adversarial patch on the person's shirt, and the modified image may be submitted to an object detection model. The object detection model may output any identified objects (including people). If the system determines that no people are present in an image that is known to include a person, the accuracy of the output may be evaluated as 0%. If the system determines that three people are present in an image that is known to include four people, the accuracy of the output may be 75%. If the system determines that four people are present in an image that is known to include two people, the accuracy of the output may be 50%, etc.</p><p id="p-0076" num="0070">The object detection model utilized at operation <b>208</b> may be an existing, pretrained system (e.g., YOLOv3) or can be a newly-trained system. In general, an accurate system is preferable, as an inaccurate system may erroneously result in the generator being designated as effective.</p><p id="p-0077" num="0071">Inclusion of an adversarial patch in an image may negatively impact accuracy of an object detection model. Such an impact can be described as a performance of the patch. Patches can be associated with one or more performance criteria, describing how effective the patch is at disrupting performance of object detection models.</p><p id="p-0078" num="0072">Method <b>200</b> further comprises determining whether performance criteria of the patch are met at operation <b>210</b>. Operation <b>210</b> may include, for example, comparing the performance identified at operation <b>208</b> to one or more thresholds, such as an accuracy threshold. For example, an accuracy threshold of 15% or lower may be a performance criterion; if the system is unable to defeat the patch at least 85% of the time, the patch's performance criteria may be met.</p><p id="p-0079" num="0073">If the patch's performance criteria are not met (<b>210</b> &#x201c;No&#x201d;), method <b>200</b> further comprises updating, at operation <b>212</b>, the patch generator based on the performance. Operation <b>212</b> may include adjusting one or more weights and/or connections in a machine learning model utilized in the patch generator. The adjustments made at operation <b>212</b> may be based on impact of previous adjustments; for example, if a first weight is increased and the patch's performance decreases, then operation <b>212</b> may include decreasing the first weight. In some instances, adjustments made at operation <b>212</b> may be random. In some instances, changes that are evaluated to result in more significant impacts on performance may be focused on. For example, a patch generator may be an artificial neural network (ANN) having a number of interconnected nodes, with each connection having a given weight. Changes to the generator may comprise changes to the weights of the connections. For example, a weight may be increased or decreased, and a resulting impact on generated patch performance can be evaluated.</p><p id="p-0080" num="0074">Once updates are made, method <b>200</b> loops back to operation <b>204</b>, generating a new patch from the updated patch generator. Operations <b>206</b>-<b>210</b> may then be repeated. As stated above, impacts of the adjustments made at operation <b>212</b> may be evaluated at subsequent iterations. For example, once a weight is increased or decreased, the resulting impact can be evaluated by generating a new patch via the revised generator, inserting the new patch into a sample image, submitting the new modified image to an object detection model, and comparing an accuracy of the object detection model to that previously evaluated at operation <b>208</b>. If increasing a first weight results in a performance improvement, further increases to the first weight may be tested.</p><p id="p-0081" num="0075">Once the patch's performance criteria are met (<b>210</b> &#x201c;Yes&#x201d;), method <b>200</b> proceeds to end at operation <b>214</b>. In some instances, performance criteria may require several successful iterations before the generator is considered fully trained. As an example, a single patch in a single modified image may successfully deceive an object detection model. However, in order to ensure that this is not a coincidence/fluke, the same patch may be tested using other sample images.</p><p id="p-0082" num="0076">In some instances, trained patch generators may include some degree of randomization when generating patches; in other words, the same generator may be able to generate multiple different adversarial patches designed to perform the same task (e.g., hinder performance of an object detection model). Training a patch generator to generate unique adversarial patches that are all consistently able to deceive object detection models may yield a more expansive training set, useful for training an object detection model to defeat adversarial patches. Thus. In some instances, in order to ensure that a single successful iteration is not a fluke, the patch generator may generate additional patches, to be used in additional images, to verify that the patch generator is consistently able to generate patches that can deceive detection systems.</p><p id="p-0083" num="0077">As different object detection models may perform differently, in some instances, method <b>200</b> may be performed using several different models to attempt to train a patch generator to generate adversarial patches capable of deceiving all available models. However, this may substantially increase training time and resource requirements, and there is no guarantee that a single patch generator may be able to generate patches that consistently deceive all object detection models. Thus, in some instances, method <b>200</b> may be repeated with different models, resulting in multiple patch generators. For example, method <b>200</b> may be performed with a first object detection model, resulting in a first adversarial patch generator capable of consistently generating adversarial patches that can fool the first object detection model. Method <b>200</b> may then be repeated for a second object detection model, resulting in a second adversarial patch generator (different from the first generator) capable of consistently generating adversarial patches that can fool the second object detection model.</p><p id="p-0084" num="0078">As noted, the generator trained via method <b>200</b> may be a near patch generator or a far patch generator. Whether the generator is a near patch generator or a far patch generator can be controlled at operations <b>202</b> and <b>206</b> (i.e., the operations involving the image). For example, in order to train a near patch generator, operation <b>202</b> may include selecting a sample image depicting a person close to a camera capturing the image. Then, after inserting the adversarial patch into the image at operation <b>206</b>, the resulting modified image may simulate the adversarial patch relatively near to the camera. In contrast, in order to train a far patch generator, operation <b>202</b> may include selecting a sample image depicting a person far from a camera capturing the image. Then, after inserting the adversarial patch into the image at operation <b>206</b>, the resulting modified image may simulate the adversarial patch relatively near to the camera. In some instances, the sample image can be further modified to adjust the apparent distance of the patch/object. This is described in further detail below with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0085" num="0079">Therefore, in some instances, multiple different generators can be trained for each object detection model. For example, method <b>200</b> can be performed a first time to train a first near generator using a first object detection model. Method <b>200</b> can be performed a second time to train a first far generator (distinct from the first near generator) using the same first object detection model, and so on.</p><p id="p-0086" num="0080"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of a system <b>300</b> including adversarial patch generators and an object detection model, consistent with several embodiments of the present disclosure.</p><p id="p-0087" num="0081">System <b>300</b> includes a set of adversarial patch generators <b>302</b>. Patch generators <b>302</b> include near patch generator <b>304</b>, far patch generator <b>308</b>, and hybrid patch generator <b>306</b>. Patch generators <b>302</b> may be stored in memory (not shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>). Each of patch generators <b>302</b> may initially be untrained (e.g., outputting random pixels). However, patch generators <b>302</b> can be trained over time to output adversarial patches that can be effective against various object detection models at various distances.</p><p id="p-0088" num="0082">Near patch generator <b>304</b> may be trained to generate adversarial patches that fool detection systems at close distances (e.g., &#x3c;10 m, &#x3c;5 m, etc.). Far patch generator <b>308</b> may be similar to <b>304</b>, but trained to generate patches that fool systems at far distances (e.g., &#x3e;=10 m, &#x3e;=5 m, etc.).</p><p id="p-0089" num="0083">Hybrid patch generator <b>306</b> may be trained to learn from both near patch generator <b>304</b> and far patch generator <b>308</b> to generate patches that have good performance at both close and far distances (&#x201c;hybrid patches&#x201d;). In some instances, hybrid patch generator <b>306</b> may simply merge patches generated by generators <b>304</b> and <b>308</b>. For example, each pixel of a hybrid patch may be generated based on an average of red-green-blue (RGB) values of both a near patch's corresponding pixel and a far patch's corresponding pixel. In other instances, hybrid patch generator <b>306</b> may utilize more complex algorithms such as, for example, striping, weighted merging, etc., as described in further detail below with reference to method <b>500</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0090" num="0084">System <b>300</b> further includes patch image modifier <b>310</b>. Image modifier <b>310</b> is configured to generate training images. Each training image may depict an object and a patch generated from one of generators <b>302</b>. The training images may be generated based in part on sample images received from sample image database <b>320</b>.</p><p id="p-0091" num="0085">Sample image database <b>320</b> is a database including multiple images such as image <b>321</b>, image <b>322</b>, and image <b>323</b>. Database <b>320</b> may be stored locally or online (e.g., cloud-based). As an example, sample image <b>321</b> may be an image depicting a user wearing a shirt having a blank target designating a spot for an adversarial patch. Sample image <b>322</b> may be a different image of the same user wearing the same shirt (for example, the different image may have been captured at a different location, from a different distance, under different lighting conditions, etc.). Sample image <b>323</b> may, for example, be an image of a different user wearing a different shirt, etc.</p><p id="p-0092" num="0086">Patch image modifier <b>310</b> may acquire a patch from near patch generator <b>304</b> as well as sample image <b>321</b>. Patch image modifier may then edit sample image <b>321</b> to include a depiction of the patch received from generator <b>304</b>, resulting in a modified image depicting the user wearing the shirt with the adversarial patch on the shirt. Examples of sample and modified images are provided in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, as discussed in further detail below.</p><p id="p-0093" num="0087">Patch image modifier <b>310</b> advantageously enables usage of multiple training images without needing to, for example, print a patch onto a shirt and capture a new image for every patch generated from one of generators <b>302</b>. As an illustrative example, without patch image modifier <b>310</b> to generate test images by inserting patches into sample images, method <b>200</b>'s operation <b>206</b> might require creating a test image, which would require a physical example of the adversarial patch generated at operation <b>204</b>. As an example, a user might acquire a shirt, print out an adversarial patch to be evaluated, affix the patch to the shirt, and capture a picture of a person wearing the shirt. A second test image might require printing out an additional patch and affixing it to the shirt (instead of or on top of the previous patch), and capturing a new picture, etc. In contrast, patch image modifier can circumvent this process by acquiring a sample image and digitally modifying the sample image into a test image. In this way, patch image modifier <b>310</b> can result in significant time and effort savings.</p><p id="p-0094" num="0088">System <b>300</b> further comprises object detection model <b>312</b>. Object detection model <b>312</b> may be trained to detect and/or classify objects, people, etc., though various adversarial patches may be able to deceive it. Object detection model <b>312</b> may be one of several existing object detection models such as, for example, YOLOv3. In some instances, object detection model <b>312</b> may be a newly-created model that is being trained to defeat patches generated by generators <b>302</b>. Access to many existing object detection models may be limited as they may be closed-source and/or proprietary.</p><p id="p-0095" num="0089">As modified images including patches generated by generators <b>302</b> may be input to object detection model <b>312</b>, system <b>312</b> may produce outputs including classifications of the images. These outputs can be compared to known values and used to update generators <b>302</b>.</p><p id="p-0096" num="0090"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram <b>400</b> of a sample image <b>410</b> as well as several modified images including adversarial patches, consistent with several embodiments of the present disclosure.</p><p id="p-0097" num="0091">Sample image <b>410</b> may be an example of image <b>321</b> of database <b>320</b>, as discussed above with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Sample image <b>410</b> includes person <b>412</b> and a building <b>418</b>. Person <b>412</b> is wearing a shirt <b>414</b>. Shirt <b>414</b> includes a scan target <b>416</b>, which may be interpreted by a patch image generator as designating a region in which to insert an adversarial patch.</p><p id="p-0098" num="0092">As mentioned, sample image <b>410</b> is provided for exemplary purposes only; sample images do not need to depict a person wearing a shirt, nor do they require a designated patch region (the patch region can be identified automatically).</p><p id="p-0099" num="0093">Building <b>418</b> is included in the background of sample image <b>410</b> merely for purposes of illustration and to provide a reference point and scale.</p><p id="p-0100" num="0094">Diagram <b>400</b> also depicts a first modified image <b>420</b>. First modified image <b>420</b> (&#x201c;image <b>420</b>&#x201d;) depicts the same person <b>412</b> wearing the same shirt <b>414</b> (with the same building <b>418</b> in the background). However, scan target <b>416</b> has been replaced with adversarial patch <b>426</b>. First modified image <b>420</b> may be generated by, for example, patch image modifier <b>310</b> of system <b>300</b>, described above with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Adversarial patch <b>426</b> may be, for example, a near adversarial patch generated by near patch generator <b>304</b>.</p><p id="p-0101" num="0095">In some instances, sample image <b>410</b> may be input to a patch generator, and patch <b>426</b> may be generated based upon an estimated (or annotated) distance of user <b>412</b> from the camera capturing image <b>410</b>.</p><p id="p-0102" num="0096">Diagram <b>400</b> also depicts a second modified image <b>430</b>. Second modified image <b>430</b> (&#x201c;image <b>430</b>&#x201d;) is similar to image <b>420</b> (and thus <b>410</b>), but with a different patch <b>436</b>. Patch <b>436</b> may be, for example, a far distance patch generated by a far patch generator such as generator <b>308</b> of system <b>300</b>. Thus, an object detection model such as model <b>312</b> may be provided with multiple test images including different adversarial patches (e.g., <b>426</b>, <b>436</b>) even with only a single sample image <b>410</b>.</p><p id="p-0103" num="0097">Diagram <b>400</b> also depicts a third modified image <b>440</b>. Third modified image <b>440</b> (&#x201c;image <b>440</b>&#x201d;) is generally similar to images <b>410</b>-<b>430</b>, except user <b>412</b> is depicted at a greater distance from the camera, as evidence by a size of user <b>412</b> relative to building <b>418</b>. This results in patch <b>436</b> appearing smaller in image <b>440</b> than it does in image <b>430</b>. In turn, this results in less detail of patch <b>436</b> being available to an object detection model attempting to evaluate image <b>440</b>.</p><p id="p-0104" num="0098">In some instances, a far image can be generated based on a near image. For example, in some instances, image <b>440</b> can be generated based upon image <b>430</b> (e.g., user <b>412</b>/shirt <b>414</b>/patch <b>436</b> can be digitally &#x201c;shrunk&#x201d;). This shrinking may be performed after inclusion of patch <b>436</b>, to simulate loss of information in patch <b>436</b> resulting from the increased distance from the camera. In some instances, the shrinking operation may include pixelating or compressing the patch to reduce a level of detail of the patch.</p><p id="p-0105" num="0099">In contrast, in some instances, a near image may be generated based on a far image. For example, in some instances, image <b>430</b> may be generated based on <b>440</b>. This may involve, for example, cropping far image <b>440</b> to just a region of the image containing user <b>412</b> and patch <b>436</b>, upscaling the cropped region, and then replacing the upscaled patch <b>436</b> with a new copy of patch <b>436</b> (so patch <b>436</b> is at a level of detail expected for near image <b>430</b>). This may enable comparison of patch performance at different apparent distances with minimal change to other aspects of the image. For example, the background, user pose, lighting, etc. may be identical between images <b>430</b> and <b>440</b>, allowing for a more reliable comparison of performance of the patch at various distances. In some instances, image <b>440</b> can be generated based upon a second sample image (not shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0106" num="0100"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a detailed method <b>500</b> of training a hybrid-distance adversarial patch generator, consistent with several embodiments of the present disclosure. In some instances, method <b>500</b> may be performed to train a hybrid patch generator while method <b>200</b> is being performed to train near and/or far patch generators.</p><p id="p-0107" num="0101">Method <b>500</b> comprises generating a hybrid patch via a hybrid patch generator at operation <b>502</b>. In some instances, operation <b>502</b> may be similar to operation <b>204</b> (e.g., a dedicated hybrid patch generator generates the patch). In some instances, operation <b>502</b> may include receiving a near patch and a far patch and performing a merge operation on the two, resulting in a combined hybrid patch. An example merge operation is depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, as discussed below.</p><p id="p-0108" num="0102">Method <b>500</b> further comprises evaluating, at operation <b>504</b>, performance of the hybrid patch at multiple distances. In order to evaluate performance of the hybrid patch, a system performing method <b>500</b> may submit an image depicting the hybrid patch to an object detection system. In order to acquire such an image, operation <b>504</b> may include modifying a sample image to insert the hybrid patch at a particular apparent distance. As the hybrid patch is to be evaluated at multiple distances, operation <b>504</b> may include modifying images to insert the hybrid patch at different distances. For example, operation <b>504</b> may include receiving a sample image such as image <b>410</b> and generating multiple modified images such as image <b>430</b> and image <b>440</b>. Notably, the images include the same patch but at different distances.</p><p id="p-0109" num="0103">In some instances, the near and far images utilized at operation <b>504</b> may be based on the same sample image. This may advantageously reduce a number of variables in evaluations performed at operation <b>504</b>, enabling a more reliable evaluation of the patch's performance at different distances. As an example, a first sample image depicting a person at a near distance may have been captured during daytime in a desert, while a second sample image depicting a second person at a near distance may have been captured at dusk in a rainforest. Even though both sample images depict a person at a near distance, an object detection system analyzing the two sample images may have different levels of effectiveness (e.g., accuracy). This can be due to the differences between the sample images (such as lighting, background, the person to be identified, etc.). As performance of the hybrid patch is evaluated based upon effectiveness of the object detection system, inserting the hybrid patch into both of these sample images may result in a range of values describing the patch's near-distance performance. While this may be useful for determining how consistently the patch may perform at a near distance, use of different sample images for different distances may skew evaluations of performance of the patch at multiple distances. In other words, using the same sample image for multiple distances increases confidence that any differences in patch performance between the two distances are actually a result of the different distance.</p><p id="p-0110" num="0104">Operation <b>504</b> may further include submitting the images to an object detection model and receiving an output. Operation <b>504</b> includes receiving multiple object detection evaluations; at least one near evaluation and at least one far evaluation. A single object detection model may be utilized for the near and far object detection. This may further reduce variability in results, as different models perform differently. For example, the hybrid adversarial patch being evaluated may be effective against a first object detection model but ineffective against a second object detection model (though a system performing method <b>500</b> may not yet be &#x201c;aware&#x201d; of this). Further, a first test image may depict the hybrid patch and an object at a far distance, while a second test image may depict the object and the hybrid patch at a near distance. In such an example, submitting the first test image to the first model and submitting the second test image to the second model may result in the first model failing to identify the object and the second model succeeding in identifying the object. This may appear to suggest that the hybrid patch is effective at far distances but ineffective at near distances. However, because the hybrid patch is generally ineffective against the second model, the second model's ability to identify the object may not have anything to do with the distance. Utilizing the same model for multiple distance evaluations can help alleviate this uncertainty. Further, training a patch generator to defeat multiple models at once can significantly impact training time and is not guaranteed to be possible.</p><p id="p-0111" num="0105">Method <b>500</b> further comprises determining whether hybrid patch performance criteria have been met at operation <b>506</b>. Operation <b>506</b> may be generally similar to operation <b>210</b> of method <b>200</b>, except hybrid patches may have additional performance criteria. For example, in some instances, a minimum performance for each distance may be enforced. As an example, a minimum performance of 80% success in deceiving an object detection model may be required. Thus, a hybrid patch generated that is 95% successful at deceiving an object detection model when the patch is in a &#x201c;near&#x201d; image and 76% successful at deceiving the same object detection model when the patch is in a &#x201c;far&#x201d; image may not be considered adequate in terms of performance criteria.</p><p id="p-0112" num="0106">As another example, hybrid patches may have a maximum overall performance criterion. For example, a hybrid patch may be required to have a total success rate (e.g., near success %+far success %) of at least 170%. Thus, a hybrid patch generated that is 81% successful in &#x201c;near&#x201d; images and 87% successful in &#x201c;far&#x201d; images may satisfy the minimum criterion but may fail to satisfy the overall performance criterion, as 81%+87%=168%&#x3c;170%.</p><p id="p-0113" num="0107">If the performance criteria are not met (<b>506</b> &#x201c;No&#x201d;), the hybrid patch generator may be updated. As method <b>500</b> may be performed contemporaneously with method <b>200</b>, a system performing method <b>500</b> can leverage updates made to a near patch generator and a far patch generator in order to inform updates to the hybrid patch generator. Thus, method <b>500</b> may further comprise receiving, at operation <b>508</b>, updates to a near patch generator and a far patch generator (if any updates have been made to a near and/or far generator since a last iteration of operation <b>508</b> or since method <b>500</b> has begun). For example, if method <b>500</b> is being performed contemporaneously with method <b>200</b>, operation <b>508</b> may include receiving any updates that were made as part of operation <b>212</b>. For example, in some instances, operation <b>508</b> may include identifying weights being changed in a previous iteration or the last several iterations for both near and far generators. In some instances (for example, if no updates have been made to near or far patch generators, or if no near or far patch generators are available / being trained at all during performance of method <b>500</b>), operation <b>508</b> may be skipped.</p><p id="p-0114" num="0108">In some instances, operation <b>508</b> may be omitted entirely. For example, even though updates to near and/or far patch generators may be helpful in making updates for training a hybrid patch generator, they are not required; method <b>500</b> can be performed to train a hybrid adversarial patch generator alone. Training the hybrid patch generator without a near or far patch generator may take additional time/iterations, but can still result in an effective generator.</p><p id="p-0115" num="0109">Method <b>500</b> further comprises updating, at operation <b>510</b>, the hybrid patch generator based on the evaluated performances and/or the near/far patch generator updates.</p><p id="p-0116" num="0110">Performance-based updates may be similar to those of operation <b>212</b>, except operation <b>504</b> includes evaluating multiple performances, so operation <b>510</b> is basing updates on multiple variables. As a result, there may be two competing pressures (i.e., stemming from the near performance and stemming from the far performance). For example, the near performance evaluated at operation <b>504</b> might suggest that a first weight should be increased by a first amount (e.g., 3) while the far performance evaluated at operation <b>504</b> might suggest that the same first weight should be decreased by a second amount (e.g., 7), resulting in a conflict. In order to determine how to resolve the conflict and update the generator, operation <b>510</b> may include, for example, summing the changes (e.g., the weight may have been increased by 3 and decreased by 7, yielding in a net update of 3&#x2212;7=&#x2212;4, resulting in decreasing the weight by 4).</p><p id="p-0117" num="0111">If any updates to near and/or far patch generators were received at operation <b>508</b>, operation <b>510</b> may further include consolidating updates made to two generators (i.e., updates to near/far generators) into an update for one (i.e., an update to the hybrid generator). For example, if a given weight was adjusted in a near generator but not adjusted in a far generator, the given weight may be adjusted by a reduced amount (e.g., half) in the hybrid generator. If the same weight is adjusted by the same amount in both generators, then a corresponding weight can be adjusted by the same amount in the hybrid generator.</p><p id="p-0118" num="0112">As noted, in some instances, the hybrid patch generator can just be a simple algorithm to merge a near patch and a far patch. In such instances, operation <b>510</b> may simply include adjusting how the two patches are merged (e.g., 60% near/40% far, etc.).</p><p id="p-0119" num="0113">Method <b>500</b> further comprises looping back to operation <b>502</b> and generating a new hybrid patch, repeating operations <b>502</b>-<b>506</b>. In some instances, the same hybrid patch may be evaluated in multiple different ways. For example, the same patch may be tested with different test images input into several different object detection model. In either case, operation <b>510</b> may loop to <b>504</b>.</p><p id="p-0120" num="0114">Once the hybrid patch performance criteria are met (<b>506</b> &#x201c;Yes&#x201d;), method <b>500</b> ends at <b>512</b>. As with method <b>200</b>, in some instances, multiple consecutive successes may be required to satisfy the performance criteria at <b>506</b>. As an example, a single patch in a single pair of test images may successfully deceive an object detection model at both near and far distances. However, in order to ensure that this is not a coincidence/fluke, the same patch may be tested using other sample images.</p><p id="p-0121" num="0115">Method <b>500</b> may result in a trained hybrid patch generator that can be effective at deceiving object detection models at both near and far distances.</p><p id="p-0122" num="0116"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts an example of merging a near-distance patch <b>602</b> and a far-distance patch <b>604</b> into a hybrid-distance patch <b>606</b>, consistent with several embodiments of the present disclosure. Patches <b>602</b>, <b>604</b>, and <b>606</b>, as depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, are merely provided as abstract examples.</p><p id="p-0123" num="0117">In some instances, patches <b>602</b> and <b>604</b> may be combined on a pixel-by-pixel basis; for example, each pixel of patch <b>606</b> may be an average (in terms of red/green/blue (RGB) values) of a corresponding pixel from both near patch <b>602</b> and far patch <b>604</b>. In some instances, the merging can be adjusted based on performance. For example, if hybrid patch <b>606</b> is underperforming in a near image, a merging algorithm may be weighted more heavily in favor of near patch <b>602</b>.</p><p id="p-0124" num="0118">As discussed above with reference to method <b>500</b>, merging two patches is not the only way to generate a hybrid patch; hybrid patches can be generated from a unique hybrid patch generator. Such a hybrid generator may be trained based on far and near patch generators (such as via operation <b>508</b> and <b>510</b>), but in some instances, the hybrid generator may not require near and far patch generators at all (i.e., <b>508</b> can be skipped). Thus, near patch <b>602</b> and far patch <b>604</b> may not necessarily be generated or required as inputs for generation of hybrid patch <b>606</b>.</p><p id="p-0125" num="0119"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a detailed method <b>700</b> of training an object detection model to defeat hybrid-distance adversarial patches, consistent with several embodiments of the present disclosure. Method <b>700</b> comprises generating a hybrid patch via a hybrid patch generator at operation <b>702</b>. Operation <b>702</b> may be performed in a substantially similar manner as operation <b>502</b> of method <b>500</b>, discussed above with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. However, operation <b>702</b> may include generating a patch via a trained hybrid patch generator.</p><p id="p-0126" num="0120">Method <b>700</b> further comprises modifying, at operation <b>704</b>, images including the hybrid patch in a near setting and a far setting. Operation <b>704</b> may include, for example, utilizing a sample image and digitally inserting the hybrid patch generated at operation <b>702</b> into the sample image, resulting in a first modified image. The first modified image can then be further modified to adjust an apparent distance of the patch, resulting in a second modified image. Image <b>430</b> and image <b>440</b>, discussed above with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, are provided as examples of a first and second modified image including the same patch.</p><p id="p-0127" num="0121">Method <b>700</b> further comprises evaluating an object detection model's &#x201c;near&#x201d; performance based on one of the modified images at operation <b>706</b>. Operation <b>706</b> may include, for example, submitting the modified image to an object detection model and causing the system to attempt to classify/detect objects in the image. Results of the system can then be compared to known values. For example, operation <b>706</b> may include determining whether an object in the image bearing an adversarial patch was correctly detected and/or identified. Further, a confidence value of the object detection model may be retrieved.</p><p id="p-0128" num="0122">Method <b>700</b> further comprises evaluating the object detection model's &#x201c;far&#x201d; performance based on another of the modified images at operation <b>708</b>. Operation <b>708</b> may be performed in a substantially similar manner to operation <b>706</b>, except with a different modified image. For example, operation <b>706</b> may be performed utilizing an image such as image <b>430</b>, while operation <b>708</b> may be performed utilizing an image such as image <b>440</b>.</p><p id="p-0129" num="0123">Method <b>700</b> further comprises determining whether object detection model performance criteria have been met at operation <b>710</b>. Operation <b>710</b> may include performing multiple comparisons (similar to operation <b>506</b> of method <b>500</b>), such as evaluating a maximum overall performance, enforcing a minimum performance, etc. For example, an object detection model may be evaluated to identify an object at a near distance in a first image including a hybrid adversarial patch with 85% confidence and to identify an object at a far distance in a second image including the hybrid adversarial patch with 93% confidence. An overall performance may be a simple arithmetic mean of the two (i.e., (85%+93%)/2=89%). In some instances, performance criteria of the model may include a minimum overall performance of 95%, meaning the overall performance must meet or exceed 95% in order for the criterion to be met (in which case, the example object detection model would have failed, as 89%&#x3c;95%). In some instances, performance criteria of the model may include a minimum absolute performance of 90%, all particular performances must meet or exceed 90% in order for the criterion to be met (in which case, the example object detection model would also have failed, as even though 93%&#x3e;90%, 85%&#x3c;90%). Of course, the specific thresholds can vary and be controlled by users. These thresholds can be changed based upon use case; for example, a higher minimum overall performance of 99% may result in a more adversarial-patch-resilient object detection model, but may require additional resources to train. As with operations <b>506</b> and <b>210</b>, in some instances, performance criteria can require multiple consecutive successes in order to confirm that a given success if not a coincidence or fluke.</p><p id="p-0130" num="0124">If system performance criteria have not been met (<b>710</b> &#x201c;No&#x201d;), method <b>700</b> further comprises updating, at operation <b>712</b>, the object detection model based on the performances. Operation <b>712</b> may include, for example, adjusting one or more weights of a machine learning model, etc.</p><p id="p-0131" num="0125">Once performance criteria have been met (<b>710</b> &#x201c;Yes&#x201d;), method <b>700</b> ends at operation <b>714</b>. Thus, method <b>700</b> can train an object detection model to reliably defeat hybrid adversarial patches at multiple distances.</p><p id="p-0132" num="0126">Referring now to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, shown is a high-level block diagram of an example computer system <b>800</b> that may be configured to perform various aspects of the present disclosure, including, for example, methods <b>100</b>, <b>200</b>, <b>500</b>, and <b>700</b>. The example computer system <b>800</b> may be used in implementing one or more of the methods or modules, and any related functions or operations, described herein (e.g., using one or more processor circuits or computer processors of the computer), in accordance with embodiments of the present disclosure. In some embodiments, the major components of the computer system <b>800</b> may comprise one or more CPUs <b>802</b>, a memory subsystem <b>808</b>, a terminal interface <b>816</b>, a storage interface <b>818</b>, an I/O (Input/Output) device interface <b>820</b>, and a network interface <b>822</b>, all of which may be communicatively coupled, directly or indirectly, for inter-component communication via a memory bus <b>806</b>, an I/O bus <b>814</b>, and an I/O bus interface unit <b>812</b>.</p><p id="p-0133" num="0127">The computer system <b>800</b> may contain one or more general-purpose programmable processors <b>802</b> (such as central processing units (CPUs)), some or all of which may include one or more cores <b>804</b>A, <b>804</b>B, <b>804</b>C, and <b>804</b>N, herein generically referred to as the CPU <b>802</b>. In some embodiments, the computer system <b>800</b> may contain multiple processors typical of a relatively large system; however, in other embodiments the computer system <b>800</b> may alternatively be a single CPU system. Each CPU <b>802</b> may execute instructions stored in the memory subsystem <b>808</b> on a CPU core <b>804</b> and may comprise one or more levels of on-board cache.</p><p id="p-0134" num="0128">In some embodiments, the memory subsystem <b>808</b> may comprise a random-access semiconductor memory, storage device, or storage medium (either volatile or non-volatile) for storing data and programs. In some embodiments, the memory subsystem <b>808</b> may represent the entire virtual memory of the computer system <b>800</b> and may also include the virtual memory of other computer systems coupled to the computer system <b>800</b> or connected via a network. The memory subsystem <b>808</b> may be conceptually a single monolithic entity, but, in some embodiments, the memory subsystem <b>808</b> may be a more complex arrangement, such as a hierarchy of caches and other memory devices. For example, memory may exist in multiple levels of caches, and these caches may be further divided by function, so that one cache holds instructions while another holds non-instruction data, which is used by the processor or processors. Memory may be further distributed and associated with different CPUs or sets of CPUs, as is known in any of various so-called non-uniform memory access (NUMA) computer architectures. In some embodiments, the main memory or memory subsystem <b>808</b> may contain elements for control and flow of memory used by the CPU <b>802</b>. This may include a memory controller <b>810</b>.</p><p id="p-0135" num="0129">Although the memory bus <b>806</b> is shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> as a single bus structure providing a direct communication path among the CPU <b>802</b>, the memory subsystem <b>808</b>, and the <b>110</b> bus interface <b>812</b>, the memory bus <b>806</b> may, in some embodiments, comprise multiple different buses or communication paths, which may be arranged in any of various forms, such as point-to-point links in hierarchical, star or web configurations, multiple hierarchical buses, parallel and redundant paths, or any other appropriate type of configuration. Furthermore, while the I/O bus interface <b>812</b> and the I/O bus <b>814</b> are shown as single respective units, the computer system <b>800</b> may, in some embodiments, contain multiple I/O bus interface units <b>812</b>, multiple I/O buses <b>814</b>, or both. Further, while multiple I/O interface units are shown, which separate the I/O bus <b>814</b> from various communications paths running to the various I/O devices, in other embodiments some or all of the I/O devices may be connected directly to one or more system I/O buses.</p><p id="p-0136" num="0130">In some embodiments, the computer system <b>800</b> may be a multi-user mainframe computer system, a single-user system, or a server computer or similar device that has little or no direct user interface but receives requests from other computer systems (clients). Further, in some embodiments, the computer system <b>800</b> may be implemented as a desktop computer, portable computer, laptop or notebook computer, tablet computer, pocket computer, telephone, smart phone, mobile device, or any other appropriate type of electronic device.</p><p id="p-0137" num="0131">It is noted that <figref idref="DRAWINGS">FIG. <b>8</b></figref> is intended to depict the representative major components of an exemplary computer system <b>800</b>. In some embodiments, however, individual components may have greater or lesser complexity than as represented in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, components other than or in addition to those shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> may be present, and the number, type, and configuration of such components may vary.</p><p id="p-0138" num="0132">The present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration. The computer program product may include a computer readable storage medium (or media) having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.</p><p id="p-0139" num="0133">The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be, for example, but is not limited to, an electronic storage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing. A non-exhaustive list of more specific examples of the computer readable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a portable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable combination of the foregoing. A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through a waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.</p><p id="p-0140" num="0134">Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the Internet, a local area network, a wide area network and/or a wireless network. The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. A network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing/processing device.</p><p id="p-0141" num="0135">Computer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware instructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++, or the like, and procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider). In some embodiments, electronic circuitry including, for example, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry, in order to perform aspects of the present invention.</p><p id="p-0142" num="0136">Aspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.</p><p id="p-0143" num="0137">These computer readable program instructions may be provided to a processor of a computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0144" num="0138">The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus, or other device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0145" num="0139">The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s). In some alternative implementations, the functions noted in the blocks may occur out of the order noted in the Figures. For example, two blocks shown in succession may, in fact, be accomplished as one step, executed concurrently, substantially concurrently, in a partially or wholly temporally overlapping manner, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.</p><p id="p-0146" num="0140">The descriptions of the various embodiments of the present disclosure have been presented for purposes of illustration but are not intended to be exhaustive or limited to the embodiments disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments. The terminology used herein was chosen to explain the principles of the embodiments, the practical application or technical improvement over technologies found in the marketplace, or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230005111A1-20230105-M00001.NB"><img id="EMI-M00001" he="16.26mm" wi="76.20mm" file="US20230005111A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005111A1-20230105-M00002.NB"><img id="EMI-M00002" he="13.38mm" wi="76.20mm" file="US20230005111A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>training a hybrid adversarial patch generator;</claim-text><claim-text>generating, via the hybrid adversarial patch generator, a hybrid adversarial patch;</claim-text><claim-text>inserting the hybrid adversarial patch into a first image, resulting in a first modified image depicting the hybrid adversarial patch at a first distance and a first object; and</claim-text><claim-text>inserting the hybrid adversarial patch into a second image, resulting in a second modified image depicting the hybrid adversarial patch at a second distance and a second object;</claim-text><claim-text>submitting the first image and the second image to a first object detection model;</claim-text><claim-text>receiving an output from the first object detection model; and</claim-text><claim-text>updating the hybrid adversarial patch generator based on the output.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>training a near adversarial patch generator, the training the near adversarial patch generator including updating a first weight by a first amount; and</claim-text><claim-text>training a far adversarial patch generator, the training the far adversarial patch generator including updating the first weight by a second amount, wherein the training the hybrid adversarial patch generator includes updating the first weight by a third amount, the third amount based on the first amount and the second amount.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the training the near adversarial patch generator includes training an adversarial patch modifier to modify patches generated by a pretrained patch generator.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising training the first object detection model to detect:<claim-text>the first object in the first modified image; and</claim-text><claim-text>the second object in the second modified image, wherein the training the first object detection model includes:<claim-text>submitting the first modified image to the first object detection model;</claim-text><claim-text>receiving a first output from the first object detection model as a result of the submitting the first modified image;</claim-text><claim-text>submitting the second modified image to the first object detection model;</claim-text><claim-text>receiving a second output from the first object detection model as a result of the submitting the second modified image;</claim-text><claim-text>evaluating a performance of the first object detection model based on the first output and the second output; and</claim-text><claim-text>updating the first object detection model based on the performance.</claim-text></claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the generating the hybrid adversarial patch includes:<claim-text>receiving a near adversarial patch;</claim-text><claim-text>receiving a far adversarial patch; and</claim-text><claim-text>merging the near adversarial patch and the far adversarial patch into the hybrid adversarial patch.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the merging includes:<claim-text>determining a first Red-Green-Blue (RGB) value of a first pixel of the near adversarial patch, the first pixel in a first relative location in the near adversarial patch;</claim-text><claim-text>determining a second RGB value of a second pixel of the far adversarial patch, the second pixel in the first relative location in the far adversarial patch; and</claim-text><claim-text>generating a third RGB value of a third pixel of the hybrid adversarial patch, the generating based on the first RGB value and the second RGB value, the third pixel in the first location in the hybrid adversarial patch.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first object is the second object.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A system, comprising:<claim-text>a memory; and</claim-text><claim-text>a processor coupled to the memory, the processor configured to:<claim-text>train a hybrid adversarial patch generator;</claim-text><claim-text>generate, via the hybrid adversarial patch generator, a hybrid adversarial patch;</claim-text><claim-text>insert the hybrid adversarial patch into a first image, resulting in a first modified image depicting the hybrid adversarial patch at a first distance and a first object;</claim-text><claim-text>insert the hybrid adversarial patch into a second image, resulting in a second modified image depicting the hybrid adversarial patch at a second distance and a second object;</claim-text><claim-text>submit the first image and the second image to a first object detection model;</claim-text><claim-text>receive an output from the first object detection model; and</claim-text><claim-text>update the hybrid adversarial patch generator based on the output.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the processor is further configured to:<claim-text>train a near adversarial patch generator, the training the near adversarial patch generator including updating a first weight by a first amount; and</claim-text><claim-text>train a far adversarial patch generator, the training the far adversarial patch generator including updating the first weight by a second amount, wherein the training the hybrid adversarial patch generator includes updating the first weight by a third amount, the third amount based on the first amount and the second amount.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the training the near adversarial patch generator includes training an adversarial patch modifier to modify patches generated by a pretrained patch generator.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the processor is further configured to train the first object detection model to detect:<claim-text>the first object in the first modified image; and</claim-text><claim-text>the second object in the second modified image, wherein the training the first object detection model includes:<claim-text>wherein the training the first object detection model includes:</claim-text><claim-text>submitting the first modified image to the first object detection model;</claim-text><claim-text>receiving a first output from the first object detection model as a result of the submitting the first modified image;</claim-text><claim-text>submitting the second modified image to the first object detection model;</claim-text><claim-text>receiving a second output from the first object detection model as a result of the submitting the second modified image;</claim-text><claim-text>evaluating a performance of the first object detection model based on the first output and the second output; and</claim-text><claim-text>updating the first object detection model based on the performance.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the generating the hybrid adversarial patch includes:<claim-text>receiving a near adversarial patch;</claim-text><claim-text>receiving a far adversarial patch; and</claim-text><claim-text>merging the near adversarial patch and the far adversarial patch into the hybrid adversarial patch.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the merging includes:<claim-text>determining a first Red-Green-Blue (RGB) value of a first pixel of the near adversarial patch, the first pixel in a first relative location in the near adversarial patch;</claim-text><claim-text>determining a second RGB value of a second pixel of the far adversarial patch, the second pixel in the first relative location in the far adversarial patch; and</claim-text><claim-text>generating a third RGB value of a third pixel of the hybrid adversarial patch, the generating based on the first RGB value and the second RGB value, the third pixel in the first location in the hybrid adversarial patch.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the first object is the second object.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A computer program product, the computer program product comprising a computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to:<claim-text>train a hybrid adversarial patch generator;</claim-text><claim-text>generate, via the hybrid adversarial patch generator, a hybrid adversarial patch;</claim-text><claim-text>insert the hybrid adversarial patch into a first image, resulting in a first modified image depicting the hybrid adversarial patch at a first distance and a first object;</claim-text><claim-text>insert the hybrid adversarial patch into a second image, resulting in a second modified image depicting the hybrid adversarial patch at a second distance and a second object;</claim-text><claim-text>submit the first image and the second image to a first object detection model;</claim-text><claim-text>receive an output from the first object detection model; and</claim-text><claim-text>update the hybrid adversarial patch generator based on the output.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions further cause the computer to:<claim-text>train a near adversarial patch generator, the training the near adversarial patch generator including updating a first weight by a first amount; and</claim-text><claim-text>train a far adversarial patch generator, the training the far adversarial patch generator including updating the first weight by a second amount, wherein the training the hybrid adversarial patch generator includes updating the first weight by a third amount, the third amount based on the first amount and the second amount.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions further cause the computer to train the first object detection model to detect:<claim-text>the first object in the first modified image; and</claim-text><claim-text>the second object in the second modified image, wherein the training the first object detection model includes:<claim-text>submitting the first modified image to the first object detection model;</claim-text><claim-text>receiving a first output from the first object detection model as a result of the submitting the first modified image;</claim-text><claim-text>submitting the second modified image to the first object detection model;</claim-text><claim-text>receiving a second output from the first object detection model as a result of the submitting the second modified image;</claim-text><claim-text>evaluating a performance of the first object detection model based on the first output and the second output; and</claim-text><claim-text>updating the first object detection model based on the performance.</claim-text></claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the generating the hybrid adversarial patch includes:<claim-text>receiving a near adversarial patch;</claim-text><claim-text>receiving a far adversarial patch; and</claim-text><claim-text>merging the near adversarial patch and the far adversarial patch into the hybrid adversarial patch.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer program product of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the merging includes:<claim-text>determining a first Red-Green-Blue (RGB) value of a first pixel of the near adversarial patch, the first pixel in a first relative location in the near adversarial patch;</claim-text><claim-text>determining a second RGB value of a second pixel of the far adversarial patch, the second pixel in the first relative location in the far adversarial patch; and</claim-text><claim-text>generating a third RGB value of a third pixel of the hybrid adversarial patch, the generating based on the first RGB value and the second RGB value, the third pixel in the first location in the hybrid adversarial patch.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first object is the second object.</claim-text></claim></claims></us-patent-application>