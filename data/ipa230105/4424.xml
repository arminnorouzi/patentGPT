<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004425A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004425</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782131</doc-number><date>20191205</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>48</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>54</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>4843</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>54</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Distributed Processing System</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Nippon Telegraph and Telephone Corporation</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Ito</last-name><first-name>Tsuyoshi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Kawai</last-name><first-name>Kenji</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Tanaka</last-name><first-name>Kenji</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Arikawa</last-name><first-name>Yuki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Terada</last-name><first-name>Kazuhiko</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Sakamoto</last-name><first-name>Takeshi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2019/047633</doc-number><date>20191205</date></document-id><us-371c12-date><date>20220602</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A distributed processing system to which a plurality of distributed nodes are connected, each of the distributed nodes including a plurality of arithmetic devices and an interconnect device, wherein, in the interconnect device and/or the arithmetic devices of one of the distributed nodes, memory areas are assigned to each job to be processed by the distributed processing system, and direct memory access between memories for processing the job is executed at least between interconnect devices, between arithmetic devices or between an interconnect device and an arithmetic device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="105.24mm" wi="154.77mm" file="US20230004425A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="156.80mm" wi="117.69mm" orientation="landscape" file="US20230004425A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="194.39mm" wi="125.81mm" file="US20230004425A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="233.93mm" wi="138.94mm" file="US20230004425A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="227.58mm" wi="154.01mm" file="US20230004425A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="196.43mm" wi="78.74mm" orientation="landscape" file="US20230004425A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a national phase entry of PCT Application No. PCT/JP2019/047633, filed on Dec. 5, 2019, which application is hereby incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present invention relates to a distributed processing system that processes tasks that occur by jobs from a plurality of users, at a high speed and with a high efficiency.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Recently, the arrival of the so-called Post-Moore era, to which Moore's Law cannot be applied due to limitations of miniaturization of a silicon process, has been discussed. For the Post-Moore era, efforts have been made to break through the limitations of computational performance due to miniaturization of a silicon process for a processor such as a CPU to dramatically improve the computational performance.</p><p id="p-0005" num="0004">As such efforts, there is a multi-core approach of providing a plurality of arithmetic cores in one processor. However, the size of one silicon chip is limited, and there are limitations to drastic improvement of a single processor. In order to exceed such limitations of a single processor, attention has been paid to a distributed processing system technology for processing a high-load task that has been difficult to process by a single device or a single server, at a high speed using a distributed processing system in which a plurality of servers equipped with arithmetic devices are connected via large-capacity interconnects.</p><p id="p-0006" num="0005">For example, in deep learning, which is an example of a high-load job (hereinafter, a job executed in deep learning will be referred to as a learning job), inference accuracy is improved by updating, for a learning target constituted by multi-layered neuron models, a weight for each neuron model (a coefficient by which a value outputted by a neuron model at a previous stage is to be multiplied) using a large amount of sample data inputted.</p><p id="p-0007" num="0006">In general, a mini batch method is used as a method for improving inference accuracy. In the mini batch method, a gradient computation process for computing a gradient relative to the weight for each piece of sample data, an aggregation process for aggregating gradients for a plurality of different pieces of sample data (adding up the gradients obtained for the pieces of sample data, by weight) and a weight update process for updating each weight based on the aggregated gradient are repeated.</p><p id="p-0008" num="0007">Further, in order to perform the aggregation process in distributed deep learning to which the distributed processing system technology is applied, communication from each distributed processing node to an aggregation processing node (aggregation communication) for collecting data obtained at each distributed processing node (distributed data) to the aggregation processing node, an aggregation process for all nodes at the aggregation node, and communication from the aggregation processing node to each distributed processing node (distribution communication) for transferring data aggregated by the aggregation processing node (aggregated data) to each distributed processing node are required.</p><p id="p-0009" num="0008">These processes, especially the gradient computation process in deep learning requires many computations. Therefore, when the number of weights and the number of pieces of inputted sample data increase in order to improve inference accuracy, time required for the deep learning increases. Therefore, in order to improve the inference accuracy but not to increase the time required for the deep learning, it is necessary to increase the number of distributed nodes and design a large-scale distributed processing system.</p><p id="p-0010" num="0009">An actual learning job does not necessarily always require a maximum processing load. A processing load differs for each user, and there are various learning jobs from such that has an extremely heavy processing load to such that has an extremely light processing load. In conventional technologies, however, there are problems that a process for sharing a processor by a plurality of users is difficult and that, in a large-scale distributed processing system responding to a learning job with a heavy load, a process in a case where learning jobs with different processing loads occur from different users at the same time is difficult (see, for example, Non-Patent Literature 1).</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a distributed processing system in which a conventional distributed processing system is divided and used among a plurality of users. In the case of using a distributed processing system by a plurality of users, a learning job can be executed by assigning a user to each of distributed systems configured by dividing a plurality of distributed nodes <b>102</b> constituting a distributed processing system <b>101</b> as in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. However, since a memory area for one user or job is assigned to an arithmetic device of one distributed node, split loss due to assigning one distributed node even to a job with a light processing load occurs. Therefore, there is a problem that, when a job with a light processing load and a process with a heavy processing load are performed at the same time, assignment of distributed nodes to the plurality of jobs with different processing loads becomes inefficient.</p><heading id="h-0004" level="1">CITATION LIST</heading><heading id="h-0005" level="1">Non-Patent Literature</heading><p id="p-0012" num="0011">Non-Patent Literature 1: &#x201c;NVIDIA TESLA V100 GPU ARCHITECTURE&#x201d; by NVIDIA Corporation, p. 30, published in August 2017, Internet &#x3c;https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf&#x3e;</p><heading id="h-0006" level="1">SUMMARY</heading><heading id="h-0007" level="1">Technical Problem</heading><p id="p-0013" num="0012">Embodiments of the present invention have been made in view of the above situation, and an object is to provide a highly efficient distributed processing system capable of suppressing reduction in computational efficiency due to node split loss and efficiently process a plurality of learning jobs with different processing loads.</p><heading id="h-0008" level="1">Means for Solving the Problem</heading><p id="p-0014" num="0013">In order to solve the problem as described above, a distributed processing system of embodiments of the present invention is a distributed processing system to which a plurality of distributed nodes are connected, each of the distributed nodes including a plurality of arithmetic devices and an interconnect device, wherein, in the interconnect device and/or the arithmetic devices of one of the distributed nodes, memory areas are assigned to each job to be processed by the distributed processing system, and direct memory access between memories for processing the job is executed at least between interconnect devices, between arithmetic devices or between an interconnect device and an arithmetic device.</p><heading id="h-0009" level="1">Effects of Embodiments of the Invention</heading><p id="p-0015" num="0014">According to embodiments of the present invention, it becomes possible to provide a highly efficient distributed processing system capable of, when a plurality of users execute learning jobs with different processing loads at the same time, suppressing reduction in computational efficiency due to node split loss and efficiently processing the plurality of learning jobs with different processing loads.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0010" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram showing a configuration example of a distributed processing system according to a first embodiment of the present invention.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram showing a configuration example of a distributed processing system according to a second embodiment of the present invention.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is a diagram showing a configuration example of a distributed processing system according to a third embodiment of the present invention.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is a diagram showing an operation example of the distributed processing system according to the third embodiment of the present invention.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a diagram showing a configuration example of a distributed node according to a fourth embodiment of the present invention.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a time chart showing an operation of the distributed node according to the fourth embodiment of the present invention.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a diagram showing a configuration example of a distributed node according to a fifth embodiment of the present invention.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a time chart showing an operation of the distributed node according to the fifth embodiment of the present invention.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram showing a conventional distributed processing system.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0011" level="1">DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS</heading><p id="p-0025" num="0024">A first embodiment of the present invention will be explained below with reference to drawings. In the present embodiment, &#x201c;fixed&#x201d; relates to a memory that performs direct memory access and means that memory swap out is prevented by settings. Therefore, &#x201c;a fixed memory&#x201d; means that a user or a job can exclusively use a particular area of the memory, and it is also possible to make a change to share the memory with another user or job or use the memory as a memory area for direct memory access for another user or job, by the settings. It is not meant that the particular area is fixed in advance and cannot be changed. The same goes for other embodiments.</p><p id="p-0026" num="0025">Further, &#x201c;job&#x201d; means a process performed by a program executed by a user, and there may be a case where jobs are different though users are the same. Further, &#x201c;task&#x201d; means a unit of each individual computation performed by an arithmetic device or the like in a job executed by a user. The same goes for the other embodiments.</p><heading id="h-0012" level="1">First Embodiment</heading><p id="p-0027" num="0026">&#x3c;Configuration of Distributed Processing System&#x3e;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram showing an embodiment of the present invention. A distributed processing system <b>101</b> is configured with a plurality of distributed nodes <b>102</b> constituting the distributed processing system <b>101</b>. Each distributed node <b>102</b> is provided with a plurality of arithmetic devices <b>103</b> and an interconnect device <b>104</b>. Each of the arithmetic devices <b>103</b> and the interconnect device <b>104</b> is provided with one or more memory areas.</p><p id="p-0029" num="0028">In the configuration example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a case where computational resources in the distributed processing system <b>101</b> are assigned to a job A and a job B is assumed. An arithmetic device <b>103</b>-<b>1</b> is an arithmetic device assigned to process the job A, and arithmetic devices <b>103</b>-<b>2</b> to <b>103</b>-<b>4</b> are arithmetic devices assigned to process the job B.</p><p id="p-0030" num="0029">A memory area <b>106</b>-<b>1</b> is a memory area in the arithmetic device <b>103</b>-i assigned to process the job A. A memory area <b>107</b>-<b>1</b> is a memory area in an interconnect device <b>104</b> assigned to the job A. Memory areas <b>106</b>-<b>2</b> to <b>106</b>-<b>4</b> are memory areas in the arithmetic devices <b>103</b>, which are assigned to a user B. A memory area <b>107</b>-<b>2</b> is a memory area in an interconnect device <b>104</b> assigned to the user B. Further, a surrounding broken line <b>300</b> indicates computational resources used by the job A, and a surrounding solid line <b>400</b> indicates computational resources used by the job B.</p><p id="p-0031" num="0030">&#x3c;Device Configuration of Distributed Node&#x3e;</p><p id="p-0032" num="0031">Next, a specific device configuration example of a distributed node will be described. In the present embodiment, for example, a SYS-4028GR-TR2 server made by Super Micro Computer, Inc. (hereinafter referred to as &#x201c;a server&#x201d;) is used as each distributed node <b>103</b>. On the CPU motherboard of the server, two Intel Xeon CPU processors E5-2600V4 are mounted as CPUs, and eight 32-GB DDR4-2400DIMM memory cards are mounted as a main memory.</p><p id="p-0033" num="0032">Further, on the CPU motherboard, a <b>16</b> lane slot daughter board of PCI Express 3.0 (Gen 3) is implemented. In the slots, four NVIDIA V100 and one VCU118 Evaluation board made by Xillinx Inc. are mounted as the arithmetic devices <b>103</b> and the interconnect device <b>104</b>, respectively. On the Evaluation board, two QSFP28 optical transceivers are implemented as interconnects. The distributed processing system is configured by connecting distributed nodes in a ring shape via optical fibers connected to the QSFP28 transceivers.</p><p id="p-0034" num="0033">As the arithmetic devices, specifically, CPUs (central processing units), GPUs (graphics processing units), FPGAs, quantum computation devices, artificial intelligence (neuron) chips or the like can be used.</p><p id="p-0035" num="0034">In the case of flexibly connecting the distributed nodes using a configuration other than a ring configuration, it is necessary to use an aggregation switch in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As the aggregation switch, for example, SB7800 Infini Band Switch made by Mellanox Technologies Ltd. can be used.</p><p id="p-0036" num="0035">&#x3c;Operation of Distributed Node&#x3e;</p><p id="p-0037" num="0036">Operation of the distributed nodes in the present embodiment will be explained using <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, it is assumed that a user A and a user B are executing distributed deep learning in the distributed processing system.</p><p id="p-0038" num="0037">Specifically, after a gradient computation process, which is one of tasks of a learning job, ends, addition of pieces of gradient data is performed for pieces of gradient data of jobs obtained at the arithmetic devices, for example, among arithmetic devices in the same distributed node by a collective communication protocol such as All-Reduce. The added gradient data is further aggregated to arithmetic devices of an adjacent distributed node via the interconnects by aggregation communication and is addition-processed.</p><p id="p-0039" num="0038">Similarly, when gradient data from a distributed node executing a learning job is aggregated at an aggregation node, the gradient data average-processed there is distributedly communicated to arithmetic devices involved in the aggregation and shared. Learning is repeated based on the shared gradient data, and learning parameters are updated at each arithmetic device.</p><p id="p-0040" num="0039">In such aggregation communication and distribution communication, in order to move gradient data at a high speed, memory areas included in devices are fixedly assigned, and data transfer is performed between fixedly assigned memory addresses of the memory areas between an arithmetic device and an interconnect device in a distributed node and between interconnect devices of different distributed nodes. The former data transfer in a distributed node is called direct memory access, and the latter data transfer between distributed nodes is called remote direct memory access. Conventionally, in the four arithmetic devices in the distributed node <b>102</b> on the upper left of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, memories of the distributed node are assigned to a single job, and the memories of the one distributed node <b>102</b> are occupied by one user.</p><p id="p-0041" num="0040">In the present embodiment, however, the fixed memory area <b>106</b>-i for the job A is assigned to the leftmost arithmetic device <b>103</b>-<b>1</b>, and the fixed memory areas <b>106</b>-<b>2</b> to <b>106</b>-<b>4</b> for the job B are assigned to the other three arithmetic devices <b>103</b>-<b>2</b> to <b>103</b>-<b>4</b>, among the four arithmetic devices of the distributed node on the upper left of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Further, in the interconnect device <b>104</b> in this distributed node <b>102</b>, the individual fixed memory areas <b>107</b>-<b>1</b> and <b>107</b>-<b>2</b> are assigned to the job A and job B, respectively.</p><p id="p-0042" num="0041">By assigning memories of arithmetic devices and an interconnect device in one distributed node to each of a plurality of jobs as described above, direct memory access accompanying the job A is executed between the fixed memory area <b>106</b>-<b>1</b> provided in the leftmost arithmetic device <b>103</b>-<b>1</b> of the distributed node on the upper left of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the fixed memory area <b>107</b>-<b>1</b> for the user A in the interconnect device <b>104</b>. Further, as for access between different distributed nodes, remote direct access memory is performed between the fixed memory area <b>107</b>-<b>1</b> for the user A in the interconnect device <b>104</b> and a fixed memory area <b>107</b> assigned to an interconnect device <b>104</b> of a distributed node <b>102</b> on the lower left of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0043" num="0042">Similarly, for the job B, direct memory access accompanying the job B is executed between the fixed memory areas <b>106</b>-<b>2</b> to <b>106</b>-<b>4</b> assigned to the right-side three arithmetic devices <b>103</b>-<b>2</b> to <b>103</b>-<b>4</b> in the distributed node on the upper left of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the fixed memory area <b>107</b>-<b>2</b> for the user B in the interconnect device <b>104</b>. Further, as for access between different distributed nodes, remote direct access memory is performed between the fixed memory area <b>107</b>-<b>2</b> for the user B in the interconnect device <b>104</b> and a fixed memory area assigned to an interconnect device of a distributed node <b>102</b> on the upper right of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0044" num="0043">As described above, in the present embodiment, by providing, for each of a plurality of jobs, a fixed memory area for the job in a device of each distributed node, it is possible to realize distributed processing corresponding to the number of users or jobs using the distributed processing system, not for each distributed nodes but for each arithmetic device. Therefore, in the present embodiment, it is possible to realize a distributed processing system capable of highly efficient distributed processing according to the number of users and the magnitude of processing load of a learning job.</p><heading id="h-0013" level="1">Second Embodiment</heading><p id="p-0045" num="0044">&#x3c;Configuration of Distributed Processing System&#x3e;</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram showing a second embodiment of the present invention. The second embodiments shows a state of a memory assignment process in a case where, in addition to the job A and the job B of the first embodiment, a job C and a job D are further added, and a load of a learning job of each of added users is small. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a dotted line <b>500</b> indicates fixed memory areas in the arithmetic device and the interconnect device for the job C, and the fixed memory area <b>106</b>-<b>2</b> for the job C coexists with the fixed memory area <b>106</b>-i for the job A in the same arithmetic device <b>103</b>-<b>1</b>. The memory area <b>107</b>-<b>2</b> is a fixed memory area in the interconnect device <b>104</b> assigned to the job C. The memory areas <b>106</b>-<b>3</b> and <b>106</b>-<b>4</b> are fixed memory areas in the arithmetic devices <b>103</b>-<b>2</b> and <b>103</b>-<b>3</b>, which are assigned to a user D. A memory area <b>107</b>-<b>3</b> is a fixed memory area in the interconnect device <b>104</b> assigned to the user D.</p><p id="p-0047" num="0046">&#x3c;Operation of Distributed Node&#x3e;</p><p id="p-0048" num="0047">In the second embodiment, it is assumed that, in addition to requests for the learning jobs A and B, the learning jobs C and D with a processing load lighter than the processing load of the jobs A and B are newly requested by users. Since the processing load of the job C is the lightest, the small memory area <b>106</b>-<b>2</b> is assigned to a user C separately from the memory area <b>106</b>-i assigned to the job A, in the leftmost arithmetic device <b>103</b>-i on the upper left that has been used by the user A. Further, since the processing load of the job D is heavier than the processing load of the job C, the two arithmetic devices <b>103</b>-<b>2</b> and <b>103</b>-<b>3</b> among the arithmetic devices used by the job B are assigned to the job D. At this time, assignment is changed to assign fixed memory areas assigned to the job B are assigned to the job D.</p><p id="p-0049" num="0048">Next, in the interconnect device <b>104</b>, fixed memory areas for the job C and the job D are secured in addition to the fixed memory areas assigned to the job A and the job B. Thus, assignment of a fixed memory area to each job is performed in each device, and a learning job by each user is executed in each arithmetic device.</p><p id="p-0050" num="0049">As described above, in the second embodiment, a configuration is made to individually assign a fixed memory area in a device to each job and, furthermore, cause fixed device areas for a plurality of jobs to coexist in one arithmetic device. Therefore, it is possible to flexibly divide a distributed processing system not into units of distributed nodes but into units of arithmetic devices each of which constitutes a distributed node and, furthermore, into units of fixed memory areas in the arithmetic devices. Therefore, in the second embodiment, it is possible to provide a distributed processing system capable of processing a plurality of jobs with different magnitudes of processing loads efficiently at a high speed.</p><heading id="h-0014" level="1">Third Embodiment</heading><p id="p-0051" num="0050">&#x3c;Configuration of Distributed Processing System&#x3e;</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref> are diagrams showing a configuration example and an operation example of a distributed processing system according to a third embodiment of the present invention. In the second embodiment, a fixed memory area for each of a plurality of jobs is provided in an interconnect device. In the third embodiment, a memory area shared by a plurality of jobs is provided in an interconnect device.</p><p id="p-0053" num="0052">&#x3c;Operation of Distributed Node&#x3e;</p><p id="p-0054" num="0053">In the present embodiment, when the number of jobs increases, and fixed memory areas to be assigned to the jobs are insufficient, one fixed memory area is shared by a plurality of jobs. When all of fixed memory areas in the interconnect device that can be assigned as fixed memory areas are consumed as fixed memory areas for the job B, there are no fixed memory areas to be assigned to the other jobs A, C and D. Therefore, the memory areas of the interconnect device <b>104</b> are set as a fixed shared memory area <b>107</b> to be shared by the jobs A, B, C and D as shown in the right diagram in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is shows a specific example of an aspect of the sharing. In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the job B performs direct memory access using all the fixed memory area <b>107</b> at time t<b>1</b>, and the users A, C and D not requiring a large fixed memory area share the fixed memory at the same time at time t<b>2</b>.</p><p id="p-0056" num="0055">By not assigning fixed memory areas to a plurality of jobs individually but causing the fixed memory areas to be a shared memory to be shared by the plurality of jobs, it is possible to provide a distributed processing system that can be used by a plurality of jobs even if resources to be assigned as fixed memory areas are small like an interconnect device. According to the present embodiment, it is possible to provide a distributed processing system capable of processing a plurality of jobs efficiently at a high speed.</p><p id="p-0057" num="0056">Further, in the case of sharing a fixed memory area by time division, a bandwidth secured for direct memory access can be occupied by one user. Therefore, it is possible to preferentially assign a user from whom high-speed data transfer is required, and there is a merit that QoS for each job can be provided.</p><heading id="h-0015" level="1">Fourth Embodiment</heading><p id="p-0058" num="0057">&#x3c;Configuration of Distributed Processing System&#x3e;</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> are diagrams showing a configuration example and an operation time chart of a distributed node according to a fourth embodiment of the present invention.</p><p id="p-0060" num="0059">In an arithmetic device <b>103</b> in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, an arithmetic unit A <b>105</b>-<b>1</b> and a fixed memory area A <b>106</b>-i are assigned for a job A, and an arithmetic unit B <b>105</b>-<b>2</b> and a fixed memory area B <b>106</b>-<b>2</b> are assigned for a job B. In an interconnect device <b>104</b>, a fixed memory area A <b>107</b>-<b>1</b> is assigned for the job A, and a fixed memory area B <b>107</b>-<b>2</b> is assigned for the job B.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> shows a time chart of computation in the arithmetic device <b>103</b> and a time chart of communication between the arithmetic device and the interconnect device. In the time chart of the computation in the arithmetic device <b>103</b>, a task A<b>1</b> and a task A<b>2</b> are computation time for the job A in the arithmetic device <b>103</b>, and computation time for a task B is computation time for the job B. The time chart of communication between the arithmetic device and the interconnect device shows time of communication of computation data of the job A between the arithmetic device and the interconnect. &#x3c;Operation of Distributed Node&#x3e;</p><p id="p-0062" num="0061">In the computation time chart in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the job A is started at start time. When the task A ends, inter-memory direct memory access is performed between the arithmetic device and the interconnect device. In the example of deep learning, aggregation and sharing of computation results among distributed nodes are performed via communication by a protocol called collective communication such as All-Reduce. At this time, when the user B starts a job (in this case, it is assumed that communication does not occur between the arithmetic device and the interconnect after the task B), computation of the task B accompanying the start of the job B cannot be started while the job A is being executed.</p><p id="p-0063" num="0062">When All Reduce communication of the job A is executed, however, computation for the job A by the arithmetic device is not performed. Therefore, during the time, a part of the task of the job B can be executed. For example, a case is assumed where 1-GB gradient data is sent in the job A by direct memory access. When the 1-GB data of the job A is transferred to the interconnect device <b>104</b>, the interconnect device <b>104</b> starts direct memory access to the memory of the interconnect device from a cash memory or a global memory in an adjacent distributed node. When the bandwidth for the interconnects is 100 Gbit/s, time required to transfer the 1-GB data is 80 milliseconds. During the 80 milliseconds, the task of the job B can be executed.</p><p id="p-0064" num="0063">If it is assumed that the tasks A<b>1</b> and A<b>2</b> of the job A are repeatedly executed, for example, such that, after the execution time of 800 milliseconds of the task A of the job A, the task A of the job A is executed next, the rate of the execution time of the job A relative to operation time of all the arithmetic devices is 90% when the job A is processed. Here, if the rate of the load of the job B is assumed to be 10% of the load of the job A, all the remaining 10% operation time of the arithmetic devices that the job A could not use up can be utilized, and the efficiency of the arithmetic devices becomes 100%.</p><p id="p-0065" num="0064">Thus, by providing a dedicated fixed memory area for transferring processing data of a predetermined job to an interconnect device, in an arithmetic device and performing scheduling control of direct memory access processes for a plurality of jobs in the arithmetic device, it is possible to increase operation time of the arithmetic device and improve computational efficiency. According to the present embodiment, it is possible to provide a distributed processing system capable of processing a plurality of jobs efficiently at a high speed.</p><heading id="h-0016" level="1">Fifth Embodiment</heading><p id="p-0066" num="0065">(Operation of Distributed Node)</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are diagrams showing a configuration example and an operation time chart of a distributed node according to a fifth embodiment of the present invention. In the fifth embodiment, a communication controller having a communication control function generated by a hardware circuit is installed between memories that perform direct memory access.</p><p id="p-0068" num="0067">In the present embodiment, a case where there are a job A with a heavy load and a job B with a light load, and direct memory accesses for the job A and the job B are performed at the same time is assumed. As shown in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, a fixed memory area is assigned to each of a plurality of jobs in one arithmetic device. Therefore, if direct memory accesses are performed at the same time, bandwidths for the direct memory accesses conflict. Further, if there is a high-priority job among the plurality of jobs, it is necessary to process the high-priority task first.</p><p id="p-0069" num="0068">In <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, it is assumed that a job of a user B is started at time t<b>1</b>, the task is processed by an arithmetic device, and, after that, a job of a user A is started at time t<b>2</b>. Since the user A has a high priority, a communication controller <b>109</b> stops direct access by the user B when the direct access by the user B is started at the time t<b>2</b>, and immediately feeds back information about it to a scheduler <b>108</b> of the arithmetic device <b>103</b>.</p><p id="p-0070" num="0069">The scheduler <b>108</b> of the arithmetic devices <b>103</b> causes direct memory access for the job A to start at time t<b>3</b> after the computation of the job A is completed. When detecting end of data transfer for the job A, the communication controller <b>109</b> feeds back it to the scheduler <b>108</b> and re-starts the direct memory access for the job B at time t<b>4</b>.</p><p id="p-0071" num="0070">Thus, by realizing a communication controller that, for high-priority memory, causes direct memory access to be preferentially performed, by a hardware circuit between fixed memory areas that perform direct memory access between an arithmetic devices and an interconnect device, a process of, when a high-priority job occurs, causing data transfer for a low-priority job to wait and performing the data transfer for the low-priority job after data transfer for the high-priority job is completed becomes possible, without deteriorating latency and bandwidth characteristics. Therefore, even when there are a plurality of jobs with different priorities, it is possible to improve processing efficiency of a high-priory job.</p><p id="p-0072" num="0071">As for the hardware circuit that realizes the communication controller, by equipping the communication controller <b>109</b> on the direct memory access transmission side with a function of giving an identifier that associates a job and data to be transmitted, and equipping a communication controller <b>111</b> on the reception side with an identification function of identifying for which job the direct memory access is, it is possible to perform identification of each job on the reception side at a high speed even when complicated control such as priority processing is performed on the transmission side. Therefore, it is preferable for efficient and highly reliable control to provide the identifier giving function for associating a user and the identification function between memories for direct memory access.</p><p id="p-0073" num="0072">When data is transmitted from the interconnect device <b>104</b> to the arithmetic device <b>103</b>, a similar process is also performed by a scheduler no of the interconnect device <b>104</b> and the communication controllers in and <b>109</b>.</p><heading id="h-0017" level="1">INDUSTRIAL APPLICABILITY</heading><p id="p-0074" num="0073">Embodiments of the present invention can be used for a large-scale distributed processing system that performs a large amount of information processing or a distributed processing system that processes a plurality of jobs with different loads at the same time. Especially, the present invention is applicable to a system that performs machine learning in neural networks, large-scale computation (such as large-scale matrix operation) or a large amount of data information processing.</p><heading id="h-0018" level="1">REFERENCE SIGNS LIST</heading><p id="p-0075" num="0074"><b>101</b> Distributed processing system</p><p id="p-0076" num="0075"><b>102</b> Distributed node</p><p id="p-0077" num="0076"><b>103</b>-<b>1</b> to <b>103</b>-<b>4</b> Arithmetic device</p><p id="p-0078" num="0077"><b>104</b> Interconnect device</p><p id="p-0079" num="0078"><b>105</b>, <b>105</b>-<b>1</b> to <b>105</b>-<b>4</b> Arithmetic unit</p><p id="p-0080" num="0079"><b>106</b>, <b>106</b>-i to <b>106</b>-<b>4</b> Memory area (arithmetic device)</p><p id="p-0081" num="0080"><b>107</b>, <b>107</b>-<b>1</b> to <b>107</b>-<b>2</b> Memory area (interconnect device).</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-001-7" num="001-7"><claim-text><b>1</b>-<b>7</b>. (canceled)</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A distributed processing system to which a plurality of distributed nodes are connected, each of the distributed nodes comprising:<claim-text>a plurality of arithmetic devices; and</claim-text><claim-text>an interconnect device, wherein in the interconnect device or the plurality of arithmetic devices, one or more memory areas are configured to be assigned to each job to be processed by the distributed processing system, and wherein direct memory access between memories for processing a job is executed at least between interconnect devices, between arithmetic devices, or between an interconnect device and an arithmetic device.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The distributed processing system according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the memory areas for processing a plurality of jobs, respectively, are assigned to a first arithmetic device of the arithmetic devices.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The distributed processing system according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein, during time during which a process for a particular job is not being executed, the first arithmetic device is configured to execute a process for another job different from the particular job.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The distributed processing system according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the memory areas for processing a plurality of jobs, respectively, are assigned to one interconnect device.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The distributed processing system according to claim ii, wherein the memory areas for processing the plurality of jobs, respectively, are assigned to the one interconnect device by time division.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The distributed processing system according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the distributed node is configured to select direct memory access to be executed from among direct memory accesses for a plurality of jobs, according to priorities of the plurality of jobs, each of the direct memory accesses being executed at least between interconnect devices, between arithmetic devices, or between an interconnect device and an arithmetic device.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The distributed processing system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein<claim-text>data transferred by the direct memory access has an identifier that is different for each of the plurality of jobs; and</claim-text><claim-text>the interconnect device is configured to select data transferred by the direct memory access based on the identifier.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A distributed node comprising:<claim-text>a plurality of arithmetic devices; and</claim-text><claim-text>an interconnect device, wherein in the interconnect device or the plurality of arithmetic devices, one or more memory areas are configured to be assigned to each job to be processed by a distributed processing system, wherein direct memory access between memories for processing a job is executed at least between interconnect devices, between arithmetic devices, or between an interconnect device and an arithmetic device, wherein the distributed node is a part of the distributed processing system.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The distributed node according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the memory areas for processing a plurality of jobs, respectively, are assigned to a first arithmetic device of the arithmetic devices.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The distributed node according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein, during time during which a process for a particular job is not being executed, the first arithmetic device is configured to execute a process for another job different from the particular job.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The distributed node according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the memory areas for processing a plurality of jobs, respectively, are assigned to one interconnect device.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The distributed node according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the memory areas for processing the plurality of jobs, respectively, are assigned to the one interconnect device by time division.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The distributed node according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the distributed node is configured to select direct memory access to be executed from among direct memory accesses for a plurality of jobs, according to priorities of the plurality of jobs, each of the direct memory accesses being executed at least between interconnect devices, between arithmetic devices, or between an interconnect device and an arithmetic device.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The distributed node according to <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein:<claim-text>data transferred by the direct memory access has an identifier that is different for each of the plurality of jobs; and</claim-text><claim-text>the interconnect device is configured to select data transferred by the direct memory access based on the identifier.</claim-text></claim-text></claim></claims></us-patent-application>