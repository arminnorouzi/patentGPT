<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004790A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004790</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17944454</doc-number><date>20220914</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2019-0010293</doc-number><date>20190128</date></priority-claim><priority-claim sequence="02" kind="national"><country>KR</country><doc-number>10-2019-0034583</doc-number><date>20190326</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>5027</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30145</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e79">NEURAL NETWORK ACCELERATOR AND OPERATING METHOD THEREOF</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16751503</doc-number><date>20200124</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11475285</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17944454</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SAMSUNG ELECTRONICS CO., LTD.</orgname><address><city>Suwon-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KANG</last-name><first-name>Seokhyeong</first-name><address><city>Pohang-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>KANG</last-name><first-name>Yesung</first-name><address><city>Ulsan</city><country>KR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>KIM</last-name><first-name>Sunghoon</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>PARK</last-name><first-name>Yoonho</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>SAMSUNG ELECTRONICS CO., LTD.</orgname><role>03</role><address><city>Suwon-si</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A neural network accelerator includes an operator that calculates a first operation result based on a first tiled input feature map and first tiled filter data, a quantizer that generates a quantization result by quantizing the first operation result based on a second bit width extended compared with a first bit width of the first tiled input feature map, a compressor that generates a partial sum by compressing the quantization result, and a decompressor that generates a second operation result by decompressing the partial sum, the operator calculates a third operation result based on a second tiled input feature map, second tiled filter data, and the second operation result, and an output feature map is generated based on the third operation result.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="90.51mm" wi="158.75mm" file="US20230004790A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="101.52mm" wi="161.29mm" file="US20230004790A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="215.98mm" wi="151.47mm" orientation="landscape" file="US20230004790A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="229.53mm" wi="145.29mm" orientation="landscape" file="US20230004790A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="200.32mm" wi="161.04mm" orientation="landscape" file="US20230004790A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="247.57mm" wi="132.25mm" orientation="landscape" file="US20230004790A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="211.33mm" wi="140.21mm" orientation="landscape" file="US20230004790A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="227.50mm" wi="170.10mm" orientation="landscape" file="US20230004790A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority under 35 U.S.C. &#xa7; 119 to Korean Patent Application Nos. 10-2019-0010293, filed on Jan. 28, 2019, and 10-2019-0034583 filed on Mar. 26, 2019, in the Korean Intellectual Property Office, the disclosures of which are incorporated by reference herein in their entireties.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Embodiments of the inventive concept described herein relate to a semiconductor device, and more particularly, relate to a neural network accelerator operating based on channel loop tiling of a neural network and an operating method thereof.</p><p id="p-0004" num="0003">A convolutional neural network (CNN) is widely used in an artificial intelligence (AI) application field, in particular, in image recognition, detection, and segmentation. The accuracy of CNN is improved, but a network size, the number of times of calculation, and a memory usage are increased. Even though there are a lot of attempts to reduce the network size, a high-tech CNN requires a memory of tens to hundreds megabytes. An on-chip memory capacity of a neural network accelerator is increasing but is still very small to handle the CNN.</p><p id="p-0005" num="0004">Loop tiling may be performed such that the CNN is capable of being implemented on a hardware platform. The loop tiling means segmenting the CNN into a plurality of blocks. In the case where the CNN is tiled, the segmented CNN block may be accommodated in the on-chip memory. That is, the loop tiling may allow a relatively large CNN to be implemented at the neural network accelerator. However, the loop tiling may cause repeated data movement between an on-chip memory and an off-chip memory. An increase in the data movement may cause a bottleneck phenomenon, thereby reducing a data throughput and an energy efficiency. Also, in the case where an error occurs as a channel loop is tiled, the accuracy of calculation of the neural network accelerator may decrease.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">Embodiments of the inventive concept provide a neural network accelerator for improving the accuracy of calculation which decreases due to an error according to channel loop tiling and an operating method thereof.</p><p id="p-0007" num="0006">According to an exemplary embodiment, a neural network accelerator includes an operator that calculates a first operation result based on a first tiled input feature map and first tiled filter data, a quantizer that generates a quantization result by quantizing the first operation result based on a second bit width extended compared with a first bit width of the first tiled input feature map, a compressor that generates a partial sum by compressing the quantization result, and a decompressor that generates a second operation result by decompressing the partial sum, the operator calculates a third operation result based on a second tiled input feature map, second tiled filter data, and the second operation result, and an output feature map is generated based on the third operation result.</p><p id="p-0008" num="0007">In an embodiment, the operator may include a multiplier that generates a multiplication result by multiplying the second tiled input feature map and the second tiled filter data, and an accumulator that generates the third operation result by adding the multiplication result and the second operation result.</p><p id="p-0009" num="0008">In an embodiment, the quantizer may quantize the first operation result through a round-off.</p><p id="p-0010" num="0009">In an embodiment, the quantization result may include a sign bit and remaining bits composed of an integer part and a fractional part, and a bit width of at least one of the integer part and the fractional part may be extended depending on the second bit width.</p><p id="p-0011" num="0010">In an embodiment, the compressor may include absolute value generation logic that generates an absolute value of partial bits of the remaining bits of the quantization result, and a run-length encoder that generates compression bits by performing run-length encoding based on the generated absolute value.</p><p id="p-0012" num="0011">In an embodiment, the partial bits may be selected in the order from an upper bit to a lower bit of the remaining bits based on a difference between the first bit width and the second bit width.</p><p id="p-0013" num="0012">In an embodiment, the partial sum may include the compression bits generated from the run-length encoder and remaining bits of the quantization result other than the partial bits.</p><p id="p-0014" num="0013">In an embodiment, the compressor may store the generated partial sum in an external memory, and the decompressor may receive the stored partial sum from the external memory.</p><p id="p-0015" num="0014">In an embodiment, the quantizer may generate the output feature map by quantizing the third operation result based on the first bit width.</p><p id="p-0016" num="0015">According to an exemplary embodiment, an operation method of a neural network accelerator which operates based on channel loop tiling includes generating a first operation result based on a first tiled input feature map and first tiled filter data, generating a first quantization result by quantizing the first operation result based on a second bit width extended compared with a first bit width of the first tiled input feature map, generating a first partial sum by compressing the first quantization result, storing the generated first partial sum in an external memory, and generating a second operation result based on a second tiled input feature map, second tiled filter data, and the first partial sum provided from the external memory.</p><p id="p-0017" num="0016">In an embodiment, the generating of the second operation result may include generating a multiplication result by multiplying the second tiled input feature map and the second tiled filter data, and generating the second operation result by adding the multiplication result and the first operation result.</p><p id="p-0018" num="0017">In an embodiment, the method may further include, when the second tiled input feature map and the second tiled filter data are not data lastly received, generating a second quantization result by quantizing the second operation result based on the second bit width, generating a second partial sum by compressing the second quantization result, storing the generated second partial sum in the external memory, and generating a third operation result based on a third tiled input feature map, third tiled filter data, and the second partial sum provided from the external memory.</p><p id="p-0019" num="0018">In an embodiment, the method may further include, when the second tiled input feature map and the second tiled filter data are data lastly received, generating a third quantization result by quantizing the second operation result based on the first bit width, and generating an output feature map based on the third quantization result.</p><p id="p-0020" num="0019">In an embodiment, the first quantization result may include a sign bit and remaining bits composed of an integer part and a fractional part, and a bit width of at least one of the integer part and the fractional part may be extended depending on the second bit width.</p><p id="p-0021" num="0020">In an embodiment, the generating of the first partial sum may include generating an absolute value of partial bits of the remaining bits of the first quantization result, and performing run-length encoding based on the generated absolute value.</p><p id="p-0022" num="0021">In an embodiment, the partial bits may be selected in the order from an upper bit to a lower bit of the remaining bits based on a difference between the first bit width and the second bit width.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading><p id="p-0023" num="0022">The above and other objects and features of the inventive concept will become apparent by describing in detail exemplary embodiments thereof with reference to the accompanying drawings.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of an accelerator system according to an embodiment of the inventive concept.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example in which a neural network accelerator of <figref idref="DRAWINGS">FIG. <b>1</b></figref> generates an output feature map.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a block diagram of a neural network accelerator of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of operations of a quantizer and a compressor of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating an example of an effect of bit width extension according to an embodiment of the inventive concept.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating an example of an effect of absolute value generation according to an embodiment of the inventive concept.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating an example of an operation of a neural network accelerator according to an embodiment of the inventive concept.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0031" num="0030">Hereinafter, embodiments of the inventive concept are described in detail with reference to the accompanying drawings. In the following description, specific details such as detailed components and structures are merely provided to assist the overall understanding of the embodiments of the inventive concept. Therefore, it should be apparent to those skilled in the art that various changes and modifications of the embodiments described herein may be made without departing from the scope and spirit of the present invention. In addition, descriptions of well-known functions and structures are omitted for clarity and conciseness. The terms described below are terms defined in consideration of the functions in the inventive concept and are not limited to a specific function. The definitions of the terms should be determined based on the contents throughout the specification.</p><p id="p-0032" num="0031">In the following drawings or in the detailed description, modules may be connected with any other components other than components illustrated in drawings below or described in the detailed description. Modules or components may be connected directly or indirectly. Modules or components may be connected through communication or may be physically connected.</p><p id="p-0033" num="0032">Components that are described in the detailed description with reference to the terms &#x201c;unit&#x201d;, &#x201c;module&#x201d;, &#x201c;layer&#x201d;, &#x201c;logic&#x201d;, etc. will be implemented with software, hardware, or a combination thereof. For example, the software may be a machine code, firmware, an embedded code, and application software. For example, the hardware may be implemented with an electrical circuit, an electronic circuit, a processor, a computer, an integrated circuit, integrated circuit cores, a pressure sensor, an inertial sensor, a microelectromechanical system (MEMS), a passive element, or a combination thereof.</p><p id="p-0034" num="0033">Unless otherwise defined, all terms used herein, which include technical terminologies or scientific terminologies, have the same meaning as that understood by a person skilled in the art to which the present invention belongs. Terms defined in a generally used dictionary are to be interpreted to have meanings equal to the contextual meanings in a relevant technical field, and are not interpreted to have ideal or excessively formal meanings unless clearly defined in the specification.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of an accelerator system according to an embodiment of the inventive concept. An accelerator system <b>1000</b> may operate based on the CNN. For example, the accelerator system <b>1000</b> may generate an output feature map OFM based on an input feature map and filter (or weight/kernel) data.</p><p id="p-0036" num="0035">In the case where the size of the input feature map and the filter data are large, the accelerator system <b>1000</b> may perform channel loop tiling. The input feature map and filter data may be segmented into a plurality of blocks depending on the channel loop tiling. The accelerator system <b>1000</b> may sequentially process the segmented blocks of the input feature map and the segmented blocks of the filter data and may generate the output feature map OFM. Below, one segmented block of the input feature map is referred to as a &#x201c;tiled input feature map TIFM&#x201d;, and one segmented block of the filter data is referred to as &#x201c;tiled filter data TFD&#x201d;. As such, the accelerator system <b>1000</b> may generate the output feature map OFM based on the tiled input feature map TIFM and the tiled filter data TFD.</p><p id="p-0037" num="0036">The accelerator system <b>1000</b> includes a neural network accelerator <b>100</b>, a first memory <b>200</b>, a second memory <b>300</b>, and a processor <b>400</b>. The neural network accelerator <b>100</b> may operate based on a control signal CTRL from the processor <b>400</b>. The neural network accelerator <b>100</b> may generate the output feature map OFM in response to the control signal CTRL.</p><p id="p-0038" num="0037">In detail, the neural network accelerator <b>100</b> may receive the tiled input feature map TIFM and the tiled filter data TFD from the first memory <b>200</b>. The neural network accelerator <b>100</b> may generate a partial sum PS based on the tiled input feature map TIFM and the tiled filter data TFD. Here, the partial sum PS may be data that are calculated by performing a convolution operation on the input feature map and the filter data. The neural network accelerator <b>100</b> may store the generated partial sum PS in the second memory <b>300</b>. Afterwards, in the case where a new tiled input feature map TIFM and a new tiled filter data TFD are provided from the first memory <b>200</b>, the neural network accelerator <b>100</b> may generate a new partial sum PS based on the tiled input feature map TIFM, the tiled filter data TFD, and a previous partial sum PPS provided from the second memory <b>300</b>. Here, the previous partial sum PPS may be data identical to the partial sum PS stored in the second memory <b>300</b>. Alternatively, the neural network accelerator <b>100</b> may generate the output feature map OFM based on the tiled input feature map TIFM, the tiled filter data TFD, and the previous partial sum PPS.</p><p id="p-0039" num="0038">The first memory <b>200</b> may store the input feature map and the filter data. In this case, the input feature map and the filter data may be stored in the form of being tiled by the processor <b>400</b>. The first memory <b>200</b> may provide the tiled input feature map TIFM and the tiled filter data TFD to the neural network accelerator <b>100</b>.</p><p id="p-0040" num="0039">The second memory <b>300</b> may store the partial sum PS provided from the neural network accelerator <b>100</b>. The second memory <b>300</b> may output the stored partial sum PS in response to a request of the neural network accelerator <b>100</b> so as to be provided to the neural network accelerator <b>100</b> as the previous partial sum PPS. The second memory <b>300</b> may store the output feature map OFM provided from the neural network accelerator <b>100</b>. The output feature map OFM may be used as the input feature map at another layer of the CNN. An example is illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> as the first memory <b>200</b> and the second memory <b>300</b> are independent of each other, but the inventive concept is not limited thereto. The first memory <b>200</b> and the second memory <b>300</b> may be implemented with one memory.</p><p id="p-0041" num="0040">For example, each of the first memory <b>200</b> and the second memory <b>300</b> may be implemented with a volatile memory such as a register, a static random access memory (SRAM) or a dynamic random access memory (DRAM), or a nonvolatile memory such as a read only memory (ROM), a flash memory, a resistance random access memory (ReRAM) or a magnetic random access memory (MRAM).</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example in which the neural network accelerator <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> generates an output feature map. Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an input feature map may be segmented into a first tiled input feature map TIFM1 and a second tiled input feature map TIFM2, and filter data may be segmented into first tiled filter data TFD1 and second tiled filter data TFD2.</p><p id="p-0043" num="0042">In the case where the first tiled input feature map TIFM1 and the first tiled filter input TFD1 are provided, the neural network accelerator <b>100</b> may generate the partial sum PS based on the first tiled input feature map TIFM1 and the first tiled filter data TFD1. The generated partial sum PS may be stored in an external memory (e.g., the second memory <b>300</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0044" num="0043">Afterwards, in the case where the second tiled input feature map TIFM2 and the second tiled filter input TFD2 are provided, the neural network accelerator <b>100</b> may generate the output feature map OFM based on the second tiled input feature map TIFM2, the second tiled filter data TFD2, and the previous partial sum PPS from the external memory. In this case, the previous partial sum PPS may be data identical to the partial sum PS.</p><p id="p-0045" num="0044">The description is given with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref> as the output feature map OFM is generated from the firstly generated partial sum PS, but the inventive concept is not limited thereto. For example, in the case where an input feature map is segmented into two tiled input feature maps, the neural network accelerator <b>100</b> may generate a second partial sum PS based on a first partial sum PS. The neural network accelerator <b>100</b> may generate the output feature map OFM based on the second partial sum PS. That is, the neural network accelerator <b>100</b> may stepwise generate the partial sum PS depending on a tiling count and may generate the output feature map OFM based on the partial sum PS thus generated.</p><p id="p-0046" num="0045">As described above, the accelerator system <b>1000</b> may tile an input feature map and filter data based on the channel loop tiling. In the case where the input feature map and the filter data are tiled, even though the size of the input feature map and the filter data is large, the input feature map and the filter data may be accommodated in the on-chip memory of the neural network accelerator <b>100</b>. In the case where the output feature map OFM is generated based on the channel loop tiling, the partial sum PS may be generated from the neural network accelerator <b>100</b>, and the generated partial sum PS may be stored in an external memory. The partial sum PS stored in the external memory may be provided to the neural network accelerator <b>100</b> as the previous partial sum PPS for the purpose of generating the output feature map OFM. As such, there may be required a space of the external memory for storing the partial sum PS, and there may occur communication overhead according to the exchange of the partial sum PS between the neural network accelerator <b>100</b> and the external memory.</p><p id="p-0047" num="0046">The neural network accelerator <b>100</b> according to an embodiment of the inventive concept may minimize a necessary space of the external memory for storing the partial sum PS. As such, an increase in the communication overhead between the neural network accelerator <b>100</b> and the external memory may be minimized.</p><p id="p-0048" num="0047">Also, in the case where the partial sum PS is generated due to the channel loop tiling, quantization and compression may be performed to reduce a memory usage and communication overhead. In this case, the quantization and compression may cause a decrease in the accuracy of calculation of the neural network accelerator <b>100</b>. The neural network accelerator <b>100</b> according to an embodiment of the inventive concept may restore the accuracy of calculation that is reduced as an input feature map and filter data are tiled.</p><p id="p-0049" num="0048">Below, an operation of the neural network accelerator <b>100</b> will be more fully described with reference to <figref idref="DRAWINGS">FIGS. <b>3</b> to <b>7</b></figref>.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a block diagram of a neural network accelerator of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the neural network accelerator <b>100</b> may include an operator <b>110</b>, a quantizer <b>120</b>, a compressor <b>130</b>, and a decompressor <b>140</b>. The operator <b>110</b> may include a multiplier <b>111</b> and an accumulator <b>112</b>.</p><p id="p-0051" num="0050">The operator <b>110</b> may generate an operation result OR based on the tiled input feature map TIFM, the tiled filter data TFD, and a previous operation result POR from the decompressor <b>140</b>. In the case where the tiled input feature map TIFM and the tiled filter data TFD are provided for the first time, because the previous operation result POR does not exist, the operator <b>110</b> may generate the operation result OR based on the tiled input feature map TIFM and the tiled filter data TFD.</p><p id="p-0052" num="0051">In detail, the multiplier <b>111</b> may multiply the tiled input feature map TIFM and the tiled filter data TFD to generate a multiplication result MR. The accumulator <b>112</b> may add the multiplication result MR and the previous operation result POR to generate the operation result OR. In the case where the previous operation result POR does not exist, the accumulator <b>112</b> may output the multiplication result MR as the operation result OR.</p><p id="p-0053" num="0052">A bit width of the multiplier <b>111</b> and the accumulator <b>112</b> may be set depending on a bit width of each of the tiled input feature map TIFM and the tiled filter data TFD. For example, in the case where a bit width of each of the tiled input feature map TIFM and the tiled filter data TFD is 8-bit, the bit width of the multiplier <b>111</b> may be set to perform 8-bit multiplication. In the case where a 16-bit multiplication result MR is output from the multiplier <b>111</b> as a result of the 8-bit multiplication, the bit width of the accumulator <b>112</b> may be set to perform 16-bit addition. That is, the bit width of the multiplier <b>111</b> and the accumulator <b>112</b> may be set to a range capable of preventing an overflow.</p><p id="p-0054" num="0053">The quantizer <b>120</b> may receive the operation result OR from the operator <b>110</b>. The quantizer <b>120</b> may quantize the operation result OR based on a given bit width or based on an extended bit width. Here, the given bit width and the extended bit width may be smaller than the bit width of the operation result OR, and the extended bit width may be greater than the given bit width. The given bit width may be identical to the bit width of the tiled input feature map TIFM or the bit width of the output feature map OFM. In this case, the bit width of the tiled input feature map TIFM may be identical to the bit width of the output feature map OFM. For example, the bit width of the operation result OR may be 16-bit, the given bit width may be 8-bit, and the extended bit width may be 10-bit.</p><p id="p-0055" num="0054">For example, to generate the output feature map OFM, in the case where the last tiled input feature map TIFM and the last tiled filter data TFD are provided, the quantizer <b>120</b> may quantize the operation result OR based on the given bit width to generate a quantization result QR. In this case, the output feature map OFM may be generated based on the quantization result QR. In the case where the tiled input feature map TIFM and the tiled filter data TFD provided to the quantizer <b>120</b> are not the last tiled input feature map TIFM and the last tiled filter data TFD, the quantizer <b>120</b> may quantize the operation result OR based on the extended bit width to generate the quantization result QR.</p><p id="p-0056" num="0055">A quantization operation of the quantizer <b>120</b> will be more fully described with reference to <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref>.</p><p id="p-0057" num="0056">The compressor <b>130</b> may compress the quantization result QR to generate the partial sum PS. In this case, the number of bits of the partial sum PS may be less than the number of bits of the quantization result QR. In the compression, the compressor <b>130</b> may divide bits of the quantization result QR into partial bits and remaining bits RB. The compressor <b>130</b> may perform encoding on the partial bits to generate compression bits CTB. For example, the compressor <b>130</b> may perform run-length encoding on the partial bits. As such, the partial sum PS generated from the compressor <b>130</b> may include the remaining bits RB and the compression bits CTB.</p><p id="p-0058" num="0057">A compression operation of the compressor <b>130</b> will be more fully described with reference to <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>6</b></figref>.</p><p id="p-0059" num="0058">The decompressor <b>140</b> may decompress the previous partial sum PPS to generate the previous operation result POR. Because the partial sum PS includes the remaining bits RB and the compression bits CTB, the previous partial sum PPS may include previous remaining bits PRB and previous compression bits PCTB. In this case, the previous remaining bits PRB may be identical to the remaining bits RB, and the previous compression bits PCTB may be identical to the compression bits CTB.</p><p id="p-0060" num="0059">The decompressor <b>140</b> may decompress the previous partial sum PPS based on a compression manner of the compressor <b>130</b>. For example, the decompressor <b>140</b> may perform run-length decoding on the previous compression bits PCTB to generate partial bits. The decompressor <b>140</b> may combine the generated partial bits and the previous remaining bits PRB to generate the previous operation result POR. In this case, the decompressor <b>140</b> may generate the previous operation result POR having the same bit width as the bit width of the multiplication result MR.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of operations of a quantizer and a compressor of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Referring to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>, the operation result OR generated from the operator <b>110</b> may include a sign bit &#x201c;s&#x201d; and remaining bits db. For example, the operation result OR may be expressed by a fixed point. In this case, the remaining bits db may be composed of an integer part and a fractional part. The integer part may indicate a range of a number that the operation result OR indicates, and the fractional part may indicate a precision.</p><p id="p-0062" num="0061">The quantizer <b>120</b> may quantize the operation result OR through bit extension and round-off. The quantizer <b>120</b> may increase a bit width so as to be greater than the given bit width as much as an additional bit width and may quantize the operation result OR to the extended bit width. In this case, a bit width of the integer part may be increased, or a bit width of the fractional part may be increased. For example, the integer part may be extended as much as 2 bits. Alternatively, each of the integer part and the fractional part may be extended as much as one bit. In the case where the bit width is extended depending on the additional bit width, the accuracy of calculation may be increased. Accordingly, the additional bit width may vary depending on the required accuracy of calculation.</p><p id="p-0063" num="0062">In the case where the operation result OR is quantized, the quantization result QR may be generated. The quantization result QR may include a sign bit &#x201c;s&#x201d;, partial bits tb, and remaining bits rb. The compressor <b>130</b> may separate the partial bits tb from the quantization result QR. The compressor <b>130</b> may connect the sign bit &#x201c;s&#x201d; and the remaining bits rb to generate the remaining bits RB of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In this case the partial bits tb may be selected from bits other than the sign bit &#x201c;s&#x201d; in the order from an upper bit to a lower bit. A bit width of the partial bits tb may be identical to the additional bit width.</p><p id="p-0064" num="0063">The compressor <b>130</b> may generate an absolute value of the partial bits tb through absolute value generation logic <b>131</b>. The absolute value generation logic <b>131</b> may be included in the compressor <b>130</b>. For example, the absolute value generation logic <b>131</b> may calculate an absolute value of the partial bits tb by performing an exclusive OR (XOR) operation on the sign bit &#x201c;s&#x201d; and the partial bits tb. As such, absolute value bits avb may be generated.</p><p id="p-0065" num="0064">The compressor <b>130</b> may encode the absolute value bits avb through a run-length encoder <b>132</b> to generate the compression bits CTB. The run-length encoder <b>132</b> may be included in the compressor <b>130</b>. For example, in the case where the absolute value bits avb are &#x201c;00000&#x201d;, the run-length encoder <b>132</b> may generate &#x201c;0&#x201d; as a run value and may generate &#x201c;5&#x201d; as a length value. As such, the number of compression bits CTB thus generate may be less than the number of absolute value bits avb.</p><p id="p-0066" num="0065">The remaining bits RB and the compression bits CTB generated from the compressor <b>130</b> may be stored in an output buffer <b>150</b>. The output buffer <b>150</b> may be a memory in which the partial sum PS generated through the compressor <b>130</b> is temporarily stored before being provided to an external memory. That is, the neural network accelerator <b>100</b> may further include the output buffer <b>150</b>, but the inventive concept is not limited thereto.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating an example of an effect of bit width extension according to an embodiment of the inventive concept. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a horizontal axis represents the number of channel tiles, and a vertical axis represents the accuracy of calculation. Compared to the case where the bit width is not extended, the accuracy of calculation may be increased when the bit width is extended. Compared to the case where an integer part is extended as much as one bit, the accuracy of calculation may be increased when a fractional part is extended as much as one bit. Also, as the number of extended bits increases, the accuracy of calculation may increase.</p><p id="p-0068" num="0067">In the case where the bit width is not extended, the accuracy of calculation may decrease as the number of channel tiles increases. The reason is that a quantization procedure becomes more complicated as the number of channel tiles increases and an error occurs due to a decrease in the number of bits and the round-off. In the case where the bit width is extended, even though the number of channel tiles increases, the accuracy of calculation may be maintained. Accordingly, a decrease in the accuracy of calculation may be restored in the case where a bit width is extended in the quantization procedure according to an embodiment of the inventive concept.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating an example of an effect of absolute value generation according to an embodiment of the inventive concept. Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a horizontal axis represents a bit position of the quantization result QR, and a vertical axis represents the probability that each bit is a value of &#x201c;1&#x201d;. In detail, the quantization result QR that is generated through one-bit extension at each of the fractional part FP and the integer part IP is illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0070" num="0069">In the case where a sign bit is excluded, the probability that the most significant bit MSB to the least significant bit LSB of the quantization result QR all are a value of &#x201c;1&#x201d; may be about 45%. In contrast, the probability that a bit value of an absolute value of the quantization result QR is a value of &#x201c;1&#x201d; as it goes toward an upper bit may decrease. That is, in the case of generating an absolute value from upper bits of the quantization result QR, the probability that continuous bits each having a value of &#x201c;0&#x201d; are generated may be high.</p><p id="p-0071" num="0070">As such, the compressor <b>130</b> may select partial bits tb in the order from an upper bit to a lower bit of the quantization result QR and may generate an absolute value of the partial bits tb. Accordingly, the absolute value bits avb may include continuous bits each having a value of &#x201c;0&#x201d;. In this case, the efficiency of compression according to the run-length encoding may be improved. In the case where the compression efficiency is improved, a usage of the external memory storing the partial sum PS may be decreased, and the communication overhead between the external memory and the neural network accelerator <b>100</b> may be decreased.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating an example of an operation of a neural network accelerator according to an embodiment of the inventive concept. Referring to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>7</b></figref>, in operation S<b>101</b>, the neural network accelerator <b>100</b> may receive the tiled input feature map TIFM and the tiled filter data TFD. In operation S<b>102</b>, the neural network accelerator <b>100</b> may determine whether the received data are data received for the first time. When the received data are data received for the first time, in operation S<b>103</b>, the neural network accelerator <b>100</b> may generate the operation result OR based on the tiled input feature map TIFM and the tiled filter data TFD. For example, the operation result OR may be the multiplication result MR of the tiled input feature map TIFM and the tiled filter data TFD.</p><p id="p-0073" num="0072">In operation S<b>104</b>, the neural network accelerator <b>100</b> may quantize the operation result OR depending on the extended bit width. In this case, the neural network accelerator <b>100</b> may quantize the operation result OR through the round-off. The extended bit width may be in advance determined in consideration of the required accuracy of calculation.</p><p id="p-0074" num="0073">In operation S<b>105</b>, the neural network accelerator <b>100</b> may compress the quantization result QR to generate the partial sum PS. As described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the neural network accelerator <b>100</b> may generate an absolute value of partial bits of the quantization result QR and may compress the quantization result QR by performing the run-length encoding on bits of the absolute value thus generated.</p><p id="p-0075" num="0074">In operation S<b>106</b>, the neural network accelerator <b>100</b> may store the generated partial sum PS in an external memory. Afterwards, in operation S<b>101</b>, the neural network accelerator <b>100</b> may receive the tiled input feature map TIFM and the tiled filter data TFD. In this case, the received tiled input feature map TIFM and the received tiled filter data TFD may be different from the data received for the first time.</p><p id="p-0076" num="0075">When the received data are not the data received for the first time, in operation S<b>107</b>, the neural network accelerator <b>100</b> may generate the operation result OR based on the tiled input feature map TIFM, the tiled filter data TFD, and the partial sum PS from the external memory. In this case, the partial sum PS from the external memory may be provided to the neural network accelerator <b>100</b> as the previous partial sum PPS.</p><p id="p-0077" num="0076">In operation S<b>108</b>, the neural network accelerator <b>100</b> may determine whether the tiled input feature map TIFM and the tiled filter data TFD are data lastly received. When the received data are not the data lastly received, the neural network accelerator <b>100</b> may generate the partial sum PS through operation S<b>104</b> to operation S<b>106</b> and may store the generated partial sum PS in the external memory.</p><p id="p-0078" num="0077">When the received data are the data lastly received, in operation S<b>109</b>, the neural network accelerator <b>100</b> may output the output feature map OFM based on the operation result OR. For example, the neural network accelerator <b>100</b> may generate the output feature map OFM by quantizing the operation result OR depending on a given bit width. The generated output feature map OFM may be stored in the external memory.</p><p id="p-0079" num="0078">According to the inventive concept, there may be provided a neural network accelerator restoring the loss of the accuracy of calculation decreased due to an error occurring due to the channel loop tiling.</p><p id="p-0080" num="0079">Also, the neural network accelerator according to the inventive concept may minimize a usage of an external memory and may minimize the overhead of communication with the external memory.</p><p id="p-0081" num="0080">While the inventive concept has been described with reference to exemplary embodiments thereof, it will be apparent to those of ordinary skill in the art that various changes and modifications may be made thereto without departing from the spirit and scope of the inventive concept as set forth in the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A neural network accelerator comprising:<claim-text>an operator configured to calculate a first operation result based on a first tiled input feature map and first tiled filter data;</claim-text><claim-text>a quantizer configured to generate a quantization result by quantizing the first operation result based on a second bit width extended compared with a first bit width of the first tiled input feature map;</claim-text><claim-text>a compressor configured to generate a partial sum by compressing the quantization result; and</claim-text><claim-text>a decompressor configured to generate a second operation result by decompressing the partial sum.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The neural network accelerator of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operator calculates a third operation result based on a second tiled input feature map, second tiled filter data, and the second operation result, and<claim-text>wherein an output feature map is generated based on the third operation result.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The neural network accelerator of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the operator includes:<claim-text>a multiplier configured to generate a multiplication result by multiplying the second tiled input feature map and the second tiled filter data; and</claim-text><claim-text>an accumulator configured to generate the third operation result by adding the multiplication result and the second operation result.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The neural network accelerator of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the quantizer quantizes the first operation result through a round-off.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The neural network accelerator of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the quantization result includes a sign bit and remaining bits composed of an integer part and a fractional part, and<claim-text>wherein a bit width of at least one of the integer part and the fractional part is extended depending on the second bit width.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The neural network accelerator of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the compressor includes:<claim-text>absolute value generation logic configured to generate an absolute value of partial bits of the remaining bits of the quantization result; and</claim-text><claim-text>a run-length encoder configured to generate compression bits by performing run-length encoding based on the generated absolute value.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The neural network accelerator of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the partial bits are selected in the order from an upper bit to a lower bit of the remaining bits based on a difference between the first bit width and the second bit width.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The neural network accelerator of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the partial sum includes the compression bits generated from the run-length encoder and remaining bits of the quantization result other than the partial bits.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The neural network accelerator of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the compressor stores the generated partial sum in an external memory, and<claim-text>wherein the decompressor receives the stored partial sum from the external memory.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The neural network accelerator of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the quantizer generates the output feature map by quantizing the third operation result based on the first bit width.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An operation method of a neural network accelerator which operates based on channel loop tiling, the method comprising:<claim-text>generating a first operation result based on a first tiled input feature map and first tiled filter data;</claim-text><claim-text>generating a first quantization result by quantizing the first operation result based on a second bit width extended compared with a first bit width of the first tiled input feature map;</claim-text><claim-text>generating a first partial sum by compressing the first quantization result;</claim-text><claim-text>generating a second operation result by decompressing the partial sum.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the generating of the second operation result includes generating a second operation result based on a second tiled input feature map, second tiled filter data, and the first partial sum.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the generating of the second operation result includes:<claim-text>generating a multiplication result by multiplying the second tiled input feature map and the second tiled filter data; and</claim-text><claim-text>generating the second operation result by adding the multiplication result and the first operation result.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>when the second tiled input feature map and the second tiled filter data are not data lastly received,</claim-text><claim-text>generating a second quantization result by quantizing the second operation result based on the second bit width;</claim-text><claim-text>generating a second partial sum by compressing the second quantization result;</claim-text><claim-text>storing the generated second partial sum in the external memory; and</claim-text><claim-text>generating a third operation result based on a third tiled input feature map, third tiled filter data, and the second partial sum provided from the external memory.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>when the second tiled input feature map and the second tiled filter data are data lastly received,</claim-text><claim-text>generating a third quantization result by quantizing the second operation result based on the first bit width; and</claim-text><claim-text>generating an output feature map based on the third quantization result.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the first quantization result includes a sign bit and remaining bits composed of an integer part and a fractional part, and<claim-text>wherein a bit width of at least one of the integer part and the fractional part is extended depending on the second bit width.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the generating of the first partial sum includes:<claim-text>generating an absolute value of partial bits of the remaining bits of the first quantization result; and</claim-text><claim-text>performing run-length encoding based on the generated absolute value.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the partial bits are selected in the order from an upper bit to a lower bit of the remaining bits based on a difference between the first bit width and the second bit width.</claim-text></claim></claims></us-patent-application>