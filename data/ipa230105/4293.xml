<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004294A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004294</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17858820</doc-number><date>20220706</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>34</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>064</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3034</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3447</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>7</main-group><subgroup>005</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">DISK USAGE GROWTH PREDICTION SYSTEM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17153803</doc-number><date>20210120</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11449253</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17858820</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16355533</doc-number><date>20190315</date></document-id><parent-status>ABANDONED</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17153803</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62780156</doc-number><date>20181214</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Commvault Systems, Inc.</orgname><address><city>Tinton Falls</city><state>NJ</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>DWARAMPUDI</last-name><first-name>Bheemesh R.</first-name><address><city>Morganville</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>MISHRA</last-name><first-name>Vibhor</first-name><address><city>West Long Branch</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>BEDADALA</last-name><first-name>Pavan Kumar Reddy</first-name><address><city>Piscataway</city><state>NJ</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Certain embodiments described herein relate to an improved disk usage growth prediction system. In some embodiments, one or more components in an information management system can determine usage status data of a given storage device, perform a validation check on the usage status data using multiple prediction models, compare validation results of the multiple prediction models to identify the best performing prediction model, generate a disk usage growth prediction using the identified prediction model, and adjust the available space of the storage device according to the disk usage growth prediction.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="88.73mm" wi="158.75mm" file="US20230004294A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="163.75mm" wi="154.09mm" file="US20230004294A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="216.07mm" wi="158.24mm" file="US20230004294A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="228.35mm" wi="148.42mm" orientation="landscape" file="US20230004294A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="186.27mm" wi="160.10mm" file="US20230004294A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="233.51mm" wi="160.78mm" file="US20230004294A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="231.73mm" wi="144.36mm" orientation="landscape" file="US20230004294A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="231.22mm" wi="142.32mm" file="US20230004294A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="215.14mm" wi="155.96mm" orientation="landscape" file="US20230004294A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="201.17mm" wi="142.49mm" orientation="landscape" file="US20230004294A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="241.22mm" wi="158.83mm" orientation="landscape" file="US20230004294A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="239.10mm" wi="156.46mm" orientation="landscape" file="US20230004294A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="228.09mm" wi="137.75mm" file="US20230004294A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="228.09mm" wi="143.59mm" file="US20230004294A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="228.09mm" wi="130.47mm" file="US20230004294A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="228.09mm" wi="130.47mm" file="US20230004294A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="228.09mm" wi="155.19mm" file="US20230004294A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="196.77mm" wi="144.44mm" orientation="landscape" file="US20230004294A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">INCORPORATION BY REFERENCE TO ANY PRIORITY APPLICATIONS</heading><p id="p-0002" num="0001">This application a continuation of U.S. patent application Ser. No. 17/153,803, filed on Jan. 20, 2021 and titled &#x201c;DISK USAGE GROWTH PREDICTION SYSTEM,&#x201d; which is a continuation of U.S. patent application Ser. No. 16/355,533, filed on Mar. 15, 2019 and titled &#x201c;DISK USAGE GROWTH PREDICTION SYSTEM,&#x201d; which claims the benefit of U.S. Provisional Patent Application No. 62/780,156, filed Dec. 14, 2018, and entitled &#x201c;DISK USAGE GROWTH PREDICTION SYSTEM&#x201d; (attorney docket no. COMMV.426PR; applicant docket no. 100.621.USP1.125). Any and all applications, if any, for which a foreign or domestic priority claim is identified in the Application Data Sheet of the present application are hereby incorporated by reference in their entireties under 37 CFR 1.57.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">COPYRIGHT NOTICE</heading><p id="p-0003" num="0002">A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document and/or the patent disclosure as it appears in the United States Patent and Trademark Office patent file and/or records, but otherwise reserves all copyrights whatsoever.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Businesses recognize the commercial value of their data and seek reliable, cost-effective ways to protect the information stored on their computer networks while minimizing impact on productivity. A company might back up critical computing systems such as databases, file servers, web servers, virtual machines, and so on as part of a daily, weekly, or monthly maintenance schedule. The company may similarly protect computing systems used by its employees, such as those used by an accounting department, marketing department, engineering department, and so forth. Given the rapidly expanding volume of data under management, companies also continue to seek innovative techniques for managing data growth, for example by migrating data to lower-cost storage over time, reducing redundant data, pruning lower priority data, etc. Enterprises also increasingly view their stored data as a valuable asset and look for solutions that leverage their data. For instance, data analysis capabilities, information management, improved data presentation and access features, and the like, are in increasing demand.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">Enterprises can generate and manage stored data using an information management system. Such an information management system may include numerous storage devices that can store data generated by the enterprises. Modern day enterprises often encounter rapidly growth in the volume of data that they need to store. Thus, it is important for these enterprises to be able to monitor and manage their data growth and expand their storage capacity accordingly.</p><p id="p-0006" num="0005">However, storage devices in a given information management system often have different growth patterns and sometimes experience anomalous behaviors. Thus, it is often difficult to predict when the various storage devices will reach their respective maximum storage capacities. Without adequate disk usage growth prediction, system administrators run the risk of under-provisioning or over-provisioning storage devices in such information management systems.</p><p id="p-0007" num="0006">Certain embodiments described herein relate to an improved disk usage growth prediction system. In some embodiments, one or more components in the information management system can determine usage status data of a given storage device, perform a validation check on the usage status data using multiple prediction models, compare validation results of the multiple prediction models to identify the best performing prediction model, generate a disk usage growth prediction using the identified prediction model, and adjust the available space of the storage device according to the disk usage growth prediction.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a block diagram illustrating an exemplary information management system.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a detailed view of a primary storage device, a secondary storage device, and some examples of primary data and secondary copy data.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b>C</figref> is a block diagram of an exemplary information management system including a storage manager, one or more data agents, and one or more media agents.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b>D</figref> is a block diagram illustrating a scalable information management system.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b>E</figref> illustrates certain secondary copy operations according to an exemplary storage policy.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. <b>1</b>F-<b>1</b>H</figref> are block diagrams illustrating suitable data structures that may be employed by the information management system.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> illustrates a system and technique for synchronizing primary data to a destination such as a failover site using secondary copy data.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> illustrates an information management system architecture incorporating use of a network file system (NFS) protocol for communicating between the primary and secondary storage subsystems.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b>C</figref> is a block diagram of an example of a highly scalable managed data pool architecture.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating some salient portions of a system <b>300</b> for performing disk usage growth prediction, according to an illustrative embodiment of the present invention.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts some salient operations of a method <b>400</b> for performing disk usage growth prediction according to an illustrative embodiment of the present invention.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts some salient operations of a method <b>500</b> for updating disk usage growth prediction according to an illustrative embodiment of the present invention.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts some salient operations of a method <b>600</b> for adjusting backup storage based on disk usage growth prediction according to an illustrative embodiment of the present invention.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts some salient operations of a method <b>700</b> for adjusting deduplication storage based on disk usage growth prediction according to an illustrative embodiment of the present invention.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts some salient operations of a method <b>800</b> for adjusting index cache storage based on disk usage growth prediction according to an illustrative embodiment of the present invention.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts an illustrative graphical user interface showing an example disk usage growth prediction feature in the illustrative system.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0024" num="0023">Detailed descriptions and examples of systems and methods according to one or more illustrative embodiments of the present invention may be found in the section entitled Disk Usage Growth Prediction System, as well as in the section entitled Example Embodiments, and also in <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>9</b></figref> herein. Furthermore, components and functionality for performing disk usage growth prediction may be configured and/or incorporated into information management systems such as those described herein in <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H and <b>2</b>A-<b>2</b>C</figref>.</p><p id="p-0025" num="0024">Various embodiments described herein are intimately tied to, enabled by, and would not exist except for, computer technology. For example, disk usage growth prediction described herein in reference to various embodiments cannot reasonably be performed by humans alone, without the computer technology upon which they are implemented.</p><heading id="h-0007" level="2">Information Management System Overview</heading><p id="p-0026" num="0025">With the increasing importance of protecting and leveraging data, organizations simply cannot risk losing critical data. Moreover, runaway data growth and other modern realities make protecting and managing data increasingly difficult. There is therefore a need for efficient, powerful, and user-friendly solutions for protecting and managing data and for smart and efficient management of data storage. Depending on the size of the organization, there may be many data production sources which are under the purview of tens, hundreds, or even thousands of individuals. In the past, individuals were sometimes responsible for managing and protecting their own data, and a patchwork of hardware and software point solutions may have been used in any given organization. These solutions were often provided by different vendors and had limited or no interoperability. Certain embodiments described herein address these and other shortcomings of prior approaches by implementing scalable, unified, organization-wide information management, including data storage management.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> shows one such information management system <b>100</b> (or &#x201c;system <b>100</b>&#x201d;), which generally includes combinations of hardware and software configured to protect and manage data and metadata that are generated and used by computing devices in system <b>100</b>. System <b>100</b> may be referred to in some embodiments as a &#x201c;storage management system&#x201d; or a &#x201c;data storage management system.&#x201d; System <b>100</b> performs information management operations, some of which may be referred to as &#x201c;storage operations&#x201d; or &#x201c;data storage operations,&#x201d; to protect and manage the data residing in and/or managed by system <b>100</b>. The organization that employs system <b>100</b> may be a corporation or other business entity, non-profit organization, educational institution, household, governmental agency, or the like.</p><p id="p-0028" num="0027">Generally, the systems and associated components described herein may be compatible with and/or provide some or all of the functionality of the systems and corresponding components described in one or more of the following U.S. patents/publications and patent applications assigned to Commvault Systems, Inc., each of which is hereby incorporated by reference in its entirety herein:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0028">U.S. Pat. No. 7,035,880, entitled &#x201c;Modular Backup and Retrieval System Used in Conjunction With a Storage Area Network&#x201d;;</li>    <li id="ul0001-0002" num="0029">U.S. Pat. No. 7,107,298, entitled &#x201c;System And Method For Archiving Objects In An Information Store&#x201d;;</li>    <li id="ul0001-0003" num="0030">U.S. Pat. No. 7,246,207, entitled &#x201c;System and Method for Dynamically Performing Storage Operations in a Computer Network&#x201d;;</li>    <li id="ul0001-0004" num="0031">U.S. Pat. No. 7,315,923, entitled &#x201c;System And Method For Combining Data Streams In Pipelined Storage Operations In A Storage Network&#x201d;;</li>    <li id="ul0001-0005" num="0032">U.S. Pat. No. 7,343,453, entitled &#x201c;Hierarchical Systems and Methods for Providing a Unified View of Storage Information&#x201d;;</li>    <li id="ul0001-0006" num="0033">U.S. Pat. No. 7,395,282, entitled &#x201c;Hierarchical Backup and Retrieval System&#x201d;;</li>    <li id="ul0001-0007" num="0034">U.S. Pat. No. 7,529,782, entitled &#x201c;System and Methods for Performing a Snapshot and for Restoring Data&#x201d;;</li>    <li id="ul0001-0008" num="0035">U.S. Pat. No. 7,617,262, entitled &#x201c;System and Methods for Monitoring Application Data in a Data Replication System&#x201d;;</li>    <li id="ul0001-0009" num="0036">U.S. Pat. No. 7,734,669, entitled &#x201c;Managing Copies Of Data&#x201d;;</li>    <li id="ul0001-0010" num="0037">U.S. Pat. No. 7,747,579, entitled &#x201c;Metabase for Facilitating Data Classification&#x201d;;</li>    <li id="ul0001-0011" num="0038">U.S. Pat. No. 8,156,086, entitled &#x201c;Systems And Methods For Stored Data Verification&#x201d;;</li>    <li id="ul0001-0012" num="0039">U.S. Pat. No. 8,170,995, entitled &#x201c;Method and System for Offline Indexing of Content and Classifying Stored Data&#x201d;;</li>    <li id="ul0001-0013" num="0040">U.S. Pat. No. 8,230,195, entitled &#x201c;System And Method For Performing Auxiliary Storage Operations&#x201d;;</li>    <li id="ul0001-0014" num="0041">U.S. Pat. No. 8,285,681, entitled &#x201c;Data Object Store and Server for a Cloud Storage Environment, Including Data Deduplication and Data Management Across Multiple Cloud Storage Sites&#x201d;;</li>    <li id="ul0001-0015" num="0042">U.S. Pat. No. 8,307,177, entitled &#x201c;Systems And Methods For Management Of Virtualization Data&#x201d;;</li>    <li id="ul0001-0016" num="0043">U.S. Pat. No. 8,364,652, entitled &#x201c;Content-Aligned, Block-Based Deduplication&#x201d;;</li>    <li id="ul0001-0017" num="0044">U.S. Pat. No. 8,578,120, entitled &#x201c;Block-Level Single Instancing&#x201d;;</li>    <li id="ul0001-0018" num="0045">U.S. Pat. No. 8,954,446, entitled &#x201c;Client-Side Repository in a Networked Deduplicated Storage System&#x201d;;</li>    <li id="ul0001-0019" num="0046">U.S. Pat. No. 9,020,900, entitled &#x201c;Distributed Deduplicated Storage System&#x201d;;</li>    <li id="ul0001-0020" num="0047">U.S. Pat. No. 9,098,495, entitled &#x201c;Application-Aware and Remote Single Instance Data Management&#x201d;;</li>    <li id="ul0001-0021" num="0048">U.S. Pat. No. 9,239,687, entitled &#x201c;Systems and Methods for Retaining and Using Data Block Signatures in Data Protection Operations&#x201d;;</li>    <li id="ul0001-0022" num="0049">U.S. Pat. Pub. No. 2006/0224846, entitled &#x201c;System and Method to Support Single Instance Storage Operations&#x201d;;</li>    <li id="ul0001-0023" num="0050">U.S. Pat. Pub. No. 2014/0201170, entitled &#x201c;High Availability Distributed Deduplicated Storage System&#x201d;;</li>    <li id="ul0001-0024" num="0051">U.S. Pat. Pub. No. 2016/0350391, entitled &#x201c;Replication Using Deduplicated Secondary Copy Data&#x201d;;</li>    <li id="ul0001-0025" num="0052">U.S. Patent Application Pub. No. 2017/0168903 entitled &#x201c;Live Synchronization and Management of Virtual Machines across Computing and Virtualization Platforms and Using Live Synchronization to Support Disaster Recovery&#x201d;;</li>    <li id="ul0001-0026" num="0053">U.S. Patent Application Pub. No. 2017/0193003 entitled &#x201c;Redundant and Robust Distributed Deduplication Data Storage System&#x201d;;</li>    <li id="ul0001-0027" num="0054">U.S. Patent Application Pub. No. 2017/0235647 entitled &#x201c;Data Protection Operations Based on Network Path Information&#x201d;;</li>    <li id="ul0001-0028" num="0055">U.S. Patent Application Pub. No. 2017/0242871, entitled &#x201c;Data Restoration Operations Based on Network Path Information&#x201d;; and</li>    <li id="ul0001-0029" num="0056">U.S. Patent Application Pub. No. 2017/0185488, entitled &#x201c;Application-Level Live Synchronization Across Computing Platforms Including Synchronizing Co-Resident Applications To Disparate Standby Destinations And Selectively Synchronizing Some Applications And Not Others&#x201d;.</li></ul></p><p id="p-0029" num="0057">System <b>100</b> includes computing devices and computing technologies. For instance, system <b>100</b> can include one or more client computing devices <b>102</b> and secondary storage computing devices <b>106</b>, as well as storage manager <b>140</b> or a host computing device for it. Computing devices can include, without limitation, one or more: workstations, personal computers, desktop computers, or other types of generally fixed computing systems such as mainframe computers, servers, and minicomputers. Other computing devices can include mobile or portable computing devices, such as one or more laptops, tablet computers, personal data assistants, mobile phones (such as smartphones), and other mobile or portable computing devices such as embedded computers, set top boxes, vehicle-mounted devices, wearable computers, etc. Servers can include mail servers, file servers, database servers, virtual machine servers, and web servers. Any given computing device comprises one or more processors (e.g., CPU and/or single-core or multi-core processors), as well as corresponding non-transitory computer memory (e.g., random-access memory (RAM)) for storing computer programs which are to be executed by the one or more processors. Other computer memory for mass storage of data may be packaged/configured with the computing device (e.g., an internal hard disk) and/or may be external and accessible by the computing device (e.g., network-attached storage, a storage array, etc.). In some cases, a computing device includes cloud computing resources, which may be implemented as virtual machines. For instance, one or more virtual machines may be provided to the organization by a third-party cloud service vendor.</p><p id="p-0030" num="0058">In some embodiments, computing devices can include one or more virtual machine(s) running on a physical host computing device (or &#x201c;host machine&#x201d;) operated by the organization. As one example, the organization may use one virtual machine as a database server and another virtual machine as a mail server, both virtual machines operating on the same host machine. A Virtual machine (&#x201c;VM&#x201d;) is a software implementation of a computer that does not physically exist and is instead instantiated in an operating system of a physical computer (or host machine) to enable applications to execute within the VM's environment, i.e., a VM emulates a physical computer. A VM includes an operating system and associated virtual resources, such as computer memory and processor(s). A hypervisor operates between the VM and the hardware of the physical host machine and is generally responsible for creating and running the VMs. Hypervisors are also known in the art as virtual machine monitors or a virtual machine managers or &#x201c;VMMs&#x201d;, and may be implemented in software, firmware, and/or specialized hardware installed on the host machine. Examples of hypervisors include ESX Server, by VMware, Inc. of Palo Alto, Calif.; Microsoft Virtual Server and Microsoft Windows Server Hyper-V, both by Microsoft Corporation of Redmond, Wash.; Sun xVM by Oracle America Inc. of Santa Clara, Calif.; and Xen by Citrix Systems, Santa Clara, Calif. The hypervisor provides resources to each virtual operating system such as a virtual processor, virtual memory, a virtual network device, and a virtual disk. Each virtual machine has one or more associated virtual disks. The hypervisor typically stores the data of virtual disks in files on the file system of the physical host machine, called virtual machine disk files (&#x201c;VMDK&#x201d; in VMware lingo) or virtual hard disk image files (in Microsoft lingo). For example, VMware's ESX Server provides the Virtual Machine File System (VMFS) for the storage of virtual machine disk files. A virtual machine reads data from and writes data to its virtual disk much the way that a physical machine reads data from and writes data to a physical disk. Examples of techniques for implementing information management in a cloud computing environment are described in U.S. Pat. No. 8,285,681. Examples of techniques for implementing information management in a virtualized computing environment are described in U.S. Pat. No. 8,307,177.</p><p id="p-0031" num="0059">Information management system <b>100</b> can also include electronic data storage devices, generally used for mass storage of data, including, e.g., primary storage devices <b>104</b> and secondary storage devices <b>108</b>. Storage devices can generally be of any suitable type including, without limitation, disk drives, storage arrays (e.g., storage-area network (SAN) and/or network-attached storage (NAS) technology), semiconductor memory (e.g., solid state storage devices), network attached storage (NAS) devices, tape libraries, or other magnetic, non-tape storage devices, optical media storage devices, DNA/RNA-based memory technology, combinations of the same, etc. In some embodiments, storage devices form part of a distributed file system. In some cases, storage devices are provided in a cloud storage environment (e.g., a private cloud or one operated by a third-party vendor), whether for primary data or secondary copies or both.</p><p id="p-0032" num="0060">Depending on context, the term &#x201c;information management system&#x201d; can refer to generally all of the illustrated hardware and software components in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>, or the term may refer to only a subset of the illustrated components. For instance, in some cases, system <b>100</b> generally refers to a combination of specialized components used to protect, move, manage, manipulate, analyze, and/or process data and metadata generated by client computing devices <b>102</b>. However, system <b>100</b> in some cases does not include the underlying components that generate and/or store primary data <b>112</b>, such as the client computing devices <b>102</b> themselves, and the primary storage devices <b>104</b>. Likewise secondary storage devices <b>108</b> (e.g., a third-party provided cloud storage environment) may not be part of system <b>100</b>. As an example, &#x201c;information management system&#x201d; or &#x201c;storage management system&#x201d; may sometimes refer to one or more of the following components, which will be described in further detail below: storage manager, data agent, and media agent.</p><p id="p-0033" num="0061">One or more client computing devices <b>102</b> may be part of system <b>100</b>, each client computing device <b>102</b> having an operating system and at least one application <b>110</b> and one or more accompanying data agents executing thereon; and associated with one or more primary storage devices <b>104</b> storing primary data <b>112</b>. Client computing device(s) <b>102</b> and primary storage devices <b>104</b> may generally be referred to in some cases as primary storage subsystem <b>117</b>.</p><heading id="h-0008" level="2">Client Computing Devices, Clients, and Subclients</heading><p id="p-0034" num="0062">Typically, a variety of sources in an organization produce data to be protected and managed. As just one illustrative example, in a corporate environment such data sources can be employee workstations and company servers such as a mail server, a web server, a database server, a transaction server, or the like. In system <b>100</b>, data generation sources include one or more client computing devices <b>102</b>. A computing device that has a data agent <b>142</b> installed and operating on it is generally referred to as a &#x201c;client computing device&#x201d; <b>102</b>, and may include any type of computing device, without limitation. A client computing device <b>102</b> may be associated with one or more users and/or user accounts.</p><p id="p-0035" num="0063">A &#x201c;client&#x201d; is a logical component of information management system <b>100</b>, which may represent a logical grouping of one or more data agents installed on a client computing device <b>102</b>. Storage manager <b>140</b> recognizes a client as a component of system <b>100</b>, and in some embodiments, may automatically create a client component the first time a data agent <b>142</b> is installed on a client computing device <b>102</b>. Because data generated by executable component(s) <b>110</b> is tracked by the associated data agent <b>142</b> so that it may be properly protected in system <b>100</b>, a client may be said to generate data and to store the generated data to primary storage, such as primary storage device <b>104</b>. However, the terms &#x201c;client&#x201d; and &#x201c;client computing device&#x201d; as used herein do not imply that a client computing device <b>102</b> is necessarily configured in the client/server sense relative to another computing device such as a mail server, or that a client computing device <b>102</b> cannot be a server in its own right. As just a few examples, a client computing device <b>102</b> can be and/or include mail servers, file servers, database servers, virtual machine servers, and/or web servers.</p><p id="p-0036" num="0064">Each client computing device <b>102</b> may have application(s) <b>110</b> executing thereon which generate and manipulate the data that is to be protected from loss and managed in system <b>100</b>. Applications <b>110</b> generally facilitate the operations of an organization, and can include, without limitation, mail server applications (e.g., Microsoft Exchange Server), file system applications, mail client applications (e.g., Microsoft Exchange Client), database applications or database management systems (e.g., SQL, Oracle, SAP, Lotus Notes Database), word processing applications (e.g., Microsoft Word), spreadsheet applications, financial applications, presentation applications, graphics and/or video applications, browser applications, mobile applications, entertainment applications, and so on. Each application <b>110</b> may be accompanied by an application-specific data agent <b>142</b>, though not all data agents <b>142</b> are application-specific or associated with only application. A file system, e.g., Microsoft Windows Explorer, may be considered an application <b>110</b> and may be accompanied by its own data agent <b>142</b>. Client computing devices <b>102</b> can have at least one operating system (e.g., Microsoft Windows, Mac OS X, iOS, IBM z/OS, Linux, other Unix-based operating systems, etc.) installed thereon, which may support or host one or more file systems and other applications <b>110</b>. In some embodiments, a virtual machine that executes on a host client computing device <b>102</b> may be considered an application <b>110</b> and may be accompanied by a specific data agent <b>142</b> (e.g., virtual server data agent).</p><p id="p-0037" num="0065">Client computing devices <b>102</b> and other components in system <b>100</b> can be connected to one another via one or more electronic communication pathways <b>114</b>. For example, a first communication pathway <b>114</b> may communicatively couple client computing device <b>102</b> and secondary storage computing device <b>106</b>; a second communication pathway <b>114</b> may communicatively couple storage manager <b>140</b> and client computing device <b>102</b>; and a third communication pathway <b>114</b> may communicatively couple storage manager <b>140</b> and secondary storage computing device <b>106</b>, etc. (see, e.g., <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>). A communication pathway <b>114</b> can include one or more networks or other connection types including one or more of the following, without limitation: the Internet, a wide area network (WAN), a local area network (LAN), a Storage Area Network (SAN), a Fibre Channel (FC) connection, a Small Computer System Interface (SCSI) connection, a virtual private network (VPN), a token ring or TCP/IP based network, an intranet network, a point-to-point link, a cellular network, a wireless data transmission system, a two-way cable system, an interactive kiosk network, a satellite network, a broadband network, a baseband network, a neural network, a mesh network, an ad hoc network, other appropriate computer or telecommunications networks, combinations of the same or the like. Communication pathways <b>114</b> in some cases may also include application programming interfaces (APIs) including, e.g., cloud service provider APIs, virtual machine management APIs, and hosted service provider APIs. The underlying infrastructure of communication pathways <b>114</b> may be wired and/or wireless, analog and/or digital, or any combination thereof; and the facilities used may be private, public, third-party provided, or any combination thereof, without limitation.</p><p id="p-0038" num="0066">A &#x201c;subclient&#x201d; is a logical grouping of all or part of a client's primary data <b>112</b>. In general, a subclient may be defined according to how the subclient data is to be protected as a unit in system <b>100</b>. For example, a subclient may be associated with a certain storage policy. A given client may thus comprise several subclients, each subclient associated with a different storage policy. For example, some files may form a first subclient that requires compression and deduplication and is associated with a first storage policy. Other files of the client may form a second subclient that requires a different retention schedule as well as encryption, and may be associated with a different, second storage policy. As a result, though the primary data may be generated by the same application <b>110</b> and may belong to one given client, portions of the data may be assigned to different subclients for distinct treatment by system <b>100</b>. More detail on subclients is given in regard to storage policies below.</p><heading id="h-0009" level="2">Primary Data and Exemplary Primary Storage Devices</heading><p id="p-0039" num="0067">Primary data <b>112</b> is generally production data or &#x201c;live&#x201d; data generated by the operating system and/or applications <b>110</b> executing on client computing device <b>102</b>. Primary data <b>112</b> is generally stored on primary storage device(s) <b>104</b> and is organized via a file system operating on the client computing device <b>102</b>. Thus, client computing device(s) <b>102</b> and corresponding applications <b>110</b> may create, access, modify, write, delete, and otherwise use primary data <b>112</b>. Primary data <b>112</b> is generally in the native format of the source application <b>110</b>. Primary data <b>112</b> is an initial or first stored body of data generated by the source application <b>110</b>. Primary data <b>112</b> in some cases is created substantially directly from data generated by the corresponding source application <b>110</b>. It can be useful in performing certain tasks to organize primary data <b>112</b> into units of different granularities. In general, primary data <b>112</b> can include files, directories, file system volumes, data blocks, extents, or any other hierarchies or organizations of data objects. As used herein, a &#x201c;data object&#x201d; can refer to (i) any file that is currently addressable by a file system or that was previously addressable by the file system (e.g., an archive file), and/or to (ii) a subset of such a file (e.g., a data block, an extent, etc.). Primary data <b>112</b> may include structured data (e.g., database files), unstructured data (e.g., documents), and/or semi-structured data. See, e.g., <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>.</p><p id="p-0040" num="0068">It can also be useful in performing certain functions of system <b>100</b> to access and modify metadata within primary data <b>112</b>. Metadata generally includes information about data objects and/or characteristics associated with the data objects. For simplicity herein, it is to be understood that, unless expressly stated otherwise, any reference to primary data <b>112</b> generally also includes its associated metadata, but references to metadata generally do not include the primary data. Metadata can include, without limitation, one or more of the following: the data owner (e.g., the client or user that generates the data), the last modified time (e.g., the time of the most recent modification of the data object), a data object name (e.g., a file name), a data object size (e.g., a number of bytes of data), information about the content (e.g., an indication as to the existence of a particular search term), user-supplied tags, to/from information for email (e.g., an email sender, recipient, etc.), creation date, file type (e.g., format or application type), last accessed time, application type (e.g., type of application that generated the data object), location/network (e.g., a current, past or future location of the data object and network pathways to/from the data object), geographic location (e.g., GPS coordinates), frequency of change (e.g., a period in which the data object is modified), business unit (e.g., a group or department that generates, manages or is otherwise associated with the data object), aging information (e.g., a schedule, such as a time period, in which the data object is migrated to secondary or long term storage), boot sectors, partition layouts, file location within a file folder directory structure, user permissions, owners, groups, access control lists (ACLs), system metadata (e.g., registry information), combinations of the same or other similar information related to the data object. In addition to metadata generated by or related to file systems and operating systems, some applications <b>110</b> and/or other components of system <b>100</b> maintain indices of metadata for data objects, e.g., metadata associated with individual email messages. The use of metadata to perform classification and other functions is described in greater detail below.</p><p id="p-0041" num="0069">Primary storage devices <b>104</b> storing primary data <b>112</b> may be relatively fast and/or expensive technology (e.g., flash storage, a disk drive, a hard-disk storage array, solid state memory, etc.), typically to support high-performance live production environments. Primary data <b>112</b> may be highly changeable and/or may be intended for relatively short term retention (e.g., hours, days, or weeks). According to some embodiments, client computing device <b>102</b> can access primary data <b>112</b> stored in primary storage device <b>104</b> by making conventional file system calls via the operating system. Each client computing device <b>102</b> is generally associated with and/or in communication with one or more primary storage devices <b>104</b> storing corresponding primary data <b>112</b>. A client computing device <b>102</b> is said to be associated with or in communication with a particular primary storage device <b>104</b> if it is capable of one or more of: routing and/or storing data (e.g., primary data <b>112</b>) to the primary storage device <b>104</b>, coordinating the routing and/or storing of data to the primary storage device <b>104</b>, retrieving data from the primary storage device <b>104</b>, coordinating the retrieval of data from the primary storage device <b>104</b>, and modifying and/or deleting data in the primary storage device <b>104</b>. Thus, a client computing device <b>102</b> may be said to access data stored in an associated storage device <b>104</b>.</p><p id="p-0042" num="0070">Primary storage device <b>104</b> may be dedicated or shared. In some cases, each primary storage device <b>104</b> is dedicated to an associated client computing device <b>102</b>, e.g., a local disk drive. In other cases, one or more primary storage devices <b>104</b> can be shared by multiple client computing devices <b>102</b>, e.g., via a local network, in a cloud storage implementation, etc. As one example, primary storage device <b>104</b> can be a storage array shared by a group of client computing devices <b>102</b>, such as EMC Clariion, EMC Symmetrix, EMC Celerra, Dell EqualLogic, IBM XIV, NetApp FAS, HP EVA, and HP 3PAR.</p><p id="p-0043" num="0071">System <b>100</b> may also include hosted services (not shown), which may be hosted in some cases by an entity other than the organization that employs the other components of system <b>100</b>. For instance, the hosted services may be provided by online service providers. Such service providers can provide social networking services, hosted email services, or hosted productivity applications or other hosted applications such as software-as-a-service (SaaS), platform-as-a-service (PaaS), application service providers (ASPs), cloud services, or other mechanisms for delivering functionality via a network. As it services users, each hosted service may generate additional data and metadata, which may be managed by system <b>100</b>, e.g., as primary data <b>112</b>. In some cases, the hosted services may be accessed using one of the applications <b>110</b>. As an example, a hosted mail service may be accessed via browser running on a client computing device <b>102</b>.</p><heading id="h-0010" level="2">Secondary Copies and Exemplary Secondary Storage Devices</heading><p id="p-0044" num="0072">Primary data <b>112</b> stored on primary storage devices <b>104</b> may be compromised in some cases, such as when an employee deliberately or accidentally deletes or overwrites primary data <b>112</b>. Or primary storage devices <b>104</b> can be damaged, lost, or otherwise corrupted. For recovery and/or regulatory compliance purposes, it is therefore useful to generate and maintain copies of primary data <b>112</b>. Accordingly, system <b>100</b> includes one or more secondary storage computing devices <b>106</b> and one or more secondary storage devices <b>108</b> configured to create and store one or more secondary copies <b>116</b> of primary data <b>112</b> including its associated metadata. The secondary storage computing devices <b>106</b> and the secondary storage devices <b>108</b> may be referred to as secondary storage subsystem <b>118</b>.</p><p id="p-0045" num="0073">Secondary copies <b>116</b> can help in search and analysis efforts and meet other information management goals as well, such as: restoring data and/or metadata if an original version is lost (e.g., by deletion, corruption, or disaster); allowing point-in-time recovery; complying with regulatory data retention and electronic discovery (e-discovery) requirements; reducing utilized storage capacity in the production system and/or in secondary storage; facilitating organization and search of data; improving user access to data files across multiple computing devices and/or hosted services; and implementing data retention and pruning policies.</p><p id="p-0046" num="0074">A secondary copy <b>116</b> can comprise a separate stored copy of data that is derived from one or more earlier-created stored copies (e.g., derived from primary data <b>112</b> or from another secondary copy <b>116</b>). Secondary copies <b>116</b> can include point-in-time data, and may be intended for relatively long-term retention before some or all of the data is moved to other storage or discarded. In some cases, a secondary copy <b>116</b> may be in a different storage device than other previously stored copies; and/or may be remote from other previously stored copies. Secondary copies <b>116</b> can be stored in the same storage device as primary data <b>112</b>. For example, a disk array capable of performing hardware snapshots stores primary data <b>112</b> and creates and stores hardware snapshots of the primary data <b>112</b> as secondary copies <b>116</b>. Secondary copies <b>116</b> may be stored in relatively slow and/or lower cost storage (e.g., magnetic tape). A secondary copy <b>116</b> may be stored in a backup or archive format, or in some other format different from the native source application format or other format of primary data <b>112</b>.</p><p id="p-0047" num="0075">Secondary storage computing devices <b>106</b> may index secondary copies <b>116</b> (e.g., using a media agent <b>144</b>), enabling users to browse and restore at a later time and further enabling the lifecycle management of the indexed data. After creation of a secondary copy <b>116</b> that represents certain primary data <b>112</b>, a pointer or other location indicia (e.g., a stub) may be placed in primary data <b>112</b>, or be otherwise associated with primary data <b>112</b>, to indicate the current location of a particular secondary copy <b>116</b>. Since an instance of a data object or metadata in primary data <b>112</b> may change over time as it is modified by application <b>110</b> (or hosted service or the operating system), system <b>100</b> may create and manage multiple secondary copies <b>116</b> of a particular data object or metadata, each copy representing the state of the data object in primary data <b>112</b> at a particular point in time. Moreover, since an instance of a data object in primary data <b>112</b> may eventually be deleted from primary storage device <b>104</b> and the file system, system <b>100</b> may continue to manage point-in-time representations of that data object, even though the instance in primary data <b>112</b> no longer exists. For virtual machines, the operating system and other applications <b>110</b> of client computing device(s) <b>102</b> may execute within or under the management of virtualization software (e.g., a VMM), and the primary storage device(s) <b>104</b> may comprise a virtual disk created on a physical storage device. System <b>100</b> may create secondary copies <b>116</b> of the files or other data objects in a virtual disk file and/or secondary copies <b>116</b> of the entire virtual disk file itself (e.g., of an entire .vmdk file).</p><p id="p-0048" num="0076">Secondary copies <b>116</b> are distinguishable from corresponding primary data <b>112</b>. First, secondary copies <b>116</b> can be stored in a different format from primary data <b>112</b> (e.g., backup, archive, or other non-native format). For this or other reasons, secondary copies <b>116</b> may not be directly usable by applications <b>110</b> or client computing device <b>102</b> (e.g., via standard system calls or otherwise) without modification, processing, or other intervention by system <b>100</b> which may be referred to as &#x201c;restore&#x201d; operations. Secondary copies <b>116</b> may have been processed by data agent <b>142</b> and/or media agent <b>144</b> in the course of being created (e.g., compression, deduplication, encryption, integrity markers, indexing, formatting, application-aware metadata, etc.), and thus secondary copy <b>116</b> may represent source primary data <b>112</b> without necessarily being exactly identical to the source.</p><p id="p-0049" num="0077">Second, secondary copies <b>116</b> may be stored on a secondary storage device <b>108</b> that is inaccessible to application <b>110</b> running on client computing device <b>102</b> and/or hosted service. Some secondary copies <b>116</b> may be &#x201c;offline copies,&#x201d; in that they are not readily available (e.g., not mounted to tape or disk). Offline copies can include copies of data that system <b>100</b> can access without human intervention (e.g., tapes within an automated tape library, but not yet mounted in a drive), and copies that the system <b>100</b> can access only with some human intervention (e.g., tapes located at an offsite storage site).</p><heading id="h-0011" level="2">Using Intermediate Devices for Creating Secondary Copies&#x2014;Secondary Storage Computing Devices</heading><p id="p-0050" num="0078">Creating secondary copies can be challenging when hundreds or thousands of client computing devices <b>102</b> continually generate large volumes of primary data <b>112</b> to be protected. Also, there can be significant overhead involved in the creation of secondary copies <b>116</b>. Moreover, specialized programmed intelligence and/or hardware capability is generally needed for accessing and interacting with secondary storage devices <b>108</b>. Client computing devices <b>102</b> may interact directly with a secondary storage device <b>108</b> to create secondary copies <b>116</b>, but in view of the factors described above, this approach can negatively impact the ability of client computing device <b>102</b> to serve/service application <b>110</b> and produce primary data <b>112</b>. Further, any given client computing device <b>102</b> may not be optimized for interaction with certain secondary storage devices <b>108</b>.</p><p id="p-0051" num="0079">Thus, system <b>100</b> may include one or more software and/or hardware components which generally act as intermediaries between client computing devices <b>102</b> (that generate primary data <b>112</b>) and secondary storage devices <b>108</b> (that store secondary copies <b>116</b>). In addition to off-loading certain responsibilities from client computing devices <b>102</b>, these intermediate components provide other benefits. For instance, as discussed further below with respect to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>, distributing some of the work involved in creating secondary copies <b>116</b> can enhance scalability and improve system performance. For instance, using specialized secondary storage computing devices <b>106</b> and media agents <b>144</b> for interfacing with secondary storage devices <b>108</b> and/or for performing certain data processing operations can greatly improve the speed with which system <b>100</b> performs information management operations and can also improve the capacity of the system to handle large numbers of such operations, while reducing the computational load on the production environment of client computing devices <b>102</b>. The intermediate components can include one or more secondary storage computing devices <b>106</b> as shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> and/or one or more media agents <b>144</b>. Media agents are discussed further below (e.g., with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>C-<b>1</b>E</figref>). These special-purpose components of system <b>100</b> comprise specialized programmed intelligence and/or hardware capability for writing to, reading from, instructing, communicating with, or otherwise interacting with secondary storage devices <b>108</b>.</p><p id="p-0052" num="0080">Secondary storage computing device(s) <b>106</b> can comprise any of the computing devices described above, without limitation. In some cases, secondary storage computing device(s) <b>106</b> also include specialized hardware componentry and/or software intelligence (e.g., specialized interfaces) for interacting with certain secondary storage device(s) <b>108</b> with which they may be specially associated.</p><p id="p-0053" num="0081">To create a secondary copy <b>116</b> involving the copying of data from primary storage subsystem <b>117</b> to secondary storage subsystem <b>118</b>, client computing device <b>102</b> may communicate the primary data <b>112</b> to be copied (or a processed version thereof generated by a data agent <b>142</b>) to the designated secondary storage computing device <b>106</b>, via a communication pathway <b>114</b>. Secondary storage computing device <b>106</b> in turn may further process and convey the data or a processed version thereof to secondary storage device <b>108</b>. One or more secondary copies <b>116</b> may be created from existing secondary copies <b>116</b>, such as in the case of an auxiliary copy operation, described further below.</p><heading id="h-0012" level="2">Exemplary Primary Data and an Exemplary Secondary Copy</heading><p id="p-0054" num="0082"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a detailed view of some specific examples of primary data stored on primary storage device(s) <b>104</b> and secondary copy data stored on secondary storage device(s) <b>108</b>, with other components of the system removed for the purposes of illustration. Stored on primary storage device(s) <b>104</b> are primary data <b>112</b> objects including word processing documents <b>119</b>A-B, spreadsheets <b>120</b>, presentation documents <b>122</b>, video files <b>124</b>, image files <b>126</b>, email mailboxes <b>128</b> (and corresponding email messages <b>129</b>A-C), HTML/XML or other types of markup language files <b>130</b>, databases <b>132</b> and corresponding tables or other data structures <b>133</b>A-<b>133</b>C. Some or all primary data <b>112</b> objects are associated with corresponding metadata (e.g., &#x201c;Meta1-11&#x201d;), which may include file system metadata and/or application-specific metadata. Stored on the secondary storage device(s) <b>108</b> are secondary copy <b>116</b> data objects <b>134</b>A-C which may include copies of or may otherwise represent corresponding primary data <b>112</b>.</p><p id="p-0055" num="0083">Secondary copy data objects <b>134</b>A-C can individually represent more than one primary data object. For example, secondary copy data object <b>134</b>A represents three separate primary data objects <b>133</b>C, <b>122</b>, and <b>129</b>C (represented as <b>133</b>C&#x2032;, <b>122</b>&#x2032;, and <b>129</b>C&#x2032;, respectively, and accompanied by corresponding metadata Meta11, Meta3, and Meta8, respectively). Moreover, as indicated by the prime mark (&#x2032;), secondary storage computing devices <b>106</b> or other components in secondary storage subsystem <b>118</b> may process the data received from primary storage subsystem <b>117</b> and store a secondary copy including a transformed and/or supplemented representation of a primary data object and/or metadata that is different from the original format, e.g., in a compressed, encrypted, deduplicated, or other modified format. For instance, secondary storage computing devices <b>106</b> can generate new metadata or other information based on said processing, and store the newly generated information along with the secondary copies. Secondary copy data object <b>1346</b> represents primary data objects <b>120</b>, <b>1336</b>, and <b>119</b>A as <b>120</b>&#x2032;, <b>1336</b>&#x2032;, and <b>119</b>A&#x2032;, respectively, accompanied by corresponding metadata Meta2, Meta10, and Meta1, respectively. Also, secondary copy data object <b>134</b>C represents primary data objects <b>133</b>A, <b>1196</b>, and <b>129</b>A as <b>133</b>A&#x2032;, <b>1196</b>&#x2032;, and <b>129</b>A&#x2032;, respectively, accompanied by corresponding metadata Meta9, Meta5, and Meta6, respectively.</p><heading id="h-0013" level="2">Exemplary Information Management System Architecture</heading><p id="p-0056" num="0084">System <b>100</b> can incorporate a variety of different hardware and software components, which can in turn be organized with respect to one another in many different configurations, depending on the embodiment. There are critical design choices involved in specifying the functional responsibilities of the components and the role of each component in system <b>100</b>. Such design choices can impact how system <b>100</b> performs and adapts to data growth and other changing circumstances. <figref idref="DRAWINGS">FIG. <b>1</b>C</figref> shows a system <b>100</b> designed according to these considerations and includes: storage manager <b>140</b>, one or more data agents <b>142</b> executing on client computing device(s) <b>102</b> and configured to process primary data <b>112</b>, and one or more media agents <b>144</b> executing on one or more secondary storage computing devices <b>106</b> for performing tasks involving secondary storage devices <b>108</b>.</p><p id="p-0057" num="0085">Storage Manager</p><p id="p-0058" num="0086">Storage manager <b>140</b> is a centralized storage and/or information manager that is configured to perform certain control functions and also to store certain critical information about system <b>100</b>&#x2014;hence storage manager <b>140</b> is said to manage system <b>100</b>. As noted, the number of components in system <b>100</b> and the amount of data under management can be large. Managing the components and data is therefore a significant task, which can grow unpredictably as the number of components and data scale to meet the needs of the organization. For these and other reasons, according to certain embodiments, responsibility for controlling system <b>100</b>, or at least a significant portion of that responsibility, is allocated to storage manager <b>140</b>. Storage manager <b>140</b> can be adapted independently according to changing circumstances, without having to replace or re-design the remainder of the system. Moreover, a computing device for hosting and/or operating as storage manager <b>140</b> can be selected to best suit the functions and networking needs of storage manager <b>140</b>. These and other advantages are described in further detail below and with respect to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>.</p><p id="p-0059" num="0087">Storage manager <b>140</b> may be a software module or other application hosted by a suitable computing device. In some embodiments, storage manager <b>140</b> is itself a computing device that performs the functions described herein. Storage manager <b>140</b> comprises or operates in conjunction with one or more associated data structures such as a dedicated database (e.g., management database <b>146</b>), depending on the configuration. The storage manager <b>140</b> generally initiates, performs, coordinates, and/or controls storage and other information management operations performed by system <b>100</b>, e.g., to protect and control primary data <b>112</b> and secondary copies <b>116</b>. In general, storage manager <b>140</b> is said to manage system <b>100</b>, which includes communicating with, instructing, and controlling in some circumstances components such as data agents <b>142</b> and media agents <b>144</b>, etc.</p><p id="p-0060" num="0088">As shown by the dashed arrowed lines <b>114</b> in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>, storage manager <b>140</b> may communicate with, instruct, and/or control some or all elements of system <b>100</b>, such as data agents <b>142</b> and media agents <b>144</b>. In this manner, storage manager <b>140</b> manages the operation of various hardware and software components in system <b>100</b>. In certain embodiments, control information originates from storage manager <b>140</b> and status as well as index reporting is transmitted to storage manager <b>140</b> by the managed components, whereas payload data and metadata are generally communicated between data agents <b>142</b> and media agents <b>144</b> (or otherwise between client computing device(s) <b>102</b> and secondary storage computing device(s) <b>106</b>), e.g., at the direction of and under the management of storage manager <b>140</b>. Control information can generally include parameters and instructions for carrying out information management operations, such as, without limitation, instructions to perform a task associated with an operation, timing information specifying when to initiate a task, data path information specifying what components to communicate with or access in carrying out an operation, and the like. In other embodiments, some information management operations are controlled or initiated by other components of system <b>100</b> (e.g., by media agents <b>144</b> or data agents <b>142</b>), instead of or in combination with storage manager <b>140</b>.</p><p id="p-0061" num="0089">According to certain embodiments, storage manager <b>140</b> provides one or more of the following functions:<ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0000">    <ul id="ul0003" list-style="none">        <li id="ul0003-0001" num="0090">communicating with data agents <b>142</b> and media agents <b>144</b>, including transmitting instructions, messages, and/or queries, as well as receiving status reports, index information, messages, and/or queries, and responding to same;</li>        <li id="ul0003-0002" num="0091">initiating execution of information management operations;</li>        <li id="ul0003-0003" num="0092">initiating restore and recovery operations;</li>        <li id="ul0003-0004" num="0093">managing secondary storage devices <b>108</b> and inventory/capacity of the same;</li>        <li id="ul0003-0005" num="0094">allocating secondary storage devices <b>108</b> for secondary copy operations;</li>        <li id="ul0003-0006" num="0095">reporting, searching, and/or classification of data in system <b>100</b>;</li>        <li id="ul0003-0007" num="0096">monitoring completion of and status reporting related to information management operations and jobs;</li>        <li id="ul0003-0008" num="0097">tracking movement of data within system <b>100</b>;</li>        <li id="ul0003-0009" num="0098">tracking age information relating to secondary copies <b>116</b>, secondary storage devices <b>108</b>, comparing the age information against retention guidelines, and initiating data pruning when appropriate;</li>        <li id="ul0003-0010" num="0099">tracking logical associations between components in system <b>100</b>;</li>        <li id="ul0003-0011" num="0100">protecting metadata associated with system <b>100</b>, e.g., in management database <b>146</b>;</li>        <li id="ul0003-0012" num="0101">implementing job management, schedule management, event management, alert management, reporting, job history maintenance, user security management, disaster recovery management, and/or user interfacing for system administrators and/or end users of system <b>100</b>;</li>        <li id="ul0003-0013" num="0102">sending, searching, and/or viewing of log files; and</li>        <li id="ul0003-0014" num="0103">implementing operations management functionality.</li>    </ul>    </li></ul></p><p id="p-0062" num="0104">Storage manager <b>140</b> may maintain an associated database <b>146</b> (or &#x201c;storage manager database <b>146</b>&#x201d; or &#x201c;management database <b>146</b>&#x201d;) of management-related data and information management policies <b>148</b>. Database <b>146</b> is stored in computer memory accessible by storage manager <b>140</b>. Database <b>146</b> may include a management index <b>150</b> (or &#x201c;index <b>150</b>&#x201d;) or other data structure(s) that may store: logical associations between components of the system; user preferences and/or profiles (e.g., preferences regarding encryption, compression, or deduplication of primary data or secondary copies; preferences regarding the scheduling, type, or other aspects of secondary copy or other operations; mappings of particular information management users or user accounts to certain computing devices or other components, etc.; management tasks; media containerization; other useful data; and/or any combination thereof. For example, storage manager <b>140</b> may use index <b>150</b> to track logical associations between media agents <b>144</b> and secondary storage devices <b>108</b> and/or movement of data to/from secondary storage devices <b>108</b>. For instance, index <b>150</b> may store data associating a client computing device <b>102</b> with a particular media agent <b>144</b> and/or secondary storage device <b>108</b>, as specified in an information management policy <b>148</b>.</p><p id="p-0063" num="0105">Administrators and others may configure and initiate certain information management operations on an individual basis. But while this may be acceptable for some recovery operations or other infrequent tasks, it is often not workable for implementing on-going organization-wide data protection and management. Thus, system <b>100</b> may utilize information management policies <b>148</b> for specifying and executing information management operations on an automated basis. Generally, an information management policy <b>148</b> can include a stored data structure or other information source that specifies parameters (e.g., criteria and rules) associated with storage management or other information management operations. Storage manager <b>140</b> can process an information management policy <b>148</b> and/or index <b>150</b> and, based on the results, identify an information management operation to perform, identify the appropriate components in system <b>100</b> to be involved in the operation (e.g., client computing devices <b>102</b> and corresponding data agents <b>142</b>, secondary storage computing devices <b>106</b> and corresponding media agents <b>144</b>, etc.), establish connections to those components and/or between those components, and/or instruct and control those components to carry out the operation. In this manner, system <b>100</b> can translate stored information into coordinated activity among the various computing devices in system <b>100</b>.</p><p id="p-0064" num="0106">Management database <b>146</b> may maintain information management policies <b>148</b> and associated data, although information management policies <b>148</b> can be stored in computer memory at any appropriate location outside management database <b>146</b>. For instance, an information management policy <b>148</b> such as a storage policy may be stored as metadata in a media agent database <b>152</b> or in a secondary storage device <b>108</b> (e.g., as an archive copy) for use in restore or other information management operations, depending on the embodiment. Information management policies <b>148</b> are described further below. According to certain embodiments, management database <b>146</b> comprises a relational database (e.g., an SQL database) for tracking metadata, such as metadata associated with secondary copy operations (e.g., what client computing devices <b>102</b> and corresponding subclient data were protected and where the secondary copies are stored and which media agent <b>144</b> performed the storage operation(s)). This and other metadata may additionally be stored in other locations, such as at secondary storage computing device <b>106</b> or on the secondary storage device <b>108</b>, allowing data recovery without the use of storage manager <b>140</b> in some cases. Thus, management database <b>146</b> may comprise data needed to kick off secondary copy operations (e.g., storage policies, schedule policies, etc.), status and reporting information about completed jobs (e.g., status and error reports on yesterday's backup jobs), and additional information sufficient to enable restore and disaster recovery operations (e.g., media agent associations, location indexing, content indexing, etc.).</p><p id="p-0065" num="0107">Storage manager <b>140</b> may include a jobs agent <b>156</b>, a user interface <b>158</b>, and a management agent <b>154</b>, all of which may be implemented as interconnected software modules or application programs. These are described further below.</p><p id="p-0066" num="0108">Jobs agent <b>156</b> in some embodiments initiates, controls, and/or monitors the status of some or all information management operations previously performed, currently being performed, or scheduled to be performed by system <b>100</b>. A job is a logical grouping of information management operations such as daily storage operations scheduled for a certain set of subclients (e.g., generating incremental block-level backup copies <b>116</b> at a certain time every day for database files in a certain geographical location). Thus, jobs agent <b>156</b> may access information management policies <b>148</b> (e.g., in management database <b>146</b>) to determine when, where, and how to initiate/control jobs in system <b>100</b>.</p><p id="p-0067" num="0109">Storage Manager User Interfaces</p><p id="p-0068" num="0110">User interface <b>158</b> may include information processing and display software, such as a graphical user interface (GUI), an application program interface (API), and/or other interactive interface(s) through which users and system processes can retrieve information about the status of information management operations or issue instructions to storage manager <b>140</b> and other components. Via user interface <b>158</b>, users may issue instructions to the components in system <b>100</b> regarding performance of secondary copy and recovery operations. For example, a user may modify a schedule concerning the number of pending secondary copy operations. As another example, a user may employ the GUI to view the status of pending secondary copy jobs or to monitor the status of certain components in system <b>100</b> (e.g., the amount of capacity left in a storage device). Storage manager <b>140</b> may track information that permits it to select, designate, or otherwise identify content indices, deduplication databases, or similar databases or resources or data sets within its information management cell (or another cell) to be searched in response to certain queries. Such queries may be entered by the user by interacting with user interface <b>158</b>.</p><p id="p-0069" num="0111">Various embodiments of information management system <b>100</b> may be configured and/or designed to generate user interface data usable for rendering the various interactive user interfaces described. The user interface data may be used by system <b>100</b> and/or by another system, device, and/or software program (for example, a browser program), to render the interactive user interfaces. The interactive user interfaces may be displayed on, for example, electronic displays (including, for example, touch-enabled displays), consoles, etc., whether direct-connected to storage manager <b>140</b> or communicatively coupled remotely, e.g., via an internet connection. The present disclosure describes various embodiments of interactive and dynamic user interfaces, some of which may be generated by user interface agent <b>158</b>, and which are the result of significant technological development. The user interfaces described herein may provide improved human-computer interactions, allowing for significant cognitive and ergonomic efficiencies and advantages over previous systems, including reduced mental workloads, improved decision-making, and the like. User interface <b>158</b> may operate in a single integrated view or console (not shown). The console may support a reporting capability for generating a variety of reports, which may be tailored to a particular aspect of information management.</p><p id="p-0070" num="0112">User interfaces are not exclusive to storage manager <b>140</b> and in some embodiments a user may access information locally from a computing device component of system <b>100</b>. For example, some information pertaining to installed data agents <b>142</b> and associated data streams may be available from client computing device <b>102</b>. Likewise, some information pertaining to media agents <b>144</b> and associated data streams may be available from secondary storage computing device <b>106</b>.</p><p id="p-0071" num="0113">Storage Manager Management Agent</p><p id="p-0072" num="0114">Management agent <b>154</b> can provide storage manager <b>140</b> with the ability to communicate with other components within system <b>100</b> and/or with other information management cells via network protocols and application programming interfaces (APIs) including, e.g., HTTP, HTTPS, FTP, REST, virtualization software APIs, cloud service provider APIs, and hosted service provider APIs, without limitation. Management agent <b>154</b> also allows multiple information management cells to communicate with one another. For example, system <b>100</b> in some cases may be one information management cell in a network of multiple cells adjacent to one another or otherwise logically related, e.g., in a WAN or LAN. With this arrangement, the cells may communicate with one another through respective management agents <b>154</b>. Inter-cell communications and hierarchy is described in greater detail in e.g., U.S. Pat. No. 7,343,453.</p><p id="p-0073" num="0115">Information Management Cell</p><p id="p-0074" num="0116">An &#x201c;information management cell&#x201d; (or &#x201c;storage operation cell&#x201d; or &#x201c;cell&#x201d;) may generally include a logical and/or physical grouping of a combination of hardware and software components associated with performing information management operations on electronic data, typically one storage manager <b>140</b> and at least one data agent <b>142</b> (executing on a client computing device <b>102</b>) and at least one media agent <b>144</b> (executing on a secondary storage computing device <b>106</b>). For instance, the components shown in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref> may together form an information management cell. Thus, in some configurations, a system <b>100</b> may be referred to as an information management cell or a storage operation cell. A given cell may be identified by the identity of its storage manager <b>140</b>, which is generally responsible for managing the cell.</p><p id="p-0075" num="0117">Multiple cells may be organized hierarchically, so that cells may inherit properties from hierarchically superior cells or be controlled by other cells in the hierarchy (automatically or otherwise). Alternatively, in some embodiments, cells may inherit or otherwise be associated with information management policies, preferences, information management operational parameters, or other properties or characteristics according to their relative position in a hierarchy of cells. Cells may also be organized hierarchically according to function, geography, architectural considerations, or other factors useful or desirable in performing information management operations. For example, a first cell may represent a geographic segment of an enterprise, such as a Chicago office, and a second cell may represent a different geographic segment, such as a New York City office. Other cells may represent departments within a particular office, e.g., human resources, finance, engineering, etc. Where delineated by function, a first cell may perform one or more first types of information management operations (e.g., one or more first types of secondary copies at a certain frequency), and a second cell may perform one or more second types of information management operations (e.g., one or more second types of secondary copies at a different frequency and under different retention rules). In general, the hierarchical information is maintained by one or more storage managers <b>140</b> that manage the respective cells (e.g., in corresponding management database(s) <b>146</b>).</p><p id="p-0076" num="0118">Data Agents</p><p id="p-0077" num="0119">A variety of different applications <b>110</b> can operate on a given client computing device <b>102</b>, including operating systems, file systems, database applications, e-mail applications, and virtual machines, just to name a few. And, as part of the process of creating and restoring secondary copies <b>116</b>, the client computing device <b>102</b> may be tasked with processing and preparing the primary data <b>112</b> generated by these various applications <b>110</b>. Moreover, the nature of the processing/preparation can differ across application types, e.g., due to inherent structural, state, and formatting differences among applications <b>110</b> and/or the operating system of client computing device <b>102</b>. Each data agent <b>142</b> is therefore advantageously configured in some embodiments to assist in the performance of information management operations based on the type of data that is being protected at a client-specific and/or application-specific level.</p><p id="p-0078" num="0120">Data agent <b>142</b> is a component of information system <b>100</b> and is generally directed by storage manager <b>140</b> to participate in creating or restoring secondary copies <b>116</b>. Data agent <b>142</b> may be a software program (e.g., in the form of a set of executable binary files) that executes on the same client computing device <b>102</b> as the associated application <b>110</b> that data agent <b>142</b> is configured to protect. Data agent <b>142</b> is generally responsible for managing, initiating, or otherwise assisting in the performance of information management operations in reference to its associated application(s) <b>110</b> and corresponding primary data <b>112</b> which is generated/accessed by the particular application(s) <b>110</b>. For instance, data agent <b>142</b> may take part in copying, archiving, migrating, and/or replicating of certain primary data <b>112</b> stored in the primary storage device(s) <b>104</b>. Data agent <b>142</b> may receive control information from storage manager <b>140</b>, such as commands to transfer copies of data objects and/or metadata to one or more media agents <b>144</b>. Data agent <b>142</b> also may compress, deduplicate, and encrypt certain primary data <b>112</b>, as well as capture application-related metadata before transmitting the processed data to media agent <b>144</b>. Data agent <b>142</b> also may receive instructions from storage manager <b>140</b> to restore (or assist in restoring) a secondary copy <b>116</b> from secondary storage device <b>108</b> to primary storage <b>104</b>, such that the restored data may be properly accessed by application <b>110</b> in a suitable format as though it were primary data <b>112</b>.</p><p id="p-0079" num="0121">Each data agent <b>142</b> may be specialized for a particular application <b>110</b>. For instance, different individual data agents <b>142</b> may be designed to handle Microsoft Exchange data, Lotus Notes data, Microsoft Windows file system data, Microsoft Active Directory Objects data, SQL Server data, Share Point data, Oracle database data, SAP database data, virtual machines and/or associated data, and other types of data. A file system data agent, for example, may handle data files and/or other file system information. If a client computing device <b>102</b> has two or more types of data <b>112</b>, a specialized data agent <b>142</b> may be used for each data type. For example, to backup, migrate, and/or restore all of the data on a Microsoft Exchange server, the client computing device <b>102</b> may use: (1) a Microsoft Exchange Mailbox data agent <b>142</b> to back up the Exchange mailboxes; (2) a Microsoft Exchange Database data agent <b>142</b> to back up the Exchange databases; (3) a Microsoft Exchange Public Folder data agent <b>142</b> to back up the Exchange Public Folders; and (4) a Microsoft Windows File System data agent <b>142</b> to back up the file system of client computing device <b>102</b>. In this example, these specialized data agents <b>142</b> are treated as four separate data agents <b>142</b> even though they operate on the same client computing device <b>102</b>. Other examples may include archive management data agents such as a migration archiver or a compliance archiver, Quick Recovery&#xae; agents, and continuous data replication agents. Application-specific data agents <b>142</b> can provide improved performance as compared to generic agents. For instance, because application-specific data agents <b>142</b> may only handle data for a single software application, the design, operation, and performance of the data agent <b>142</b> can be streamlined. The data agent <b>142</b> may therefore execute faster and consume less persistent storage and/or operating memory than data agents designed to generically accommodate multiple different software applications <b>110</b>.</p><p id="p-0080" num="0122">Each data agent <b>142</b> may be configured to access data and/or metadata stored in the primary storage device(s) <b>104</b> associated with data agent <b>142</b> and its host client computing device <b>102</b>, and process the data appropriately. For example, during a secondary copy operation, data agent <b>142</b> may arrange or assemble the data and metadata into one or more files having a certain format (e.g., a particular backup or archive format) before transferring the file(s) to a media agent <b>144</b> or other component. The file(s) may include a list of files or other metadata. In some embodiments, a data agent <b>142</b> may be distributed between client computing device <b>102</b> and storage manager <b>140</b> (and any other intermediate components) or may be deployed from a remote location or its functions approximated by a remote process that performs some or all of the functions of data agent <b>142</b>. In addition, a data agent <b>142</b> may perform some functions provided by media agent <b>144</b>. Other embodiments may employ one or more generic data agents <b>142</b> that can handle and process data from two or more different applications <b>110</b>, or that can handle and process multiple data types, instead of or in addition to using specialized data agents <b>142</b>. For example, one generic data agent <b>142</b> may be used to back up, migrate and restore Microsoft Exchange Mailbox data and Microsoft Exchange Database data, while another generic data agent may handle Microsoft Exchange Public Folder data and Microsoft Windows File System data.</p><p id="p-0081" num="0123">Media Agents</p><p id="p-0082" num="0124">As noted, off-loading certain responsibilities from client computing devices <b>102</b> to intermediate components such as secondary storage computing device(s) <b>106</b> and corresponding media agent(s) <b>144</b> can provide a number of benefits including improved performance of client computing device <b>102</b>, faster and more reliable information management operations, and enhanced scalability. In one example which will be discussed further below, media agent <b>144</b> can act as a local cache of recently-copied data and/or metadata stored to secondary storage device(s) <b>108</b>, thus improving restore capabilities and performance for the cached data.</p><p id="p-0083" num="0125">Media agent <b>144</b> is a component of system <b>100</b> and is generally directed by storage manager <b>140</b> in creating and restoring secondary copies <b>116</b>. Whereas storage manager <b>140</b> generally manages system <b>100</b> as a whole, media agent <b>144</b> provides a portal to certain secondary storage devices <b>108</b>, such as by having specialized features for communicating with and accessing certain associated secondary storage device <b>108</b>. Media agent <b>144</b> may be a software program (e.g., in the form of a set of executable binary files) that executes on a secondary storage computing device <b>106</b>. Media agent <b>144</b> generally manages, coordinates, and facilitates the transmission of data between a data agent <b>142</b> (executing on client computing device <b>102</b>) and secondary storage device(s) <b>108</b> associated with media agent <b>144</b>. For instance, other components in the system may interact with media agent <b>144</b> to gain access to data stored on associated secondary storage device(s) <b>108</b>, (e.g., to browse, read, write, modify, delete, or restore data). Moreover, media agents <b>144</b> can generate and store information relating to characteristics of the stored data and/or metadata, or can generate and store other types of information that generally provides insight into the contents of the secondary storage devices <b>108</b>&#x2014;generally referred to as indexing of the stored secondary copies <b>116</b>. Each media agent <b>144</b> may operate on a dedicated secondary storage computing device <b>106</b>, while in other embodiments a plurality of media agents <b>144</b> may operate on the same secondary storage computing device <b>106</b>.</p><p id="p-0084" num="0126">A media agent <b>144</b> may be associated with a particular secondary storage device <b>108</b> if that media agent <b>144</b> is capable of one or more of: routing and/or storing data to the particular secondary storage device <b>108</b>; coordinating the routing and/or storing of data to the particular secondary storage device <b>108</b>; retrieving data from the particular secondary storage device <b>108</b>; coordinating the retrieval of data from the particular secondary storage device <b>108</b>; and modifying and/or deleting data retrieved from the particular secondary storage device <b>108</b>. Media agent <b>144</b> in certain embodiments is physically separate from the associated secondary storage device <b>108</b>. For instance, a media agent <b>144</b> may operate on a secondary storage computing device <b>106</b> in a distinct housing, package, and/or location from the associated secondary storage device <b>108</b>. In one example, a media agent <b>144</b> operates on a first server computer and is in communication with a secondary storage device(s) <b>108</b> operating in a separate rack-mounted RAID-based system.</p><p id="p-0085" num="0127">A media agent <b>144</b> associated with a particular secondary storage device <b>108</b> may instruct secondary storage device <b>108</b> to perform an information management task. For instance, a media agent <b>144</b> may instruct a tape library to use a robotic arm or other retrieval means to load or eject a certain storage media, and to subsequently archive, migrate, or retrieve data to or from that media, e.g., for the purpose of restoring data to a client computing device <b>102</b>. As another example, a secondary storage device <b>108</b> may include an array of hard disk drives or solid state drives organized in a RAID configuration, and media agent <b>144</b> may forward a logical unit number (LUN) and other appropriate information to the array, which uses the received information to execute the desired secondary copy operation. Media agent <b>144</b> may communicate with a secondary storage device <b>108</b> via a suitable communications link, such as a SCSI or Fibre Channel link.</p><p id="p-0086" num="0128">Each media agent <b>144</b> may maintain an associated media agent database <b>152</b>. Media agent database <b>152</b> may be stored to a disk or other storage device (not shown) that is local to the secondary storage computing device <b>106</b> on which media agent <b>144</b> executes. In other cases, media agent database <b>152</b> is stored separately from the host secondary storage computing device <b>106</b>. Media agent database <b>152</b> can include, among other things, a media agent index <b>153</b> (see, e.g., <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>). In some cases, media agent index <b>153</b> does not form a part of and is instead separate from media agent database <b>152</b>.</p><p id="p-0087" num="0129">Media agent index <b>153</b> (or &#x201c;index <b>153</b>&#x201d;) may be a data structure associated with the particular media agent <b>144</b> that includes information about the stored data associated with the particular media agent and which may be generated in the course of performing a secondary copy operation or a restore. Index <b>153</b> provides a fast and efficient mechanism for locating/browsing secondary copies <b>116</b> or other data stored in secondary storage devices <b>108</b> without having to access secondary storage device <b>108</b> to retrieve the information from there. For instance, for each secondary copy <b>116</b>, index <b>153</b> may include metadata such as a list of the data objects (e.g., files/subdirectories, database objects, mailbox objects, etc.), a logical path to the secondary copy <b>116</b> on the corresponding secondary storage device <b>108</b>, location information (e.g., offsets) indicating where the data objects are stored in the secondary storage device <b>108</b>, when the data objects were created or modified, etc. Thus, index <b>153</b> includes metadata associated with the secondary copies <b>116</b> that is readily available for use from media agent <b>144</b>. In some embodiments, some or all of the information in index <b>153</b> may instead or additionally be stored along with secondary copies <b>116</b> in secondary storage device <b>108</b>. In some embodiments, a secondary storage device <b>108</b> can include sufficient information to enable a &#x201c;bare metal restore,&#x201d; where the operating system and/or software applications of a failed client computing device <b>102</b> or another target may be automatically restored without manually reinstalling individual software packages (including operating systems).</p><p id="p-0088" num="0130">Because index <b>153</b> may operate as a cache, it can also be referred to as an &#x201c;index cache.&#x201d; In such cases, information stored in index cache <b>153</b> typically comprises data that reflects certain particulars about relatively recent secondary copy operations. After some triggering event, such as after some time elapses or index cache <b>153</b> reaches a particular size, certain portions of index cache <b>153</b> may be copied or migrated to secondary storage device <b>108</b>, e.g., on a least-recently-used basis. This information may be retrieved and uploaded back into index cache <b>153</b> or otherwise restored to media agent <b>144</b> to facilitate retrieval of data from the secondary storage device(s) <b>108</b>. In some embodiments, the cached information may include format or containerization information related to archives or other files stored on storage device(s) <b>108</b>.</p><p id="p-0089" num="0131">In some alternative embodiments media agent <b>144</b> generally acts as a coordinator or facilitator of secondary copy operations between client computing devices <b>102</b> and secondary storage devices <b>108</b>, but does not actually write the data to secondary storage device <b>108</b>. For instance, storage manager <b>140</b> (or media agent <b>144</b>) may instruct a client computing device <b>102</b> and secondary storage device <b>108</b> to communicate with one another directly. In such a case, client computing device <b>102</b> transmits data directly or via one or more intermediary components to secondary storage device <b>108</b> according to the received instructions, and vice versa. Media agent <b>144</b> may still receive, process, and/or maintain metadata related to the secondary copy operations, i.e., may continue to build and maintain index <b>153</b>. In these embodiments, payload data can flow through media agent <b>144</b> for the purposes of populating index <b>153</b>, but not for writing to secondary storage device <b>108</b>. Media agent <b>144</b> and/or other components such as storage manager <b>140</b> may in some cases incorporate additional functionality, such as data classification, content indexing, deduplication, encryption, compression, and the like. Further details regarding these and other functions are described below.</p><heading id="h-0014" level="2">Distributed, Scalable Architecture</heading><p id="p-0090" num="0132">As described, certain functions of system <b>100</b> can be distributed amongst various physical and/or logical components. For instance, one or more of storage manager <b>140</b>, data agents <b>142</b>, and media agents <b>144</b> may operate on computing devices that are physically separate from one another. This architecture can provide a number of benefits. For instance, hardware and software design choices for each distributed component can be targeted to suit its particular function. The secondary storage computing devices <b>106</b> on which media agents <b>144</b> operate can be tailored for interaction with associated secondary storage devices <b>108</b> and provide fast index cache operation, among other specific tasks. Similarly, client computing device(s) <b>102</b> can be selected to effectively service applications <b>110</b> in order to efficiently produce and store primary data <b>112</b>.</p><p id="p-0091" num="0133">Moreover, in some cases, one or more of the individual components of information management system <b>100</b> can be distributed to multiple separate computing devices. As one example, for large file systems where the amount of data stored in management database <b>146</b> is relatively large, database <b>146</b> may be migrated to or may otherwise reside on a specialized database server (e.g., an SQL server) separate from a server that implements the other functions of storage manager <b>140</b>. This distributed configuration can provide added protection because database <b>146</b> can be protected with standard database utilities (e.g., SQL log shipping or database replication) independent from other functions of storage manager <b>140</b>. Database <b>146</b> can be efficiently replicated to a remote site for use in the event of a disaster or other data loss at the primary site. Or database <b>146</b> can be replicated to another computing device within the same site, such as to a higher performance machine in the event that a storage manager host computing device can no longer service the needs of a growing system <b>100</b>.</p><p id="p-0092" num="0134">The distributed architecture also provides scalability and efficient component utilization. <figref idref="DRAWINGS">FIG. <b>1</b>D</figref> shows an embodiment of information management system <b>100</b> including a plurality of client computing devices <b>102</b> and associated data agents <b>142</b> as well as a plurality of secondary storage computing devices <b>106</b> and associated media agents <b>144</b>. Additional components can be added or subtracted based on the evolving needs of system <b>100</b>. For instance, depending on where bottlenecks are identified, administrators can add additional client computing devices <b>102</b>, secondary storage computing devices <b>106</b>, and/or secondary storage devices <b>108</b>. Moreover, where multiple fungible components are available, load balancing can be implemented to dynamically address identified bottlenecks. As an example, storage manager <b>140</b> may dynamically select which media agents <b>144</b> and/or secondary storage devices <b>108</b> to use for storage operations based on a processing load analysis of media agents <b>144</b> and/or secondary storage devices <b>108</b>, respectively.</p><p id="p-0093" num="0135">Where system <b>100</b> includes multiple media agents <b>144</b> (see, e.g., <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>), a first media agent <b>144</b> may provide failover functionality for a second failed media agent <b>144</b>. In addition, media agents <b>144</b> can be dynamically selected to provide load balancing. Each client computing device <b>102</b> can communicate with, among other components, any of the media agents <b>144</b>, e.g., as directed by storage manager <b>140</b>. And each media agent <b>144</b> may communicate with, among other components, any of secondary storage devices <b>108</b>, e.g., as directed by storage manager <b>140</b>. Thus, operations can be routed to secondary storage devices <b>108</b> in a dynamic and highly flexible manner, to provide load balancing, failover, etc. Further examples of scalable systems capable of dynamic storage operations, load balancing, and failover are provided in U.S. Pat. No. 7,246,207.</p><p id="p-0094" num="0136">While distributing functionality amongst multiple computing devices can have certain advantages, in other contexts it can be beneficial to consolidate functionality on the same computing device. In alternative configurations, certain components may reside and execute on the same computing device. As such, in other embodiments, one or more of the components shown in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref> may be implemented on the same computing device. In one configuration, a storage manager <b>140</b>, one or more data agents <b>142</b>, and/or one or more media agents <b>144</b> are all implemented on the same computing device. In other embodiments, one or more data agents <b>142</b> and one or more media agents <b>144</b> are implemented on the same computing device, while storage manager <b>140</b> is implemented on a separate computing device, etc. without limitation.</p><heading id="h-0015" level="2">Exemplary Types of Information Management Operations, Including Storage Operations</heading><p id="p-0095" num="0137">In order to protect and leverage stored data, system <b>100</b> can be configured to perform a variety of information management operations, which may also be referred to in some cases as storage management operations or storage operations. These operations can generally include (i) data movement operations, (ii) processing and data manipulation operations, and (iii) analysis, reporting, and management operations.</p><p id="p-0096" num="0138">Data Movement Operations, Including Secondary Copy Operations</p><p id="p-0097" num="0139">Data movement operations are generally storage operations that involve the copying or migration of data between different locations in system <b>100</b>. For example, data movement operations can include operations in which stored data is copied, migrated, or otherwise transferred from one or more first storage devices to one or more second storage devices, such as from primary storage device(s) <b>104</b> to secondary storage device(s) <b>108</b>, from secondary storage device(s) <b>108</b> to different secondary storage device(s) <b>108</b>, from secondary storage devices <b>108</b> to primary storage devices <b>104</b>, or from primary storage device(s) <b>104</b> to different primary storage device(s) <b>104</b>, or in some cases within the same primary storage device <b>104</b> such as within a storage array.</p><p id="p-0098" num="0140">Data movement operations can include by way of example, backup operations, archive operations, information lifecycle management operations such as hierarchical storage management operations, replication operations (e.g., continuous data replication), snapshot operations, deduplication or single-instancing operations, auxiliary copy operations, disaster-recovery copy operations, and the like. As will be discussed, some of these operations do not necessarily create distinct copies. Nonetheless, some or all of these operations are generally referred to as &#x201c;secondary copy operations&#x201d; for simplicity, because they involve secondary copies. Data movement also comprises restoring secondary copies.</p><p id="p-0099" num="0141">Backup Operations</p><p id="p-0100" num="0142">A backup operation creates a copy of a version of primary data <b>112</b> at a particular point in time (e.g., one or more files or other data units). Each subsequent backup copy <b>116</b> (which is a form of secondary copy <b>116</b>) may be maintained independently of the first. A backup generally involves maintaining a version of the copied primary data <b>112</b> as well as backup copies <b>116</b>. Further, a backup copy in some embodiments is generally stored in a form that is different from the native format, e.g., a backup format. This contrasts to the version in primary data <b>112</b> which may instead be stored in a format native to the source application(s) <b>110</b>. In various cases, backup copies can be stored in a format in which the data is compressed, encrypted, deduplicated, and/or otherwise modified from the original native application format. For example, a backup copy may be stored in a compressed backup format that facilitates efficient long-term storage. Backup copies <b>116</b> can have relatively long retention periods as compared to primary data <b>112</b>, which is generally highly changeable. Backup copies <b>116</b> may be stored on media with slower retrieval times than primary storage device <b>104</b>. Some backup copies may have shorter retention periods than some other types of secondary copies <b>116</b>, such as archive copies (described below). Backups may be stored at an offsite location.</p><p id="p-0101" num="0143">Backup operations can include full backups, differential backups, incremental backups, &#x201c;synthetic full&#x201d; backups, and/or creating a &#x201c;reference copy.&#x201d; A full backup (or &#x201c;standard full backup&#x201d;) in some embodiments is generally a complete image of the data to be protected. However, because full backup copies can consume a relatively large amount of storage, it can be useful to use a full backup copy as a baseline and only store changes relative to the full backup copy afterwards.</p><p id="p-0102" num="0144">A differential backup operation (or cumulative incremental backup operation) tracks and stores changes that occurred since the last full backup. Differential backups can grow quickly in size, but can restore relatively efficiently because a restore can be completed in some cases using only the full backup copy and the latest differential copy.</p><p id="p-0103" num="0145">An incremental backup operation generally tracks and stores changes since the most recent backup copy of any type, which can greatly reduce storage utilization. In some cases, however, restoring can be lengthy compared to full or differential backups because completing a restore operation may involve accessing a full backup in addition to multiple incremental backups.</p><p id="p-0104" num="0146">Synthetic full backups generally consolidate data without directly backing up data from the client computing device. A synthetic full backup is created from the most recent full backup (i.e., standard or synthetic) and subsequent incremental and/or differential backups. The resulting synthetic full backup is identical to what would have been created had the last backup for the subclient been a standard full backup. Unlike standard full, incremental, and differential backups, however, a synthetic full backup does not actually transfer data from primary storage to the backup media, because it operates as a backup consolidator. A synthetic full backup extracts the index data of each participating subclient. Using this index data and the previously backed up user data images, it builds new full backup images (e.g., bitmaps), one for each subclient. The new backup images consolidate the index and user data stored in the related incremental, differential, and previous full backups into a synthetic backup file that fully represents the subclient (e.g., via pointers) but does not comprise all its constituent data.</p><p id="p-0105" num="0147">Any of the above types of backup operations can be at the volume level, file level, or block level. Volume level backup operations generally involve copying of a data volume (e.g., a logical disk or partition) as a whole. In a file-level backup, information management system <b>100</b> generally tracks changes to individual files and includes copies of files in the backup copy. For block-level backups, files are broken into constituent blocks, and changes are tracked at the block level. Upon restore, system <b>100</b> reassembles the blocks into files in a transparent fashion. Far less data may actually be transferred and copied to secondary storage devices <b>108</b> during a file-level copy than a volume-level copy. Likewise, a block-level copy may transfer less data than a file-level copy, resulting in faster execution. However, restoring a relatively higher-granularity copy can result in longer restore times. For instance, when restoring a block-level copy, the process of locating and retrieving constituent blocks can sometimes take longer than restoring file-level backups.</p><p id="p-0106" num="0148">A reference copy may comprise copy(ies) of selected objects from backed up data, typically to help organize data by keeping contextual information from multiple sources together, and/or help retain specific data for a longer period of time, such as for legal hold needs. A reference copy generally maintains data integrity, and when the data is restored, it may be viewed in the same format as the source data. In some embodiments, a reference copy is based on a specialized client, individual subclient and associated information management policies (e.g., storage policy, retention policy, etc.) that are administered within system <b>100</b>.</p><p id="p-0107" num="0149">Archive Operations</p><p id="p-0108" num="0150">Because backup operations generally involve maintaining a version of the copied primary data <b>112</b> and also maintaining backup copies in secondary storage device(s) <b>108</b>, they can consume significant storage capacity. To reduce storage consumption, an archive operation according to certain embodiments creates an archive copy <b>116</b> by both copying and removing source data. Or, seen another way, archive operations can involve moving some or all of the source data to the archive destination. Thus, data satisfying criteria for removal (e.g., data of a threshold age or size) may be removed from source storage. The source data may be primary data <b>112</b> or a secondary copy <b>116</b>, depending on the situation. As with backup copies, archive copies can be stored in a format in which the data is compressed, encrypted, deduplicated, and/or otherwise modified from the format of the original application or source copy. In addition, archive copies may be retained for relatively long periods of time (e.g., years) and, in some cases are never deleted. In certain embodiments, archive copies may be made and kept for extended periods in order to meet compliance regulations.</p><p id="p-0109" num="0151">Archiving can also serve the purpose of freeing up space in primary storage device(s) <b>104</b> and easing the demand on computational resources on client computing device <b>102</b>. Similarly, when a secondary copy <b>116</b> is archived, the archive copy can therefore serve the purpose of freeing up space in the source secondary storage device(s) <b>108</b>. Examples of data archiving operations are provided in U.S. Pat. No. 7,107,298.</p><p id="p-0110" num="0152">Snapshot Operations</p><p id="p-0111" num="0153">Snapshot operations can provide a relatively lightweight, efficient mechanism for protecting data. From an end-user viewpoint, a snapshot may be thought of as an &#x201c;instant&#x201d; image of primary data <b>112</b> at a given point in time, and may include state and/or status information relative to an application <b>110</b> that creates/manages primary data <b>112</b>. In one embodiment, a snapshot may generally capture the directory structure of an object in primary data <b>112</b> such as a file or volume or other data set at a particular moment in time and may also preserve file attributes and contents. A snapshot in some cases is created relatively quickly, e.g., substantially instantly, using a minimum amount of file space, but may still function as a conventional file system backup.</p><p id="p-0112" num="0154">A &#x201c;hardware snapshot&#x201d; (or &#x201c;hardware-based snapshot&#x201d;) operation occurs where a target storage device (e.g., a primary storage device <b>104</b> or a secondary storage device <b>108</b>) performs the snapshot operation in a self-contained fashion, substantially independently, using hardware, firmware and/or software operating on the storage device itself. For instance, the storage device may perform snapshot operations generally without intervention or oversight from any of the other components of the system <b>100</b>, e.g., a storage array may generate an &#x201c;array-created&#x201d; hardware snapshot and may also manage its storage, integrity, versioning, etc. In this manner, hardware snapshots can off-load other components of system <b>100</b> from snapshot processing. An array may receive a request from another component to take a snapshot and then proceed to execute the &#x201c;hardware snapshot&#x201d; operations autonomously, preferably reporting success to the requesting component.</p><p id="p-0113" num="0155">A &#x201c;software snapshot&#x201d; (or &#x201c;software-based snapshot&#x201d;) operation, on the other hand, occurs where a component in system <b>100</b> (e.g., client computing device <b>102</b>, etc.) implements a software layer that manages the snapshot operation via interaction with the target storage device. For instance, the component executing the snapshot management software layer may derive a set of pointers and/or data that represents the snapshot. The snapshot management software layer may then transmit the same to the target storage device, along with appropriate instructions for writing the snapshot. One example of a software snapshot product is Microsoft Volume Snapshot Service (VSS), which is part of the Microsoft Windows operating system.</p><p id="p-0114" num="0156">Some types of snapshots do not actually create another physical copy of all the data as it existed at the particular point in time, but may simply create pointers that map files and directories to specific memory locations (e.g., to specific disk blocks) where the data resides as it existed at the particular point in time. For example, a snapshot copy may include a set of pointers derived from the file system or from an application. In some other cases, the snapshot may be created at the block-level, such that creation of the snapshot occurs without awareness of the file system. Each pointer points to a respective stored data block, so that collectively, the set of pointers reflect the storage location and state of the data object (e.g., file(s) or volume(s) or data set(s)) at the point in time when the snapshot copy was created.</p><p id="p-0115" num="0157">An initial snapshot may use only a small amount of disk space needed to record a mapping or other data structure representing or otherwise tracking the blocks that correspond to the current state of the file system. Additional disk space is usually required only when files and directories change later on. Furthermore, when files change, typically only the pointers which map to blocks are copied, not the blocks themselves. For example for &#x201c;copy-on-write&#x201d; snapshots, when a block changes in primary storage, the block is copied to secondary storage or cached in primary storage before the block is overwritten in primary storage, and the pointer to that block is changed to reflect the new location of that block. The snapshot mapping of file system data may also be updated to reflect the changed block(s) at that particular point in time. In some other cases, a snapshot includes a full physical copy of all or substantially all of the data represented by the snapshot. Further examples of snapshot operations are provided in U.S. Pat. No. 7,529,782. A snapshot copy in many cases can be made quickly and without significantly impacting primary computing resources because large amounts of data need not be copied or moved. In some embodiments, a snapshot may exist as a virtual file system, parallel to the actual file system. Users in some cases gain read-only access to the record of files and directories of the snapshot. By electing to restore primary data <b>112</b> from a snapshot taken at a given point in time, users may also return the current file system to the state of the file system that existed when the snapshot was taken.</p><p id="p-0116" num="0158">Replication Operations</p><p id="p-0117" num="0159">Replication is another type of secondary copy operation. Some types of secondary copies <b>116</b> periodically capture images of primary data <b>112</b> at particular points in time (e.g., backups, archives, and snapshots). However, it can also be useful for recovery purposes to protect primary data <b>112</b> in a more continuous fashion, by replicating primary data <b>112</b> substantially as changes occur. In some cases a replication copy can be a mirror copy, for instance, where changes made to primary data <b>112</b> are mirrored or substantially immediately copied to another location (e.g., to secondary storage device(s) <b>108</b>). By copying each write operation to the replication copy, two storage systems are kept synchronized or substantially synchronized so that they are virtually identical at approximately the same time. Where entire disk volumes are mirrored, however, mirroring can require significant amount of storage space and utilizes a large amount of processing resources.</p><p id="p-0118" num="0160">According to some embodiments, secondary copy operations are performed on replicated data that represents a recoverable state, or &#x201c;known good state&#x201d; of a particular application running on the source system. For instance, in certain embodiments, known good replication copies may be viewed as copies of primary data <b>112</b>. This feature allows the system to directly access, copy, restore, back up, or otherwise manipulate the replication copies as if they were the &#x201c;live&#x201d; primary data <b>112</b>. This can reduce access time, storage utilization, and impact on source applications <b>110</b>, among other benefits. Based on known good state information, system <b>100</b> can replicate sections of application data that represent a recoverable state rather than rote copying of blocks of data. Examples of replication operations (e.g., continuous data replication) are provided in U.S. Pat. No. 7,617,262.</p><p id="p-0119" num="0161">Deduplication/Single-Instancing Operations</p><p id="p-0120" num="0162">Deduplication or single-instance storage is useful to reduce the amount of non-primary data. For instance, some or all of the above-described secondary copy operations can involve deduplication in some fashion. New data is read, broken down into data portions of a selected granularity (e.g., sub-file level blocks, files, etc.), compared with corresponding portions that are already in secondary storage, and only new/changed portions are stored. Portions that already exist are represented as pointers to the already-stored data. Thus, a deduplicated secondary copy <b>116</b> may comprise actual data portions copied from primary data <b>112</b> and may further comprise pointers to already-stored data, which is generally more storage-efficient than a full copy.</p><p id="p-0121" num="0163">In order to streamline the comparison process, system <b>100</b> may calculate and/or store signatures (e.g., hashes or cryptographically unique IDs) corresponding to the individual source data portions and compare the signatures to already-stored data signatures, instead of comparing entire data portions. In some cases, only a single instance of each data portion is stored, and deduplication operations may therefore be referred to interchangeably as &#x201c;single-instancing&#x201d; operations. Depending on the implementation, however, deduplication operations can store more than one instance of certain data portions, yet still significantly reduce stored-data redundancy. Depending on the embodiment, deduplication portions such as data blocks can be of fixed or variable length. Using variable length blocks can enhance deduplication by responding to changes in the data stream, but can involve more complex processing. In some cases, system <b>100</b> utilizes a technique for dynamically aligning deduplication blocks based on changing content in the data stream, as described in U.S. Pat. No. 8,364,652.</p><p id="p-0122" num="0164">System <b>100</b> can deduplicate in a variety of manners at a variety of locations. For instance, in some embodiments, system <b>100</b> implements &#x201c;target-side&#x201d; deduplication by deduplicating data at the media agent <b>144</b> after being received from data agent <b>142</b>. In some such cases, media agents <b>144</b> are generally configured to manage the deduplication process. For instance, one or more of the media agents <b>144</b> maintain a corresponding deduplication database that stores deduplication information (e.g., datablock signatures). Examples of such a configuration are provided in U.S. Pat. No. 9,020,900. Instead of or in combination with &#x201c;target-side&#x201d; deduplication, &#x201c;source-side&#x201d; (or &#x201c;client-side&#x201d;) deduplication can also be performed, e.g., to reduce the amount of data to be transmitted by data agent <b>142</b> to media agent <b>144</b>. Storage manager <b>140</b> may communicate with other components within system <b>100</b> via network protocols and cloud service provider APIs to facilitate cloud-based deduplication/single instancing, as exemplified in U.S. Pat. No. 8,954,446. Some other deduplication/single instancing techniques are described in U.S. Pat. Pub. No. 2006/0224846 and in U.S. Pat. No. 9,098,495.</p><p id="p-0123" num="0165">Information Lifecycle Management and Hierarchical Storage Management</p><p id="p-0124" num="0166">In some embodiments, files and other data over their lifetime move from more expensive quick-access storage to less expensive slower-access storage. Operations associated with moving data through various tiers of storage are sometimes referred to as information lifecycle management (ILM) operations.</p><p id="p-0125" num="0167">One type of ILM operation is a hierarchical storage management (HSM) operation, which generally automatically moves data between classes of storage devices, such as from high-cost to low-cost storage devices. For instance, an HSM operation may involve movement of data from primary storage devices <b>104</b> to secondary storage devices <b>108</b>, or between tiers of secondary storage devices <b>108</b>. With each tier, the storage devices may be progressively cheaper, have relatively slower access/restore times, etc. For example, movement of data between tiers may occur as data becomes less important over time. In some embodiments, an HSM operation is similar to archiving in that creating an HSM copy may (though not always) involve deleting some of the source data, e.g., according to one or more criteria related to the source data. For example, an HSM copy may include primary data <b>112</b> or a secondary copy <b>116</b> that exceeds a given size threshold or a given age threshold. Often, and unlike some types of archive copies, HSM data that is removed or aged from the source is replaced by a logical reference pointer or stub. The reference pointer or stub can be stored in the primary storage device <b>104</b> or other source storage device, such as a secondary storage device <b>108</b> to replace the deleted source data and to point to or otherwise indicate the new location in (another) secondary storage device <b>108</b>.</p><p id="p-0126" num="0168">For example, files are generally moved between higher and lower cost storage depending on how often the files are accessed. When a user requests access to HSM data that has been removed or migrated, system <b>100</b> uses the stub to locate the data and may make recovery of the data appear transparent, even though the HSM data may be stored at a location different from other source data. In this manner, the data appears to the user (e.g., in file system browsing windows and the like) as if it still resides in the source location (e.g., in a primary storage device <b>104</b>). The stub may include metadata associated with the corresponding data, so that a file system and/or application can provide some information about the data object and/or a limited-functionality version (e.g., a preview) of the data object.</p><p id="p-0127" num="0169">An HSM copy may be stored in a format other than the native application format (e.g., compressed, encrypted, deduplicated, and/or otherwise modified). In some cases, copies which involve the removal of data from source storage and the maintenance of stub or other logical reference information on source storage may be referred to generally as &#x201c;on-line archive copies.&#x201d; On the other hand, copies which involve the removal of data from source storage without the maintenance of stub or other logical reference information on source storage may be referred to as &#x201c;off-line archive copies.&#x201d; Examples of HSM and ILM techniques are provided in U.S. Pat. No. 7,343,453.</p><p id="p-0128" num="0170">Auxiliary Copy Operations</p><p id="p-0129" num="0171">An auxiliary copy is generally a copy of an existing secondary copy <b>116</b>. For instance, an initial secondary copy <b>116</b> may be derived from primary data <b>112</b> or from data residing in secondary storage subsystem <b>118</b>, whereas an auxiliary copy is generated from the initial secondary copy <b>116</b>. Auxiliary copies provide additional standby copies of data and may reside on different secondary storage devices <b>108</b> than the initial secondary copies <b>116</b>. Thus, auxiliary copies can be used for recovery purposes if initial secondary copies <b>116</b> become unavailable. Exemplary auxiliary copy techniques are described in further detail in U.S. Pat. No. 8,230,195.</p><p id="p-0130" num="0172">Disaster-Recovery Copy Operations</p><p id="p-0131" num="0173">System <b>100</b> may also make and retain disaster recovery copies, often as secondary, high-availability disk copies. System <b>100</b> may create secondary copies and store them at disaster recovery locations using auxiliary copy or replication operations, such as continuous data replication technologies. Depending on the particular data protection goals, disaster recovery locations can be remote from the client computing devices <b>102</b> and primary storage devices <b>104</b>, remote from some or all of the secondary storage devices <b>108</b>, or both.</p><p id="p-0132" num="0174">Data Manipulation, Including Encryption and Compression</p><p id="p-0133" num="0175">Data manipulation and processing may include encryption and compression as well as integrity marking and checking, formatting for transmission, formatting for storage, etc. Data may be manipulated &#x201c;client-side&#x201d; by data agent <b>142</b> as well as &#x201c;target-side&#x201d; by media agent <b>144</b> in the course of creating secondary copy <b>116</b>, or conversely in the course of restoring data from secondary to primary.</p><p id="p-0134" num="0176">Encryption Operations</p><p id="p-0135" num="0177">System <b>100</b> in some cases is configured to process data (e.g., files or other data objects, primary data <b>112</b>, secondary copies <b>116</b>, etc.), according to an appropriate encryption algorithm (e.g., Blowfish, Advanced Encryption Standard (AES), Triple Data Encryption Standard (3-DES), etc.) to limit access and provide data security. System <b>100</b> in some cases encrypts the data at the client level, such that client computing devices <b>102</b> (e.g., data agents <b>142</b>) encrypt the data prior to transferring it to other components, e.g., before sending the data to media agents <b>144</b> during a secondary copy operation. In such cases, client computing device <b>102</b> may maintain or have access to an encryption key or passphrase for decrypting the data upon restore. Encryption can also occur when media agent <b>144</b> creates auxiliary copies or archive copies. Encryption may be applied in creating a secondary copy <b>116</b> of a previously unencrypted secondary copy <b>116</b>, without limitation. In further embodiments, secondary storage devices <b>108</b> can implement built-in, high performance hardware-based encryption.</p><p id="p-0136" num="0178">Compression Operations</p><p id="p-0137" num="0179">Similar to encryption, system <b>100</b> may also or alternatively compress data in the course of generating a secondary copy <b>116</b>. Compression encodes information such that fewer bits are needed to represent the information as compared to the original representation. Compression techniques are well known in the art. Compression operations may apply one or more data compression algorithms. Compression may be applied in creating a secondary copy <b>116</b> of a previously uncompressed secondary copy, e.g., when making archive copies or disaster recovery copies. The use of compression may result in metadata that specifies the nature of the compression, so that data may be uncompressed on restore if appropriate.</p><p id="p-0138" num="0180">Data Analysis, Reporting, and Management Operations</p><p id="p-0139" num="0181">Data analysis, reporting, and management operations can differ from data movement operations in that they do not necessarily involve copying, migration or other transfer of data between different locations in the system. For instance, data analysis operations may involve processing (e.g., offline processing) or modification of already stored primary data <b>112</b> and/or secondary copies <b>116</b>. However, in some embodiments data analysis operations are performed in conjunction with data movement operations. Some data analysis operations include content indexing operations and classification operations which can be useful in leveraging data under management to enhance search and other features.</p><p id="p-0140" num="0182">Classification Operations/Content Indexing</p><p id="p-0141" num="0183">In some embodiments, information management system <b>100</b> analyzes and indexes characteristics, content, and metadata associated with primary data <b>112</b> (&#x201c;online content indexing&#x201d;) and/or secondary copies <b>116</b> (&#x201c;off-line content indexing&#x201d;). Content indexing can identify files or other data objects based on content (e.g., user-defined keywords or phrases, other keywords/phrases that are not defined by a user, etc.), and/or metadata (e.g., email metadata such as &#x201c;to,&#x201d; &#x201c;from,&#x201d; &#x201c;cc,&#x201d; &#x201c;bcc,&#x201d; attachment name, received time, etc.). Content indexes may be searched and search results may be restored.</p><p id="p-0142" num="0184">System <b>100</b> generally organizes and catalogues the results into a content index, which may be stored within media agent database <b>152</b>, for example. The content index can also include the storage locations of or pointer references to indexed data in primary data <b>112</b> and/or secondary copies <b>116</b>. Results may also be stored elsewhere in system <b>100</b> (e.g., in primary storage device <b>104</b> or in secondary storage device <b>108</b>). Such content index data provides storage manager <b>140</b> or other components with an efficient mechanism for locating primary data <b>112</b> and/or secondary copies <b>116</b> of data objects that match particular criteria, thus greatly increasing the search speed capability of system <b>100</b>. For instance, search criteria can be specified by a user through user interface <b>158</b> of storage manager <b>140</b>. Moreover, when system <b>100</b> analyzes data and/or metadata in secondary copies <b>116</b> to create an &#x201c;off-line content index,&#x201d; this operation has no significant impact on the performance of client computing devices <b>102</b> and thus does not take a toll on the production environment. Examples of content indexing techniques are provided in U.S. Pat. No. 8,170,995.</p><p id="p-0143" num="0185">One or more components, such as a content index engine, can be configured to scan data and/or associated metadata for classification purposes to populate a database (or other data structure) of information, which can be referred to as a &#x201c;data classification database&#x201d; or a &#x201c;metabase.&#x201d; Depending on the embodiment, the data classification database(s) can be organized in a variety of different ways, including centralization, logical sub-divisions, and/or physical sub-divisions. For instance, one or more data classification databases may be associated with different subsystems or tiers within system <b>100</b>. As an example, there may be a first metabase associated with primary storage subsystem <b>117</b> and a second metabase associated with secondary storage subsystem <b>118</b>. In other cases, metabase(s) may be associated with individual components, e.g., client computing devices <b>102</b> and/or media agents <b>144</b>. In some embodiments, a data classification database may reside as one or more data structures within management database <b>146</b>, may be otherwise associated with storage manager <b>140</b>, and/or may reside as a separate component. In some cases, metabase(s) may be included in separate database(s) and/or on separate storage device(s) from primary data <b>112</b> and/or secondary copies <b>116</b>, such that operations related to the metabase(s) do not significantly impact performance on other components of system <b>100</b>. In other cases, metabase(s) may be stored along with primary data <b>112</b> and/or secondary copies <b>116</b>. Files or other data objects can be associated with identifiers (e.g., tag entries, etc.) to facilitate searches of stored data objects. Among a number of other benefits, the metabase can also allow efficient, automatic identification of files or other data objects to associate with secondary copy or other information management operations. For instance, a metabase can dramatically improve the speed with which system <b>100</b> can search through and identify data as compared to other approaches that involve scanning an entire file system. Examples of metabases and data classification operations are provided in U.S. Pat. Nos. 7,734,669 and 7,747,579.</p><p id="p-0144" num="0186">Management and Reporting Operations</p><p id="p-0145" num="0187">Certain embodiments leverage the integrated ubiquitous nature of system <b>100</b> to provide useful system-wide management and reporting. Operations management can generally include monitoring and managing the health and performance of system <b>100</b> by, without limitation, performing error tracking, generating granular storage/performance metrics (e.g., job success/failure information, deduplication efficiency, etc.), generating storage modeling and costing information, and the like. As an example, storage manager <b>140</b> or another component in system <b>100</b> may analyze traffic patterns and suggest and/or automatically route data to minimize congestion. In some embodiments, the system can generate predictions relating to storage operations or storage operation information. Such predictions, which may be based on a trending analysis, may predict various network operations or resource usage, such as network traffic levels, storage media use, use of bandwidth of communication links, use of media agent components, etc. Further examples of traffic analysis, trend analysis, prediction generation, and the like are described in U.S. Pat. No. 7,343,453.</p><p id="p-0146" num="0188">In some configurations having a hierarchy of storage operation cells, a master storage manager <b>140</b> may track the status of subordinate cells, such as the status of jobs, system components, system resources, and other items, by communicating with storage managers <b>140</b> (or other components) in the respective storage operation cells. Moreover, the master storage manager <b>140</b> may also track status by receiving periodic status updates from the storage managers <b>140</b> (or other components) in the respective cells regarding jobs, system components, system resources, and other items. In some embodiments, a master storage manager <b>140</b> may store status information and other information regarding its associated storage operation cells and other system information in its management database <b>146</b> and/or index <b>150</b> (or in another location). The master storage manager <b>140</b> or other component may also determine whether certain storage-related or other criteria are satisfied, and may perform an action or trigger event (e.g., data migration) in response to the criteria being satisfied, such as where a storage threshold is met for a particular volume, or where inadequate protection exists for certain data. For instance, data from one or more storage operation cells is used to dynamically and automatically mitigate recognized risks, and/or to advise users of risks or suggest actions to mitigate these risks. For example, an information management policy may specify certain requirements (e.g., that a storage device should maintain a certain amount of free space, that secondary copies should occur at a particular interval, that data should be aged and migrated to other storage after a particular period, that data on a secondary volume should always have a certain level of availability and be restorable within a given time period, that data on a secondary volume may be mirrored or otherwise migrated to a specified number of other volumes, etc.). If a risk condition or other criterion is triggered, the system may notify the user of these conditions and may suggest (or automatically implement) a mitigation action to address the risk. For example, the system may indicate that data from a primary copy <b>112</b> should be migrated to a secondary storage device <b>108</b> to free up space on primary storage device <b>104</b>. Examples of the use of risk factors and other triggering criteria are described in U.S. Pat. No. 7,343,453.</p><p id="p-0147" num="0189">In some embodiments, system <b>100</b> may also determine whether a metric or other indication satisfies particular storage criteria sufficient to perform an action. For example, a storage policy or other definition might indicate that a storage manager <b>140</b> should initiate a particular action if a storage metric or other indication drops below or otherwise fails to satisfy specified criteria such as a threshold of data protection. In some embodiments, risk factors may be quantified into certain measurable service or risk levels. For example, certain applications and associated data may be considered to be more important relative to other data and services. Financial compliance data, for example, may be of greater importance than marketing materials, etc. Network administrators may assign priority values or &#x201c;weights&#x201d; to certain data and/or applications corresponding to the relative importance. The level of compliance of secondary copy operations specified for these applications may also be assigned a certain value. Thus, the health, impact, and overall importance of a service may be determined, such as by measuring the compliance value and calculating the product of the priority value and the compliance value to determine the &#x201c;service level&#x201d; and comparing it to certain operational thresholds to determine whether it is acceptable. Further examples of the service level determination are provided in U.S. Pat. No. 7,343,453.</p><p id="p-0148" num="0190">System <b>100</b> may additionally calculate data costing and data availability associated with information management operation cells. For instance, data received from a cell may be used in conjunction with hardware-related information and other information about system elements to determine the cost of storage and/or the availability of particular data. Exemplary information generated could include how fast a particular department is using up available storage space, how long data would take to recover over a particular pathway from a particular secondary storage device, costs over time, etc. Moreover, in some embodiments, such information may be used to determine or predict the overall cost associated with the storage of certain information. The cost associated with hosting a certain application may be based, at least in part, on the type of media on which the data resides, for example. Storage devices may be assigned to a particular cost categories, for example. Further examples of costing techniques are described in U.S. Pat. No. 7,343,453.</p><p id="p-0149" num="0191">Any of the above types of information (e.g., information related to trending, predictions, job, cell or component status, risk, service level, costing, etc.) can generally be provided to users via user interface <b>158</b> in a single integrated view or console (not shown). Report types may include: scheduling, event management, media management and data aging. Available reports may also include backup history, data aging history, auxiliary copy history, job history, library and drive, media in library, restore history, and storage policy, etc., without limitation. Such reports may be specified and created at a certain point in time as a system analysis, forecasting, or provisioning tool. Integrated reports may also be generated that illustrate storage and performance metrics, risks and storage costing information. Moreover, users may create their own reports based on specific needs. User interface <b>158</b> can include an option to graphically depict the various components in the system using appropriate icons. As one example, user interface <b>158</b> may provide a graphical depiction of primary storage devices <b>104</b>, secondary storage devices <b>108</b>, data agents <b>142</b> and/or media agents <b>144</b>, and their relationship to one another in system <b>100</b>.</p><p id="p-0150" num="0192">In general, the operations management functionality of system <b>100</b> can facilitate planning and decision-making. For example, in some embodiments, a user may view the status of some or all jobs as well as the status of each component of information management system <b>100</b>. Users may then plan and make decisions based on this data. For instance, a user may view high-level information regarding secondary copy operations for system <b>100</b>, such as job status, component status, resource status (e.g., communication pathways, etc.), and other information. The user may also drill down or use other means to obtain more detailed information regarding a particular component, job, or the like. Further examples are provided in U.S. Pat. No. 7,343,453.</p><p id="p-0151" num="0193">System <b>100</b> can also be configured to perform system-wide e-discovery operations in some embodiments. In general, e-discovery operations provide a unified collection and search capability for data in the system, such as data stored in secondary storage devices <b>108</b> (e.g., backups, archives, or other secondary copies <b>116</b>). For example, system <b>100</b> may construct and maintain a virtual repository for data stored in system <b>100</b> that is integrated across source applications <b>110</b>, different storage device types, etc. According to some embodiments, e-discovery utilizes other techniques described herein, such as data classification and/or content indexing.</p><heading id="h-0016" level="2">Information Management Policies</heading><p id="p-0152" num="0194">An information management policy <b>148</b> can include a data structure or other information source that specifies a set of parameters (e.g., criteria and rules) associated with secondary copy and/or other information management operations.</p><p id="p-0153" num="0195">One type of information management policy <b>148</b> is a &#x201c;storage policy.&#x201d; According to certain embodiments, a storage policy generally comprises a data structure or other information source that defines (or includes information sufficient to determine) a set of preferences or other criteria for performing information management operations. Storage policies can include one or more of the following: (1) what data will be associated with the storage policy, e.g., subclient; (2) a destination to which the data will be stored; (3) datapath information specifying how the data will be communicated to the destination; (4) the type of secondary copy operation to be performed; and (5) retention information specifying how long the data will be retained at the destination (see, e.g., <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>). Data associated with a storage policy can be logically organized into subclients, which may represent primary data <b>112</b> and/or secondary copies <b>116</b>. A subclient may represent static or dynamic associations of portions of a data volume. Subclients may represent mutually exclusive portions. Thus, in certain embodiments, a portion of data may be given a label and the association is stored as a static entity in an index, database or other storage location. Subclients may also be used as an effective administrative scheme of organizing data according to data type, department within the enterprise, storage preferences, or the like. Depending on the configuration, subclients can correspond to files, folders, virtual machines, databases, etc. In one exemplary scenario, an administrator may find it preferable to separate e-mail data from financial data using two different subclients.</p><p id="p-0154" num="0196">A storage policy can define where data is stored by specifying a target or destination storage device (or group of storage devices). For instance, where the secondary storage device <b>108</b> includes a group of disk libraries, the storage policy may specify a particular disk library for storing the subclients associated with the policy. As another example, where the secondary storage devices <b>108</b> include one or more tape libraries, the storage policy may specify a particular tape library for storing the subclients associated with the storage policy, and may also specify a drive pool and a tape pool defining a group of tape drives and a group of tapes, respectively, for use in storing the subclient data. While information in the storage policy can be statically assigned in some cases, some or all of the information in the storage policy can also be dynamically determined based on criteria set forth in the storage policy. For instance, based on such criteria, a particular destination storage device(s) or other parameter of the storage policy may be determined based on characteristics associated with the data involved in a particular secondary copy operation, device availability (e.g., availability of a secondary storage device <b>108</b> or a media agent <b>144</b>), network status and conditions (e.g., identified bottlenecks), user credentials, and the like.</p><p id="p-0155" num="0197">Datapath information can also be included in the storage policy. For instance, the storage policy may specify network pathways and components to utilize when moving the data to the destination storage device(s). In some embodiments, the storage policy specifies one or more media agents <b>144</b> for conveying data associated with the storage policy between the source and destination. A storage policy can also specify the type(s) of associated operations, such as backup, archive, snapshot, auxiliary copy, or the like. Furthermore, retention parameters can specify how long the resulting secondary copies <b>116</b> will be kept (e.g., a number of days, months, years, etc.), perhaps depending on organizational needs and/or compliance criteria.</p><p id="p-0156" num="0198">When adding a new client computing device <b>102</b>, administrators can manually configure information management policies <b>148</b> and/or other settings, e.g., via user interface <b>158</b>. However, this can be an involved process resulting in delays, and it may be desirable to begin data protection operations quickly, without awaiting human intervention. Thus, in some embodiments, system <b>100</b> automatically applies a default configuration to client computing device <b>102</b>. As one example, when one or more data agent(s) <b>142</b> are installed on a client computing device <b>102</b>, the installation script may register the client computing device <b>102</b> with storage manager <b>140</b>, which in turn applies the default configuration to the new client computing device <b>102</b>. In this manner, data protection operations can begin substantially immediately. The default configuration can include a default storage policy, for example, and can specify any appropriate information sufficient to begin data protection operations. This can include a type of data protection operation, scheduling information, a target secondary storage device <b>108</b>, data path information (e.g., a particular media agent <b>144</b>), and the like.</p><p id="p-0157" num="0199">Another type of information management policy <b>148</b> is a &#x201c;scheduling policy,&#x201d; which specifies when and how often to perform operations. Scheduling parameters may specify with what frequency (e.g., hourly, weekly, daily, event-based, etc.) or under what triggering conditions secondary copy or other information management operations are to take place. Scheduling policies in some cases are associated with particular components, such as a subclient, client computing device <b>102</b>, and the like.</p><p id="p-0158" num="0200">Another type of information management policy <b>148</b> is an &#x201c;audit policy&#x201d; (or &#x201c;security policy&#x201d;), which comprises preferences, rules and/or criteria that protect sensitive data in system <b>100</b>. For example, an audit policy may define &#x201c;sensitive objects&#x201d; which are files or data objects that contain particular keywords (e.g., &#x201c;confidential,&#x201d; or &#x201c;privileged&#x201d;) and/or are associated with particular keywords (e.g., in metadata) or particular flags (e.g., in metadata identifying a document or email as personal, confidential, etc.). An audit policy may further specify rules for handling sensitive objects. As an example, an audit policy may require that a reviewer approve the transfer of any sensitive objects to a cloud storage site, and that if approval is denied for a particular sensitive object, the sensitive object should be transferred to a local primary storage device <b>104</b> instead. To facilitate this approval, the audit policy may further specify how a secondary storage computing device <b>106</b> or other system component should notify a reviewer that a sensitive object is slated for transfer.</p><p id="p-0159" num="0201">Another type of information management policy <b>148</b> is a &#x201c;provisioning policy,&#x201d; which can include preferences, priorities, rules, and/or criteria that specify how client computing devices <b>102</b> (or groups thereof) may utilize system resources, such as available storage on cloud storage and/or network bandwidth. A provisioning policy specifies, for example, data quotas for particular client computing devices <b>102</b> (e.g., a number of gigabytes that can be stored monthly, quarterly or annually). Storage manager <b>140</b> or other components may enforce the provisioning policy. For instance, media agents <b>144</b> may enforce the policy when transferring data to secondary storage devices <b>108</b>. If a client computing device <b>102</b> exceeds a quota, a budget for the client computing device <b>102</b> (or associated department) may be adjusted accordingly or an alert may trigger.</p><p id="p-0160" num="0202">While the above types of information management policies <b>148</b> are described as separate policies, one or more of these can be generally combined into a single information management policy <b>148</b>. For instance, a storage policy may also include or otherwise be associated with one or more scheduling, audit, or provisioning policies or operational parameters thereof. Moreover, while storage policies are typically associated with moving and storing data, other policies may be associated with other types of information management operations. The following is a non-exhaustive list of items that information management policies <b>148</b> may specify:<ul id="ul0004" list-style="none">    <li id="ul0004-0001" num="0000">    <ul id="ul0005" list-style="none">        <li id="ul0005-0001" num="0203">schedules or other timing information, e.g., specifying when and/or how often to perform information management operations;</li>        <li id="ul0005-0002" num="0204">the type of secondary copy <b>116</b> and/or copy format (e.g., snapshot, backup, archive, HSM, etc.);</li>        <li id="ul0005-0003" num="0205">a location or a class or quality of storage for storing secondary copies <b>116</b> (e.g., one or more particular secondary storage devices <b>108</b>);</li>        <li id="ul0005-0004" num="0206">preferences regarding whether and how to encrypt, compress, deduplicate, or otherwise modify or transform secondary copies <b>116</b>;</li>        <li id="ul0005-0005" num="0207">which system components and/or network pathways (e.g., preferred media agents <b>144</b>) should be used to perform secondary storage operations;</li>        <li id="ul0005-0006" num="0208">resource allocation among different computing devices or other system components used in performing information management operations (e.g., bandwidth allocation, available storage capacity, etc.);</li>        <li id="ul0005-0007" num="0209">whether and how to synchronize or otherwise distribute files or other data objects across multiple computing devices or hosted services; and</li>        <li id="ul0005-0008" num="0210">retention information specifying the length of time primary data <b>112</b> and/or secondary copies <b>116</b> should be retained, e.g., in a particular class or tier of storage devices, or within the system <b>100</b>.</li>    </ul>    </li></ul></p><p id="p-0161" num="0211">Information management policies <b>148</b> can additionally specify or depend on historical or current criteria that may be used to determine which rules to apply to a particular data object, system component, or information management operation, such as:<ul id="ul0006" list-style="none">    <li id="ul0006-0001" num="0000">    <ul id="ul0007" list-style="none">        <li id="ul0007-0001" num="0212">frequency with which primary data <b>112</b> or a secondary copy <b>116</b> of a data object or metadata has been or is predicted to be used, accessed, or modified;</li>        <li id="ul0007-0002" num="0213">time-related factors (e.g., aging information such as time since the creation or modification of a data object);</li>        <li id="ul0007-0003" num="0214">deduplication information (e.g., hashes, data blocks, deduplication block size, deduplication efficiency or other metrics);</li>        <li id="ul0007-0004" num="0215">an estimated or historic usage or cost associated with different components (e.g., with secondary storage devices <b>108</b>);</li>        <li id="ul0007-0005" num="0216">the identity of users, applications <b>110</b>, client computing devices <b>102</b> and/or other computing devices that created, accessed, modified, or otherwise utilized primary data <b>112</b> or secondary copies <b>116</b>;</li>        <li id="ul0007-0006" num="0217">a relative sensitivity (e.g., confidentiality, importance) of a data object, e.g., as determined by its content and/or metadata;</li>        <li id="ul0007-0007" num="0218">the current or historical storage capacity of various storage devices;</li>        <li id="ul0007-0008" num="0219">the current or historical network capacity of network pathways connecting various components within the storage operation cell;</li>        <li id="ul0007-0009" num="0220">access control lists or other security information; and</li>        <li id="ul0007-0010" num="0221">the content of a particular data object (e.g., its textual content) or of metadata associated with the data object.</li>    </ul>    </li></ul></p><p id="p-0162" num="0222">Exemplary Storage Policy and Secondary Copy Operations</p><p id="p-0163" num="0223"><figref idref="DRAWINGS">FIG. <b>1</b>E</figref> includes a data flow diagram depicting performance of secondary copy operations by an embodiment of information management system <b>100</b>, according to an exemplary storage policy <b>148</b>A. System <b>100</b> includes a storage manager <b>140</b>, a client computing device <b>102</b> having a file system data agent <b>142</b>A and an email data agent <b>142</b>B operating thereon, a primary storage device <b>104</b>, two media agents <b>144</b>A, <b>144</b>B, and two secondary storage devices <b>108</b>: a disk library <b>108</b>A and a tape library <b>108</b>B. As shown, primary storage device <b>104</b> includes primary data <b>112</b>A, which is associated with a logical grouping of data associated with a file system (&#x201c;file system subclient&#x201d;), and primary data <b>112</b>B, which is a logical grouping of data associated with email (&#x201c;email subclient&#x201d;). The techniques described with respect to <figref idref="DRAWINGS">FIG. <b>1</b>E</figref> can be utilized in conjunction with data that is otherwise organized as well.</p><p id="p-0164" num="0224">As indicated by the dashed box, the second media agent <b>144</b>B and tape library <b>108</b>B are &#x201c;off-site,&#x201d; and may be remotely located from the other components in system <b>100</b> (e.g., in a different city, office building, etc.). Indeed, &#x201c;off-site&#x201d; may refer to a magnetic tape located in remote storage, which must be manually retrieved and loaded into a tape drive to be read. In this manner, information stored on the tape library <b>108</b>B may provide protection in the event of a disaster or other failure at the main site(s) where data is stored.</p><p id="p-0165" num="0225">The file system subclient <b>112</b>A in certain embodiments generally comprises information generated by the file system and/or operating system of client computing device <b>102</b>, and can include, for example, file system data (e.g., regular files, file tables, mount points, etc.), operating system data (e.g., registries, event logs, etc.), and the like. The e-mail subclient <b>112</b>B can include data generated by an e-mail application operating on client computing device <b>102</b>, e.g., mailbox information, folder information, emails, attachments, associated database information, and the like. As described above, the subclients can be logical containers, and the data included in the corresponding primary data <b>112</b>A and <b>112</b>B may or may not be stored contiguously.</p><p id="p-0166" num="0226">The exemplary storage policy <b>148</b>A includes backup copy preferences or rule set <b>160</b>, disaster recovery copy preferences or rule set <b>162</b>, and compliance copy preferences or rule set <b>164</b>. Backup copy rule set <b>160</b> specifies that it is associated with file system subclient <b>166</b> and email subclient <b>168</b>. Each of subclients <b>166</b> and <b>168</b> are associated with the particular client computing device <b>102</b>. Backup copy rule set <b>160</b> further specifies that the backup operation will be written to disk library <b>108</b>A and designates a particular media agent <b>144</b>A to convey the data to disk library <b>108</b>A. Finally, backup copy rule set <b>160</b> specifies that backup copies created according to rule set <b>160</b> are scheduled to be generated hourly and are to be retained for 30 days. In some other embodiments, scheduling information is not included in storage policy <b>148</b>A and is instead specified by a separate scheduling policy.</p><p id="p-0167" num="0227">Disaster recovery copy rule set <b>162</b> is associated with the same two subclients <b>166</b> and <b>168</b>. However, disaster recovery copy rule set <b>162</b> is associated with tape library <b>108</b>B, unlike backup copy rule set <b>160</b>. Moreover, disaster recovery copy rule set <b>162</b> specifies that a different media agent, namely <b>144</b>B, will convey data to tape library <b>108</b>B. Disaster recovery copies created according to rule set <b>162</b> will be retained for 60 days and will be generated daily. Disaster recovery copies generated according to disaster recovery copy rule set <b>162</b> can provide protection in the event of a disaster or other catastrophic data loss that would affect the backup copy <b>116</b>A maintained on disk library <b>108</b>A.</p><p id="p-0168" num="0228">Compliance copy rule set <b>164</b> is only associated with the email subclient <b>168</b>, and not the file system subclient <b>166</b>. Compliance copies generated according to compliance copy rule set <b>164</b> will therefore not include primary data <b>112</b>A from the file system subclient <b>166</b>. For instance, the organization may be under an obligation to store and maintain copies of email data for a particular period of time (e.g., 10 years) to comply with state or federal regulations, while similar regulations do not apply to file system data. Compliance copy rule set <b>164</b> is associated with the same tape library <b>108</b>B and media agent <b>144</b>B as disaster recovery copy rule set <b>162</b>, although a different storage device or media agent could be used in other embodiments. Finally, compliance copy rule set <b>164</b> specifies that the copies it governs will be generated quarterly and retained for 10 years.</p><p id="p-0169" num="0229">Secondary Copy Jobs</p><p id="p-0170" num="0230">A logical grouping of secondary copy operations governed by a rule set and being initiated at a point in time may be referred to as a &#x201c;secondary copy job&#x201d; (and sometimes may be called a &#x201c;backup job,&#x201d; even though it is not necessarily limited to creating only backup copies). Secondary copy jobs may be initiated on demand as well. Steps <b>1</b>-<b>9</b> below illustrate three secondary copy jobs based on storage policy <b>148</b>A.</p><p id="p-0171" num="0231">Referring to <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>, at step <b>1</b>, storage manager <b>140</b> initiates a backup job according to the backup copy rule set <b>160</b>, which logically comprises all the secondary copy operations necessary to effectuate rules <b>160</b> in storage policy <b>148</b>A every hour, including steps <b>1</b>-<b>4</b> occurring hourly. For instance, a scheduling service running on storage manager <b>140</b> accesses backup copy rule set <b>160</b> or a separate scheduling policy associated with client computing device <b>102</b> and initiates a backup job on an hourly basis. Thus, at the scheduled time, storage manager <b>140</b> sends instructions to client computing device <b>102</b> (i.e., to both data agent <b>142</b>A and data agent <b>142</b>B) to begin the backup job.</p><p id="p-0172" num="0232">At step <b>2</b>, file system data agent <b>142</b>A and email data agent <b>142</b>B on client computing device <b>102</b> respond to instructions from storage manager <b>140</b> by accessing and processing the respective subclient primary data <b>112</b>A and <b>112</b>B involved in the backup copy operation, which can be found in primary storage device <b>104</b>. Because the secondary copy operation is a backup copy operation, the data agent(s) <b>142</b>A, <b>142</b>B may format the data into a backup format or otherwise process the data suitable for a backup copy.</p><p id="p-0173" num="0233">At step <b>3</b>, client computing device <b>102</b> communicates the processed file system data (e.g., using file system data agent <b>142</b>A) and the processed email data (e.g., using email data agent <b>142</b>B) to the first media agent <b>144</b>A according to backup copy rule set <b>160</b>, as directed by storage manager <b>140</b>. Storage manager <b>140</b> may further keep a record in management database <b>146</b> of the association between media agent <b>144</b>A and one or more of: client computing device <b>102</b>, file system subclient <b>112</b>A, file system data agent <b>142</b>A, email subclient <b>112</b>B, email data agent <b>142</b>B, and/or backup copy <b>116</b>A.</p><p id="p-0174" num="0234">The target media agent <b>144</b>A receives the data-agent-processed data from client computing device <b>102</b>, and at step <b>4</b> generates and conveys backup copy <b>116</b>A to disk library <b>108</b>A to be stored as backup copy <b>116</b>A, again at the direction of storage manager <b>140</b> and according to backup copy rule set <b>160</b>. Media agent <b>144</b>A can also update its index <b>153</b> to include data and/or metadata related to backup copy <b>116</b>A, such as information indicating where the backup copy <b>116</b>A resides on disk library <b>108</b>A, where the email copy resides, where the file system copy resides, data and metadata for cache retrieval, etc. Storage manager <b>140</b> may similarly update its index <b>150</b> to include information relating to the secondary copy operation, such as information relating to the type of operation, a physical location associated with one or more copies created by the operation, the time the operation was performed, status information relating to the operation, the components involved in the operation, and the like. In some cases, storage manager <b>140</b> may update its index <b>150</b> to include some or all of the information stored in index <b>153</b> of media agent <b>144</b>A. At this point, the backup job may be considered complete. After the 30-day retention period expires, storage manager <b>140</b> instructs media agent <b>144</b>A to delete backup copy <b>116</b>A from disk library <b>108</b>A and indexes <b>150</b> and/or <b>153</b> are updated accordingly.</p><p id="p-0175" num="0235">At step <b>5</b>, storage manager <b>140</b> initiates another backup job for a disaster recovery copy according to the disaster recovery rule set <b>162</b>. Illustratively this includes steps <b>5</b>-<b>7</b> occurring daily for creating disaster recovery copy <b>1168</b>. Illustratively, and by way of illustrating the scalable aspects and off-loading principles embedded in system <b>100</b>, disaster recovery copy <b>1168</b> is based on backup copy <b>116</b>A and not on primary data <b>112</b>A and <b>1128</b>.</p><p id="p-0176" num="0236">At step <b>6</b>, illustratively based on instructions received from storage manager <b>140</b> at step <b>5</b>, the specified media agent <b>1448</b> retrieves the most recent backup copy <b>116</b>A from disk library <b>108</b>A.</p><p id="p-0177" num="0237">At step <b>7</b>, again at the direction of storage manager <b>140</b> and as specified in disaster recovery copy rule set <b>162</b>, media agent <b>144</b>B uses the retrieved data to create a disaster recovery copy <b>1168</b> and store it to tape library <b>1088</b>. In some cases, disaster recovery copy <b>1168</b> is a direct, mirror copy of backup copy <b>116</b>A, and remains in the backup format. In other embodiments, disaster recovery copy <b>1168</b> may be further compressed or encrypted, or may be generated in some other manner, such as by using primary data <b>112</b>A and <b>1128</b> from primary storage device <b>104</b> as sources. The disaster recovery copy operation is initiated once a day and disaster recovery copies <b>1168</b> are deleted after 60 days; indexes <b>153</b> and/or <b>150</b> are updated accordingly when/after each information management operation is executed and/or completed. The present backup job may be considered completed.</p><p id="p-0178" num="0238">At step <b>8</b>, storage manager <b>140</b> initiates another backup job according to compliance rule set <b>164</b>, which performs steps <b>8</b>-<b>9</b> quarterly to create compliance copy <b>116</b>C. For instance, storage manager <b>140</b> instructs media agent <b>144</b>B to create compliance copy <b>116</b>C on tape library <b>1088</b>, as specified in the compliance copy rule set <b>164</b>.</p><p id="p-0179" num="0239">At step <b>9</b> in the example, compliance copy <b>116</b>C is generated using disaster recovery copy <b>1168</b> as the source. This is efficient, because disaster recovery copy resides on the same secondary storage device and thus no network resources are required to move the data. In other embodiments, compliance copy <b>116</b>C is instead generated using primary data <b>1128</b> corresponding to the email subclient or using backup copy <b>116</b>A from disk library <b>108</b>A as source data. As specified in the illustrated example, compliance copies <b>116</b>C are created quarterly, and are deleted after ten years, and indexes <b>153</b> and/or <b>150</b> are kept up-to-date accordingly.</p><p id="p-0180" num="0240">Exemplary Applications of Storage Policies&#x2014;Information Governance Policies and Classification</p><p id="p-0181" num="0241">Again referring to <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>, storage manager <b>140</b> may permit a user to specify aspects of storage policy <b>148</b>A. For example, the storage policy can be modified to include information governance policies to define how data should be managed in order to comply with a certain regulation or business objective. The various policies may be stored, for example, in management database <b>146</b>. An information governance policy may align with one or more compliance tasks that are imposed by regulations or business requirements. Examples of information governance policies might include a Sarbanes-Oxley policy, a HIPAA policy, an electronic discovery (e-discovery) policy, and so on.</p><p id="p-0182" num="0242">Information governance policies allow administrators to obtain different perspectives on an organization's online and offline data, without the need for a dedicated data silo created solely for each different viewpoint. As described previously, the data storage systems herein build an index that reflects the contents of a distributed data set that spans numerous clients and storage devices, including both primary data and secondary copies, and online and offline copies. An organization may apply multiple information governance policies in a top-down manner over that unified data set and indexing schema in order to view and manipulate the data set through different lenses, each of which is adapted to a particular compliance or business goal. Thus, for example, by applying an e-discovery policy and a Sarbanes-Oxley policy, two different groups of users in an organization can conduct two very different analyses of the same underlying physical set of data/copies, which may be distributed throughout the information management system.</p><p id="p-0183" num="0243">An information governance policy may comprise a classification policy, which defines a taxonomy of classification terms or tags relevant to a compliance task and/or business objective. A classification policy may also associate a defined tag with a classification rule. A classification rule defines a particular combination of criteria, such as users who have created, accessed or modified a document or data object; file or application types; content or metadata keywords; clients or storage locations; dates of data creation and/or access; review status or other status within a workflow (e.g., reviewed or un-reviewed); modification times or types of modifications; and/or any other data attributes in any combination, without limitation. A classification rule may also be defined using other classification tags in the taxonomy. The various criteria used to define a classification rule may be combined in any suitable fashion, for example, via Boolean operators, to define a complex classification rule. As an example, an e-discovery classification policy might define a classification tag &#x201c;privileged&#x201d; that is associated with documents or data objects that (1) were created or modified by legal department staff, or (2) were sent to or received from outside counsel via email, or (3) contain one of the following keywords: &#x201c;privileged&#x201d; or &#x201c;attorney&#x201d; or &#x201c;counsel,&#x201d; or other like terms. Accordingly, all these documents or data objects will be classified as &#x201c;privileged.&#x201d;</p><p id="p-0184" num="0244">One specific type of classification tag, which may be added to an index at the time of indexing, is an &#x201c;entity tag.&#x201d; An entity tag may be, for example, any content that matches a defined data mask format. Examples of entity tags might include, e.g., social security numbers (e.g., any numerical content matching the formatting mask XXX-XX-XXXX), credit card numbers (e.g., content having a 13-16 digit string of numbers), SKU numbers, product numbers, etc. A user may define a classification policy by indicating criteria, parameters or descriptors of the policy via a graphical user interface, such as a form or page with fields to be filled in, pull-down menus or entries allowing one or more of several options to be selected, buttons, sliders, hypertext links or other known user interface tools for receiving user input, etc. For example, a user may define certain entity tags, such as a particular product number or project ID. In some implementations, the classification policy can be implemented using cloud-based techniques. For example, the storage devices may be cloud storage devices, and the storage manager <b>140</b> may execute cloud service provider API over a network to classify data stored on cloud storage devices.</p><p id="p-0185" num="0000">Restore Operations from Secondary Copies</p><p id="p-0186" num="0245">While not shown in <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>, at some later point in time, a restore operation can be initiated involving one or more of secondary copies <b>116</b>A, <b>116</b>B, and <b>116</b>C. A restore operation logically takes a selected secondary copy <b>116</b>, reverses the effects of the secondary copy operation that created it, and stores the restored data to primary storage where a client computing device <b>102</b> may properly access it as primary data. A media agent <b>144</b> and an appropriate data agent <b>142</b> (e.g., executing on the client computing device <b>102</b>) perform the tasks needed to complete a restore operation. For example, data that was encrypted, compressed, and/or deduplicated in the creation of secondary copy <b>116</b> will be correspondingly rehydrated (reversing deduplication), uncompressed, and unencrypted into a format appropriate to primary data. Metadata stored within or associated with the secondary copy <b>116</b> may be used during the restore operation. In general, restored data should be indistinguishable from other primary data <b>112</b>. Preferably, the restored data has fully regained the native format that may make it immediately usable by application <b>110</b>.</p><p id="p-0187" num="0246">As one example, a user may manually initiate a restore of backup copy <b>116</b>A, e.g., by interacting with user interface <b>158</b> of storage manager <b>140</b> or with a web-based console with access to system <b>100</b>. Storage manager <b>140</b> may accesses data in its index <b>150</b> and/or management database <b>146</b> (and/or the respective storage policy <b>148</b>A) associated with the selected backup copy <b>116</b>A to identify the appropriate media agent <b>144</b>A and/or secondary storage device <b>108</b>A where the secondary copy resides. The user may be presented with a representation (e.g., stub, thumbnail, listing, etc.) and metadata about the selected secondary copy, in order to determine whether this is the appropriate copy to be restored, e.g., date that the original primary data was created. Storage manager <b>140</b> will then instruct media agent <b>144</b>A and an appropriate data agent <b>142</b> on the target client computing device <b>102</b> to restore secondary copy <b>116</b>A to primary storage device <b>104</b>. A media agent may be selected for use in the restore operation based on a load balancing algorithm, an availability based algorithm, or other criteria. The selected media agent, e.g., <b>144</b>A, retrieves secondary copy <b>116</b>A from disk library <b>108</b>A. For instance, media agent <b>144</b>A may access its index <b>153</b> to identify a location of backup copy <b>116</b>A on disk library <b>108</b>A, or may access location information residing on disk library <b>108</b>A itself.</p><p id="p-0188" num="0247">In some cases a backup copy <b>116</b>A that was recently created or accessed, may be cached to speed up the restore operation. In such a case, media agent <b>144</b>A accesses a cached version of backup copy <b>116</b>A residing in index <b>153</b>, without having to access disk library <b>108</b>A for some or all of the data. Once it has retrieved backup copy <b>116</b>A, the media agent <b>144</b>A communicates the data to the requesting client computing device <b>102</b>. Upon receipt, file system data agent <b>142</b>A and email data agent <b>142</b>B may unpack (e.g., restore from a backup format to the native application format) the data in backup copy <b>116</b>A and restore the unpackaged data to primary storage device <b>104</b>. In general, secondary copies <b>116</b> may be restored to the same volume or folder in primary storage device <b>104</b> from which the secondary copy was derived; to another storage location or client computing device <b>102</b>; to shared storage, etc. In some cases, the data may be restored so that it may be used by an application <b>110</b> of a different version/vintage from the application that created the original primary data <b>112</b>.</p><heading id="h-0017" level="2">Exemplary Secondary Copy Formatting</heading><p id="p-0189" num="0248">The formatting and structure of secondary copies <b>116</b> can vary depending on the embodiment. In some cases, secondary copies <b>116</b> are formatted as a series of logical data units or &#x201c;chunks&#x201d; (e.g., 512 MB, 1 GB, 2 GB, 4 GB, or 8 GB chunks). This can facilitate efficient communication and writing to secondary storage devices <b>108</b>, e.g., according to resource availability. For example, a single secondary copy <b>116</b> may be written on a chunk-by-chunk basis to one or more secondary storage devices <b>108</b>. In some cases, users can select different chunk sizes, e.g., to improve throughput to tape storage devices. Generally, each chunk can include a header and a payload. The payload can include files (or other data units) or subsets thereof included in the chunk, whereas the chunk header generally includes metadata relating to the chunk, some or all of which may be derived from the payload. For example, during a secondary copy operation, media agent <b>144</b>, storage manager <b>140</b>, or other component may divide files into chunks and generate headers for each chunk by processing the files. Headers can include a variety of information such as file and/or volume identifier(s), offset(s), and/or other information associated with the payload data items, a chunk sequence number, etc. Importantly, in addition to being stored with secondary copy <b>116</b> on secondary storage device <b>108</b>, chunk headers can also be stored to index <b>153</b> of the associated media agent(s) <b>144</b> and/or to index <b>150</b> associated with storage manager <b>140</b>. This can be useful for providing faster processing of secondary copies <b>116</b> during browsing, restores, or other operations. In some cases, once a chunk is successfully transferred to a secondary storage device <b>108</b>, the secondary storage device <b>108</b> returns an indication of receipt, e.g., to media agent <b>144</b> and/or storage manager <b>140</b>, which may update their respective indexes <b>153</b>, <b>150</b> accordingly. During restore, chunks may be processed (e.g., by media agent <b>144</b>) according to the information in the chunk header to reassemble the files.</p><p id="p-0190" num="0249">Data can also be communicated within system <b>100</b> in data channels that connect client computing devices <b>102</b> to secondary storage devices <b>108</b>. These data channels can be referred to as &#x201c;data streams,&#x201d; and multiple data streams can be employed to parallelize an information management operation, improving data transfer rate, among other advantages. Example data formatting techniques including techniques involving data streaming, chunking, and the use of other data structures in creating secondary copies are described in U.S. Pat. Nos. 7,315,923, 8,156,086, and 8,578,120.</p><p id="p-0191" num="0250"><figref idref="DRAWINGS">FIGS. <b>1</b>F and <b>1</b>G</figref> are diagrams of example data streams <b>170</b> and <b>171</b>, respectively, which may be employed for performing information management operations. Referring to <figref idref="DRAWINGS">FIG. <b>1</b>F</figref>, data agent <b>142</b> forms data stream <b>170</b> from source data associated with a client computing device <b>102</b> (e.g., primary data <b>112</b>). Data stream <b>170</b> is composed of multiple pairs of stream header <b>172</b> and stream data (or stream payload) <b>174</b>. Data streams <b>170</b> and <b>171</b> shown in the illustrated example are for a single-instanced storage operation, and a stream payload <b>174</b> therefore may include both single-instance (SI) data and/or non-SI data. A stream header <b>172</b> includes metadata about the stream payload <b>174</b>. This metadata may include, for example, a length of the stream payload <b>174</b>, an indication of whether the stream payload <b>174</b> is encrypted, an indication of whether the stream payload <b>174</b> is compressed, an archive file identifier (ID), an indication of whether the stream payload <b>174</b> is single instanceable, and an indication of whether the stream payload <b>174</b> is a start of a block of data.</p><p id="p-0192" num="0251">Referring to <figref idref="DRAWINGS">FIG. <b>1</b>G</figref>, data stream <b>171</b> has the stream header <b>172</b> and stream payload <b>174</b> aligned into multiple data blocks. In this example, the data blocks are of size 64 KB. The first two stream header <b>172</b> and stream payload <b>174</b> pairs comprise a first data block of size 64 KB. The first stream header <b>172</b> indicates that the length of the succeeding stream payload <b>174</b> is 63 KB and that it is the start of a data block. The next stream header <b>172</b> indicates that the succeeding stream payload <b>174</b> has a length of 1 KB and that it is not the start of a new data block. Immediately following stream payload <b>174</b> is a pair comprising an identifier header <b>176</b> and identifier data <b>178</b>. The identifier header <b>176</b> includes an indication that the succeeding identifier data <b>178</b> includes the identifier for the immediately previous data block. The identifier data <b>178</b> includes the identifier that the data agent <b>142</b> generated for the data block. The data stream <b>171</b> also includes other stream header <b>172</b> and stream payload <b>174</b> pairs, which may be for SI data and/or non-SI data.</p><p id="p-0193" num="0252"><figref idref="DRAWINGS">FIG. <b>1</b>H</figref> is a diagram illustrating data structures <b>180</b> that may be used to store blocks of SI data and non-SI data on a storage device (e.g., secondary storage device <b>108</b>). According to certain embodiments, data structures <b>180</b> do not form part of a native file system of the storage device. Data structures <b>180</b> include one or more volume folders <b>182</b>, one or more chunk folders <b>184</b>/<b>185</b> within the volume folder <b>182</b>, and multiple files within chunk folder <b>184</b>. Each chunk folder <b>184</b>/<b>185</b> includes a metadata file <b>186</b>/<b>187</b>, a metadata index file <b>188</b>/<b>189</b>, one or more container files <b>190</b>/<b>191</b>/<b>193</b>, and a container index file <b>192</b>/<b>194</b>. Metadata file <b>186</b>/<b>187</b> stores non-SI data blocks as well as links to SI data blocks stored in container files. Metadata index file <b>188</b>/<b>189</b> stores an index to the data in the metadata file <b>186</b>/<b>187</b>. Container files <b>190</b>/<b>191</b>/<b>193</b> store SI data blocks. Container index file <b>192</b>/<b>194</b> stores an index to container files <b>190</b>/<b>191</b>/<b>193</b>. Among other things, container index file <b>192</b>/<b>194</b> stores an indication of whether a corresponding block in a container file <b>190</b>/<b>191</b>/<b>193</b> is referred to by a link in a metadata file <b>186</b>/<b>187</b>. For example, data block B<b>2</b> in the container file <b>190</b> is referred to by a link in metadata file <b>187</b> in chunk folder <b>185</b>. Accordingly, the corresponding index entry in container index file <b>192</b> indicates that data block B<b>2</b> in container file <b>190</b> is referred to. As another example, data block B<b>1</b> in container file <b>191</b> is referred to by a link in metadata file <b>187</b>, and so the corresponding index entry in container index file <b>192</b> indicates that this data block is referred to.</p><p id="p-0194" num="0253">As an example, data structures <b>180</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>H</figref> may have been created as a result of separate secondary copy operations involving two client computing devices <b>102</b>. For example, a first secondary copy operation on a first client computing device <b>102</b> could result in the creation of the first chunk folder <b>184</b>, and a second secondary copy operation on a second client computing device <b>102</b> could result in the creation of the second chunk folder <b>185</b>. Container files <b>190</b>/<b>191</b> in the first chunk folder <b>184</b> would contain the blocks of SI data of the first client computing device <b>102</b>. If the two client computing devices <b>102</b> have substantially similar data, the second secondary copy operation on the data of the second client computing device <b>102</b> would result in media agent <b>144</b> storing primarily links to the data blocks of the first client computing device <b>102</b> that are already stored in the container files <b>190</b>/<b>191</b>. Accordingly, while a first secondary copy operation may result in storing nearly all of the data subject to the operation, subsequent secondary storage operations involving similar data may result in substantial data storage space savings, because links to already stored data blocks can be stored instead of additional instances of data blocks.</p><p id="p-0195" num="0254">If the operating system of the secondary storage computing device <b>106</b> on which media agent <b>144</b> operates supports sparse files, then when media agent <b>144</b> creates container files <b>190</b>/<b>191</b>/<b>193</b>, it can create them as sparse files. A sparse file is a type of file that may include empty space (e.g., a sparse file may have real data within it, such as at the beginning of the file and/or at the end of the file, but may also have empty space in it that is not storing actual data, such as a contiguous range of bytes all having a value of zero). Having container files <b>190</b>/<b>191</b>/<b>193</b> be sparse files allows media agent <b>144</b> to free up space in container files <b>190</b>/<b>191</b>/<b>193</b> when blocks of data in container files <b>190</b>/<b>191</b>/<b>193</b> no longer need to be stored on the storage devices. In some examples, media agent <b>144</b> creates a new container file <b>190</b>/<b>191</b>/<b>193</b> when a container file <b>190</b>/<b>191</b>/<b>193</b> either includes 100 blocks of data or when the size of the container file <b>190</b> exceeds 50 MB. In other examples, media agent <b>144</b> creates a new container file <b>190</b>/<b>191</b>/<b>193</b> when a container file <b>190</b>/<b>191</b>/<b>193</b> satisfies other criteria (e.g., it contains from approx. 100 to approx. 1000 blocks or when its size exceeds approximately 50 MB to 1 GB). In some cases, a file on which a secondary copy operation is performed may comprise a large number of data blocks. For example, a 100 MB file may comprise 400 data blocks of size 256 KB. If such a file is to be stored, its data blocks may span more than one container file, or even more than one chunk folder. As another example, a database file of 20 GB may comprise over 40,000 data blocks of size 512 KB. If such a database file is to be stored, its data blocks will likely span multiple container files, multiple chunk folders, and potentially multiple volume folders. Restoring such files may require accessing multiple container files, chunk folders, and/or volume folders to obtain the requisite data blocks.</p><heading id="h-0018" level="2">Using Backup Data for Replication and Disaster Recovery (&#x201c;Live Synchronization&#x201d;)</heading><p id="p-0196" num="0255">There is an increased demand to off-load resource intensive information management tasks (e.g., data replication tasks) away from production devices (e.g., physical or virtual client computing devices) in order to maximize production efficiency. At the same time, enterprises expect access to readily-available up-to-date recovery copies in the event of failure, with little or no production downtime.</p><p id="p-0197" num="0256"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> illustrates a system <b>200</b> configured to address these and other issues by using backup or other secondary copy data to synchronize a source subsystem <b>201</b> (e.g., a production site) with a destination subsystem <b>203</b> (e.g., a failover site). Such a technique can be referred to as &#x201c;live synchronization&#x201d; and/or &#x201c;live synchronization replication.&#x201d; In the illustrated embodiment, the source client computing devices <b>202</b><i>a </i>include one or more virtual machines (or &#x201c;VMs&#x201d;) executing on one or more corresponding VM host computers <b>205</b><i>a</i>, though the source need not be virtualized. The destination site <b>203</b> may be at a location that is remote from the production site <b>201</b>, or may be located in the same data center, without limitation. One or more of the production site <b>201</b> and destination site <b>203</b> may reside at data centers at known geographic locations, or alternatively may operate &#x201c;in the cloud.&#x201d;</p><p id="p-0198" num="0257">The synchronization can be achieved by generally applying an ongoing stream of incremental backups from the source subsystem <b>201</b> to the destination subsystem <b>203</b>, such as according to what can be referred to as an &#x201c;incremental forever&#x201d; approach. <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> illustrates an embodiment of a data flow which may be orchestrated at the direction of one or more storage managers (not shown). At step <b>1</b>, the source data agent(s) <b>242</b><i>a </i>and source media agent(s) <b>244</b><i>a </i>work together to write backup or other secondary copies of the primary data generated by the source client computing devices <b>202</b><i>a </i>into the source secondary storage device(s) <b>208</b><i>a</i>. At step <b>2</b>, the backup/secondary copies are retrieved by the source media agent(s) <b>244</b><i>a </i>from secondary storage. At step <b>3</b>, source media agent(s) <b>244</b><i>a </i>communicate the backup/secondary copies across a network to the destination media agent(s) <b>244</b><i>b </i>in destination subsystem <b>203</b>.</p><p id="p-0199" num="0258">As shown, the data can be copied from source to destination in an incremental fashion, such that only changed blocks are transmitted, and in some cases multiple incremental backups are consolidated at the source so that only the most current changed blocks are transmitted to and applied at the destination. An example of live synchronization of virtual machines using the &#x201c;incremental forever&#x201d; approach is found in U.S. Patent Application No. 62/265,339 entitled &#x201c;Live Synchronization and Management of Virtual Machines across Computing and Virtualization Platforms and Using Live Synchronization to Support Disaster Recovery.&#x201d; Moreover, a deduplicated copy can be employed to further reduce network traffic from source to destination. For instance, the system can utilize the deduplicated copy techniques described in U.S. Pat. No. 9,239,687, entitled &#x201c;Systems and Methods for Retaining and Using Data Block Signatures in Data Protection Operations.&#x201d;</p><p id="p-0200" num="0259">At step <b>4</b>, destination media agent(s) <b>244</b><i>b </i>write the received backup/secondary copy data to the destination secondary storage device(s) <b>208</b><i>b</i>. At step <b>5</b>, the synchronization is completed when the destination media agent(s) and destination data agent(s) <b>242</b><i>b </i>restore the backup/secondary copy data to the destination client computing device(s) <b>202</b><i>b</i>. The destination client computing device(s) <b>202</b><i>b </i>may be kept &#x201c;warm&#x201d; awaiting activation in case failure is detected at the source. This synchronization/replication process can incorporate the techniques described in U.S. patent application Ser. No. 14/721,971, entitled &#x201c;Replication Using Deduplicated Secondary Copy Data.&#x201d;</p><p id="p-0201" num="0260">Where the incremental backups are applied on a frequent, on-going basis, the synchronized copies can be viewed as mirror or replication copies. Moreover, by applying the incremental backups to the destination site <b>203</b> using backup or other secondary copy data, the production site <b>201</b> is not burdened with the synchronization operations. Because the destination site <b>203</b> can be maintained in a synchronized &#x201c;warm&#x201d; state, the downtime for switching over from the production site <b>201</b> to the destination site <b>203</b> is substantially less than with a typical restore from secondary storage. Thus, the production site <b>201</b> may flexibly and efficiently fail over, with minimal downtime and with relatively up-to-date data, to a destination site <b>203</b>, such as a cloud-based failover site. The destination site <b>203</b> can later be reverse synchronized back to the production site <b>201</b>, such as after repairs have been implemented or after the failure has passed.</p><p id="p-0202" num="0000">Integrating with the Cloud Using File System Protocols</p><p id="p-0203" num="0261">Given the ubiquity of cloud computing, it can be increasingly useful to provide data protection and other information management services in a scalable, transparent, and highly plug-able fashion. <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> illustrates an information management system <b>200</b> having an architecture that provides such advantages, and incorporates use of a standard file system protocol between primary and secondary storage subsystems <b>217</b>, <b>218</b>. As shown, the use of the network file system (NFS) protocol (or any another appropriate file system protocol such as that of the Common Internet File System (CIFS)) allows data agent <b>242</b> to be moved from the primary storage subsystem <b>217</b> to the secondary storage subsystem <b>218</b>. For instance, as indicated by the dashed box <b>206</b> around data agent <b>242</b> and media agent <b>244</b>, data agent <b>242</b> can co-reside with media agent <b>244</b> on the same server (e.g., a secondary storage computing device such as component <b>106</b>), or in some other location in secondary storage subsystem <b>218</b>.</p><p id="p-0204" num="0262">Where NFS is used, for example, secondary storage subsystem <b>218</b> allocates an NFS network path to the client computing device <b>202</b> or to one or more target applications <b>210</b> running on client computing device <b>202</b>. During a backup or other secondary copy operation, the client computing device <b>202</b> mounts the designated NFS path and writes data to that NFS path. The NFS path may be obtained from NFS path data <b>215</b> stored locally at the client computing device <b>202</b>, and which may be a copy of or otherwise derived from NFS path data <b>219</b> stored in the secondary storage subsystem <b>218</b>.</p><p id="p-0205" num="0263">Write requests issued by client computing device(s) <b>202</b> are received by data agent <b>242</b> in secondary storage subsystem <b>218</b>, which translates the requests and works in conjunction with media agent <b>244</b> to process and write data to a secondary storage device(s) <b>208</b>, thereby creating a backup or other secondary copy. Storage manager <b>240</b> can include a pseudo-client manager <b>217</b>, which coordinates the process by, among other things, communicating information relating to client computing device <b>202</b> and application <b>210</b> (e.g., application type, client computing device identifier, etc.) to data agent <b>242</b>, obtaining appropriate NFS path data from the data agent <b>242</b> (e.g., NFS path information), and delivering such data to client computing device <b>202</b>.</p><p id="p-0206" num="0264">Conversely, during a restore or recovery operation client computing device <b>202</b> reads from the designated NFS network path, and the read request is translated by data agent <b>242</b>. The data agent <b>242</b> then works with media agent <b>244</b> to retrieve, re-process (e.g., re-hydrate, decompress, decrypt), and forward the requested data to client computing device <b>202</b> using NFS.</p><p id="p-0207" num="0265">By moving specialized software associated with system <b>200</b> such as data agent <b>242</b> off the client computing devices <b>202</b>, the illustrative architecture effectively decouples the client computing devices <b>202</b> from the installed components of system <b>200</b>, improving both scalability and plug-ability of system <b>200</b>. Indeed, the secondary storage subsystem <b>218</b> in such environments can be treated simply as a read/write NFS target for primary storage subsystem <b>217</b>, without the need for information management software to be installed on client computing devices <b>202</b>. As one example, an enterprise implementing a cloud production computing environment can add VM client computing devices <b>202</b> without installing and configuring specialized information management software on these VMs. Rather, backups and restores are achieved transparently, where the new VMs simply write to and read from the designated NFS path. An example of integrating with the cloud using file system protocols or so-called &#x201c;infinite backup&#x201d; using NFS share is found in U.S. Patent Application No. 62/294,920, entitled &#x201c;Data Protection Operations Based on Network Path Information.&#x201d; Examples of improved data restoration scenarios based on network-path information, including using stored backups effectively as primary data sources, may be found in U.S. Patent Application No. 62/297,057, entitled &#x201c;Data Restoration Operations Based on Network Path Information.&#x201d;</p><heading id="h-0019" level="2">Highly Scalable Managed Data Pool Architecture</heading><p id="p-0208" num="0266">Enterprises are seeing explosive data growth in recent years, often from various applications running in geographically distributed locations. <figref idref="DRAWINGS">FIG. <b>2</b>C</figref> shows a block diagram of an example of a highly scalable, managed data pool architecture useful in accommodating such data growth. The illustrated system <b>200</b>, which may be referred to as a &#x201c;web-scale&#x201d; architecture according to certain embodiments, can be readily incorporated into both open compute/storage and common-cloud architectures.</p><p id="p-0209" num="0267">The illustrated system <b>200</b> includes a grid <b>245</b> of media agents <b>244</b> logically organized into a control tier <b>231</b> and a secondary or storage tier <b>233</b>. Media agents assigned to the storage tier <b>233</b> can be configured to manage a secondary storage pool <b>208</b> as a deduplication store, and be configured to receive client write and read requests from the primary storage subsystem <b>217</b>, and direct those requests to the secondary tier <b>233</b> for servicing. For instance, media agents CMA<b>1</b>-CMA<b>3</b> in the control tier <b>231</b> maintain and consult one or more deduplication databases <b>247</b>, which can include deduplication information (e.g., data block hashes, data block links, file containers for deduplicated files, etc.) sufficient to read deduplicated files from secondary storage pool <b>208</b> and write deduplicated files to secondary storage pool <b>208</b>. For instance, system <b>200</b> can incorporate any of the deduplication systems and methods shown and described in U.S. Pat. No. 9,020,900, entitled &#x201c;Distributed Deduplicated Storage System,&#x201d; and U.S. Pat. Pub. No. 2014/0201170, entitled &#x201c;High Availability Distributed Deduplicated Storage System.&#x201d;</p><p id="p-0210" num="0268">Media agents SMA<b>1</b>-SMA<b>6</b> assigned to the secondary tier <b>233</b> receive write and read requests from media agents CMA<b>1</b>-CMA<b>3</b> in control tier <b>231</b>, and access secondary storage pool <b>208</b> to service those requests. Media agents CMA<b>1</b>-CMA<b>3</b> in control tier <b>231</b> can also communicate with secondary storage pool <b>208</b>, and may execute read and write requests themselves (e.g., in response to requests from other control media agents CMA<b>1</b>-CMA<b>3</b>) in addition to issuing requests to media agents in secondary tier <b>233</b>. Moreover, while shown as separate from the secondary storage pool <b>208</b>, deduplication database(s) <b>247</b> can in some cases reside in storage devices in secondary storage pool <b>208</b>.</p><p id="p-0211" num="0269">As shown, each of the media agents <b>244</b> (e.g., CMA<b>1</b>-CMA<b>3</b>, SMA<b>1</b>-SMA<b>6</b>, etc.) in grid <b>245</b> can be allocated a corresponding dedicated partition <b>251</b>A-<b>2511</b>, respectively, in secondary storage pool <b>208</b>. Each partition <b>251</b> can include a first portion <b>253</b> containing data associated with (e.g., stored by) media agent <b>244</b> corresponding to the respective partition <b>251</b>. System <b>200</b> can also implement a desired level of replication, thereby providing redundancy in the event of a failure of a media agent <b>244</b> in grid <b>245</b>. Along these lines, each partition <b>251</b> can further include a second portion <b>255</b> storing one or more replication copies of the data associated with one or more other media agents <b>244</b> in the grid.</p><p id="p-0212" num="0270">System <b>200</b> can also be configured to allow for seamless addition of media agents <b>244</b> to grid <b>245</b> via automatic configuration. As one illustrative example, a storage manager (not shown) or other appropriate component may determine that it is appropriate to add an additional node to control tier <b>231</b>, and perform some or all of the following: (i) assess the capabilities of a newly added or otherwise available computing device as satisfying a minimum criteria to be configured as or hosting a media agent in control tier <b>231</b>; (ii) confirm that a sufficient amount of the appropriate type of storage exists to support an additional node in control tier <b>231</b> (e.g., enough disk drive capacity exists in storage pool <b>208</b> to support an additional deduplication database <b>247</b>); (iii) install appropriate media agent software on the computing device and configure the computing device according to a pre-determined template; (iv) establish a partition <b>251</b> in the storage pool <b>208</b> dedicated to the newly established media agent <b>244</b>; and (v) build any appropriate data structures (e.g., an instance of deduplication database <b>247</b>). An example of highly scalable managed data pool architecture or so-called web-scale architecture for storage and data management is found in U.S. Patent Application No. 62/273,286 entitled &#x201c;Redundant and Robust Distributed Deduplication Data Storage System.&#x201d;</p><p id="p-0213" num="0271">The embodiments and components thereof disclosed in <figref idref="DRAWINGS">FIGS. <b>2</b>A, <b>2</b>B, and <b>2</b>C</figref>, as well as those in <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H</figref>, may be implemented in any combination and permutation to satisfy data storage management and information management needs at one or more locations and/or data centers.</p><heading id="h-0020" level="2">Disk Usage Growth Prediction System</heading><p id="p-0214" num="0272">Enterprises can generate and manage stored data using an information management system. Such an information management system may include numerous storage devices that can store data generated by the enterprises. Modern day enterprises often encounter rapidly growth in the volume of data that they need to store. Thus, it is important for these enterprises to be able to monitor and manage their data growth and expand their storage capacity accordingly.</p><p id="p-0215" num="0273">However, storage devices in a given information management system often have different growth patterns and sometimes experience anomalous behaviors. Thus, it is often difficult to predict when the various storage devices will reach their respective maximum storage capacities. Without adequate disk usage growth prediction, system administrators run the risk of under-provisioning or over-provisioning storage devices in such information management systems.</p><p id="p-0216" num="0274">Certain embodiments described herein relate to an improved disk usage growth prediction system. In some embodiments, one or more components in the information management system can determine usage status data of a given storage device, perform a validation check on the usage status data using multiple prediction models, compare validation results of the multiple prediction models to identify the best performing prediction model, generate a disk usage growth prediction using the identified prediction model, and adjust the available space of the storage device according to the disk usage growth prediction.</p><p id="p-0217" num="0275"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating an example system <b>300</b> configured to implement a disk usage growth prediction process in accordance with one or more embodiments disclosed herein. As illustrated, the example system <b>300</b> includes a client computing devices <b>302</b>A-N (also collectively referred to herein as computing device <b>302</b>), a storage manager <b>304</b>, a secondary storage computing device <b>306</b>, and a secondary storage device <b>308</b>. The client computing device <b>302</b> may sometimes be referred to in some cases as a primary storage subsystem <b>300</b>A, and some or all of the storage manager <b>304</b>, the secondary storage computing device <b>306</b>, and the secondary storage device <b>308</b> may sometimes be referred to as a secondary storage subsystem <b>300</b>B. In some embodiments, the storage manager <b>304</b> may be part of the primary storage subsystem <b>300</b>A instead of the secondary storage subsystem <b>300</b>B, or may be part of neither the primary storage subsystem <b>300</b>A nor the secondary storage subsystem <b>300</b>B.</p><p id="p-0218" num="0276">The client computing device <b>302</b> may include one or more applications and one or more data agents that are identical or similar to those described herein with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H and <b>2</b>A-<b>2</b>C</figref>. Further, the client computing device <b>302</b> may be associated with one or more primary storage devices identical or similar to those described herein with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H and <b>2</b>A-<b>2</b>C</figref>. In some embodiments, the client computing device <b>302</b> may be a virtual machine. As previously described, for example with respect to <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>, the data agents may correspond to one or more of the applications running on the client computing device <b>302</b> and, as previously described, may facilitate data operations with respect to the corresponding application(s). Although multiple client computing devices are illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in some embodiments, the example system includes a single client computing device.</p><p id="p-0219" num="0277">The storage manger <b>304</b> may manage and store various preferences, rules, and policies regarding storage, backup, encryption, compression, or deduplication of the data (e.g., application data generated by one or more applications running the client computing devices) stored in one or more primary storage devices associated with the client computing device <b>302</b>. A system administrator may create, modify, and/or delete one or more storage policies via the storage manager <b>304</b>. For example, a storage policy may comprise a data structure or other information which includes a set of preferences and other storage criteria for performing a storage operation. The preferences and storage criteria may include, but are not limited to: a storage location, relationships between system components, network pathway to utilize, retention policies, data characteristics, compression or encryption requirements, preferred system components to utilize in a storage operation, and other criteria relating to a storage operation. Such storage policies may be stored in any appropriate location, such as in the storage manager <b>304</b>, the client computing device <b>302</b>, the secondary storage device <b>308</b>, or any other location in the system <b>300</b>. The storage manager <b>304</b> may initiate one or more data backup or archiving operations based on one or more thresholds specified by such storage policies. For example, a storage policy may mandate that the client computing devices <b>302</b>A-N be backed up at midnight every night. In this example, the storage manager <b>304</b> may initiate the backup operations at midnight every night, based on the storage policy.</p><p id="p-0220" num="0278">As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the storage manager <b>304</b> includes a disk usage growth prediction manager <b>336</b>. According to certain aspects of this disclosure, the disk usage growth prediction manager <b>336</b> can determine the current usage levels of various storage devices in the system <b>300</b> and perform disk usage growth prediction for each storage device (or for each group of storage devices) based on the current usage levels. In some cases, prior to performing such disk usage growth prediction, the disk usage growth prediction manager <b>336</b> modifies the current usage data obtained from the storage devices to add, update, or remove certain data points included in the current usage data (e.g., to fill in missing data or remove anomalies). By so modifying the current usage data based on which the disk usage growth prediction is performed, the disk usage growth prediction manager <b>336</b> can provide more accurate predictions and/or reduce the number of false alarms. Upon completing the disk usage growth predictions, the disk usage growth prediction manager <b>336</b> may output an indication of the predicted date or date range when the storage device is expected to become full and/or the amount of time left until the storage device is expected to become full. By doing so, the disk usage growth prediction manager <b>336</b> allows the system administrator sufficient time to add additional storage capacity to the system <b>300</b>. The details of the disk usage growth prediction manager <b>336</b> are described in greater detail below with reference to <figref idref="DRAWINGS">FIGS. <b>4</b>-<b>9</b></figref>.</p><p id="p-0221" num="0279">Although the disk usage growth prediction manager <b>336</b> is illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> as being included in the storage manager <b>304</b>, in other embodiments, the disk usage growth prediction manager <b>336</b> may exist independently in the system <b>300</b>, or be associated with another system component, such as the client computing device <b>302</b> or the secondary storage computing device <b>306</b>.</p><p id="p-0222" num="0280">Other than having the disk usage growth prediction manager <b>336</b>, the storage manager <b>304</b> (or one or more components thereof) may be identical or similar to the storage manager <b>140</b> (or one or more components thereof) described above with reference <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H and <b>2</b>A-<b>2</b>C</figref>. For example, the storage manager <b>304</b> may further provide one or more functionalities of the storage manager <b>140</b> described above.</p><p id="p-0223" num="0281">The secondary storage computing device <b>306</b> may include media agent(s) <b>332</b> and media agent index data <b>334</b>. The media agent(s) <b>332</b> may be implemented as a software module that manages, coordinates, and facilitates the transmission of data, as directed by the storage manager <b>304</b>, to and/or from the secondary storage device <b>308</b> (e.g., between the client computing device <b>302</b> and the secondary storage device <b>308</b>). Although the secondary storage device <b>308</b> is shown as a single secondary storage device in the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, it should be appreciated that any number of secondary storage devices may be used to implement the secondary storage device <b>308</b>, as described with reference to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>. For example, upon receiving a data backup request from the client computing device <b>302</b> or the storage manager <b>304</b>, the media agent(s) <b>332</b> may route and/or store the data to the appropriate secondary storage device <b>308</b>, or modify or add to the existing copy of the data stored in the secondary storage device <b>308</b>.</p><p id="p-0224" num="0282">In some embodiments, each of the media agent(s) <b>332</b> may maintain an associated media agent database, which may be stored to a disk or other storage device (not shown) that is local to the secondary storage computing device <b>306</b> on which the media agent executes. In other cases, such a media agent database may be stored separately from the host secondary storage computing device <b>306</b>. The media agent database can include, among other things, a media agent index data <b>334</b> (e.g., similar to media agent index <b>153</b> of <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>). In some cases, the media agent index data <b>334</b> does not form a part of and is instead separate from the media agent database.</p><p id="p-0225" num="0283">The media agent index data <b>334</b> may be a data structure associated with the particular media agent <b>306</b> that includes information about the stored data associated with the particular media agent and which may be generated in the course of performing a secondary copy operation or a restore. The media agent index data <b>334</b> may provide a fast and efficient mechanism for locating/browsing secondary copies or other data stored in the secondary storage devices <b>308</b> without having to access the secondary storage device <b>308</b> to retrieve the information from there. For instance, for each secondary copy stored in the secondary storage device <b>308</b>, the media agent index data <b>334</b> may include metadata such as a list of the data objects (e.g., files/subdirectories, database objects, mailbox objects, etc.), a logical path to the secondary copy on the corresponding secondary storage device <b>308</b>, location information (e.g., offsets) indicating where the data objects are stored in the secondary storage device <b>308</b>, when the data objects were created or modified, etc. Thus, the media agent index data <b>334</b> includes metadata associated with the secondary copies that is readily available for use from the media agent <b>332</b>. In some embodiments, some or all of the information in the media agent index data <b>334</b> may instead or additionally be stored along with the secondary copies in the secondary storage device <b>308</b>.</p><p id="p-0226" num="0284">The information stored in the media agent index data <b>334</b> typically comprises data that reflects certain particulars about relatively recent secondary copy operations. After some triggering event, such as after some time elapses or the media agent index data <b>334</b> reaches a particular size, certain portions of the media agent index data <b>334</b> may be copied or migrated to the secondary storage device <b>308</b>, e.g., on a least-recently-used basis. This information may be retrieved and uploaded back into the media agent index data <b>334</b> or otherwise restored to the media agent <b>332</b> to facilitate retrieval of data from the secondary storage device <b>308</b>. In some embodiments, the cached information may include format or containerization information related to archives or other files stored on storage device <b>308</b>. The media agent(s) <b>332</b> may update the media agent index data <b>334</b> upon every backup operation or other data protection operation.</p><p id="p-0227" num="0285">The secondary storage device <b>308</b> may store data backed up from the client computing devices <b>302</b>. For example, such backup data may include a secondary copy of the application data generated by one or more applications running on the client computing device <b>302</b>. Further, the secondary storage device <b>308</b> may communicate with components other than those illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> and store data other than those described herein.</p><p id="p-0228" num="0286">The secondary storage device <b>308</b> (or one or more components thereof) may be identical or similar to the secondary storage device <b>108</b> (or one or more components thereof) described above with reference <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H and <b>2</b>A-<b>2</b>C</figref>. For example, the secondary storage device <b>308</b> may further provide one or more functionalities of the secondary storage device <b>108</b> described above.</p><p id="p-0229" num="0287">The system <b>300</b> and corresponding components of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be identical or similar to the systems <b>100</b> or <b>200</b> (and similarly named components shown in any of <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H and <b>2</b>A-<b>2</b>C</figref>, where applicable, such as <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>). Moreover, depending on the embodiment, the system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may additionally include any of the other components shown in <figref idref="DRAWINGS">FIG. <b>1</b>D</figref> that are not specifically shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The system <b>300</b> may include one or more of each such component. All components of the system <b>300</b> can be in direct communication with each other or communicate indirectly via the client computing device <b>302</b>, the storage manager <b>304</b>, the secondary storage computing device <b>306</b>, the secondary storage device <b>308</b>, or the like. In certain embodiments, some of the components in <figref idref="DRAWINGS">FIG. <b>3</b></figref> shown as separate components can reside on a single computing device. Alternatively, or additionally, one or more components shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> as residing on a single computing device can be distributed across multiple devices.</p><p id="p-0230" num="0288"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram illustrative of one embodiment of a method <b>400</b> for performing disk usage growth prediction. The method <b>400</b> is described with respect to the system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. However, one or more of the steps of method <b>400</b> may be implemented by other information management systems, such as those described in greater detail above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>. The method <b>400</b> can be implemented by any one of, or a combination of, of a client computing device, a storage manager, a data agent, a media agent, and the like. Although the steps in the method <b>400</b> are described as being performed by the disk usage growth prediction manager <b>336</b> of the system <b>300</b>, the embodiments discussed herein are not limited as such, and one or more of the steps in the method <b>400</b> may be performed by other components of the system <b>300</b>, either alone or in combination.</p><p id="p-0231" num="0289">At block <b>402</b>, the disk usage growth prediction manager <b>336</b> determines usage status data of a storage device. The disk usage growth prediction manager <b>336</b> may poll the storage device for such usage status data. Alternatively, the disk usage growth prediction manager <b>336</b> accesses such usage status data previously provided by the storage device (e.g., for another purpose such as logging or reporting). For example, the usage status data may include an indication of any one or combination of disk capacity, used space, free space, incremental usage (e.g., disk usage for a given hour, day, week, month, year, etc.), disk type (e.g., backup disk, deduplication disk, index cache disk, etc.), and the like. In some cases, the usage status data includes both the amount of data written and the amount of data removed. In such cases, the disk usage growth prediction manager <b>336</b> may calculate the amount of effective data written (e.g., total amount of data written to the storage device&#x2014;total amount of data removed from the storage device). The usage status data may include such an indication for the storage device in its current form (e.g., current used space, free space, etc.) and/or for a specific time in the past (e.g., separate indications for the past 24 hours, 7 days, 12 months, or the like). For example, the usage status data may include 30 indications of the used space for the past 30 days (e.g., day 1: 3% of the storage capacity used, day 2: 4% of the storage capacity used, day 3: 7% of the storage capacity used . . . day 30: 61% of the storage capacity used).</p><p id="p-0232" num="0290">At block <b>404</b>, the disk usage growth prediction manager <b>336</b> performs a validation check for multiple prediction models. For example, the disk usage growth prediction manager <b>336</b> may identify a portion (e.g., validation data set) of the usage status data (e.g., the first 10 days) and apply each of the prediction models on the portion of the usage status data. Each of the prediction models may yield a prediction of the disk usage growth (e.g., for at least the next 20 days).</p><p id="p-0233" num="0291">In some embodiments, the prediction models used to perform disk usage growth prediction comprise the Auto Regressive Integrated Moving Average (ARIMA) model. Alternatively or additionally, the models used to perform disk usage growth prediction comprise the Holt-Winters model. Although the ARIMA and Holt-Winters models are provided as example prediction models, in some embodiments, one or more other time-series forecasting methods may be used instead of or in addition to the ARIMA and Holt-Winters models.</p><p id="p-0234" num="0292">At block <b>406</b>, the disk usage growth prediction manager <b>336</b> compares the validation check results to the remaining portion (e.g., training data set) of the usage status data (e.g., the last 20 days). For example, the disk usage growth prediction manager <b>336</b> may calculate an error rate (or another metric for evaluating the prediction model) for each prediction model based on the comparison between the results yielded by the prediction model and the remaining portion of the usage status data.</p><p id="p-0235" num="0293">At block <b>408</b>, the disk usage growth prediction manager <b>336</b> selects, based on the calculated error rates or metrics, one of the prediction models to be used to generate the disk usage growth prediction for the storage device. For example, the disk usage growth prediction manager <b>336</b> may select the prediction model with the lowest error rate. In another example, the disk usage growth prediction manager <b>336</b> may select the prediction model having the highest correlation to the usage status data.</p><p id="p-0236" num="0294">At block <b>410</b>, the disk usage growth prediction manager <b>336</b> generates a disk usage growth prediction for the storage device. The generated disk usage growth prediction may indicate a date or date range when the storage device is expected to become full and/or an amount of time left until the date or date range.</p><p id="p-0237" num="0295">In some cases, prior to performing such disk usage growth prediction (or prior to performing the validation check), the disk usage growth prediction manager <b>336</b> modifies the current usage data obtained from the storage devices to add, update, or remove certain data points included in the current usage data (e.g., to fill in missing data or remove anomalies). By so modifying the current usage data based on which the disk usage growth prediction is performed, the disk usage growth prediction manager <b>336</b> can provide more accurate predictions.</p><p id="p-0238" num="0296">For example, if the disk usage growth prediction manager <b>336</b> determines that the current usage data over the past 7 days is missing the usage data for one of the 7 days, the disk usage growth prediction manager <b>336</b> can fill in the missing data using information available to the disk usage growth prediction manager <b>336</b> (e.g., by averaging the usage data for the 6 remaining days, by averaging the usage data for the past 5 Mondays, if the missing day is a Monday, and the like). As another example, if the disk usage growth prediction manager <b>336</b> determines that the current usage data over the past 24 hours is missing the usage data for one of the 24 hours, the disk usage growth prediction manager <b>336</b> can fill in the missing data using information available to the disk usage growth prediction manager <b>336</b> (e.g., by averaging the usage data for the 23 remaining hours, by averaging the usage data for the past 5 instances of the same hour, and the like).</p><p id="p-0239" num="0297">Additionally or alternatively, the disk usage growth prediction manager <b>336</b> may remove certain portions of the current usage data based on determining that such portions correspond to anomalous usage behavior. For example, if the disk usage growth prediction manager <b>336</b> determines that on one of the 7 days included in the usage status data, the amount of data added to the storage device was over 10 times the average amount (or maximum amount) of data added to the storage device on a single day, based on the unusually large amount of data added to the storage device, the disk usage growth prediction manager <b>336</b> may remove the usage status data for that particular day from being considered in its disk usage growth prediction. The threshold(s) for identifying an anomalous data usage behavior can be based on historical disk usage patterns associated with the particular storage device. For example, usage that is above the maximum recorded usage or below the minimum recorded usage may be discarded as being anomalous. As another example, usage that is greater than the average recorded usage by a threshold amount (e.g., 10 times) or lower than the average recorded usage by a threshold amount (e.g., one-tenth) may be discarded as being anomalous.</p><p id="p-0240" num="0298">At block <b>412</b>, the disk usage growth prediction manager <b>336</b> adjusts the available space of the storage device. For example, if the disk usage growth prediction manager <b>336</b> determines that the amount of time left until the date or date range when the storage device is expected to become full is lower than a threshold amount, the disk usage growth prediction manager <b>336</b> may delete or move certain data from the storage device. Alternatively, the disk usage growth prediction manager <b>336</b> may replace the storage device with another storage device having a higher storage capacity. In yet case, the disk usage growth prediction manager <b>336</b> may cause an additional storage device to share the storage responsibility with the storage device such that data that would otherwise be stored on the storage device could be stored on the additional storage device instead.</p><p id="p-0241" num="0299">The method <b>400</b> can include fewer, more, or different blocks than those illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref> and/or one or more blocks illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref> may be modified, omitted, or switched without departing from the spirit and scope of the description. For example, the method <b>400</b> may further include outputting or displaying the generated disk usage growth prediction to an administrative user in a user interface. In some cases, if the disk usage growth prediction manager <b>336</b> determines that the amount of time left until the date or date range when the storage device is expected to become full is lower than a threshold amount, the disk usage growth prediction manager <b>336</b> may cause a warning message to be generated and output to the administrative user. Further, one or more of the operations described as being performed by the disk usage growth prediction manager <b>336</b> may be performed by the client computing device <b>302</b>, the secondary storage computing device <b>306</b>, or another component of the system <b>300</b> not illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Moreover, it will be appreciated by those skilled in the art and others that some or all of the functions described in this disclosure may be embodied in software executed by one or more processors of the disclosed components and mobile communication devices. The software may be persistently stored in any type of non-volatile storage.</p><p id="p-0242" num="0300"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow diagram illustrative of one embodiment of a method <b>500</b> for updating disk usage growth prediction. The method <b>500</b> is described with respect to the system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. However, one or more of the steps of method <b>500</b> may be implemented by other information management systems, such as those described in greater detail above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>. The method <b>500</b> can be implemented by any one of, or a combination of, of a client computing device, a storage manager, a data agent, a media agent, and the like. Although the steps in the method <b>500</b> are described as being performed by the disk usage growth prediction manager <b>336</b> of the system <b>300</b>, the embodiments discussed herein are not limited as such, and one or more of the steps in the method <b>500</b> may be performed by other components of the system <b>300</b>, either alone or in combination.</p><p id="p-0243" num="0301">At block <b>502</b>, the disk usage growth prediction manager <b>336</b> determines updated usage status data of a storage device. For example, the disk usage growth prediction manager <b>336</b> may, after a predetermined time period, poll the storage device for which the disk usage growth prediction was generated at block <b>410</b>, and receive the updated usage status data of the storage device (e.g., for a more recent time period than that associated with the usage status data determined at block <b>402</b>). For example, the updated usage status data may include identical or similar types of information as those included in the usage status data determined at block <b>402</b>. The update usage status data may correspond to May 22, 2018, and the usage status data most recently determined by the disk usage growth prediction manager <b>336</b> may correspond to May 21, 2018. As another example, the update usage status data may correspond to the third week of March 2018, and the usage status data most recently determined by the disk usage growth prediction manager <b>336</b> may correspond to the second week of March 2018.</p><p id="p-0244" num="0302">At block <b>504</b>, the disk usage growth prediction manager <b>336</b> performs a validation check using the last-used model (or any one of the multiple prediction models). For example, the disk usage growth prediction manager <b>336</b> may identify a portion of the usage status data (e.g., the first 10 days) and apply the last-used prediction model on the portion of the usage status data.</p><p id="p-0245" num="0303">At block <b>506</b>, the disk usage growth prediction manager <b>336</b> compares the validation check result to the remaining portion of the usage status data. For example, the disk usage growth prediction manager <b>336</b> may calculate an error rate (or another metric for evaluating the prediction model) for the prediction model based on the comparison between the result yielded by the prediction model and the remaining portion of the usage status data.</p><p id="p-0246" num="0304">At block <b>508</b>, the disk usage growth prediction manager <b>336</b> determines whether the comparison result satisfies a condition for keeping the last-used prediction model. For example, the disk usage growth prediction manager <b>336</b> may determine whether the resulting error rate is below a threshold value. If so, the method <b>500</b> proceeds to block <b>512</b>. Otherwise, the method <b>500</b> proceeds to block <b>510</b>, where the disk usage growth prediction manager <b>336</b> identifies an alternative prediction model to be used for generating the disk usage growth prediction for the storage device. For example, the disk usage growth prediction manager <b>336</b> may repeat the operations at blocks <b>404</b>-<b>408</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> to identify the alternative prediction model. In some embodiments, the last-used prediction model is excluded from the pool of prediction models from which the alternative prediction model is identified. In other embodiments, the last-used prediction model is included in the pool of prediction models from which the alternative prediction model is identified.</p><p id="p-0247" num="0305">At block <b>510</b>, the disk usage growth prediction manager <b>336</b> generates a disk usage growth prediction for the storage device (e.g., using the last-used prediction model if the condition is satisfied at block <b>508</b>, or using the alternative prediction model identified at block <b>510</b>). The generated disk usage growth prediction may indicate a date or date range when the storage device is expected to become full and/or an amount of time left until the date or date range.</p><p id="p-0248" num="0306">In some cases, prior to performing such disk usage growth prediction (or prior to performing the validation check), the disk usage growth prediction manager <b>336</b> modifies the current usage data obtained from the storage devices to add, update, or remove certain data points included in the current usage data (e.g., to fill in missing data or remove anomalies). By so modifying the current usage data based on which the disk usage growth prediction is performed, the disk usage growth prediction manager <b>336</b> can provide more accurate predictions.</p><p id="p-0249" num="0307">For example, if the disk usage growth prediction manager <b>336</b> determines that the current usage data over the past 7 days is missing the usage data for one of the 7 days, the disk usage growth prediction manager <b>336</b> can fill in the missing data using information available to the disk usage growth prediction manager <b>336</b> (e.g., by averaging the usage data for the 6 remaining days, by averaging the usage data for the past 5 Mondays, if the missing day is a Monday, and the like). As another example, if the disk usage growth prediction manager <b>336</b> determines that the current usage data over the past 24 hours is missing the usage data for one of the 24 hours, the disk usage growth prediction manager <b>336</b> can fill in the missing data using information available to the disk usage growth prediction manager <b>336</b> (e.g., by averaging the usage data for the 23 remaining hours, by averaging the usage data for the past 5 instances of the same hour, and the like).</p><p id="p-0250" num="0308">Additionally or alternatively, the disk usage growth prediction manager <b>336</b> may remove certain portions of the current usage data based on determining that such portions correspond to anomalous usage behavior. For example, if the disk usage growth prediction manager <b>336</b> determines that on one of the 7 days included in the usage status data, the amount of data added to the storage device was over 10 times the average amount (or maximum amount) of data added to the storage device on a single day, based on the unusually large amount of data added to the storage device, the disk usage growth prediction manager <b>336</b> may remove the usage status data for that particular day from being considered in its disk usage growth prediction.</p><p id="p-0251" num="0309">At block <b>514</b>, the disk usage growth prediction manager <b>336</b> adjusts the available space of the storage device. For example, if the disk usage growth prediction manager <b>336</b> determines that the amount of time left until the date or date range when the storage device is expected to become full is lower than a threshold amount, the disk usage growth prediction manager <b>336</b> may delete or move certain data from the storage device. Alternatively, the disk usage growth prediction manager <b>336</b> may replace the storage device with another storage device having a higher storage capacity. In yet case, the disk usage growth prediction manager <b>336</b> may cause an additional storage device to share the storage responsibility with the storage device such that data that would otherwise be stored on the storage device could be stored on the additional storage device instead.</p><p id="p-0252" num="0310">The method <b>500</b> can include fewer, more, or different blocks than those illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> and/or one or more blocks illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> may be modified, omitted, or switched without departing from the spirit and scope of the description. For example, the method <b>500</b> may include outputting or displaying the generated disk usage growth prediction to an administrative user in a user interface. In some cases, if the disk usage growth prediction manager <b>336</b> determines that the amount of time left until the date or date range when the storage device is expected to become full is lower than a threshold amount, the disk usage growth prediction manager <b>336</b> may cause a warning message to be generated and output to the administrative user. Further, one or more of the operations described as being performed by the disk usage growth prediction manager <b>336</b> may be performed by the client computing device <b>302</b>, the secondary storage computing device <b>306</b>, or another component of the system <b>300</b> not illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Moreover, it will be appreciated by those skilled in the art and others that some or all of the functions described in this disclosure may be embodied in software executed by one or more processors of the disclosed components and mobile communication devices. The software may be persistently stored in any type of non-volatile storage.</p><p id="p-0253" num="0311"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow diagram illustrative of one embodiment of a method <b>600</b> for adjusting backup storage based on disk usage growth prediction. The method <b>600</b> is described with respect to the system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. However, one or more of the steps of method <b>600</b> may be implemented by other information management systems, such as those described in greater detail above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>. The method <b>600</b> can be implemented by any one of, or a combination of, of a client computing device, a storage manager, a data agent, a media agent, and the like. Although the steps in the method <b>600</b> are described as being performed by the storage manager <b>304</b> of the system <b>300</b>, the embodiments discussed herein are not limited as such, and one or more of the steps in the method <b>600</b> may be performed by other components of the system <b>300</b>, either alone or in combination.</p><p id="p-0254" num="0312">At block <b>602</b>, the storage manager <b>304</b> initiates a backup operation. The backup operation may be identical or similar to those described herein with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H and <b>2</b>A-<b>2</b>C</figref>.</p><p id="p-0255" num="0313">At block <b>604</b>, the storage manager <b>304</b> causes primary data from a first storage device (e.g., the primary storage device associated with the client computing device <b>302</b>) to be copied onto a second storage device different from the first storage device (e.g., secondary storage device <b>308</b>).</p><p id="p-0256" num="0314">At block <b>606</b>, the storage manager <b>304</b> polls the second storage device for usage status data. In some embodiments, the polling of the second storage device occurs according to a predetermined polling schedule (e.g., daily, weekly, monthly, etc.). In other embodiments, the polling of the second storage device occurs in response to the backup operation (e.g., upon initiation, during, or upon completion).</p><p id="p-0257" num="0315">At block <b>608</b>, the storage manager <b>304</b> updates the disk usage growth prediction for the second storage device. For example, the generated disk usage growth prediction may reflect the data stored onto the second storage device as part of the backup operation initiated at block <b>602</b>.</p><p id="p-0258" num="0316">At block <b>610</b>, the storage manager <b>304</b> adjusts the available space of the second storage device. For example, if the storage manager <b>304</b> determines that the amount of time left until the date or date range when the second storage device is expected to become full is lower than a threshold amount, the storage manager <b>304</b> may delete or move certain data from the second storage device. Alternatively, the storage manager <b>304</b> may replace the second storage device with another storage device having a higher storage capacity. In yet case, the storage manager <b>304</b> may cause an additional storage device to share the storage responsibility with the second storage device such that data that would otherwise be stored on the second storage device could be stored on the additional storage device instead.</p><p id="p-0259" num="0317">The method <b>600</b> can include fewer, more, or different blocks than those illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> and/or one or more blocks illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> may be modified, omitted, or switched without departing from the spirit and scope of the description. For example, the method <b>600</b> may include outputting or displaying the generated disk usage growth prediction to an administrative user in a user interface. In some cases, if the storage manager <b>304</b> determines that the amount of time left until the date or date range when the second storage device is expected to become full is lower than a threshold amount, the storage manager <b>304</b> may cause a warning message to be generated and output to the administrative user. Further, one or more of the operations described as being performed by the storage manager <b>306</b> may be performed by the client computing device <b>302</b>, the secondary storage computing device <b>306</b>, or another component of the system <b>300</b> not illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Moreover, it will be appreciated by those skilled in the art and others that some or all of the functions described in this disclosure may be embodied in software executed by one or more processors of the disclosed components and mobile communication devices. The software may be persistently stored in any type of non-volatile storage.</p><p id="p-0260" num="0318"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flow diagram illustrative of one embodiment of a method <b>700</b> for adjusting deduplication storage based on disk usage growth prediction. The method <b>700</b> is described with respect to the system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. However, one or more of the steps of method <b>700</b> may be implemented by other information management systems, such as those described in greater detail above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>. The method <b>700</b> can be implemented by any one of, or a combination of, of a client computing device, a storage manager, a data agent, a media agent, and the like. Although the steps in the method <b>700</b> are described as being performed by the storage manager <b>304</b> of the system <b>300</b>, the embodiments discussed herein are not limited as such, and one or more of the steps in the method <b>700</b> may be performed by other components of the system <b>300</b>, either alone or in combination.</p><p id="p-0261" num="0319">At block <b>702</b>, the storage manager <b>304</b> initiates a deduplication operation. The deduplication operation may be identical or similar to those described herein with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H and <b>2</b>A-<b>2</b>C</figref>.</p><p id="p-0262" num="0320">At block <b>704</b>, the storage manager <b>304</b> causes deduplication data (e.g., deduplication database) to be stored in a storage device (e.g., storage device associated with the secondary storage computing device <b>306</b>.</p><p id="p-0263" num="0321">At block <b>706</b>, the storage manager <b>304</b> polls the storage device for usage status data. In some embodiments, the polling of the storage device occurs according to a predetermined polling schedule (e.g., daily, weekly, monthly, etc.). In other embodiments, the polling of the storage device occurs in response to the deduplication operation (e.g., upon initiation, during, or upon completion).</p><p id="p-0264" num="0322">At block <b>708</b>, the storage manager <b>304</b> updates the disk usage growth prediction for the storage device. For example, the generated disk usage growth prediction may reflect the deduplication data stored onto the storage device as part of the deduplication operation initiated at block <b>702</b>.</p><p id="p-0265" num="0323">At block <b>710</b>, the storage manager <b>304</b> adjusts the available space of the storage device. For example, if the storage manager <b>304</b> determines that the amount of time left until the date or date range when the storage device is expected to become full is lower than a threshold amount, the storage manager <b>304</b> may delete or move certain data from the storage device. Alternatively, the storage manager <b>304</b> may replace the storage device with another storage device having a higher storage capacity. In yet case, the storage manager <b>304</b> may cause an additional storage device to share the storage responsibility with the storage device such that data that would otherwise be stored on the storage device could be stored on the additional storage device instead.</p><p id="p-0266" num="0324">The method <b>700</b> can include fewer, more, or different blocks than those illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref> and/or one or more blocks illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref> may be modified, omitted, or switched without departing from the spirit and scope of the description. For example, the method <b>700</b> may include outputting or displaying the generated disk usage growth prediction to an administrative user in a user interface. In some cases, if the storage manager <b>304</b> determines that the amount of time left until the date or date range when the storage device is expected to become full is lower than a threshold amount, the storage manager <b>304</b> may cause a warning message to be generated and output to the administrative user. Further, one or more of the operations described as being performed by the storage manager <b>306</b> may be performed by the client computing device <b>302</b>, the secondary storage computing device <b>306</b>, or another component of the system <b>300</b> not illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Moreover, it will be appreciated by those skilled in the art and others that some or all of the functions described in this disclosure may be embodied in software executed by one or more processors of the disclosed components and mobile communication devices. The software may be persistently stored in any type of non-volatile storage.</p><p id="p-0267" num="0325"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow diagram illustrative of one embodiment of a method <b>800</b> for adjusting index cache storage based on disk usage growth prediction. The method <b>800</b> is described with respect to the system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. However, one or more of the steps of method <b>800</b> may be implemented by other information management systems, such as those described in greater detail above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>. The method <b>800</b> can be implemented by any one of, or a combination of, of a client computing device, a storage manager, a data agent, a media agent, and the like. Although the steps in the method <b>800</b> are described as being performed by the storage manager <b>304</b> of the system <b>300</b>, the embodiments discussed herein are not limited as such, and one or more of the steps in the method <b>800</b> may be performed by other components of the system <b>300</b>, either alone or in combination.</p><p id="p-0268" num="0326">At block <b>802</b>, the storage manager <b>304</b> generates index cache data for backup data stored in a first storage device (e.g., secondary storage device <b>308</b>). The index cache data may be identical or similar to those described herein with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H and <b>2</b>A-<b>2</b>C</figref>.</p><p id="p-0269" num="0327">At block <b>804</b>, the storage manager <b>304</b> causes the index cache data to be stored in a second storage device (e.g., another secondary storage device <b>308</b> or another storage device associated with the secondary storage computing device <b>306</b>).</p><p id="p-0270" num="0328">At block <b>806</b>, the storage manager <b>304</b> polls the second storage device for usage status data. In some embodiments, the polling of the second storage device occurs according to a predetermined polling schedule (e.g., daily, weekly, monthly, etc.). In other embodiments, the polling of the second storage device occurs in response to the backup operation (e.g., upon initiation, during, or upon completion).</p><p id="p-0271" num="0329">At block <b>808</b>, the storage manager <b>304</b> updates the disk usage growth prediction for the second storage device. For example, the generated disk usage growth prediction may reflect the index cache data stored onto the second storage device at block <b>804</b>.</p><p id="p-0272" num="0330">At block <b>810</b>, the storage manager <b>304</b> adjusts the available space of the second storage device. For example, if the storage manager <b>304</b> determines that the amount of time left until the date or date range when the second storage device is expected to become full is lower than a threshold amount, the storage manager <b>304</b> may delete or move certain data from the second storage device. Alternatively, the storage manager <b>304</b> may replace the second storage device with another storage device having a higher storage capacity. In yet case, the storage manager <b>304</b> may cause an additional storage device to share the storage responsibility with the second storage device such that data that would otherwise be stored on the second storage device could be stored on the additional storage device instead.</p><p id="p-0273" num="0331">The method <b>800</b> can include fewer, more, or different blocks than those illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> and/or one or more blocks illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> may be modified, omitted, or switched without departing from the spirit and scope of the description. For example, the method <b>800</b> may include outputting or displaying the generated disk usage growth prediction to an administrative user in a user interface. In some cases, if the storage manager <b>304</b> determines that the amount of time left until the date or date range when the storage device is expected to become full is lower than a threshold amount, the storage manager <b>304</b> may cause a warning message to be generated and output to the administrative user. Further, one or more of the operations described as being performed by the storage manager <b>306</b> may be performed by the client computing device <b>302</b>, the secondary storage computing device <b>306</b>, or another component of the system <b>300</b> not illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Moreover, it will be appreciated by those skilled in the art and others that some or all of the functions described in this disclosure may be embodied in software executed by one or more processors of the disclosed components and mobile communication devices. The software may be persistently stored in any type of non-volatile storage.</p><p id="p-0274" num="0332"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a user interface <b>900</b> indicating the disk usage growth predictions for the respective storage devices in the system <b>300</b>. Although not shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the user interface <b>900</b> may further include the amount of time left until each of the predicted dates and/or the rate at which the storage devices are being filled with data (i.e. fill rate). The user interface <b>900</b> may further include one or more graphical indicators that allow the system administrator to prioritize his or her storage device acquisition activities (e.g., green=over a year left, yellow=less than a year left, orange=less than 3 months left, red=less than 2 weeks left, and the like).</p><p id="p-0275" num="0333">In some embodiments, machine learning can be used to update or optimize the way that the disk usage growth prediction manager <b>336</b> performs the disk usage growth prediction. For example, the disk usage growth prediction manager <b>336</b> can be trained, using unsupervised learning (e.g., in some cases, further based on additional usage status data or feedback data) and/or previously done supervised learning, to update one or more of its thresholds (e.g., for identifying data usage anomalies, for re-selecting the prediction model, etc.). In some embodiments, based on the accuracy of prior disk usage growth predictions (e.g., based on whether a prior disk usage growth prediction ended up being correct or wrong, or based on how close the prior disk usage growth prediction was to the actual disk usage schedule), the disk usage growth prediction manager <b>336</b> updates the one or more of its thresholds.</p><p id="p-0276" num="0334">Although the phrase &#x201c;disk usage growth prediction&#x201d; is used as an example in the present disclosure, the techniques described herein may be used to perform usage growth predictions of any types of storage devices including, without limitation, disk drives, storage arrays (e.g., storage-area network (SAN) and/or network-attached storage (NAS) technology), semiconductor memory (e.g., solid state storage devices), network attached storage (NAS) devices, tape libraries, or other magnetic, non-tape storage devices, optical media storage devices, DNA/RNA-based memory technology, combinations of the same, etc.</p><p id="p-0277" num="0335">Although the disk usage growth prediction techniques are described herein as being applied to a storage device, in some embodiments, the disk usage growth prediction is generated for a group of storage devices. In such embodiments, adjusting the available space for the storage device (e.g., blocks <b>412</b>, <b>514</b>, <b>610</b>, <b>710</b>, or <b>810</b>) may comprise adding one or more storage devices to the group (or removing one or more storage devices from the group).</p><p id="p-0278" num="0336">In regard to the figures described herein, other embodiments are possible within the scope of the present invention, such that the above-recited components, steps, blocks, operations, and/or messages/requests/queries/instructions are differently arranged, sequenced, sub-divided, organized, and/or combined. In some embodiments, a different component may initiate or execute a given operation. For example, in some embodiments, some or all of the operations described herein with respect to the storage manager <b>304</b> may instead be performed by the client computing device <b>302</b> (or a component therein) or the secondary storage computing device <b>306</b> (or a component therein). As another example, in some embodiments, some or all of the operations described herein with respect to the secondary storage device <b>308</b> may instead be performed on the one or more primary storage devices associated with the client computing device <b>302</b>.</p><heading id="h-0021" level="1">Example Embodiments</heading><p id="p-0279" num="0337">Some example enumerated embodiments of the present invention are recited in this section in the form of methods, systems, and non-transitory computer-readable media, without limitation.</p><p id="p-0280" num="0338">According to an illustrative embodiment of the present invention, a method comprises: determining usage status data of one or more secondary storage devices storing secondary copies of primary data associated with one or more client computing devices; for each one of a plurality of prediction models, performing a validation check using a portion of the usage status data; comparing validation results corresponding to the plurality of prediction models to a remaining portion of the usage status data; based on the comparison, selecting one of the plurality of prediction models; using the selected prediction model, generating a disk usage growth prediction of the one or more secondary storage devices; and based on the generated disk usage growth prediction, adjusting available capacity of the one or more secondary storage devices.</p><p id="p-0281" num="0339">The above-recited method, wherein the method further comprises initiating a backup operation, causing at least a portion of the primary data to be copied onto the one or more secondary storage devices, polling the one or more secondary storage devices for the usage status data based at least on the portion of the primary data copied onto the one or more secondary storage devices, updating the disk usage growth prediction of the one or more secondary storage devices storing the portion of the primary data copied onto the one or more secondary storage devices, and further adjusting available capacity of the one or more secondary storage devices storing the portion of the primary data copied onto the one or more secondary storage devices; wherein the method further comprises initiating a deduplication operation, causing deduplication data to be stored in the one or more secondary storage devices, polling the one or more secondary storage devices for the usage status data based at least on the deduplication data, updating the disk usage growth prediction of the one or more secondary storage devices storing the deduplication data, and further adjusting available capacity of the one or more secondary storage devices storing the deduplication data; wherein the method further comprises generating index cache data for the secondary copies stored in the one or more secondary storage devices, causing the index cache data to be stored in another storage device different from the one or more secondary storage devices, polling said another storage device for usage status data based at least on the index cache data, generating disk usage growth prediction of said another storage device storing the index cache data, and adjusting available capacity of said another storage device storing the index cache data; wherein the method further comprises determining additional usage status data of the one or more secondary storage devices, performing a subsequent validation check for the selected prediction model and not for other prediction modules of the plurality of prediction models, comparing a result of the subsequent validation check to the additional usage status data, and based on the comparison of the result to the to the additional usage status data, further adjusting available capacity of the one or more secondary storage devices; wherein the usage data comprises one or more of storage device maximum capacity, storage device current usage, storage device remaining space, storage device incremental usage associated with a predetermined time period, and storage device type; wherein comparing the validation results comprises calculating, for each validation result, a difference between the validation result and the remaining portion of the usage status data not used to perform the validation check; wherein the generated disk usage growth prediction comprises a time or a time period when the one or more secondary storage devices are expected to become full; wherein the generated disk usage growth prediction comprises an amount of time remaining until a time or a time period when the one or more secondary storage devices are expected to become full; and wherein the method further comprises modifying, prior to performing the validation check, the usage status data to add missing data to the usage status data or to remove anomalous data from the usage status data.</p><p id="p-0282" num="0340">According to another illustrative embodiment, a system comprises one or more client computing devices, one or more primary storage devices configured to store primary data generated by the one or more client computing devices, one or more secondary storage devices configured to store secondary copies of the primary data, and a storage manager comprising physical hardware and configured to: determine usage status data of the one or more secondary storage devices; for each one of a plurality of prediction models, perform a validation check using a portion of the usage status data; compare validation results corresponding to the plurality of prediction models to a remaining portion of the usage status data; based on the comparison, select one of the plurality of prediction models; using the selected prediction model, generate a disk usage growth prediction of the one or more secondary storage devices; and based on the generated disk usage growth prediction, adjust available capacity of the one or more secondary storage devices.</p><p id="p-0283" num="0341">The above-recited system, wherein the storage manager is further configured to initiate a backup operation, cause at least a portion of the primary data to be copied onto the one or more secondary storage devices, poll the one or more secondary storage devices for the usage status data based at least on the portion of the primary data copied onto the one or more secondary storage devices, update the disk usage growth prediction of the one or more secondary storage devices storing the portion of the primary data copied onto the one or more secondary storage devices, and further adjust available capacity of the one or more secondary storage devices storing the portion of the primary data copied onto the one or more secondary storage devices; wherein the storage manager is further configured to initiate a deduplication operation, cause deduplication data to be stored in the one or more secondary storage devices, poll the one or more secondary storage devices for the usage status data based at least on the deduplication data, update the disk usage growth prediction of the one or more secondary storage devices storing the deduplication data, and further adjust available capacity of the one or more secondary storage devices storing the deduplication data; wherein the storage manager is further configured to generate index cache data for the secondary copies stored in the one or more secondary storage devices, cause the index cache data to be stored in another storage device different from the one or more secondary storage devices, poll said another storage device for usage status data based at least on the index cache data, generate disk usage growth prediction of said another storage device storing the index cache data, and adjust available capacity of said another storage device storing the index cache data; wherein the storage manager is further configured to determine additional usage status data of the one or more secondary storage devices, perform a subsequent validation check for the selected prediction model and not for other prediction modules of the plurality of prediction models, compare a result of the subsequent validation check to the additional usage status data, and based on the comparison of the result to the to the additional usage status data, further adjust available capacity of the one or more secondary storage devices.</p><p id="p-0284" num="0342">According to yet another illustrative embodiment, a non-transitory computer-readable medium comprising instructions that, when executed by a computing device comprising one or more processors and computer memory, cause the computing device to perform operations that comprise: determining usage status data of one or more secondary storage devices storing secondary copies of primary data associated with one or more client computing devices, for each one of a plurality of prediction models, performing a validation check using a portion of the usage status data, comparing validation results corresponding to the plurality of prediction models to a remaining portion of the usage status data, based on the comparison, selecting one of the plurality of prediction models, using the selected prediction model, generating a disk usage growth prediction of the one or more secondary storage devices, and based on the generated disk usage growth prediction, adjusting available capacity of the one or more secondary storage devices.</p><p id="p-0285" num="0343">The above-recited non-transitory computer-readable medium, wherein the operations further comprise initiating a backup operation, causing at least a portion of the primary data to be copied onto the one or more secondary storage devices, polling the one or more secondary storage devices for the usage status data based at least on the portion of the primary data copied onto the one or more secondary storage devices, updating the disk usage growth prediction of the one or more secondary storage devices storing the portion of the primary data copied onto the one or more secondary storage devices, and further adjusting available capacity of the one or more secondary storage devices storing the portion of the primary data copied onto the one or more secondary storage devices; wherein the operations further comprise initiating a deduplication operation, causing deduplication data to be stored in the one or more secondary storage devices, polling the one or more secondary storage devices for the usage status data based at least on the deduplication data, updating the disk usage growth prediction of the one or more secondary storage devices storing the deduplication data, and further adjusting available capacity of the one or more secondary storage devices storing the deduplication data; wherein the operations further comprise generating index cache data for the secondary copies stored in the one or more secondary storage devices, causing the index cache data to be stored in another storage device different from the one or more secondary storage devices, polling said another storage device for usage status data based at least on the index cache data, generating disk usage growth prediction of said another storage device storing the index cache data, and adjusting available capacity of said another storage device storing the index cache data; and wherein the operations further comprise determining additional usage status data of the one or more secondary storage devices, performing a subsequent validation check for the selected prediction model and not for other prediction modules of the plurality of prediction models, comparing a result of the subsequent validation check to the additional usage status data, and based on the comparison of the result to the to the additional usage status data, further adjusting available capacity of the one or more secondary storage devices.</p><p id="p-0286" num="0344">In other embodiments, a system or systems may operate according to one or more of the methods and/or computer-readable media recited in the preceding paragraphs. In yet other embodiments, a method or methods may operate according to one or more of the systems and/or computer-readable media recited in the preceding paragraphs. In yet more embodiments, a computer-readable medium or media, excluding transitory propagating signals, may cause one or more computing devices having one or more processors and non-transitory computer-readable memory to operate according to one or more of the systems and/or methods recited in the preceding paragraphs.</p><heading id="h-0022" level="1">Terminology</heading><p id="p-0287" num="0345">Conditional language, such as, among others, &#x201c;can,&#x201d; &#x201c;could,&#x201d; &#x201c;might,&#x201d; or &#x201c;may,&#x201d; unless specifically stated otherwise, or otherwise understood within the context as used, is generally intended to convey that certain embodiments include, while other embodiments do not include, certain features, elements and/or steps. Thus, such conditional language is not generally intended to imply that features, elements and/or steps are in any way required for one or more embodiments or that one or more embodiments necessarily include logic for deciding, with or without user input or prompting, whether these features, elements and/or steps are included or are to be performed in any particular embodiment.</p><p id="p-0288" num="0346">Unless the context clearly requires otherwise, throughout the description and the claims, the words &#x201c;comprise,&#x201d; &#x201c;comprising,&#x201d; and the like are to be construed in an inclusive sense, as opposed to an exclusive or exhaustive sense, i.e., in the sense of &#x201c;including, but not limited to.&#x201d; As used herein, the terms &#x201c;connected,&#x201d; &#x201c;coupled,&#x201d; or any variant thereof means any connection or coupling, either direct or indirect, between two or more elements; the coupling or connection between the elements can be physical, logical, or a combination thereof. Additionally, the words &#x201c;herein,&#x201d; &#x201c;above,&#x201d; &#x201c;below,&#x201d; and words of similar import, when used in this application, refer to this application as a whole and not to any particular portions of this application. Where the context permits, words using the singular or plural number may also include the plural or singular number respectively. The word &#x201c;or&#x201d; in reference to a list of two or more items, covers all of the following interpretations of the word: any one of the items in the list, all of the items in the list, and any combination of the items in the list. Likewise the term &#x201c;and/or&#x201d; in reference to a list of two or more items, covers all of the following interpretations of the word: any one of the items in the list, all of the items in the list, and any combination of the items in the list.</p><p id="p-0289" num="0347">In some embodiments, certain operations, acts, events, or functions of any of the algorithms described herein can be performed in a different sequence, can be added, merged, or left out altogether (e.g., not all are necessary for the practice of the algorithms). In certain embodiments, operations, acts, functions, or events can be performed concurrently, e.g., through multi-threaded processing, interrupt processing, or multiple processors or processor cores or on other parallel architectures, rather than sequentially.</p><p id="p-0290" num="0348">Systems and modules described herein may comprise software, firmware, hardware, or any combination(s) of software, firmware, or hardware suitable for the purposes described. Software and other modules may reside and execute on servers, workstations, personal computers, computerized tablets, PDAs, and other computing devices suitable for the purposes described herein. Software and other modules may be accessible via local computer memory, via a network, via a browser, or via other means suitable for the purposes described herein. Data structures described herein may comprise computer files, variables, programming arrays, programming structures, or any electronic information storage schemes or methods, or any combinations thereof, suitable for the purposes described herein. User interface elements described herein may comprise elements from graphical user interfaces, interactive voice response, command line interfaces, and other suitable interfaces.</p><p id="p-0291" num="0349">Further, processing of the various components of the illustrated systems can be distributed across multiple machines, networks, and other computing resources. Two or more components of a system can be combined into fewer components. Various components of the illustrated systems can be implemented in one or more virtual machines, rather than in dedicated computer hardware systems and/or computing devices. Likewise, the data repositories shown can represent physical and/or logical data storage, including, e.g., storage area networks or other distributed storage systems. Moreover, in some embodiments the connections between the components shown represent possible paths of data flow, rather than actual connections between hardware. While some examples of possible connections are shown, any of the subset of the components shown can communicate with any other subset of components in various implementations.</p><p id="p-0292" num="0350">Embodiments are also described above with reference to flow chart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products. Each block of the flow chart illustrations and/or block diagrams, and combinations of blocks in the flow chart illustrations and/or block diagrams, may be implemented by computer program instructions. Such instructions may be provided to a processor of a general purpose computer, special purpose computer, specially-equipped computer (e.g., comprising a high-performance database server, a graphics subsystem, etc.) or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor(s) of the computer or other programmable data processing apparatus, create means for implementing the acts specified in the flow chart and/or block diagram block or blocks. These computer program instructions may also be stored in a non-transitory computer-readable memory that can direct a computer or other programmable data processing apparatus to operate in a particular manner, such that the instructions stored in the computer-readable memory produce an article of manufacture including instruction means which implement the acts specified in the flow chart and/or block diagram block or blocks. The computer program instructions may also be loaded to a computing device or other programmable data processing apparatus to cause operations to be performed on the computing device or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the computing device or other programmable apparatus provide steps for implementing the acts specified in the flow chart and/or block diagram block or blocks.</p><p id="p-0293" num="0351">Any patents and applications and other references noted above, including any that may be listed in accompanying filing papers, are incorporated herein by reference. Aspects of the invention can be modified, if necessary, to employ the systems, functions, and concepts of the various references described above to provide yet further implementations of the invention. These and other changes can be made to the invention in light of the above Detailed Description. While the above description describes certain examples of the invention, and describes the best mode contemplated, no matter how detailed the above appears in text, the invention can be practiced in many ways. Details of the system may vary considerably in its specific implementation, while still being encompassed by the invention disclosed herein. As noted above, particular terminology used when describing certain features or aspects of the invention should not be taken to imply that the terminology is being redefined herein to be restricted to any specific characteristics, features, or aspects of the invention with which that terminology is associated. In general, the terms used in the following claims should not be construed to limit the invention to the specific examples disclosed in the specification, unless the above Detailed Description section explicitly defines such terms. Accordingly, the actual scope of the invention encompasses not only the disclosed examples, but also all equivalent ways of practicing or implementing the invention under the claims.</p><p id="p-0294" num="0352">To reduce the number of claims, certain aspects of the invention are presented below in certain claim forms, but the applicant contemplates other aspects of the invention in any number of claim forms. For example, while only one aspect of the invention is recited as a means-plus-function claim under 35 U.S.C sec. 112(f) (AIA), other aspects may likewise be embodied as a means-plus-function claim, or in other forms, such as being embodied in a computer-readable medium. Any claims intended to be treated under 35 U.S.C. &#xa7; 112(f) will begin with the words &#x201c;means for,&#x201d; but use of the term &#x201c;for&#x201d; in any other context is not intended to invoke treatment under 35 U.S.C. &#xa7; 112(f). Accordingly, the applicant reserves the right to pursue additional claims after filing this application, in either this application or in a continuing application.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method, the computer-implemented method comprising:<claim-text>performing a backup of primary data stored in accordance with an information management policy,<claim-text>wherein the information management policy comprises a set of parameters for performing backup operations on data assigned to the information management policy,</claim-text><claim-text>wherein the primary data is stored in a native format to create a first copy of secondary data stored in a backup format that is different than the native format, the secondary data stored on one or more secondary storage devices;</claim-text></claim-text><claim-text>performing a plurality of incremental backups that backup changes in the primary data to the first copy of the secondary data;</claim-text><claim-text>determining usage status data of the one or more secondary storage devices associated with a first portion of the plurality of incremental backups;</claim-text><claim-text>for each one of a plurality of prediction models, performing a validation check using the usage status data associated with the first portion of the plurality of incremental backups;</claim-text><claim-text>comparing validation results corresponding to the plurality of prediction models to a usage status data associated with a second portion of the plurality of incremental backups;</claim-text><claim-text>based on the comparing, selecting one of the plurality of prediction models;</claim-text><claim-text>using the selected prediction model, generating a disk usage growth prediction of the one or more secondary storage devices; and</claim-text><claim-text>based on the disk usage growth prediction, adjusting available capacity of the one or more secondary storage devices.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>copying the first copy of the secondary data to a create a second copy of the secondary data in the backup format;</claim-text><claim-text>restore the second copy of the secondary data in the backup format into a destination copy of the primary data in the native format.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>deleting at least a portion of the first copy of the secondary data.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>initiating a deduplication operation;</claim-text><claim-text>causing deduplication data to be stored in the one or more secondary storage devices;</claim-text><claim-text>polling the one or more secondary storage devices for the usage status data based at least on the deduplication data;</claim-text><claim-text>updating the disk usage growth prediction of the one or more secondary storage devices storing the deduplication data; and</claim-text><claim-text>further adjusting available capacity of the one or more secondary storage devices storing the deduplication data.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>generating index cache data for copies of secondary data stored in the one or more secondary storage devices;</claim-text><claim-text>causing the index cache data to be stored in another storage device different from the one or more secondary storage devices;</claim-text><claim-text>polling said another storage device for usage status data based at least on the index cache data;</claim-text><claim-text>generating disk usage growth prediction of said another storage device storing the index cache data; and</claim-text><claim-text>adjusting available capacity of said another storage device storing the index cache data.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining additional usage status data of the one or more secondary storage devices;</claim-text><claim-text>performing a subsequent validation check for the selected prediction model and not for other prediction modules of the plurality of prediction models;</claim-text><claim-text>comparing a result of the subsequent validation check to the additional usage status data; and</claim-text><claim-text>based on the comparison of the result to the to the additional usage status data, further adjusting available capacity of the one or more secondary storage devices.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the usage data comprises one or more of storage device maximum capacity, storage device current usage, storage device remaining space, storage device incremental usage associated with a predetermined time period, and storage device type.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein comparing the validation results comprises calculating, for each validation result, a difference between the validation result and the associated with the second portion of the plurality of incremental backups.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the disk usage growth prediction comprises a time or a time period when the one or more secondary storage devices are expected to become full.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the disk usage growth prediction comprises an amount of time remaining until a time or a time period when the one or more secondary storage devices are expected to become full.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising modifying, prior to performing the validation check, the usage status data to add missing data to the usage status data or to remove anomalous data from the usage status data.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A networked information management system configured to perform disk usage growth prediction, the networked information management system comprising:<claim-text>one or more processors comprising physical hardware and configured to:<claim-text>perform a backup of primary data stored in accordance with an information management policy,</claim-text><claim-text>wherein the information management policy comprises a set of parameters for performing backup operations on data assigned to the information management policy,</claim-text><claim-text>wherein the primary data is stored in a native format to create a first copy of secondary data stored in a backup format that is different than the native format, the secondary data stored on one or more secondary storage devices;</claim-text><claim-text>perform a plurality of incremental backups that backup changes in the primary data to the first copy of the secondary data;</claim-text><claim-text>determine usage status data of the one or more secondary storage devices associated with a first portion of the plurality of incremental backups;</claim-text><claim-text>for each one of a plurality of prediction models, perform a validation check using the usage status data associated with the first portion of the plurality of incremental backups;</claim-text><claim-text>compare validation results corresponding to the plurality of prediction models to a usage status data associated with a second portion of the plurality of incremental backups;</claim-text><claim-text>based on the compare, select one of the plurality of prediction models;</claim-text><claim-text>using the selected prediction model, generate a disk usage growth prediction of the one or more secondary storage devices; and</claim-text><claim-text>based on the disk usage growth prediction, adjust available capacity of the one or more secondary storage devices.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The networked information management system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the one or more processors is further configured to:<claim-text>copy the first copy of the secondary data to create a second copy of the secondary data in the backup format;</claim-text><claim-text>restore the second copy of the secondary data in the backup format into a destination copy of the primary data in the native format;</claim-text><claim-text>perform a plurality of incremental backups that copy changes in the first copy of the secondary data to the second copy of the secondary data and restore the plurality of incremental backups to synchronize the destination copy with the primary data.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The networked information management system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the one or more processors is further configured to:<claim-text>further adjust available capacity of the one or more secondary storage devices by deleting at least a portion of the first copy of the secondary data.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The networked information management system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the one or more processors is further configured to:<claim-text>initiate a deduplication operation;</claim-text><claim-text>cause deduplication data to be stored in the one or more secondary storage devices;</claim-text><claim-text>poll the one or more secondary storage devices for the usage status data based at least on the deduplication data;</claim-text><claim-text>update the disk usage growth prediction of the one or more secondary storage devices storing the deduplication data; and</claim-text><claim-text>further adjust available capacity of the one or more secondary storage devices storing the deduplication data.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The networked information management system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the one or more processors is further configured to:<claim-text>generate index cache data for copies of the secondary data stored in the one or more secondary storage devices;</claim-text><claim-text>cause the index cache data to be stored in another storage device different from the one or more secondary storage devices;</claim-text><claim-text>poll said another storage device for usage status data based at least on the index cache data;</claim-text><claim-text>generate disk usage growth prediction of said another storage device storing the index cache data; and</claim-text><claim-text>adjust available capacity of said another storage device storing the index cache data.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The networked information management system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the one or more processors is further configured to:<claim-text>determine additional usage status data of the one or more secondary storage devices;</claim-text><claim-text>perform a subsequent validation check for the selected prediction model and not for other prediction modules of the plurality of prediction models;</claim-text><claim-text>compare a result of the subsequent validation check to the additional usage status data; and</claim-text><claim-text>based on the comparison of the result to the to the additional usage status data, further adjust available capacity of the one or more secondary storage devices.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The networked information management system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the usage data comprises one or more of storage device maximum capacity, storage device current usage, storage device remaining space, storage device incremental usage associated with a predetermined time period, and storage device type.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The networked information management system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein comparing the validation results comprises calculating, for each validation result, a difference between the validation result and the associated with the second portion of the plurality of incremental backups.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The networked information management system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the disk usage growth prediction comprises a time or a time period when the one or more secondary storage devices are expected to become full.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The networked information management system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the disk usage growth prediction comprises an amount of time remaining until a time or a time period when the one or more secondary storage devices are expected to become full.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The networked information management system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the one or more processors is further configured to: modify, prior to performing the validation check, the usage status data to add missing data to the usage status data or to remove anomalous data from the usage status data.</claim-text></claim></claims></us-patent-application>