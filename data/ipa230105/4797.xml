<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004798A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004798</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17825303</doc-number><date>20220526</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202110736458.3</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>279</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>279</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INTENT RECOGNITION MODEL TRAINING AND INTENT RECOGNITION METHOD AND APPARATUS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Hongyang</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>JIAO</last-name><first-name>Zhenyu</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SUN</last-name><first-name>Shuqi</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>CHANG</last-name><first-name>Yue</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>LI</last-name><first-name>Tingting</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.</orgname><role>03</role><address><city>Beijing</city><country>CN</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure provides intent recognition model training and intent recognition methods and apparatuses, and relates to the field of artificial intelligence technologies. The intent recognition model training method includes: acquiring training data including a plurality of training texts and first annotation intents of the plurality of training texts; constructing a neural network model including a feature extraction layer and a first recognition layer; and training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model. The method for intent recognition includes: acquiring a to-be-recognized text; and inputting word segmentation results of the to-be-recognized text to an intent recognition model, and obtaining a first intent result and a second intent result of the to-be-recognized text according to an output result of the intent recognition model.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="74.17mm" wi="158.75mm" file="US20230004798A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="86.53mm" wi="164.93mm" file="US20230004798A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="191.18mm" wi="164.93mm" file="US20230004798A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="96.77mm" wi="151.72mm" file="US20230004798A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="213.95mm" wi="78.74mm" file="US20230004798A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="96.44mm" wi="145.03mm" file="US20230004798A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application claims the priority of Chinese Patent Application No. 202110736458.3, filed on Jun. 30, 2021, with the title of &#x201c;INTENT RECOGNITION MODEL TRAINING AND INTENT RECOGNITION METHOD AND APPARATUS.&#x201d; The disclosure of the above application is incorporated herein by reference in its entirety.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to the field of computer technologies, and in particular, to the field of artificial intelligence technologies such as natural language processing and deep learning. Intent recognition model training and intent recognition methods and apparatuses, an electronic device and a readable storage medium are provided.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">During human-machine dialogue interaction, a machine is required to understand intents of dialogue statements. However, in the prior art, during recognition of an intent of a dialogue statement, generally, only one of a sentence-level intent and a word-level intent of the dialogue statement can be recognized, which cannot be recognized at the same time.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">According to a first aspect of the present disclosure, a method is provided, including: acquiring training data including a plurality of training texts and first annotation intents of the plurality of training texts; constructing a neural network model including a feature extraction layer and a first recognition layer, the first recognition layer being configured to output, according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent; and training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model.</p><p id="p-0006" num="0005">According to a second aspect of the present disclosure, a method for intent recognition is provided, including: acquiring a to-be-recognized text; and inputting word segmentation results of the to-be-recognized text to an intent recognition model, and obtaining a first intent result and a second intent result of the to-be-recognized text according to an output result of the intent recognition model.</p><p id="p-0007" num="0006">According to a third aspect of the present disclosure, an electronic device is provided, including: at least one processor; and a memory communicatively connected with the at least one processor; wherein the memory stores instructions executable by the at least one processor, and the instructions are executed by the at least one processor to enable the at least one processor to perform a method, wherein the method includes: acquiring training data including a plurality of training texts and first annotation intents of the plurality of training texts; constructing a neural network model including a feature extraction layer and a first recognition layer, the first recognition layer being configured to output, according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent; and training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model.</p><p id="p-0008" num="0007">According to a fourth aspect of the present disclosure, there is provided a non-transitory computer readable storage medium with computer instructions stored thereon, wherein the computer instructions are used for causing a method, wherein the method includes: acquiring training data including a plurality of training texts and first annotation intents of the plurality of training texts; constructing a neural network model including a feature extraction layer and a first recognition layer, the first recognition layer being configured to output, according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent; and training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model.</p><p id="p-0009" num="0008">It should be understood that the content described in this part is neither intended to identify key or significant features of the embodiments of the present disclosure, nor intended to limit the scope of the present disclosure. Other features of the present disclosure will be made easier to understand through the following description.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">The accompanying drawings are intended to provide a better understanding of the solutions and do not constitute a limitation on the present disclosure. In the drawings,</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a first embodiment according to the present disclosure;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of a second embodiment according to the present disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of a third embodiment according to the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of a fourth embodiment according to the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of a fifth embodiment according to the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of a sixth embodiment according to the present disclosure; and</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram of an electronic device configured to perform intent recognition model training and intent recognition methods according to embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0018" num="0017">Exemplary embodiments of the present disclosure are illustrated below with reference to the accompanying drawings, which include various details of the present disclosure to facilitate understanding and should be considered only as exemplary. Therefore, those of ordinary skill in the art should be aware that various changes and modifications can be made to the embodiments described herein without departing from the scope and spirit of the present disclosure. Similarly, for clarity and simplicity, descriptions of well-known functions and structures are omitted in the following description.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a first embodiment according to the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, an intent recognition model training method according to the present disclosure may specifically include the following steps.</p><p id="p-0020" num="0019">In S<b>101</b>, training data including a plurality of training texts and first annotation intents of the plurality of training texts is acquired.</p><p id="p-0021" num="0020">In S<b>102</b>, a neural network model including a feature extraction layer and a first recognition layer is constructed, the first recognition layer being configured to output, according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent.</p><p id="p-0022" num="0021">In S<b>103</b>, the neural network model is trained according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model.</p><p id="p-0023" num="0022">In the intent recognition model training method according to this embodiment, a neural network model including a feature extraction layer and a first recognition layer is constructed, and a semantic vector of a candidate intent is set, so that the first recognition layer in the neural network model can output, according to the semantic vector of the candidate intent and an output result of the feature extraction layer, a first intent result of a training text and a score between each segmented word in the training text and the candidate intent, and an intent corresponding to each segmented word in the training text can also be obtained according to the score between each segmented word in the training text and the candidate intent. Therefore, a trained intent recognition model, in addition to being capable of recognizing a sentence-level intent of a text, is also capable of recognizing a word-level intent of the text, thereby improving recognition performance of the intent recognition model.</p><p id="p-0024" num="0023">In this embodiment, in the training data acquired by performing S<b>101</b>, the first annotation intents of the plurality of training texts are annotation results of sentence-level intents of the plurality of training texts. Each training text may correspond to one first annotation intent or correspond to a plurality of first annotation intents.</p><p id="p-0025" num="0024">For example, if a training text is &#x201c;Open the navigation app and take the highway&#x201d; and word segmentation results corresponding to the training text are &#x201c;open&#x201d;, &#x201c;navigation app&#x201d;, &#x201c;take&#x201d; and &#x201c;highway&#x201d;, a first annotation intent of the training text may include &#x201c;NAVI&#x201d; and &#x201c;HIGHWAY&#x201d;, and a second annotation intent of the training text may include &#x201c;NAVI&#x201d; corresponding to &#x201c;open&#x201d;, &#x201c;NAVI&#x201d; corresponding to &#x201c;navigation app&#x201d;, &#x201c;HIGHWAY&#x201d; corresponding to &#x201c;take&#x201d; and &#x201c;HIGHWAY&#x201d; corresponding to &#x201c;highway&#x201d;.</p><p id="p-0026" num="0025">In this embodiment, after S<b>101</b> is performed to acquire the training data including a plurality of training texts and first annotation intents of the plurality of training texts, S<b>102</b> is performed to construct a neural network model including a feature extraction layer and a first recognition layer.</p><p id="p-0027" num="0026">In this embodiment, when S<b>102</b> is performed to construct the neural network model, a plurality of candidate intents and a semantic vector corresponding to each candidate intent may also be preset. The semantic vector of the candidate intent is configured to represent semantics of the candidate intent, which may be constantly updated with the training of the neural network model.</p><p id="p-0028" num="0027">Specifically, in this embodiment, in the neural network model constructed by performing S<b>102</b>, when outputting a first semantic vector of each segmented word in a training text according to word segmentation results of the training text inputted, the feature extraction layer may adopt the following optional implementation manner. For each training text, a word vector of each segmented word in the training text is obtained. For example, the word vector of each segmented word is obtained by performing embedding processing on the segmented word. An encoding result and an attention calculation result of each segmented word are obtained according to the word vector of each segmented word. For example, the word vector is inputted to a bidirectional long short term memory (Bi-Lstm) encoder to obtain the encoding result, and the word vector is inputted to a multi-attention layer to obtain the attention calculation result. A splicing result between the encoding result and the attention calculation result of each segmented word is decoded, and a decoding result is taken as the first semantic vector of each segmented word. For example, the splicing result is inputted to a long short term memory (Lstm) decoder to obtain the decoding result.</p><p id="p-0029" num="0028">In this embodiment, when S<b>102</b> is performed to input the word vector to the multi-attention layer to obtain the attention calculation result, the word vector may be transformed by using three different linear layers, to obtain Q (queries matrices), K (keys matrices), and V (values matrices), respectively. Then, the attention calculation result of each segmented word is obtained according to the obtained Q, K and V.</p><p id="p-0030" num="0029">In this embodiment, the attention calculation result of each segmented word may be obtained by using the following formula:</p><p id="p-0031" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mi>C</mi>  <mo>=</mo>  <mrow>   <mi>softmax</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mrow>    <mo>(</mo>    <mfrac>     <msup>      <mi>QK</mi>      <mi>T</mi>     </msup>     <msqrt>      <msub>       <mi>d</mi>       <mi>k</mi>      </msub>     </msqrt>    </mfrac>    <mo>)</mo>   </mrow>   <mo>&#x2062;</mo>   <mi>V</mi>  </mrow> </mrow></math></maths></p><p id="p-0032" num="0030">In the formula, C denotes an attention calculation result of a segmented word; Q denotes a queries matrix; K denotes a keys matrix; V denotes a values matrix; and d<sub>k </sub>denotes a number of segmented words.</p><p id="p-0033" num="0031">Specifically, in this embodiment, in the neural network model constructed by performing S<b>102</b>, when outputting, according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent, the first recognition layer may adopt the following optional implementation manner: obtaining, for each training text according to a first semantic vector of each segmented word in the training text and the semantic vector of the candidate intent, a second semantic vector of each segmented word and a score between each segmented word and the candidate intent, wherein the score between each segmented word and the candidate intent may be an attention score between the two; and performing classification according to the second semantic vector of each segmented word, and taking a classification result as the first intent result of the training text. For example, the second semantic vector of the segmented word is inputted into a classifier after linear layer transformation, and a score of each candidate intent is obtained by the classifier. Then, the candidate intent whose score exceeds a preset threshold is selected as the first intent result of the training text.</p><p id="p-0034" num="0032">In this embodiment, when S<b>102</b> is performed to obtain the second semantic vector of each segmented word, a result obtained after linear layer transformation on the semantic vector of the candidate intent may be taken as Q, results obtained after the first semantic vector of the segmented word is transformed by two different linear layers are taken as K and V respectively, and then the second semantic vector of the segmented word is calculated according to the obtained Q, K and V.</p><p id="p-0035" num="0033">In this embodiment, after S<b>102</b> is performed to construct the neural network model including the feature extraction layer and the first recognition layer, S<b>103</b> is performed to train the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model.</p><p id="p-0036" num="0034">In this embodiment, the intent recognition model trained by performing S<b>103</b> can output a sentence-level intent and a word-level intent of a text according to word segmentation results of the text inputted.</p><p id="p-0037" num="0035">Specifically, in this embodiment, when S<b>103</b> is performed to train the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model, the following optional implementation manner may be adopted: inputting the word segmentation results of the plurality of training texts to the neural network model to obtain a first intent result outputted by the neural network model for each training text; calculating a loss function value according to the first intent results of the plurality of training texts and the first annotation intents of the plurality of training texts; and adjusting parameters of the neural network model and the semantic vector of the candidate intent according to the calculated loss function value, and completing the training of the neural network model in a case where it is determined that the calculated loss function value converges, to obtain the intent recognition model.</p><p id="p-0038" num="0036">That is, in this embodiment, during the training of the neural network model, the semantic vector of the candidate intent may be constantly adjusted, so that the semantic vector of the candidate intent can represent the semantics of the candidate intent more accurately, thereby improving the accuracy of the first intent result of the training text obtained according to the semantic vector of the candidate intent and the first semantic vector of each segmented word in the training text.</p><p id="p-0039" num="0037"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of a second embodiment according to the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an intent recognition model training method according to the present disclosure may specifically include the following steps.</p><p id="p-0040" num="0038">In S<b>201</b>, training data including the plurality of training texts, the first annotation intents of the plurality of training texts and second annotation intents of the plurality of training texts are acquired.</p><p id="p-0041" num="0039">In S<b>202</b>, the neural network model including the feature extraction layer, the first recognition layer and a second recognition layer is constructed, the second recognition layer being configured to output, according to the first semantic vector of each segmented word in the training text outputted by the feature extraction layer, a second intent result of the training text.</p><p id="p-0042" num="0040">In S<b>203</b>, the neural network model is trained according to word segmentation results of the plurality of training texts, the first annotation intents of the plurality of training texts and the second annotation intents of the plurality of training texts to obtain an intent recognition model.</p><p id="p-0043" num="0041">That is, in this embodiment, the acquired training data may further include second annotation intents of the training texts, and a neural network model including a second recognition layer is corresponding constructed, so as to obtain an intent recognition model by training according to the training texts including the first annotation intents and the second annotation intents. Through the trained intent recognition model according to this embodiment, there is no need to obtain an intent recognition result of each segmented word in the training text according to the score between each segmented word in the training text and the candidate intent outputted by the first recognition layer, which further improves efficiency of intent recognition performed by the intent recognition model.</p><p id="p-0044" num="0042">In this embodiment, in the training data acquired by performing S<b>201</b>, the second annotation intents of the plurality of training texts are word-level intents of the plurality of training texts. One segmented word in each training text corresponds to one second annotation intent.</p><p id="p-0045" num="0043">In this embodiment, in the neural network model constructed by performing S<b>202</b>, when outputting, according to a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a second intent result of the training text, the second recognition layer may adopt the following optional implementation manner: for each training text, performing classification according to the first semantic vector of each segmented word in the training text, to take a classification result of each segmented word as the second intent result of the training text. For example, the first semantic vector of each segmented word is inputted into a classifier after linear layer transformation, and a score of each candidate intent is obtained by the classifier. Then, the candidate intent whose score exceeds a preset threshold is selected as the second intent result corresponding to the segmented word.</p><p id="p-0046" num="0044">In this embodiment, when S<b>203</b> is performed to train the neural network model according to word segmentation results of the plurality of training texts, the first annotation intents of the plurality of training texts and the second annotation intents of the plurality of training texts to obtain an intent recognition model, the following optional implementation manner may be adopted: inputting the word segmentation results of the plurality of training texts to the neural network model to obtain a first intent result and a second intent result outputted by the neural network model for each training text; calculating a first loss function value according to the first intent results of the plurality of training texts and the first annotation intents of the plurality of training texts, and calculating a second loss function value according to the second intent results of the plurality of training texts and the second annotation intents of the plurality of training texts; and adjusting parameters of the neural network model and the semantic vector of the candidate intent according to the calculated first loss function value and second loss function value, and completing the training of the neural network model in a case where it is determined that the calculated first loss function value and second loss function value converge, to obtain the intent recognition model.</p><p id="p-0047" num="0045"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of a third embodiment according to the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, an intent recognition method according to the present disclosure may specifically include the following steps.</p><p id="p-0048" num="0046">In S<b>301</b>, a to-be-recognized text is acquired.</p><p id="p-0049" num="0047">In S<b>302</b>, word segmentation results of the to-be-recognized text are inputted to an intent recognition model, and a first intent result and a second intent result of the to-be-recognized text are obtained according to an output result of the intent recognition model.</p><p id="p-0050" num="0048">That is, in this embodiment, intent recognition is performed on the to-be-recognized text by using a pre-trained intent recognition model. Since the intent recognition model can output a sentence-level intent and a word-level intent of the to-be-recognized text, types of recognized intents are enriched and the accuracy of intent recognition is improved.</p><p id="p-0051" num="0049">The intent recognition model used in this embodiment may be obtained in different training manners. If the intent recognition model is trained by constructing a neural network model including a second recognition layer and training data including second annotation intents, in this embodiment, after word segmentation results of the to-be-recognized text are inputted to the intent recognition model, the intent recognition model may output the first intent result through the first recognition layer and output the second intent result through the second recognition layer.</p><p id="p-0052" num="0050">If the intent recognition model is not trained by constructing a neural network model including a second recognition layer and training data including second annotation intents, in this embodiment, after word segmentation results of the to-be-recognized text are inputted to the intent recognition model, the intent recognition model outputs the first intent result and scores between segmented words in the to-be-recognized text and the candidate intent through the first recognition layer. In this embodiment, when S<b>302</b> is performed to obtain a second intent result according to an output result of the intent recognition model, the following optional implementation manner may be adopted: obtaining the second intent result of the to-be-recognized text according to the scores between the segmented words in the to-be-recognized text and the candidate intent outputted by the intent recognition model. For example, in this embodiment, a score matrix may be constructed according to the scores between the segmented words and the candidate intent, and the second intent result corresponding to each segmented word is obtained by conducting a search with a viterbi algorithm.</p><p id="p-0053" num="0051"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of a fourth embodiment according to the present disclosure. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of intent recognition according to this embodiment. If a to-be-recognized text is &#x201c;Open the navigation app and take the highway&#x201d;, word segmentation results corresponding to the to-be-recognized text are &#x201c;open&#x201d;, &#x201c;navigation app&#x201d;, &#x201c;take&#x201d; and &#x201c;highway&#x201d;, and candidate intents include &#x201c;NAVI&#x201d;, &#x201c;HIGHWAY&#x201d; and &#x201c;POI&#x201d;, semantic vectors of the candidate intents are 11, 12 and 13 respectively. The word segmentation results corresponding to the to-be-recognized text are inputted to an intent recognition model, and a feature extraction layer in the intent recognition model passes a word vector of each word segmentation result through an encoder layer, an attention layer, a connection layer and a decoder layer to obtain a first semantic vector h<b>1</b> corresponding to &#x201c;open&#x201d;, a first semantic vector h<b>2</b> corresponding to &#x201c;navigation app&#x201d;, a first semantic vector h<b>3</b> corresponding to &#x201c;take&#x201d; and a first semantic vector h<b>4</b> corresponding to &#x201c;highway&#x201d;. Then, the first semantic vectors of the word segmentation results are inputted to a second recognition layer, to obtain second intent results corresponding to the word segmentation results outputted by the second recognition layer, which are &#x201c;NAVI&#x201d;, &#x201c;NAVI&#x201d;, &#x201c;HIGHWAY&#x201d; and &#x201c;HIGHWAY&#x201d;. The first semantic vectors of the word segmentation results and the semantic vectors of the candidate intents are inputted to a first recognition layer, to obtain first intent results corresponding to the to-be-recognized text outputted by the first recognition layer are &#x201c;NAVI&#x201d; and &#x201c;HIGHWAY&#x201d;. In addition, the first recognition layer may further output scores between the word segmentation results in the to-be-recognized text and the candidate intents, for example, the score matrix on the left of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0054" num="0052"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of a fifth embodiment according to the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, an intent recognition model training apparatus <b>500</b> according to this embodiment includes: a first acquisition unit <b>501</b> configured to acquire training data including a plurality of training texts and first annotation intents of the plurality of training texts; a construction unit <b>502</b> configured to construct a neural network model including a feature extraction layer and a first recognition layer, the first recognition layer being configured to output, according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent; and a training unit <b>503</b> configured to train the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model.</p><p id="p-0055" num="0053">In the training data acquired by the first acquisition unit <b>501</b>, the first annotation intents of the plurality of training texts are annotation results of sentence-level intents of the plurality of training texts. Each training text may correspond to one first annotation intent or correspond to a plurality of first annotation intents.</p><p id="p-0056" num="0054">When acquiring the training data, the first acquisition unit <b>501</b> may further acquire second annotation intents of the plurality of training texts, which are word-level intents of the plurality of training texts. One segmented word in each training text corresponds to one second annotation intent.</p><p id="p-0057" num="0055">After the first acquisition unit <b>501</b> acquires the training data, the construction unit <b>502</b> constructs a neural network model including a feature extraction layer and a first recognition layer.</p><p id="p-0058" num="0056">When the construction unit <b>502</b> constructs the neural network model, a plurality of candidate intents and a semantic vector corresponding to each candidate intent may also be preset. The semantic vector of the candidate intent is configured to represent semantics of the candidate intent, which may be constantly updated with the training of the neural network model.</p><p id="p-0059" num="0057">Specifically, in the neural network model constructed by the construction unit <b>502</b>, when outputting a first semantic vector of each segmented word in a training text according to word segmentation results of the training text inputted, the feature extraction layer may adopt the following optional implementation manner: obtaining, for each training text, a word vector of each segmented word in the training text; obtaining an encoding result and an attention calculation result of each segmented word according to the word vector of each segmented word; and decoding a splicing result between the encoding result and the attention calculation result of each segmented word, and taking a decoding result as the first semantic vector of each segmented word.</p><p id="p-0060" num="0058">When the construction unit <b>502</b> inputs the word vector to the multi-attention layer to obtain the attention calculation result, the word vector may be transformed by using three different linear layers, to obtain Q (queries matrices), K (keys matrices), and V (values matrices), respectively. Then, the attention calculation result of each segmented word is obtained according to the obtained Q, K and V.</p><p id="p-0061" num="0059">Specifically, in the neural network model constructed by the construction unit <b>502</b>, when outputting, according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent, the first recognition layer may adopt the following optional implementation manner: obtaining, for each training text according to a first semantic vector of each segmented word in the training text and the semantic vector of the candidate intent, a second semantic vector of each segmented word and a score between each segmented word and the candidate intent, wherein the score between each segmented word and the candidate intent may be an attention score between the two; and performing classification according to the second semantic vector of each segmented word, and taking a classification result as the first intent result of the training text.</p><p id="p-0062" num="0060">When the construction unit <b>502</b> obtains the second semantic vector of each segmented word, a result obtained after linear layer transformation on the semantic vector of the candidate intent may be taken as Q, results obtained after the first semantic vector of the segmented word is transformed by two different linear layers are taken as K and V respectively, and then the second semantic vector of the segmented word is calculated according to the obtained Q, K and V.</p><p id="p-0063" num="0061">The construction unit <b>502</b> may further construct a neural network model including a second recognition layer, when outputting, according to a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a second intent result of the training text, the second recognition layer may adopt the following optional implementation manner: for each training text, performing classification according to the first semantic vector of each segmented word in the training text, to take a classification result of each segmented word as the second intent result of the training text.</p><p id="p-0064" num="0062">In this embodiment, after the construction unit <b>502</b> constructs the neural network model including the feature extraction layer and the first recognition layer, the training unit <b>503</b> trains the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model.</p><p id="p-0065" num="0063">Specifically, when the training unit <b>503</b> trains the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model, the following optional implementation manner may be adopted: inputting the word segmentation results of the plurality of training texts to the neural network model to obtain a first intent result outputted by the neural network model for each training text; calculating a loss function value according to the first intent results of the plurality of training texts and the first annotation intents of the plurality of training texts; and adjusting parameters of the neural network model and the semantic vector of the candidate intent according to the calculated loss function value, and completing the training of the neural network model in a case where it is determined that the calculated loss function value converges, to obtain the intent recognition model.</p><p id="p-0066" num="0064">That is, in this embodiment, during the training of the neural network model, the semantic vector of the candidate intent may be constantly adjusted, so that the semantic vector of the candidate intent can represent the semantics of the candidate intent more accurately, thereby improving the accuracy of the first intent result of the training text obtained according to the semantic vector of the candidate intent and the first semantic vector of each segmented word in the training text.</p><p id="p-0067" num="0065">When the training unit <b>503</b> trains the neural network model according to word segmentation results of the plurality of training texts, the first annotation intents of the plurality of training texts and the second annotation intents of the plurality of training texts to obtain an intent recognition model, the following optional implementation manner may be adopted: inputting the word segmentation results of the plurality of training texts to the neural network model to obtain a first intent result and a second intent result outputted by the neural network model for each training text; calculating a first loss function value according to the first intent results of the plurality of training texts and the first annotation intents of the plurality of training texts, and calculating a second loss function value according to the second intent results of the plurality of training texts and the second annotation intents of the plurality of training texts; and adjusting parameters of the neural network model and the semantic vector of the candidate intent according to the calculated first loss function value and second loss function value, and completing the training of the neural network model in a case where it is determined that the calculated first loss function value and second loss function value converge, to obtain the intent recognition model.</p><p id="p-0068" num="0066"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of a sixth embodiment according to the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, an intent recognition model training apparatus <b>600</b> according to this embodiment includes:</p><p id="p-0069" num="0067">a second acquisition unit <b>601</b> configured to acquire a to-be-recognized text; and</p><p id="p-0070" num="0068">a recognition unit <b>602</b> configured to input word segmentation results of the to-be-recognized text to an intent recognition model, and obtain a first intent result and a second intent result of the to-be-recognized text according to an output result of the intent recognition model.</p><p id="p-0071" num="0069">The intent recognition model used in this embodiment may be obtained in different training manners. If the intent recognition model is trained by constructing a neural network model including a second recognition layer and training data including second annotation intents, after the recognition unit <b>602</b> inputs word segmentation results of the to-be-recognized text to the intent recognition model, the intent recognition model may output the first intent result through the first recognition layer and output the second intent result through the second recognition layer.</p><p id="p-0072" num="0070">If the intent recognition model is not trained by constructing a neural network model including a second recognition layer and training data including second annotation intents, after the recognition unit <b>602</b> inputs word segmentation results of the to-be-recognized text to the intent recognition model, the intent recognition model outputs the first intent result and scores between segmented words in the to-be-recognized text and the candidate intent through the first recognition layer. In this embodiment, when the recognition unit <b>602</b> obtains a second intent result according to an output result of the intent recognition model, the following optional implementation manner may be adopted: obtaining the second intent result of the to-be-recognized text according to the scores between the segmented words in the to-be-recognized text and the candidate intent outputted by the intent recognition model.</p><p id="p-0073" num="0071">Acquisition, storage and application of users' personal information involved in the technical solutions of the present disclosure comply with relevant laws and regulations, and do not violate public order and moral.</p><p id="p-0074" num="0072">According to embodiments of the present disclosure, the present disclosure further provides an electronic device, a readable storage medium and a computer program product.</p><p id="p-0075" num="0073"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram of an electronic device configured to perform intent recognition model training and intent recognition methods according to embodiments of the present disclosure. The electronic device is intended to represent various forms of digital computers, such as laptops, desktops, workbenches, personal digital assistants, servers, blade servers, mainframe computers and other suitable computing devices. The electronic device may further represent various forms of mobile devices, such as personal digital assistants, cellular phones, smart phones, wearable devices and other similar computing devices. The components, their connections and relationships, and their functions shown herein are examples only, and are not intended to limit the implementation of the present disclosure as described and/or required herein.</p><p id="p-0076" num="0074">As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the device <b>700</b> includes a computing unit <b>701</b>, which may perform various suitable actions and processing according to a computer program stored in a read-only memory (ROM) <b>702</b> or a computer program loaded from a storage unit <b>708</b> into a random access memory (RAM) <b>703</b>. The RAM <b>703</b> may also store various programs and data required to operate the device <b>700</b>. The computing unit <b>701</b>, the ROM <b>702</b> and the RAM <b>703</b> are connected to one another by a bus <b>704</b>. An input/output (I/O) interface <b>705</b> may also be connected to the bus <b>704</b>.</p><p id="p-0077" num="0075">A plurality of components in the device <b>700</b> are connected to the I/O interface <b>705</b>, including an input unit <b>706</b>, such as a keyboard and a mouse; an output unit <b>707</b>, such as various displays and speakers; a storage unit <b>708</b>, such as disks and discs; and a communication unit <b>709</b>, such as a network card, a modem and a wireless communication transceiver. The communication unit <b>709</b> allows the device <b>700</b> to exchange information/data with other devices over computer networks such as the Internet and/or various telecommunications networks.</p><p id="p-0078" num="0076">The computing unit <b>701</b> may be a variety of general-purpose and/or special-purpose processing components with processing and computing capabilities. Some examples of the computing unit <b>701</b> include, but are not limited to, a central processing unit (CPU), a graphics processing unit (GPU), various artificial intelligence (AI) computing chips, various computing units that run machine learning model algorithms, a digital signal processor (DSP), and any appropriate processor, controller or microcontroller, etc. The computing unit <b>701</b> performs the methods and processing described above, such as the operator registration method for a deep learning framework. For example, in some embodiments, the intent recognition model training and intent recognition methods may be implemented as a computer software program that is tangibly embodied in a machine-readable medium, such as the storage unit <b>708</b>.</p><p id="p-0079" num="0077">In some embodiments, part or all of a computer program may be loaded and/or installed on the device <b>700</b> via the ROM <b>702</b> and/or the communication unit <b>709</b>. One or more steps of the intent recognition model training and intent recognition methods described above may be performed when the computer program is loaded into the RAM <b>703</b> and executed by the computing unit <b>701</b>. Alternatively, in other embodiments, the computing unit <b>701</b> may be configured to perform the intent recognition model training and intent recognition methods described in the present disclosure by any other appropriate means (for example, by means of firmware).</p><p id="p-0080" num="0078">Various implementations of the systems and technologies disclosed herein can be realized in a digital electronic circuit system, an integrated circuit system, a field programmable gate array (FPGA), an application-specific integrated circuit (ASIC), an application-specific standard product (ASSP), a system on chip (SOC), a load programmable logic device (CPLD), computer hardware, firmware, software, and/or combinations thereof. Such implementations may include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which can be special or general purpose, configured to receive data and instructions from a storage system, at least one input apparatus, and at least one output apparatus, and to transmit data and instructions to the storage system, the at least one input apparatus, and the at least one output apparatus.</p><p id="p-0081" num="0079">Program codes configured to implement the methods in the present disclosure may be written in any combination of one or more programming languages. Such program codes may be supplied to a processor or controller of a general-purpose computer, a special-purpose computer, or another programmable data processing apparatus to enable the function/operation specified in the flowchart and/or block diagram to be implemented when the program codes are executed by the processor or controller. The program codes may be executed entirely on a machine, partially on a machine, partially on a machine and partially on a remote machine as a stand-alone package, or entirely on a remote machine or a server.</p><p id="p-0082" num="0080">In the context of the present disclosure, machine-readable media may be tangible media which may include or store programs for use by or in conjunction with an instruction execution system, apparatus or device. The machine-readable media may be machine-readable signal media or machine-readable storage media. The machine-readable media may include, but are not limited to, electronic, magnetic, optical, electromagnetic, infrared, or semiconductor systems, apparatuses or devices, or any suitable combinations thereof. More specific examples of machine-readable storage media may include electrical connections based on one or more wires, a portable computer disk, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read only memory (EPROM or flash memory), an optical fiber, a compact disk read only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination thereof.</p><p id="p-0083" num="0081">To provide interaction with a user, the systems and technologies described here can be implemented on a computer. The computer has: a display apparatus (e.g., a cathode-ray tube (CRT) or a liquid crystal display (LCD) monitor) for displaying information to the user; and a keyboard and a pointing apparatus (e.g., a mouse or trackball) through which the user may provide input for the computer. Other kinds of apparatuses may also be configured to provide interaction with the user. For example, a feedback provided for the user may be any form of sensory feedback (e.g., visual, auditory, or tactile feedback); and input from the user may be received in any form (including sound input, voice input, or tactile input).</p><p id="p-0084" num="0082">The systems and technologies described herein can be implemented in a computing system including background components (e.g., as a data server), or a computing system including middleware components (e.g., an application server), or a computing system including front-end components (e.g., a user computer with a graphical user interface or web browser through which the user can interact with the implementation mode of the systems and technologies described here), or a computing system including any combination of such background components, middleware components or front-end components. The components of the system can be connected to each other through any form or medium of digital data communication (e.g., a communication network). Examples of the communication network include: a local area network (LAN), a wide area network (WAN) and the Internet.</p><p id="p-0085" num="0083">The computer system may include a client and a server. The client and the server are generally far away from each other and generally interact via the communication network. A relationship between the client and the server is generated through computer programs that run on a corresponding computer and have a client-server relationship with each other. The server may be a cloud server, also known as a cloud computing server or cloud host, which is a host product in the cloud computing service system to solve the problems of difficult management and weak business scalability in the traditional physical host and a virtual private server (VPS). The server may also be a distributed system server, or a server combined with blockchain.</p><p id="p-0086" num="0084">It should be understood that the steps can be reordered, added, or deleted using the various forms of processes shown above. For example, the steps described in the present disclosure may be executed in parallel or sequentially or in different sequences, provided that desired results of the technical solutions disclosed in the present disclosure are achieved, which is not limited herein.</p><p id="p-0087" num="0085">The above specific implementations do not limit the extent of protection of the present disclosure. Those skilled in the art should understand that various modifications, combinations, sub-combinations, and replacements can be made according to design requirements and other factors. Any modifications, equivalent substitutions and improvements made within the spirit and principle of the present disclosure all should be included in the extent of protection of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004798A1-20230105-M00001.NB"><img id="EMI-M00001" he="7.03mm" wi="76.20mm" file="US20230004798A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>acquiring training data comprising a plurality of training texts and first annotation intents of the plurality of training texts;</claim-text><claim-text>constructing a neural network model comprising a feature extraction layer and a first recognition layer, the first recognition layer being configured to output, according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent; and</claim-text><claim-text>training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of outputting, by the feature extraction layer, a first semantic vector of each segmented word in a training text comprises:<claim-text>obtaining, for each training text, a word vector of each segmented word in the training text;</claim-text><claim-text>obtaining an encoding result and an attention calculation result of each segmented word according to the word vector of each segmented word; and</claim-text><claim-text>decoding a splicing result between the encoding result and the attention calculation result of each segmented word, and taking a decoding result as the first semantic vector of each segmented word.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of outputting, by the first recognition layer according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent comprises:<claim-text>obtaining, for each training text according to a first semantic vector of each segmented word in the training text and the semantic vector of the candidate intent, a second semantic vector of each segmented word and a score between each segmented word and the candidate intent; and</claim-text><claim-text>performing classification according to the second semantic vector of each segmented word, and taking a classification result as the first intent result of the training text.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model comprises:<claim-text>inputting the word segmentation results of the plurality of training texts to the neural network model to obtain a first intent result outputted by the neural network model for each training text;</claim-text><claim-text>calculating a loss function value according to the first intent results of the plurality of training texts and the first annotation intents of the plurality of training texts; and</claim-text><claim-text>adjusting parameters of the neural network model and the semantic vector of the candidate intent according to the calculated loss function value, until the neural network model converges, to obtain the intent recognition model.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of acquiring training data comprising a plurality of training texts and first annotation intents of the plurality of training texts comprises:<claim-text>acquiring training data comprising the plurality of training texts, the first annotation intents of the plurality of training texts and second annotation intents of the plurality of training texts.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the step of constructing a neural network model comprising a feature extraction layer and a first recognition layer comprises:<claim-text>constructing the neural network model comprising the feature extraction layer, the first recognition layer and a second recognition layer, the second recognition layer being configured to output, according to the first semantic vector of each segmented word in the training text outputted by the feature extraction layer, a second intent result of the training text.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the step of training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model comprises:<claim-text>inputting the word segmentation results of the plurality of training texts to the neural network model to obtain the first intent result and the second intent result outputted by the neural network model for each training text;</claim-text><claim-text>calculating a first loss function value according to the first intent results of the plurality of training texts and the first annotation intents of the plurality of training texts, and calculating a second loss function value according to the second intent results of the plurality of training texts and the second annotation intents of the plurality of training texts; and</claim-text><claim-text>adjusting parameters of the neural network model and the semantic vector of the candidate intent according to the calculated first loss function value and second loss function value, until the neural network model converges, to obtain the intent recognition model.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method for intent recognition, comprising:<claim-text>acquiring a to-be-recognized text; and</claim-text><claim-text>inputting word segmentation results of the to-be-recognized text to an intent recognition model, and obtaining a first intent result and a second intent result of the to-be-recognized text according to an output result of the intent recognition model;</claim-text><claim-text>wherein the intent recognition model is pre-trained with the method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the step of obtaining a first intent result and a second intent result of the to-be-recognized text according to an output result of the intent recognition model comprises:<claim-text>obtaining the second intent result of the to-be-recognized text according to scores between segmented words in the to-be-recognized text and a candidate intent outputted by the intent recognition model.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An electronic device, comprising:<claim-text>at least one processor; and</claim-text><claim-text>a memory communicatively connected with the at least one processor;</claim-text><claim-text>wherein the memory stores instructions executable by the at least one processor, and the instructions are executed by the at least one processor to enable the at least one processor to perform a method, wherein the method comprises:</claim-text><claim-text>acquiring training data comprising a plurality of training texts and first annotation intents of the plurality of training texts;</claim-text><claim-text>constructing a neural network model comprising a feature extraction layer and a first recognition layer, the first recognition layer being configured to output, according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent; and</claim-text><claim-text>training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The electronic device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the step of outputting, by the feature extraction layer, a first semantic vector of each segmented word in a training text comprises:<claim-text>obtaining, for each training text, a word vector of each segmented word in the training text;</claim-text><claim-text>obtaining an encoding result and an attention calculation result of each segmented word according to the word vector of each segmented word; and</claim-text><claim-text>decoding a splicing result between the encoding result and the attention calculation result of each segmented word, and taking a decoding result as the first semantic vector of each segmented word.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The electronic device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the step of outputting, by the first recognition layer according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent comprises:<claim-text>obtaining, for each training text according to a first semantic vector of each segmented word in the training text and the semantic vector of the candidate intent, a second semantic vector of each segmented word and a score between each segmented word and the candidate intent; and</claim-text><claim-text>performing classification according to the second semantic vector of each segmented word, and taking a classification result as the first intent result of the training text.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The electronic device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the step of training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model comprises:<claim-text>inputting the word segmentation results of the plurality of training texts to the neural network model to obtain a first intent result outputted by the neural network model for each training text;</claim-text><claim-text>calculating a loss function value according to the first intent results of the plurality of training texts and the first annotation intents of the plurality of training texts; and</claim-text><claim-text>adjusting parameters of the neural network model and the semantic vector of the candidate intent according to the calculated loss function value, until the neural network model converges, to obtain the intent recognition model.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The electronic device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the step of acquiring training data comprising a plurality of training texts and first annotation intents of the plurality of training texts comprises:<claim-text>acquiring training data comprising the plurality of training texts, the first annotation intents of the plurality of training texts and second annotation intents of the plurality of training texts.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The electronic device according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the step of constructing a neural network model comprising a feature extraction layer and a first recognition layer comprises:<claim-text>constructing the neural network model comprising the feature extraction layer, the first recognition layer and a second recognition layer, the second recognition layer being configured to output, according to the first semantic vector of each segmented word in the training text outputted by the feature extraction layer, a second intent result of the training text.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The electronic device according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the step of training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model comprises:<claim-text>inputting the word segmentation results of the plurality of training texts to the neural network model to obtain the first intent result and the second intent result outputted by the neural network model for each training text;</claim-text><claim-text>calculating a first loss function value according to the first intent results of the plurality of training texts and the first annotation intents of the plurality of training texts, and calculating a second loss function value according to the second intent results of the plurality of training texts and the second annotation intents of the plurality of training texts; and</claim-text><claim-text>adjusting parameters of the neural network model and the semantic vector of the candidate intent according to the calculated first loss function value and second loss function value, until the neural network model converges, to obtain the intent recognition model.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A non-transitory computer readable storage medium with computer instructions stored thereon, wherein the computer instructions are used for causing a method, wherein the method comprises:<claim-text>acquiring training data comprising a plurality of training texts and first annotation intents of the plurality of training texts;</claim-text><claim-text>constructing a neural network model comprising a feature extraction layer and a first recognition layer, the first recognition layer being configured to output, according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent; and</claim-text><claim-text>training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the step of outputting, by the feature extraction layer, a first semantic vector of each segmented word in a training text comprises:<claim-text>obtaining, for each training text, a word vector of each segmented word in the training text;</claim-text><claim-text>obtaining an encoding result and an attention calculation result of each segmented word according to the word vector of each segmented word; and</claim-text><claim-text>decoding a splicing result between the encoding result and the attention calculation result of each segmented word, and taking a decoding result as the first semantic vector of each segmented word.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the step of outputting, by the first recognition layer according to a semantic vector of a candidate intent and a first semantic vector of each segmented word in a training text outputted by the feature extraction layer, a first intent result of the training text and a score between each segmented word in the training text and the candidate intent comprises:<claim-text>obtaining, for each training text according to a first semantic vector of each segmented word in the training text and the semantic vector of the candidate intent, a second semantic vector of each segmented word and a score between each segmented word and the candidate intent; and</claim-text><claim-text>performing classification according to the second semantic vector of each segmented word, and taking a classification result as the first intent result of the training text.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the step of training the neural network model according to word segmentation results of the plurality of training texts and the first annotation intents of the plurality of training texts to obtain an intent recognition model comprises:<claim-text>inputting the word segmentation results of the plurality of training texts to the neural network model to obtain a first intent result outputted by the neural network model for each training text;</claim-text><claim-text>calculating a loss function value according to the first intent results of the plurality of training texts and the first annotation intents of the plurality of training texts; and</claim-text><claim-text>adjusting parameters of the neural network model and the semantic vector of the candidate intent according to the calculated loss function value, until the neural network model converges, to obtain the intent recognition model.</claim-text></claim-text></claim></claims></us-patent-application>