<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007428A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007428</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782911</doc-number><date>20201211</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-229636</doc-number><date>20191229</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>S</subclass><main-group>3</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>303</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>3</main-group><subgroup>008</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION PROCESSING DEVICE, CONTROL METHOD, AND NON-TRANSITORY COMPUTER-READABLE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NEC Corporation</orgname><address><city>Minato-ku, Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MARUYAMA</last-name><first-name>Toshikazu</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>KANEGAE</last-name><first-name>Shizuko</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>ENDO</last-name><first-name>lzumi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>SOMEYA</last-name><first-name>Kazunari</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>SHIBATA</last-name><first-name>Goh</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>MORISAKI</last-name><first-name>Kiyoshi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>NEC Corporation</orgname><role>03</role><address><city>Minato-ku, Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/046283</doc-number><date>20201211</date></document-id><us-371c12-date><date>20220606</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An information processing apparatus, a control method, and a control program capable of providing an experience close to one experienced in a real space to a user are provided. An information processing apparatus includes: an acquisition unit configured to acquire terminal position information of a communication terminal; a holding unit configured to hold a predetermined area and acoustic-image localization position information of an audio content to be output to the communication terminal while associating them with each other; a generation unit configured to generate acoustic-image localization information based on the acoustic-image localization position information and the terminal position information when the terminal position information is included in the predetermined area; and an output unit configured to output the acoustic-image localization information.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="120.23mm" wi="69.17mm" file="US20230007428A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="155.53mm" wi="71.20mm" file="US20230007428A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="174.67mm" wi="119.30mm" file="US20230007428A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="225.55mm" wi="104.73mm" file="US20230007428A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="232.49mm" wi="133.94mm" orientation="landscape" file="US20230007428A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="215.73mm" wi="164.34mm" orientation="landscape" file="US20230007428A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="227.84mm" wi="129.29mm" file="US20230007428A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="232.33mm" wi="157.65mm" orientation="landscape" file="US20230007428A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="104.06mm" wi="122.34mm" file="US20230007428A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="213.02mm" wi="130.73mm" file="US20230007428A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="240.45mm" wi="168.99mm" orientation="landscape" file="US20230007428A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="235.80mm" wi="164.34mm" orientation="landscape" file="US20230007428A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="247.73mm" wi="145.46mm" file="US20230007428A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="138.68mm" wi="103.29mm" file="US20230007428A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to an information processing apparatus, a control method, and a control program.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">A technology for generating, in order to provide a sound emitted from a personified object to a user, a sound of which the acoustic image is localized at the personified object has been known. Patent Literature 1 discloses a technology in which audio data of a personified object displayed in augmented reality (AR: Augmented Reality) is output from a speaker with a volume that is determined according to the position of this displayed personified object based on sensor data acquired by a wearable information display apparatus.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0004" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0003">Patent Literature 1: Japanese Unexamined Patent Application Publication No. 2018-097437</li></ul></p><heading id="h-0005" level="1">SUMMARY OF INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0005" num="0004">In the technology disclosed in Patent Literature 1, the audio data of the object is processed based on sensor data related to the line of sight of a user, and the direction of the movement and the motion of the user. In other words, in the related art disclosed in Patent Literature 1, the audio data is processed based solely on information about the user under the precondition that the position of the object at which the acoustic image is localized is fixed.</p><p id="p-0006" num="0005">As information services have become diversified and sophisticated in recent years, new experience services that cannot be experienced in a real space have been studied. For example, a service in which a user is virtually accompanied by a personified object has been studied. The technology disclosed in Patent Literature 1 is based on the precondition that the object at which the acoustic image is localized does not move. Therefore, when the technology disclosed in Patent Literature 1 is used, it is impossible to provide a service in which a user can have an experience equivalent to one experienced in a real space. That is, when the technology disclosed in Patent Literature 1 is used, it may not possible to provide an experience close to one experienced in a real space.</p><p id="p-0007" num="0006">In view of the above-described problem, an object of the present disclosure is to provide an information processing apparatus, a control method, and a control program capable of providing an experience close to one experienced in a real space to a user.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0008" num="0007">An information processing apparatus according to the present disclosure includes:</p><p id="p-0009" num="0008">an acquisition unit configured to acquire terminal position information of a communication terminal;</p><p id="p-0010" num="0009">a holding unit configured to hold a predetermined area and acoustic-image localization position information of an audio content to be output to the communication terminal while associating them with each other;</p><p id="p-0011" num="0010">a generation unit configured to generate acoustic-image localization information based on the acoustic-image localization position information and the terminal position information when the terminal position information is included in the predetermined area; and</p><p id="p-0012" num="0011">an output unit configured to output the acoustic-image localization information.</p><p id="p-0013" num="0012">A control method according to the present disclosure includes:</p><p id="p-0014" num="0013">acquiring terminal position information of a communication terminal;</p><p id="p-0015" num="0014">holding a predetermined area and acoustic-image localization position information of an audio content to be output to the communication terminal while associating them with each other;</p><p id="p-0016" num="0015">generating acoustic-image localization information based on the acoustic-image localization position information and the terminal position information when the terminal position information is included in the predetermined area; and</p><p id="p-0017" num="0016">outputting the acoustic-image localization information.</p><p id="p-0018" num="0017">A control program according to the present disclosure is a control program for causing a computer to perform:</p><p id="p-0019" num="0018">acquiring terminal position information of a communication terminal;</p><p id="p-0020" num="0019">holding a predetermined area and acoustic-image localization position information of an audio content to be output to the communication terminal while associating them with each other;</p><p id="p-0021" num="0020">generating acoustic-image localization information based on the acoustic-image localization position information and the terminal position information when the terminal position information is included in the predetermined area; and</p><p id="p-0022" num="0021">outputting the acoustic-image localization information.</p><heading id="h-0008" level="1">Advantageous Effects of Invention</heading><p id="p-0023" num="0022">According to the present disclosure, it is possible to provide an information processing apparatus, a control method, and a control program capable of providing an experience close to one experienced in a real space to a user.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example of a configuration of an information processing apparatus according to a first example embodiment;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart showing an example of operations performed by the information processing apparatus according to the first example embodiment;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining an outline of an information processing system according to a second example embodiment;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example of a configuration of the information processing system according to the second example embodiment;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example of an acoustic-image localization relation table;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart showing an example of operations performed by a server apparatus according to the second example embodiment;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example of a configuration of an information processing system according to a third example embodiment;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an example of a configuration of a server apparatus according to a fourth example embodiment;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart for explaining details of an operation in a process for generating acoustic-image localization information according to the fourth example embodiment;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example of a configuration of an information system according to a fifth example embodiment;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows an example of an acoustic-image localization relation table according to the fifth example embodiment;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart showing an example of operations performed by an information processing system according to the fifth example embodiment; and</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram showing an example of a hardware configuration of an information processing apparatus or the like according to each example embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">EXAMPLE EMBODIMENT</heading><p id="p-0037" num="0036">An example embodiment will be described hereinafter with reference to the drawings. Note that, in the example embodiment, the same elements are denoted by the same reference numerals (or symbols), and redundant descriptions thereof are omitted.</p><heading id="h-0011" level="1">First Example Embodiment</heading><p id="p-0038" num="0037">An example of a configuration of an information processing apparatus <b>1</b> according to a first example embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example of the configuration of the information processing apparatus according to the first example embodiment. The information processing apparatus <b>1</b> includes an acquisition unit <b>2</b>, a holding unit <b>3</b>, a generation unit <b>4</b>, and an output unit <b>5</b>.</p><p id="p-0039" num="0038">The acquisition unit <b>2</b> acquires terminal position information of a communication terminal (not shown) from the communication terminal.</p><p id="p-0040" num="0039">The holding unit <b>3</b> holds a predetermined area and acoustic-image localization position information of an audio content to be output to the communication terminal while associating them with each other (i.e., in a state in which they are associated with each other).</p><p id="p-0041" num="0040">The holding unit <b>3</b> may hold, in advance, a table in which pieces of position information for specifying predetermined areas, audio contents to be output to communication terminals, and pieces of acoustic-image localization position information of the audio contents are associated with each other. Alternatively, the holding unit <b>3</b> may acquire the above-described table from other communication apparatuses and hold the acquired table. The predetermined area may be a predefined area called a geofence.</p><p id="p-0042" num="0041">The audio content may be an audio content related to a virtual object, or may be, when a voice recognition service is used, an audio content corresponding to a result of voice recognition. The audio content may be an audio content for which an acoustic-image localizing process has not been performed yet, and may be an audio content stored in advance in the information processing apparatus <b>1</b>. In other words, the audio content may be an audio content to which a parameter(s) for performing an acoustic-image localizing process has not been added yet.</p><p id="p-0043" num="0042">The virtual object may be, for example, a virtual character such as a virtual friend, a virtual boyfriend/girlfriend, or a virtual guide, or may be a character in an animated cartoon or an actor/actress who appears in a TV drama or the like. Alternatively, the virtual object may be a virtual object such as a shop, a signboard, or a pair of sandals.</p><p id="p-0044" num="0043">The acoustic-image localization position information is position information related to a relative position with respect to the terminal position information (i.e., with respect to the position of the terminal indicted by the terminal position information). Therefore, in the following description, the fact that the acoustic-image localization position information or the acoustic image localized position is changed or adjusted means that the relative position of the acoustic-image localization position information or the acoustic image localized position with respect to the terminal position information of the communication terminal is changed.</p><p id="p-0045" num="0044">When the terminal position information is included in the predetermined area, the generation unit <b>4</b> generates acoustic-image localization information based on the acoustic-image localization position information, the terminal position information, and the like. The acoustic-image localization information is a parameter used to perform an acoustic-image localizing process for the audio content associated with the predetermined area. In other words, the acoustic-image localization information is a parameter used to correct the audio content associated with the predetermined area so that a user can hear the audio content as a sound emitted from the acoustic-image localization position. Therefore, the acoustic-image localization information may include the terminal position information, and also include the position information of an object, i.e., the position information of a target object, the terminal position information and the acoustic-image localization position information, and a relative angle between the terminal position information and the position information of the object. Further, the acoustic-image localization information may be generated, or may be performing a process for changing the acoustic-image localization position for predetermined acoustic-image localization information.</p><p id="p-0046" num="0045">The output unit <b>5</b> outputs the acoustic-image localization information. The output unit <b>5</b> outputs the acoustic-image localization information to at least one of a control unit (not shown) provided in the information processing apparatus <b>1</b> and a communication terminal. When the output unit <b>5</b> outputs the acoustic-image localization information to the control unit, the control unit may correct the audio content associated with the predetermined area based on the acoustic-image localization information, and output the corrected audio content toward the left and right ears of the user who possesses the communication terminal. Alternatively, when the output unit <b>5</b> outputs the acoustic-image localization information to the communication terminal, the output unit <b>5</b> may transmit the audio content associated with the predetermined area and the acoustic-image localization information to the communication terminal. Then, the output unit <b>5</b> may transmit, to the communication terminal, control information by which the communication terminal corrects the audio content based on the acoustic-image localization information and outputs the corrected audio content toward the left and right ears of the user who possesses the communication terminal. Further, when the communication terminal holds the audio content in advance, the output unit <b>5</b> may output only the acoustic-image localization information.</p><p id="p-0047" num="0046">Next, an example of operations performed by the information processing apparatus <b>1</b> according to the first example embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart showing an example of operations performed by the information processing apparatus according to the first example embodiment. Note that the following description will be given on the precondition that the holding unit <b>3</b> holds a table in which pieces of position information for specifying predetermined areas, audio contents to be output to communication terminals, and pieces of acoustic-image localization position information of the audio contents are associated with each other. Further, the information processing apparatus <b>1</b> performs the example of operations shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> every time the information processing apparatus <b>1</b> acquires terminal position information.</p><p id="p-0048" num="0047">The acquisition unit <b>2</b> acquires the terminal position information from a communication terminal (not shown) (Step S<b>1</b>).</p><p id="p-0049" num="0048">The generation unit <b>4</b> determines whether or not the terminal position information (i.e., the position of the terminal indicted by the terminal position information) is included in the predetermined area (Step S<b>2</b>). The generation unit <b>4</b> compares the terminal position information with position information specifying a predetermined area listed in the table, and determines whether or not the terminal position information is included in the predetermined area.</p><p id="p-0050" num="0049">Note that the generation unit <b>4</b> may calculate an approach angle of the communication terminal to the predetermined area based on the terminal position information, and determine whether or not the terminal position information is included in the predetermined area by further using the approach angle. Specifically, the acquisition unit <b>2</b> and the generation unit <b>4</b> successively acquire the terminal position information of the communication terminal. The generation unit <b>4</b> calculates a moving path of the communication terminal based on a group of pieces of terminal position information composed of a plurality of acquired pieces of terminal position information. The generation unit <b>4</b> determines at what angle the communication terminal has entered the predetermined area based on the calculated moving path. The generation unit <b>4</b> calculates an approach angle with respect to the center of the predetermined area from the moving path, and determines whether or not the approach angle is included in a predetermined range of angles. The generation unit <b>4</b> may determine that the terminal position information is included in the predetermined area when the terminal position information is included in the predetermined area and the approach angle is within the predetermined range of angles.</p><p id="p-0051" num="0050">When the terminal position information is included in the predetermined area (Yes in Step S<b>2</b>), the generation unit <b>4</b> generates acoustic-image localization information based on the acoustic-image localization position information associated with the predetermined area and the terminal position information (Step S<b>3</b>).</p><p id="p-0052" num="0051">On the other hand, when the terminal position information is not included in the predetermined area (No in Step S<b>2</b>), the process returns to the step S<b>1</b> and the information processing apparatus <b>1</b> performs the operation in the step S<b>1</b>.</p><p id="p-0053" num="0052">The output unit <b>5</b> outputs the acoustic-image localization information to at least one of the control unit provided in the information processing apparatus <b>1</b> and the communication terminal (Step S<b>4</b>).</p><p id="p-0054" num="0053">As described above, when the terminal position information is included in the predetermined area, the information processing apparatus <b>1</b> generates acoustic-image localization information based on the acoustic-image localization position information associated with the predetermined area and the terminal position information. The information processing apparatus <b>1</b> outputs the generated acoustic-image localization information to at least one of the control unit of the information processing apparatus itself and the communication terminal. The control unit of the information processing apparatus itself and the communication terminal correct the audio content to be output for the user based on the acoustic-image localization information, so that they can output the corrected audio content for the user. That is, according to the information processing apparatus <b>1</b> in accordance with the first example embodiment, it is possible to generate acoustic-image localization information that makes it possible to make the user hear such a sound that the user perceives it as if a virtual object is moving, and thereby to provide an experience close to one experienced in a real space to the user.</p><heading id="h-0012" level="1">Second Example Embodiment</heading><p id="p-0055" num="0054">Next, a second example embodiment will be described. The second example embodiment is a specific example of the first example embodiment. Firstly, prior to describing a specific example of a configuration according to the second example embodiment, an outline of the second example embodiment will be described.</p><heading id="h-0013" level="2">&#x3c;Overview&#x3e;</heading><p id="p-0056" num="0055">In recent years, services using AR technologies have been studied. As a service using an AR technology, for example, a service in which a user feels as if he/she is accompanied by a virtual character, such as a character in an animated cartoon or an actor/actress in a TV drama or the like, has been studied. In such a service, a situation in which the aforementioned character speaks to a user from a virtual position where the virtual character is present so that the user can feel that he/she is virtually accompanied by the virtual character has also been studied. Since the aforementioned character is a virtual character, the service may be referred to as an AR service in which the real world is augmented, and may also be referred to as an acoustic AR service. Note that the virtual character may be a virtual object such as a shop, a signboard, or a pair of sandals.</p><p id="p-0057" num="0056">The second example embodiment relates to an information processing system for realizing the aforementioned so-called acoustic AR service. Note that, as described above, since the information processing system is a system for realizing an acoustic AR service, it may also be referred to as an acoustic AR system.</p><p id="p-0058" num="0057">An outline of the information system according to the second example embodiment will be described hereinafter with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining an outline of the information processing system according to the second example embodiment. The information processing system according to the second example embodiment will be described on the assumption that it is, for example, a system that provides an acoustic AR service in which a user feels as if he/she is virtually accompanied by a virtual character such as a virtual friend or a virtual boyfriend/girlfriend.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram schematically showing an area where a user U is present as viewed from above in the vertically downward direction, and schematically showing a state in which, for example, the user U, who is accompanied by a virtual character C, is moving (e.g., walking) toward a building O in a sightseeing spot. In the information processing system according to the second example embodiment, a communication terminal <b>40</b>, which is carried (e.g., worn) by the user U, outputs an audio content for the user U in such a manner that the user U feels as if the character C present near him/her speaks to him/her at an arbitrary timing.</p><p id="p-0060" num="0059">Note that, in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the building O is an example of the target object toward which the user U moves, and the target object may be, for example, a facility or a shop, or may be any of various types of objects, such as a sign, a signboard, a mannequin, a mascot doll, an animal, and a firework. Further, although the character C does not actually exist in the real space, it is shown in the drawing for the sake of explanation. In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, solid arrows indicate the front, rear, left and right directions of the user U. The virtual position of the character C is set as a relative position with respect to the position of the user U, and in the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the virtual position of the character C is a position located on the right side of the user U. Note that the virtual position of the character C can be set at an arbitral position.</p><p id="p-0061" num="0060">Here, it is assumed that the information processing system according to the second example embodiment provides an acoustic AR service in which the character C is always present on the right side of the user U as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In the real world, for example, a friend or a boyfriend/girlfriend moves toward a shop Sh he/she has an interest at an arbitrary timing, or moves around the user U at an arbitrary timing. Therefore, if the information processing system according to the second example embodiment provides an acoustic AR service in which the character C is always present on the right side of the user U, it cannot provide an experience close to one experienced in a real space to the user. Therefore, the information processing system according to the second example embodiment provides an audio content to a user U in such a manner that the user U feels as if the character C moves close to the shop Sh as indicated by a dotted line L<b>1</b> or moves from the right side of the user U to the left side thereof as indicated by a dotted line L<b>2</b>.</p><p id="p-0062" num="0061">Further, the information processing system according to the second example embodiment provides an experience close to one experienced in a real space or an experience that cannot be experienced in a real space to the user U, so that it realizes such a situation that, for example, when the user U moves close to the shop Sh, the shop Sh virtually speaks to the user U.</p><p id="p-0063" num="0062">Note that, in the present disclosure, the communication terminal <b>40</b> is described as a communication terminal including a left unit <b>40</b>L attached to the left ear of the user U and a right unit <b>40</b>R attached to the right ear thereof. Further, the audio content output for the user U is described as audio contents including a left-ear audio content corresponding to the left unit <b>40</b>L and a right-ear audio content corresponding to the right unit <b>40</b>R.</p><heading id="h-0014" level="2">&#x3c;Example of Configuration of Information Processing System&#x3e;</heading><p id="p-0064" num="0063">Next, an example of a configuration of an information processing system <b>100</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example of the configuration of the information processing system according to the second example embodiment. The information processing system <b>100</b> includes communication terminals <b>40</b> and <b>50</b>, and a server apparatus <b>60</b>.</p><p id="p-0065" num="0064">The communication terminal <b>40</b>, which is the communication terminal <b>40</b> shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, is possessed by the user U and attached to (e.g., worn by) the user U. As described above, the communication terminal <b>40</b> is a communication terminal attached to each of both ears of the user, and includes a left unit <b>40</b>L attached to the left ear of the user and a right unit <b>40</b>R attached to the right ear thereof. Since the communication terminal <b>40</b> is composed of devices attached to both ears of the user, it may also be referred to as a hearable device(s). Note that the communication terminal <b>40</b> may be a communication terminal in which a left unit <b>40</b>L and a right unit <b>40</b>R are integrally formed (i.e., formed as one component).</p><p id="p-0066" num="0065">The communication terminal <b>40</b> is, for example, a communication terminal capable of performing radio communication provided by a communication carrier, and communicates with the server apparatus <b>60</b> through a network provided by a communication carrier. The communication terminal <b>40</b> acquires direction information of the communication terminal <b>40</b> and transmits the acquired direction information to the server apparatus <b>60</b>. The communication terminal <b>40</b> outputs an audio content for which an acoustic-image localizing process has been performed to each of both ears of the user. Note that although the following description will be given on the assumption that the communication terminal <b>40</b> (the left and right units <b>40</b>L and <b>40</b>R) directly communicates with the server apparatus <b>60</b>, the communication terminal <b>40</b> may be configured so as to communicate with the server apparatus <b>60</b> through the communication terminal <b>50</b>.</p><p id="p-0067" num="0066">The communication terminal <b>50</b> may be, for example, a smartphone terminal, a tablet-type terminal, a cellular phone, or a personal computer apparatus. The communication terminal <b>50</b> is also a communication terminal possessed by the user U shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The communication terminal <b>50</b> connects to and communicates with the communication terminal <b>40</b> through radio communication such as Bluetooth (Registered Trademark) or WiFi. Further, the communication terminal <b>50</b> communicates with the server apparatus <b>60</b> through, for example, a network provided by a communication carrier. The communication terminal <b>50</b> acquires terminal position information of the communication terminal <b>40</b> (the left and right units <b>40</b>L and <b>40</b>R) and transmits the acquired terminal position information to the server apparatus <b>60</b>. Note that, in the present disclosure, although it is assumed that the communication terminal <b>50</b> acquires the terminal position information of the communication terminal <b>40</b> (the left and right units <b>40</b>L and <b>40</b>R), the position information of the communication terminal <b>50</b> may be used as the terminal position information of the left and right units <b>40</b>L and <b>40</b>R.</p><p id="p-0068" num="0067">Note that although the information processing system <b>100</b> includes two communication terminals (the communication terminals <b>40</b> and <b>50</b>) in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the communication terminals <b>40</b> and <b>50</b> may be constructed, for example, as one communication terminal such as a head-mounted display. Further, the communication terminal <b>40</b> may acquire not only the direction information of the communication terminal <b>40</b> but also the terminal position information thereof. That is, it is sufficient if the information processing system <b>100</b> includes at least one communication terminal.</p><p id="p-0069" num="0068">The server apparatus <b>60</b> corresponds to the information processing apparatus <b>1</b> in the first example embodiment. The server apparatus <b>60</b> communicates with the communication terminals <b>40</b> and <b>50</b> through, for example, a network provided by a communication carrier. The server apparatus <b>60</b> acquires the direction information and the terminal position information of the communication terminal <b>40</b> from each of the communication terminals <b>40</b> and <b>50</b>.</p><p id="p-0070" num="0069">When the terminal position information is included in the predetermined area, the server apparatus <b>60</b> changes the acoustic-image localization position of the character C or the like shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The server apparatus <b>60</b> outputs an audio content that has been corrected based on acoustic-image localization information corresponding to the changed acoustic-image localization position for the user.</p><heading id="h-0015" level="2">&#x3c;Example of Configuration of Communication Terminal&#x3e;</heading><p id="p-0071" num="0070">Next, an example of a configuration of the communication terminal <b>40</b> will be described. The communication terminal <b>40</b> includes a direction information acquisition unit <b>41</b> and an output unit <b>42</b>. Note that since the communication terminal <b>40</b> includes the left and right units <b>40</b>L and <b>40</b>R, each of the left and right units <b>40</b>L and <b>40</b>R may include a direction information acquisition unit <b>41</b> and an output unit <b>42</b>.</p><p id="p-0072" num="0071">The direction information acquisition unit <b>41</b> includes, for example, a 9-axis sensor (a 3-axis acceleration sensor, a 3-axis gyroscopic sensor, and a 3-axis compass sensor). The direction information acquisition unit <b>41</b> acquires direction information of the communication terminal <b>40</b> by using the 9-axis sensor. The direction information acquisition unit <b>41</b> acquires the direction information in a periodic manner or in a non-periodic manner.</p><p id="p-0073" num="0072">The direction information acquisition unit <b>41</b> acquires, as the direction information, direction information including the orientation of the communication terminal <b>40</b> (i.e., the direction in which the communication terminal <b>40</b> faces) and the moving direction of the communication terminal <b>40</b>, both of which are acquired by the 9-axis sensor. The direction information acquisition unit <b>41</b> transmits the acquired direction information to the communication terminal <b>50</b>. Note that the direction information acquisition unit <b>41</b> may transmit the acquired direction information to the server apparatus <b>60</b>.</p><p id="p-0074" num="0073">Since the direction information acquisition unit <b>41</b> includes the 9-axis sensor, it can acquire not only the orientation and the moving direction of the communication terminal <b>40</b> but also the posture of the user. Therefore, the direction information may include the posture of the user, and may also be referred to as posture information. Further, since the direction information is data acquired by the 9-axis sensor, it may also be referred to as sensing data.</p><p id="p-0075" num="0074">The output unit <b>42</b> includes, for example, stereo speakers or the like. The output unit <b>42</b> also functions as a communication unit, so that it receives an audio content for which an acoustic-image localizing process has already been performed by the server apparatus <b>60</b>, and outputs the received audio content toward the ears of the user. The audio content for which the acoustic-image localizing process has already been performed by the server apparatus <b>60</b> includes a left-ear audio content for the left unit <b>40</b>L and a right-ear audio content for the right unit <b>40</b>R. The output unit <b>42</b> of the left unit <b>40</b>L outputs an audio content for the left ear, and the output unit <b>42</b> of the right unit <b>40</b>R outputs an audio content for the right ear.</p><p id="p-0076" num="0075">Next, an example of a configuration of the communication terminal <b>50</b> will be described. The communication terminal <b>50</b> includes a terminal position information acquisition unit <b>51</b>.</p><p id="p-0077" num="0076">The terminal position information acquisition unit <b>51</b> includes, for example, a GPS (Global Positioning System) receiver, an altitude sensor, and the like. The terminal position information acquisition unit <b>51</b> receives GPS signals and acquires latitude and longitude information of the communication terminal <b>50</b> based on the GPS signals. The terminal position information acquisition unit <b>51</b> acquires altitude information of the communication terminal <b>50</b> by the altitude sensor.</p><p id="p-0078" num="0077">The terminal position information acquisition unit <b>51</b> acquires terminal position information of each of the left and right units <b>40</b>L and <b>40</b>R of the communication terminal <b>40</b>. As described above, the communication terminal <b>50</b> communicates with the left and right units <b>40</b>L and <b>40</b>R, for example, through radio communication such as Bluetooth or WiFi. The terminal position information acquisition unit <b>51</b> calculates latitude and longitude information and altitude information of each of the left and right units <b>40</b>L and <b>40</b>R by using the direction information (the sensing data) acquired by the direction information acquisition unit <b>41</b> of each of the left and right units <b>40</b>L and <b>40</b>R. The terminal position information acquisition unit <b>51</b> acquires the latitude and longitude information and the altitude information of each of the left and right units <b>40</b>L and <b>40</b>R as terminal position information. The terminal position information acquisition unit <b>51</b> periodically acquires the position of each of the left and right units <b>40</b>L and <b>40</b>R. The terminal position information acquisition unit <b>51</b> transmits the terminal position information of each of the left and right units <b>40</b>L and <b>40</b>R to the server apparatus <b>60</b>.</p><p id="p-0079" num="0078">Note that the terminal position information acquisition unit <b>51</b> acquires the latitude and longitude information and the altitude information of each of the left and right units <b>40</b>L and <b>40</b>R based on the signal strength and the arriving direction of a radio signal used for the communication with the left and right units <b>40</b>L and <b>40</b>R. Further, the terminal position information acquisition unit <b>51</b> may use information including the latitude and longitude information and the altitude information as the terminal position information.</p><heading id="h-0016" level="2">&#x3c;Example of Configuration of Server Apparatus&#x3e;</heading><p id="p-0080" num="0079">Next, an example of a configuration of the server apparatus <b>60</b> will be described. The server apparatus <b>60</b> includes a terminal information acquisition unit <b>61</b>, a storage unit <b>62</b>, a generation unit <b>63</b>, an output unit <b>64</b>, and a control unit <b>65</b>.</p><p id="p-0081" num="0080">The terminal information acquisition unit <b>61</b> corresponds to the acquisition unit <b>2</b> in the first example embodiment. The terminal information acquisition unit <b>61</b> acquires terminal position information of the communication terminal <b>40</b>. The terminal information acquisition unit <b>61</b> also functions as a communication unit, so that it acquires terminal position information by receiving it from the communication terminal <b>50</b>. The terminal information acquisition unit <b>61</b> outputs the acquired terminal position information to the generation unit <b>63</b>.</p><p id="p-0082" num="0081">The storage unit <b>62</b> corresponds to the holding unit <b>3</b> in the first example embodiment. The storage unit <b>62</b> stores an acoustic-image localization relation table T<b>1</b>. The storage unit <b>62</b> may store the acoustic-image localization relation table T<b>1</b> in advance, or may receive the acoustic-image localization relation table T<b>1</b> from other communication apparatus and holds the received table.</p><p id="p-0083" num="0082">An example of the acoustic-image localization relation table T<b>1</b> will be described hereinafter with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example of the acoustic-image localization relation table. The acoustic-image localization relation table T<b>1</b> is a table in which target areas to which the acoustic-image localization position is changed, audio contents that are output when the terminal position information is included in the target area, and pieces of information related to the acoustic image localized position are associated with each other.</p><p id="p-0084" num="0083">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in the acoustic-image localization relation table T<b>1</b>, starting from the leftmost column, target areas, pieces of area information, pieces of object information, pieces of audio content information, original positions (i.e., positions before the change), and pieces of change information are set (i.e., listed). Further, in the acoustic-image localization relation table T<b>1</b>, for example, labels indicating which item the respective columns represent are set (i.e., defined) in the first row, and details of setting specified by the administrator of the server apparatus <b>60</b> or the information processing system <b>100</b> are set (i.e., specified) in the second and subsequent rows.</p><p id="p-0085" num="0084">In the target area column, area numbers of target areas to which the acoustic-image localization position is changed are set.</p><p id="p-0086" num="0085">In the area information column, pieces of information about the areas set in the target area column are set. In the area information column, pieces of information for specifying target areas to which the acoustic-image localization position is changed are set, and they can include a latitude, a longitude, a range, and a size.</p><p id="p-0087" num="0086">Further, the area information may include, for example, characteristic information indicating a characteristic of the target area such as &#x201c;Area where night view is beautiful from 18:00 to 24:00&#x201d; or &#x201c;Area which is crowded from 11:00 to 17:00&#x201d;. For example, the characteristic information may be set in such a manner that a time period is associated with a characteristic of the target area.</p><p id="p-0088" num="0087">Further, the area information may include angle information related to an approach angle when the terminal position information enters the target area. When the area information includes the angle information, the characteristic information may be associated with the angle information. For example, the characteristic information &#x201c;Area where night view is beautiful from 18:00 to 24:00&#x201d; is associated with angle information &#x3b8;<b>1</b>, and the characteristic information &#x201c;Area which is crowded from 11:00 to 17:00&#x201d; is associated with angle information &#x3b8;<b>2</b>. In this way, it is possible to provide no audio service to a user having a communication terminal that enters &#x201c;Area where night view is beautiful from 18:00 to 24:00&#x201d; at 15:00 at an approach angle of &#x3b8;<b>1</b> or smaller, and to provide an audio content for guiding the night view to a user who enters the area at an approach angle of &#x3b8;<b>1</b> or smaller at or after 18:00. Therefore, in this way, it is possible to develop services closer to those provided in a real space.</p><p id="p-0089" num="0088">In the object information column, object names related to audio contents to be output for the user are set. For example, when the terminal position information is included in the Area <b>1</b> and an audio content of the shop Sh or the character C shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> is output, the shop Sh or the character C is set in the object information.</p><p id="p-0090" num="0089">In the audio content information column, pieces of audio content information that are output for the user when the terminal position information is included in the area information are set. In the audio content information column, audio content names that are stored in advance in the storage unit <b>62</b> may be set. Alternatively, in the audio content information column, use of audio contents that are generated by processing parts of audio contents stored in the storage unit <b>62</b> by the control unit <b>65</b> (which will be described later) may be set. Alternatively, in the audio content information column, use of audio contents that are newly generated by the control unit <b>65</b> (which will be described later), such as responses to what are spoken by the user, may be set. Note that, in the following description, the audio contents generated by the control unit <b>65</b> may also be collectively referred to as generated contents (or contents to be generated).</p><p id="p-0091" num="0090">In the original position column, original acoustic image localized positions (i.e., acoustic image localized positions before the change) for the respective audio contents are set. In the original position column, relative positions with respect to the terminal position information are set. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in the original position column, directions with respect to the communication terminal <b>40</b> and distances from the communication terminal <b>40</b> are set. For a virtual object that virtually (i.e., imaginarily) accompanies the user U, such as the character C shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the virtual position of the character C is set in the original position. For an object of which the position is fixed, such as the shop Sh shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, no information may be set in the original position. Further, for a character that is different from the character C shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> and appears only in a specific area, such as a guide, no information may be set in the original position.</p><p id="p-0092" num="0091">Note that, in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, although &#x201c;Front Right&#x201d; and &#x201c;Right&#x201d; are set as the directions with respect to the communication terminal <b>40</b> in the original position column, an angle between the direction indicated by the direction information of the communication terminal <b>40</b> and the direction of the acoustic-image localization position with respect to the position of the communication terminal <b>40</b> may be set therein. In other words, a combination of an angle between the forward direction of the communication terminal <b>40</b> and the direction of the acoustic image localized position with respect to the position of the communication terminal <b>40</b> and a distance to the acoustic-image localization position with respect to the position of the communication terminal <b>40</b> may be set in the original position. Note that, in practice, the acoustic-image localization position may be determined according to the positions of various real objects, such as in a scene in which a virtual person speaks from the vicinity of various real objects. Therefore, an angle between the direction indicated by the direction information of the communication terminal <b>40</b> and the direction of the position of the target object, which is a specific object, with respect to the position of the communication terminal <b>40</b> may be set in the original position.</p><p id="p-0093" num="0092">In the change information column, pieces of information for specifying changed acoustic image localized positions are set for the respective audio contents. In the change information column, a relative position of the acoustic-image localization position with respect to the terminal position information may be set. Alternatively, in the change information column, a change distance (i.e., a distance of a change) from the original acoustic image localized position may be set. Alternatively, in the acoustic-image localization position information, height information (i.e., information about the height) by which the acoustic image localized position is changed from the original one may be set.</p><p id="p-0094" num="0093">Although only one change in position or height is set in each cell in the change information column in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a plurality of changes in position and height may be set in a time-series manner in one cell in the change information column. In the change information column, for example, repetitive changes in the height that occur at predetermined interval may be set, or changes representing movements of the character C may be set.</p><p id="p-0095" num="0094">Note that although no information related to changed acoustic-image localization positions is included in the acoustic-image localization relation table T<b>1</b>, the storage unit <b>62</b> holds acoustic-image localization position information related to the changed acoustic-image localization position determined by the generation unit <b>63</b> (which will be described later) while associating it with other relevant information. Further, in the present disclosure, although the acoustic-image localization relation table T<b>1</b> is described on the assumption that it holds (i.e., contains) pieces of change information, the acoustic-image localization relation table T<b>1</b> may hold (contain) changed acoustic-image localization positions in addition to or instead of the pieces of change information.</p><p id="p-0096" num="0095">The description will be continued by referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref> again. The storage unit <b>62</b> stores one or a plurality of audio contents set in the audio content column in the acoustic-image localization relation table T<b>1</b>. One or a plurality of audio contents are audio contents for which an acoustic-image localizing process has not been performed yet. That is, the storage unit <b>62</b> stores one or a plurality of audio contents to which a parameter(s) for performing an acoustic-image localizing process has not been added yet.</p><p id="p-0097" num="0096">The generation unit <b>63</b> corresponds to the generation unit <b>4</b> in the first example embodiment. When the terminal position information is included in one of the target areas listed in the acoustic-image localization relation table T<b>1</b>, the generation unit <b>63</b> generates acoustic-image localization information based on the acoustic-image localization position information and the terminal position information. The acoustic-image localization position information is information related to the changed acoustic image localized position. The acoustic-image localization information is a parameter that is used to perform an acoustic-image localizing process for the audio content associated with the target area in which the terminal position information is included. The acoustic-image localization information is a parameter for correcting the audio content associated with the target area in which the terminal information is included so that the audio content can be heard as a sound emitted from the changed acoustic-image localization position (the acoustic-image localization position information).</p><p id="p-0098" num="0097">The generation unit <b>63</b> compares area information in the acoustic-image localization relation table T<b>1</b> with the terminal position information, and thereby determines, for each of the target areas in the acoustic-image localization relation table T<b>1</b>, whether or not the terminal position information is included in that target areas.</p><p id="p-0099" num="0098">Note that the generation unit <b>63</b> may calculate, for each of the target areas, an approach angle of the communication terminal <b>40</b> to that target area based on the terminal position information, and thereby determine whether or not the terminal position information is included in any of the target areas. In such a case, the generation unit <b>63</b> calculates the moving path of the communication terminal <b>40</b> based on a group of pieces of terminal position information composed of a plurality of acquired pieces of terminal position information. The generation unit <b>63</b> calculates, for each of the target areas, an approach angle of the communication terminal <b>40</b> to that target area based on the calculated moving path. The generation unit <b>63</b> calculates, for each of the target areas, an approach angle of the communication terminal <b>40</b> to the center of that target area obtained (e.g., calculated) from the area information of that target area based on the calculated moving path. The generation unit <b>63</b> determines, for each of the target areas, whether the approach angle to that target area is within a predetermined range of angles. The generation unit <b>63</b> determines a target area of which the terminal position information is included in the area information, and in which the approach angle to the target area corresponding to the area information is within the predetermined range of angles.</p><p id="p-0100" num="0099">When one of the pieces of area information listed in the acoustic-image localization relation table T<b>1</b> includes the terminal position information, the generation unit <b>63</b> acquires the original position, the change information, and the audio content information associated with the area information including the terminal position information in the acoustic-image localization relation table T<b>1</b>. The generation unit <b>63</b> outputs the acquired audio content information to the control unit <b>65</b>.</p><p id="p-0101" num="0100">The generation unit <b>63</b> determines changed acoustic-image localization position based on the acquired original position and the change information, and holds the determined changed acoustic-image localized position as acoustic-image localization position information. Note that the generation unit <b>63</b> may set the changed acoustic-image localization position by using, in addition to the original position and the change information, which are the terminal position information, position information of an actually-existing object(s) associated with the target area and an approach angle to the target area. The approach angle may preferably be an approach angle or the like of the terminal to the center (the central position) of the target area. Specifically, the generation unit <b>63</b> may change, based on the acquired original position and the change information, and the approach angle to the target area, the acoustic-image localization position only when the approach angle of the user to the target area specified by the terminal position information is within a predetermined angle range. On the other hand, when the approach angle to the target area is not within the predetermined angle range, the generation unit <b>63</b> may not change the acoustic-image localization position.</p><p id="p-0102" num="0101">The generation unit <b>63</b> stores the determined changed acoustic-image localization position in the storage unit <b>62</b>. When the object information associated with the original position to be updated is related to a plurality of pieces of information (a plurality of records) listed in the acoustic-image localization relation table T<b>1</b> as in the case of the character C, the generation unit <b>63</b> updates the original positions of all the pieces of information (all the records) of which the object information is the character C.</p><p id="p-0103" num="0102">The generation unit <b>63</b> generates acoustic-image localization information based on the acoustic-image localization position information and the terminal position information. Note that when a plurality of changes in the position and/or a plurality of changes in the height are set in a time-series manner in the change information set (i.e., recorded) in the acoustic-image localization relation table T<b>1</b>, the generation unit <b>63</b> generates a plurality of pieces of acoustic-image localization position information corresponding to respective times and respective acoustic-image localization positions.</p><p id="p-0104" num="0103">As described above, the terminal position information includes the terminal position information of each of the left and right units <b>40</b>L and <b>40</b>R. The generation unit <b>63</b> generates left-ear acoustic-image localization information for the left unit <b>40</b>L based on the terminal position information of the left unit <b>40</b>L and the changed acoustic-image localization position information. The generation unit <b>63</b> generates right-ear acoustic-image localization information based on the terminal position information of the right unit <b>40</b>R and the changed acoustic-image localization position information. The generation unit <b>63</b> outputs acoustic-image localization information including the left-ear acoustic-image localization information and the right-ear acoustic-image localization information to the output unit <b>64</b>.</p><p id="p-0105" num="0104">The output unit <b>64</b> corresponds to the output unit <b>5</b> in the first example embodiment. The output unit <b>64</b> outputs the acoustic-image localization information including the left-ear acoustic-image localization information and the right-ear acoustic-image localization information generated by the generation unit <b>63</b> to the control unit <b>65</b>.</p><p id="p-0106" num="0105">The control unit <b>65</b> acquires, from the storage unit <b>62</b>, an audio content corresponding to the audio content information output from the generation unit <b>63</b>. When the audio content information output from the generation unit <b>63</b> is an audio content to be generated by the control unit <b>65</b> (i.e., an audio content that should be generated by the control unit <b>65</b>) like the Generated Content <b>4</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the control unit <b>65</b> generates the audio content. For example, when the control unit <b>65</b> generates an audio content based on a voice signal of a user, the terminal information acquisition unit <b>61</b> receives a voice signal of the user who possesses the communication terminal <b>40</b>. Then, the control unit <b>65</b> performs voice recognition for this voice signal, performs a morphological analysis for information obtained by the voice recognition, and generates an audio content corresponding to the voice signal of the user.</p><p id="p-0107" num="0106">The control unit <b>65</b> performs an acoustic-image localizing process for the acquired or generated audio content based on the acoustic-image localization information generated by the generation unit <b>63</b>. In other words, the control unit <b>65</b> corrects the acquired or generated audio content based on the acoustic-image localization information. The control unit <b>65</b> generates a left-ear audio content by correcting the audio content based on the left-ear acoustic-image localization information. The control unit <b>65</b> generates a right-ear audio content by correcting the audio content based on the right-ear acoustic-image localization information.</p><p id="p-0108" num="0107">The control unit <b>65</b> also functions as a communication unit, so that it transmits the left-ear and right-ear audio contents to the left and right units <b>40</b>L and <b>40</b>R, respectively, of the communication terminal <b>40</b>. Every time the generation unit <b>63</b> generates acoustic-image localization information, the control unit <b>65</b> generates left-ear and right-ear audio contents based on the latest acoustic-image localization information and transmits the generated left-ear and right-ear audio contents to the left and right units <b>40</b>L and <b>40</b>R, respectively. Then, the control unit <b>65</b> controls the output unit(s) <b>42</b> of the left and right units <b>40</b>L and <b>40</b>R of the communication terminal <b>40</b> so as to output the left-ear and right-ear audio contents.</p><heading id="h-0017" level="2">&#x3c;Example of Operation of Server Apparatus&#x3e;</heading><p id="p-0109" num="0108">Next, an example of operations performed by the server apparatus <b>60</b> according to the second example embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart showing an example of operations performed by the server apparatus according to the second example embodiment. Every time the server apparatus <b>60</b> acquires terminal position information, it performs the example of the operations shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Note that the following description will be given on the assumption that: the storage unit <b>62</b> stores the acoustic-image localization relation table T<b>1</b> in advance; n target areas (n is a natural number) are set (i.e., listed) in the acoustic-image localization relation table T<b>1</b>; and in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a variable i represents an area number.</p><p id="p-0110" num="0109">The terminal information acquisition unit <b>61</b> acquires terminal position information (Step S<b>11</b>). The terminal information acquisition unit <b>61</b> acquires terminal position information by receiving it from the communication terminal <b>50</b>.</p><p id="p-0111" num="0110">The server apparatus <b>60</b> repeatedly performs steps S<b>12</b> to S<b>15</b> the number of times corresponding to the number of target areas.</p><p id="p-0112" num="0111">The generation unit <b>63</b> determines whether or not the terminal position information is included in the area i (Step S<b>12</b>). The generation unit <b>63</b> determines whether or not the terminal position information is included in the area information of the area i by comparing the terminal position information with area information set (i.e., recorded) in a row (a record) in which the target area is the area i in the acoustic-image localization relation table T<b>1</b>.</p><p id="p-0113" num="0112">When the terminal position information is included in the area information of the area i (Yes in Step S<b>12</b>), the generation unit <b>63</b> generates acoustic-image localization information (Step S<b>13</b>). The generation unit <b>63</b> acquires an original position (i.e., a position before the change) and change information in the row (the record) in which the target area is the area i in the acoustic-image localization relation table T<b>1</b>. The generation unit <b>63</b> determines changed acoustic-image localization position based on the acquired original position and the change information, and holds the determined changed acoustic-image localized position as acoustic-image localization position information. The generation unit <b>63</b> stores the acoustic-image localization position information in the storage unit <b>62</b> while associating it with the area i. The generation unit <b>63</b> generates acoustic-image localization information based on the acoustic-image localization position information and the terminal position information.</p><p id="p-0114" num="0113">The output unit <b>64</b> outputs the acoustic-image localization information to the control unit <b>65</b> (Step S<b>14</b>).</p><p id="p-0115" num="0114">The control unit <b>65</b> corrects the audio content and transmits the corrected audio content to the output unit <b>42</b> of the communication terminal <b>40</b> (Step S<b>15</b>).</p><p id="p-0116" num="0115">The control unit <b>65</b> acquires, from the storage unit <b>62</b>, an audio content corresponding to the audio content information set (i.e., recorded) in the row (the record) in which the target area is the area i in the acoustic-image localization relation table T<b>1</b>. When the audio content in the row (the record) in which the target area is the area i in the acoustic-image localization relation table T<b>1</b> is an audio content to be generated (i.e., an audio content that should be generated), the control unit <b>65</b> generates the audio content. The control unit <b>65</b> corrects the acquired or generated audio content based on the acoustic-image localization information, and transmits the corrected audio content to the communication terminal <b>40</b>.</p><p id="p-0117" num="0116">Since the communication terminal <b>40</b> includes the left and right units <b>40</b>L and <b>40</b>R, the control unit <b>65</b> generates left-ear and right-ear audio contents and transmits them to the left and right units <b>40</b>L and <b>40</b>R, respectively, of the communication terminal <b>40</b>. Every time the generation unit <b>63</b> generates acoustic-image localization information, the control unit <b>65</b> generates left-ear and right-ear audio contents based on the latest acoustic-image localization information and transmits them to the left and right units <b>40</b>L and <b>40</b>R, respectively.</p><p id="p-0118" num="0117">When the step S<b>15</b> is finished, the variable I, which indicates the area number, is incremented, and a loop process for the next area is performed.</p><p id="p-0119" num="0118">In the step S<b>12</b>, when the terminal position information is not included in the area information of the area i (No in Step S<b>12</b>), the server apparatus <b>60</b> does not perform the processes in the steps S<b>13</b> to S<b>15</b>, increments the variable i, and performs a loop process for the next area.</p><p id="p-0120" num="0119">As described above, the server apparatus <b>60</b> specifies an area in which the terminal information is included based on the acoustic-image localization relation table T<b>1</b>, and generates acoustic-image localization information based on the acoustic-image localization position information, which has been determined by using the change information associated with the area, and the terminal position information. The server apparatus <b>60</b> corrects the audio content associated with this area based on the acoustic-image localization information, and outputs the corrected audio content toward the left and right ears of the user. In this way, the server apparatus <b>60</b> provides the audio content to the user in such a manner that the user feels, for example, as if the character C shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> is moving. Therefore, according to the server apparatus <b>60</b> in accordance with the second example embodiment, it is possible to provide an experience close to one experienced in a real space to the user.</p><p id="p-0121" num="0120">Further, by setting a row in the acoustic-image localization relation table T<b>1</b> like the row for the Area <b>1</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the server apparatus <b>60</b> can provide, to the user, an audio content in such a manner that the user feels as if the shop Sh shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> virtually speaks to the user from the position at which the shop Sh is located. Therefore, according to the server apparatus <b>60</b> in accordance with the second example embodiment, it is possible provide a virtual experience that cannot be experienced in a real space to the user.</p><heading id="h-0018" level="1">Third Example Embodiment</heading><p id="p-0122" num="0121">Next, a third example embodiment will be described. The third example embodiment is a modified example of the second example embodiment. While the server apparatus <b>60</b> performs an acoustic-image localizing process for an audio content in the second example embodiment, a communication terminal performs an acoustic-image localizing process for an audio content in the third example embodiment. Note that the third example embodiment includes components/structures and operations similar to those in the second example embodiment, and therefore descriptions thereof will be omitted as appropriate.</p><heading id="h-0019" level="2">&#x3c;Example of Configuration of Information Processing System&#x3e;</heading><p id="p-0123" num="0122">An information processing system <b>200</b> according to the third example embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. <figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example of a configuration of the information processing system according to the third example embodiment. The information processing system <b>200</b> includes a communication terminal <b>40</b>, a communication terminal <b>70</b>, and a server apparatus <b>80</b>. The information processing system <b>200</b> has a configuration that is obtained by replacing the communication terminal <b>50</b> and the server apparatus <b>60</b> according to the second example embodiment with the communication terminal <b>70</b> and the server apparatus <b>80</b>, respectively. Examples of the configuration and operations of the communication terminal <b>40</b> are similar to those in the second example embodiment, and therefore descriptions thereof will be omitted as appropriate.</p><heading id="h-0020" level="2">&#x3c;Example of Configuration of Communication Terminal&#x3e;</heading><p id="p-0124" num="0123">Next, an example of a configuration of the communication terminal <b>70</b> will be described. The communication terminal <b>70</b> includes a terminal position information acquisition unit <b>51</b> and a control unit <b>71</b>. The communication terminal <b>70</b> has a configuration that is obtained by adding the control unit <b>71</b> to the configuration of the communication terminal <b>50</b> according to the second example embodiment. Since the configuration of the terminal position information acquisition unit <b>51</b> is similar to that in the second example embodiment, the description thereof will be omitted as appropriate. Note that although this example embodiment will be described on the assumption that the communication terminal <b>70</b> includes the control unit <b>71</b>, the communication terminal <b>40</b> may include the control unit <b>71</b> and the communication terminal <b>70</b> may not include the control unit <b>71</b>.</p><p id="p-0125" num="0124">The control unit <b>71</b> communicates with the communication terminal <b>40</b> and the server apparatus <b>80</b>. The control unit <b>71</b> receives an audio content and acoustic-image localization information from an output unit <b>81</b> of the server apparatus <b>80</b>. The control unit <b>71</b> performs an acoustic-image localizing process for the audio content based on the acoustic-image localization information. In other words, the control unit <b>71</b> corrects the audio content based on the acoustic-image localization information.</p><p id="p-0126" num="0125">Similarly to the second example embodiment, the acoustic-image localization information includes left-ear acoustic-image localization information and right-ear acoustic-image localization information. The control unit <b>71</b> generates a left-ear audio content by correcting the audio content based on the left-ear acoustic-image localization information. The control unit <b>71</b> generates a right-ear audio content by correcting the audio content based on the right-ear acoustic-image localization information.</p><p id="p-0127" num="0126">The control unit <b>71</b> transmits the left-ear and right-ear audio contents to the left and right units <b>40</b>L and <b>40</b>R, respectively, of the communication terminal <b>40</b>. Every time the output unit <b>81</b> generates acoustic-image localization information, the control unit <b>71</b> generates left-ear and right-ear audio contents based on the latest acoustic-image localization information and transmits them to the left and right units <b>40</b>L and <b>40</b>R, respectively. The control unit <b>71</b> controls the output unit(s) <b>42</b> of the left and right units <b>40</b>L and <b>40</b>R of the communication terminal <b>40</b> so as to output the left-ear and right-ear audio contents.</p><heading id="h-0021" level="2">&#x3c;Example of Configuration of Server Apparatus&#x3e;</heading><p id="p-0128" num="0127">Next, an example of a configuration of the server apparatus <b>80</b> will be described. The server apparatus <b>80</b> includes a terminal information acquisition unit <b>61</b>, a storage unit <b>62</b>, a generation unit <b>63</b>, and an output unit <b>81</b>. The server apparatus <b>80</b> has a configuration that is obtained by removing the control unit <b>65</b>, and replacing the output unit <b>64</b> with the output unit <b>81</b> in the configuration according to the second example embodiment. Since the configurations of the terminal information acquisition unit <b>61</b>, the storage unit <b>62</b>, and the generation unit <b>63</b> are similar to those in the second example embodiment, descriptions thereof will be omitted as appropriate.</p><p id="p-0129" num="0128">The generation unit <b>63</b> acquires audio content information associated with area information including terminal position information in the acoustic-image localization relation table T<b>1</b>, and outputs the acquired audio content information to the output unit <b>81</b>.</p><p id="p-0130" num="0129">The output unit <b>81</b> also functions as a communication unit, so that it transmits (outputs), to the control unit <b>71</b>, acoustic-image localization information including left-ear acoustic-image localization information and right-ear acoustic-image localization information generated by the generation unit <b>63</b>. Every time the generation unit <b>63</b> generates acoustic-image localization information, the output unit <b>81</b> transmits the acoustic-image localization information to the control unit <b>71</b>. The output unit <b>81</b> controls the control unit <b>71</b> so as to perform an acoustic-image localizing process by using the latest acoustic-image localization information.</p><p id="p-0131" num="0130">The output unit <b>81</b> acquires, from the storage unit <b>62</b>, an audio content corresponding to the audio content information output from the generation unit <b>63</b>. When the audio content information output from the generation unit <b>63</b> is a content to be generated (i.e., an audio content that should be generated) like the Generated Content <b>4</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the output unit <b>81</b> generates the audio content. The output unit <b>81</b> transmits the acquired or generated audio content to the control unit <b>71</b>. Note that when the last audio content information transmitted to the control unit <b>71</b> and the audio content information output from the generation unit <b>63</b> are the same as each other, the output unit <b>81</b> does not have to transmit the audio content corresponding to the audio content information to the control unit <b>71</b>.</p><heading id="h-0022" level="2">&#x3c;Example of Operation of Information Processing System&#x3e;</heading><p id="p-0132" num="0131">Next, an example of operations performed by the information processing system <b>200</b> according to the third example embodiment will be described. The example of the operations performed by the information processing system <b>200</b> is basically similar to that shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, and therefore it will be described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0133" num="0132">Operations in steps S<b>11</b> to S<b>13</b> are similar to those in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, and therefore the descriptions thereof will be omitted.</p><p id="p-0134" num="0133">The output unit <b>81</b> outputs (transmits) acoustic-image localization information to the control unit <b>71</b> (Step S<b>14</b>). The output unit <b>81</b> transmits acoustic-image localization information generated by the generation unit <b>63</b> to the control unit <b>71</b>. The output unit <b>81</b> acquires, from the storage unit <b>62</b>, an audio content corresponding to audio content information in the row (the record) in which the target area is the area i in the acoustic-image localization relation table T<b>1</b>. When the audio content information in the row (the record) in which the target area is the area i in the acoustic-image localization relation table T<b>1</b> is an audio content to be generated (i.e., an audio content that should be generated), the output unit <b>81</b> generates the audio content. The output unit <b>81</b> transmits the acquired or generated audio content to the control unit <b>71</b>.</p><p id="p-0135" num="0134">The control unit <b>71</b> corrects the audio content and transmits the corrected audio content to the output unit <b>42</b> of the communication terminal <b>40</b> (Step S<b>15</b>). The control unit <b>71</b> receives the audio content and the acoustic-image localization information from the output unit <b>81</b>. The control unit <b>71</b> corrects the audio content based on the acoustic-image localization information, and transmits the corrected audio content to the communication terminal <b>40</b>.</p><p id="p-0136" num="0135">The control unit <b>71</b> generates a left-ear audio content and a right-ear audio content, and transmits them to the left and right units <b>40</b>L and <b>40</b>R, respectively, of the communication terminal <b>40</b>. Every time the control unit <b>71</b> receives acoustic-image localization information from the output unit <b>81</b>, the control unit <b>71</b> generates left-ear and right-ear audio contents based on the latest acoustic-image localization information and transmits them to the left and right units <b>40</b>L and <b>40</b>R, respectively.</p><p id="p-0137" num="0136">As described above, even when the configuration according to the second example embodiment is modified to the one according to the third example embodiment, effects similar to those in the second example embodiment can be obtained.</p><p id="p-0138" num="0137">The third example embodiment provides a configuration in which the communication terminal <b>70</b> performs the acoustic-image localizing process for the audio content. If the server apparatus <b>80</b> performs the acoustic-image localizing process for audio contents to be output to all communication terminals as in the second example embodiment, the processing load on the server apparatus <b>80</b> increases as the number of communication terminals increases. Therefore, it is necessary to increase the number of server apparatuses according to the number of communication terminals. In contrast to this, in the third example embodiment, since the server apparatus <b>80</b> does not perform the acoustic-image localizing process for the audio content and the communication terminal <b>70</b> instead performs the acoustic-image localizing process, so that it is possible to reduce the processing load on the server apparatus <b>80</b>, and thereby to reduce the cost for the facility/equipment that would otherwise be required to increase the number of servers.</p><p id="p-0139" num="0138">Further, it is possible to reduce the network load by adopting the configuration according to the third example embodiment. Specifically, as in the case of the Areas <b>2</b> and <b>3</b> in the acoustic-image localization relation table T<b>1</b> shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, it is conceivable that when the same audio content is used in a plurality of areas and the communication terminal <b>70</b> moves between the Areas <b>2</b> and <b>3</b>, only the acoustic-image localization position is changed. Further, when time-series changes are set in change information in the acoustic-image localization relation table T<b>1</b>, only the acoustic-image localization position changes in a time-series manner while the audio content is unchanged. In this case, in the second example embodiment, even when only the acoustic-image localization information is updated, the server apparatus <b>60</b> has to transmit an audio content for which an acoustic-image localizing process has been performed to the communication terminal <b>40</b>. In contrast to this, in the third example embodiment, the server apparatus <b>80</b> needs to transmit only the acoustic-image localization information. Therefore, it is possible to reduce the network load by adopting the configuration according to the third example embodiment.</p><heading id="h-0023" level="1">Fourth Example Embodiment</heading><p id="p-0140" num="0139">Next, a fourth example embodiment will be described. The fourth example embodiment is an improved example of the example embodiment 2 or 3. Therefore, differences of the fourth example embodiment from the third example embodiment will be described with reference to the third example embodiment. The configuration of the fourth example embodiment is obtained by replacing the server apparatus <b>80</b> by a server apparatus <b>90</b> in the configuration according to the third example embodiment. Note that examples of the configurations of the information processing system and the communication terminals <b>40</b> and <b>70</b> according to the fourth example embodiment are basically similar to those in the third example embodiment. Therefore, descriptions of the examples of the configurations of the information processing system and the communication terminals <b>40</b> and <b>70</b> will be omitted as appropriate.</p><heading id="h-0024" level="2">&#x3c;Example of Configuration of Communication Terminal&#x3e;</heading><p id="p-0141" num="0140">Although the example of the configuration of the communication terminal <b>40</b> is basically similar to that in the third example embodiment, the direction information acquisition unit <b>41</b> also transmits the acquired direction information to the server apparatus <b>60</b>.</p><heading id="h-0025" level="2">&#x3c;Example of Configuration of Server Apparatus&#x3e;</heading><p id="p-0142" num="0141">An example of a configuration of the server apparatus <b>90</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. <figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an example of the configuration of the server apparatus according to the fourth example embodiment. The server apparatus <b>90</b> includes a terminal information acquisition unit <b>91</b>, a storage unit <b>62</b>, a generation unit <b>92</b>, and an output unit <b>81</b>. Since the storage unit <b>62</b> and the output unit <b>81</b> are similar to those in the third example embodiment, descriptions thereof will be omitted.</p><p id="p-0143" num="0142">The terminal information acquisition unit <b>91</b> receives direction information from the communication terminal <b>40</b>.</p><p id="p-0144" num="0143">In addition to the function in the third example embodiment, the generation unit <b>92</b> adjusts the acoustic-image localization position information.</p><p id="p-0145" num="0144">Specifically, the generation unit <b>92</b> determines, based on area information in the acoustic-image localization relation table T<b>1</b> and terminal position information, whether or not the terminal position information is included in the area information. When the terminal position information is included in the area information, the generation unit <b>92</b> determines changed acoustic-image localization position based on original position (i.e., a position before the change) and change information associated with the area information in the acoustic-image localization relation table T<b>1</b>, and holds the determined acoustic image localized position as acoustic-image localization position information. Note that the generation unit <b>92</b> stores acoustic-image localization position information in the storage unit <b>62</b> while associating the acoustic-image localization position information with the area information (the target area) in which terminal position information is included.</p><p id="p-0146" num="0145">The generation unit <b>92</b> determines whether or not the changed acoustic-image localization position (the acoustic-image localization position information) needs to be adjusted. When the generation unit <b>92</b> determines that the acoustic-image localization position information needs to be adjusted, it adjusts the acoustic-image localization position information. When the generation unit <b>92</b> has adjusted the changed acoustic-image localization position, it holds the adjusted acoustic-image localization position as acoustic-image localization position information. The generation unit <b>92</b> stores the acoustic-image localization position information in the storage unit <b>62</b> while associating it with the area information (the target area) in which the terminal position information is included. Then, the generation unit <b>92</b> generates acoustic-image localization information based on the acoustic-image localization position information and the terminal position information.</p><p id="p-0147" num="0146">The generation unit <b>92</b> may determine whether or not the acoustic-image localization position information needs to be adjusted according to the distance between the communication terminal position information and the acoustic-image localization position information, and adjust, when necessary, the acoustic-image localization position information. The generation unit <b>92</b> determines the changed acoustic-image localization position based on the change information set (i.e., recorded) in the acoustic-image localization relation table T<b>1</b>, and the change distance (i.e., the distance of the change) from the original acoustic image localized position (i.e., the acoustic image localized position before the change) may be set in the change information set in the acoustic-image localization relation table T<b>1</b>. Therefore, when the original acoustic image localized position is far from the terminal position information, there is a possibility that the generation unit <b>92</b> sets the changed acoustic-image localization position at a position far from the position of the communication terminal <b>40</b>. Therefore, when the distance between the terminal position information and the acoustic-image localization position information is equal to or longer than a predetermined value, the generation unit <b>92</b> may determine that the acoustic-image localization position information needs to be adjusted and adjust the acoustic-image localization position information so that the distance between the terminal position information and the acoustic-image localization position information becomes equal to the predetermined value.</p><p id="p-0148" num="0147">Further, when characteristic information of the target area is set in the area information set (i.e., recorded) in the acoustic-image localization relation table T<b>1</b>, the generation unit <b>92</b> may determine whether or not the acoustic-image localization position information needs to be adjusted based on the characteristic information of the target area including the terminal position information and the current time (i.e., the time at the present moment), and adjust, when necessary, the acoustic-image localization position information.</p><p id="p-0149" num="0148">When the current time is included in the time period included (i.e., specified) in the characteristic information of the target area including the terminal position information and the distance between the terminal position information and the acoustic-image localization position information is equal to or longer than the predetermined value, the generation unit <b>92</b> may determine that the acoustic-image localization position information needs to be adjusted. Then, the generation unit <b>92</b> may adjust the acoustic-image localization position information so that the changed acoustic image localized position gets closer to the communication terminal. Alternatively, when the current time is included in the time period included in the characteristic information and the distance between the terminal position information and the acoustic-image localization position information is shorter than a predetermined value, the generation unit <b>92</b> may determine that the acoustic-image localization position information needs to be adjusted. Then, the generation unit <b>92</b> may adjust the acoustic-image localization position information so that the changed acoustic-image localization position gets away from (i.e., recedes from) the communication terminal.</p><p id="p-0150" num="0149">In this way, for example, when the target area including the terminal position information is an area where night view is beautiful and the virtual character is a virtual boyfriend/girlfriend, the generation unit <b>92</b> can adjust the acoustic-image localization position information so that the boyfriend/girlfriend moves closer to the user, thus making it possible to provide an experience closer to one experienced in a real space to the user. Further, for example, when the target area including the terminal position information is crowded during the daytime, the generation unit <b>92</b> can adjust the acoustic-image localization position information so that the user feels that the virtual character speaks in a more easily-to-listen manner, thus making it possible to provide an experience closer to one experienced in a real space to the user. Alternatively, for example, when the target area including the terminal position information is quiet (i.e., is not crowded at all) during a certain time period, the generation unit <b>92</b> can adjust the acoustic-image localization position information so that the user feels that the virtual character has changed its standing position to a different position in consideration of the congestion state, thus making it possible to provide an experience closer to one experienced in a real space to the user.</p><p id="p-0151" num="0150">Further, the generation unit <b>92</b> may determine whether or not the acoustic-image localization position information needs to be adjusted based on altitude information included in the terminal position information, and adjust, when necessary, the acoustic-image localization position information. Specifically, when the height of the communication terminal with respect to the horizontal plane changes based on the altitude information, the generation unit <b>92</b> may determine that the acoustic-image localization position information needs to be adjusted and adjust the acoustic-image localization position information.</p><p id="p-0152" num="0151">For example, it is conceivable that the target area including the terminal position information is an area where there is a step. When the position of the communication terminal is raised with respect to the horizontal plane based on the altitude information, the generation unit <b>92</b> may adjust the acoustic-image localization position information so that the changed acoustic-image localization position becomes lower than the position of the communication terminal. Alternatively, when the position of the communication terminal is lowered with respect to the horizontal plane based on the altitude information, the generation unit <b>92</b> may adjust the acoustic-image localization position information so that the changed acoustic-image localization position becomes higher than the position of the communication terminal. In this way, for example, when the target area including the terminal position information is an area where there are a lot of steps, the generation unit <b>63</b> can adjust the acoustic-image localization position information according to the step, thus making it possible to provide an experience closer to one experienced in a real space to the user.</p><p id="p-0153" num="0152">Further, the generation unit <b>92</b> may determine whether or not the acoustic-image localization position information needs to be adjusted based on direction information, and adjust, when necessary, the acoustic-image localization position information. When the communication terminal <b>40</b> is moving away from (i.e., receding from) the target object based on at least one of the terminal position information and the direction information, the generation unit <b>92</b> may determine that the acoustic-image localization position information needs to be adjusted, and adjust the acoustic-image localization position information in the direction toward the target object with respect to the terminal position information. The target object is, for example, the building O shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In this way, it is possible, when the user who possesses the communication terminal <b>40</b> has passed the building O, to make the user aware of that he/she has already passed the building O. In this case, for example, the generation unit <b>92</b> may make an adjustment so that a special-purpose audio content such as &#x201c;Building O is this way&#x201d; is output.</p><p id="p-0154" num="0153">Further, the generation unit <b>92</b> may determine whether or not the acoustic-image localization position information needs to be adjusted based on the distance between the terminal position information and the position information of the target object (target object position information) and the direction from the communication terminal <b>40</b> toward the target object (a target object direction), and adjust, when necessary, the acoustic-image localization position information. When the distance between the terminal position information and the target object position information becomes equal to a predetermined distance, and the direction specified by the direction information is coincident with the target object direction with respect to the communication terminal <b>40</b>, the generation unit <b>92</b> may adjust the acoustic-image localization position information so that the changed acoustic-image localization position is not set in the target object direction. In other words, when the distance between the terminal position information and the target object position information becomes equal to the predetermined distance, and the direction indicated by the direction information is coincident with the target object direction with respect to the communication terminal <b>40</b>, the generation unit <b>92</b> may adjust the acoustic-image localization position information so that the changed acoustic-image localization position is set at a position different from the target object direction.</p><p id="p-0155" num="0154">Note that, in such a case, the generation unit <b>92</b> may be configured so as to detect a change in the direction information included in the terminal position information, and may adjust the acoustic-image localization position information so that the acoustic-image localization position information is not set in the target object direction at the timing at which the direction specified by the direction information becomes coincident with the target object direction. Alternatively, the generation unit <b>92</b> may adjust the acoustic-image localization position information so that the acoustic-image localization position information is not set in the target object direction at a timing at which the terminal position information does not change for a predetermined time.</p><p id="p-0156" num="0155">In this way, the generation unit <b>92</b> can make an adjustment so that when the user who possesses the communication terminal <b>40</b> is in a position where he/she can see the building O, no audio content is heard from the direction in which the user sees the building O. That is, the generation unit <b>92</b> can prevent the user from seeing the target object such as the building O with interest.</p><heading id="h-0026" level="2">&#x3c;Example of Operation of Information Processing System&#x3e;</heading><p id="p-0157" num="0156">Next, an example of operations performed by the information processing system according to the fourth example embodiment will be described with reference to <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>9</b></figref>. Firstly, the information processing system according to the fourth example embodiment performs the example of operations shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, but the details of the operation in the step S<b>13</b> are different from those of the operation in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The details of the operation in the step S<b>13</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> will be described with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart for explaining detailed operations in a process for generating acoustic-image localization information according to the fourth example embodiment. The operations shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> are performed by the generation unit <b>92</b>.</p><p id="p-0158" num="0157">In a step S<b>12</b>, when the generation unit <b>92</b> determines that the terminal position information is included in the area i (Yes in Step S<b>12</b>), the generation unit <b>92</b> acquires original position (i.e., a position before the change) and change information associated with the area i from the acoustic-image localization relation table T<b>1</b> (Step S<b>131</b>).</p><p id="p-0159" num="0158">The generation unit <b>92</b> determines changed acoustic-image localization position based on the acquired original position and the change information, and holds the determined changed acoustic-image localization position as acoustic-image localization position information (Step S<b>132</b>).</p><p id="p-0160" num="0159">The generation unit <b>92</b> determines whether or not the changed acoustic-image localization position needs to be adjusted (Step S<b>133</b>).</p><p id="p-0161" num="0160">The generation unit <b>92</b> may determine whether or not the acoustic-image localization position information needs to be adjusted according to the distance between the communication terminal position information and the acoustic-image localization position information. Alternatively, when characteristic information of the target area is set in the area information set (i.e., recorded) in the acoustic-image localization relation table T<b>1</b>, the generation unit <b>92</b> may determine whether or not the acoustic-image localization position information needs to be adjusted based on the characteristic information of the target area including the terminal position information and the current time (i.e., the time at the present moment). Alternatively, the generation unit <b>92</b> may determine whether or not the acoustic-image localization position information needs to be adjusted based on altitude information included in the terminal position information. Alternatively, the generation unit <b>92</b> may determine whether or not the acoustic-image localization position information needs to be adjusted or not based on direction information. Alternatively, the generation unit <b>92</b> may determine whether or not the acoustic-image localization position information needs to be adjusted based on the distance between the terminal position information and the target object position information, and the communication terminal <b>40</b> and the target object direction, and adjust, when necessary, the acoustic-image localization position information.</p><p id="p-0162" num="0161">When the generation unit <b>92</b> determines that the changed acoustic-image localization position needs to be adjusted (Yes in Step S<b>133</b>), it adjusts the changed acoustic-image localization position and holds the adjusted acoustic-image localization position as adjusted acoustic-image localization position information (Step S<b>134</b>).</p><p id="p-0163" num="0162">On the other hand, the generation unit <b>92</b> determines that the changed acoustic-image localization position does not need to be changed (No in Step S<b>133</b>), the generation unit <b>92</b> skips the step S<b>134</b> and performs a step S<b>135</b>.</p><p id="p-0164" num="0163">The generation unit <b>92</b> generates acoustic-image localization information based on the acoustic-image localization position information and the terminal position information (Step S<b>135</b>). When the generation unit <b>92</b> generates the acoustic-image localization information, it outputs the generated acoustic-image localization information to the output unit <b>81</b>.</p><p id="p-0165" num="0164">As described above, in addition to determining the changed acoustic-image localization position based on the original position and the change information set in the acoustic-image localization relation table T<b>1</b>, the generation unit <b>92</b> adjusts the determined changed acoustic-image localization position. As described above, since the generation unit <b>92</b> adjusts the changed acoustic-image localization position, it is possible to provide, to the user, an experience that is closer to one experienced in a real space than that provided in the third example embodiment.</p><p id="p-0166" num="0165">Further, as described above, the generation unit <b>92</b> adjusts the changed acoustic-image position so as to make the user aware that he/she has passed the target object such as the building O, and/or adjusts the changed acoustic-image position so as to prevent the user from seeing the target object such as the building O with interest. Therefore, according to the server apparatus <b>90</b> in accordance with the fourth example embodiment, it is possible to provide, to the user, an experience that is closer to one experienced in a real space than that provided in the third example embodiment, and thereby to provide a user-friendly service to the user.</p><heading id="h-0027" level="1">MODIFIED EXAMPLE</heading><p id="p-0167" num="0166">The generation unit <b>92</b> according to the fourth example embodiment may be modified so as to adjust the acoustic-image localization position information based on relevant information related to the audio content output for the user who possesses the communication terminal <b>40</b>.</p><p id="p-0168" num="0167">The generation unit <b>92</b> may adjust the acoustic-image localization position information according to the length of the audio content. In the case where the audio content is an audio content to be generated (i.e., an audio content that should be generated), it is conceivable that the length of the audio content may be long or may be short. Therefore, the generation unit <b>92</b> acquires the generated audio content and checks the length of the audio content. Then, when the length of the audio content is longer than a predetermined time, the generation unit <b>92</b> may adjust the acoustic-image localization position information so that it gets closer to the terminal position information. On the other hand, when the length of the audio content is shorter than a predetermined time, the generation unit <b>92</b> may adjust the acoustic-image localization position information so that it gets away from (recedes from) the terminal position information. In this way, when the audio content is long, the generation unit <b>92</b> can make an adjustment so that the user listens to the audio content more carefully.</p><p id="p-0169" num="0168">Further, when the audio content output for the user is an audio content related to a virtual character, the generation unit <b>92</b> may adjust the acoustic-image localization position information according to the degree of intimacy (or closeness) between the user who possesses the communication terminal <b>40</b> and this virtual character. The degree of intimacy may be set according to the length of time for which the user has used the aforementioned character. The generation unit <b>92</b> sets the degree of intimacy so that the degree of intimacy increases as the time for which the user has used the aforementioned character increases. Further, the generation unit <b>92</b> may adjust the acoustic-image localization position information in such a manner that the higher the degree of intimacy is, the closer the acoustic-image localization position gets (i.e., moves) to the position of the communication terminal <b>40</b>.</p><heading id="h-0028" level="1">Fifth Example Embodiment</heading><p id="p-0170" num="0169">Next, a fifth example embodiment will be described. The fifth example embodiment is an improved example of any of the above-described second to fourth example embodiments. While the server apparatus is configured to output only an audio content for a user in the second to fourth example embodiments, a server apparatus according to this example embodiment also outputs display information to a user. In the following description, differences from the third example embodiment will be described with reference to the third example embodiment.</p><heading id="h-0029" level="2">&#x3c;Example of Configuration of Information Processing System&#x3e;</heading><p id="p-0171" num="0170">An example of a configuration of an information processing system <b>300</b> according to the fifth example embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. <figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example of the configuration of the information processing system according to the fifth example embodiment. The information processing system <b>300</b> has a configuration that is obtained by replacing the communication terminal <b>70</b> and the server apparatus <b>80</b> with a communication terminal <b>110</b> and a server apparatus <b>120</b>, respectively, in the third example embodiment. Note that since the example of the configuration of the communication terminal <b>40</b> is similar to that in the third example embodiment, the description thereof will be omitted as appropriate.</p><heading id="h-0030" level="2">&#x3c;Example of Configuration of Communication Terminal&#x3e;</heading><p id="p-0172" num="0171">Next, an example of a configuration of the communication terminal <b>110</b> will be described. The communication terminal <b>110</b> has a configuration that is obtained by adding an image pickup unit <b>112</b> and a display unit <b>113</b>, and replacing the terminal position information acquisition unit <b>51</b> with a terminal position information acquisition unit <b>111</b> in the configuration of the communication terminal <b>70</b> according to the third example embodiment. Note that since the configuration of the control unit <b>71</b> is similar to that in the third example embodiment, the description thereof will be omitted as appropriate.</p><p id="p-0173" num="0172">In addition to the function of the terminal position information acquisition unit <b>51</b> according to the third example embodiment, the terminal position information acquisition unit <b>111</b> acquires direction information of the communication terminal <b>110</b>. The terminal position information acquisition unit <b>111</b> includes, for example, a 9-axis sensor as in the case of the communication terminal <b>40</b>, and acquires direction information of the communication terminal <b>110</b>. The terminal position information acquisition unit <b>111</b> also transmits the direction information of the communication terminal <b>110</b> to the server apparatus <b>120</b>. Further, the terminal position information acquisition unit <b>111</b> also includes the terminal position information of the communication terminal <b>110</b> in the terminal position information, and transmits the terminal position information to the server apparatus <b>120</b>. Note that since the direction information of the communication terminal <b>110</b> coincides with the shooting direction (i.e., the photographing direction) of the image pickup unit <b>112</b>, the direction information of the communication terminal <b>110</b> is also referred to as shooting direction information.</p><p id="p-0174" num="0173">The image pickup unit <b>112</b> includes, for example, a camera or the like. The image pickup unit <b>112</b> shoots (i.e., photographs or films) a predetermined range and generates a photograph image thereof. The image pickup unit <b>112</b> outputs the generated photograph image to the display unit <b>113</b>. The image pickup unit <b>112</b> transmits the generated photograph image to a terminal information acquisition unit <b>121</b> of the server apparatus <b>120</b> through the terminal position information acquisition unit <b>111</b> or the control unit <b>71</b>. Note that the photograph image may be a still image or a moving image.</p><p id="p-0175" num="0174">The display unit <b>113</b> includes, for example, a display or the like. The display unit <b>113</b> displays the photograph image taken by the image pickup unit <b>112</b> on the display. Further, the display unit <b>113</b> receives display information generated by the server apparatus <b>120</b> and displays the received display information on the display. In the display information, coordinates with respect to reference coordinates defined in the photograph image are associated with video AR (Augmented Reality) information. The display unit <b>113</b> superimposes the video AR information at the aforementioned coordinates on the photograph image generated by the image pickup unit <b>112</b>, and displays them on the display.</p><heading id="h-0031" level="2">&#x3c;Example of Configuration of Server Apparatus&#x3e;</heading><p id="p-0176" num="0175">Next, an example of a configuration of the server apparatus <b>120</b> will be described. The server apparatus <b>120</b> includes a terminal information acquisition unit <b>121</b>, a storage unit <b>122</b>, a generation unit <b>123</b>, and an output unit <b>124</b>. The terminal information acquisition unit <b>121</b>, the storage unit <b>122</b>, the generation unit <b>123</b>, and the output unit <b>124</b> have configurations corresponding to those of the terminal information acquisition unit <b>61</b>, the storage unit <b>62</b>, the generation unit <b>63</b>, and the output unit <b>81</b>, respectively, according to the third example embodiment.</p><p id="p-0177" num="0176">The terminal information acquisition unit <b>121</b> has a configuration corresponding to that of the terminal information acquisition unit <b>61</b> in the third example embodiment, and further acquires the photograph image and the shooting direction information from the communication terminal <b>110</b>. The terminal information acquisition unit <b>121</b> outputs the acquired photograph image and the shooting direction information to the generation unit <b>123</b>.</p><p id="p-0178" num="0177">The storage unit <b>122</b> stores an acoustic-image localization relation table T<b>2</b>. The acoustic-image localization relation table T<b>2</b> is a table corresponding to the acoustic-image localization relation table T<b>1</b> according to the third example embodiment.</p><p id="p-0179" num="0178">An example of the acoustic-image localization relation table T<b>2</b> will be described hereinafter with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref>. <figref idref="DRAWINGS">FIG. <b>11</b></figref> shows an example of the acoustic-image localization relation table according to the fifth example embodiment. As shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the acoustic-image localization relation table T<b>2</b> is a table that is obtained by adding a video AR information column to the acoustic-image localization relation table T<b>1</b> according to the third example embodiment. In the video AR information column, pieces of video AR information associated with target areas are set.</p><p id="p-0180" num="0179">The generation unit <b>123</b> will be described by referring <figref idref="DRAWINGS">FIG. <b>10</b></figref> again. The generation unit <b>123</b> has a configuration that is obtained by adding a function of generating display information to the function of the generation unit <b>63</b> according to the third example embodiment. When there is a target area including terminal position information, the generation unit <b>123</b> determines whether or not a changed acoustic image localized position determined based on changed position and change information associated with this target area is included in the photograph image. When there is a target area including the terminal position information, the generation unit <b>123</b> determines, based on the terminal position information and the shooting direction information of the communication terminal <b>110</b>, whether or not the changed acoustic image localized position determined based on the changed position and the change information associated with this target area is included in the photograph image.</p><p id="p-0181" num="0180">When the changed acoustic-image localization position is included in the photograph image, the generation unit <b>123</b> acquires video AR information set (i.e., recorded) in the acoustic-image localization relation table T<b>2</b>. The generation unit <b>123</b> defines reference coordinates on the photograph image, and determines coordinates corresponding to the changed acoustic-image localization position with respect to the reference coordinates. The generation unit <b>123</b> generates the display information by associating the determined coordinates with the acquired video AR information. The generation unit <b>123</b> outputs the display information to the output unit <b>124</b>.</p><p id="p-0182" num="0181">The output unit <b>124</b> has a configuration that is obtained by adding a function of outputting display information to the function of the generation unit <b>63</b> according to the third example embodiment. The output unit <b>124</b> outputs (transmits) the display information generated by the generation unit <b>123</b> to the control unit <b>71</b>.</p><heading id="h-0032" level="2">&#x3c;Example of Operation of Information Processing System&#x3e;</heading><p id="p-0183" num="0182">Next, an example of operations performed by the information processing system <b>300</b> according to the fifth example embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart showing an example of operations performed by the information processing system according to the fifth example embodiment. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart that is obtained by adding steps S<b>31</b> to S<b>34</b> in the example of operations performed by the information processing system according to the third example embodiment. Steps S<b>11</b> to S<b>15</b> are similar to those in the third example embodiment, and therefore the descriptions thereof will be omitted as appropriate in the following description.</p><p id="p-0184" num="0183">In a step S<b>31</b>, the terminal information acquisition unit <b>121</b> acquires a photograph image generated by the image pickup unit <b>112</b> and shooting direction information from the communication terminal <b>110</b> (Step S<b>31</b>). The terminal information acquisition unit <b>121</b> outputs the photograph image generated by the image pickup unit <b>112</b> and the shooting direction information to the generation unit <b>123</b>.</p><p id="p-0185" num="0184">In a step S<b>32</b>, the generation unit <b>123</b> determines whether or not changed acoustic-image localization position is included in the photograph image (Step S<b>32</b>). In a step S<b>13</b>, the generation unit <b>123</b> determines the changed acoustic-image localization position based on the original position (i.e., the position before the change) and the change information set (i.e., recorded) in the record in which the target area is the area i in the acoustic-image localization relation table T<b>2</b>. The generation unit <b>123</b> determines, by using the terminal position information of the communication terminal <b>110</b>, the photograph image, the shooting direction information, and the changed acoustic-image localization position, whether or not the changed acoustic-image localization position is included in the photograph image.</p><p id="p-0186" num="0185">When the changed acoustic-image localization position is included in the photograph image (Yes in Step S<b>32</b>), the generation unit <b>123</b> generates display information (Step S<b>33</b>). When the changed acoustic-image localization position is included in the photograph image, the generation unit <b>123</b> acquires video AR information set (i.e., recorded) in the record of the area i in the acoustic-image localization relation table T<b>2</b>. The generation unit <b>123</b> defines reference coordinates on the photograph image, and determines coordinates corresponding to the changed acoustic image localized position with respect to the reference coordinates. The generation unit <b>123</b> generates the display information by associating the determined coordinates with the video AR information. The generation unit <b>123</b> outputs the display information to the output unit <b>124</b>.</p><p id="p-0187" num="0186">The output unit <b>124</b> outputs (transmits) the display information generated by the generation unit <b>123</b> to the control unit <b>71</b> (Step S<b>34</b>), and increments the variable i.</p><p id="p-0188" num="0187">On the other hand, in the step S<b>32</b>, when the changed acoustic-image localization position is not included in the photograph image (No in Step S<b>32</b>), the steps S<b>33</b> and S<b>34</b> are not performed while the variable i is incremented.</p><p id="p-0189" num="0188">As described above, in this example embodiment, the server apparatus <b>120</b> generates not only the audio content but also the display information, and when the acoustic-image localization position is included in the photograph image, displays video AR information at the acoustic image localized position. Therefore, according to the information processing system <b>300</b> in accordance with the fifth example embodiment, in addition to the audio content, the video AR information is also displayed, so that it is possible provide, in addition to an experience close to one experienced in a real space, a new experience that cannot be experienced in a real space to the user.</p><heading id="h-0033" level="1">MODIFIED EXAMPLE</heading><p id="p-0190" num="0189">Although the server apparatus <b>120</b> determines whether or not the changed acoustic-image localization position is included in the photograph image based on the direction information of the communication terminal <b>110</b> in the above-described fifth example embodiment, the server apparatus <b>120</b> may determine whether or not the changed acoustic-image localization position is included in the photograph image by using an AR marker. In such a case, the AR marker is disposed at a position corresponding to the changed acoustic-image localization position. When an AR marker is included in a photograph image, the generation unit <b>123</b> determines that a changed acoustic-image localization position is included in the photograph image. Then, the generation unit <b>123</b> generates display information by associating video AR information associated with the aforementioned changed acoustic-image localization position with the coordinates in the photograph image corresponding to a predetermined position that is determined based on the position where the AR marker is disposed. The predetermined position may be determined in an arbitrary manner, e.g., may be coincident with the position where the AR marker is disposed or may be a position to which it is desired to attract user's attention.</p><p id="p-0191" num="0190">Further, the image pickup unit <b>112</b> of the communication terminal <b>110</b> may determine whether or not an AR marker is included in a photograph image. When the image pickup unit <b>112</b> determines that an AR marker is included in a photograph image, the display unit <b>113</b> may display video AR information recorded in the AR marker at a predetermined position on the display that is determined based on the position of the AR marker.</p><p id="p-0192" num="0191">Alternatively, when the image pickup unit <b>112</b> determines that an AR marker is included in a photograph image, it transmits the photograph image and the position of the AR marker on the photograph image to the terminal information acquisition unit <b>121</b>. Then, the generation unit <b>123</b> may generate display information by associating video AR information associated with the aforementioned changed acoustic-image localization position with the coordinates on the photograph image corresponding to the predetermined position that is determined based on the position where the AR marker is disposed.</p><heading id="h-0034" level="1">Other Example Embodiment</heading><p id="p-0193" num="0192">&#x3c;1&#x3e; Although the above-described example embodiments have been described on the assumption that the generation unit generates acoustic-image localization position information, the generation unit may instead perform a process for selecting corresponding acoustic-image localization position information from a plurality of pieces of acoustic-image localization position information held (i.e., stored) in advance. For example, when the moving direction of a user is limited, required acoustic-image localization position information is also limited. Therefore, a holding unit or a storage unit holds pieces of acoustic-image localization position information that will be possibly used in advance. Then, the generation unit performs a process for selecting corresponding acoustic-image localization position information based on terminal position information. In this way, it is possible to reduce the processing load on the information processing apparatus or the server apparatus.</p><p id="p-0194" num="0193">&#x3c;2&#x3e; Each of the information processing apparatus <b>1</b>, the communication terminals <b>40</b>, <b>50</b>, <b>70</b> and <b>110</b>, and the server apparatuses <b>60</b>, <b>80</b>, <b>90</b> and <b>120</b> (hereinafter referred to as the information processing apparatus <b>1</b> and the like) described in the above-described example embodiments may have a hardware configuration described below. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram for explaining a hardware configuration of an information processing apparatus or the like according to each example embodiment of the present disclosure.</p><p id="p-0195" num="0194">Referring to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the information processing apparatus <b>1</b> or the like includes a network interface <b>1201</b>, a processor <b>1202</b>, and a memory <b>1203</b>. The network interface <b>1201</b> is used to communicate with other communication apparatuses having communication functions. The network interface <b>1201</b> may include, for example, a network interface card (NIC) in conformity with communication modes including IEEE (Institute of Electrical and Electronics Engineers) 802.11 series and IEEE 802.3 series.</p><p id="p-0196" num="0195">The processor <b>1202</b> may load software (a computer program) from the memory <b>1203</b> and execute the loaded software, thereby performing the processes of the information processing apparatus <b>1</b> or the like described by using the flowchart in the above-described example embodiments. The processor <b>1202</b> may be, for example, a microprocessor, an MPU (Micro Processing Unit), or a CPU (Central Processing Unit). The processor <b>1202</b> may include a plurality of processors.</p><p id="p-0197" num="0196">The memory <b>1203</b> is composed of a combination of a volatile memory and a nonvolatile memory. The memory <b>1203</b> may include a storage located remotely from the processor <b>1202</b>. In such a case, the processor <b>1202</b> may access the memory <b>1203</b> through an I/O interface (not shown).</p><p id="p-0198" num="0197">In the example shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the memory <b>1203</b> is used to store a group of software modules. The processor <b>1202</b> can perform the processes of the information processing apparatus <b>1</b> or the like described in the above-described example embodiments by loading the group of software modules from the memory <b>1203</b> and execute the loaded software modules.</p><p id="p-0199" num="0198">As described above with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, each of the processors included in the information processing apparatus <b>1</b> and the like performs one or a plurality of programs including a set of instructions for causing a computer to perform the algorithm described above with reference to the drawings.</p><p id="p-0200" num="0199">In the above-described examples, the program may be stored in various types of non-transitory computer readable media and thereby supplied to the computer. The non-transitory computer readable media includes various types of tangible storage media. Examples of the non-transitory computer readable media include a magnetic recording medium (such as a flexible disk, a magnetic tape, and a hard disk drive) and a magneto-optic recording medium (such as a magneto-optic disk). Further, examples of the non-transitory computer readable media include CD-ROM (Read Only Memory), CD-R, and CD-R/W. Further, examples of the non-transitory computer readable media include a semiconductor memory. The semiconductor memory includes, for example, a mask ROM, a PROM (Programmable ROM), an EPROM (Erasable PROM), a flash ROM, and a RAM (Random Access Memory). These programs may be supplied to the computer by using various types of transitory computer readable media. Examples of the transitory computer readable media include an electrical signal, an optical signal, and an electromagnetic wave. The transitory computer readable media can be used to supply programs to the computer through a wired communication line (e.g., electric wires and optical fibers) or a wireless communication line.</p><p id="p-0201" num="0200">Although the present invention is explained above with reference to example embodiments, the present invention is not limited to the above-described example embodiments. Various modifications that can be understood by those skilled in the art can be made to the configuration and details of the present invention within the scope of the invention. Further, the present disclosure may also be implemented by combining any two or more of the example embodiments with one another as desired.</p><p id="p-0202" num="0201">Further, the whole or part of the example embodiments disclosed above can be described as, but not limited to, the following supplementary notes.</p><heading id="h-0035" level="2">(Supplementary Note 1)</heading><p id="p-0203" num="0202">An information processing apparatus comprising:</p><p id="p-0204" num="0203">an acquisition unit configured to acquire terminal position information of a communication terminal;</p><p id="p-0205" num="0204">a holding unit configured to hold a predetermined area and acoustic-image localization position information of an audio content to be output to the communication terminal while associating them with each other;</p><p id="p-0206" num="0205">a generation unit configured to generate acoustic-image localization information based on the acoustic-image localization position information and the terminal position information when the terminal position information is included in the predetermined area; and</p><p id="p-0207" num="0206">an output unit configured to output the acoustic-image localization information.</p><heading id="h-0036" level="2">(Supplementary Note 2)</heading><p id="p-0208" num="0207">The information processing apparatus described in Supplementary note 1, wherein the generation unit adjusts the acoustic-image localization position information according to a distance between the terminal position information and the acoustic-image localization position information.</p><heading id="h-0037" level="2">(Supplementary Note 3)</heading><p id="p-0209" num="0208">The information processing apparatus described in Supplementary note 1 or 2, wherein when the communication terminal is moving away from a target object based on the terminal position information and position information of the target object, the generation unit adjusts the acoustic-image localization position information in a direction toward the target object with respect to the terminal position information.</p><heading id="h-0038" level="2">(Supplementary Note 4)</heading><p id="p-0210" num="0209">The information processing apparatus described in any one of Supplementary notes 1 to 3, wherein</p><p id="p-0211" num="0210">the acquisition unit acquires direction information of the communication terminal, and</p><p id="p-0212" num="0211">the generation unit adjusts, when a distance between the terminal position information and position information of a target object becomes equal to a predetermined distance, and a direction indicated by the direction information is coincident with a target object direction indicating a direction of the target object with respect to the terminal position information, the acoustic-image localization position information so that the acoustic-image localization position information is not set in the target object direction.</p><heading id="h-0039" level="2">(Supplementary Note 5)</heading><p id="p-0213" num="0212">The information processing apparatus described in Supplementary note 4, wherein the generation unit adjusts the acoustic-image localization position information at a timing at which the direction information changes and the direction indicated by the direction information becomes coincident with the target object direction.</p><heading id="h-0040" level="2">(Supplementary Note 6)</heading><p id="p-0214" num="0213">The information processing apparatus described in Supplementary note 4 or 5, wherein the generation unit adjusts the acoustic-image localization position information at a timing at which the position information does not change for a predetermined time.</p><heading id="h-0041" level="2">(Supplementary Note 7)</heading><p id="p-0215" num="0214">The information processing apparatus described in any one of Supplementary notes 1 to 6, wherein</p><p id="p-0216" num="0215">the terminal position information includes altitude information, and</p><p id="p-0217" num="0216">the generation unit adjusts the acoustic-image localization position information according to a change in the altitude information.</p><heading id="h-0042" level="2">(Supplementary Note 8)</heading><p id="p-0218" num="0217">The information processing apparatus described in any one of Supplementary notes 1 to 7, wherein the generation unit adjusts the acoustic-image localization position information according to a length of the audio content.</p><heading id="h-0043" level="2">(Supplementary Note 9)</heading><p id="p-0219" num="0218">The information processing apparatus described in any one of Supplementary notes 1 to 8, wherein</p><p id="p-0220" num="0219">the audio content includes an audio content related to a virtual object, and</p><p id="p-0221" num="0220">when the audio content related to the virtual object is output to the communication terminal, the generation unit adjusts the acoustic-image localization position information according to a degree of intimacy between a user possessing the communication terminal and the virtual object.</p><heading id="h-0044" level="2">(Supplementary Note 10)</heading><p id="p-0222" num="0221">The information processing apparatus described in any one of Supplementary notes 1 to 9, wherein</p><p id="p-0223" num="0222">the acquisition unit acquires a photograph image taken by the communication terminal,</p><p id="p-0224" num="0223">when a position corresponding to the acoustic-image localization position information is included in the photograph image, the generation unit generates display information related to the audio content, and</p><p id="p-0225" num="0224">the output unit outputs the display information to the communication terminal.</p><heading id="h-0045" level="2">(Supplementary Note 11)</heading><p id="p-0226" num="0225">A control method comprising:</p><p id="p-0227" num="0226">acquiring terminal position information of a communication terminal;</p><p id="p-0228" num="0227">holding a predetermined area and acoustic-image localization position information of an audio content to be output to the communication terminal while associating them with each other;</p><p id="p-0229" num="0228">generating acoustic-image localization information based on the acoustic-image localization position information and the terminal position information when the terminal position information is included in the predetermined area; and</p><p id="p-0230" num="0229">outputting the acoustic-image localization information.</p><heading id="h-0046" level="2">(Supplementary Note 12)</heading><p id="p-0231" num="0230">A control program for causing a computer to perform:</p><p id="p-0232" num="0231">acquiring terminal position information of a communication terminal;</p><p id="p-0233" num="0232">holding a predetermined area and acoustic-image localization position information of an audio content to be output to the communication terminal while associating them with each other;</p><p id="p-0234" num="0233">generating acoustic-image localization information based on the acoustic-image localization position information and the terminal position information when the terminal position information is included in the predetermined area; and</p><p id="p-0235" num="0234">outputting the acoustic-image localization information.</p><p id="p-0236" num="0235">This application is based upon and claims the benefit of priority from Japanese patent application No. 2019-229636, filed on Dec. 19, 2019, the disclosure of which is incorporated herein in its entirety by reference.</p><heading id="h-0047" level="1">REFERENCE SIGNS LIST</heading><p id="p-0237" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0236"><b>1</b> INFORMATION PROCESSING APPARATUS</li>    <li id="ul0002-0002" num="0237"><b>2</b> ACQUISITION UNIT</li>    <li id="ul0002-0003" num="0238"><b>3</b> HOLDING UNIT</li>    <li id="ul0002-0004" num="0239"><b>4</b>, <b>63</b>, <b>92</b>, <b>123</b> GENERATION UNIT</li>    <li id="ul0002-0005" num="0240"><b>5</b>, <b>42</b>, <b>64</b>, <b>81</b>, <b>124</b> OUTPUT UNIT</li>    <li id="ul0002-0006" num="0241"><b>40</b>, <b>50</b>, <b>70</b>, <b>110</b> COMMUNICATION TERMINAL</li>    <li id="ul0002-0007" num="0242"><b>41</b> DIRECTION INFORMATION ACQUISITION UNIT</li>    <li id="ul0002-0008" num="0243"><b>51</b>, <b>111</b> TERMINAL POSITION INFORMATION ACQUISITION UNIT</li>    <li id="ul0002-0009" num="0244"><b>60</b>, <b>80</b>, <b>90</b>, <b>120</b> SERVER APPARATUS</li>    <li id="ul0002-0010" num="0245"><b>61</b>, <b>91</b>, <b>121</b> TERMINAL INFORMATION ACQUISITION UNIT</li>    <li id="ul0002-0011" num="0246"><b>62</b>, <b>122</b> STORAGE UNIT</li>    <li id="ul0002-0012" num="0247"><b>65</b>, <b>71</b> CONTROL UNIT</li>    <li id="ul0002-0013" num="0248"><b>100</b>, <b>200</b>, <b>300</b> INFORMATION PROCESSING SYSTEM</li>    <li id="ul0002-0014" num="0249"><b>112</b> IMAGE PICKUP UNIT</li>    <li id="ul0002-0015" num="0250"><b>113</b> DISPLAY UNIT</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing apparatus comprising:<claim-text>at least one memory storing instructions, and</claim-text><claim-text>at least one processor configured to execute the instructions to:</claim-text><claim-text>acquire terminal position information of a communication terminal;</claim-text><claim-text>hold a predetermined area and acoustic-image localization position information of an audio content to be output to the communication terminal while associating them with each other;</claim-text><claim-text>generate acoustic-image localization information based on the acoustic-image localization position information and the terminal position information when the terminal position information is included in the predetermined area; and</claim-text><claim-text>output the acoustic-image localization information.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one processor is further configured to execute the instructions to adjust the acoustic-image localization position information according to a distance between the terminal position information and the acoustic-image localization position information.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein when the communication terminal is moving away from a target object based on the terminal position information and position information of the target object, the at least one processor is further configured to execute the instructions to adjust the acoustic-image localization position information in a direction toward the target object with respect to the terminal position information.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the at least one processor is further configured to execute the instructions to:</claim-text><claim-text>acquire direction information of the communication terminal; and</claim-text><claim-text>adjust, when a distance between the terminal position information and position information of a target object becomes equal to a predetermined distance, and a direction indicated by the direction information is coincident with a target object direction indicating a direction of the target object with respect to the terminal position information, the acoustic-image localization position information so that the acoustic-image localization position information is not set in the target object direction.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processing apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the at least one processor is further configured to execute the instructions to adjust the acoustic-image localization position information at a timing at which the direction information changes and the direction indicated by the direction information becomes coincident with the target object direction.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processing apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the at least one processor is further configured to execute the instructions to adjust the acoustic-image localization position information at a timing at which the position information does not change for a predetermined time.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the terminal position information includes altitude information, and</claim-text><claim-text>the at least one processor is further configured to execute the instructions to adjust the acoustic-image localization position information according to a change in the altitude information.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one processor is further configured to execute the instructions to adjust the acoustic-image localization position information according to a length of the audio content.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the audio content includes an audio content related to a virtual object, and</claim-text><claim-text>when the audio content related to the virtual object is output to the communication terminal, the at least one processor is further configured to execute the instructions to adjust the acoustic-image localization position information according to a degree of intimacy between a user possessing the communication terminal and the virtual object.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the at least one processor is further configured to execute the instructions to:</claim-text><claim-text>acquire a photograph image taken by the communication terminal;</claim-text><claim-text>when a position corresponding to the acoustic-image localization position information is included in the photograph image, generate display information related to the audio content; and</claim-text><claim-text>output the display information to the communication terminal.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A control method comprising:<claim-text>acquiring terminal position information of a communication terminal;</claim-text><claim-text>holding a predetermined area and acoustic-image localization position information of an audio content to be output to the communication terminal while associating them with each other;</claim-text><claim-text>generating acoustic-image localization information based on the acoustic-image localization position information and the terminal position information when the terminal position information is included in the predetermined area; and</claim-text><claim-text>outputting the acoustic-image localization information.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A non-transitory computer readable medium storing a control program for causing a computer to perform:<claim-text>acquiring terminal position information of a communication terminal;</claim-text><claim-text>holding a predetermined area and acoustic-image localization position information of an audio content to be output to the communication terminal while associating them with each other;</claim-text><claim-text>generating acoustic-image localization information based on the acoustic-image localization position information and the terminal position information when the terminal position information is included in the predetermined area; and</claim-text><claim-text>outputting the acoustic-image localization information.</claim-text></claim-text></claim></claims></us-patent-application>