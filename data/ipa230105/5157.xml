<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005158A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005158</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17780974</doc-number><date>20201207</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="regional"><country>EP</country><doc-number>19214693.4</doc-number><date>20191210</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>168</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>37</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>168</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>37</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30056</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">MEDICAL IMAGE SEGMENTATION AND ATLAS IMAGE SELECTION</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>KONINKLIJKE PHILIPS N.V.</orgname><address><city>EINDHOVEN</city><country>NL</country></address></addressbook><residence><country>NL</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LENGA</last-name><first-name>MATTHIAS</first-name><address><city>MAINZ</city><country>DE</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>WISSEL</last-name><first-name>TOBIAS</first-name><address><city>LUBECK</city><country>DE</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>WEESE</last-name><first-name>ROLF JUERGEN</first-name><address><city>NORDERSTEDT</city><country>DE</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/EP2020/084860</doc-number><date>20201207</date></document-id><us-371c12-date><date>20220528</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Some embodiments are directed to a segmentation of medical images. For example, a medical image may be registering to multiple atlas images after which a segmentation function may be applied. Multiple segmentation may be fused into a final overall segmentation. The atlas images may be selected on the basis of high segmentation quality or low registration quality.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="192.62mm" wi="146.39mm" file="US20230005158A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="226.99mm" wi="151.21mm" file="US20230005158A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="48.68mm" wi="150.88mm" file="US20230005158A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="222.42mm" wi="151.05mm" file="US20230005158A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="212.26mm" wi="151.81mm" file="US20230005158A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="141.99mm" wi="102.02mm" file="US20230005158A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="188.55mm" wi="126.66mm" file="US20230005158A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="203.79mm" wi="70.36mm" file="US20230005158A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD OF THE INVENTION</heading><p id="p-0002" num="0001">The presently disclosed subject matter relates to a method for medical image segmentation, a method for selecting atlas images for use in a medical image segmentation method, a medical image segmentation system, an atlas images selecting system, and a computer readable medium.</p><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0003" num="0002">Accurate and robust medical image segmentation is still a challenging task.</p><p id="p-0004" num="0003">One approach to medical image segmentation are multi-atlas based image segmentation methods. These methods, can be constructed with a limited set of segmented reference images, known as atlases. Unfortunately, atlas based methods often lack the desired accuracy. Examples, of registration methods and application thereof to medical image segmentation are given in &#x201c;Deformable medical image registration: a survey&#x201d;, by A. Sotiras, et al., or in &#x201c;Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain&#x201d;, by B. B. Avants, et al., both articles being included herein by reference.</p><p id="p-0005" num="0004">Another approach to medical image segmentation are neural networks, e.g., convolutional neural networks. Examples of neural networks suitable for medical image segmentation are given in the article &#x201c;Comparison of deep learning-based techniques for organ segmentation in abdominal CT images&#x201d;, by Groza, et al., or in &#x201c;Foveal fully convolutional nets for multi-organ segmentation&#x201d;, by T. Brosch, and A. Saalbach, both articles being included herein by reference.</p><p id="p-0006" num="0005">A neural network based approach enables the efficient construction of accurate segmentation algorithms. Unfortunately, neural networks do not generalize properly to new datasets with slightly different properties. When presenting a new image to a trained neural network that lies even slightly outside of the images that it was trained on, the resulting segmentation may be fully incorrect.</p><p id="p-0007" num="0006">There is a desire for a medical image segmentation method that on the one hand allows for accurate segmentation but on the other hand generalizes better to unfamiliar images.</p><heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0008" num="0007">To address these and other issues medical segmentation methods are provided that use both a segmentation function and multiple atlas images. Furthermore, methods are provided for selecting atlas images for use in a medical image segmentation method. Furthermore, corresponding devices and software are provided.</p><p id="p-0009" num="0008">Machine learning-based segmentation algorithms are obtained that produce accurate and robust segmentations of medical images, yet generalize well to new datasets with slightly different properties. The improved generalization may also be used to reduce the amount of training data.</p><p id="p-0010" num="0009">By registering an image on an atlas image with which the segmentation function is familiar, e.g., on which the segmentation function performs well, it is likely that an accurate segmentation is obtained. It is avoided that the segmentation function is applied to images for which it will produce a failure-mode segmentation, e.g., showing a collapse of segmentation ability. By registering an image on multiple atlas images the probability that a majority of them will show a poor segmentation output is reduced, and the accuracy of the process is improved. It is avoided for example that accidental peculiarities of an atlas or input image will dominate the result. Combining a set of multiple predictors with a suitable voting scheme improves their predictive performance.</p><p id="p-0011" num="0010">An advantage of embodiments of atlas image selection, is that the atlas image selection may be performed after training of the segmentation function is complete. In fact, the training of the segmentation function may be independent of the atlas image selection. This makes it possible to enhance existing segmentation functions according to an embodiment.</p><p id="p-0012" num="0011">For example, in an embodiment, the segmentation function may be machine learning segmentation function. For example, training images may be used to train the segmentation function. For example, the segmentation function may be, e.g., a neural network, e.g., a convolutional neural network (CNN). Instead of a neural network, a machine learning segmentation function may be a decision forest. The machine learning segmentation function may be an ensemble of neural networks. For example, a final result may be composed from the results of the individual network responses in the ensemble. For example, the ensemble of neural networks may be trained with different parameters.</p><p id="p-0013" num="0012">A subset of n training images may be selected as atlas images. To segment a new image, it is registered to the selected n atlas images, and the segmentation function is applied to registered images to obtain n segmentations. For example, these may be pixel or voxel segmentations. After applying to each of the n segmentations the corresponding inverse transformation, the resulting segmentation may be fused, e.g., using majority voting to obtain a final segmentation.</p><p id="p-0014" num="0013">A method for segmentation and/or a method for selecting atlas images may be implemented on an electronic device, for example on a computer. The segmentation methods described herein may be applied in a wide range of practical applications, for example, in clinical workstations or web-/cloud-based clinical applications for diagnosis, quantification or treatment planning.</p><p id="p-0015" num="0014">A person skilled in the art will appreciate that the method may be applied to multi-dimensional image data, e.g., to two-dimensional (2D), three-dimensional (3D) or four-dimensional (4D) images, acquired by various acquisition modalities such as, but not limited to, standard X-ray Imaging, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Ultrasound (US), Positron Emission Tomography (PET), Single Photon Emission Computed Tomography (SPECT), and Nuclear Medicine (NM).</p><p id="p-0016" num="0015">An embodiment of the method may be implemented on a computer as a computer implemented method, or in dedicated hardware, or in a combination of both. Executable code for an embodiment of the method may be stored on a computer program product. Examples of computer program products include memory devices, optical storage devices, integrated circuits, servers, online software, etc. Preferably, the computer program product comprises non-transitory program code stored on a computer readable medium for performing an embodiment of the method when said program product is executed on a computer.</p><p id="p-0017" num="0016">In an embodiment, the computer program comprises computer program code adapted to perform all or part of the steps of an embodiment of the method when the computer program is run on a computer. Preferably, the computer program is embodied on a computer readable medium.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0018" num="0017">Further details, aspects, and embodiments will be described, by way of example, with reference to the drawings. Elements in the Figs. are illustrated for simplicity and clarity and have not necessarily been drawn to scale. In the Figs., elements which correspond to elements already described may have the same reference numerals. In the drawings,</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b><i>a </i></figref>schematically shows an example of an embodiment of a medical image segmentation system,</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b><i>b </i></figref>schematically shows an example of an embodiment of an atlas images selecting system,</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>2</b><i>a </i></figref>schematically shows an example of an embodiment of a segmentation function,</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>2</b><i>b </i></figref>schematically shows an example of an embodiment of an image segmentation,</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>2</b><i>c </i></figref>schematically shows an example of an embodiment of a segmentation quality determination,</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically shows an example of an embodiment of a method for medical image segmentation,</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. <b>4</b><i>a </i>and <b>4</b><i>b </i></figref>schematically illustrate an example of an embodiment of a method for selecting atlas images for use in a medical image segmentation method,</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>5</b><i>a </i></figref>schematically shows an example of an embodiment of a method for medical image segmentation,</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>5</b><i>b </i></figref>schematically shows an example of an embodiment of a method for selecting atlas images for use in a medical image segmentation method,</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIGS. <b>6</b><i>a </i>and <b>6</b><i>b </i></figref>schematically show an example of an embodiment of a medical image,</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>6</b><i>c </i></figref>schematically shows Dice scores for an example of an embodiment of a segmentation function operating correctly,</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>6</b><i>d </i></figref>schematically shows Dice scores for an example of an embodiment of a segmentation function showing a failure mode,</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>7</b><i>a </i></figref>schematically shows a computer readable medium having a writable part comprising a computer program according to an embodiment,</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>7</b><i>b </i></figref>schematically shows a representation of a processor system according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">LIST OF REFERENCE NUMERALS</heading><p id="p-0033" num="0032"><b>100</b> a machine learning system</p><p id="p-0034" num="0033"><b>110</b> a medical image segmentation system</p><p id="p-0035" num="0034"><b>112</b> a segmentation function storage</p><p id="p-0036" num="0035"><b>114</b> an atlas image storage</p><p id="p-0037" num="0036"><b>130</b> a processor system</p><p id="p-0038" num="0037"><b>140</b> a memory</p><p id="p-0039" num="0038"><b>150</b> a communication interface</p><p id="p-0040" num="0039"><b>160</b> an atlas images selecting system</p><p id="p-0041" num="0040"><b>164</b> a test image storage</p><p id="p-0042" num="0041"><b>170</b> a processor system</p><p id="p-0043" num="0042"><b>180</b> a memory</p><p id="p-0044" num="0043"><b>190</b> a communication interface</p><p id="p-0045" num="0044"><b>210</b> a segmentation function</p><p id="p-0046" num="0045"><b>212</b>, <b>213</b> a medical input image</p><p id="p-0047" num="0046"><b>214</b>, <b>215</b> an image segmentation</p><p id="p-0048" num="0047"><b>216</b> a true image segmentation</p><p id="p-0049" num="0048"><b>220</b> a comparator</p><p id="p-0050" num="0049"><b>217</b> a segmentation quality</p><p id="p-0051" num="0050"><b>320</b> multiple atlas images</p><p id="p-0052" num="0051"><b>321</b>-<b>323</b> an atlas image</p><p id="p-0053" num="0052"><b>312</b> a medical image</p><p id="p-0054" num="0053"><b>325</b> a registering function</p><p id="p-0055" num="0054"><b>330</b> multiple registered images</p><p id="p-0056" num="0055"><b>331</b>-<b>333</b> a registered image</p><p id="p-0057" num="0056"><b>340</b> multiple registered image segmentations,</p><p id="p-0058" num="0057"><b>341</b>-<b>343</b> a registered image segmentation</p><p id="p-0059" num="0058"><b>350</b> multiple deregistered image segmentations,</p><p id="p-0060" num="0059"><b>351</b>-<b>353</b> a deregistered image segmentation</p><p id="p-0061" num="0060"><b>361</b> an output segmentation</p><p id="p-0062" num="0061"><b>610</b> multiple test images</p><p id="p-0063" num="0062"><b>610</b>&#x2032; not yet selected test images</p><p id="p-0064" num="0063"><b>611</b>-<b>613</b> a test image</p><p id="p-0065" num="0064"><b>620</b> multiple test image segmentations</p><p id="p-0066" num="0065"><b>621</b>-<b>623</b> a test image segmentation</p><p id="p-0067" num="0066"><b>630</b> multiple segmentation qualities</p><p id="p-0068" num="0067"><b>631</b>-<b>633</b> a segmentation quality</p><p id="p-0069" num="0068"><b>651</b> an atlas image</p><p id="p-0070" num="0069"><b>640</b> multiple registration qualities</p><p id="p-0071" num="0070"><b>641</b>-<b>643</b> a registration quality</p><p id="p-0072" num="0071"><b>652</b>, <b>654</b> a Dice score for the spine</p><p id="p-0073" num="0072"><b>662</b>, <b>664</b> a Dice score for ribs</p><p id="p-0074" num="0073"><b>400</b> a method for medical image segmentation</p><p id="p-0075" num="0074"><b>410</b> obtaining a segmentation function, the segmentation function being configured to receive a medical input image and to produce an image segmentation,</p><p id="p-0076" num="0075"><b>420</b> obtaining multiple atlas images,</p><p id="p-0077" num="0076"><b>430</b> receiving a medical image,</p><p id="p-0078" num="0077"><b>440</b> registering the received image to the multiple atlas images thus obtaining multiple registered images and multiple corresponding registering transformations configured to register the received image on the multiple atlas images,</p><p id="p-0079" num="0078"><b>450</b> applying the segmentation function to the multiple registered images thus obtaining multiple registered image segmentations,</p><p id="p-0080" num="0079"><b>460</b> applying inverses of the multiple registering transformations to the multiple image segmentations thus obtaining multiple image segmentations,</p><p id="p-0081" num="0080"><b>470</b> determining an output segmentation from the multiple image segmentations.</p><p id="p-0082" num="0081"><b>500</b> a method for selecting atlas images for use in a medical image segmentation method,</p><p id="p-0083" num="0082"><b>510</b> obtaining a segmentation function configured to receive an input image and to produce an image segmentation,</p><p id="p-0084" num="0083"><b>520</b> obtaining multiple test images and corresponding test image segmentations,</p><p id="p-0085" num="0084"><b>530</b> determining a segmentation quality for the multiple test images by comparing the associated test image segmentation with an image segmentation generated by the segmentation function,</p><p id="p-0086" num="0085"><b>540</b> selecting one or more of the test images as atlas images having a segmentation quality over a threshold.</p><p id="p-0087" num="0086"><b>1000</b> a computer readable medium</p><p id="p-0088" num="0087"><b>1010</b> a writable part</p><p id="p-0089" num="0088"><b>1020</b> a computer program</p><p id="p-0090" num="0089"><b>1110</b> integrated circuit(s)</p><p id="p-0091" num="0090"><b>1120</b> a processing unit</p><p id="p-0092" num="0091"><b>1122</b> a memory</p><p id="p-0093" num="0092"><b>1124</b> a dedicated integrated circuit</p><p id="p-0094" num="0093"><b>1126</b> a communication element</p><p id="p-0095" num="0094"><b>1130</b> an interconnect</p><p id="p-0096" num="0095"><b>1140</b> a processor system</p><heading id="h-0006" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading><p id="p-0097" num="0096">While the presently disclosed subject matter is susceptible of embodiment in many different forms, there are shown in the drawings and will herein be described in detail one or more specific embodiments, with the understanding that the present disclosure is to be considered as exemplary of the principles of the presently disclosed subject matter and not intended to limit it to the specific embodiments shown and described.</p><p id="p-0098" num="0097">In the following, for the sake of understanding, elements of embodiments are described in operation. However, it will be apparent that the respective elements are arranged to perform the functions being described as performed by them.</p><p id="p-0099" num="0098">Further, the presently disclosed subject matter is not limited to the embodiments, as feature described herein or recited in mutually different dependent claims may be combined.</p><p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. <b>1</b><i>a </i></figref>schematically shows an example of an embodiment of a medical image segmentation system <b>110</b>. For example, image segmentation system <b>110</b> may be configured for an image segmentation method such as shown herein. <figref idref="DRAWINGS">FIG. <b>1</b><i>b </i></figref>schematically shows an example of an embodiment of an atlas images selecting system <b>160</b>. For example, system <b>160</b> may be configured for a method of selecting atlas images as shown herein. For example, the atlas images that are selected in system <b>160</b> may be used in system <b>110</b> for image segmentation.</p><p id="p-0101" num="0100">Medical image segmentation system <b>110</b> and atlas selection system <b>160</b> may be distinct systems. Medical image segmentation system <b>110</b> and atlas selection system <b>160</b> could be combined in a machine learning system <b>100</b>. System <b>100</b> may be configured for atlas selection and for image segmentation. The latter may be part of segmentation function training and/or of segmentation on new images.</p><p id="p-0102" num="0101">For example, medical image segmentation system <b>110</b> may be used in a production environment. For example, medical image segmentation system <b>110</b> may be used by medical professionals, or by operators of medical imaging systems, and the like, to produce segmentations of new images. The segmentation function in medical image segmentation system <b>110</b> and/or is typically a machine learning segmentation function, e.g., the segmentation function may be or may have been, trained on multiple training images. For convenience it will be assumed that the segmentation function is of machine learning type, but this is not strictly necessary. For example, a segmentation function which is hand-crafted, e.g., an expert system, or the like could also be improved according to an embodiment. A machine learning segmentation function may comprise a neural network, e.g., configured to receive an image as input and to produce a segmentation as output, but this is not necessary. For example, the segmentation function may comprise other machine learning functions, e.g., deep forests, or may comprise an ensemble of one or more machine learning functions. For example, atlas images selecting system <b>160</b> may be used when preparing the image segmentation system <b>110</b>. For example, system <b>160</b> may be used together with a system for training the segmentation function of system <b>110</b>. For example, system <b>160</b> may further be configured to train the parameters of the segmentation function that may be used in the image segmentation system <b>110</b>. The latter is optional though, e.g., segmentation function training may be performed by a different device. In fact, it is an advantage that atlas images can be selected after the training of the segmentation function is complete. Interestingly, a fully trained segmentation function may be obtained from any source, e.g., from some third party, after which atlas images may be selected for it. In this way, accuracy and robustness of any existing segmentation function for image segmentation may be increased later. This holds even if the use of atlas images was not intended or anticipated during the design or training of the segmentation function.</p><p id="p-0103" num="0102">Image segmentation system <b>110</b> may comprise a processor system <b>130</b>, a memory <b>140</b>, and a communication interface <b>150</b>. Image segmentation system <b>110</b> may be configured to communicate with a segmentation function storage <b>112</b> and an atlas image storage <b>114</b>.</p><p id="p-0104" num="0103">Segmentation function storage <b>112</b> may be configured to store a segmentation function, the segmentation function being configured to receive a medical input image and to produce an image segmentation. Atlas storage <b>114</b> may be configured to store multiple atlas images.</p><p id="p-0105" num="0104">Atlas image selection system <b>160</b> may comprise a processor system <b>170</b>, a memory <b>180</b>, and a communication interface <b>190</b>. System <b>160</b> may comprise a segmentation function storage <b>112</b>, e.g., as in system <b>110</b> and a test image storage <b>164</b>.</p><p id="p-0106" num="0105">Segmentation function storage <b>112</b> may be configured to store a segmentation function, the segmentation function being configured to receive a medical input image and to produce an image segmentation.</p><p id="p-0107" num="0106">Test image storage <b>164</b> may be configured to store multiple test images and corresponding test image segmentations.</p><p id="p-0108" num="0107">Storages <b>112</b>, <b>114</b> and <b>164</b> may be a local storage of system <b>110</b> or <b>160</b>, e.g., a local hard drive or memory. Storage <b>112</b>, <b>114</b> and <b>164</b> may be non-local storage, e.g., cloud storage. In the latter case, storage <b>112</b>, <b>114</b> and <b>164</b> may be implemented as a storage interface to the non-local storage.</p><p id="p-0109" num="0108">Systems <b>110</b> and/or <b>160</b> may communicate with each other, external storage, input devices, output devices, and/or one or more sensors over a computer network. The computer network may be an internet, an intranet, a LAN, a WLAN, etc. The computer network may be the Internet. The systems comprise a connection interface which is arranged to communicate within the system or outside of the system as needed. For example, the connection interface may comprise a connector, e.g., a wired connector, e.g., an Ethernet connector, an optical connector, etc., or a wireless connector, e.g., an antenna, e.g., a Wi-Fi, 4G or 5G antenna.</p><p id="p-0110" num="0109">The communication interfaces may be used to send or receive input images for segmentation, atlas images, segmentation function parameters, output images, etc.</p><p id="p-0111" num="0110">The execution of systems <b>110</b> and <b>160</b> may be implemented in a processor system, e.g., one or more processor circuits, e.g., microprocessors, examples of which are shown herein. The system <b>110</b> may be implemented in a single device which may or may not include the storage parts. The system <b>160</b> may be implemented in a single device which may or may not include the storage. Systems <b>110</b> and <b>160</b> may be implemented in a single system or device, etc. Systems <b>110</b> and/or <b>160</b> may comprise functional units which may configured for elements, e.g., steps, or parts, etc., of embodiments of image segmentation methods and/or atlas selection methods. The functional units may be wholly or partially implemented in computer instructions that are stored at system <b>110</b> and <b>160</b>, e.g., in an electronic memory of system <b>110</b> and <b>160</b>, and are executable by a microprocessor of system <b>110</b> and <b>160</b>. In hybrid embodiments, functional units are implemented partially in hardware, e.g., as coprocessors, e.g., segmentation function coprocessors, graphic coprocessors, etc., and partially in software stored and executed on system <b>110</b> and <b>160</b>. Parameters of the segmentation function and/or training data may be stored locally at system <b>110</b> and <b>160</b> or may be stored in cloud storage.</p><p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. <b>2</b><i>a </i></figref>schematically shows an example of an embodiment of a segmentation function <b>210</b> such as may be used in embodiments of image segmentation methods and/or in embodiments of atlas image selection methods. For example, segmentation function <b>210</b> may be configured to receive a medical input image <b>212</b> and to produce an image segmentation <b>214</b>. For example, the segmentation function may be configured to receive a 2d or 3d image I and to produce an annotation, e.g., a segmentation A. The segmentation may be a binary image. For example, the image may be an image of a lung, and the segmentation may be a binary image that indicates whether or not a pixel/voxel of the input image corresponds to a lung or not. Segmentation function <b>210</b> may be trained or may have been trained on a set of images I<sub>1</sub>, I<sub>2</sub>, . . . , I<sub>m </sub>with corresponding annotations A<sub>1</sub>, A<sub>2</sub>, . . . , A<sub>m</sub>. Training of a segmentation function may be done in a conventional manner. For example, a neural network may be trained, e.g., using backpropagation. Training may use an Adam optimizer.</p><p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. <b>2</b><i>b </i></figref>schematically shows an example of an embodiment of an image segmentation. Shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>b </i></figref>is a medical image <b>213</b>. Medical image <b>213</b> shows an object of interest, say, a lung, a kidney, a heart, etc., but also objects that are not of interest. In image <b>213</b> the object of interest is schematically indicated by a square. The segmentation <b>215</b> is in this case a binary image in which black pixels indicate the presence of the object of interest.</p><p id="p-0114" num="0113">Instead of a binary image, the segmentation may also have more than two different classes, e.g., to indicate more than one object of interest. For example, the segmentation may be a two or three-dimensional array that indicates for pixels or voxels in the input to what object they belong. The size of the segmentation along its dimensions may be the same as of the input image, but it may also be lower; for example, one output element may classify multiple input elements, e.g., pixels/voxels. The size of the segmentation along its dimensions may be also be higher, e.g., shape-based interpolation may be used to upsample results.</p><p id="p-0115" num="0114">In an embodiment, the input images may be an abdominal CT and the objects of interest may be liver, spleen, left and right kidney. The segmentation function may be a neural network, e.g., a deep convolutional neural networks (CNN), e.g., of U-type, or of F-type. A segmentation function may also be trained for other objects and image modalities, e.g., arteries, bones, etc., or MRI, x-ray, etc. The segmentation method may indicate common anatomical features of the medical image, e.g., anatomical features that are present for anatomically normal, e.g., average, persons. The segmentation method may also or instead indicate medical anomalies, e.g., a tumor, a bone fracture, etc.</p><p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically shows an example of an embodiment of a method for medical image segmentation. <figref idref="DRAWINGS">FIG. <b>5</b><i>a </i></figref>schematically shows an example of an embodiment of a method <b>400</b> for medical image segmentation. <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows data structures and data items that may be used in method <b>400</b>. On the other hand, the data structures and data items, etc. that are shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be manipulated using an embodiment of method <b>400</b>.</p><p id="p-0117" num="0116">Method <b>400</b> may comprise obtaining <b>410</b> a segmentation function <b>210</b>, the segmentation function being configured to receive a medical input image and to produce an image segmentation.</p><p id="p-0118" num="0117">For example, the segmentation function <b>210</b> may be a convolutional network, or a ResNet type architecture, or the like. Obtaining the segmentation function may comprise retrieving the segmentation function from a storage, or receiving the segmentation function from a computer network, etc. The segmentation function may be represented as a collection of segmentation function parameters.</p><p id="p-0119" num="0118">Typically, the segmentation function <b>210</b> is fully trained before atlas image selection, although this is not necessary. For example, after atlas selection the segmentation function may be further trained while input image registration is included. For example, this may be a fine-tuning. In fact, during training one may even perform multiple segmentation function applications to multiple registered input images, and fuse the resulting multiple segmentations, e.g., as in an embodiment, and compute an error signal from the final fused segmentation. The latter error signal may be used for further training, e.g., fine tuning. An advantage of this approach is that the segmentation function is optimized for use with the selected atlas images. This fine-tuning is however not needed; for example, one may use atlas selection as a way to boost a segmentation function's performance without having to further train it.</p><p id="p-0120" num="0119">Method <b>400</b> may comprise obtaining <b>420</b> multiple atlas images <b>320</b>. Shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> are atlas images <b>321</b>, <b>322</b> and <b>323</b>. There may be more or fewer atlas images. For example, there may be 4 or more, 8 or more, 16 or more atlas images. The atlas images may have been selected according to a method for atlas image selection. For example, the atlas images may have been selected for various positive properties. For example, the atlas images may segment well. For example, the atlas images may represent a good cross section of the image population.</p><p id="p-0121" num="0120">Method <b>400</b> may comprise receiving <b>430</b> a medical image <b>312</b>. For example, the medical image may be obtained from a medical imaging device, e.g., a CT device, MRI device, x-ray device, and the like. For example, the image may be represented as a 2d-array of pixels, or a 3d-array of voxels. The image may be compressed, or may be in a raw format, etc.</p><p id="p-0122" num="0121">Method <b>400</b> may comprise registering <b>440</b> the received image <b>312</b> to the multiple atlas images thus obtaining multiple registered images <b>330</b> and multiple corresponding registering transformations configured to register the received image on the multiple atlas images. <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows registered images <b>331</b>, <b>332</b> and <b>333</b>. For clarity, the corresponding transformations are not shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a registering function <b>325</b> configured to register the image <b>312</b> on the atlas images <b>320</b>.</p><p id="p-0123" num="0122">Image registration is a process in which a source image is transformed to better align with a target image. In case of function <b>325</b>, image <b>312</b> is the source image and the atlas images in turn function as target image. The registration function may be configured for selecting an allowable transformation from a defined class of transformations. For example, the class of transformations may be, e.g., translation registration, rigid registration, similarity registration, affine registration or non-rigid registration. A rigid transformation includes translation and rotation. A similarity transformation includes translation, rotation and scaling. The selection of the registration may be through optimizing a loss function.</p><p id="p-0124" num="0123">For registration function <b>325</b> an elastic registration may be used. For example, a diffeomorphism may be selected by registration function <b>325</b>. An elastic transformation allows a close alignment between source image and target image. Accordingly, it is expected that the segmentation qualities of the atlas images may be matched by the transformed images.</p><p id="p-0125" num="0124">After registration of the single input <b>312</b>, multiple registered images are obtained, e.g., one for each atlas image. When registering image <b>312</b> to an atlas image, the function <b>325</b> produces the registered image, but also the transformation that maps image <b>312</b> to the atlas image. For example, given an input image I, and atlas images I<sub>i </sub>the resulting transformations may be selected from the transformation class, so that T<sub>i</sub>(I) aligns with image I<sub>i </sub>at least to some extent.</p><p id="p-0126" num="0125">Method <b>400</b> may comprise applying <b>450</b> the segmentation function <b>210</b> to the multiple registered images <b>330</b> thus obtaining multiple registered image segmentations <b>340</b>. For example, a segmentation function NN may be applied as NN(T<sub>i</sub>(I)), wherein i runs over the number of atlas images. The output of the segmentation function may be a 2 or 3 dimensional array of the same or smaller dimension of image <b>312</b>. The elements of the array may indicate the segmentation determined by the segmentation function. A segmentation element may be a value indicating the type of the corresponding pixel/voxel, but may also be vector. For example, in case a segmentation into p objects is desired. The elements in the output array of the segmentation function may be p-dimensional vectors. Elements in the vector may indicate the found object. The sum of the vector may be 1, or scaled to 1, etc.</p><p id="p-0127" num="0126">Method <b>400</b> may comprise applying <b>460</b> inverses of the multiple registering transformations to the multiple image segmentations <b>340</b> thus obtaining multiple image segmentations <b>350</b>. For example, the method may compute T<sub>i</sub><sup>&#x2212;1</sup>(NN(T<sub>i</sub>(I))). If the size of the segmentation function inputs and outputs differ, then the inverse transformation T<sub>i</sub><sup>&#x2212;1 </sup>may have to be downsampled, to fit the smaller array. For example, the latter could be done by interpolation.</p><p id="p-0128" num="0127">Method <b>400</b> may comprise determining <b>470</b> an output segmentation <b>361</b> from the multiple image segmentations. For example, the determining may comprise a majority voting. For example, the image segmentations <b>350</b> may comprise binary images; segmentation <b>316</b> may also comprise a binary image in which a pixels value may be determined by the value that occurs most often, in the corresponding pixel of the segmentations <b>350</b>. The same approach may be used for multi-value images, instead of binary images. In case the segmentations <b>350</b> are arrays, e.g., 2d or 3d arrays, which contain p-dimensinal vectors, the corresponding vector-elements may be averaged according to some average function. For example, given a set of vectors v<sub>i </sub>which each correspond to the same elements, e.g., pixel, etc. of segmentations <b>350</b>, one could determine the corresponding vector in segmentation <b>361</b> as 1/n&#x3a3;v<sub>i</sub>. Interestingly, a majority function may be approximated by using a power-average, e.g., a root-mean-square. For example,</p><p id="p-0129" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mroot>   <mrow>    <mrow>     <mn>1</mn>     <mo>/</mo>     <mi>n</mi>    </mrow>    <mo>&#x2062;</mo>    <mrow>     <mo>&#x2211;</mo>     <msubsup>      <mi>v</mi>      <mi>i</mi>      <mi>d</mi>     </msubsup>    </mrow>   </mrow>   <mi>d</mi>  </mroot>  <mo>,</mo> </mrow></math></maths></p><p id="p-0130" num="0000">wherein the d-power is computed component wise. For example, one may select d&#x2265;2. Using higher values of d, e.g., 2 or higher has the advantage that a majority like determination is computed, while still allowing for multi-object segmentation using vectors instead of single-value classifiers.</p><p id="p-0131" num="0128">As shown above, a segmentation function which has been trained for image segmentation, may be enhanced by registering an image toward atlas images. This has the advantage, that instead of a single segmentation, the segmentation function is forced to provide multiple segmentations. These segmentations are in essence for the same image, but this information is not available to the segmentation function.</p><p id="p-0132" num="0129">Some types of segmentation functions may be invariant for some transformation. For example, if the segmentation function comprises a convolutional neural network, it may be invariant under translation, at least to some extent, however this will not hold for slightly more complicated transformations. In fact, a convolutional network will typically not even be invariant under rigid transformations, let alone, elastic transformations, such as a diffeomorphic transformation. Thus, if an image <b>312</b> happens to fall in a failure mode of segmentation function <b>210</b>, this is likely not true for a registered version of image <b>312</b>, and even less likely to occur for the majority of the registered images <b>330</b>.</p><p id="p-0133" num="0130">In an embodiment, the segmentation function is not invariant under the transformation class used to determine registration quality and/or used for image segmentation. For example, in an embodiment, the segmentation function is invariant under a first set of transformations, and a second set of transformations is used to determine registration quality and/or used for image segmentation, wherein the second set is larger than the first set, e.g., the first set is a subset of the second set. For example, in an embodiment, the segmentation function comprises a convolutional neural network, and the transformations used to determine registration quality and/or for image segmentation is larger than the set of transformation, e.g., comprises also rotations. For example, suppose the probability is about 5% that an image <b>312</b> will be mis-segmented, and 10 atlas images are used, then the probability that a majority will be mis-segmented is about:</p><p id="p-0134" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <mn>1</mn>   <mo>-</mo>   <mrow>    <msubsup>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>=</mo>      <mn>6</mn>     </mrow>     <mn>10</mn>    </msubsup>    <mrow>     <mrow>      <mo>(</mo>      <mtable>       <mtr>        <mtd>         <mn>10</mn>        </mtd>       </mtr>       <mtr>        <mtd>         <mi>i</mi>        </mtd>       </mtr>      </mtable>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <msup>      <mrow>       <mo>(</mo>       <mn>0.95</mn>       <mo>)</mo>      </mrow>      <mi>i</mi>     </msup>     <mo>&#x2062;</mo>     <msup>      <mrow>       <mo>(</mo>       <mn>0.05</mn>       <mo>)</mo>      </mrow>      <mrow>       <mn>10</mn>       <mo>-</mo>       <mi>i</mi>      </mrow>     </msup>    </mrow>   </mrow>  </mrow>  <mo>=</mo>  <mrow>   <mn>0.00006</mn>   <mo>.</mo>  </mrow> </mrow></math></maths></p><p id="p-0135" num="0000">The latter is a far smaller value. The above computation assumes independence between the various applications of the segmentation function, but a main cause of dependence between the multiple segmentation obtained from the segmentation function are caused by imaging failures. For example, if the input image is of very low quality, e.g., caused by failure of the imaging device, then any application of the segmentation function may fail for any registration. However, such a failure mode can be ascribed to different causes than to the segmentation function technology. The number of ten atlas images above is an example. A reduction in failure probability may also be obtained with fewer atlas images.</p><p id="p-0136" num="0131">Atlas images could be selected from the images on which the segmentation function was trained. For example, one could select the atlas images at random. Selecting the atlas images from training images has the advantage that the segmentation function is likely familiar with those images. However, better results may be achieved with a more careful selection, e.g., as described herein.</p><p id="p-0137" num="0132"><figref idref="DRAWINGS">FIG. <b>4</b><i>a </i></figref>schematically illustrates an example of an embodiment of a method for selecting atlas images for use in a medical image segmentation method. <figref idref="DRAWINGS">FIG. <b>5</b><i>b </i></figref>schematically shows an example of an embodiment of a method <b>500</b> for selecting atlas images for use in a medical image segmentation method. The atlas images may be selected for a method such as method <b>400</b>.</p><p id="p-0138" num="0133">For example, method <b>500</b> may use the data items and structures shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>a </i></figref>and vice versa. Method <b>500</b> may also use the data shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>b </i></figref>which is described below.</p><p id="p-0139" num="0134">Method <b>500</b> may comprise obtaining <b>510</b> a segmentation function <b>210</b> configured to receive an input image and to produce an image segmentation. The segmentation function <b>210</b> may be same as the segmentation function that will be used in production use. As above, the segmentation function may comprise a neural network, e.g., a U or F net, CNN, etc., may be used.</p><p id="p-0140" num="0135">Method <b>500</b> may comprise obtaining <b>520</b> multiple test images <b>610</b> and corresponding test image segmentations <b>620</b>. For example, the multiple test images <b>610</b> may be obtained from the training of segmentation function <b>210</b>. For example, segmentation function <b>210</b> may be trained on multiple pairs of a training image and a training image segmentation. For example back propagation methods may be used to train a neural network on the training set. Typically, some set of images is kept aside for testing purposes, e.g., to test convergence of the segmentation function. The test images <b>610</b> may be obtained from the images used for training and/or from those used for testing the segmentation function. It is preferred that test images <b>610</b> are obtained from the same or a similar image distribution from which the images were drawn on which the segmentation function is trained.</p><p id="p-0141" num="0136">Test images <b>610</b> may be all images on which segmentation function <b>210</b> was trained and/or tested, especially if these were relatively few images, e.g., less than a 100 or so. Test images <b>610</b> may also be a subset, e.g., a random selection of the images used for training and/or testing. The latter is useful if the number of training/testing images was large.</p><p id="p-0142" num="0137">Method <b>500</b> may comprise determining <b>530</b> a segmentation quality <b>630</b> for the multiple test images by comparing the associated test image segmentation <b>620</b> with an image segmentation generated by the segmentation function. For example, the test images <b>610</b> may have an associated segmentation, e.g., a ground truth segmentation <b>620</b>. The latter may have been used in training or testing as well. There are several ways to compute a segmentation quality. One way to do so, is to compute a Dice score between the ground truth image segmentation and the segmentation produced by the segmentation function. For example, a segmentation quality may be obtained for image <b>611</b> by computing a Dice score between segmentation <b>621</b> and segmentation <b>631</b>. A high Dice score indicates a good overlap, and thus a high segmentation quality.</p><p id="p-0143" num="0138">Method <b>500</b> may comprise selecting <b>540</b> one or more of the test images as atlas images having a segmentation quality over a threshold. For example, a Dice score over a threshold may be required, e.g., a Dice score over 0.8. The threshold may be predetermined. The threshold may be dynamic. For example, the atlas images may be selected from the best performing images. For example, a percentage of the worst scoring images may be discarded.</p><p id="p-0144" num="0139">The inventors found that in practice most images perform well in segmentation but some percentage, say about 5% of the images show a worse segmentation that most images. By selecting the atlas images from the best performing 95% or 90% or 80%, or the like, of the images, it is avoided that an input image <b>312</b> is registered on an image for which the segmentation function performs bad, and thus making it more likely that the segmentation function would also perform bad on the registered image.</p><p id="p-0145" num="0140">For example, in an embodiment, a worst performing part is discarded of the images <b>610</b>, e.g., the bottom 5%. The atlas images may then be selected from the remaining images; this may be done at random, e.g., selecting 10 images at random.</p><p id="p-0146" num="0141">Additional criteria may be imposed on selecting the atlas images, e.g., in addition to segmentation quality; or possibly instead of segmentation quality.</p><p id="p-0147" num="0142"><figref idref="DRAWINGS">FIG. <b>2</b><i>c </i></figref>schematically shows an example of an embodiment of a segmentation quality determination. Shown is an image <b>212</b> with an associated true image segmentation, e.g., a ground truth segmentation. The image <b>212</b> is segmented by segmentation function <b>210</b> to obtain a generated image segmentation <b>214</b>. Generated image segmentation <b>214</b> and true image segmentation <b>216</b> may then be compared, e.g., using a comparator <b>220</b> to obtain a segmentation quality <b>217</b>. For example, comparator <b>220</b> may compute a Dice score.</p><p id="p-0148" num="0143"><figref idref="DRAWINGS">FIG. <b>4</b><i>b </i></figref>schematically illustrates an example of an embodiment of a method for selecting atlas images for use in a medical image segmentation method. Shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>b </i></figref>are images <b>610</b>&#x2032; that have not yet been selected as atlas image, and which have not been discarded yet for other reasons, e.g., for having a too poor segmentation quality. Also shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>b </i></figref>is an image <b>651</b> which has already been selected as atlas image. For example, image <b>651</b> may have been selected as having a best segmentation quality, or may have been selected at random from images having a good segmentation quality.</p><p id="p-0149" num="0144">A registration function is applied to the images <b>610</b>&#x2032; on the selected image <b>651</b>. A registration quality is obtained by comparing the registered test image to the selected test image <b>651</b>. In this way registration qualities <b>640</b> are obtained. The comparing may use comparator <b>220</b>, e.g., a Dice score. Other similarity measures may be used instead of a Dice score, e.g., correlation.</p><p id="p-0150" num="0145">Interestingly, and different from the segmentation quality, an atlas image may be selected which has low registration quality. A low registration quality indicates that the image is quite different from the images that are already selected as atlas image. For example, the image may have the lowest registration score. For example, the image with the best registration quality may be discarded and one or more random images may then be selected. For example, the images may be selected from the 50% of the images having the lowest registration quality.</p><p id="p-0151" num="0146">In an embodiment, the registration performed in method <b>400</b>, e.g., in registration function <b>325</b> may select the registration from the same class of transformations as the registration done to compute the registration quality. In particular, the registration type for atlas image selection and form image segmentation may be translation-only, e.g., shifting, rigid, similarity, affine or non-rigid, e.g., elastic, e.g., a diffeomorphic transformation. While the latter transformation may be non-elastic or at least much less elastic.</p><p id="p-0152" num="0147">Interestingly, it is possible for the registration performed in method <b>400</b>, e.g., in registration function <b>325</b> may select the registration from a different, e.g., larger class of transformations than the registration done to compute the registration quality. In particular, the former may be an elastic registration, e.g., a diffeomorphic transformation. While the latter transformation may be non-elastic or at least much less elastic. For example, one could use rigid registrations to compute registration quality. A rigid transformation only allows transformations and rotations. A somewhat larger class may be allowed, e.g., transformations, rotations and scaling, or affine transformations.</p><p id="p-0153" num="0148">An elastic registration in method <b>400</b> allows an image to match an atlas image as close as possible, and thus obtain a segmentation quality that is comparable to that of the atlas image. On the other hand rigid transformation allow a better variety of images to be found, as elastic registrations would make images look too much alike.</p><p id="p-0154" num="0149">In <figref idref="DRAWINGS">FIG. <b>4</b><i>b </i></figref>multiple test images are compared to a single atlas image. For example, one may use this to select the second atlas image after the first atlas image has been selected only on the basis of segmentation quality. For example, this may be used add an atlas image that is different in registration terms from a particular previously selected atlas image. The latter atlas image may be randomly selected from the already selected atlas images, or, for example, one may iteratively use each selected images one by one, etc.</p><p id="p-0155" num="0150">Another approach is to compare the test image <b>610</b>&#x2032; not with a single selected atlas image <b>651</b> but with multiple atlas images, and even with all atlas images that have so far been selected. For example, one may compute a registration quality between test images <b>610</b>&#x2032; and the atlas images selected so far. One can then determine an overall registration quality for the test images <b>610</b>&#x2032;. The latter may for example, be an average of the registration qualities. A high average indicates that registration quality is often high. Instead of an arithmetic average a power arithmetic may be used, this emphasizes atlas images that are very different to some, even if similar to most. Selecting a further atlas image with a low overall registration quality is more likely to give an independent segmentation by the segmentation function. On the other hand, if the further atlas image has a high segmentation quality, the segmentation function is likely to perform well on in it.</p><p id="p-0156" num="0151">Selecting on the basis of overall registration may be done by discarding images that are too similar and selecting at random from the rest. Other selecting methods, e.g., as indicated herein are possible as well.</p><p id="p-0157" num="0152">For example, one may use the following procedure</p><p id="p-0158" num="0000">1. Select a random pool of test images, e.g., 100 or 1000 training images that have an associated true segmentation.<br/>2. Discard a % of the pool with worst segmentation quality. For example, a may be 5%.<br/>3. Select a random atlas image from the current pool and remove the selected image from the pool.<br/>4. Compute an overall registration quality for the pool of test images compared to selected atlas images<br/>5. Discard images from the pool with too high overall registration quality.<br/>6. Unless the pool is empty or a sufficient number of atlas images have been selected go to part 3.</p><p id="p-0159" num="0153">Step 2 may use an absolute score instead of a relative score. For example, images with a Dice score below 0.8 between generated segmentation and true segmentation may be discarded.</p><p id="p-0160" num="0154">Step 4 may be performed efficiently, since registration quality between an individual test image and an atlas image may be stored, and need not be computed again. Thus, if a single atlas image is added only registration qualities between the test images and the added atlas image need to be computed.</p><p id="p-0161" num="0155">Step 5 may remove b %, say also 5% or somewhat higher, e.g., 25% of the pool. But instead of a fixed percentage also an absolute threshold may be used. The absolute threshold may be determined empirically.</p><p id="p-0162" num="0156">Instead of random selection, e.g., in step <b>3</b> one may also select images with the best segmentation quality and/or worst registration quality or combination thereof, e.g., the sum or the like. The above atlas image selection algorithm is an example, but different embodiments are possible.</p><p id="p-0163" num="0157">In general, the bounds for segmentation quality and/or registration quality, whether absolute or relative, may depend on the difficulty of the segmentation problem, the quality of the segmentation function, e.g., the amount of training data, the acceptance of false positives, and so on. If the segmentation function comprises a neural network, its quality may be dependent on its depth. Bounds may be established empirically by keeping a set of training images apart as test data and evaluating the resulting segmentation system. In the various embodiments of system <b>110</b> and <b>160</b>, the communication interfaces may be selected from various alternatives. For example, the interface may be a network interface to a local or wide area network, e.g., the Internet, a storage interface to an internal or external data storage, a keyboard, an application interface (API), etc.</p><p id="p-0164" num="0158">The systems <b>110</b> and <b>160</b> may have a user interface, which may include well-known elements such as one or more buttons, a keyboard, display, touch screen, etc. The user interface may be arranged for accommodating user interaction for configuring the systems, training the segmentation functions on a training set, or applying the system to new image data, or selecting atlas images, etc.</p><p id="p-0165" num="0159">Storage may be implemented as an electronic memory, say a flash memory, or magnetic memory, say hard disk or the like. Storage may comprise multiple discrete memories together making up storage <b>140</b>, <b>180</b>. Storage may comprise a temporary memory, say a RAM. The storage may be cloud storage.</p><p id="p-0166" num="0160">System <b>110</b> may be implemented in a single device. System <b>160</b> may be implemented in a single device. Typically, the system <b>110</b> and <b>160</b> each comprise a microprocessor which executes appropriate software stored at the system; for example, that software may have been downloaded and/or stored in a corresponding memory, e.g., a volatile memory such as RAM or a non-volatile memory such as Flash. Alternatively, the systems may, in whole or in part, be implemented in programmable logic, e.g., as field-programmable gate array (FPGA). The systems may be implemented, in whole or in part, as a so-called application-specific integrated circuit (ASIC), e.g., an integrated circuit (IC) customized for their particular use. For example, the circuits may be implemented in CMOS, e.g., using a hardware description language such as Verilog, VHDL, etc. In particular, systems <b>110</b> and <b>160</b> may comprise circuits for the evaluation of segmentation functions.</p><p id="p-0167" num="0161">A processor circuit may be implemented in a distributed fashion, e.g., as multiple sub-processor circuits. A storage may be distributed over multiple distributed sub-storages. Part or all of the memory may be an electronic memory, magnetic memory, etc. For example, the storage may have volatile and a non-volatile part. Part of the storage may be read-only.</p><p id="p-0168" num="0162">In an embodiment, test-time data augmentation is included, e.g., in method <b>400</b>. The segmentation function, e.g., segmentation function <b>210</b>, may use a conventional segmentation function training; for example, the segmentation function may be configured for a conventional machine learning algorithm. For example, the machine learning algorithm may be decision forests. For example, the machine learning algorithms may be a neural network. For example, the segmentation function may comprise a neural network, e.g., a convolutional neural network (CNN) like a U-net or an F-net, etc. Atlas images may be selected from the training images. Below several further optional refinements, details, and embodiments are illustrated.</p><p id="p-0169" num="0163">In an embodiment, n&#x2265;2 atlas images are used, say n=10, etc. A new image for segmentation may be registered to the n atlases resulting in n invertible transformations. The segmentation function, e.g., the CNN, is applied to each of the n transformed images resulting in n annotation images, e.g., segmentation images. The inverse transformation resulting from registration is applied to the annotation images, and the labels of transformed annotation images are fused to obtain a final segmentation.</p><p id="p-0170" num="0164">The basic idea may also be applied more generally to other tasks like image classification (for example, using a ResNet-type neural network architecture). In this case the inverse transformation step after application of the segmentation function can be skipped. The segmentation function training may use a set of m 2D or 3D images I<sub>1</sub>, . . . , I<sub>m </sub>with corresponding annotations A<sub>1</sub>, . . . , A<sub>m</sub>. Training of a 2D/3D segmentation function for image segmentation can be done as conventional. For example, a neural network may be trained, e.g., using backpropagation, e.g., using the Adams optimizer. The m images together with the corresponding annotations may altogether be used for segmentation function training or subdivided into a training and a test set.</p><p id="p-0171" num="0165">Atlas selection may select a subset n (&#x2264;m) of the m images as atlases. The individual images used as atlases might be selected using various criteria. One criterion can be the segmentation quality observed during training (e.g. Dice score of the image after training when being part of the training data). Another criterion can be the segmentation quality observed during testing (e.g. Dice score of the image after training when being part of the test data). Another criterion can be the difference of images. For instance, rigid registration may be applied to the training images (e.g. to atlas candidates) and subsequently the candidate with the least mutual annotation overlap in terms of Dice score after registration to the already chosen candidates may be selected, and this process may be repeated iteratively to select n atlas images.</p><p id="p-0172" num="0166">For image-to-atlas registration and image transformation, there have been a large number of algorithms presented and applied to medical image registration. Recently also registration algorithms have been used that exploit deep learning and neural networks. An algorithm resulting in a diffeomorphic transformation has the advantage that the inverse transformation can be obtained easily. Depending on the images to be handled, a similarity measure suitable for the registration of images of the same modality like cross-correlation or for the registration of images of different types like mutual information can be used. Application of the (non-rigid) registration algorithm to an image I<sub>new </sub>and the atlas images I<sub>1</sub>, . . . , I<sub>n </sub>results in n transformations T<sub>1</sub>, . . . , T<sub>n </sub>that transform the new image into the space of the corresponding atlas, e.g., as T<sub>i</sub>(I<sub>new</sub>).</p><p id="p-0173" num="0167">In case that an atlas image has a larger Field-of-View than the new image to be segmented, the atlas image can be used to complement the Field-of-View of the new image.</p><p id="p-0174" num="0168">In addition to a spatial image transformation, the image gray-values of the new image I<sub>new </sub>may also be transformed to better represent the atlas images. This transformation may be included in transform T<sub>i</sub>. For instance, a parametric transformation may be applied to the intensities; the parameters may be determined so that the resulting histogram of the new image after intensity transformation corresponds to the atlas image.</p><p id="p-0175" num="0169">The transformed images T<sub>1</sub>(I<sub>new</sub>), . . . , T<sub>n</sub>(I<sub>new</sub>), e.g., the multiple registered images may be segmented using a previously trained segmentation function resulting in annotations A<sub>1,new</sub>, . . . , A<sub>n,new</sub>. After segmentation the annotations may be transformed back into the space of the new image resulting in the annotations T<sub>1</sub><sup>&#x2212;1</sup>(A<sub>1,new</sub>), . . . , T<sub>n</sub><sup>&#x2212;1</sup>(A<sub>n,new</sub>). Segmentation functions can be implemented using, for instance, a neural network, e.g., U-Net or F-Net architectures, but are not restricted to any specific type of network. Other neural networks can be utilized instead.</p><p id="p-0176" num="0170">Label fusion: Given the annotations T<sub>1</sub><sup>&#x2212;1 </sup>(A<sub>1,new</sub>), . . . , T<sub>n</sub><sup>&#x2212;1 </sup>(A<sub>n,new</sub>), a final annotation of the new image can be constructed, for instance, by majority voting; for example, to each voxel at position x the label may be assigned that corresponds to the most frequent label at position x in the annotations to be fused. Other label fusion techniques can be used alternatively. In an embodiment, information about the fusing, e.g., voting, may be used for further processing or display to the user. For example, a second, or top-k, label may be displayed, so that the operator know what a likely second option was. The latter may be used for quick feedback. For example, when an operator encounters a mis-segmentation, but, as is likely the second, or a top-k option was correct, then this information may be provided, say using an input device of the segmentation device. For example, the operator can use a mouse or the like to indicate that particular object or pixel/voxel belongs to the second most frequent label, etc. This information can later be used for retraining or fine-tuning the segmentation function.</p><p id="p-0177" num="0171">An embodiment is illustrated using the example of rib and spinal column segmentation in 3D CT images; <figref idref="DRAWINGS">FIG. <b>6</b><i>a </i></figref>shows a visual representation of such a CT-scan. In particular, an F-net neural network has been trained to address this task. The image of <figref idref="DRAWINGS">FIG. <b>6</b><i>a </i></figref>is segmented using the segmentation function comprising the neural network and a Dice score between the segmentation function output and an expert segmentation has been computed. The diagram of <figref idref="DRAWINGS">FIG. <b>6</b><i>c </i></figref>shows the Dice score for the spine at <b>652</b> and for ribs at <b>662</b>. Note that both scores are high which indicates that the segmentation function performs well on the image of <figref idref="DRAWINGS">FIG. <b>6</b></figref><i>a. </i></p><p id="p-0178" num="0172">During training of the neural network in the segmentation function, data augmentation was used; in this case rotations of up to 7&#xb0; of the CT data set. Accurate outputs are expected from the segmentation function across this range. <figref idref="DRAWINGS">FIG. <b>6</b><i>b </i></figref>shows the same image as <figref idref="DRAWINGS">FIG. <b>6</b><i>a</i></figref>, but with a considerably differently orientation; here, a rotation of 45&#xb0;. Note that apart from this rotation, the images are the same. When the segmentation function is applied to the <figref idref="DRAWINGS">FIG. <b>6</b><i>b</i></figref>, the resulting Dice scores are substantially smaller. The diagram of <figref idref="DRAWINGS">FIG. <b>6</b><i>d </i></figref>shows the Dice score for the spine at <b>654</b> and for ribs at <b>664</b>. Note that both scores are lower, which indicates that the segmentation function is not performing well on the image of <figref idref="DRAWINGS">FIG. <b>6</b><i>b</i></figref>. Note that the score for the spine is about half the previous value, indicating a collapse of predictive ability for the spine object in this case.</p><p id="p-0179" num="0173">Even rigid registration for <figref idref="DRAWINGS">FIG. <b>6</b><i>b </i></figref>to an atlas image like that of <figref idref="DRAWINGS">FIG. <b>6</b><i>a </i></figref>would return the segmentation capabilities of the segmentation function. More subtle predictive collapse, e.g., cause by unusual anatomical variations can be addressed with non-rigid registration. Applying the segmentation function to the transformed versions of the test image label fusion improves the segmentation results.</p><p id="p-0180" num="0174">For example, a segmentation function, atlas images or test images may be obtained from an electronic storage system, which may internal, or which may be external, e.g., addressed over a computer cable, or computer network, etc. The segmentation function may be trained on images, e.g., medical images that may be obtained from a medical imaging device, e.g., a CT scanner, etc. Receiving a medical image for segmentation may be done in the same way as the obtaining above. The receiving may be over an API, or other interface, which may be internal or external. The image may be received in a medical terminal from the imaging device. Registering the image, determining a registration or segmentation quality, etc., may be done in an electronic device, such as a computer. A neural network for a segmentation function, either during training and/or during applying may have multiple layers, which may include, e.g., convolutional layers and the like. For example, a neural network in the segmentation function may have at least 2, 5, 10, 15, 20 or 40 hidden layers, or more, etc. The number of neurons in the neural network may, e.g., be at least 10, 100, 1000, 10000, 100000, 1000000, or more, etc.</p><p id="p-0181" num="0175">Many different ways of executing the method herein, e.g., method <b>400</b> and/or <b>500</b>, are possible, as will be apparent to a person skilled in the art. For example, the steps can be performed in the shown order, but the order of the steps may also be varied or some steps may be executed in parallel. Moreover, in between steps other method steps may be inserted. The inserted steps may represent refinements of the method such as described herein, or may be unrelated to the method. Some parts may be executed, at least partially, in parallel. Moreover, a given part may not have finished completely before a next step is started.</p><p id="p-0182" num="0176">Embodiments of the method may be executed using software, which comprises instructions for causing a processor system to perform method <b>400</b> and/or <b>500</b>. Software may only include those steps taken by a particular sub-entity of the system. The software may be stored in a suitable storage medium, such as a hard disk, a floppy, a memory, an optical disc, etc. The software may be sent as a signal along a wire, or wireless, or using a data network, e.g., the Internet. The software may be made available for download and/or for remote usage on a server. Embodiments of the method may be executed using a bitstream arranged to configure programmable logic, e.g., a field-programmable gate array (FPGA), to perform the method.</p><p id="p-0183" num="0177">It will be appreciated that the presently disclosed subject matter also extends to computer programs, particularly computer programs on or in a carrier, adapted for putting the presently disclosed subject matter into practice. The program may be in the form of source code, object code, a code intermediate source, and object code such as partially compiled form, or in any other form suitable for use in the implementation of an embodiment of the method. An embodiment relating to a computer program product comprises computer executable instructions corresponding to each of the processing steps of at least one of the methods set forth. These instructions may be subdivided into subroutines and/or be stored in one or more files that may be linked statically or dynamically. Another embodiment relating to a computer program product comprises computer executable instructions corresponding to each of the devices, units and/or parts of at least one of the systems and/or products set forth.</p><p id="p-0184" num="0178"><figref idref="DRAWINGS">FIG. <b>7</b><i>a </i></figref>shows a computer readable medium <b>1000</b> having a writable part <b>1010</b> comprising a computer program <b>1020</b>, the computer program <b>1020</b> comprising instructions for causing a processor system to perform a segmentation method and/or atlas selection method, according to an embodiment. The computer program <b>1020</b> may be embodied on the computer readable medium <b>1000</b> as physical marks or by magnetization of the computer readable medium <b>1000</b>. However, any other suitable embodiment is conceivable as well. Furthermore, it will be appreciated that, although the computer readable medium <b>1000</b> is shown here as an optical disc, the computer readable medium <b>1000</b> may be any suitable computer readable medium, such as a hard disk, solid state memory, flash memory, etc., and may be non-recordable or recordable. The computer program <b>1020</b> comprises instructions for causing a processor system to perform said segmentation method and/or atlas selection method.</p><p id="p-0185" num="0179"><figref idref="DRAWINGS">FIG. <b>7</b><i>b </i></figref>shows in a schematic representation of a processor system <b>1140</b> according to an embodiment of a segmentation device and/or system and/or of an atlas selection device and/or system. The processor system comprises one or more integrated circuits <b>1110</b>. The architecture of the one or more integrated circuits <b>1110</b> is schematically shown in <figref idref="DRAWINGS">FIG. <b>7</b><i>b</i></figref>. Circuit <b>1110</b> comprises a processing unit <b>1120</b>, e.g., a CPU, for running computer program components to execute a method according to an embodiment and/or implement its modules or units. Circuit <b>1110</b> comprises a memory <b>1122</b> for storing programming code, data, etc. Part of memory <b>1122</b> may be read-only. Circuit <b>1110</b> may comprise a communication element <b>1126</b>, e.g., an antenna, connectors or both, and the like. Circuit <b>1110</b> may comprise a dedicated integrated circuit <b>1124</b> for performing part or all of the processing defined in the method. Processor <b>1120</b>, memory <b>1122</b>, dedicated IC <b>1124</b> and communication element <b>1126</b> may be connected to each other via an interconnect <b>1130</b>, say a bus. The processor system <b>1110</b> may be arranged for contact and/or contact-less communication, using an antenna and/or connectors, respectively.</p><p id="p-0186" num="0180">For example, in an embodiment, processor system <b>1140</b>, e.g., the segmentation device and/or system and/or atlas selection device and/or system may comprise a processor circuit and a memory circuit, the processor being arranged to execute software stored in the memory circuit. For example, the processor circuit may be an Intel Core i7 processor, ARM Cortex-R8, etc. In an embodiment, the processor circuit may be ARM Cortex MO. The memory circuit may be an ROM circuit, or a non-volatile memory, e.g., a flash memory. The memory circuit may be a volatile memory, e.g., an SRAM memory. In the latter case, the device may comprise a non-volatile software interface, e.g., a hard drive, a network interface, etc., arranged for providing the software.</p><p id="p-0187" num="0181">While device <b>1100</b> is shown as including one of each described component, the various components may be duplicated in various embodiments. For example, the processor <b>1120</b> may include multiple microprocessors that are configured to independently execute the methods described herein or are configured to perform steps or subroutines of the methods described herein such that the multiple processors cooperate to achieve the functionality described herein. Further, where the device <b>1100</b> is implemented in a cloud computing system, the various hardware components may belong to separate physical systems. For example, the processor <b>1120</b> may include a first processor in a first server and a second processor in a second server.</p><p id="p-0188" num="0182">It should be noted that the above-mentioned embodiments illustrate rather than limit the presently disclosed subject matter, and that those skilled in the art will be able to design many alternative embodiments.</p><p id="p-0189" num="0183">In the claims, any reference signs placed between parentheses shall not be construed as limiting the claim. Use of the verb &#x2018;comprise&#x2019; and its conjugations does not exclude the presence of elements or steps other than those stated in a claim. The article &#x2018;a&#x2019; or &#x2018;an&#x2019; preceding an element does not exclude the presence of a plurality of such elements. Expressions such as &#x201c;at least one of&#x201d; when preceding a list of elements represent a selection of all or of any subset of elements from the list. For example, the expression, &#x201c;at least one of A, B, and C&#x201d; should be understood as including only A, only B, only C, both A and B, both A and C, both B and C, or all of A, B, and C. The presently disclosed subject matter may be implemented by hardware comprising several distinct elements, and by a suitably programmed computer. In the device claim enumerating several parts, several of these parts may be embodied by one and the same item of hardware. The mere fact that certain measures are recited in mutually different dependent claims does not indicate that a combination of these measures cannot be used to advantage.</p><p id="p-0190" num="0184">In the claims references in parentheses refer to reference signs in drawings of exemplifying embodiments or to formulas of embodiments, thus increasing the intelligibility of the claim. These references shall not be construed as limiting the claim.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005158A1-20230105-M00001.NB"><img id="EMI-M00001" he="4.91mm" wi="76.20mm" file="US20230005158A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005158A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US20230005158A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for medical image segmentation, comprising:<claim-text>obtaining a segmentation function, the segmentation function being configured to receive a medical input image and to produce an image segmentation,</claim-text><claim-text>obtaining multiple atlas images,</claim-text><claim-text>receiving a medical image,</claim-text><claim-text>registering the received image to the multiple atlas images thus obtaining multiple registered images and multiple corresponding registering transformations configured to register the received image on the multiple atlas images,</claim-text><claim-text>applying the segmentation function to the multiple registered images thus obtaining multiple registered image segmentations,</claim-text><claim-text>applying inverses of the multiple registering transformations to the multiple image segmentations thus obtaining multiple image segmentations,</claim-text><claim-text>determining an output segmentation from the multiple image segmentations.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. A method as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the segmentation function is a machine learning function which has been trained on multiple training images and corresponding training image segmentations, and wherein an atlas image has been selected from the multiple training images.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. A method for selecting atlas images for use in a medical image segmentation method, the method comprising:<claim-text>obtaining a segmentation function configured to receive an input image and to produce an image segmentation,</claim-text><claim-text>obtaining multiple test images and corresponding test image segmentations,</claim-text><claim-text>determining a segmentation quality for the multiple test images by comparing the associated test image segmentation with an image segmentation generated by the segmentation function,</claim-text><claim-text>selecting one or more of the test images as atlas images having a segmentation quality over a threshold.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. A method as in <claim-ref idref="CLM-00003">claim 3</claim-ref>, comprising:<claim-text>for the test images that have not yet been selected<claim-text>applying a registration from the unselected test images to a selected test image and determine a registration quality by comparing the registered test image to the selected test image,</claim-text></claim-text><claim-text>select a test image as a further atlas image having a registration quality below a threshold.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. A method as in <claim-ref idref="CLM-00004">claim 4</claim-ref>, comprising:<claim-text>for the test images that have not yet been selected,<claim-text>applying a registration from the test image to the selected test images and determining multiple registration qualities,</claim-text><claim-text>determining an overall registration quality from the determined multiple registration qualities,</claim-text></claim-text><claim-text>select a test image as a further atlas image having an overall registration quality below a threshold.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. A method as in <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the registration is any one of: translation registration, rigid registration, similarity registration, affine registration or non-rigid registration.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A method as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining segmentation quality and/or registration quality comprises determining a Dice score.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the segmentation function has been trained on multiple training images, the test images being comprised in the training images.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the segmentation function is a neural network.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A method for medical image segmentation as in <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising a method for selecting atlas images as in any one of <claim-ref idref="CLM-00003">claims 3</claim-ref>-<claim-ref idref="CLM-00009">9</claim-ref>.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A medical image segmentation system, comprising:<claim-text>a segmentation function storage configured to store a segmentation function, the segmentation function being configured to receive a medical input image and to produce an image segmentation,</claim-text><claim-text>an atlas storage configured to store multiple atlas images,</claim-text><claim-text>a communication interface configured to receive a medical image,</claim-text><claim-text>a processor system configured for<claim-text>registering the received image to the multiple atlas images thus obtaining multiple registered images and multiple corresponding registering transformations configured to register the received image on the multiple atlas images,</claim-text><claim-text>applying the segmentation function to the multiple registered images thus obtaining multiple registered image segmentations,</claim-text><claim-text>applying inverses of the multiple registering transformations to the multiple image segmentations thus obtaining multiple image segmentations,</claim-text><claim-text>determining an output segmentation from the multiple image segmentations.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. An atlas images selecting system for use in a medical image segmentation method or device, comprising:<claim-text>a segmentation function storage configured to store a segmentation function, the segmentation function being configured to receive a medical input image and to produce an image segmentation,</claim-text><claim-text>a test image storage configured to store multiple test images and corresponding test image segmentations,</claim-text><claim-text>a processor system configured for<claim-text>determining a segmentation quality for the multiple test images by comparing the associated test image segmentation with an image segmentation generated by the segmentation function,</claim-text><claim-text>selecting one or more of the test images as atlas images having a segmentation quality over a threshold.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A transitory or non-transitory computer readable medium storing instructions that, when executed by one or more processors, cause the one or more processors to:<claim-text>obtain a segmentation function, the segmentation function being configured to receive a medical input image and to produce an image segmentation;</claim-text><claim-text>obtaining multiple atlas images;</claim-text><claim-text>receiving a medical image;</claim-text><claim-text>register the received image to the multiple atlas images thus obtaining multiple registered images and multiple corresponding registering transformations configured to register the received image on the multiple atlas images;</claim-text><claim-text>apply the segmentation function to the multiple registered images thus obtaining multiple registered image segmentations;</claim-text><claim-text>apply inverses of the multiple registering transformations to the multiple image segmentations thus obtaining multiple image segmentations; and</claim-text><claim-text>determine an output segmentation from the multiple image segmentations.</claim-text></claim-text></claim></claims></us-patent-application>