<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000584A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000584</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17662055</doc-number><date>20220504</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>90</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20160201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>90</main-group><subgroup>36</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20160201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>2090</main-group><subgroup>365</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR AIDING NON-CONTACT DETECTOR PLACEMENT IN NON-CONTACT PATIENT MONITORING SYSTEMS</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63216698</doc-number><date>20210630</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63268551</doc-number><date>20220225</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Covidien LP</orgname><address><city>Mansfield</city><state>MA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>JACQUEL</last-name><first-name>Dominique D.</first-name><address><city>Edinburgh</city><country>GB</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>MONTGOMERY</last-name><first-name>Dean</first-name><address><city>Edinburgh</city><country>GB</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SMIT</last-name><first-name>Philip C.</first-name><address><city>Edinburgh</city><country>GB</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>ADDISON</last-name><first-name>Paul S.</first-name><address><city>Edinburgh</city><country>GB</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for aiding a clinician in the proper positioning, placing or otherwise locating of a non-contact detector component of a non-contact patient monitoring system are described. The systems and methods may employ a targeting aid superimposed on a display screen component of the non-contact patient monitoring system, the targeting aid being designed to assist the clinician in properly locating the non-contact detector for proper and accurate functioning of the non-contact patient monitoring system. The systems and methods described herein may also employ a bendable mounting arm to which the non-contact detector is attached such that the non-contact detector can be easily moved into the proper location when used in conjunction with the targeting aid superimposed on the display.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="114.05mm" wi="158.75mm" file="US20230000584A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="198.04mm" wi="154.09mm" orientation="landscape" file="US20230000584A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="231.73mm" wi="174.58mm" file="US20230000584A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="231.73mm" wi="164.25mm" file="US20230000584A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="231.73mm" wi="166.12mm" file="US20230000584A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="216.15mm" wi="130.05mm" file="US20230000584A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="231.73mm" wi="161.80mm" file="US20230000584A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="178.48mm" wi="172.64mm" file="US20230000584A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="231.73mm" wi="176.61mm" file="US20230000584A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="231.73mm" wi="149.94mm" file="US20230000584A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="231.73mm" wi="175.85mm" file="US20230000584A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="231.73mm" wi="118.28mm" file="US20230000584A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="231.73mm" wi="167.30mm" file="US20230000584A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="230.21mm" wi="85.17mm" file="US20230000584A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="231.73mm" wi="176.87mm" file="US20230000584A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application claims benefit of priority to U.S. Provisional Patent Application No. 63/216,698, entitled &#x201c;SYSTEMS AND METHODS FOR AIDING NON-CONTACT DETECTOR PLACEMENT IN NON-CONTACT PATIENT MONITORING SYSTEMS,&#x201d; filed on Jun. 30, 2021, and U.S. Provisional Patent Application No. 63/268,551, entitled &#x201c;SYSTEMS AND METHODS FOR AIDING NON-CONTACT DETECTOR PLACEMENT IN NON-CONTACT PATIENT MONITORING SYSTEMS,&#x201d; filed on Feb. 25, 2022, which are specifically incorporated by reference herein for all that they disclose or teach.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to systems and methods for aiding a clinician in the proper positioning, placing or otherwise locating of a non-contact detector component of a non-contact patient monitoring system. While generally not limited to any specific equipment, the non-contact detector component of the non-contact patient monitoring system can be a camera, such as a depth sensing camera. In some embodiments, the systems and methods described herein employ a targeting aid superimposed on a display component of the non-contact patient monitoring system, the targeting aid being designed to assist the clinician in properly locating the non-contact detector for proper and accurate functioning of the non-contact patient monitoring system. The systems and methods described herein may also employ a bendable mounting arm to which the non-contact detector is attached such that the non-contact detector can be easily moved into the proper location when used in conjunction with the targeting aid superimposed on the display. In some embodiments, the attachment mechanism between the bendable arm and the non-contact detector includes a gimbal such that the non-contact detector remains aligned in the correct direction (e.g., lens aligned generally horizontally) regardless of the manner in which the bendable arm is moved or manipulated.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">A variety of technologies have been developed for non-contact patient monitoring. Some of these technologies employ depth sensing technologies, which can be employed to determine a number of physiological and contextual parameters, including, but not limited to, respiration rate, tidal volume, minute volume, effort to breath, patient activity, presence in bed, etc. By way of example, U.S. Pat. Nos. 10,702,188 and 10,939,824 and U.S. Published Patent Application Nos. 2019/0209046, 2019/0380599, 2019/0380807, and 2020/0046302, each of which is incorporated herein by reference in its entirety, describe various non-contact patient monitoring technologies employing depth sensing technology for determining patient physiological and contextual parameters.</p><p id="p-0005" num="0004">The efficacy and usefulness of such non-contact patient monitoring system may depend at least in part on the correct positioning of the depth sensing camera used as part of the non-contact patient monitoring system. Correct positioning of the depth sensing camera may include, for example, both the location of the camera relative to the patient being monitored and the camera's distance away from the patient. If either or both parameters are not properly set, the data obtained from the depth sensing camera may be inconsistent and/or inaccurate. In contrast, when the camera is properly positioned, the ability to localize the visualization on the screen to the patient's targeted region is greatly improved. This, in turn, greatly improves the ability to generate accurate and reliable data that is subsequently used to derive various patient parameters.</p><p id="p-0006" num="0005">It has been found through experimentation and clinical trials that clinicians often improperly locate the depth sensing camera, thus leading to diminished reliability and quality in patient parameters derived from non-contact patient monitoring systems. Accordingly, a need exists for methods and systems that aid the clinician in proper location of a depth sensing component of a non-contact patient monitoring system.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">Described herein are various embodiments of methods and systems for aiding non-contact detector placement in a non-contact patient monitoring system.</p><p id="p-0008" num="0007">In one embodiment, a video-based patient monitoring method includes: obtaining from a non-contact detector a video signal, the video signal encompassing at least a torso region of a patient; displaying on a display screen a video based on the video signal; superimposing a target over a portion of the video displayed on the display screen; and providing an indication that the torso region of the patient in the displayed video is located within the target.</p><p id="p-0009" num="0008">In another embodiment, a video-based patient monitoring method includes: displaying on a display screen a patient image based on a video signal obtained from a video camera, the patient image having superimposed thereon a target encompassing a portion of the patient image; and manipulating a bendable arm to which the video camera is attached to reposition the video camera until a patient target area in the patient image is located within the target on the display screen; wherein a connection between the bendable arm and the video camera includes a gimbal, the gimbal being configured to maintain the orientation of the video camera in a position generally parallel to a bed on which the patient is positioned during manipulation of the bendable arm.</p><p id="p-0010" num="0009">In another embodiments, a video-based patient monitoring system includes: a video camera configured to obtain a video signal; a bendable arm attached at a distal end to the video camera; and a display, the display configured to display a patient image based on the video signal and superimpose over the patient image a target; wherein the video camera is moveable about a patient via manipulation of the bendable arm; and wherein the system is configured to: automatically determine when a target patient area of the patient image is located within the target superimposed on the patient image via manipulation of the bendable arm and corresponding repositioning of the video camera; and provide visual and/or audible feedback when the system automatically determines that the target patient area of the patient image is located within the target superimposed on the patient image.</p><p id="p-0011" num="0010">In another embodiment, a video-based patient monitoring method includes projecting a target onto a patient using a projector connected to a non-contact detector, manipulating the position of the non-contact detector until a portion of the patient's body is located within the target, obtaining from the non-contact detector a video signal, the video signal encompassing at least the portion of the patient's body located within the target, and displaying on a display screen a video based on the video signal. In some embodiments, the display screen is located remote (e.g., not visible from) the non-contact detector.</p><p id="p-0012" num="0011">In another embodiment, a video-based patient monitoring system includes a video camera configured to obtain a video signal, a bendable arm attached at a distal end to the video camera, a projector connected to the video camera, the projector being configured to project a target on to a patient when the patient is in a field of view of the video camera, and a display located remote from the video camera and the projector, the display being configured to display a patient image based on the video signal. The video camera is moveable about the patient via manipulation of the bendable arm, and the projector is connected to the video camera in a manner that ensures that when the patient's torso is located within the projected target, the video camera can be used to obtain depth data suitable for use in calculating a patient breathing parameter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0013" num="0012">Many aspects of the present disclosure can be better understood with reference to the following drawings. The components in the drawing are not necessarily to scale. Instead, emphasis is placed on illustrating clearly the principles of the present disclosure. The drawings should not be taken to limit the disclosure to the specific embodiments depicted but are for explanation and understanding only.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic view of a video-based patient monitoring system configured in accordance with various embodiments of the present technology.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a video-based patient monitoring system having a computing device, a server, and one or more image capturing devices, and configured in accordance with various embodiments of the present technology.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is an illustration of components of a video-based patient monitoring system configured in accordance with various embodiments of the present technology.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is an illustration of components of a video-based patient monitoring system configured in accordance with various embodiments of the present technology, the video-based patient monitoring system being installed proximate a patient bed.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b>C</figref> is an illustration of components of a video-based patient monitoring system configured in accordance with various embodiments of the present technology.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an illustration of a display of a video-based patient monitoring system configured in accordance with various embodiments of the present technology.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an illustration of various states of a display of a video-based patient monitoring system based on different camera locations, the display and system configured in accordance with various embodiments of the present technology.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an illustration of various states of a display of a video-based patient monitoring system based on different patient image orientations, the display and system configured in accordance with various embodiments of the present technology.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an illustration of a display of a video-based patient monitoring system configured in accordance with various embodiments of the present technology.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is an illustration of a display of a video-based patient monitoring system configured in accordance with various embodiments of the present technology.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flow chart of a method for video-based non-contact patient monitoring configured in accordance with various embodiments of the present technology.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an illustration of components of a video-based patient monitoring system configured in accordance with various embodiments of the present technology.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is an illustration of various targets projected on a patient in accordance with various embodiments of the present technology.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flow chart of a method for video-based non-contact patient monitoring configured in accordance with various embodiments of the present technology</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0028" num="0027">Specific details of several embodiment of the present technology are described herein with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>12</b></figref>. Although many of the embodiments are described with respect to devices, systems, and methods for aiding non-contact detectors placement in non-contact patient monitoring technology, other applications and other embodiments in addition to those described herein are within the scope of the present technology. For example, at least some embodiments of the present technology can be useful for video-based monitoring of non-patients (e.g., elderly or neonatal individuals within their homes). It should be noted that other embodiments in addition to those disclosed herein are within the scope of the present technology. Further, embodiments of the present technology can have different configurations, components, and/or procedures than those shown or described herein. Moreover, a person of ordinary skill in the art will understand that embodiments of the present technology can have configurations, components, and/or procedures in addition to those shown or described herein and that these and other embodiments can be without several of the configurations, components, and/or procedures shown or described herein without deviating from the present technology.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic view of a patient <b>112</b> and a video-based patient monitoring system <b>100</b> configured in accordance with various embodiments of the present technology. The system <b>100</b> includes a non-contact detector <b>110</b> and a computing device <b>115</b>. In some embodiments, the detector <b>110</b> can include one or more image capture devices, such as one or more video cameras. In the illustrated embodiment, the non-contact detector <b>110</b> includes a video camera <b>114</b>. The non-contact detector <b>110</b> of the system <b>100</b> is placed remote from the patient <b>112</b>. More specifically, the video camera <b>114</b> of the non-contact detector <b>110</b> is positioned remote from the patient <b>112</b> in that it is spaced apart from and does not contact the patient <b>112</b>. The camera <b>114</b> includes a detector exposed to a field of view (FOV) <b>116</b> that encompasses at least a portion of the patient <b>112</b>.</p><p id="p-0030" num="0029">The camera <b>114</b> can capture a sequence of images over time. The camera <b>114</b> can be a depth sensing camera, such as a Kinect camera from Microsoft Corp. (Redmond, Wash.) or Intel camera such as the D<b>415</b>, D<b>435</b>, and SR<b>305</b> cameras from Intel Corp, (Santa Clara, Calif.). A depth sensing camera can detect a distance between the camera and objects within its field of view. Such information can be used to determine that a patient <b>112</b> is within the FOV <b>116</b> of the camera <b>114</b> and/or to determine one or more regions of interest (ROI) to monitor on the patient <b>112</b>. Once a ROI is identified, the ROI can be monitored over time, and the changes in depth of regions (e.g., pixels) within the ROI <b>102</b> can represent movements of the patient <b>112</b>.</p><p id="p-0031" num="0030">In some embodiments, the system <b>100</b> determines a skeleton-like outline of the patient <b>112</b> to identify a point or points from which to extrapolate a ROI. For example, a skeleton-like outline can be used to find a center point of a chest, shoulder points, waist points, and/or any other points on a body of the patient <b>112</b>. These points can be used to determine one or more ROIs. For example, a ROI <b>102</b> can be defined by filling in area around a center point <b>103</b> of the chest, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Certain determined points can define an outer edge of the ROI <b>102</b>, such as shoulder points. In other embodiments, instead of using a skeleton, other points are used to establish a ROI. For example, a face can be recognized, and a chest area inferred in proportion and spatial relation to the face. In other embodiments, a reference point of a patient's chest can be obtained (e.g., through a previous 3-D scan of the patient), and the reference point can be registered with a current 3-D scan of the patient. In these and other embodiments, the system <b>100</b> can define a ROI around a point using parts of the patient <b>112</b> that are within a range of depths from the camera <b>114</b>. In other words, once the system <b>100</b> determines a point from which to extrapolate a ROI, the system <b>100</b> can utilize depth information from the depth sensing camera <b>114</b> to fill out the ROI. For example, if the point <b>103</b> on the chest is selected, parts of the patient <b>112</b> around the point <b>103</b> that are a similar depth from the camera <b>114</b> as the point <b>103</b> are used to determine the ROI <b>102</b>.</p><p id="p-0032" num="0031">In another example, the patient <b>112</b> can wear specially configured clothing (not shown) that includes one or more features to indicate points on the body of the patient <b>112</b>, such as the patient's shoulders and/or the center of the patient's chest. The one or more features can include visually encoded message (e.g., bar code, QR code, etc.), and/or brightly colored shapes that contrast with the rest of the patient's clothing. In these and other embodiments, the one or more features can include one or more sensors that are configured to indicate their positions by transmitting light or other information to the camera <b>114</b>. In these and still other embodiments, the one or more features can include a grid or another identifiable pattern to aid the system <b>100</b> in recognizing the patient <b>112</b> and/or the patient's movement. In some embodiments, the one or more features can be stuck on the clothing using a fastening mechanism such as adhesive, a pin, etc. For example, a small sticker can be placed on a patient's shoulders and/or on the center of the patient's chest that can be easily identified within an image captured by the camera <b>114</b>. The system <b>100</b> can recognize the one or more features on the patient's clothing to identify specific points on the body of the patient <b>112</b>. In turn, the system <b>100</b> can use these points to recognize the patient <b>112</b> and/or to define a ROI.</p><p id="p-0033" num="0032">In some embodiments, the system <b>100</b> can receive user input to identify a starting point for defining a ROI. For example, an image can be reproduced on a display <b>122</b> of the system <b>100</b>, allowing a user of the system <b>100</b> to select a patient <b>112</b> for monitoring (which can be helpful where multiple objects are within the FOV <b>116</b> of the camera <b>114</b>) and/or allowing the user to select a point on the patient <b>112</b> from which a ROI can be determined (such as the point <b>103</b> on the chest of the patient <b>112</b>). In other embodiments, other methods for identifying a patient <b>112</b>, identifying points on the patient <b>112</b>, and/or defining one or more ROI's can be used.</p><p id="p-0034" num="0033">The images detected by the camera <b>114</b> can be sent to the computing device <b>115</b> through a wired or wireless connection <b>120</b>. The computing device <b>115</b> can include a processor <b>118</b> (e.g., a microprocessor), the display <b>122</b>, and/or hardware memory <b>126</b> for storing software and computer instructions. Sequential image frames of the patient <b>112</b> are recorded by the video camera <b>114</b> and sent to the processor <b>118</b> for analysis. The display <b>122</b> can be remote from the camera <b>114</b>, such as a video screen positioned separately from the processor <b>118</b> and the memory <b>126</b>. Other embodiments of the computing device <b>115</b> can have different, fewer, or additional components than shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In some embodiments, the computing device <b>115</b> can be a server. In other embodiments, the computing device <b>115</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can be additionally connected to a server (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> and discussed in greater detail below). The captured images/video can be processed or analyzed at the computing device <b>115</b> and/or a server to determine, e.g., a patient's position while lying in bed or a patient's change from a first position to second position while lying in bed. In some embodiments, some or all of the processing may be performed by the camera, such as by a processor integrated into the camera or when some or all of the computing device <b>115</b> is incorporated into the camera.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a video-based patient monitoring system <b>200</b> (e.g., the video-based patient monitoring system <b>100</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) having a computing device <b>210</b>, a server <b>225</b>, and one or more image capture devices <b>285</b>, and configured in accordance with various embodiments of the present technology. In various embodiments, fewer, additional, and/or different components can be used in the system <b>200</b>. The computing device <b>210</b> includes a processor <b>215</b> that is coupled to a memory <b>205</b>. The processor <b>215</b> can store and recall data and applications in the memory <b>205</b>, including applications that process information and send commands/signals according to any of the methods disclosed herein. The processor <b>215</b> can also (i) display objects, applications, data, etc. on an interface/display <b>207</b> and/or (ii) receive inputs through the interface/display <b>207</b>. As shown, the processor <b>215</b> is also coupled to a transceiver <b>220</b>.</p><p id="p-0036" num="0035">The computing device <b>210</b> can communicate with other devices, such as the server <b>225</b> and/or the image capture device(s) <b>285</b> via (e.g., wired or wireless) connections <b>270</b> and/or <b>280</b>, respectively. For example, the computing device <b>210</b> can send to the server <b>225</b> information determined about a patient from images captured by the image capture device(s) <b>285</b>. The computing device <b>210</b> can be the computing device <b>115</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Accordingly, the computing device <b>210</b> can be located remotely from the image capture device(s) <b>285</b>, or it can be local and close to the image capture device(s) <b>285</b> (e.g., in the same room). In various embodiments disclosed herein, the processor <b>215</b> of the computing device <b>210</b> can perform the steps disclosed herein. In other embodiments, the steps can be performed on a processor <b>235</b> of the server <b>225</b>. In some embodiments, the various steps and methods disclosed herein can be performed by both of the processors <b>215</b> and <b>235</b>. In some embodiments, certain steps can be performed by the processor <b>215</b> while others are performed by the processor <b>235</b>. In some embodiments, information determined by the processor <b>215</b> can be sent to the server <b>225</b> for storage and/or further processing.</p><p id="p-0037" num="0036">In some embodiments, the image capture device(s) <b>285</b> are remote sensing device(s), such as depth sensing video camera(s), as described above with respect to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In some embodiments, the image capture device(s) <b>285</b> can be or include some other type(s) of device(s), such as proximity sensors or proximity sensor arrays, heat or infrared sensors/cameras, sound/acoustic or radio wave emitters/detectors, or other devices that include a field of view and can be used to monitor the location and/or characteristics of a patient or a region of interest (ROI) on the patient. Body imaging technology can also be utilized according to the methods disclosed herein. For example, backscatter x-ray or millimeter wave scanning technology can be utilized to scan a patient, which can be used to define and/or monitor a ROI. Advantageously, such technologies can be able to &#x201c;see&#x201d; through clothing, bedding, or other materials while giving an accurate representation of the patient's skin. This can allow for more accurate measurements, particularly if the patient is wearing baggy clothing or is under bedding. The image capture device(s) <b>285</b> can be described as local because they are relatively close in proximity to a patient such that at least a part of a patient is within the field of view of the image capture device(s) <b>285</b>. In some embodiments, the image capture device(s) <b>285</b> can be adjustable to ensure that the patient is captured in the field of view. For example, the image capture device(s) <b>285</b> can be physically movable, can have a changeable orientation (such as by rotating or panning), and/or can be capable of changing a focus, zoom, or other characteristic to allow the image capture device(s) <b>285</b> to adequately capture images of a patient and/or a ROI of the patient. In various embodiments, for example, the image capture device(s) <b>285</b> can focus on a ROI, zoom in on the ROI, center the ROI within a field of view by moving the image capture device(s) <b>285</b>, or otherwise adjust the field of view to allow for better and/or more accurate tracking/measurement of the ROI.</p><p id="p-0038" num="0037">The server <b>225</b> includes a processor <b>235</b> that is coupled to a memory <b>230</b>. The processor <b>235</b> can store and recall data and applications in the memory <b>230</b>. The processor <b>235</b> is also coupled to a transceiver <b>240</b>. In some embodiments, the processor <b>235</b>, and subsequently the server <b>225</b>, can communicate with other devices, such as the computing device <b>210</b> through the connection <b>270</b>.</p><p id="p-0039" num="0038">The devices shown in the illustrative embodiment can be utilized in various ways. For example, either the connections <b>270</b> or <b>280</b> can be varied. Either of the connections <b>270</b> and <b>280</b> can be a hard-wired connection. A hard-wired connection can involve connecting the devices through a USB (universal serial bus) port, serial port, parallel port, or other type of wired connection that can facilitate the transfer of data and information between a processor of a device and a second processor of a second device. In another embodiment, either of the connections <b>270</b> and <b>280</b> can be a dock where one device can plug into another device. In other embodiments, either of the connections <b>270</b> and <b>280</b> can be a wireless connection. These connections can take the form of any sort of wireless connection, including, but not limited to, Bluetooth connectivity, Wi-Fi connectivity, infrared, visible light, radio frequency (RF) signals, or other wireless protocols/methods. For example, other possible modes of wireless communication can include near-field communications, such as passive radio-frequency identification (RFID) and active RFID technologies. RFID and similar near-field communications can allow the various devices to communicate in short range when they are placed proximate to one another. In yet another embodiment, the various devices can connect through an internet (or other network) connection. That is, either of the connections <b>270</b> and <b>280</b> can represent several different computing devices and network components that allow the various devices to communicate through the internet, either through a hard-wired or wireless connection. Either of the connections <b>270</b> and <b>280</b> can also be a combination of several modes of connection.</p><p id="p-0040" num="0039">The configuration of the devices in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is merely one physical system <b>200</b> on which the disclosed embodiments can be executed. Other configurations of the devices shown can exist to practice the disclosed embodiments. Further, configurations of additional or fewer devices than the devices shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> can exist to practice the disclosed embodiments. Additionally, the devices shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> can be combined to allow for fewer devices than shown or can be separated such that more than the three devices exist in a system. It will be appreciated that many various combinations of computing devices can execute the methods and systems disclosed herein. Examples of such computing devices can include other types of medical devices and sensors, infrared cameras/detectors, night vision cameras/detectors, thermal cameras, other types of cameras, augmented reality goggles, virtual reality goggles, mixed reality goggle, radio frequency transmitters/receivers, smart phones, personal computers, servers, laptop computers, tablets, blackberries, RFID enabled devices, smart watch or wearables, or any combinations of such devices.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>1</b></figref> generally shows non-contact detector <b>110</b> positioned over patient <b>112</b>. While not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in some embodiments, the non-contact detector <b>110</b> is moveable such that the detector can be positioned at various locations about patient <b>112</b>. For example, as shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, the non-contact detector <b>110</b> (e.g., camera <b>114</b>) may be attached to the distal end of a bendable arm <b>301</b> such that the location of camera <b>114</b> can be changed by manipulating the bendable arm <b>301</b>. As shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, the proximate end of the bendable arm <b>301</b> can be connected to a pivot point such that the entirety of the bendable arm <b>301</b> may be pivoted about the pivot point to allow for further re-locating of the camera <b>114</b>. In alternate embodiments, the bendable arm <b>301</b> is sufficiently long that a pivot point is not required and instead the camera <b>114</b> can be repositioned to any desirable location about the patient solely by manipulation of the bendable arm. <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> also shows the bendable arm <b>301</b> connected to a wheeled stand, which may allow for further re-positioning of the camera <b>114</b> through a combination of the bendable arm <b>301</b>, the pivot point and the wheeled stand. Again, the bendable arm <b>301</b> may have sufficient length such that a wheeled stand is not required for moving the camera <b>114</b> to any desired location over the patient.</p><p id="p-0042" num="0041">As shown in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, the bendable arm <b>301</b> extends over a patient bed <b>302</b> such that the camera <b>114</b> can be positioned over a patient lying in the patient bed <b>302</b>. Due to any combination of the bendable arm <b>301</b>, the pivot point to which the bendable arm <b>301</b> is attached, and the wheeled stand, the camera <b>114</b> can generally be moved such that is positioned over any portion of the patient. While the ability to move the camera <b>114</b> to any location around the patient can be beneficial to the clinician, it also allows the clinician to inadvertently position the camera <b>114</b> in such a way that diminishes the data collected by the camera <b>114</b>, such as depth sensing data acquired when the camera <b>114</b> is a depth sensing camera. When the collected data is incomplete or inaccurate due to poor camera placement, subsequent patient parameters calculated from the depth sensing data are less reliable and accurate than when the camera <b>114</b> is properly positioned.</p><p id="p-0043" num="0042">In order to aid in proper placement of the camera <b>114</b> to improve the quality and consistency of data collected by the camera <b>114</b>, the systems and methods described herein may employ a target superimposed on the display used to display the imaging captured by the camera <b>114</b>. With reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, an embodiment of a display <b>400</b> broadcasting a real-time or near real-time video stream from camera <b>114</b> is shown. The display <b>400</b> includes the patient image <b>401</b> captured by the camera <b>114</b>. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the patient image <b>401</b> is a depth sensing image captured by camera <b>114</b> when camera <b>114</b> is a depth sensing camera. Superimposed on the patient image <b>401</b> displayed on display <b>400</b> is target <b>402</b>. Target <b>402</b> is provided so that the clinician can adjust the positioning of the camera <b>114</b> until the target area of the patient (i.e., the portion of the patient from which data is to be collected) is located within the target <b>402</b>. For example, in embodiments where the non-contact patient monitoring system is being used to obtain patient parameters related to patient breathing, the target area of the patient is generally the torso area of the patient. Accordingly, the clinician can manipulate the location of camera <b>114</b> until the patient's torso is located within the target <b>402</b> when viewing the video feed of camera <b>114</b> on display <b>400</b>. By providing the clinician with the target <b>402</b> as a guide for us in proper placement of the camera <b>114</b>, the accuracy and reliability of data obtained from the camera <b>114</b> is improved.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>5</b></figref> provides an illustration of how the targeting aid <b>402</b> assists with proper placement of camera <b>114</b>. Two improper camera placements are shown on the left side of <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Displays <b>400</b><i>a </i>and <b>400</b><i>b </i>do not include target <b>402</b> and therefore the clinician is provided with no guidance for properly locating camera <b>114</b>. In display <b>400</b><i>a</i>, the camera <b>114</b> is proximate the patient's head. Further, the camera <b>114</b> is oriented transverse to the longitudinal axis <b>500</b> of the patient. As such, display <b>400</b><i>a </i>(i.e., the grey shaded area) captures only the top half of the patient and the patient is off center in the display <b>400</b><i>a</i>. In display <b>400</b><i>b</i>, the camera <b>114</b> is centered on the patient, but the orientation of the camera <b>114</b> remains transverse to the longitudinal axis <b>500</b> of the patient. As a result, display <b>400</b><i>b </i>(i.e., the grey shaded area) excludes the patient's head and feet. In both instances, the quality of the data collected by the depth sensing camera will be reduced due to the improper placement of camera <b>114</b>.</p><p id="p-0045" num="0044">In contrast, the right side of <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates the embodiment where the display <b>400</b><i>c </i>includes target <b>402</b>. The clinician is therefore able to reposition and reorient the camera <b>114</b> until the display <b>400</b><i>c </i>(i.e., the grey shaded area) includes the entirety of the patient's body and the patient's torso is located within target <b>402</b>. When the clinician visually inspects the display <b>400</b><i>c </i>to confirm that the patient's torso is located within the target <b>402</b>, this serves as confirmation that the camera <b>114</b> is properly positioned for accurate and reliable data collection.</p><p id="p-0046" num="0045">Referring back to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the target <b>402</b> is generally shown as covering the middle segment of the patient image <b>401</b> displayed on display <b>400</b> (i.e., the segment labeled B in <figref idref="DRAWINGS">FIG. <b>4</b></figref>), which will generally correlate with the patient's torso when the camera is properly positioned. Segments A and C in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, which may coincide with the patient's head and lower body, respectively, are excluded from the target <b>402</b>. In some embodiments, segments A and B each cover roughly 30% of the image <b>401</b>, while segment B covers approximately 40% of the image <b>401</b>. However, it should be appreciated that the specific location of target <b>402</b>, as well as the amount of the image <b>402</b> occupied by the target <b>402</b> is generally not limited. For example, the target may be located more toward the top or bottom of the image <b>401</b> then is shown <figref idref="DRAWINGS">FIG. <b>4</b></figref>, and the target can occupy more or less than 40% of the image.</p><p id="p-0047" num="0046">The target <b>402</b> as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> also includes essentially the entire width of the patient image <b>401</b>. However, it should be appreciated that the width of the target <b>402</b> is generally not limited, and may be less than the entire width of the image <b>401</b>. Furthermore, the target need not be laterally centered on the display <b>400</b>, but instead could be skewed to the left of right of the image <b>401</b>.</p><p id="p-0048" num="0047">The target <b>402</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref> is defined by broken corner lines of a rectangular shape, using white as the color for the broken corner lines. However, it should be appreciated that the color, shape and/or design of the target <b>402</b> is not limited. In some embodiments, the shape of the target <b>402</b> may be oval, circular, square, or any of these shapes having rounded corners. The shape of target <b>402</b> may also be completely arbitrary and/or irregular, such as a shape generally corresponding to a human torso. In some embodiments, the target <b>402</b> is defined by solid lines, rather than the broken corner lines as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Alternatively, broken lines extending across the entirety of the shape (rather than just in the corners) can also be used. Any color can be used for whatever shape and line type is used for target <b>402</b>.</p><p id="p-0049" num="0048">In other embodiments, the target <b>402</b> is displayed in the form of a shaded area, such that proper placement of the camera <b>114</b> is indicated when the patient torso (or other patient target area) is positioned within the shaded area. Alternatively, the non-target portion of the image <b>401</b> may be shaded, such that proper placement of the camera <b>114</b> is indicated when the patient torso (or other patient target area) is located in the unshaded area of the image <b>401</b>.</p><p id="p-0050" num="0049">In another embodiment a light projector may be attached next to the camera <b>114</b> on the bendable arm <b>301</b>. The light projector can project a targeting pattern, similar in scale and size to target <b>402</b>, onto the bed. The clinician can then manipulate the bendable arm <b>301</b> until the targeting pattern projected by the light projector is aligned with the target area of the patient's body, at which point the camera <b>114</b> is in the correct position for obtaining accurate and reliable data. In this method, the clinician need not look at or consult the monitor in order to properly align the camera <b>114</b>. Once camera <b>114</b> is in the correct position, the light projector may be switched off.</p><p id="p-0051" num="0050">In some embodiments, the software operated by the non-contact patient monitoring system may be designed such that the software automatically recognizes when the patient target area is located within the target and subsequently visually changes the target <b>402</b> when the patient target area is correctly positioned within the target <b>402</b> (or provides some other form of visual or audio indication of proper alignment). In such embodiments, the software includes one or more forms of recognition software capable of recognizing when the patient target area is located within the target area <b>402</b>. For example, the software may be able to recognize the general shape of a human torso and therefore provides a visual and/or audio alarm when a torso shape is recognized within the target area <b>402</b>. The recognition software may also use other parts of the patient to determine when a torso is located within the target area <b>402</b>. For example, facial recognition software can be used to identify a patient's face in the image <b>401</b>, and then determine a torso location based on the facial recognition and the probable location of the torso relative to the identified face. Once the torso is located in this manner, the software can provide a visual and/or audio indication when the identified torso is positioned within the target area <b>402</b>. Any visual and/or audio feedback can be used. In some embodiments, the color of the target <b>402</b> changes from, e.g., red to green once the torso is identified as being located within the target area <b>402</b>.</p><p id="p-0052" num="0051">In some embodiments, the target <b>402</b> superimposed on the image <b>401</b> displayed on display <b>400</b> is stationary and does not change location, shape or size. In other embodiments, the location, shape and/or size of the target <b>402</b> can be dynamic. For example, once the camera <b>114</b> is initially located in the proper position such that the patient torso (or other patient target area) is located within the target <b>402</b>, the target may &#x201c;lock on&#x201d; to the torso region. If the camera <b>114</b> is subsequently moved, the target <b>402</b> can change shape, size and or location on the display <b>400</b> to stay locked on to the previously identified torso. Such tracking may also be useful if the camera <b>114</b> remains in place, but the patient moves.</p><p id="p-0053" num="0052">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the orientation of the patient image <b>401</b> on display <b>400</b> is oriented vertically such that the head of the patient is at the top of the display <b>400</b>. In some embodiments, a user interface included on display <b>400</b> may include an option for rotating the orientation of the image <b>401</b>. With reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a button <b>601</b> is provided that allows for rotating the patient image <b>401</b> 90 degrees in a clockwise or counterclockwise direction. This feature may be advantageous in situations where the camera <b>114</b> is properly located and oriented (e.g., in parallel with the longitudinal axis of the patient), but the camera <b>114</b> is inverted such that the image transmitted to the display <b>400</b> presents the patient's head at the bottom of the display <b>400</b>. This orientation can be easily changed by rotating the image 180 degrees using, e.g., button <b>601</b>, and avoids the need for the clinician to instead further manipulate the camera <b>114</b> in order to get the desired image orientation. The ability to rotate the image also allows for horizontal orientation of the patient image <b>401</b> for clinicians that prefer a horizontal patient image orientation on display <b>400</b>.</p><p id="p-0054" num="0053">In some embodiments, the initial orientation of image <b>401</b> on display <b>400</b> may be automated using facial recognition software or other means. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, facial recognition software is used to identify the patient face <b>701</b> in patient image <b>401</b>. Once the face <b>701</b> is detected, image <b>401</b> is automatically oriented on display <b>400</b> such that the detected face <b>701</b> is at the top of the display <b>400</b>. Any method of facial recognition can be used, including video streams of the patient other than depth sensing images. For example, a RGB stream, an IR stream, thermal imaging, or any other modality or combination of modalities can be used to locate the patient face using facial recognition software.</p><p id="p-0055" num="0054">In addition to the location of camera <b>114</b> over the patient, the distance the camera <b>114</b> is placed away from the patient can also play a role in the accuracy and reliability of the data obtained from the camera <b>114</b>. For example, in some embodiments, it is desirable for the camera to be 0.9 to 1.3 meters away from the patient. A camera <b>114</b> that is positioned too close or too far away from the patient may reduce the reliability and/or quality of the data obtained from the camera <b>114</b>. Thus, in some embodiments, the systems and methods described herein further incorporate a means for providing the clinician with a distance measurement that measures the distance between the camera <b>114</b> and the patient. In some embodiments, this functionality may further include assisting and/or alerting the clinician to instances where the camera <b>114</b> is placed too far or too close to the patient, and/or to instances where the camera <b>114</b> is positioned at a desired distance from the patient to help ensure improved data collection.</p><p id="p-0056" num="0055">With reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the display <b>800</b> may include a distance reading <b>801</b>. While shown in the bottom right hand corner of display <b>800</b>, it should be appreciated that the distance reading <b>801</b> may be located anywhere on display <b>800</b>. The display <b>800</b> and associated software may be configured such that a cursor <b>802</b> can be positioned over any part of the patient image to measure the distance between the camera <b>114</b> and the location on the patient designated by the cursor <b>802</b>. This distance measurement <b>801</b> can then be used by the clinician to decide whether the camera <b>114</b> is at a good distance from the patient for reliable and accurate data collection, or whether the camera <b>114</b> needs to be moved further or closer towards the patient. Any manner of moving the cursor <b>802</b> about the display <b>800</b> can be used, such as through the use of a touch screen or a mouse.</p><p id="p-0057" num="0056">In some embodiments, the ideal distance or range of distances is known or provided to the clinician for manual determination of whether the distance of the camera <b>114</b> away from the patient needs to be changed. That is to say, the clinician changes the distance of the camera <b>114</b> and re-checks the distance reading <b>801</b> until the clinician manually confirms that the distance reading <b>801</b> is within the known or provided distance range. In other embodiments, an ideal distance or range of distances is input into the system <b>100</b>, and the display <b>800</b> and associated software provide automatic feedback regarding whether the camera <b>114</b> is at an ideal distance away from the patient or outside an ideal distance or distance range. The automatic feedback can be any type of audio and/or visual feedback. In some embodiments, the color of the distance reading <b>801</b> changes when the distance is either in the desired range or outside the desired range. For example, the distance reading <b>801</b> may be presented in green when the distance is within the desired range, and the color of the distance reading <b>801</b> may dynamically change as the camera distance is changed, such as dynamically changing from green to red when the camera <b>114</b> is moved outside the desired distance range.</p><p id="p-0058" num="0057">As described previously with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, visual and/or audio feedback may also be provided to denote proper positioning and orientation of the camera <b>114</b>, such as through the use of software to identify a patient target area (such as a patient torso) and then determine when the identified patient target area is properly positioned within the target <b>402</b>. In some embodiments, the feedback provided for the distance of the camera <b>114</b> is combined with the feedback provided for the positioning/orientation of the camera <b>114</b>. For example, the target <b>402</b> may be colored red until both the positioning/orientation of the camera <b>114</b> and the distance of the camera <b>114</b> are correct, at which point the target may change to a green color, indicating that the camera is in an optimal position for data collection. In some embodiments, if only one parameter is achieved (i.e., only correct camera position/orientation or only correct camera distance), the target <b>402</b> remains red until both parameters are met. In other embodiments, the color of target <b>402</b> may change from red to yellow when one parameter is met and from yellow to green when both parameters are met. Combining the feedback from both camera positioning/orientation and camera distance provides a relatively simple and streamlined process for assisting the clinician with proper location (both position and distance) of camera <b>114</b>.</p><p id="p-0059" num="0058">While the above description and <figref idref="DRAWINGS">FIG. <b>8</b></figref> relate to the use of a moveable cursor to set a distance measurement, the target <b>402</b> may also be configured to include a fixed location within the target and from which is obtained the distance measurement. For example, a fixed crosshair can be provided in the center of target <b>402</b> such that the distance reading is always obtained from the center of the target <b>402</b>, rather than requiring the clinician to manually set the cursor <b>802</b> before obtaining a distance measurement. The set location within the target <b>402</b> that distance is always measured from need not be in the center of the target <b>402</b>, but can instead be located anywhere within target <b>402</b>. The target may also include more than one fixed points for measuring distance. In such embodiments, the distance measured from each of the fixed points established within the target <b>402</b> may be averaged (including using weighted averages for each point, if desired), and the clinician may be provided with the averaged distance for determining if the camera is at an appropriate distance away from the patient.</p><p id="p-0060" num="0059">With reference back to <figref idref="DRAWINGS">FIGS. <b>3</b>A, <b>3</b>B and <b>3</b>C</figref>, the camera <b>114</b> may be attached to the distal end of a bendable/movable arm <b>301</b>. Any type of bendable/movable arm <b>301</b> may be used provided that the arm <b>301</b> is capable of moving the camera <b>114</b> to different locations about the patient. The bendable arm <b>301</b> should also be able to hold and retain the camera <b>114</b> in whatever location the clinician moves it to. The length of the bendable arm <b>301</b> is generally not limited, but may preferably be of a length that allows for the camera <b>114</b> to be located at any location about the patient.</p><p id="p-0061" num="0060">As shown in <figref idref="DRAWINGS">FIG. <b>3</b>C</figref>, the bendable arm <b>301</b> may have a bendable component <b>301</b><i>a </i>and a fixed component <b>301</b><i>b</i>. In some embodiments, the fixed component <b>301</b><i>b </i>is aligned generally vertically. This vertical fixed component <b>301</b><i>b </i>may be telescoping to allow the bendable component <b>310</b><i>a </i>to be raised or lowered, and the fixed component <b>301</b><i>b </i>may be rotatable to allow for further re-positioning and movement of the bendable component <b>301</b><i>a. </i></p><p id="p-0062" num="0061">In some embodiments, camera <b>114</b> is connected to the distal end of the bendable arm <b>301</b> via a connection mechanism <b>310</b>. The connection mechanism <b>310</b> is generally not limited provided that the connection mechanism <b>310</b> maintains a connection between the camera <b>114</b> and the bendable arm <b>301</b>. The connection mechanism may allow for varying degrees of freedom of the camera <b>114</b> relative to the bendable arm. In some embodiments, the connection mechanism includes or incorporates a gimbal in order to allow for free movement of the bendable arm <b>301</b> but without altering a desired orientation of camera <b>114</b>. For example, in some embodiments, it may be desirable that the camera <b>114</b> (and more specifically the lens or lenses of the camera <b>114</b>) be fixedly oriented such that the camera/lens is always aligned parallel to the bed on which the patient is positioned. This camera/lens orientation may be desirable as a means for helping to ensure accurate and reliable data collection. For example, if the camera/lens is oriented at an angle with respect to the bed when directed at the patient, the depth sensing data obtained from the camera may be less reliable/accurate than if the camera/lens is positioned to be facing directly down on a patient (i.e., camera/lens aligned in parallel with the bed). By incorporating a gimbal as part of the connection mechanism <b>310</b>, the camera <b>114</b> is generally able to be moved to any location about the patient (through manipulation of the bendable arm <b>301</b>) but without changing the orientation of the camera/lens. That is to say, no matter where the camera <b>114</b> is located about the patient via movement of the bendable arm <b>301</b>, the orientation of the camera <b>114</b> in a &#x201c;parallel to the bed&#x201d; position remains the same through the use of the gimbal.</p><p id="p-0063" num="0062">While a gimbal component of connection mechanism <b>310</b> is generally described as being used to ensuring that the camera <b>114</b> remains oriented parallel to the bed on which the patient is disposed, it should be appreciated that the general, non-limiting, purpose of the gimbal may be to maintain a line of sight of the camera that is approximately orthogonal to the patient's chest, and that this orientation may require the camera and/or lens to be positioned differently from the above description depending on the specific camera and/or lens configuration. Regardless, the gimbal component may be used in any way necessary to help ensure this &#x201c;line of sight orthogonal to patient's chest&#x201d; alignment.</p><p id="p-0064" num="0063">The gimbal may include the ability to lock and unlock the orientation of the camera <b>114</b>. When in the locked position, the gimbal maintains the orientation of the camera <b>114</b> (e.g., in a &#x201c;parallel to the horizon&#x201d; orientation) regardless of where the camera is moved via manipulation of the bendable arm <b>301</b>. When in the unlocked position, the gimbal may provide freedom of movement to the orientation of the camera such that it is not retained in a fixed &#x201c;parallel to the horizon&#x201d; orientation as the bendable arm is moved to change the position of the camera <b>114</b>. The unlocked feature may provide flexibility for unique situations when the clinician requires the camera orientation to be something other than in the &#x201c;parallel to the horizon&#x201d; configuration.</p><p id="p-0065" num="0064">In some embodiments, the bendable arm <b>301</b> and/or connection mechanism <b>310</b> may include one or more locking mechanism to lock the camera <b>114</b> in position once the camera <b>114</b> has been located where the patient target area is located within the target <b>402</b>. Any suitable locking mechanism that prevents further movement of the bendable arm <b>301</b> and/or the connection mechanism <b>310</b> can be used. The ability to lock the camera <b>114</b> in position after it has been correctly located by the clinician may help to avoid situations where the camera is inadvertently bumped or moved after it has been correctly positioned. As such, this may help to ensure that the patient target area remains within the target <b>402</b>. As noted previously, the software associated with the system <b>100</b> may include tracking technology such that the target <b>402</b> moves and/or changes shape or size if the patient moves. Such tracking technology may be specifically suitable for situations where the camera <b>114</b> is locked in place such that the patient target area remains within the target <b>402</b> even if a patient moves.</p><p id="p-0066" num="0065">The connection mechanism <b>310</b> of the bendable arm <b>301</b> may also include actuators, for example a servo motor or any device that allows kinematic manipulation via a control input signal. The bendable arm <b>301</b> can then be manipulated so that the camera <b>114</b> is located at the optimum location to collect the most beneficial data. Manipulation of the bendable arm <b>301</b> can be performed automatically by analyzing the camera <b>114</b> images and subsequently adjusting the bendable arm <b>301</b> so that the most beneficial data can be collected.</p><p id="p-0067" num="0066">With reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a method <b>900</b> for assisting in the proper placement of a non-contact detector used in a video-based non-contact patient monitoring system may generally include a step <b>910</b> of obtaining from a non-contact detector a video signal, the video signal encompassing at least a torso region of a patient; a step <b>920</b> of displaying on a display screen a video based on the video signal; a step <b>930</b> of superimposing a target over a portion of the video displayed on the display screen; and a step <b>940</b> of providing an indication that the torso region of the patient in the displayed video is located within the target. As discussed in greater detail previously, steps <b>910</b> and <b>920</b> generally related to the use of the system <b>100</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, wherein the non-contact detector <b>110</b> (such as camera <b>114</b>) provides a video signal of the patient, and a video of the patient based on the video signal is displayed on display screen <b>122</b>. Step <b>930</b> generally relates to the target <b>402</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, and more specifically how the target <b>402</b> is superimposed over a portion of the video displayed on the display screen. In step <b>940</b>, the methods described herein generally work to provide an indication or alert to the clinician to let the clinician know when the camera <b>114</b> has been properly positioned to best ensure the collection of accurate and reliable data from the camera. As discussed in greater detail previously, such an indication is typically provided to the clinician when the camera <b>114</b> has been positioned such that the target <b>402</b> is encompassing the torso region of the patient as displayed on the display screen. As also described in greater detail previously, the indication or alert may be provided in the form of a visual and/or audio alert, such as the target <b>402</b> changing color (e.g., from red to green) when the camera <b>114</b> is properly positioned.</p><p id="p-0068" num="0067">Various previously described embodiments generally relate to systems and methods wherein a screen or display is visible to the clinician when moving the camera such that the clinician can use the screen or display to confirm when the camera is properly positioned. That is to say, the screen or display is located sufficiently local to the camera that while the clinician manipulates the camera, the clinician can view and use the display to confirm when the camera is properly positioned (e.g., when the patient's torso as displayed on the screen is located within a target superimposed on the screen of the display). This configuration is shown in, e.g., <figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref>, wherein a screen or display is mounted directly to a portion of the bendable arm <b>301</b> having a camera <b>114</b> attached to a distal end thereof. However, it should be appreciated that in some configurations, a screen or display proximate the camera <b>114</b> will not be available. For example, a screen configured to display the image obtained from the camera <b>114</b> may be located outside of the room in which the camera <b>114</b> and patient being monitored are located, such as at a central monitoring location (e.g., nurse's station). In such embodiments, the clinician is not able to rely on the display to confirm proper positioning of the camera <b>114</b>, and therefore other features must be provided to assist with proper camera placement.</p><p id="p-0069" num="0068">In some embodiments, systems and methods wherein a display is not locally available include a projector configured to project a target on to an object (e.g., the patient, the patient's bed, etc.) located within the camera's field of view. This configuration is generally shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. As previously described, camera <b>114</b> is moveable such that camera <b>114</b> can be positioned such that the patient <b>112</b> or a portion of patient <b>112</b> is within the field of view of the camera <b>112</b>. A projector <b>1050</b> is mounted on or otherwise secured to or with the camera <b>114</b>, with the lens of the projector <b>1050</b> facing generally in the same direction as the lens of the camera <b>114</b>. In this manner, the projector <b>1050</b> is capable of projecting on to the patient <b>112</b> a target <b>1051</b>, with the target <b>1051</b> being within the field of view of the camera <b>114</b>.</p><p id="p-0070" num="0069">In some embodiments, the manner in which the projector <b>1050</b> is secured to or with the camera <b>114</b> and otherwise calibrated is such that when a targeted area of the patient <b>112</b> (e.g., the patient's torso) is located within the projected target <b>1051</b>, the camera <b>114</b> is in a position capable of obtaining depth data suitable for use in calculating a patient breathing parameter. Calibration of the positioning and/or alignment of the projector <b>1050</b> connected to the camera <b>114</b> can be carried out in a suitable way, including by setting the camera <b>114</b> in the desired position where sufficient depth data can be obtained, and then manipulating the positioning and/or other settings of the projector until the target <b>1050</b> is appropriately aligned (e.g., encompasses the desired portion of the patient <b>112</b>). Following calibration, subsequent use of the camera <b>114</b> and projector <b>1050</b> should ensure that when the projected target <b>1051</b> encompasses the targeted portion of the patient <b>112</b>, the camera <b>114</b> will be in a position to acquire the required depth sensing data for determining a patient breathing parameter.</p><p id="p-0071" num="0070">The specific visual scheme or appearance of the target <b>1051</b> projected by projector <b>1050</b> is generally not limited. That is to say, the shape, size, color, etc., of the target <b>1051</b> may be of any desired visual appearance. <figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates four exemplary, though non-limiting, targets <b>1051</b><i>a </i>through <b>1051</b><i>d</i>. In one embodiment (image A of <figref idref="DRAWINGS">FIG. <b>11</b></figref>), target <b>1051</b><i>a </i>projected on patient <b>112</b> has a relatively large rectangular shape whose entire interior is colored yellow. In another embodiment (image B of <figref idref="DRAWINGS">FIG. <b>11</b></figref>), target <b>1051</b><i>b </i>is similar in shape, color and fill to target <b>1051</b><i>a </i>shown in image A, but the size of the rectangle is smaller, which can allow for targeting of more localized regions of the patient <b>112</b>. In another embodiment (image C of <figref idref="DRAWINGS">FIG. <b>11</b></figref>), target <b>1051</b><i>c </i>is a similar shape and size to target <b>1051</b><i>a </i>shown in image A, but does not include a colored interior. Instead, the target <b>1051</b><i>c </i>uses a yellow outline to denote the boundaries of the target. In another embodiment (image D of <figref idref="DRAWINGS">FIG. <b>11</b></figref>), target <b>1051</b><i>d </i>is a similar shape and size to target <b>1051</b><i>a </i>shown in image A, but does not include a colored interior or a boundary line extending around the entirety of the shape. Instead, the target <b>1051</b><i>d </i>uses yellow corner segments to denote the boundaries of the target.</p><p id="p-0072" num="0071">While <figref idref="DRAWINGS">FIG. <b>11</b></figref> provides some examples of the visual representation of target <b>1051</b> as projected by projector <b>1050</b>, other visual representations of the target <b>1051</b> can be used. For example, the shape of the target <b>1051</b> may be a circle, ellipses, triangle, or any regular or irregular shape. In some embodiments where an irregular shape is used, the shape of the target <b>1051</b> is the outline of a human torso, which can further aid in positioning the camera (i.e., the camera is properly positioned when the torso-shaped target is aligned with the torso of the patient <b>112</b>). As noted previously with respect to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the target <b>1050</b>, regardless of shape and size, can have a colored interior portion or a clear interior portion, in which case the outline of the shape denotes the target <b>1051</b>. When the interior of the shape is colored, the coloring may be a solid coloring or a patterned coloring. When an outline is used for target <b>1051</b>, the outline can be solid lines, dashed lines, corners segments only, or any other pattern. The specific color used for the target <b>1051</b> is also not limited.</p><p id="p-0073" num="0072">The projector <b>1050</b> may be configured, in connection with other components of the system <b>100</b> previously described (e.g., computing device <b>115</b>, processor <b>118</b>, etc.) such that the projector <b>1050</b> only projects target <b>1051</b> at certain times and/or under certain conditions. In a simple configuration, the projector <b>1050</b> projects target <b>1051</b> when a switch associated with the projector <b>1050</b> is turned on. The switch can be, for example, a piece of hardware that is depressed, turned or otherwise maneuvered, or an icon on a touch screen associated with the projector <b>1050</b> or some other component of system <b>100</b>. In some embodiments, the projector <b>1050</b> and/or an associated component of system <b>100</b> that is communicatively coupled with the projector <b>1050</b> includes a timer such that when the target <b>1051</b> is turned on, the target <b>1051</b> remains on for a predetermined period of time monitored by the timer. For example, once turned on, the target <b>1051</b> may remain on for a predetermined period of time (e.g., 30 second, 60 seconds, 90 seconds, etc.), after which the projector <b>1050</b> is automatically turned off. The predetermined period of time that the projector <b>1050</b> remains on may be selected based on the amount of time typically needed for a clinician to correctly position the camera <b>114</b> over the patient <b>112</b>. In other embodiments, the projector <b>1050</b> remains on until the manually turns off the projector <b>1050</b>.</p><p id="p-0074" num="0073">In some embodiments, a switch or other manual means is not used to turn on the projector <b>1050</b>, but instead the projector <b>1050</b> is configured to automatically turn on when motion of the camera <b>114</b> to which the projector <b>1050</b> is attached, or motion of the projector <b>1050</b> itself, is detected. Motion of the projector <b>1050</b> and/or camera <b>114</b> can be determined using any suitable means. In some embodiments, the projector <b>1050</b> and/or camera <b>114</b> is equipped with an accelerometer, and motion detected by the accelerometer results in the projector <b>1050</b> being automatically turned on. In other embodiments, a sufficient change in the depth data being collected by the camera <b>114</b> initiates the projector <b>1050</b>. For example, if the camera <b>114</b> collects depth data indicating that the camera is 1.1 meters away from the patient <b>112</b> and this measurement remains constant over a period of time, then the projector <b>1050</b> remains off. However, once the camera <b>114</b> begins to collect different depth data (e.g., that the camera is now 1.5 meters away from the patient), the assumption is that the camera <b>114</b> is being moved, and therefore the projector <b>1050</b> is automatically turned on to assist the clinician in accurately and correctly positioning the camera <b>114</b>.</p><p id="p-0075" num="0074">Regardless of the manner in which the projector <b>1050</b> is automatically turned on (e.g., via accelerometer, via change in depth reading, etc.), the projector <b>1050</b> can be programmed to automatically turn off after a predetermined period of time as described previously. Alternatively, the projector <b>1050</b> can be programmed so that is remains on until motion of the camera <b>114</b> and/or projector <b>1050</b> has ceased for longer than a predetermined period of time. For example, an accelerometer associated with the camera <b>114</b> and/or projector <b>1050</b> can detect motion, at which point the projector <b>1050</b> is automatically turned on. The accelerometer can continue to sense motion for the next 3 minutes, and as a result, the projector <b>1050</b> remains on. However, once the accelerometer senses no movement for longer than a predetermined period of time (e.g., 15 seconds, 30 second, etc.), the projector <b>1050</b> can be automatically turned off. A similar manner of turning off the projector <b>1050</b> can be used when changes in depth measurements are used to detect motion of the camera <b>114</b>. In such embodiments, once the camera <b>114</b> determines that the depth measurement has stopped changing for longer than a predetermined period of time (and after previously detecting changes in depth data such that the projector <b>1050</b> has been turned on) the projector <b>1050</b> can be automatically turned off.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an exemplary method <b>1200</b> for automatically turning the projector <b>1050</b> on when motion of the camera <b>114</b> is detected. At step <b>1201</b>, it is determined whether motion of the camera <b>114</b> is detected. Detecting motion of the camera <b>114</b> can be by any means, including those described previously (e.g., via use of an accelerometer). If motion is not detected, then no action is taken at step <b>1201</b>. If motion is detected at step <b>1201</b>, then a timer may be initiated at step <b>1202</b>. Because camera <b>114</b>, projector <b>1050</b>, and means for determining motion (e.g., accelerometer) are communicatively coupled with system <b>100</b>, the detection of motion may be communicated to, for example, the computing device <b>115</b> of the system <b>100</b>, which may then initiate the timer. At step <b>1203</b>, it is determined whether the detected motion persists for longer than a predetermined period of time monitored by the time (e.g., longer than 3 seconds, longer than 5 seconds, longer than 10 seconds, etc.). Step <b>1203</b> helps to ensure that any minor motion of the camera of minimum duration, such as when the camera is inadvertently and only momentarily bumped by a clinician, does not initiate the projector <b>1050</b>. Instead, the motion must be prolonged, thus serving as a better indication of the camera <b>114</b> being moved, and hence the need for the projector <b>1050</b> to be turned on to aid in positioning the camera <b>114</b>. If motion is detected for longer than the predetermined period of time at step <b>1203</b>, then method <b>1200</b> proceeds to step <b>1204</b>, in which the projector <b>1050</b> is automatically turned on to thereby project target <b>1051</b>.</p><p id="p-0077" num="0076">As discussed previously, target <b>1051</b> may generally be in the form of a regular or irregular shape of any size and/or color. In some embodiments, the projector further projects, such as part of target <b>1051</b>, additional images to provide the clinician with additional information. In some embodiments, the additional information projected by projector <b>1050</b> relates to the distance between the camera <b>114</b> and the patient <b>112</b>. For example, the projector <b>1050</b> may also project on to the patient <b>112</b> (either inside of or separate from target <b>1051</b>) a number indicating the distance between the camera <b>114</b> and the patient <b>112</b>. Projection of this distance number may aid the clinician in ensuring that the camera <b>114</b> is positioned a desirable distance away from the patient <b>112</b>. For example, it may be desirable that the camera <b>114</b> is positioned about 1.1 meters away from the patient <b>112</b> to ensure collection of reliable data to be used in determining a patient breathing parameter, and therefore the clinician may move the camera <b>114</b> closer or further away from patient <b>112</b> until the projected depth reading is at or close to 1.1 meters. The projected distance may change in real time or near real time as the camera <b>114</b> is moved.</p><p id="p-0078" num="0077">Other manners for visually representing that the camera <b>114</b> is positioned a desired distance from the patient <b>112</b> can also be used. For example, a check mark can be projected (inside or outside of target <b>1051</b>) when the distance between the camera <b>114</b> and the patient <b>112</b> is determined to be within a desirable range (e.g., between 1.0 and 1.2 meters). The projector <b>1050</b> may also add text (inside or outside of targe <b>1051</b>), such as &#x201c;Good Alignment&#x201d;, when the camera <b>114</b> is located a distance away from patient <b>112</b> that is within the desired range. In other embodiments, the visual representation of the target <b>1051</b> is changed when the camera <b>114</b> is determined to be a distance away from the patient <b>112</b> that falls within a desired range. In such embodiments, the target <b>1051</b> may include at least a first visual scheme and a second visual scheme. The projector <b>1050</b> projects the target <b>1051</b> using the first visual scheme when the camera <b>114</b> is outside of the desired distance range. Once the camera <b>114</b> is moved to a distance within the desired range, the projector <b>1050</b> changes the projection of target <b>1051</b> such that the second visual scheme is used, and which thereby denotes to the clinician that camera <b>114</b> is at an acceptable distance away from the patient <b>112</b>.</p><p id="p-0079" num="0078">The change from the first visual scheme to the second visual scheme for target <b>1051</b> can be any desired change in visual scheme. In some embodiments, the first visual scheme uses a red color scheme to thereby denote the distance of camera <b>114</b> from patient <b>112</b> is outside of the desired range, and the second visual scheme uses a green color scheme to thereby denote the distance of camera <b>114</b> from patient <b>112</b> is within the desired range. Other changes from the first visual scheme may include changes in the shape or size of target <b>1051</b>, or any combination thereof.</p><p id="p-0080" num="0079">In addition to or as an alternative to changing the visual scheme of target <b>1051</b> when camera <b>114</b> is within the desired range of distances, the camera and/or projector may include a screen, display, light, or other indicator to help indicate when camera <b>114</b> is within a desired range of distance from patient <b>112</b>. For example, a small screen or display may be associated with either the camera <b>114</b> or the projector <b>1050</b>, and the screen or display may be used to indicate when the distance of the camera <b>114</b> from the patient <b>112</b> is within a desired range. In another embodiment, one or more lights may be associated with the camera <b>114</b> and/or projector <b>1050</b>, and the light may be used to indicate when the camera <b>114</b> is a desired distance away from the patient <b>112</b>. In such embodiments, the light may turn from off to on to denote correct distance, or may change from one color (e.g., red) to another color (e.g., green).</p><p id="p-0081" num="0080">The specific distance used to determine alignment of camera <b>114</b> is generally not limited. In some embodiments, the measurement used to determine when the distance between the camera <b>114</b> and the patient <b>112</b> is within a desired range is the distance from the camera to a center point of the target <b>1051</b>. In another embodiment, the measurement used to determine when the distance between the camera <b>114</b> and the patient <b>112</b> is within a desired range is the average of all distance data points, or some subset of distance data points, within the field of view of the camera <b>114</b>.</p><p id="p-0082" num="0081">Once proper alignment of camera <b>114</b> has been established (both in terms of distance of camera <b>114</b> from patient <b>112</b> and the positioning of camera <b>114</b> such that the desired portion of patient <b>112</b> is within target <b>1051</b>), system <b>100</b> can be configured to store an image of the patient <b>112</b>. The stored image may then become a reference image used to periodically or continuously check whether the camera <b>114</b> remains in proper alignment. If system <b>100</b> detects a difference between the reference image and the current image that exceeds a threshold value, then the system <b>100</b> may initiate an alarm intended to indicate to the clinician that the camera <b>114</b> is no longer in good alignment with the patient <b>112</b>. Camera <b>114</b> may fall out of good alignment for any of a variety of reasons, including, for example, inadvertent movement of the camera <b>114</b>, movement of the patient <b>112</b>, movement of the patient's bed, etc. Any type of alert may be triggered in this scenario, including a visual alert, an audio alert, etc., including any combination of alerts. Visual alerts may be displayed as part of the target <b>1051</b>, on a separate display screen associated with system <b>100</b>, on the projector <b>1050</b> and/or camera <b>114</b>, etc. For example, a screen, display, or light associated with the camera <b>114</b> or projector <b>1050</b> as described previously may be used to display a visual alert. Such visual alert could include a screen, display or light turning red, or a screen, display or light flashing.</p><p id="p-0083" num="0082">For the sake of simplicity and example, the technology described herein has been disclosed with respect to the use of the systems and methods for monitoring patient breathing parameters, and therefore has focused primarily on instances where a targeting aid is used to ensure a camera is properly positioned to view a patient's torso. However, it should be appreciated that embodiments and aspects described herein are equally applicable to monitoring other patient parameters and/or other portions of a patient's body. For example, the systems and method described herein are equally applicable to using a target to ensure that a camera is properly placed to be aimed at a patient's head for collecting data pertaining to the patient's head. In such examples, the systems and methods may be used for monitoring patient temperature, in which case the camera is a temperature sensing camera, and the target is used to ensure that the camera is focused on portions of the patient's head from which reliable and accurate temperature information can be obtained. Numerous other examples for other patient body segments and patient parameters apply.</p><p id="p-0084" num="0083">From the foregoing, it will be appreciated that specific embodiments of the invention have been described herein for purposes of illustration, but that various modifications may be made without deviating from the scope of the invention. Accordingly, the invention is not limited except as by the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>I/We claim:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A video-based patient monitoring method, comprising:<claim-text>obtaining from a non-contact detector a video signal, the video signal encompassing at least a torso region of a patient;</claim-text><claim-text>displaying on a display screen a video based on the video signal;</claim-text><claim-text>superimposing a target over a portion of the video displayed on the display screen; and</claim-text><claim-text>providing an indication that the torso region of the patient in the displayed video is located within the target.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the non-contact detector is a video camera.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the video camera is a depth-sensing video camera.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the target superimposed over a portion of the video displayed on the display screen is vertically centered on the video displayed on the display screen.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the target comprises a geometric shape defined by solid lines, broken lines or broken corner lines.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the target comprises a shaded area.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the target comprises an unshaded area and the non-target area of the video is shaded.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>automatically identifying when the torso region of the patient is positioned within the target; and</claim-text><claim-text>visually changing the appearance of the target when the torso region of the patient is positioned within the target, providing an audible sound when the torso region of the patient is positioned within the target, or both.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>automatically identifying when a vertical distance between the patient and the non-contact detector falls within a predetermined range.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein visually changing the appearance of the target when the torso region of the patient is positioned within the target, providing an audible sound when the torso region of the patient is positioned within the target, or both only occurs if the vertical distance between the patient and the non-contact detector falls within the predetermined range.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising prompting a user to align the target with the torso region of the patient.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A video-based patient monitoring method, comprising:<claim-text>displaying on a display screen a patient image based on a video signal obtained from a video camera, the patient image having superimposed thereon a target encompassing a portion of the patient image; and</claim-text><claim-text>manipulating a bendable arm to which the video camera is attached to reposition the video camera until a patient target area in the patient image is located within the target on the display screen;</claim-text><claim-text>wherein a connection between the bendable arm and the video camera includes a gimbal, the gimbal being configured to maintain the orientation of the video camera in a position generally parallel to a bed on which the patient is positioned during manipulation of the bendable arm.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the video camera is a depth-sensing video camera.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>during manipulating the bendable arm, automatically identifying when the patient target area is positioned within the target; and</claim-text><claim-text>visually changing the appearance of the target when the patient target area is positioned within the target, providing an audible sound when the patient target area is positioned within the target, or both.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:<claim-text>during manipulating the bendable arm, automatically identifying when a vertical distance between the patient and the video camera falls within a predetermined range.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein visually changing the appearance of the target when the patient target area is positioned within the target, providing an audible sound when the patient target area is positioned within the target, or both only occurs if the vertical distance between the patient and the non-contact detector falls within the predetermined range.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A video-based patient monitoring system, comprising:<claim-text>a video camera configured to obtain a video signal;</claim-text><claim-text>a bendable arm attached at a distal end to the video camera; and</claim-text><claim-text>a display, the display configured to:<claim-text>display a patient image based on the video signal; and</claim-text><claim-text>superimpose over the patient image a target;</claim-text></claim-text><claim-text>wherein the video camera is moveable about a patient via manipulation of the bendable arm; and</claim-text><claim-text>wherein the system is configured to:</claim-text><claim-text>automatically determine when a target patient area of the patient image is located within the target superimposed on the patient image via manipulation of the bendable arm and corresponding repositioning of the video camera; and</claim-text><claim-text>provide visual and/or audible feedback when the system automatically determines that the target patient area of the patient image is located within the target superimposed on the patient image.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of claim <b>18</b>, wherein the system provides visual feedback, and the visual feedback comprises the color of the target changing from a first color to a second color when the target patient area of the patient image is located within the target superimposed on the patient image.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of claim <b>18</b>, wherein the system further comprises:<claim-text>an attachment mechanism configured to attach the video camera to a distal end of the bendable arm, the attachment mechanism comprising at least a gimbal configured to maintain the video camera in an orientation generally parallel to a bed on which the patient is positioned during manipulating of the bendable arm.</claim-text></claim-text></claim></claims></us-patent-application>