<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007260A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221220" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007260</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17775565</doc-number><date>20201109</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>13</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>126</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>91</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>13</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>126</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>91</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Probability Estimation for Video Coding</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62932508</doc-number><date>20191108</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Google LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Han</last-name><first-name>Jingning</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Sun</last-name><first-name>Yue</first-name><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Xu</last-name><first-name>Yaowu</first-name><address><city>Saratoga</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/US2020/059594</doc-number><date>20201109</date></document-id><us-371c12-date><date>20220509</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Entropy coding a sequence of symbols is described. A first probability model for entropy coding is selected. At least one symbol of the sequence is coded using a probability determined using the first probability model. The probability according to the first probability model is updated with an estimation of a second probability model to entropy code a subsequent symbol. The combination may be a fixed or adaptive combination.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="178.39mm" wi="80.35mm" file="US20230007260A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="201.85mm" wi="82.38mm" file="US20230007260A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="194.48mm" wi="170.18mm" orientation="landscape" file="US20230007260A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="241.98mm" wi="119.46mm" orientation="landscape" file="US20230007260A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="194.82mm" wi="125.98mm" orientation="landscape" file="US20230007260A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="228.85mm" wi="159.94mm" file="US20230007260A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="203.45mm" wi="155.02mm" file="US20230007260A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="119.13mm" wi="79.93mm" file="US20230007260A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="233.85mm" wi="136.57mm" file="US20230007260A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims priority to U.S. Provisional Patent Application No. 62/932,508, filed Nov. 8, 2019, the entire content of which is incorporated wherein in its entirety by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Digital video streams may represent video using a sequence of frames or still images. Digital video can be used for various applications including, for example, video conferencing, high-definition video entertainment, video advertisements, or sharing of user-generated videos. A digital video stream can contain a large amount of data and consume a significant amount of computing or communication resources of a computing device for processing, transmission, or storage of the video data. Various approaches have been proposed to reduce the amount of data in video streams, including lossy and lossless compression techniques.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">Probability estimation is used for entropy coding, particularly with context-based entropy coding for lossless compression. A multimodal approach is described herein that uses multiple linear update models to accurately estimate probabilities.</p><p id="p-0005" num="0004">An aspect of the teachings herein is a method for entropy coding a sequence of symbols (i.e., multiple symbols). The method can include determining a first probability model for entropy coding the sequence, the first probability model being one of a plurality of available probability models, entropy coding at least one symbol of the sequence using a probability determined by the first probability model, after entropy coding a respective symbol of the sequence, determining a first probability estimation to update the probability using the first probability model, for a subsequent symbol relative to the at least one symbol of the sequence, determining a second probability estimation using a second probability model, and entropy coding the subsequent symbol using the probability updated by a combination of the first probability estimation and the second probability estimation.</p><p id="p-0006" num="0005">An aspect of the teachings herein is an apparatus for entropy coding a sequence of symbols including a processor. The processor is configured to determine a first probability model for entropy coding the sequence, the first probability model being one of a plurality of available probability models, entropy code at least one symbol of the sequence using a probability determined by the first probability model, after entropy coding a respective symbol of the sequence, determine a first probability estimation to update the probability using the first probability model, for a subsequent symbol relative to the at least one symbol of the sequence, determine a second probability estimation using a second probability model, and entropy code the subsequent symbol using the probability updated by a combination of the first probability estimation and the second probability estimation.</p><p id="p-0007" num="0006">Aspects of this disclosure are disclosed in the following detailed description of the implementations, the appended claims, and the accompanying figures.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007">The description herein makes reference to the accompanying drawings described below, wherein like reference numerals refer to like parts throughout the several views.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic of an example of a video encoding and decoding system.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an example of a computing device that can implement a transmitting station or a receiving station.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of an example of a video stream to be encoded and subsequently decoded.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of an example of an encoder.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of an example of a decoder.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating quantized transform coefficients according to implementations of this disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram of a coefficient token tree that can be used to entropy code blocks into a video bitstream according to implementations of this disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram of an example of a tree for binarizing a quantized transform coefficient according to implementations of this disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flow chart of a method for entropy coding a sequence of symbols according to the teachings herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0018" num="0017">Video compression schemes may include breaking respective images, or frames, into smaller portions, such as blocks, and generating an encoded bitstream using techniques to limit the information included for respective blocks thereof. The encoded bitstream can be decoded to re-create or reconstruct the source images from the limited information. The information may be limited by lossy coding, lossless coding, or some combination of lossy and lossless coding.</p><p id="p-0019" num="0018">One type of lossless coding is entropy coding, where entropy is generally considered the degree of disorder or randomness in a system. Entropy coding compresses a sequence in an informationally efficient way. That is, a lower bound of the length of the compressed sequence is the entropy of the original sequence. An efficient algorithm for entropy coding desirably generates a code (e.g., in bits) whose length approaches the entropy. For a sequence s with a length N, the entropy associated with binary codewords may be defined as equation (1), below:</p><p id="p-0020" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3a3;<sub>t=1</sub><sup>N</sup>&#x2212;log<sub>2</sub>(<i>p</i>(<i>s</i><sub>t</sub><i>|s</i><sub>t&#x2212;1</sub>, . . . ,1)):=&#x3a3;<sub>t=1</sub><sup>N</sup>&#x2212;log<sub>2</sub>(<i>p</i><sub>t</sub>(<i>s</i><sub>t</sub>))&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0021" num="0019">The variable p represents the probability of an individual symbol, and the variable p<sub>t </sub>represents the probability distribution of symbols at time t conditioned on the previously observed symbols. Arithmetic coding can use the probability to construct the codewords.</p><p id="p-0022" num="0020">However, a coder does not receive a streaming sequence of symbols together with the probability distribution for the symbols. Instead, probability estimation may be used in video codecs to implement entropy coding. That is, the probability distribution of the symbols may be estimated. Where the estimation is {circumflex over (p)}<sub>t</sub>, the codelength approaches equation (2) below:</p><p id="p-0023" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3a3;<sub>t=1</sub><sup>N</sup>&#x2212;log<sub>2</sub>(<i>{circumflex over (p)}</i><sub>t</sub>(<i>s</i><sub>t</sub>))&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0024" num="0021">Stated differently, entropy coding may rely upon probability estimation models (also called probability models herein) that model the distribution of values occurring in an encoded bitstream. By using probability models based on a measured or estimated distribution of values so that {circumflex over (p)}<sub>t </sub>is close to p<sub>t</sub>, entropy coding can reduce the number of bits required to represent the input data to close to a theoretical minimum (i.e., the lower bound).</p><p id="p-0025" num="0022">In practice, the actual reduction in the number of bits required to represent video data can be a function of the accuracy of the probability model, the number of bits over which the coding is performed, and the computational accuracy of the (e.g., fixed-point) arithmetic used to perform the coding. A significant difficulty in the estimation is that the probability is time variant, which means that p<sub>t </sub>cannot be replaced by a single value p.</p><p id="p-0026" num="0023">To address the time-variant nature of the probability, probability estimation is described herein that combines a probability estimation model, which is a first-order linear system, with another model to form a higher-order linear system. While the teachings herein may be used in either a one-pass or a two-pass coding system, the estimation of probability herein may be referred to as online estimation of probability because it is capable of use in a one-pass system with high efficiency. The available probability estimation models may be two or more models including a context-adaptive binary arithmetic coding (CABAC) model, an AV1 model, a counting model, or any other probability estimation model or algorithm.</p><p id="p-0027" num="0024">Implementations according to this disclosure can efficiently perform probability estimation for entropy coding, particularly with context-based entropy coding for lossless compression, by more accurately modeling the conditional probability of streaming symbols. The probability estimation contributes to efficient compression, reducing the number of bits required to represent video data. The probability estimation may be used in any probability estimation of a sequence of symbols but may be particularly effective for online probability estimation of such a sequence (e.g., real-time or delay sensitive applications of video coding).</p><p id="p-0028" num="0025">Further details of estimating the probability for entropy coding symbols are described herein first with reference to a system in which the teachings may be incorporated.</p><p id="p-0029" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic of an example of a video encoding and decoding system <b>100</b>. A transmitting station <b>102</b> can be, for example, a computer having an internal configuration of hardware such as that described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. However, other implementations of the transmitting station <b>102</b> are possible. For example, the processing of the transmitting station <b>102</b> can be distributed among multiple devices.</p><p id="p-0030" num="0027">A network <b>104</b> can connect the transmitting station <b>102</b> and a receiving station <b>106</b> for encoding and decoding of the video stream. Specifically, the video stream can be encoded in the transmitting station <b>102</b>, and the encoded video stream can be decoded in the receiving station <b>106</b>. The network <b>104</b> can be, for example, the Internet. The network <b>104</b> can also be a local area network (LAN), wide area network (WAN), virtual private network (VPN), cellular telephone network, or any other means of transferring the video stream from the transmitting station <b>102</b> to, in this example, the receiving station <b>106</b>.</p><p id="p-0031" num="0028">The receiving station <b>106</b>, in one example, can be a computer having an internal configuration of hardware such as that described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. However, other suitable implementations of the receiving station <b>106</b> are possible. For example, the processing of the receiving station <b>106</b> can be distributed among multiple devices.</p><p id="p-0032" num="0029">Other implementations of the video encoding and decoding system <b>100</b> are possible. For example, an implementation can omit the network <b>104</b>. In another implementation, a video stream can be encoded and then stored for later transmission to the receiving station <b>106</b> or any other device having memory. In one implementation, the receiving station <b>106</b> receives (e.g., via the network <b>104</b>, a computer bus, and/or some communication pathway) the encoded video stream and stores the video stream for later decoding. In an example implementation, a real-time transport protocol (RTP) is used for transmission of the encoded video over the network <b>104</b>. In another implementation, a transport protocol other than RTP may be used, e.g., a Hypertext Transfer Protocol-based (HTTP-based) video streaming protocol.</p><p id="p-0033" num="0030">When used in a video conferencing system, for example, the transmitting station <b>102</b> and/or the receiving station <b>106</b> may include the ability to both encode and decode a video stream as described below. For example, the receiving station <b>106</b> could be a video conference participant who receives an encoded video bitstream from a video conference server (e.g., the transmitting station <b>102</b>) to decode and view and further encodes and transmits his or her own video bitstream to the video conference server for decoding and viewing by other participants.</p><p id="p-0034" num="0031">In some implementations, the video encoding and decoding system <b>100</b> may instead be used to encode and decode data other than video data. For example, the video encoding and decoding system <b>100</b> can be used to process image data. The image data may include a block of data from an image. In such an implementation, the transmitting station <b>102</b> may be used to encode the image data and the receiving station <b>106</b> may be used to decode the image data. Alternatively, the receiving station <b>106</b> can represent a computing device that stores the encoded image data for later use, such as after receiving the encoded or pre-encoded image data from the transmitting station <b>102</b>. As a further alternative, the transmitting station <b>102</b> can represent a computing device that decodes the image data, such as prior to transmitting the decoded image data to the receiving station <b>106</b> for display.</p><p id="p-0035" num="0032"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an example of a computing device <b>200</b> that can implement a transmitting station or a receiving station. For example, the computing device <b>200</b> can implement one or both of the transmitting station <b>102</b> and the receiving station <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The computing device <b>200</b> can be in the form of a computing system including multiple computing devices, or in the form of one computing device, for example, a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and the like.</p><p id="p-0036" num="0033">A processor <b>202</b> in the computing device <b>200</b> can be a conventional central processing unit. Alternatively, the processor <b>202</b> can be another type of device, or multiple devices, capable of manipulating or processing information now existing or hereafter developed. For example, although the disclosed implementations can be practiced with one processor as shown (e.g., the processor <b>202</b>), advantages in speed and efficiency can be achieved by using more than one processor.</p><p id="p-0037" num="0034">A memory <b>204</b> in computing device <b>200</b> can be a read only memory (ROM) device or a random-access memory (RAM) device in an implementation. However, other suitable types of storage device can be used as the memory <b>204</b>. The memory <b>204</b> can include code and data <b>206</b> that is accessed by the processor <b>202</b> using a bus <b>212</b>. The memory <b>204</b> can further include an operating system <b>208</b> and application programs <b>210</b>, the application programs <b>210</b> including at least one program that permits the processor <b>202</b> to perform the techniques described herein. For example, the application programs <b>210</b> can include applications <b>1</b> through N, which further include a video coding application that performs the techniques described herein. The computing device <b>200</b> can also include a secondary storage <b>214</b>, which can, for example, be a memory card used with a mobile computing device. Because the video communication sessions may contain a significant amount of information, they can be stored in whole or in part in the secondary storage <b>214</b> and loaded into the memory <b>204</b> as needed for processing.</p><p id="p-0038" num="0035">The computing device <b>200</b> can also include one or more output devices, such as a display <b>218</b>. The display <b>218</b> may be, in one example, a touch sensitive display that combines a display with a touch sensitive element that is operable to sense touch inputs. The display <b>218</b> can be coupled to the processor <b>202</b> via the bus <b>212</b>. Other output devices that permit a user to program or otherwise use the computing device <b>200</b> can be provided in addition to or as an alternative to the display <b>218</b>. When the output device is or includes a display, the display can be implemented in various ways, including by a liquid crystal display (LCD), a cathode-ray tube (CRT) display, or a light emitting diode (LED) display, such as an organic LED (OLED) display.</p><p id="p-0039" num="0036">The computing device <b>200</b> can also include or be in communication with an image-sensing device <b>220</b>, for example, a camera, or any other image-sensing device <b>220</b> now existing or hereafter developed that can sense an image such as the image of a user operating the computing device <b>200</b>. The image-sensing device <b>220</b> can be positioned such that it is directed toward the user operating the computing device <b>200</b>. In an example, the position and optical axis of the image-sensing device <b>220</b> can be configured such that the field of vision includes an area that is directly adjacent to the display <b>218</b> and from which the display <b>218</b> is visible.</p><p id="p-0040" num="0037">The computing device <b>200</b> can also include or be in communication with a sound-sensing device <b>222</b>, for example, a microphone, or any other sound-sensing device now existing or hereafter developed that can sense sounds near the computing device <b>200</b>. The sound-sensing device <b>222</b> can be positioned such that it is directed toward the user operating the computing device <b>200</b> and can be configured to receive sounds, for example, speech or other utterances, made by the user while the user operates the computing device <b>200</b>.</p><p id="p-0041" num="0038">Although <figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts the processor <b>202</b> and the memory <b>204</b> of the computing device <b>200</b> as being integrated into a single unit, other configurations can be utilized. The operations of the processor <b>202</b> can be distributed across multiple machines (wherein individual machines can have one or more processors) that can be coupled directly or across a local area or other network. The memory <b>204</b> can be distributed across multiple machines such as a network-based memory or memory in multiple machines performing the operations of the computing device <b>200</b>. Although depicted here as one bus, the bus <b>212</b> of the computing device <b>200</b> can be composed of multiple buses. Further, the secondary storage <b>214</b> can be directly coupled to the other components of the computing device <b>200</b> or can be accessed via a network and can comprise an integrated unit such as a memory card or multiple units such as multiple memory cards. The computing device <b>200</b> can thus be implemented in a wide variety of configurations.</p><p id="p-0042" num="0039"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of an example of a video stream <b>300</b> to be encoded and subsequently decoded. The video stream <b>300</b> includes a video sequence <b>302</b>. At the next level, the video sequence <b>302</b> includes several adjacent frames <b>304</b>. While three frames are depicted as the adjacent frames <b>304</b>, the video sequence <b>302</b> can include any number of adjacent frames <b>304</b>. The adjacent frames <b>304</b> can then be further subdivided into individual frames, for example, a frame <b>306</b>. At the next level, the frame <b>306</b> can be divided into a series of planes or segments <b>308</b>. The segments <b>308</b> can be subsets of frames that permit parallel processing, for example. The segments <b>308</b> can also be subsets of frames that can separate the video data into separate colors. For example, a frame <b>306</b> of color video data can include a luminance plane and two chrominance planes. The segments <b>308</b> may be sampled at different resolutions.</p><p id="p-0043" num="0040">Whether or not the frame <b>306</b> is divided into segments <b>308</b>, the frame <b>306</b> may be further subdivided into blocks <b>310</b>, which can contain data corresponding to, for example, 16&#xd7;16 pixels in the frame <b>306</b>. The blocks <b>310</b> can also be arranged to include data from one or more segments <b>308</b> of pixel data. The blocks <b>310</b> can also be of any other suitable size such as 4&#xd7;4 pixels, 8&#xd7;8 pixels, 16&#xd7;8 pixels, 8&#xd7;16 pixels, 16&#xd7;16 pixels, or larger. Unless otherwise noted, the terms block and macroblock are used interchangeably herein.</p><p id="p-0044" num="0041"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of an example of an encoder <b>400</b>. The encoder <b>400</b> can be implemented, as described above, in the transmitting station <b>102</b>, such as by providing a computer software program stored in memory, for example, the memory <b>204</b>. The computer software program can include machine instructions that, when executed by a processor such as the processor <b>202</b>, cause the transmitting station <b>102</b> to encode video data in the manner described in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The encoder <b>400</b> can also be implemented as specialized hardware included in, for example, the transmitting station <b>102</b>. In one particularly desirable implementation, the encoder <b>400</b> is a hardware encoder.</p><p id="p-0045" num="0042">The encoder <b>400</b> has the following stages to perform the various functions in a forward path (shown by the solid connection lines) to produce an encoded or compressed bitstream <b>420</b> using the video stream <b>300</b> as input: an intra/inter prediction stage <b>402</b>, a transform stage <b>404</b>, a quantization stage <b>406</b>, and an entropy encoding stage <b>408</b>. The encoder <b>400</b> may also include a reconstruction path (shown by the dotted connection lines) to reconstruct a frame for encoding of future blocks. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the encoder <b>400</b> has the following stages to perform the various functions in the reconstruction path: a dequantization stage <b>410</b>, an inverse transform stage <b>412</b>, a reconstruction stage <b>414</b>, and a loop filtering stage <b>416</b>. Other structural variations of the encoder <b>400</b> can be used to encode the video stream <b>300</b>.</p><p id="p-0046" num="0043">When the video stream <b>300</b> is presented for encoding, respective adjacent frames <b>304</b>, such as the frame <b>306</b>, can be processed in units of blocks. At the intra/inter prediction stage <b>402</b>, respective blocks can be encoded using intra-frame prediction (also called intra-prediction) or inter-frame prediction (also called inter-prediction). In any case, a prediction block can be formed. In the case of intra-prediction, a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed. In the case of inter-prediction, a prediction block may be formed from samples in one or more previously constructed reference frames.</p><p id="p-0047" num="0044">Next, the prediction block can be subtracted from the current block at the intra/inter prediction stage <b>402</b> to produce a residual block (also called a residual). The transform stage <b>404</b> transforms the residual into transform coefficients in, for example, the frequency domain using block-based transforms. The quantization stage <b>406</b> converts the transform coefficients into discrete quantum values, which are referred to as quantized transform coefficients, using a quantizer value or a quantization level. For example, the transform coefficients may be divided by the quantizer value and truncated.</p><p id="p-0048" num="0045">The quantized transform coefficients are then entropy encoded by the entropy encoding stage <b>408</b>. The entropy-encoded coefficients, together with other information used to decode the block (which may include, for example, syntax elements such as used to indicate the type of prediction used, transform type, motion vectors, a quantizer value, or the like), are then output to the compressed bitstream <b>420</b>. The compressed bitstream <b>420</b> can be formatted using various techniques, such as variable length coding (VLC) or arithmetic coding. The compressed bitstream <b>420</b> can also be referred to as an encoded video stream or encoded video bitstream, and the terms will be used interchangeably herein.</p><p id="p-0049" num="0046">The reconstruction path (shown by the dotted connection lines) can be used to ensure that the encoder <b>400</b> and a decoder <b>500</b> (described below with respect to <figref idref="DRAWINGS">FIG. <b>5</b></figref>) use the same reference frames to decode the compressed bitstream <b>420</b>. The reconstruction path performs functions that are similar to functions that take place during the decoding process (described below with respect to <figref idref="DRAWINGS">FIG. <b>5</b></figref>), including dequantizing the quantized transform coefficients at the dequantization stage <b>410</b> and inverse transforming the dequantized transform coefficients at the inverse transform stage <b>412</b> to produce a derivative residual block (also called a derivative residual). At the reconstruction stage <b>414</b>, the prediction block that was predicted at the intra/inter prediction stage <b>402</b> can be added to the derivative residual to create a reconstructed block. The loop filtering stage <b>416</b> can be applied to the reconstructed block to reduce distortion such as blocking artifacts.</p><p id="p-0050" num="0047">Other variations of the encoder <b>400</b> can be used to encode the compressed bitstream <b>420</b>. In some implementations, a non-transform-based encoder can quantize the residual signal directly without the transform stage <b>404</b> for certain blocks or frames. In some implementations, an encoder can have the quantization stage <b>406</b> and the dequantization stage <b>410</b> combined in a common stage.</p><p id="p-0051" num="0048"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of an example of a decoder <b>500</b>. The decoder <b>500</b> can be implemented in the receiving station <b>106</b>, for example, by providing a computer software program stored in the memory <b>204</b>. The computer software program can include machine instructions that, when executed by a processor such as the processor <b>202</b>, cause the receiving station <b>106</b> to decode video data in the manner described in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. The decoder <b>500</b> can also be implemented in hardware included in, for example, the transmitting station <b>102</b> or the receiving station <b>106</b>.</p><p id="p-0052" num="0049">The decoder <b>500</b>, like the reconstruction path of the encoder <b>400</b> discussed above, includes in one example the following stages to perform various functions to produce an output video stream <b>516</b> from the compressed bitstream <b>420</b>: an entropy decoding stage <b>502</b>, a dequantization stage <b>504</b>, an inverse transform stage <b>506</b>, an intra/inter prediction stage <b>508</b>, a reconstruction stage <b>510</b>, a loop filtering stage <b>512</b>, and a deblocking filtering stage <b>514</b>. Other structural variations of the decoder <b>500</b> can be used to decode the compressed bitstream <b>420</b>.</p><p id="p-0053" num="0050">When the compressed bitstream <b>420</b> is presented for decoding, the data elements within the compressed bitstream <b>420</b> can be decoded by the entropy decoding stage <b>502</b> to produce a set of quantized transform coefficients. The dequantization stage <b>504</b> dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by the quantizer value), and the inverse transform stage <b>506</b> inverse transforms the dequantized transform coefficients to produce a derivative residual that can be identical to that created by the inverse transform stage <b>412</b> in the encoder <b>400</b>. Using header information decoded from the compressed bitstream <b>420</b>, the decoder <b>500</b> can use the intra/inter prediction stage <b>508</b> to create the same prediction block as was created in the encoder <b>400</b> (e.g., at the intra/inter prediction stage <b>402</b>).</p><p id="p-0054" num="0051">At the reconstruction stage <b>510</b>, the prediction block can be added to the derivative residual to create a reconstructed block. The loop filtering stage <b>512</b> can be applied to the reconstructed block to reduce blocking artifacts. Other filtering can be applied to the reconstructed block. In this example, the deblocking filtering stage <b>514</b> is applied to the reconstructed block to reduce blocking distortion, and the result is output as the output video stream <b>516</b>. The output video stream <b>516</b> can also be referred to as a decoded video stream, and the terms will be used interchangeably herein. Other variations of the decoder <b>500</b> can be used to decode the compressed bitstream <b>420</b>. In some implementations, the decoder <b>500</b> can produce the output video stream <b>516</b> without the deblocking filtering stage <b>514</b>.</p><p id="p-0055" num="0052">As can be discerned from the description of the encoder <b>400</b> and the decoder above, bits are generally used for one of two things in an encoded video bitstream: either content prediction (e.g., inter mode/motion vector coding, intra prediction mode coding, etc.) or residual or coefficient coding (e.g., transform coefficients). Encoders may use techniques to decrease the bits spent on coefficient coding. For example, a coefficient token tree (which may also be referred to as a binary token tree) specifies the scope of the value, with forward-adaptive probabilities for each branch in this token tree. The token base value is subtracted from the value to be coded to form a residual then the block is coded with fixed probabilities. A similar scheme with minor variations including backward-adaptivity is also possible. Adaptive techniques can alter the probability models as the video stream is being encoded to adapt to changing characteristics of the data. In any event, a decoder is informed of (or has available) the probability model used to encode an entropy-coded video bitstream in order to decode the video bitstream.</p><p id="p-0056" num="0053">Before describing updating of the probability estimation for a sequence of symbols, the development of the sequence of symbols is described starting with <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0057" num="0054"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram <b>600</b> illustrating quantized transform coefficients according to implementations of this disclosure. The diagram <b>600</b> depicts a current block <b>620</b>, a scan order <b>602</b>, a quantized transform block <b>604</b>, a non-zero map <b>606</b>, an end-of-block map <b>622</b>, and a sign map <b>626</b>. The current block <b>620</b> is illustrated as a 4&#xd7;4 block. However, any block size is possible. For example, the current block can have a size (i.e., dimensions) of 4&#xd7;4, 8&#xd7;8, 16&#xd7;16, 32&#xd7;32, or any other square or rectangular block size. The current block <b>620</b> can be a block of a current frame. In another example, the current frame may be partitioned into segments (such as the segments <b>308</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>), tiles, or the like, each including a collection of blocks, where the current block is a block of the partition.</p><p id="p-0058" num="0055">The quantized transform block <b>604</b> can be a block of a size similar to the size of the current block <b>620</b>. The quantized transform block <b>604</b> includes non-zero coefficients (e.g., a coefficient <b>608</b>) and zero coefficients (e.g., a coefficient <b>610</b>). As described above, the quantized transform block <b>604</b> contains quantized transform coefficients for the residual block corresponding to the current block <b>620</b>. Also as described above, the quantized transform coefficients are entropy coded by an entropy-coding phase, such as the entropy coding stage <b>408</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0059" num="0056">Entropy coding a quantized transform coefficient can involve the selection of a context model (also referred to as probability context model, probability model, model, and context) which provides estimates of conditional probabilities for coding the binary symbols of a binarized transform coefficient as described below with respect to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. When entropy coding a quantized transform coefficient, additional information may be used as the context for selecting a context model. For example, the magnitudes of the previously coded transform coefficients can be used, at least partially, for determining a probability model.</p><p id="p-0060" num="0057">To encode a transform block, a video coding system may traverse the transform block in a scan order and encode (e.g., entropy encode) the quantized transform coefficients as the quantized transform coefficients are respectively traversed (i.e., visited). In a zigzag scan order, such as the scan order <b>602</b>, the top left corner of the transform block (also known as the DC coefficient) is first traversed and encoded, the next coefficient in the scan order (i.e., the transform coefficient corresponding to the location labeled &#x201c;1&#x201d;) is traversed and encoded, and so on. In the zigzag scan order (i.e., scan order <b>602</b>), some quantized transform coefficients above and to the left of a current quantized transform coefficient (e.g., a to-be-encoded transform coefficient) are traversed first. Other scan orders are possible. A one-dimensional structure (e.g., an array) of quantized transform coefficients can result from the traversal of the two-dimensional quantized transform block using the scan order.</p><p id="p-0061" num="0058">In some examples, encoding the quantized transform block <b>604</b> can include determining the non-zero map <b>606</b>, which indicates which quantized transform coefficients of the quantized transform block <b>604</b> are zero and which are non-zero. A non-zero coefficient and a zero coefficient can be indicated with values one (1) and zero (0), respectively, in the non-zero map. For example, the non-zero map <b>606</b> includes a non-zero <b>607</b> at Cartesian location (0, 0) corresponding to the coefficient <b>608</b> and a zero <b>608</b> at Cartesian location (2, 0) corresponding to the coefficient <b>610</b>.</p><p id="p-0062" num="0059">In some examples, encoding the quantized transform block <b>604</b> can include generating and encoding the end-of-block map <b>622</b>. The end-of-block map indicates whether a non-zero quantized transform coefficient of the quantized transform block <b>604</b> is the last non-zero coefficient with respect to a given scan order. If a non-zero coefficient is not the last non-zero coefficient in the transform block, then it can be indicated with the binary bit zero (0) in the end-of-block map. If, on the other hand, a non-zero coefficient is the last non-zero coefficient in the transform block, then it can be indicated with the binary value one (1) in the end-of-block map. For example, as the quantized transform coefficient corresponding to the scan location <b>11</b> (i.e., the last non-zero quantized transform coefficient <b>628</b>) is the last non-zero coefficient of the quantized transform block <b>604</b>, it is indicated with the end-of-block value <b>624</b> of one (1); all other non-zero transform coefficients are indicated with a zero.</p><p id="p-0063" num="0060">In some examples, encoding the quantized transform block <b>604</b> can include generating and encoding the sign map <b>626</b>. The sign map <b>626</b> indicates which non-zero quantized transform coefficients of the quantized transform block <b>604</b> have positive values and which quantized transform coefficients have negative values. Transform coefficients that are zero need not be indicated in the sign map. The sign map <b>626</b> illustrates the sign map for the quantized transform block <b>604</b>. In the sign map, negative quantized transform coefficients can be indicated with a &#x2212;1 and positive quantized transform coefficients can be indicated with a one (1).</p><p id="p-0064" num="0061"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram of a coefficient token tree <b>700</b> that can be used to entropy code blocks into a video bitstream according to implementations of this disclosure. The coefficient token tree <b>700</b> is referred to as a binary tree because, at each node of the tree, one of two branches must be taken (i.e., traversed). The coefficient token tree <b>700</b> includes a root node <b>701</b> and a node <b>703</b> corresponding, respectively, to the nodes labeled A and B.</p><p id="p-0065" num="0062">As described above with respect to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, when an end-of-block (EOB) token is detected for a block, coding of coefficients in the current block can terminate and the remaining coefficients in the block can be inferred to be zero. As such, the coding of EOB positions can be an essential part of coefficient in a video coding system.</p><p id="p-0066" num="0063">In some video coding systems, a binary decision determining whether (or not) a current token is equal to the EOB token of the current block is coded immediately after an nonzero coefficient is decoded or at the first scan position (DC). In an example, for a transform block of size M&#xd7;N, where M denotes the number of columns and N denotes the number of rows in the transform block, the maximum number of times of coding whether a current token is equal to the EOB token is equal to M&#xd7;N. M and N can take values, such as the values 2, 4, 8, 16, 32, and 64. As described below, the binary decision corresponds to the coding of a &#x201c;1&#x201d; bit corresponding to the decision to move from the root node <b>701</b> to the node <b>703</b> in the coefficient token tree <b>700</b>. Herein, &#x201c;coding a bit&#x201d; can mean the outputting or generating of a bit in the codeword representing a transform coefficient being encoded. Similarly, &#x201c;decoding a bit&#x201d; can mean the reading (such as from an encoded bitstream) of a bit of the codeword corresponding to a quantized transform coefficient being decoded such that the bit corresponds to a branch being traversed in the coefficient token tree.</p><p id="p-0067" num="0064">Using the coefficient token tree <b>700</b>, a string of binary digits is generated for a quantized coefficient (e.g., the coefficients <b>608</b>, <b>610</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>) of the quantized transform block (such as the quantized transform block <b>604</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0068" num="0065">In an example, the quantized coefficients in an N&#xd7;N block (e.g., quantized transform block <b>604</b>) are organized into a 1D (one-dimensional) array (herein, an array u) following a prescribed scan order (e.g., the scan order <b>602</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>). N can be 4, 8, 16, 32, or any other value. The quantized coefficient at the i<sup>th </sup>position of the 1D array can be referred as u[i], where i=0, . . . , N*N&#x2212;1. The starting position of the last run of zeroes in u[i], . . . , u[N*N&#x2212;1] can be denoted as eob. In the case where when u[N*N&#x2212;1] is not zero, the eob can be set to the value N*N. That is, if the last coefficient of the 1D array u is not zero, then eob can be set to the value N*N. Using the examples of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the 1D array u can have the entries u[ ]=[&#x2212;6, 0, &#x2212;1, 0, 2, 4, 1, 0, 0, 1, 0, &#x2212;1, 0, 0, 0, 0]. The values at each of the u[i]s is a quantized transform coefficient. The quantized transform coefficients of the 1D array u may also be referred herein simply as &#x201c;coefficients&#x201d; or &#x201c;transform coefficients.&#x201d; The coefficient at position i=0 (i.e., u[0]=&#x2014;6) corresponds to the DC coefficient. In this example, the eob is equal to 12 because there are no non-zero coefficients after the zero coefficient at position <b>12</b> of the 1D array u.</p><p id="p-0069" num="0066">To encode and decode the coefficients u[i], . . . , u[N*N&#x2212;1], for i=0 to N*N&#x2212;1, a token t[i] is generated at each position i&#x3c;=eob. The token t[i], for i&#x3c;eob, can be indicative of the size and/or size range of the corresponding quantized transform coefficient at u[i]. The token for the quantized transform coefficient at eob can be an EOB_TOKEN, which is a token that indicates that the 1D array u contains no non-zero coefficients following the eob position (inclusive). That is, t[eob]=EOB_TOKEN indicates the EOB position of the current block. Table I below provides a listing of an example of token values, excluding the EOB_TOKEN, and their corresponding names according to an implementation of this disclosure.</p><p id="p-0070" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="42pt" align="left"/><colspec colname="1" colwidth="56pt" align="left"/><colspec colname="2" colwidth="119pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="2" rowsep="1">TABLE I</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Token</entry><entry>Name of Token</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>&#x2002;0</entry><entry>ZERO_TOKEN</entry></row><row><entry/><entry>&#x2002;1</entry><entry>ONE_TOKEN</entry></row><row><entry/><entry>&#x2002;2</entry><entry>TWO_TOKEN</entry></row><row><entry/><entry>&#x2002;3</entry><entry>THREE_TOKEN</entry></row><row><entry/><entry>&#x2002;4</entry><entry>FOUR_TOKEN</entry></row><row><entry/><entry>&#x2002;5</entry><entry>DCT_VAL_CAT1 (5, 6)</entry></row><row><entry/><entry>&#x2002;6</entry><entry>DCT_VAL_CAT2 (7-10)</entry></row><row><entry/><entry>&#x2002;7</entry><entry>DCT_VAL_CAT3 (11-18)</entry></row><row><entry/><entry>&#x2002;8</entry><entry>DCT_VAL_CAT4 (19-34)</entry></row><row><entry/><entry>&#x2002;9</entry><entry>DCT_VAL_CAT5 (35-66)</entry></row><row><entry/><entry>10</entry><entry>DCT_VAL_CAT6 (67-2048)</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0071" num="0067">In an example, quantized coefficient values are taken to be signed 12-bit integers. To represent a quantized coefficient value, the range of 12-bit signed values can be divided into 11 tokens (the tokens 0-10 in Table I) plus the end of block token (EOB_TOKEN). To generate a token to represent a quantized coefficient value, the coefficient token tree <b>700</b> can be traversed. The result (i.e., the bit string) of traversing the tree can then be encoded into a bitstream (such as the bitstream <b>420</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) by an encoder as described with respect to the entropy encoding stage <b>408</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0072" num="0068">The coefficient token tree <b>700</b> includes the tokens EOB_TOKEN (token <b>702</b>), ZERO_TOKEN (token <b>704</b>), ONE_TOKEN (token <b>706</b>), TWO_TOKEN (token <b>708</b>), THREE_TOKEN (token <b>710</b>), FOUR_TOKEN (token <b>712</b>), CAT1 (token <b>714</b> that is DCT_VAL_CAT1 in Table I), CAT2 (token <b>716</b> that is DCT_VAL_CAT2 in Table I), CAT3 (token <b>718</b> that is DCT_VAL_CAT3 in Table I), CAT4 (token <b>720</b> that is DCT_VAL_CAT4 in Table I), CAT5 (token <b>722</b> that is DCT_VAL_CAT5 in Table I) and CAT6 (token <b>724</b> that is DCT_VAL_CAT6 in Table I). As can be seen, the coefficient token tree maps a single quantized coefficient value into a single token, such as one of the tokens 704, 706, 708, 710 and 712. Other tokens, such as the tokens 714, 716, 718, 720, 722 and 724, represent ranges of quantized coefficient values. For example, a quantized transform coefficient with a value of 37 can be represented by the token DCT_VAL_CAT5&#x2014;the token <b>722</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0073" num="0069">The base value for a token is defined as the smallest number in its range. For example, the base value for the token <b>720</b> is 19. Entropy coding identifies a token for each quantized coefficient and, if the token represents a range, can form a residual by subtracting the base value from the quantized coefficient. For example, a quantized transform coefficient with a value of 20 can be represented by including the token <b>720</b> and a residual value of 1 (i.e., 20 minus 19) in the encoded video bitstream to permit a decoder to reconstruct the original quantized transform coefficient. The end of block token (i.e., the token <b>702</b>) signals that no further non-zero quantized coefficients remain in the transformed block data.</p><p id="p-0074" num="0070">To encode or decode a token t[i] by using a binary arithmetic coding engine (such as by the entropy encoding stage <b>408</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), the coefficient token tree <b>700</b> can be used. The coefficient token tree <b>700</b> is traversed starting at the root node <b>701</b> (i.e., the node labeled A). Traversing the coefficient token tree generates a bit string (a codeword) that will be encoded into the bitstream using, for example, binary arithmetic coding. The bit string is a representation of the current coefficient (i.e., the quantized transform coefficient being encoded).</p><p id="p-0075" num="0071">If a current coefficient is zero, and there are no more non-zero values for the remaining transform coefficients, the token <b>702</b> (i.e., the EOB_TOKEN) is added into the bitstream. This is the case, for example, for the transform coefficient at scan order position <b>12</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. On the other hand, if the current coefficient is non-zero, or if there are non-zero values among any remaining coefficients of the current block, a &#x201c;1&#x201d; bit is added to the codeword and traversal passes to the node <b>703</b> (i.e., the node labeled B). At node B, the current coefficient is tested to see if it is equal to zero. If so, the left-hand branch is taken such that token <b>704</b> representing the value ZERO_TOKEN and a bit &#x201c;0&#x201d; is added to the codeword. If not, a bit &#x201c;1&#x201d; is added to the codeword and traversal passes to node C. At node C, the current coefficient is tested to see if it is greater than 1. If the current coefficient is equal to one (1), the left-hand branch is taken and token <b>706</b> representing the value ONE_TOKEN is added to the bitstream (i.e., a &#x201c;0&#x201d; bit is added to the codeword). If the current coefficient is greater than one (1), traversal passes to node D to check the value of the current coefficient as compared to the value 4. If the current coefficient is less than or equal to 4, traversal passes to node E and a &#x201c;0&#x201d; bit is added to the codeword. At node E, a test for equality to the value &#x201c;2&#x201d; may be made. If true, token <b>706</b> representing the value &#x201c;2&#x201d; is added to the bitstream (i.e., a bit &#x201c;0&#x201d; is added to the codeword). Otherwise, at node F, the current coefficient is tested against either the value &#x201c;3&#x201d; or the value &#x201c;4&#x201d; and either token <b>710</b> (i.e., bit &#x201c;0&#x201d; is added to the codeword) or token <b>712</b> (i.e., bit &#x201c;1&#x201d; is added to the codeword) to the bitstream as appropriate; and so on.</p><p id="p-0076" num="0072">In brief, a &#x201c;0&#x201d; bit is added to the codeword upon traversal to a left child node and a &#x201c;1&#x201d; bit is added to the codeword upon traversal to a right child node. A similar process is undertaken by a decoder when decoding a codeword from a compressed bitstream. The decoder reads a bit from bit stream. If the bit is a &#x201c;1,&#x201d; the coefficient token tree is traversed to the right and if the bit is a &#x201c;0,&#x201d; the tree is traversed to the left. The decoder reads then a next bit and repeats the process until traversal of the tree reaches a leaf node (i.e., a token). As an example, to encode a token t[i]=THREE_TOKEN, starting from the root node (i.e., the root node <b>701</b>), a binary string of 111010 is encoded. As another example, decoding the codeword 11100 results in the token TWO_TOKEN.</p><p id="p-0077" num="0073">Note that the correspondence between &#x201c;0&#x201d; and &#x201c;1&#x201d; bits to left and right child nodes is merely a convention used to describe the encoding and decoding processes. In some implementations, a different convention, for example, in one where &#x201c;1&#x201d; corresponds to the left child node, and &#x201c;0&#x201d; corresponds to the right child node, can be used. As long as both the encoder and the decoder adopt the same convention, the processes described herein can be applied.</p><p id="p-0078" num="0074">Since an EOB_TOKEN is only possible after a nonzero coefficient, when u[i&#x2212;1] is zero (that is, when the quantized transform coefficient at location i&#x2212;1 of the 1D array u is equal to zero), a decoder can infer that the first bit must be 1. The first bit has to be 1 since, in traversing the tree, for a transform coefficient (e.g., transform coefficient at the zigzag scan order location 2 of <figref idref="DRAWINGS">FIG. <b>6</b></figref>) following a zero transform coefficient (e.g., transform coefficient at the zigzag scan order location 1 of <figref idref="DRAWINGS">FIG. <b>6</b></figref>), the traversal necessarily moves from the root node <b>701</b> to the node <b>703</b>.</p><p id="p-0079" num="0075">As such, a binary flag checkEob can be used to instruct the encoder and the decoder to skip encoding and decoding the first bit leading from the root node in the coefficient token tree <b>700</b>. In effect, when the binary flag checkEob is 0 (i.e., indicating that the root node should not be checked), the root node <b>701</b> of the coefficient token tree <b>700</b> is skipped and the node <b>703</b> becomes the first node of coefficient token tree <b>700</b> to be visited for traversal. That is, when the root node <b>701</b> is skipped, the encoder can skip encoding and the decoder can skip decoding and can infer a first bit (i.e., a binary bit &#x201c;1&#x201d;) of the encoded string.</p><p id="p-0080" num="0076">At the start of encoding or decoding a block, the binary flag checkEob can be initialized to 1 (i.e., indicating that the root node should be checked). The following steps illustrate an example process for decoding quantized transform coefficients in an N&#xd7;N block.</p><p id="p-0081" num="0077">At step 1, the binary flag checkEob is set to zero (i.e., checkEob=0) and an index i is also set to zero (i.e., i=0).</p><p id="p-0082" num="0078">At step 2, a token t[i] is decoded by using either (1) the full coefficient token tree (i.e., starting at the root node <b>701</b> of the coefficient token tree <b>700</b>) if the binary flag checkEob is equal to 1; or (2) using the partial tree (e.g., starting at the node <b>703</b>) where the EOB_TOKEN is skipped, if checkEob is equal to 0.</p><p id="p-0083" num="0079">At step 3, If the token t[i]=EOB_TOKEN, then the quantized transform coefficients u[i], . . . , u[N*N&#x2014; <b>1</b>] are all to zero and the decoding process terminates; otherwise, extra bits can be decoded if necessary (i.e., when t[i] is not equal to the ZERO_TOKEN) and reconstruct u[i].</p><p id="p-0084" num="0080">At step 4, the binary flag checkEob is set to 1 if u[i] is equal to zero, otherwise checkEob is set to 0. That is, checkEob can be set to the value (u[i]!=0).</p><p id="p-0085" num="0081">At step 5, the index i is incremented (i.e., i=i+1).</p><p id="p-0086" num="0082">At step 6, the steps 2-5 are repeated until all quantized transform coefficients have been decoded (i.e., until the index i=N*N) or until the EOB_TOKEN is decoded.</p><p id="p-0087" num="0083">At step 2 above, decoding a token t[i] can include the steps of determining a context ctx, determining a binary probability distribution (i.e., a model) from the context ctx, and using a boolean arithmetic code to decode a path from the root node of the coefficient token tree <b>700</b> to a leaf node by using the determined probability distributions. The context ctx can be determined using a method of context derivation. The method of context derivation can use one or more of the block size, plane type (i.e., luminance or chrominance), the position i, and previously decoded tokens t [0], . . . , t[i&#x2212;1] to determine the context ctx. Other criteria can be used to determine the context ctx. The binary probability distribution can be determined for any internal node of the coefficient token tree <b>700</b> starting from the root node <b>701</b> when checkEOB=1 or from the node <b>703</b> when checkEOB=0.</p><p id="p-0088" num="0084">In some coding systems, the probability used to encode or decode a token t[i] given a context ctx may be fixed and does not adapt in a picture (i.e., a frame). For example, the probability may be either a default value that is defined for the given context ctx or the probability may be coded (i.e., signaled) as part of the frame header for that frame. Coding the probability for every context in coding a frame can be costly. As such, an encoder may analyze, for each context, whether it is beneficial to code the context's associated probability in the frame header and signal its decision to the decoder by using a binary flag. Furthermore, coding the probability for a context may use prediction to reduce cost (e.g., in bit rate) where the prediction may be derived from the probability of the same context in a previously decoded frame.</p><p id="p-0089" num="0085">In some coding systems, instead of traversing a coefficient token tree, such as the coefficient token tree <b>700</b>, to code a transform coefficient, each token can be associated with a value that is coded. As such, instead of coding binary symbols (i.e., selected from an alphabet comprised of the symbols {0, 1}), an alphabet of symbols that includes more than two symbols is used for coding transform coefficients. In an example, the alphabet includes 12 symbols, namely {EOB_TOKEN, ZERO_TOKEN, ONE_TOKEN, TWO_TOKEN, THREE_TOKEN, FOUR_TOKEN, DCT_VAL_CAT1, DCT_VAL_CAT2, DCT_VAL_CAT3, DCT_VAL_CAT4, DCT_VAL_CAT5, DCT_VAL_CAT6}. As such, the alphabet for coding transform coefficients includes 12 symbols, which are also referred to as tokens. Other token alphabets that include more, fewer, or different tokens are possible. An alphabet that includes only the symbols {0, 1} is referred to herein as a binary alphabet. An alphabet that includes symbols other than and/or in addition to the symbols {0, 1} is referred to herein as a non-binary alphabet. Each of the tokens can be associated with a value. In an example, the EOB_TOKEN can have a value of 255. Each of the other tokens can each be associated with a different value.</p><p id="p-0090" num="0086"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram of an example of a tree <b>800</b> for binarizing a quantized transform coefficient according to implementations of this disclosure. The tree <b>800</b> is a binary tree that can be used for binarizing quantized transform coefficients in some video coding systems. The tree <b>800</b> can be used by a video coding system that uses the steps of binarization, context modelling, and binary arithmetic coding for encoding and decoding of quantized transform coefficients. The process may be referred to as context-adaptive binary arithmetic coding (CABAC). For example, to code a quantized transform coefficient x, the coding system may perform the following steps. The quantized transform coefficient x can be any of the coefficients (e.g., the coefficient <b>608</b>) of the quantized transform block <b>604</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0091" num="0087">In the binarization step, a coefficient x is first binarized into a binary string by using the tree <b>800</b>. The binarization process may binarize the unsigned value of the coefficient x. For example, binarizing the coefficient <b>628</b> (i.e., the value &#x2212;1), binarizes the value 1. This results in traversing the tree <b>800</b> and generating the binary string <b>10</b>. Each of the bits of the binary string <b>10</b> is referred to as a bin.</p><p id="p-0092" num="0088">In the context derivation step, for each bin to be coded, a context is derived. A context can be derived from information such as one or more of the block size, plane type (i.e., luminance or chrominance), block position of the coefficient x, and previously decoded coefficients (e.g., a left and/or above neighboring coefficients, if available). Other information can be used to derive the context.</p><p id="p-0093" num="0089">In the binary arithmetic coding step, given a context, a bin is coded by using, e.g., a binary arithmetic coding engine into a binary codeword together with a probability value associated with the context.</p><p id="p-0094" num="0090">The steps of coding a transform coefficient can include a step that is referred as context update. In the context update step, after a bin is coded, the probability associated with the context is updated to reflect the value of the bin.</p><p id="p-0095" num="0091">As described briefly above, entropy coding a sequence of symbols may be achieved by using a probability model to determine a probability p for the sequence. Then, binary arithmetic coding may be used to map the sequence to a binary codeword at the encoder and to decode that sequence from the binary codeword at the decoder. The length (i.e., number of bits) of the codeword or string is given by equation (2) above. As the length is an integer number, however, the length is the smallest integer that is greater than the value calculated by equation (2). The efficiency of entropy coding can be directly related to the probability model.</p><p id="p-0096" num="0092">In the following description, when referring to a sequence s of N symbols, a subscript of t refers to the symbol at position t in the sequence. For example, where s is a sequence of five (5) binary symbols, such as 11010, s<sub>5 </sub>refers to the symbol in the 5<sup>th </sup>position, such as the last 0 in the sequence 11010. As such the sequence s can be expressed as s<sub>1</sub>, s<sub>2</sub>, . . . , s<sub>N</sub>.</p><p id="p-0097" num="0093">In some implementations, a symbol can refer to a token that is selected from a non-binary token alphabet that includes more than two tokens. As such, the symbol (i.e., token) can have one of the available values. The token can be a token that is used to code, and is indicative of, a transform coefficient. In such cases, &#x201c;a sequence of symbols s&#x201d; refers to the list of tokens s<sub>1</sub>, s<sub>2</sub>, . . . , s<sub>N </sub>used to code the transform coefficients at scan positions <b>1</b>, <b>2</b>, . . . , N, respectively, in a scan order.</p><p id="p-0098" num="0094">As used herein, probability values, such as the probability {circumflex over (p)}<sub>t</sub>(s<sub>t</sub>) of a current symbol s<sub>t</sub>, can have either floating-point or fixed-point representations. Accordingly, operations applied to these values may use either floating-point arithmetic or fixed-point arithmetic.</p><p id="p-0099" num="0095">Given two estimated probabilities for the same symbol {circumflex over (p)}<sub>t1</sub>(s<sub>t</sub>) and {circumflex over (p)}<sub>t2 </sub>(s<sub>t</sub>) such that {circumflex over (p)}<sub>t1</sub>(s<sub>t</sub>)&#x3c;{circumflex over (p)}<sub>t2</sub>(s<sub>t</sub>), the probability {umlaut over (p)}<sub>t2 </sub>(s<sub>t</sub>) results in a codeword that is no shorter than the probability {umlaut over (p)}<sub>t1</sub>(s<sub>t</sub>). That is, a smaller probability typically produces a longer codeword than a larger probability.</p><p id="p-0100" num="0096">A probability estimation model, which is a first-order linear system, is derived generally from equation (3) below, which estimates the probabilities that a symbol at t+1 is either 0 or 1 based on a weighted combination of the probabilities for the prior symbol at t and a conditional probability. The weighted combination uses a fixed weight or a variable weight as described below.</p><p id="p-0101" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mrow>       <mo>[</mo>       <mtable>        <mtr>         <mtd>          <mrow>           <mover accent="true">            <mi>p</mi>            <mi>&#x2c6;</mi>           </mover>           <mo>(</mo>           <mn>0</mn>           <mo>)</mo>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mover accent="true">            <mi>p</mi>            <mi>&#x2c6;</mi>           </mover>           <mo>(</mo>           <mn>1</mn>           <mo>)</mo>          </mrow>         </mtd>        </mtr>       </mtable>       <mo>]</mo>      </mrow>      <mrow>       <mi>t</mi>       <mo>+</mo>       <mn>1</mn>      </mrow>     </msub>     <mo>=</mo>     <mrow>      <msub>       <mrow>        <mi>&#x3b1;</mi>        <mo>[</mo>        <mtable>         <mtr>          <mtd>           <mrow>            <mover accent="true">             <mi>p</mi>             <mi>&#x2c6;</mi>            </mover>            <mo>(</mo>            <mn>0</mn>            <mo>)</mo>           </mrow>          </mtd>         </mtr>         <mtr>          <mtd>           <mrow>            <mover accent="true">             <mi>p</mi>             <mi>&#x2c6;</mi>            </mover>            <mo>(</mo>            <mn>1</mn>            <mo>)</mo>           </mrow>          </mtd>         </mtr>        </mtable>        <mo>]</mo>       </mrow>       <mi>t</mi>      </msub>      <mo>+</mo>      <mrow>       <mrow>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>-</mo>         <mi>&#x3b1;</mi>        </mrow>        <mo>)</mo>       </mrow>       <mo>&#xb7;</mo>       <mrow>        <mo>{</mo>        <mtable>         <mtr>          <mtd>           <mrow>            <mrow>             <mo>[</mo>             <mtable>              <mtr>               <mtd>                <mn>1</mn>               </mtd>              </mtr>              <mtr>               <mtd>                <mn>0</mn>               </mtd>              </mtr>             </mtable>             <mo>]</mo>            </mrow>            <mo>,</mo>            <mrow>             <mi>s</mi>             <mo>=</mo>             <mn>0</mn>            </mrow>           </mrow>          </mtd>         </mtr>         <mtr>          <mtd>           <mrow>            <mrow>             <mo>[</mo>             <mtable>              <mtr>               <mtd>                <mn>0</mn>               </mtd>              </mtr>              <mtr>               <mtd>                <mn>1</mn>               </mtd>              </mtr>             </mtable>             <mo>]</mo>            </mrow>            <mo>,</mo>            <mrow>             <mi>s</mi>             <mo>=</mo>             <mn>1</mn>            </mrow>           </mrow>          </mtd>         </mtr>        </mtable>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0102" num="0097">This estimate is based on part on understanding that, where p<sub>t </sub>&#x2208;<img id="CUSTOM-CHARACTER-00001" he="2.79mm" wi="2.46mm" file="US20230007260A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>2 </sup>(i.e., a vector with two elements) is denoted as the probability estimation at time t (i.e., t represents the index of the current symbol), the equation {circumflex over (p)}<sub>t</sub>(0)+{circumflex over (p)}<sub>t</sub>(1)=1 is true where {circumflex over (p)}<sub>t</sub>(0) and {circumflex over (p)}<sub>t</sub>(1) are probabilities of the current symbol at t being 0 and 1. The value &#x3b1; may depend upon the particular codec used for the encoding and decoding operations. For example, the probability model may be from the probability estimation module in the CABAC framework used in H.264/AVC, such as described in Section III.C. of D. Marpe et al., &#x201c;Context-based adaptive binary arithmetic coding in the H. 264/AVC video compression standard,&#x201d; IEEE Transactions on Circuits and Systems for Video Technology, Vol. 13, No. 7, pp. 620-636 (2003). In such an example, the value &#x3b1; is a constant of almost 0.95. In another example, the probability model may be from the probability estimation module in AV1, such as described in P. de Rivaz and J. Haughton, &#x201c;AV1 bitstream &#x26; decoding process specification,&#x201d; The Alliance for Open Media, p. 182 (2018) or Y. Chen et al., &#x201c;An overview of core coding tools in the AV1 video codec,&#x201d; in 2018 Picture Coding Symposium (PCS), IEEE, pp. 41-45 (2018). In this example, the probability update would use an adaptive a in terms of time (i.e., current symbol index) and number of symbols. In either example, there may be a barrier value p<sub>barrier </sub>such that if {circumflex over (p)}(0) or {circumflex over (p)}(1) is too small (i.e., is too close to 0 or 1 as indicated by a defined criteria), the value &#x3b1; is raised to p<sub>barrier</sub>. Stated more simply, p<sub>barrier </sub>prevents the probability estimation from being equal to 0. In certain examples herein, p<sub>barrier </sub>is referred to as p<sub>62</sub>.</p><p id="p-0103" num="0098">Equation (3) is considered an update rule that corresponds to a linear dynamic system that is used for prediction of sequential data. It is a first-order linear system that may be even more generalized to be written as equation (4) below, where the observed outcome u of the random system at time t is treated as an input.</p><p id="p-0104" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i><sub>t+1</sub><i>=&#x3b1;p</i><sub>t</sub>+(1&#x2212;&#x3b1;)<i>u</i>&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0105" num="0099">If another model in combination (e.g., generating the observed outcome u), the probability estimation model used for entropy coding may instead correspond to a higher-order linear system that produces more accurate results (e.g., lower entropy). In a possible technique, a probability model may include an update algorithm that uses conditions other than those of a baseline probability model in its update rule. For example, instead of using {circumflex over (p)}(s<sub>t</sub>), an estimate of the conditional probability over r symbols {circumflex over (p)}(s<sub>t </sub>s<sub>t&#x2212;1 </sub>. . . s<sub>t&#x2212;&#x3c4;</sub>) may be used. In this estimation, a list may be used to apply multiple probability updates. In a possible technique, a weighted average of models may be used to create a higher-order linear system. In a possible technique, update rates may be self-adaptive as described in more detail below.</p><p id="p-0106" num="0100">Each of these techniques may be used separately or in combination for the probability estimation, that is, to entropy code a sequence of symbols. In the examples described below, the sequence of symbols input into the entropy coding and update algorithm may comprise a sequence s of N symbols. The sequence may correspond to the binarization of symbols representing any portion of a frame, such as the frame, a segment, a slice, a block, or some other portion of the frame, such as the data described with regards to <figref idref="DRAWINGS">FIGS. <b>6</b>-<b>8</b></figref>.</p><p id="p-0107" num="0101"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flow chart of a method <b>900</b> for entropy coding a sequence of symbols according to the teachings herein. At <b>902</b>, a sequence of symbols is received. In the examples herein, the sequence is a sequence s of N binary symbols, where s&#x2208;{0,1}N, is entropy coded. The next step is to select a symbol at <b>904</b>. For example, the current symbol may be a first symbol in the sequence. At <b>906</b>, the current symbol is entropy coded using a probability. In some implementations, the probability may be the probability determined by a first, or baseline, probability model of a plurality of probability models. In other implementations, the probability may be an updated probability that uses a combination of estimations of the probability determined using respective probability models. In either event, the probability for the next symbol may be updated at <b>908</b>. The probability of the baseline and any other probability models may be updated, and a combination of these estimations may be used to update the probability at <b>908</b>. The combination is a second-order linear system different from each of the first-order linear systems represented by the models. This method <b>900</b> proceeds to check for remaining symbols at <b>910</b> and repeats until no symbols remain to be entropy coded.</p><p id="p-0108" num="0102">The method <b>900</b> is next described with certain examples. First described is an implementation where a fixed probability estimation is used to update the probability for entropy coding symbols of the sequence. The first example is followed by a second example that uses an adaptive probability estimation.</p><p id="p-0109" num="0103">Parameters or variables used for the entropy coding and probability of estimation are defined, initialized, or otherwise determined, either before, after, or concurrent with receipt of the sequence at <b>902</b>. Because this example uses binary symbols, probability values may be initialized so that the probability that the current symbol is 0 or is 1 is {circumflex over (p)}<sub>inf</sub>, ={circumflex over (p)}<sub>CABAC</sub>=p=[0.5,0.5]. That is, the probability that the first symbol is 0 or is 1 is set to be equal at the start of the sequence s. In this example, multiple probability models may be available for probability estimation, while two are shown. The probability p is used to entropy code the current symbol, the probability {circumflex over (p)}<sub>inf</sub>, is a first probability estimate from a first probability model based on counting as described below, and the probability {circumflex over (p)}<sub>CABAC </sub>is a second probability estimation from a second probability model based on {circumflex over (p)}<sub>CABAC</sub>. A parameter mode is selected from the set comprising 0 and 1 (mode &#x2208;{0,1}). The parameter mode indicates which of the first probability model or the second probability model is a baseline model. In an example described herein, mode=0 such that the baseline model comprises a CABAC model.</p><p id="p-0110" num="0104">A weight w used to combine the probability estimate of the first probability model with a conditional probability is set to a value of 0.5 in this fixed probability estimation, but a variable or adaptive weighting can be used in other examples of the teachings herein. For reasons described in additional detail below, a variable r and a variable t<sub>thres </sub>are set. For binary entropy coding, the variable r is set to 5, but the variable r may be equal to a different value. For example, when performing multi-symbol entropy coding, the variable r may be set equal to 8. One use of the variable r is to define the size L of a list for storing probability values, which list is used to determine the conditional probability. Entries of probability values within the list are initialized as follows: List=[[0,0]]<sup>L</sup>, where L=2<sup>&#x3c4;</sup>. The variable t<sub>thres</sub>, also described below, is set equal to 25, but it could be set to a lower or higher value.</p><p id="p-0111" num="0105">The value &#x3b1; described with regards to equations (3) and (4) may depend upon the particular codec used for the encoding and decoding operations as described above. In some examples, the value &#x3b1; may be a constant or may be adaptive in terms of time and number of symbols. In the following example, the value &#x3b1; is fixed such that &#x3b1;=(0.01875/0.5) 1/63 (approximately equal to 0.95), which is consistent with the CABAC model. As mentioned above, a barrier value p<sub>barrier </sub>(also referred to as p<sub>62</sub>) may be used to limit p to a minimum value. In this example, p<sub>62</sub>=0.5&#x3b1;<sup>62</sup>.</p><p id="p-0112" num="0106">The index (time) t is initialized to 1 to indicate that processing starts with the first symbol s<sub>1 </sub>in the sequence. While t remains less than or equal to the total number of symbols N, the processing receives the symbol s<sub>t</sub>, codes the symbol s<sub>t </sub>by p, which may also be described herein as {circumflex over (p)}<sub>t </sub>or {circumflex over (p)}, and then updates the probability as described below. The index t is updated to proceed to the next symbol s<sub>t+1 </sub>in the sequence, if any. The symbol s<sub>t+1 </sub>is entropy coded by the updated probability p. This process continues until all symbols in the sequence s are entropy coded (i.e., entropy encoded or entropy decoded).</p><p id="p-0113" num="0107">Pseudocode that represents this outer loop of the entropy coding and probability estimation is shown below.</p><p id="p-0114" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>while t &#x3c; N do</entry></row><row><entry>&#x2003;Receive symbol s<sub>t</sub>.</entry></row><row><entry>&#x2003;Entropy code s<sub>t </sub>by p.</entry></row><row><entry>&#x2003;p, {circumflex over (p)}<sub>inf</sub>, {circumflex over (p)}<sub>CABAC</sub>, List &#x2190;</entry></row><row><entry>&#x2003;&#x2003;ProbUpdate({circumflex over (p)}<sub>inf</sub>, {circumflex over (p)}<sub>CABAC</sub>, &#x3c4;, s<sub>t-&#x3c4;</sub>: s<sub>t</sub>, t, w, t<sub>thres</sub>, List, mode).</entry></row><row><entry>&#x2003;t &#x2190; t + 1</entry></row><row><entry>end while</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0115" num="0108">As can be seen from the above pseudocode, the function ProbUpdate is called after s<sub>t </sub>is entropy coded. The function ProbUpdate receives as input the probability p<sub>inf</sub>, the probability {circumflex over (p)}<sub>CABAC</sub>, the parameter &#x3c4;, the values of the symbols in the range s<sub>t&#x2212;&#x3c4; </sub>to s<sub>t</sub>, the index t of the current symbol s<sub>t</sub>, the weight w, the variable t<sub>thres</sub>, List, and the parameter mode. The function ProbUpdate returns the probability p, the probability p<sub>inf</sub>, the probability {circumflex over (p)}<sub>CABAC</sub>, and the entries in the List. More generally, the function ProbUpdate updates the probability p for coding the next symbol in the sequence of symbols.</p><p id="p-0116" num="0109">In an implementation of the teachings herein, the probability estimate updates may incorporate two probability estimation models&#x2014;the CABAC model previously described (and represented by {circumflex over (p)}<sub>CABAC</sub>) as well as the maximum likelihood estimate (MLE) for an independent identical distribution (i.i.d) sequence of symbols based on counting (represented by {circumflex over (p)}<sub>inf</sub>). The MLE for i.i.d sequence may be explained using a binary sequence for simplicity. Assume s<sub>1 </sub>. . . s<sub>t </sub>is i.i.d Bernoulli (i.e., a Bernoulli distribution) where 0 happens with probability p, and there is no preference of p, i.e., the prior of p is U[0, 1]. From observation of the sequence, if 0 occurs k times and 1 occurs 1 times, the estimator that satisfies equation (5) below</p><p id="p-0117" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <munder>      <mi>argmin</mi>      <mover accent="true">       <mi>p</mi>       <mi>&#x2c6;</mi>      </mover>     </munder>     <mo>-</mo>     <mrow>      <msub>       <mi>E</mi>       <mi>p</mi>      </msub>      <mo>(</mo>      <mrow>       <mrow>        <mi>p</mi>        <mo>&#x2062;</mo>        <mrow>         <mi>log</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mover accent="true">          <mi>p</mi>          <mi>&#x2c6;</mi>         </mover>         <mo>)</mo>        </mrow>       </mrow>       <mo>+</mo>       <mrow>        <mrow>         <mo>(</mo>         <mrow>          <mn>1</mn>          <mo>-</mo>          <mi>p</mi>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#x2062;</mo>        <mi>log</mi>        <mo>&#x2062;</mo>        <mrow>         <mo>(</mo>         <mrow>          <mn>1</mn>          <mo>-</mo>          <mover accent="true">           <mi>p</mi>           <mi>&#x2c6;</mi>          </mover>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0118" num="0110">corresponds to equation (6) below for the estimated probability {circumflex over (p)}.</p><p id="p-0119" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mover accent="true">      <mi>p</mi>      <mi>&#x2c6;</mi>     </mover>     <mo>=</mo>     <mfrac>      <mrow>       <mi>k</mi>       <mo>+</mo>       <mn>1</mn>      </mrow>      <mrow>       <mi>k</mi>       <mo>+</mo>       <mi>l</mi>       <mo>+</mo>       <mn>2</mn>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0120" num="0111">These models, and the others herein, may be referred to as a first probability model, a second probability model, a third probability model, etc., to distinguish one from another without any regard to the sequence of performance. Whether mode indicates that the baseline model is the CABAC model or the MLE model, an updated probability may be determined using an estimate of conditional probability {circumflex over (p)}(s<sub>t</sub>|s<sub>t&#x2212;1 </sub>. . . s<sub>t&#x2212;&#x3c4;</sub>) over the previous symbols. To obtain the estimate of conditional probability p<sub>cond</sub>, the List is used with an adjustable size 2<sup>&#x3c4; </sup>that stores all possible context sequences s<sub>t&#x2212;1</sub>:s<sub>t&#x2212;&#x3c4;</sub>. The List functions as a hashtable for conditions to store the conditional probability. When a symbol arrives, its previous &#x3c4; symbols are taken as the context. Then, the corresponding context in the list is accessed, and the count is updated. The probability estimation is the frequency. Until the number of symbols coded is greater than &#x3c4; (i.e., t&#x3e;&#x3c4;), the baseline estimation ({circumflex over (p)}<sub>inf </sub>or {circumflex over (p)}<sub>CABAC</sub>) may be output as the probability p.</p><p id="p-0121" num="0112">When the corresponding list item has too few counts, the estimation may not be accurate. There are at least two possible solutions. First, the condition has the length &#x3c4; (which, as described above, varies with the number of symbols). When the list item has few counts, the history of shorter lengths &#x3c4;&#x2212;1, &#x3c4;&#x2212;2, etc., may be considered. This involves taking unions of counts in multiple dictionary items. Whenever the count over this union reaches the threshold t<sub>thres</sub>, this probability estimation is recorded. For example, this may result in merging 00000 and 00001 as 0000. Second, if the total list is not large enough, the baseline estimation ({circumflex over (p)}<sub>inf</sub>, or {circumflex over (p)}<sub>CABAC</sub>) may be output as the probability p.</p><p id="p-0122" num="0113">The mode input mode lets a user decide whether to use a function ProbUpdateCABAC (corresponding to the CABAC model) or ProbUpdateCount (corresponding to the MLE model) to produce the baseline probability estimation and take its average (because weight w=0.5) with the conditional probability estimation (p<sub>cond</sub>) to provide a stable version of an output. Taking the average is non-trivial compared to changing an update rate (analogous to a in CABAC). This is because an average of two fixed rate update algorithms results in a second-order linear dynamic essentially different from a first-order update.</p><p id="p-0123" num="0114">That is, referring back to equation (4), a weighted average of probability update may be considered as follows.</p><p id="p-0124" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>q</i><sub>t+1</sub><i>=aq</i><sub>t</sub>+(1&#x2212;<i>a</i>)<i>u</i><sub>t </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0125" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>r</i><sub>t+i</sub><i>=ar</i><sub>t</sub>+(1&#x2212;<i>b</i>)<i>u</i><sub>t </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0126" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i><sub>t</sub><i>=wq</i><sub>t</sub>+(1&#x2212;<i>w</i>)<i>r</i><sub>t </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0127" num="0115">Substituting in equation (4) and solving by canceling q and r results in equation (7) below, which is a second order system that covers CABAC when a=b=0.95.</p><p id="p-0128" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i><sub>t+i</sub>=(<i>a+b</i>)<i>p</i><sub>t</sub><i>&#x2212;abp</i><sub>t&#x2212;1</sub>+(<i>w</i>(1&#x2212;<i>a</i>)+(1&#x2212;<i>w</i>)(1&#x2212;<i>b</i>))<i>u</i><sub>t</sub>+(<i>ab</i>&#x2212;(1&#x2212;<i>w</i>)<i>a&#x2212;wb</i>)<i>u</i><sub>t&#x2212;1</sub>&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0129" num="0116">This second order system cannot be trivially reduced to a first order system only involving p<sub>t+1</sub>, p<sub>t</sub>, and u<sub>t</sub>.</p><p id="p-0130" num="0117">The probability update described above used a fixed (e.g., a linear) combination of update algorithms for context-based probability estimation. One example of a function ProbUpdate that implements the second order system described above is shown by the following pseudocode. In brief, when the function ProbUpdate is called after s<sub>t </sub>is entropy coded by p in the outer loop, the probability estimation models available as the baseline model are used to generate a respective estimated probability.</p><p id="p-0131" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>DefProbUpdate({circumflex over (p)}<sub>inf</sub>, {circumflex over (p)}<sub>CABAC</sub>, &#x3c4;, s<sub>t-&#x3c4;</sub>: s<sub>t</sub>, t, w, t<sub>thres</sub>, List, mode):</entry></row><row><entry>&#x2003;{circumflex over (p)}<sub>inf </sub>&#x2190; ProbUpdateCount({circumflex over (p)}<sub>inf</sub>, s<sub>t</sub>, t).</entry></row><row><entry>&#x2003;{circumflex over (p)}<sub>CABAC </sub>&#x2190; ProbUpdateCABAC({circumflex over (p)}<sub>CABAC</sub>) s<sub>t</sub>, &#x3b1;).</entry></row><row><entry>&#x2003;t<sub>tmp </sub>= 0.</entry></row><row><entry>&#x2003;if t &#x3e; &#x3c4; then</entry></row><row><entry>&#x2003;&#x2003;List(s<sub>t</sub>; s<sub>t-&#x3c4;</sub> : s<sub>t-1</sub>) &#x2190; List(s<sub>t</sub>; s<sub>t-&#x3c4;</sub> : s<sub>t-1</sub>) + 1.</entry></row><row><entry>&#x2003;&#x2003;t<sub>tmp </sub>&#x2190; &#x3c4;</entry></row><row><entry>&#x2003;while t<sub>tmp </sub>&#x3e; 0 and &#x3a3;<sub>i </sub>List (i; s<sub>t-t</sub><sub><sub2>tmp</sub2></sub> + 1: s<sub>t</sub>) &#x3c; t<sub>thres </sub>do</entry></row><row><entry>&#x2003;&#x2003;t<sub>tmp </sub>&#x2190; t<sub>tmp </sub>&#x2212; 1.</entry></row><row><entry>&#x2003;end while</entry></row><row><entry>&#x2003;if t<sub>tmp </sub>&#x3e; 0 then</entry></row><row><entry> </entry></row><row><entry>&#x2003;&#x2003;<maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <msub>   <mi>p</mi>   <mi>cond</mi>  </msub>  <mo>&#x2190;</mo>  <mrow>   <mfrac>    <mrow>     <mi>List</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mrow>      <mrow>       <mtext>:;</mtext>       <msub>        <mi>s</mi>        <mrow>         <mi>t</mi>         <mo>-</mo>         <msub>          <mi>t</mi>          <mi>tmp</mi>         </msub>        </mrow>       </msub>      </mrow>      <mo>+</mo>      <mrow>       <mn>1:</mn>       <mtext>  </mtext>       <msub>        <mi>s</mi>        <mi>t</mi>       </msub>      </mrow>     </mrow>     <mo>)</mo>    </mrow>    <mrow>     <msub>      <mo>&#x2211;</mo>      <mi>i</mi>     </msub>     <mrow>      <mi>List</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mrow>        <mi>i</mi>        <mo>;</mo>        <msub>         <mi>s</mi>         <mrow>          <mi>t</mi>          <mo>-</mo>          <msub>           <mi>t</mi>           <mi>tmp</mi>          </msub>         </mrow>        </msub>       </mrow>       <mo>+</mo>       <mrow>        <mn>1</mn>        <mo>:</mo>        <mtext>  </mtext>        <msub>         <mi>s</mi>         <mi>t</mi>        </msub>       </mrow>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mfrac>   <mo>.</mo>  </mrow> </mrow></math></maths></entry></row><row><entry> </entry></row><row><entry>&#x2003;end if</entry></row><row><entry>end if</entry></row><row><entry>if t<sub>tmp </sub>&#x3e; 0 then</entry></row><row><entry>&#x2003;if mode then</entry></row><row><entry>&#x2003;&#x2003;p &#x2190; w{circumflex over (p)}<sub>inf </sub>+ (1 &#x2212; w)p<sub>cond</sub>.</entry></row><row><entry>&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;p &#x2190; w{circumflex over (p)}<sub>CABAC </sub>+ (1 &#x2212; w)p<sub>cond</sub>.</entry></row><row><entry>&#x2003;end if</entry></row><row><entry>else</entry></row><row><entry>&#x2003;if mode then</entry></row><row><entry>&#x2003;&#x2003;p &#x2190; {circumflex over (p)}<sub>inf</sub>.</entry></row><row><entry>&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;p &#x2190; {circumflex over (p)}<sub>CABAC</sub>.</entry></row><row><entry>&#x2003;end if</entry></row><row><entry>end if</entry></row><row><entry>return p, {circumflex over (p)}<sub>inf</sub>, {circumflex over (p)}<sub>CABAC</sub>, List.</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0132" num="0118">In brief, when the function ProbUpdate is called after s<sub>t </sub>is entropy coded by p in the outer loop, the probability estimation models available as the baseline model are used to generate a respective estimated probability ({circumflex over (p)}<sub>inf </sub>and {circumflex over (p)}<sub>CABAC </sub>in this example). Thereafter, t<sub>tmp</sub>, which is used for collecting counts in the dictionary, is initialized to 0. The algorithm next counts and merges probabilities among the dictionary as described above, where i represents each possible outcome of the random symbol, and the summation calculates how many outcomes have been observed within the condition window (s<sub>t&#x2212;t</sub><sub><sub2>tmp</sub2></sub>+1:s<sub>t</sub>). This counting and merging ends at the second &#x201c;end if&#x201d;. The next portion of code queries whether the dictionary is large enough (i.e., if t<sub>tmp</sub>&#x3e;0), and if so, updates the probability estimation based on which baseline model is selected given the value of mode to update the probability according to one of two calculations. For example, if mode=0, the updated probability p takes on the value w{circumflex over (p)}<sub>CABAC </sub>(1&#x2212;w)p<sub>cond </sub>If mode=0, the updated probability p takes on the value w{circumflex over (p)}<sub>inf</sub>+(1&#x2212;w)p<sub>cond</sub>. If instead the dictionary is not large enough (i.e., the response to t<sub>tmp</sub>&#x3e;0 is no), the baseline estimation {circumflex over (p)}<sub>CABAC </sub>or {circumflex over (p)}<sub>inf </sub>is selected given the value of mode for use as the updated probability.</p><p id="p-0133" num="0119">Thereafter, p, {circumflex over (p)}<sub>inf</sub>, {circumflex over (p)}<sub>CABAC</sub>, and List are returned so that p can be used to entropy code the next symbol s<sub>t</sub>, {circumflex over (p)}<sub>inf </sub>and {circumflex over (p)}<sub>CABAC </sub>are available to update the baseline estimation after the next symbol s<sub>t </sub>is entropy coded, and List is available to optionally generate the conditional probability p<sub>cond </sub>after the next symbol s<sub>t </sub>is entropy coded.</p><p id="p-0134" num="0120">The function ProbUpdateCABAC called by the function ProbUpdate described above may be represented by the following pseudocode. This pseudocode represents the CABAC update described above, where p is the vector {circumflex over (p)}<sub>CABAC </sub>of the probability distribution, namely [p(1&#x2212;&#x3c3;), p (&#x3c3;)].</p><p id="p-0135" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>Def ProbUpdateCABAC(p, s<sub>t</sub>, &#x3b1;):</entry></row><row><entry>&#x2003;Find LPS: &#x3c3; = argmin<sub>i&#x2208;{0,1}</sub>p(i).</entry></row><row><entry>&#x2003;if s<sub>t </sub>= &#x3c3; then</entry></row><row><entry>&#x2003;&#x2003;p(&#x3c3;) &#x2190; max(&#x3b1;p(&#x3c3;), p<sub>62</sub>).</entry></row><row><entry>&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;p(&#x3c3;) &#x2190; &#x3b1;p(&#x3c3;) + 1 &#x2212; &#x3b1;.</entry></row><row><entry>&#x2003;end if</entry></row><row><entry>&#x2003;p(1 &#x2212; &#x3c3;) &#x2190; 1 &#x2212; p(&#x3c3;).</entry></row><row><entry>&#x2003;return p</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0136" num="0121">The function ProbUpdateCount called by the function ProbUpdate described above may be represented by the following pseudocode. This pseudocode represents the MLE calculation described above, where p is the vector {circumflex over (p)}<sub>inf</sub>, of the probability distribution for the given outcome value s<sub>t</sub>.</p><p id="p-0137" num="0000"><tables id="TABLE-US-00005" num="00005"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="35pt" align="left"/><colspec colname="1" colwidth="49pt" align="left"/><colspec colname="2" colwidth="133pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry/><entry>Def ProbUpdateCount(p, s<sub>t</sub>, t):</entry></row><row><entry/><entry/><entry>&#x2003;<maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mrow>  <mrow>   <mi>p</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <msub>    <mi>s</mi>    <mi>t</mi>   </msub>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mrow>     <mi>t</mi>     <mo>-</mo>     <mn>1</mn>    </mrow>    <mi>t</mi>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <mi>p</mi>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></entry></row><row><entry/><entry/><entry>&#x2003;<maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mrow>  <mrow>   <mi>p</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <msub>    <mi>s</mi>    <mi>t</mi>   </msub>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mi>p</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <msub>     <mi>s</mi>     <mi>t</mi>    </msub>    <mo>)</mo>   </mrow>   <mo>+</mo>   <mfrac>    <mn>1</mn>    <mi>t</mi>   </mfrac>  </mrow> </mrow></math></maths></entry></row><row><entry/><entry/><entry>&#x2003;return p</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0138" num="0122">Other update algorithms for context-based probability estimation are possible. For example, additional algorithms may include a data-driven method that describes learning a linear combination, as opposed to using the fixed combination described above. An implementation for this entropy coding and adaptive probability estimation (as compared to fixed probability estimation) is next described.</p><p id="p-0139" num="0123">In this implementation, instead of using the conditional probability estimated by the previous symbols using List to make a higher-order linear system, the different first-order linear models described above (CABAC, counting (MLE), AV1, etc.) are desirably used as kernels to output a linear combination through actively learning the linear combination. While these three models are used in this example, any probability estimation algorithm may be used. Denote n<sub>p </sub>as the number of kernels, {circumflex over (p)}&#x2208;<img id="CUSTOM-CHARACTER-00002" he="2.79mm" wi="2.46mm" file="US20230007260A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>n</sup><sup><sub2>p</sub2></sup><sup>&#xd7;2 </sup>where each row is a probability estimation, and w&#x2208;<img id="CUSTOM-CHARACTER-00003" he="2.79mm" wi="2.46mm" file="US20230007260A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>n</sup><sup><sub2>p </sub2></sup>is the weight/parameter of a linear combination. In other words, a weighted average of simple (i.e., first-order) probability estimations is used as the result for entropy coding the next symbol as follows.</p><p id="p-0140" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>w</i><sup>T</sup><i>{circumflex over (p)}=&#x3a3;w</i><sub>i</sub><i>{circumflex over (p)}</i>(<i>i</i>,:)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0141" num="0124">Each row of {circumflex over (p)} is updated by a probability update algorithm, and p(1, :) is fixed as an AV1 output. In this way, the AV1 model/algorithm corresponds to the case when w<sub>0</sub>=1, w<sub>i</sub>=0, &#x2200;i&#x2265;2. This may be the initialization of linear weights in the pseudocode described below. For this reason, the AV1 model may be referred to as the baseline model.</p><p id="p-0142" num="0125">Thereafter, w is updated. Because it is expected that all update algorithms chosen as kernels should result in an improvement to the output w<sup>T</sup>{circumflex over (p)}, w may be constrained so that w&#x2265;0. This also guarantees that the probability estimation is non-negative. Further, 1<sup>T</sup>w=1 is applied to guarantee that the sum of probability is 1. A stochastic gradient descent (SGD) is used to update w. For each s<sub>t</sub>, an entropy is incurred as follows.</p><p id="p-0143" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>f</i>(<i>w,{circumflex over (p)};s</i><sub>t</sub>)=&#x2212;log<sub>2</sub>((<i>w</i><sup>T</sup><i>{circumflex over (p)}</i>)(<i>s</i><sub>t</sub>))<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0144" num="0126">A gradient is taken with respect to w as follows.</p><p id="p-0145" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <msub>        <mo>&#x2207;</mo>        <mi>w</mi>       </msub>       <mrow>        <mi>f</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mi>w</mi>         <mo>,</mo>         <mover accent="true">          <mi>p</mi>          <mi>&#x2c6;</mi>         </mover>         <mo>,</mo>         <msub>          <mi>s</mi>          <mi>t</mi>         </msub>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>=</mo>      <mrow>       <mrow>        <mo>-</mo>        <mfrac>         <mi>c</mi>         <mrow>          <mrow>           <mo>(</mo>           <mrow>            <msup>             <mi>w</mi>             <mi>T</mi>            </msup>            <mo>&#x2062;</mo>            <mover accent="true">             <mi>p</mi>             <mi>&#x2c6;</mi>            </mover>           </mrow>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <mrow>           <mo>(</mo>           <msub>            <mi>s</mi>            <mi>t</mi>           </msub>           <mo>)</mo>          </mrow>         </mrow>        </mfrac>       </mrow>       <mo>&#x2062;</mo>       <mrow>        <mover accent="true">         <mi>p</mi>         <mi>&#x2c6;</mi>        </mover>        <mo>(</mo>        <mrow>         <mo>:</mo>         <mrow>          <mo>,</mo>          <msub>           <mi>s</mi>           <mi>t</mi>          </msub>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>     <mo>,</mo>     <mrow>      <mi>c</mi>      <mo>=</mo>      <mrow>       <mn>1</mn>       <mo>/</mo>       <mrow>        <mi>log</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mn>2</mn>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mtext> </mtext>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0146" num="0127">At time t, a step size &#x3b7;<sub>t</sub>=&#x3b7;<sub>0</sub>/t is used, which is standard for SGD, &#x3b7;<sub>t</sub>=&#x3b7;<sub>0</sub>/t<sup>r </sup>r&#x2208;(0,1) are allowed, and stochastic approximation defines r&#x2208;(1/2, 1). Then, w is updated by the following gradient step.</p><p id="p-0147" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mrow>  <mi>w</mi>  <mo>&#x2190;</mo>  <mrow>   <munder>    <mi>argmin</mi>    <mrow>     <mrow>      <msub>       <mi>w</mi>       <mo>+</mo>      </msub>      <mo>&#x2265;</mo>      <mn>0</mn>     </mrow>     <mo>,</mo>     <mrow>      <mrow>       <msup>        <mn>1</mn>        <mi>T</mi>       </msup>       <mo>&#x2062;</mo>       <msub>        <mi>w</mi>        <mo>+</mo>       </msub>      </mrow>      <mo>=</mo>      <mn>1</mn>     </mrow>    </mrow>   </munder>   <mo>&#x2062;</mo>   <mtext>  </mtext>   <msup>    <mrow>     <mo>&#xf605;</mo>     <mrow>      <msub>       <mi>w</mi>       <mo>+</mo>      </msub>      <mo>-</mo>      <mrow>       <mo>(</mo>       <mrow>        <mi>w</mi>        <mo>-</mo>        <mrow>         <msub>          <mi>&#x3b7;</mi>          <mi>t</mi>         </msub>         <mo>&#x2062;</mo>         <mrow>          <msub>           <mo>&#x2207;</mo>           <mi>w</mi>          </msub>          <mrow>           <mi>f</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mi>w</mi>            <mo>,</mo>            <mrow>             <mover accent="true">              <mi>p</mi>              <mi>&#x2c6;</mi>             </mover>             <mo>;</mo>             <msub>              <mi>s</mi>              <mi>t</mi>             </msub>            </mrow>           </mrow>           <mo>)</mo>          </mrow>         </mrow>        </mrow>       </mrow>       <mo>)</mo>      </mrow>     </mrow>     <mo>&#xf606;</mo>    </mrow>    <mn>2</mn>   </msup>  </mrow> </mrow></math></maths></p><p id="p-0148" num="0128">Alternatively, a fixed step size &#x3b7;=&#x3b7;<sub>0 </sub>can be used to get an inner loop argument that iterates {tilde over (w)}<sub>t </sub>and plug in for final probability estimation w<sub>t</sub>, which satisfies w<sub>t</sub>=(&#x3a3;<sub>i=1</sub><sup>t</sup>{tilde over (w)}<sub>i</sub>)/t. This cancels out the noise in SGD so that a special diminishing step size or averaging the gradient may be used. A linear dynamic is also proposed for variable iterates w<sub>t+1</sub>=&#x3b2;w<sub>t</sub>+(1&#x2212;&#x3b2;){tilde over (w)}<sub>t </sub>as a faster update. This is the process presented in the pseudocode below.</p><p id="p-0149" num="0129">To update weight w, a constrained optimization step may be included. Solving such a step may be slow. To reduce the number of calls for the step, a batch version the above algorithm may be used. At each epoch, a batch with increasing size 1, 4, 9, 16 . . . , may be taken, and the gradient of the batch is averaged. gradient in this batch. The update of w occurs only at the end of each batch, with a fixed step size &#x3b7;<sub>0</sub>. Both theoretically and empirically, the convergence rate of the SGD and batch versions are similar.</p><p id="p-0150" num="0130">A fast algorithm is next proposed that approximately solves the optimization problem. Namely, the problem may be defined as the following equation.</p><p id="p-0151" num="0000"><maths id="MATH-US-00009" num="00009"><math overflow="scroll"> <mrow>  <munder>   <mi>argmin</mi>   <mrow>    <mrow>     <msub>      <mi>w</mi>      <mo>+</mo>     </msub>     <mo>&#x2265;</mo>     <mn>0</mn>    </mrow>    <mo>,</mo>    <mrow>     <mrow>      <msup>       <mn>1</mn>       <mi>T</mi>      </msup>      <mo>&#x2062;</mo>      <msub>       <mi>w</mi>       <mo>+</mo>      </msub>     </mrow>     <mo>=</mo>     <mn>1</mn>    </mrow>   </mrow>  </munder>  <mo>&#x2062;</mo>  <mtext>  </mtext>  <msup>   <mrow>    <mo>&#xf605;</mo>    <mrow>     <msub>      <mi>w</mi>      <mo>+</mo>     </msub>     <mo>-</mo>     <mrow>      <mo>(</mo>      <mrow>       <mi>w</mi>       <mo>-</mo>       <mrow>        <msub>         <mi>&#x3b7;</mi>         <mi>t</mi>        </msub>        <mo>&#x2062;</mo>        <mrow>         <msub>          <mo>&#x2207;</mo>          <mi>w</mi>         </msub>         <mrow>          <mi>f</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mi>w</mi>           <mo>,</mo>           <mrow>            <mover accent="true">             <mi>p</mi>             <mi>&#x2c6;</mi>            </mover>            <mo>;</mo>            <msub>             <mi>s</mi>             <mi>t</mi>            </msub>           </mrow>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </mrow>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mo>&#xf606;</mo>   </mrow>   <mn>2</mn>  </msup> </mrow></math></maths></p><p id="p-0152" num="0131">Simplifying the notation results in the following equation.</p><p id="p-0153" num="0000"><maths id="MATH-US-00010" num="00010"><math overflow="scroll"> <mrow>  <munder>   <mi>argmin</mi>   <mrow>    <mrow>     <mi>x</mi>     <mo>&#x2265;</mo>     <mn>0</mn>    </mrow>    <mo>,</mo>    <mrow>     <mrow>      <msup>       <mn>1</mn>       <mi>T</mi>      </msup>      <mo>&#x2062;</mo>      <mi>x</mi>     </mrow>     <mo>=</mo>     <mn>1</mn>    </mrow>   </mrow>  </munder>  <mo>&#x2062;</mo>  <mtext>  </mtext>  <mfrac>   <mn>1</mn>   <mn>2</mn>  </mfrac>  <mo>&#x2062;</mo>  <msup>   <mrow>    <mo>&#xf605;</mo>    <mrow>     <mi>x</mi>     <mo>-</mo>     <mi>y</mi>    </mrow>    <mo>&#xf606;</mo>   </mrow>   <mn>2</mn>  </msup> </mrow></math></maths></p><p id="p-0154" num="0132">Optimality may be obtained from the Lagrangian according to the following equation.</p><p id="p-0155" num="0000"><maths id="MATH-US-00011" num="00011"><math overflow="scroll"> <mrow>  <mrow>   <mi>L</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mrow>    <mi>x</mi>    <mo>,</mo>    <mi>&#x3bb;</mi>    <mo>,</mo>    <mi>&#x3bc;</mi>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mfrac>     <mn>1</mn>     <mn>2</mn>    </mfrac>    <mo>&#x2062;</mo>    <msup>     <mrow>      <mo>&#xf605;</mo>      <mrow>       <mi>x</mi>       <mo>-</mo>       <mi>y</mi>      </mrow>      <mo>&#xf606;</mo>     </mrow>     <mn>2</mn>    </msup>   </mrow>   <mo>-</mo>   <mrow>    <msup>     <mi>&#x3bb;</mi>     <mi>T</mi>    </msup>    <mo>&#x2062;</mo>    <mi>x</mi>   </mrow>   <mo>+</mo>   <mrow>    <mi>&#x3bc;</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mrow>     <mrow>      <msup>       <mn>1</mn>       <mi>T</mi>      </msup>      <mo>&#x2062;</mo>      <mi>x</mi>     </mrow>     <mo>-</mo>     <mn>1</mn>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0156" num="0133">The Karush-Kuhn-Tucker (KKT) condition is represented by the following.</p><p id="p-0157" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x2207;<sub>x</sub><i>L</i>(<i>x</i>,&#x3bc;)=<i>x&#x2212;y&#x2212;&#x3bb;+&#x3bc;</i>1=0;<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0158" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x394;&#x2265;0;<i>x&#x2265;</i>0;&#x3bb;<sub>i</sub><i>x</i><sub>i</sub>=0,&#x2200;<i>i. </i><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0159" num="0134">The optimal value for x is represented by the following equation.</p><p id="p-0160" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i><sub>i</sub>*=max(&#x3b3;<sub>i</sub>&#x2212;&#x3bc;,0).<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0161" num="0135">Thus, the following equation may be solved to get &#x3bc;* and x*=max(&#x3b3;&#x2212;u*1, 0).</p><p id="p-0162" num="0000"><maths id="MATH-US-00012" num="00012"><math overflow="scroll"> <mrow>  <mrow>   <munder>    <mi>max</mi>    <mi>&#x3bc;</mi>   </munder>   <mtext>  </mtext>   <mfrac>    <mn>1</mn>    <mn>2</mn>   </mfrac>   <mo>&#x2062;</mo>   <msup>    <mrow>     <mo>&#xf605;</mo>     <mrow>      <mrow>       <mi>max</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mrow>         <mi>y</mi>         <mo>-</mo>         <mrow>          <mi>&#x3bc;</mi>          <mo>&#x2062;</mo>          <mn>1</mn>         </mrow>        </mrow>        <mo>,</mo>        <mn>0</mn>       </mrow>       <mo>)</mo>      </mrow>      <mo>-</mo>      <mi>y</mi>     </mrow>     <mo>&#xf606;</mo>    </mrow>    <mn>2</mn>   </msup>  </mrow>  <mo>+</mo>  <mrow>   <mi>&#x3bc;</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mrow>    <mrow>     <msup>      <mn>1</mn>      <mi>T</mi>     </msup>     <mo>&#x2062;</mo>     <mrow>      <mi>max</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mrow>        <mi>y</mi>        <mo>-</mo>        <mrow>         <mi>&#x3bc;</mi>         <mo>&#x2062;</mo>         <mn>1</mn>        </mrow>       </mrow>       <mo>,</mo>       <mn>0</mn>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mo>-</mo>    <mn>1</mn>   </mrow>   <mo>)</mo>  </mrow> </mrow></math></maths></p><p id="p-0163" num="0136">Note that the above equation is a one-dimensional convex optimization that can be solved by binary search.</p><p id="p-0164" num="0137">The above data-driven method that describes learning a linear combination may be represented by the following pseudocode, where the input is a sequence of binary symbols as described with respect to the previous implementation. As with the previous implementation, the first step is initialization. During initialization, a variable n<sub>p </sub>is set equal to 18, and the probability 3 is set equal to {circumflex over (p)}=1<sub>n</sub><sub><sub2>p </sub2></sub>[0.5,0.5]. Further, the variable w={tilde over (w)}=[1,0, . . . , 0]<sup>T</sup>&#x2208;<img id="CUSTOM-CHARACTER-00004" he="2.79mm" wi="2.46mm" file="US20230007260A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>n</sup><sup><sub2>p</sub2></sup>. The variable a is initialized according to 0.99&#xb7;2<sup>&#x2212;[0:n</sup><sup><sub2>p</sub2></sup><sup>&#x2212;2]/4(n</sup><sup><sub2>p</sub2></sup><sup>&#x2212;2)</sup>&#x2208; <img id="CUSTOM-CHARACTER-00005" he="2.79mm" wi="2.46mm" file="US20230007260A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>n</sup><sup><sub2>p</sub2></sup><sup>&#x2212;2</sup>. Other variables are initialized as follows: &#x3b7;<sub>0</sub>=5, r=1, b_=b=0, g=0<sup>n</sup><sup><sub2>p</sub2></sup>, &#xdf;=0.95, and &#x3b1;<sub>min</sub>=0.84, r&#x2208;(1/2, 1). The algorithm chooses the mode from SGD decreasing step size, SGD average argument, SGD dynamic argument, or SGD batch. It is worth noting that when the mode is SGD decreasing step size, it can be solved by the fast projected optimization algorithm described above.</p><p id="p-0165" num="0138">As with the fixed probability estimation described above, parameters or variables used for the entropy coding and probability of estimation are defined, initialized, or otherwise determined, either before, after, or concurrent with receipt of the sequence at <b>902</b>. Then, the remaining steps of the method <b>900</b> are performed according to the following pseudocode starting with receiving the first symbol s<sub>1 </sub>and entropy coding the first symbol s<sub>1</sub>. Then, the probability estimations are updated using the respective models. The functions ProbUpdateCount and ProbUpdateCABAC have been discussed above. The function ProbUpdateAV1 is described below. Once the probability estimations are updated, they are combined using the selected mode.</p><p id="p-0166" num="0000"><tables id="TABLE-US-00006" num="00006"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>while t &#x2264; N do</entry></row><row><entry>&#x2003;Receive symbol s<sub>t</sub>.</entry></row><row><entry>&#x2003;Entropy code s<sub>t </sub>by w<sup>T</sup>{circumflex over (p)}.</entry></row><row><entry>&#x2003;{circumflex over (p)}(1, :) &#x2190; ProbUpdateAV1({circumflex over (p)}(1, :), s<sub>t</sub>, t, NumOfSyms).</entry></row><row><entry>&#x2003;{circumflex over (p)}(2, :) &#x2190; ProbUpdateCount({circumflex over (p)}(2, :), s<sub>t</sub>, t).</entry></row><row><entry>&#x2003;{circumflex over (p)}(i, :) &#x2190; ProbUpdateCABAC({circumflex over (p)}(i, :), s<sub>t</sub>, &#x3b1;<sub>i-2</sub>) for all 3 &#x2264; i &#x2264; n<sub>p</sub>.</entry></row><row><entry>&#x2003;if SGD decreasing step size then</entry></row><row><entry> </entry></row><row><entry>&#x2003;&#x2003;<maths id="MATH-US-00013" num="00013"><math overflow="scroll"> <mrow>  <mi>w</mi>  <mo>&#x2190;</mo>  <mrow>   <munder>    <mrow>     <mi>arg</mi>     <mo>&#x2062;</mo>     <mi>min</mi>    </mrow>    <mrow>     <mrow>      <msub>       <mi>w</mi>       <mo>+</mo>      </msub>      <mo>&#x2265;</mo>      <mn>0</mn>     </mrow>     <mo>,</mo>     <mtext> </mtext>     <mrow>      <mrow>       <msup>        <mn>1</mn>        <mi>T</mi>       </msup>       <mo>&#x2062;</mo>       <msub>        <mi>w</mi>        <mo>+</mo>       </msub>      </mrow>      <mo>=</mo>      <mn>1</mn>     </mrow>    </mrow>   </munder>   <mo>&#x2062;</mo>   <mrow>    <msup>     <mrow>      <mo>&#xf605;</mo>      <mrow>       <msub>        <mi>w</mi>        <mo>+</mo>       </msub>       <mo>-</mo>       <mrow>        <mo>(</mo>        <mrow>         <mi>w</mi>         <mo>-</mo>         <mrow>          <mfrac>           <msub>            <mi>&#x3b7;</mi>            <mn>0</mn>           </msub>           <msup>            <mi>t</mi>            <mi>&#x3c4;</mi>           </msup>          </mfrac>          <mo>&#xb7;</mo>          <mfrac>           <mrow>            <mover accent="true">             <mi>p</mi>             <mi>&#x2c6;</mi>            </mover>            <mo>(</mo>            <mrow>             <mtext>:,</mtext>             <msub>              <mi>s</mi>              <mi>t</mi>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mrow>            <mrow>             <mo>(</mo>             <mrow>              <msup>               <mi>w</mi>               <mi>T</mi>              </msup>              <mo>&#x2062;</mo>              <mover accent="true">               <mi>p</mi>               <mi>&#x2c6;</mi>              </mover>             </mrow>             <mo>)</mo>            </mrow>            <mo>&#x2062;</mo>            <mrow>             <mo>(</mo>             <msub>              <mi>s</mi>              <mi>t</mi>             </msub>             <mo>)</mo>            </mrow>           </mrow>          </mfrac>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>&#xf606;</mo>     </mrow>     <mn>2</mn>    </msup>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></entry></row><row><entry> </entry></row><row><entry>&#x2003;else if SGD average argument then</entry></row><row><entry> </entry></row><row><entry>&#x2003;&#x2003;<maths id="MATH-US-00014" num="00014"><math overflow="scroll"> <mrow>  <mover accent="true">   <mi>w</mi>   <mi>&#x2dc;</mi>  </mover>  <mo>&#x2190;</mo>  <mrow>   <munder>    <mrow>     <mi>arg</mi>     <mo>&#x2062;</mo>     <mi>min</mi>    </mrow>    <mrow>     <mrow>      <msub>       <mi>w</mi>       <mo>+</mo>      </msub>      <mo>&#x2265;</mo>      <mn>0</mn>     </mrow>     <mo>,</mo>     <mtext> </mtext>     <mrow>      <mrow>       <msup>        <mn>1</mn>        <mi>T</mi>       </msup>       <mo>&#x2062;</mo>       <msub>        <mi>w</mi>        <mo>+</mo>       </msub>      </mrow>      <mo>=</mo>      <mn>1</mn>     </mrow>    </mrow>   </munder>   <mo>&#x2062;</mo>   <mrow>    <msup>     <mrow>      <mo>&#xf605;</mo>      <mrow>       <msub>        <mi>w</mi>        <mo>+</mo>       </msub>       <mo>-</mo>       <mrow>        <mo>(</mo>        <mrow>         <mover accent="true">          <mi>w</mi>          <mi>&#x2dc;</mi>         </mover>         <mo>-</mo>         <mrow>          <msub>           <mi>&#x3b7;</mi>           <mn>0</mn>          </msub>          <mo>&#xb7;</mo>          <mfrac>           <mrow>            <mover accent="true">             <mi>p</mi>             <mi>&#x2c6;</mi>            </mover>            <mo>(</mo>            <mrow>             <mtext>:,</mtext>             <msub>              <mi>s</mi>              <mi>t</mi>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mrow>            <mrow>             <mo>(</mo>             <mrow>              <msup>               <mover>                <mi>w</mi>                <mo>~</mo>               </mover>               <mi>T</mi>              </msup>              <mo>&#x2062;</mo>              <mover>               <mi>p</mi>               <mo>^</mo>              </mover>             </mrow>             <mo>)</mo>            </mrow>            <mo>&#x2062;</mo>            <mrow>             <mo>(</mo>             <msub>              <mi>s</mi>              <mi>t</mi>             </msub>             <mo>)</mo>            </mrow>           </mrow>          </mfrac>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>&#xf606;</mo>     </mrow>     <mn>2</mn>    </msup>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></entry></row><row><entry> </entry></row><row><entry>&#x2003;&#x2003;<maths id="MATH-US-00015" num="00015"><math overflow="scroll"> <mrow>  <mrow>   <mi>w</mi>   <mo>&#x2190;</mo>   <mrow>    <mrow>     <mrow>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <mfrac>        <mn>1</mn>        <mrow>         <mi>t</mi>         <mo>+</mo>         <mn>1</mn>        </mrow>       </mfrac>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mi>w</mi>    </mrow>    <mo>+</mo>    <mrow>     <mfrac>      <mn>1</mn>      <mrow>       <mi>t</mi>       <mo>+</mo>       <mn>1</mn>      </mrow>     </mfrac>     <mo>&#x2062;</mo>     <mrow>      <mover>       <mi>w</mi>       <mo>~</mo>      </mover>      <mo>.</mo>      <mtext>   </mtext>      <mi>\\</mi>     </mrow>    </mrow>   </mrow>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mrow>     <mi>t</mi>     <mo>+</mo>     <mn>1</mn>    </mrow>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <msubsup>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mrow>      <mi>t</mi>      <mo>+</mo>      <mn>1</mn>     </mrow>    </msubsup>    <msub>     <mover>      <mi>w</mi>      <mo>~</mo>     </mover>     <mi>i</mi>    </msub>   </mrow>  </mrow> </mrow></math></maths></entry></row><row><entry> </entry></row><row><entry>&#x2003;else if SGD dynamic argument then</entry></row><row><entry> </entry></row><row><entry>&#x2003;&#x2003;<maths id="MATH-US-00016" num="00016"><math overflow="scroll"> <mrow>  <mover accent="true">   <mi>w</mi>   <mi>&#x2dc;</mi>  </mover>  <mo>&#x2190;</mo>  <mrow>   <munder>    <mrow>     <mi>arg</mi>     <mo>&#x2062;</mo>     <mi>min</mi>    </mrow>    <mrow>     <mrow>      <msub>       <mi>w</mi>       <mo>+</mo>      </msub>      <mo>&#x2265;</mo>      <mn>0</mn>     </mrow>     <mo>,</mo>     <mtext> </mtext>     <mrow>      <mrow>       <msup>        <mn>1</mn>        <mi>T</mi>       </msup>       <mo>&#x2062;</mo>       <msub>        <mi>w</mi>        <mo>+</mo>       </msub>      </mrow>      <mo>=</mo>      <mn>1</mn>     </mrow>    </mrow>   </munder>   <mo>&#x2062;</mo>   <mrow>    <msup>     <mrow>      <mo>&#xf605;</mo>      <mrow>       <msub>        <mi>w</mi>        <mo>+</mo>       </msub>       <mo>-</mo>       <mrow>        <mo>(</mo>        <mrow>         <mover accent="true">          <mi>w</mi>          <mi>&#x2dc;</mi>         </mover>         <mo>-</mo>         <mrow>          <msub>           <mi>&#x3b7;</mi>           <mn>0</mn>          </msub>          <mo>&#xb7;</mo>          <mfrac>           <mrow>            <mover accent="true">             <mi>p</mi>             <mi>&#x2c6;</mi>            </mover>            <mo>(</mo>            <mrow>             <mtext>:,</mtext>             <msub>              <mi>s</mi>              <mi>t</mi>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mrow>            <mrow>             <mo>(</mo>             <mrow>              <msup>               <mover>                <mi>w</mi>                <mo>~</mo>               </mover>               <mi>T</mi>              </msup>              <mo>&#x2062;</mo>              <mover>               <mi>p</mi>               <mo>^</mo>              </mover>             </mrow>             <mo>)</mo>            </mrow>            <mo>&#x2062;</mo>            <mrow>             <mo>(</mo>             <msub>              <mi>s</mi>              <mi>t</mi>             </msub>             <mo>)</mo>            </mrow>           </mrow>          </mfrac>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>&#xf606;</mo>     </mrow>     <mn>2</mn>    </msup>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></entry></row><row><entry> </entry></row><row><entry>&#x2003;&#x2003;w &#x2190; &#x3b2;w + (1 &#x2212; &#x3b2;){tilde over (w)}.</entry></row><row><entry>&#x2003;else if SGD batch then</entry></row><row><entry>&#x2003;&#x2003;if t &#x2264; b then</entry></row><row><entry> </entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="133pt" align="left"/><colspec colname="2" colwidth="126pt" align="left"/><tbody valign="top"><row><entry>&#x2003;&#x2003;&#x2003;<maths id="MATH-US-00017" num="00017"><math overflow="scroll"> <mrow>  <mi>g</mi>  <mo>&#x2190;</mo>  <mrow>   <mi>g</mi>   <mo>+</mo>   <mrow>    <mfrac>     <mn>1</mn>     <mrow>      <mi>b</mi>      <mo>-</mo>      <mi>b_</mi>      <mo>+</mo>      <mn>1</mn>     </mrow>    </mfrac>    <mo>&#x2062;</mo>    <mfrac>     <mn>1</mn>     <mrow>      <mrow>       <mo>(</mo>       <mrow>        <msup>         <mi>w</mi>         <mi>T</mi>        </msup>        <mo>&#x2062;</mo>        <mover accent="true">         <mi>p</mi>         <mi>&#x2c6;</mi>        </mover>       </mrow>       <mo>)</mo>      </mrow>      <mo>&#x2062;</mo>      <mrow>       <mo>(</mo>       <msub>        <mi>s</mi>        <mi>t</mi>       </msub>       <mo>)</mo>      </mrow>     </mrow>    </mfrac>    <mo>&#x2062;</mo>    <mrow>     <mover accent="true">      <mi>p</mi>      <mi>&#x2c6;</mi>     </mover>     <mo>(</mo>     <mrow>      <mo>:</mo>      <mrow>       <mo>,</mo>       <msub>        <mi>s</mi>        <mi>t</mi>       </msub>      </mrow>     </mrow>     <mo>)</mo>    </mrow>    <mtext>  </mtext>   </mrow>  </mrow> </mrow></math></maths></entry><entry>\\ Batch from b_ to b, size 1, 4, 9,</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>&#x2003;&#x2003;16, . . . </entry></row><row><entry>&#x2003;&#x2003;end if</entry></row><row><entry>&#x2003;&#x2003;if t = b then</entry></row><row><entry> </entry></row><row><entry>&#x2003;&#x2003;&#x2003;<maths id="MATH-US-00018" num="00018"><math overflow="scroll"> <mrow>  <mi>w</mi>  <mo>&#x2190;</mo>  <mrow>   <munder>    <mrow>     <mi>arg</mi>     <mo>&#x2062;</mo>     <mi>min</mi>    </mrow>    <mrow>     <mrow>      <msub>       <mi>w</mi>       <mo>+</mo>      </msub>      <mo>&#x2265;</mo>      <mn>0</mn>     </mrow>     <mo>,</mo>     <mtext>  </mtext>     <mrow>      <mrow>       <msup>        <mn>1</mn>        <mi>T</mi>       </msup>       <mo>&#x2062;</mo>       <msub>        <mi>w</mi>        <mo>+</mo>       </msub>      </mrow>      <mo>=</mo>      <mn>1</mn>     </mrow>    </mrow>   </munder>   <mo>&#x2062;</mo>   <mrow>    <msup>     <mrow>      <mo>&#xf605;</mo>      <mrow>       <msub>        <mi>w</mi>        <mo>+</mo>       </msub>       <mo>-</mo>       <mrow>        <mo>(</mo>        <mrow>         <mi>w</mi>         <mo>-</mo>         <mrow>          <msub>           <mi>&#x3b7;</mi>           <mn>0</mn>          </msub>          <mo>&#x2062;</mo>          <mi>g</mi>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>&#xf606;</mo>     </mrow>     <mn>2</mn>    </msup>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></entry></row><row><entry> </entry></row><row><entry>&#x2003;&#x2003;&#x2003;g &#x2190; 0<sup>n</sup><sup><sub2>p</sub2></sup>.</entry></row><row><entry>&#x2003;&#x2003;&#x2003;b<sub>+</sub> &#x2190; b + ({square root over (b &#x2212; b_ + 1)} + 1)<sup>2</sup>, b_ &#x2190; b + 1, b &#x2190; b<sub>+</sub>. \\ Update batch.</entry></row><row><entry>&#x2003;&#x2003;end if</entry></row><row><entry>&#x2003;end if</entry></row><row><entry>&#x2003;t &#x2190; t + 1.</entry></row><row><entry>end while</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0167" num="0139">The function ProbUpdateAV1 may be represented by the following pseudocode. This pseudocode represents the AV1 calculation described above, where p is the vector {circumflex over (p)}(1, :) of the probability distribution for the given outcome value s<sub>t</sub>.</p><p id="p-0168" num="0000"><tables id="TABLE-US-00007" num="00007"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>Def ProbUpdateAV1(p, s, t, NumOfSyms) :</entry></row><row><entry>&#x2003;p<sub>0 </sub>&#x2190; 0.0076.</entry></row><row><entry>&#x2003;r &#x2190; 3 + (t &#x3e; 15) + (t &#x3e; 31) + (NumOfSyms &#x3e; 2) + (NumOfSyms &#x3e; 4)</entry></row><row><entry>&#x2003;p &#x2190; max((1 &#x2212; 2<sup>&#x2212;r</sup>)p,p<sub>0</sub>).</entry></row><row><entry>&#x2003;p(s) &#x2190; p(s) + 1 &#x2212; &#x3a3;<sub>i=1</sub><sup>NumOfSyms</sup>p(i)</entry></row><row><entry>&#x2003;return p</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0169" num="0140">Note that NumOfSyms (the number of symbols) is 2 in this example, but it could be a higher number. Also note that a is used as an input to ProbUpdateCABAC. While it is a constant in these examples, this would allow the value to be adaptive.</p><p id="p-0170" num="0141">Below is a table of the entropy resulting for various binary sequences using different context-based probability estimation techniques described herein. The table compares six techniques using nine different test sequences. The conventional CABAC and AV1 models/algorithms are baselines, against which different proposed models/algorithms are compared. As can be seen from the left-most column, the models for comparison are the SGD processing without SGD batch processing, the SGD batch processing, the fixed combination update algorithm for context-based probability estimation described with the parameters/variables above, and the fixed combination update algorithm for context-based probability estimation described with the parameters/variables above except that mode is set equal to 1 instead of 0. The proposed algorithms perform better than the baselines in most conditions. The differences generally relate to the parameter p<sub>62 </sub>in CABAC. Too sparse a dataset results in worse entropy when using this parameter.</p><p id="p-0171" num="0000"><tables id="TABLE-US-00008" num="00008"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="10"><colspec colname="1" colwidth="35pt" align="center"/><colspec colname="2" colwidth="28pt" align="center"/><colspec colname="3" colwidth="28pt" align="center"/><colspec colname="4" colwidth="28pt" align="center"/><colspec colname="5" colwidth="28pt" align="center"/><colspec colname="6" colwidth="28pt" align="center"/><colspec colname="7" colwidth="28pt" align="center"/><colspec colname="8" colwidth="35pt" align="center"/><colspec colname="9" colwidth="28pt" align="center"/><colspec colname="10" colwidth="35pt" align="center"/><thead><row><entry namest="1" nameend="10" align="center" rowsep="1"/></row><row><entry/><entry>allzbk</entry><entry>allzbk</entry><entry>zblk</entry><entry>zblk</entry><entry>allzbk</entry><entry>allzbk</entry><entry>zblk</entry><entry>allzbk</entry><entry>allzbk</entry></row><row><entry/><entry>16.0</entry><entry>8.0</entry><entry>4.0</entry><entry>8.0</entry><entry>16.1</entry><entry>8.1</entry><entry>4.1</entry><entry>16.2</entry><entry>8.2</entry></row><row><entry namest="1" nameend="10" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="10"><colspec colname="1" colwidth="35pt" align="center"/><colspec colname="2" colwidth="28pt" align="center"/><colspec colname="3" colwidth="28pt" align="center"/><colspec colname="4" colwidth="28pt" align="center"/><colspec colname="5" colwidth="28pt" align="char" char="."/><colspec colname="6" colwidth="28pt" align="center"/><colspec colname="7" colwidth="28pt" align="center"/><colspec colname="8" colwidth="35pt" align="center"/><colspec colname="9" colwidth="28pt" align="center"/><colspec colname="10" colwidth="35pt" align="center"/><tbody valign="top"><row><entry>SGD</entry><entry>2423</entry><entry>3160</entry><entry>1299</entry><entry>757.9/</entry><entry>245.0</entry><entry>504.2</entry><entry>96.71</entry><entry>352.8</entry><entry>748.3</entry></row><row><entry/><entry/><entry/><entry/><entry>689.1</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>Batch</entry><entry>2410</entry><entry>3160</entry><entry>1311</entry><entry>730.1/</entry><entry>243.8</entry><entry>503.5</entry><entry>98.28</entry><entry>353.3</entry><entry>747.1</entry></row><row><entry/><entry/><entry/><entry/><entry>703.1</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>Fixed</entry><entry>2404</entry><entry>3165</entry><entry>1312</entry><entry>779.0/</entry><entry>246.6</entry><entry>508.1</entry><entry>95.90</entry><entry>354.6</entry><entry>749.7</entry></row><row><entry/><entry/><entry/><entry/><entry>689.1</entry><entry/><entry/><entry/><entry/><entry/></row><row><entry>Modified</entry><entry>2414</entry><entry>3184</entry><entry>1410</entry><entry>712.5</entry><entry>248.2</entry><entry>500.2</entry><entry>96.04</entry><entry>354.3</entry><entry>744.4</entry></row><row><entry>Fixed</entry><entry/><entry/><entry/><entry/><entry/><entry/><entry/><entry/><entry/></row><row><entry>CABAC</entry><entry>2457</entry><entry>3209</entry><entry>1311</entry><entry>792.3</entry><entry>247.2</entry><entry>510.1</entry><entry>96.05</entry><entry>356.0</entry><entry>754.6</entry></row><row><entry>AV1</entry><entry>2479</entry><entry>3177</entry><entry>1322</entry><entry>692.5</entry><entry>248.2</entry><entry>505.6</entry><entry>97.11</entry><entry>354.0</entry><entry>751.3</entry></row><row><entry namest="1" nameend="10" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0172" num="0142">The underlying probability model from which symbols are emitted in video coding is typically unknown and/or is likely too complex to be fully described. As such, designing a good model for use in entropy coding can be a challenging problem in video coding. For example, a model that works well for one sequence may perform poorly for another sequence. A model, as used herein, can be, or can be a parameter in, lossless (entropy) coding. A model can be any parameter or method that affects probability estimation for the purpose of entropy coding. For example, a model can define the probability to be used to encode and decode the decision at an internal node in a token tree (such as described with respect to <figref idref="DRAWINGS">FIG. <b>7</b></figref>). In such a case, the two-pass process to learn the probabilities for a current frame may be simplified to a single-pass process by modifying a baseline model for probability estimation as described herein. In another example, a model may define a certain context derivation method. In such a case, implementations according to this disclosure can be used to combing probability estimations generated by a multitude of such methods. In yet another example, a model may define a completely new lossless coding algorithm.</p><p id="p-0173" num="0143">The probability update algorithm for entropy coding described herein may incorporate an average of different models with fast and slow update rates. An MLE estimator based on counting may be incorporated. Conditional probability and dictionary searching are options. The implementations also allow for adaptive fusion of models.</p><p id="p-0174" num="0144">For simplicity of explanation, the techniques herein are each depicted and described as a series of blocks, steps, or operations. However, the blocks, steps, or operations in accordance with this disclosure can occur in various orders and/or concurrently. Additionally, other steps or operations not presented and described herein may be used. Furthermore, not all illustrated steps or operations may be required to implement a technique in accordance with the disclosed subject matter.</p><p id="p-0175" num="0145">The aspects of encoding and decoding described above illustrate some examples of encoding and decoding techniques. However, it is to be understood that encoding and decoding, as those terms are used in the claims, could mean compression, decompression, transformation, or any other processing or change of data.</p><p id="p-0176" num="0146">The word &#x201c;example&#x201d; is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as &#x201c;example&#x201d; is not necessarily to be construed as being preferred or advantageous over other aspects or designs. Rather, use of the word &#x201c;example&#x201d; is intended to present concepts in a concrete fashion. As used in this application, the term &#x201c;or&#x201d; is intended to mean an inclusive &#x201c;or&#x201d; rather than an exclusive &#x201c;or.&#x201d; That is, unless specified otherwise or clearly indicated otherwise by the context, the statement &#x201c;X includes A or B&#x201d; is intended to mean any of the natural inclusive permutations thereof. That is, if X includes A; X includes B; or X includes both A and B, then &#x201c;X includes A or B&#x201d; is satisfied under any of the foregoing instances. In addition, the articles &#x201c;a&#x201d; and &#x201c;an&#x201d; as used in this application and the appended claims should generally be construed to mean &#x201c;one or more,&#x201d; unless specified otherwise or clearly indicated by the context to be directed to a singular form. Moreover, use of the term &#x201c;an implementation&#x201d; or the term &#x201c;one implementation&#x201d; throughout this disclosure is not intended to mean the same implementation unless described as such.</p><p id="p-0177" num="0147">Implementations of the transmitting station <b>102</b> and/or the receiving station <b>106</b> (and the algorithms, methods, instructions, etc., stored thereon and/or executed thereby, including by the encoder <b>400</b> and the decoder <b>500</b>) can be realized in hardware, software, or any combination thereof. The hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit. In the claims, the term &#x201c;processor&#x201d; should be understood as encompassing any of the foregoing hardware, either singly or in combination. The terms &#x201c;signal&#x201d; and &#x201c;data&#x201d; are used interchangeably. Further, portions of the transmitting station <b>102</b> and the receiving station <b>106</b> do not necessarily have to be implemented in the same manner.</p><p id="p-0178" num="0148">Further, in one aspect, for example, the transmitting station <b>102</b> or the receiving station <b>106</b> can be implemented using a general-purpose computer or general-purpose processor with a computer program that, when executed, carries out any of the respective methods, algorithms, and/or instructions described herein. In addition, or alternatively, for example, a special purpose computer/processor can be utilized which can contain other hardware for carrying out any of the methods, algorithms, or instructions described herein.</p><p id="p-0179" num="0149">The transmitting station <b>102</b> and the receiving station <b>106</b> can, for example, be implemented on computers in a video conferencing system. Alternatively, the transmitting station <b>102</b> can be implemented on a server, and the receiving station <b>106</b> can be implemented on a device separate from the server, such as a handheld communications device. In this instance, the transmitting station <b>102</b>, using an encoder <b>400</b>, can encode content into an encoded video signal and transmit the encoded video signal to the communications device. In turn, the communications device can then decode the encoded video signal using a decoder <b>500</b>. Alternatively, the communications device can decode content stored locally on the communications device, for example, content that was not transmitted by the transmitting station <b>102</b>. Other suitable transmitting and receiving implementation schemes are available. For example, the receiving station <b>106</b> can be a generally stationary personal computer rather than a portable communications device, and/or a device including an encoder <b>400</b> may also include a decoder <b>500</b>.</p><p id="p-0180" num="0150">Further, all or a portion of implementations of this disclosure can take the form of a computer program product accessible from, for example, a computer-usable or computer-readable medium. A computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor. The medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available.</p><p id="p-0181" num="0151">The above-described implementations and other aspects have been described to facilitate easy understanding of this disclosure and do not limit this disclosure. On the contrary, this disclosure is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation as is permitted under the law to encompass all such modifications and equivalent arrangements.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007260A1-20230105-M00001.NB"><img id="EMI-M00001" he="12.36mm" wi="76.20mm" file="US20230007260A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230007260A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US20230007260A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230007260A1-20230105-M00003.NB"><img id="EMI-M00003" he="5.67mm" wi="76.20mm" file="US20230007260A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230007260A1-20230105-M00004.NB"><img id="EMI-M00004" he="8.47mm" wi="33.53mm" file="US20230007260A1-20230105-M00004.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230007260A1-20230105-M00005.NB"><img id="EMI-M00005" he="5.67mm" wi="15.16mm" file="US20230007260A1-20230105-M00005.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230007260A1-20230105-M00006.NB"><img id="EMI-M00006" he="5.67mm" wi="16.93mm" file="US20230007260A1-20230105-M00006.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230007260A1-20230105-M00007.NB"><img id="EMI-M00007" he="6.01mm" wi="76.20mm" file="US20230007260A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230007260A1-20230105-M00008.NB"><img id="EMI-M00008" he="6.01mm" wi="76.20mm" file="US20230007260A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00009" nb-file="US20230007260A1-20230105-M00009.NB"><img id="EMI-M00009" he="6.01mm" wi="76.20mm" file="US20230007260A1-20230105-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00010" nb-file="US20230007260A1-20230105-M00010.NB"><img id="EMI-M00010" he="7.03mm" wi="76.20mm" file="US20230007260A1-20230105-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00011" nb-file="US20230007260A1-20230105-M00011.NB"><img id="EMI-M00011" he="5.25mm" wi="76.20mm" file="US20230007260A1-20230105-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00012" nb-file="US20230007260A1-20230105-M00012.NB"><img id="EMI-M00012" he="6.01mm" wi="76.20mm" file="US20230007260A1-20230105-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00013" nb-file="US20230007260A1-20230105-M00013.NB"><img id="EMI-M00013" he="8.13mm" wi="46.57mm" file="US20230007260A1-20230105-M00013.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00014" nb-file="US20230007260A1-20230105-M00014.NB"><img id="EMI-M00014" he="8.13mm" wi="45.89mm" file="US20230007260A1-20230105-M00014.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00015" nb-file="US20230007260A1-20230105-M00015.NB"><img id="EMI-M00015" he="5.67mm" wi="46.57mm" file="US20230007260A1-20230105-M00015.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00016" nb-file="US20230007260A1-20230105-M00016.NB"><img id="EMI-M00016" he="8.13mm" wi="45.89mm" file="US20230007260A1-20230105-M00016.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00017" nb-file="US20230007260A1-20230105-M00017.NB"><img id="EMI-M00017" he="6.69mm" wi="38.10mm" file="US20230007260A1-20230105-M00017.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00018" nb-file="US20230007260A1-20230105-M00018.NB"><img id="EMI-M00018" he="6.01mm" wi="35.98mm" file="US20230007260A1-20230105-M00018.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for entropy coding a sequence of symbols, comprising:<claim-text>determining a first probability model for entropy coding the sequence, the first probability model being one of a plurality of available probability models;</claim-text><claim-text>entropy coding at least one symbol of the sequence using a probability determined by the first probability model;</claim-text><claim-text>after entropy coding a respective symbol of the sequence, determining a first probability estimation to update the probability using the first probability model;</claim-text><claim-text>for a subsequent symbol relative to the at least one symbol of the sequence, determining a second probability estimation using a second probability model; and</claim-text><claim-text>entropy coding the subsequent symbol using the probability updated by a combination of the first probability estimation and the second probability estimation.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first probability model comprises a context-adaptive binary arithmetic coding (CABAC) model or an AV1 model.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first probability model comprises a Maximum Likelihood Estimate of a Bernoulli distribution.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one symbol comprises multiple symbols up to a minimum number of symbols.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>forming the combination as a linear combination of the first probability estimation and the second probability estimation.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the combination is a weighted combination of the first probability estimation and the second probability estimation.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the weighted combination uses a fixed weight.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the weighted combination uses a variable weight.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. (canceled)</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. (canceled)</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. (canceled)</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. (canceled)</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one symbol comprises a first symbol, the method comprising:<claim-text>entropy coding each symbol after the first symbol using the probability used for entropy coding a previous symbol updated using a combination of the first probability estimation and the second probability estimation.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. (canceled)</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. (canceled)</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. An apparatus, comprising:<claim-text>a processor configured to:</claim-text><claim-text>determine a first probability model for entropy coding the sequence, the first probability model being one of a plurality of available probability models;</claim-text><claim-text>entropy code at least one symbol of the sequence using a probability determined by the first probability model;</claim-text><claim-text>after entropy coding a respective symbol of the sequence, determine a first probability estimation to update the probability using the first probability model;</claim-text><claim-text>for a subsequent symbol relative to the at least one symbol of the sequence, determine a second probability estimation using a second probability model; and</claim-text><claim-text>entropy code the subsequent symbol using the probability updated by a combination of the first probability estimation and the second probability estimation.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the first probability model comprises a Maximum Likelihood Estimate of a Bernoulli distribution.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the at least one symbol comprises multiple symbols up to a minimum number of symbols.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor is configured to:<claim-text>form the combination as a linear combination of the first probability estimation and the second probability estimation.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the combination is a weighted combination of the first probability estimation and the second probability estimation, and the weighted combination uses one of a fixed weight or a variable weight.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the processor is configured to:<claim-text>determine, using a third probability model for entropy coding, a third probability estimation for the subsequent symbol, wherein the combination comprises a combination of the first probability estimation, the second probability estimation, and the third probability estimation.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The apparatus of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the combination of the first probability estimation, the second probability estimation, and the third probability estimation is a linear combination using a weighted average of the first probability estimation, the second probability estimation, and the third probability estimation.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The apparatus of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein a weight used for the weighted average is updated using a stochastic gradient descent (SGD).</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The apparatus of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the first probability model comprises an SGD decreasing step size, an SGD average argument, an SGD dynamic argument, or a SGD batch.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the at least one symbol comprises a first symbol, and the processor is configured to:<claim-text>entropy code each symbol after the first symbol using the probability used for entropy coding the previous symbol updated using a combination of the first probability estimation and the second probability estimation.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The apparatus of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the combination uses an adaptive weighting of the first probability estimation and the second probability estimation.</claim-text></claim></claims></us-patent-application>