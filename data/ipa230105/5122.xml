<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005123A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005123</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17802161</doc-number><date>20210108</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-038745</doc-number><date>20200306</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>75</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0002</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>751</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30141</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10061</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">SAMPLE OBSERVATION SYSTEM AND IMAGE PROCESSING METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Hitachi High-Tech Corporation</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KONDO</last-name><first-name>Naoaki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>HARADA</last-name><first-name>Minoru</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>MINEKAWA</last-name><first-name>Yohei</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2021/000484</doc-number><date>20210108</date></document-id><us-371c12-date><date>20220825</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The invention provides a sample observation system including a scanning electron microscope and a calculator. The calculator: (1) acquires a plurality of images captured by the scanning electron microscope; (2) acquires, from the plurality of images, a learning defect image including a defect portion and a learning reference image not including the defect portion; (3) calculates estimation processing parameters by using the learning defect image and the learning reference image; (4) acquires an inspection defect image including a defect portion; and (5) estimates a pseudo reference image by using the estimation processing parameters and the inspection defect image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="210.48mm" wi="157.31mm" file="US20230005123A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="220.81mm" wi="160.10mm" file="US20230005123A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="230.46mm" wi="140.80mm" file="US20230005123A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="100.75mm" wi="140.89mm" file="US20230005123A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="145.80mm" wi="127.08mm" file="US20230005123A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="90.93mm" wi="133.43mm" file="US20230005123A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="192.87mm" wi="148.76mm" file="US20230005123A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="182.96mm" wi="145.88mm" file="US20230005123A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="155.19mm" wi="155.19mm" file="US20230005123A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="166.12mm" wi="138.51mm" file="US20230005123A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="218.95mm" wi="148.76mm" file="US20230005123A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="156.38mm" wi="158.75mm" file="US20230005123A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="145.54mm" wi="130.64mm" file="US20230005123A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="137.08mm" wi="160.02mm" file="US20230005123A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="227.92mm" wi="140.72mm" file="US20230005123A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="202.95mm" wi="149.86mm" file="US20230005123A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="155.96mm" wi="155.45mm" file="US20230005123A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="201.08mm" wi="142.83mm" file="US20230005123A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to a sample observation system and an image processing method for observing a circuit pattern or a defect formed on a semiconductor wafer as a sample using a charged particle microscope or the like.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">In manufacture of semiconductor wafers, it is important to quickly start up a manufacturing process and early shift to a mass production system with a high yield in order to ensure profits. For this purpose, various inspection devices, devices for observing defects on a sample, and measurement devices are introduced into a manufacturing line.</p><p id="p-0004" num="0003">The device for observing defects on a sample is a device for capturing an image of a defect position on a wafer at a high resolution based on defect position coordinates (coordinate information indicating a position of a defect on a sample (wafer)) output from a defect inspection device and outputting the image, and a defect observation device (hereinafter, referred to as a review SEM) using a scanning electron microscope (SEM) is widely used.</p><p id="p-0005" num="0004">In a mass production line of semiconductors, automation of a sample observation operation is desired, and the review SEM has a function of performing an automatic defect image collection process (ADR: Automated Defect Review) of automatically collecting images at defect positions in a sample and a function of performing an automatic defect image classification process (ADC: Automated Defect Classification) of automatically classifying the collected defect images.</p><p id="p-0006" num="0005">Since the defect position coordinates output by the defect inspection device include an error, the ADR has a function of obtaining an observation image by re-detecting a defect from an image captured with a wider field of view around the defect position coordinates output by the defect inspection device and imaging a position of the re-detected defect at a high magnification. As a method for detecting a defect from an SEM image, JP-A-2001-189358 (PTL 1) discloses a method for detecting a defect by comparing an image obtained by imaging a defect portion (hereinafter, referred to as a defect image) with a reference image using an image obtained by imaging a region in which a circuit pattern same as that of the defect portion is formed as the reference image.</p><p id="p-0007" num="0006">WO2019/216303 (PTL 2) discloses a method for detecting a defect candidate by generating a reference image based on a database image corresponding to design data and a captured image, and comparing the captured image with the reference image.</p><p id="p-0008" num="0007">Non-PTL 1 discloses a method for learning a correspondence relationship between an input image and an output image using a neural network.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0009" num="0008">PTL 1: JP-A-2001-189358</p><p id="p-0010" num="0009">PTL 2: WO2019/216303</p><heading id="h-0005" level="1">Non Patent Literature</heading><p id="p-0011" num="0010">Non-PTL 1: Olaf Ronneberger, Philipp Fischer, Thomas Brox, &#x201c;U-Net: Convolutional Networks for Biomedical ImageSegmentation&#x201d;, arXiv preprint arXiv: 1505.04597 (2015)</p><heading id="h-0006" level="1">SUMMARY OF INVENTION</heading><heading id="h-0007" level="1">Technical Problem</heading><p id="p-0012" num="0011">A system for observing a defect on a sample according to the invention (hereinafter, referred to as a sample observation system) relates to a system for imaging a sample such as a semiconductor wafer, acquiring an image, and observing the image.</p><p id="p-0013" num="0012">In the sample observation system, it is important to acquire a larger number of images per unit time (to operate with high throughput). PTL 1 discloses the method for acquiring the reference image for each defect image and performing defect detection. However, if the reference image can be estimated from the defect image, the acquisition of the reference image can be omitted, and thus the throughput of a sample observation can be improved. PTL 2 discloses the method for generating the reference image based on the database image corresponding to the design data and the captured image. However, the design data is highly confidential information, and is not allowed to be taken out to a semiconductor manufacturing line, in particular, a mass production line in which high throughput is required, and it may be difficult to use the design data. As described above, when the design data cannot be used, it is difficult to estimate the reference image based on the defect image, and any of the above-described known examples does not refer to a method for solving this problem.</p><p id="p-0014" num="0013">An object of the invention is to solve the above-described problems in the related art, and to make it possible to estimate a reference image based on a defect image without using design data and to improve the throughput of a sample observation.</p><p id="p-0015" num="0014">Another object of the invention is to solve the above-described problems in the related art, and to make it possible to estimate the defect portion from the defect image without using the design data and to improve the throughput of the sample observation.</p><heading id="h-0008" level="1">Solution to Problem</heading><p id="p-0016" num="0015">A sample observation system including a scanning electron microscope and a calculator, and a method, or the calculator provided in the sample observation system according to the invention: (1) acquires a plurality of images captured by the scanning electron microscope; (2) acquires, from the plurality of images, a learning defect image including a defect portion and a learning reference image not including the defect portion; (3) calculates an estimation processing parameter using the learning defect image and the learning reference image; (4) acquires an inspection defect image including a defect portion; and (5) estimates a pseudo reference image by using the estimation processing parameter and the inspection defect image.</p><p id="p-0017" num="0016">From another point of view, a sample observation system including a scanning electron microscope and a calculator, and a method, or the calculator provided in the sample observation system according to the invention: acquires a plurality of images captured by the scanning electron microscope; acquires, from the plurality of images, a learning defect image including a defect portion; calculates an estimation processing parameter using the learning defect image; acquires an inspection defect image including a defect portion; and estimates the defect portion in the inspection defect image by using the estimation processing parameter and the inspection defect image.</p><heading id="h-0009" level="1">Advantageous Effects of Invention</heading><p id="p-0018" num="0017">According to the invention, it is possible to estimate a reference image based on a defect image even when design data cannot be used in a sample observation. Further, by estimating a reference image, it is possible to omit acquisition of the reference image, and it is possible to improve throughput of the sample observation.</p><p id="p-0019" num="0018">In addition, according to the invention, it is possible to estimate a defect portion from a defect image in the sample observation. Further, by estimating the defect portion from the defect image, it is possible to omit the acquisition of the reference image, and it is possible to improve the throughput of the sample observation.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0010" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram showing a schematic configuration of a sample observation system according to a first embodiment.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart showing a flow of sample observation in the sample observation system according to the first embodiment.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram showing an example of a result of identifying a defect portion in the sample observation of the sample observation system according to the first embodiment.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart showing a flow of a learning sequence of the sample observation system according to the first embodiment.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of a process of acquiring a learning image pair in the learning sequence of the sample observation system according to the first embodiment.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of a process of calculating an estimation processing parameter in the learning sequence of the sample observation system according to the first embodiment.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram showing a process of aligning an image pair and a process of cutting out an image in the calculation of the estimation processing parameter of the sample observation system according to the first embodiment.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram showing a configuration of a neural network for estimating a pseudo reference image in the sample observation system according to the first embodiment.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a GUI for setting a learning image size in the sample observation system according to the first embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a GUI for setting a learning end condition in the sample observation system according to the first embodiment.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a GUI for confirming an estimation error for each learning step in the sample observation system according to the first embodiment.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a GUI for displaying a designated image among the pseudo reference images estimated based on a learning defect image in the sample observation system according to the first embodiment.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a timing chart of a process of acquiring an observation defect image by a sample observation system of related art to be compared with the sample observation system according to the first embodiment.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a timing chart of a process of acquiring an observation defect image by the sample observation system according to the first embodiment.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart showing a flow of a learning sequence of the sample observation system according to a second embodiment.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a block diagram showing a schematic configuration of a calculator in a sample observation system according to a third embodiment.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart showing a flow of sample observation in the sample observation system according to the third embodiment.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a flowchart of a process of calculating an estimation processing parameter in the learning sequence of the sample observation system according to the third embodiment.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a block diagram showing a configuration of a neural network for estimating a defect portion in the sample observation system according to the third embodiment.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram showing equations used when the pseudo reference image is estimated based on partial learning defect images according to the first to third embodiments.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram showing equations used in the process of the third embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0011" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0041" num="0040">Embodiments will be described with reference to the drawings. The embodiments described below do not limit the invention according to the claims, and all of the elements and combinations thereof described in the embodiments are not necessarily essential to the solution of the invention.</p><heading id="h-0012" level="1">First Embodiment</heading><heading id="h-0013" level="2">&#x3c;System Configuration&#x3e;</heading><p id="p-0042" num="0041">A sample observation system according to the present embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In the present embodiment, a sample observation system including a scanning electron microscope (SEM) will be described as an imaging apparatus for imaging a sample. However, the imaging apparatus according to the present embodiment may be an imaging apparatus other than the SEM, and may be an optical microscope or an imaging apparatus using charged particles such as ions. In addition, although an image obtained by imaging a defect on a semiconductor wafer will be described as an image to be observed, the image may be an image obtained by imaging another sample such as a flat panel display or a biological sample.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a configuration of a sample observation apparatus <b>100</b> according to the present embodiment. The sample observation apparatus (also referred to as a sample observation system) <b>100</b> includes an SEM <b>101</b> that images a sample, a defect inspection device <b>102</b> that detects a defect of the sample, and a calculator <b>103</b>.</p><p id="p-0044" num="0043">The SEM <b>101</b> includes a stage <b>105</b> on which a sample wafer <b>104</b> to be observed is placed and which is movable on an X-Y plane or in an X-Y-Z space, an electron source <b>107</b> which generates an electron beam <b>106</b> with which the sample wafer <b>104</b> is irradiated, a detector <b>110</b> which detects a secondary electron <b>108</b>, a backscattered electron <b>109</b>, and the like generated from the sample wafer <b>104</b> irradiated with the electron beam <b>106</b>, an electron lens (not shown) which converges the electron beam <b>106</b> on the sample wafer <b>104</b>, and a deflector (not shown) for scanning the sample wafer <b>104</b> with the electron beam <b>106</b>.</p><p id="p-0045" num="0044">The defect inspection device <b>102</b> is a device that captures an optical image of a wafer surface and inspects a defect by comparing the optical image with an image of a non-defective portion (for example, an image of an adjacent chip). However, such an inspection device is affected by an illumination wavelength thereof, and a resolution limit of the acquired image is about several hundreds of nanometers. Therefore, regarding a defect an order of several tens of nanometers on the wafer, only a presence or absence of the defect can be detected, and defect coordinates on the wafer are output.</p><p id="p-0046" num="0045">The calculator <b>103</b> includes a user interface (denoted as a user I/F in the drawing) <b>111</b>, a network interface (denoted as a network I/F in the drawing) <b>112</b>, a control unit <b>113</b> that controls the SEM <b>101</b>, a storage unit <b>114</b> that stores information, and a processing unit <b>115</b>. Examples of the storage unit <b>114</b> include a magnetic disk device, and a volatile or nonvolatile semiconductor memory device, and the storage unit <b>114</b> may be another storage device. The storage unit may include a plurality of the above-described devices. In addition, an example of the processing unit may be any one of a CPU, a GPU, an FPGA, and an LSI, or may be implemented by a combination thereof. The control unit <b>113</b> described later may be a subsystem (also referred to as a control subsystem) different from the calculator <b>103</b>. In the present specification, one calculator <b>103</b> is described as the example, but a plurality of calculators may be used. For example, it is conceivable that processing for GUI display is performed by a calculator for display such as a tablet and a smartphone, and other image processing is performed by another calculator. In addition, the calculator may include a plurality of the above-described components. The user interface <b>111</b> is a touch panel, a display, a keyboard, a mouse, or the like, and may be another device as long as the device can receive an operation from an operator (user) and display information. The network interface <b>112</b> is an interface for communicating with an external device such as the defect inspection device or the SEM via a network.</p><p id="p-0047" num="0046">Configurations of the control unit <b>113</b>, the processing unit <b>115</b>, and the storage unit <b>114</b> according to the present embodiment will be described.</p><p id="p-0048" num="0047">The control unit <b>113</b> includes a stage control unit <b>116</b>, an electron beam control unit <b>117</b>, and a detector control unit <b>118</b>. The stage control unit <b>116</b> controls movement, stop, and the like of the stage <b>105</b>. The electron beam control unit <b>117</b> controls the deflector (not shown) so that the irradiation with the electron beam <b>106</b> is within a predetermined field of view, and controls a scan region of the electron beam <b>106</b> on the sample wafer <b>104</b>. The detector control unit <b>118</b> samples a signal from the detector <b>110</b> in synchronization with scanning of the electron beam <b>106</b> driven by the deflector (not shown), adjusts a gain, an offset, and the like, and generates a digital image. The control unit <b>113</b> may be implemented by, for example, a circuit, or may be implemented by the CPU, the GPU, the FPGA, or the LSI.</p><p id="p-0049" num="0048">The storage unit <b>114</b> includes an image storage area <b>119</b> for storing the digital image generated by the detector control unit <b>118</b> together with accessory information, a recipe storage area <b>120</b> for storing a recipe including information on a device or a manufacturing process, imaging conditions of an image, and the like, and an estimation processing parameter storage area <b>121</b> for storing parameters related to an estimation process of a pseudo reference image. Note that each area does not necessarily have to be a divided area. A data arrangement in the storage unit <b>114</b> may be in any format as long as the recipe, the parameters, and estimation processing parameters can be stored.</p><p id="p-0050" num="0049">The processing unit <b>115</b> includes an estimation processing parameter calculation unit <b>122</b> that calculates the estimation processing parameters for estimating the pseudo reference image based on a defect image, a pseudo reference image estimation unit <b>123</b> that estimates the pseudo reference image based on the estimation processing parameters, and a defect portion identifying unit <b>124</b> that identifies a defect portion in the defect image. When the processing unit <b>115</b> is a device, such as the CPU or the GPU, that executes a predetermined process by a program, programs (collectively referred to as image processing programs) corresponding to the estimation processing parameter calculation unit <b>122</b>, the pseudo reference image estimation unit <b>123</b>, and the defect portion identifying unit <b>124</b> are stored in the storage unit <b>114</b>. Then, the processing unit <b>115</b> reads the programs to implement these processes.</p><p id="p-0051" num="0050">Next, a process performed in the present embodiment will be described. Unless otherwise specified, the following processes related to the first to third embodiments are performed by the calculator <b>103</b>. More specifically, the control of the SEM <b>101</b> is a process performed by the control unit <b>113</b>, and other processes are performed by the processing unit <b>115</b>, which is an example of division of the processes. When the processing unit <b>115</b> is the CPU or the GPU, the processing unit <b>115</b> reads and implements the program (hereinafter, may be referred to as the image processing program) stored in the storage unit <b>114</b>. However, even in a case where the processes performed by the estimation processing parameter calculation unit <b>122</b>, the pseudo reference image estimation unit <b>123</b>, or the defect portion identifying unit <b>124</b> are described, these units are included in the processing unit <b>115</b>, and thus it may be considered that the processes are processed by the processing unit <b>115</b>.</p><heading id="h-0014" level="2">&#x3c;Sample Observation&#x3e;</heading><p id="p-0052" num="0051">An observation process of a sample will be described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0053" num="0052">First, the process waits until the sample wafer <b>104</b> to be observed is loaded onto the stage <b>105</b> (S<b>201</b>). Next, a recipe corresponding to the sample wafer to be observed is read from the recipe storage area (S<b>202</b>). A semiconductor pattern formed on the sample wafer <b>104</b> is manufactured through a large number of manufacturing processes, and an appearance may be greatly different in each process. Further, characteristics of the sample, such as ease of charging, may be different. Therefore, it is common to adjust and store imaging conditions for each device or each manufacturing process. For the same reason, estimation accuracy is improved by managing the estimation processing parameters of the pseudo reference image for each process.</p><p id="p-0054" num="0053">Next, information on the defect coordinates output from the defect inspection device <b>102</b> is received or read (S<b>203</b>). Here, all of the received or read defect coordinates may be set to be observed, or the defect coordinates sampled based on user designation conditions may be set to be observed. Next, it is checked whether the estimation processing parameters corresponding to a process in which the sample wafer <b>104</b> is processed is stored in the estimation processing parameter storage area <b>121</b> (S<b>204</b>), and when the estimation processing parameters are not stored (&#x201c;ABSENT&#x201d; in <figref idref="DRAWINGS">FIG. <b>2</b></figref>), the estimation processing parameters are calculated by a learning sequence described later, and a result is stored in the estimation processing parameter storage area <b>121</b> (S<b>205</b>). The information on the defect coordinates may be received from the defect inspection device described above via the network interface <b>112</b> or may be read from a portable storage medium such as a USB memory. Note that S<b>205</b> may be implemented by the estimation processing parameter calculation unit <b>122</b>, or may be implemented by the entire calculation unit <b>115</b> as described above.</p><p id="p-0055" num="0054">Next, the estimation processing parameters of the pseudo reference image are read from the estimation processing parameter storage area <b>121</b> (S<b>206</b>). Next, defects to be observed on the sample wafer <b>104</b> are sequentially imaged using the SEM <b>101</b>, and the following series of observations from S<b>207</b> to S<b>213</b> are performed.</p><p id="p-0056" num="0055">First, through the control unit <b>113</b>, the stage <b>105</b> is controlled and moved, so that the defects to be observed on the sample wafer <b>104</b> are included in an imaging field of view of the SEM <b>101</b> (S<b>207</b>). Next, an inspection defect image is acquired (S<b>208</b>). The acquisition of the image is performed, for example, by the SEM <b>101</b> irradiating and scanning a relatively wide region including the defects to be observed on the sample wafer <b>104</b> with the electron beam <b>106</b>, imaging the relatively wide region including the defects to be observed by the detector <b>110</b> detecting the generated secondary electrons <b>108</b> and backscattered electrons <b>109</b>, the detector control unit <b>118</b> processing a detection signal from the detector <b>110</b> obtained by the imaging, and acquiring a low-magnification image of the relatively wide region including the defects to be observed.</p><p id="p-0057" num="0056">Next, image preprocessing such as noise removal and luminance non-uniformity correction is applied to the inspection defect image (S<b>209</b>), and the pseudo reference image estimation unit <b>123</b> estimates the pseudo reference image based on the inspection defect image subjected to the image preprocessing by using the estimation processing parameters read from the estimation processing parameter storage area <b>121</b> (S<b>210</b>). This pseudo reference image corresponds to an image in which a circuit pattern same as that of the inspection defect image is observed and which does not include the defects. In the estimation of the pseudo reference image, design data is not necessary, and the estimation processing parameters and the inspection defect image subjected to the image preprocessing are used.</p><p id="p-0058" num="0057">Next, the defect portion identifying unit <b>124</b> compares the inspection defect image with the pseudo reference image, and identifies a defect portion from the inspection defect image (S<b>211</b>). As a method for specifying a defect portion, a method described in PTL 1 or the like may be used.</p><p id="p-0059" num="0058">Next, the specified defect portion is imaged at a large magnification by narrowing the field of view, a defect image for observation (hereinafter referred to as an observation defect image) is acquired (S<b>212</b>), and the inspection defect image, the pseudo reference image, and the observation defect image are stored in the image storage area <b>119</b> (S<b>213</b>).</p><p id="p-0060" num="0059">This is the end of the flow in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The series of processes may be repeatedly executed on all the defects to be observed of the sample wafer <b>104</b>, or may be performed on a part of the defects to be observed based on other criteria. Note that S<b>205</b> may be managed by the estimation processing parameter calculation unit <b>122</b>, or may be managed by the entire processing unit <b>115</b>.</p><heading id="h-0015" level="2">&#x3c;&#x3c;Example of Result of Identifying Defect Portion&#x3e;&#x3e;</heading><p id="p-0061" num="0060">An example of a result of identifying the defect portion in S<b>211</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. When a defect portion is identified using an inspection defect image <b>301</b> and a pseudo reference image <b>302</b>, an identified defect portion image <b>303</b> in which the defect portion and a region other than the defect portion can be distinguished from each other is obtained. In the identified defect portion image <b>303</b>, for example, a pixel value of the defect portion may be set to 1, and a pixel value of the region other than the defect portion may be set to 0.</p><heading id="h-0016" level="2">&#x3c;&#x3c;Learning Sequence&#x3e;&#x3e;</heading><p id="p-0062" num="0061">The process in S<b>205</b> of calculating the estimation processing parameters by the learning sequence and storing the result in the estimation processing parameter storage area <b>121</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0063" num="0062">First, a size of an image to be acquired for learning is acquired (S<b>401</b>). Next, a defect to be learned is set (S<b>402</b>). Here, all the defect coordinates read in S<b>203</b> may be set to be learned, or the defect coordinates sampled based on the user designation conditions may be set to be learned. Next, a pair of an image including the defect which is set to be learned (hereinafter, referred to as a learning defect image) and an image including a region designed to form a circuit pattern same as that around a position of the defect to be learned (hereinafter, referred to as a learning reference image) are acquired (S<b>403</b>).</p><heading id="h-0017" level="2">&#x3c;&#x3c;&#x3c;Details of S<b>403</b> of Acquisition of Learning Image Pair&#x3e;&#x3e;&#x3e;</heading><p id="p-0064" num="0063">Here, S<b>403</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. First, the stage <b>105</b> is controlled and moved, so that the region (hereinafter, referred to as a reference region) designed (or assumed) to form the circuit pattern same as that around the position of the defect to be learned is included in the imaging field of view of the SEM <b>101</b> (S<b>501</b>). Note that information with higher accuracy than the design data is not necessary for identifying the reference region. In a semiconductor wafer, a plurality of chips designed to form the same circuit pattern are disposed on the wafer, and thus, as a simplest method, it is conceivable to set a region shifted from the defect coordinates by one chip as the reference region. However, the reference region may be selected by another method.</p><p id="p-0065" num="0064">Next, the reference region on the sample wafer <b>104</b> is irradiated and scanned with the electron beam <b>106</b>, the generated secondary electrons <b>108</b> and backscattered electrons <b>109</b> are detected by the detector <b>110</b> to image the reference region, and a detection signal from the detector <b>110</b> obtained by the imaging is processed by the detector control unit <b>118</b> to acquire the learning reference image so as to have a size equal to or larger than the size acquired in S<b>401</b> (S<b>502</b>).</p><p id="p-0066" num="0065">Next, the stage <b>105</b> is controlled and moved, so that a region including the defect to be learned (hereinafter, referred to as a defect region) is included in the imaging field of view of the SEM <b>101</b> (S<b>503</b>). Next, the defect region on the sample wafer <b>104</b> is irradiated and scanned with the electron beam <b>106</b>, the generated secondary electrons <b>108</b> and backscattered electrons <b>109</b> are detected by the detector <b>110</b> to image the defect region, and a detection signal from the detector <b>110</b> obtained by the imaging is processed by the detector control unit <b>118</b> to acquire the learning defect image so as to have a size equal to or larger than the size acquired in S<b>401</b> (S<b>504</b>).</p><p id="p-0067" num="0066">After the learning defect image and the learning reference image are acquired, accessory information is added, so that the learning defect image and the learning reference image are paired and stored in the image storage area <b>119</b> (S<b>505</b>).</p><p id="p-0068" num="0067">The above is the description using <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Either the learning reference image or the learning defect image may be acquired first. That is, the processes of S<b>501</b> to S<b>502</b> may be executed after S<b>503</b> to S<b>504</b>.</p><p id="p-0069" num="0068">&#x3c;&#x3c;&#x3c;Parallel Process of Learning Sequence and Defect Observation Process&#x3e;&#x3e;&#x3e;</p><p id="p-0070" num="0069">Return to the description of <figref idref="DRAWINGS">FIGS. <b>4</b></figref>. S<b>404</b> and S<b>405</b> described below are processes intended to have merit of performing defect observation even during a time of the learning sequence requiring the time by performing processing necessary for identifying a defect portion and the defect observation in parallel with the learning sequence.</p><p id="p-0071" num="0070">The learning defect image acquired in S<b>403</b> is compared with the learning reference image, and a defect portion is identified in the same manner as S<b>211</b> (S<b>404</b>). Next, the identified defect portion is imaged at a high magnification by narrowing the field of view, an observation defect image is acquired, the observation defect image is stored in the image storage area <b>119</b>, and is excluded from the defects to be observed in S<b>207</b> to S<b>213</b> (S<b>405</b>). The processes of S<b>403</b> to S<b>405</b> described above are repeatedly executed for all or a part of the defects to be learned of the sample wafer <b>104</b>.</p><p id="p-0072" num="0071">The above is the parallel process of the learning sequence and the defect observation process.</p><p id="p-0073" num="0072">Finally, a learning end condition of the estimation processing parameters is acquired (S<b>406</b>), and the estimation processing parameter calculation unit <b>122</b> calculates the estimation processing parameters for estimating the pseudo reference image (S<b>407</b>: details will be described later).</p><p id="p-0074" num="0073">The above is the description of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Here, with respect to S<b>404</b> and S<b>405</b>, in addition to the merit of the parallel processing described above, the acquisition of the observation defect image in S<b>212</b> is not required, and thus S<b>404</b> and S<b>405</b> have merit that the sample observation can be efficiently performed. In addition, S<b>404</b> to S<b>405</b> may be omitted, and when S<b>404</b> to S<b>405</b> are omitted, the defect to be learned is not excluded from the defects to be observed in S<b>207</b> to S<b>213</b>.</p><heading id="h-0018" level="2">&#x3c;&#x3c;&#x3c;Details of Calculation of Estimation Processing Parameters in S<b>407</b>&#x3e;&#x3e;&#x3e;</heading><p id="p-0075" num="0074">The calculation process of the estimation processing parameters in S<b>407</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In the calculation of the estimation processing parameters, the design data is not necessary, and the learning defect image and the learning reference image are used.</p><p id="p-0076" num="0075">First, similar to S<b>209</b>, the image preprocessing such as the noise removal and the luminance non-uniformity correction is applied to both the learning defect image and learning reference image acquired in S<b>403</b> (S<b>601</b>). Next, in order to absorb a stage movement error or an electron beam irradiation positioning error, the pair of the learning defect image and the learning reference image are aligned based on a predetermined evaluation value, and alignment amounts AX and AY between the images are obtained (S<b>602</b>). As the predetermined evaluation value, a normalized cross-correlation coefficient, a mean square error, or the like may be used, and the alignment may be performed based on a position at which the evaluation value is maximum or minimum. When image resolutions (the number of pixels per image in the same field of view) are different from each other, the image resolutions are aligned by linear interpolation or the like before the alignment is performed.</p><p id="p-0077" num="0076">Next, based on the alignment amounts, a partial learning defect image is cut out from the learning defect image, and a partial learning reference image is cut out from the learning reference image (S<b>603</b>).</p><p id="p-0078" num="0077">Here, the processes of S<b>601</b> and S<b>602</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. <figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a learning defect image <b>701</b> and a learning reference image <b>702</b>. In S<b>601</b>, these two images are aligned based on the predetermined evaluation value to obtain an image alignment result <b>703</b>. In the image alignment result <b>703</b>, a defect portion included only in the learning defect image <b>701</b> is not illustrated. In S<b>602</b>, based on the alignment amounts AX and AY, as a region common to the learning defect image and the learning reference image, a partial learning defect image <b>704</b> is cut out from the learning defect image <b>701</b> and a partial learning reference image <b>705</b> is cut out from the learning reference image <b>702</b>.</p><p id="p-0079" num="0078">Return to the description of <figref idref="DRAWINGS">FIG. <b>6</b></figref> again. Learning estimation processing parameters are initialized (S<b>604</b>). At this time, previously learned estimation processing parameters may be read from the estimation processing parameter storage area <b>121</b> and used as initial values of the learning estimation processing parameters. Next, a pseudo defect image is estimated from the partial learning defect image based on the learning estimation processing parameters (S<b>605</b>), an estimation error between the partial learning reference image and the pseudo defect image is calculated (S<b>606</b>), and the learning estimation processing parameters are updated so that the estimation error becomes small (S<b>607</b>). Next, it is checked whether the learning end condition acquired in S<b>406</b> is satisfied (S<b>608</b>). When the learning end condition is satisfied, the learning estimation processing parameters are stored in the estimation processing parameter storage area <b>121</b> as the estimation processing parameters (S<b>609</b>). When the learning end condition is not satisfied, the process returns to S<b>605</b> again.</p><p id="p-0080" num="0079">This is the end of the flow in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0081" num="0080">For example, the learning end condition may be assumed as follows.</p><p id="p-0082" num="0081">(Learning end condition 1) The estimation error is compared with a preset estimation error threshold value TH, and the estimation error is smaller than the estimation error threshold value TH.</p><p id="p-0083" num="0082">(Learning end condition 2) An operation of ending learning is received from a user.</p><p id="p-0084" num="0083">(Learning end condition 3) The processes from S<b>605</b> to S<b>608</b> repeat for a preset specified number of times MR.</p><heading id="h-0019" level="2">&#x3c;&#x3c;Application of Neural Network&#x3e;&#x3e;</heading><p id="p-0085" num="0084">As a method for estimating the pseudo reference image based on the partial learning defect image in S<b>605</b>, a neural network described in Non-Patent Literature 1 may be used. This neural network is also used when the pseudo reference image is estimated from the inspection defect image in S<b>210</b>. Specifically, a U-shaped neural network called U-net as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> may be used. Here, Y represents an input image. F11 (Y), F12 (Y), F13 (Y), F21 (Y), F22 (Y), F23 (Y), F24 (Y), F31 (Y), and F32 (Y) represent intermediate data. F (Y) represents an estimation result of the pseudo reference image.</p><p id="p-0086" num="0085">The intermediate data and a final result are calculated by Equations 1 to 10 in <figref idref="DRAWINGS">FIG. <b>20</b></figref>. In Equations 1 to 10, &#x201c;*&#x201d; represents a convolution operation, DS represents an operation of applying a 2&#xd7;2 max filter to an input image and reducing the input image to a half in a space (XY) direction, US represents an operation of up-sampling the input image to a size twice in the space direction, and CC represents an operation of combining two input images in a channel direction.</p><p id="p-0087" num="0086">Here, meanings of variables used in Equations 1 to 10 are as follows:</p><p id="p-0088" num="0087">W1 is c1 filters of c0&#xd7;f1&#xd7;f1 size</p><p id="p-0089" num="0088">c0 is the number of channels of the input image</p><p id="p-0090" num="0089">f1 is a size of a spatial filter</p><p id="p-0091" num="0090">A c1-dimensional feature map is obtained by convolving the filter of c0&#xd7;f1&#xd7;f1 size to the input image for c1 times.</p><p id="p-0092" num="0091">The meanings of the variables used again in Equations 1 to 10 are described below:</p><p id="p-0093" num="0092">B1 is a c1-dimensional vector (bias component corresponding to c1 filters)</p><p id="p-0094" num="0093">W2 is a filter of c1&#xd7;f2&#xd7;f2 size</p><p id="p-0095" num="0094">B2 is a c2-dimensional vector</p><p id="p-0096" num="0095">W3 is a filter of c2&#xd7;f3&#xd7;f3 size</p><p id="p-0097" num="0096">B3 is a c3-dimensional vector</p><p id="p-0098" num="0097">W4 is a filter of c3&#xd7;f4&#xd7;f4 size</p><p id="p-0099" num="0098">B4 is the c2-dimensional vector</p><p id="p-0100" num="0099">W5 is a filter of (c2&#xd7;2)&#xd7;f5&#xd7;f5 size</p><p id="p-0101" num="0100">B5 is the c2-dimensional vector</p><p id="p-0102" num="0101">W6 is a filter of c2&#xd7;f6&#xd7;f6 size</p><p id="p-0103" num="0102">B6 is the c1-dimensional vector</p><p id="p-0104" num="0103">W7 is a filter of (c1&#xd7;2)&#xd7;f7&#xd7;f7 size</p><p id="p-0105" num="0104">B7 is a c4-dimensional vector</p><p id="p-0106" num="0105">W8 is a filter of c4&#xd7;f8&#xd7;f8 size</p><p id="p-0107" num="0106">B8 is a c5-dimensional vector.</p><p id="p-0108" num="0107">Among the above, c0 and c5 are values determined by the number of channels of the partial learning defect image and the partial learning reference image. Further, f1 to f8 and c1 to c4 are hyperparameters determined by the user before the learning sequence, and may be set to, for example, f1 to f8=3, c1=8, c2=16, c3=32, and c4=64. The parameters calculated by the calculation process of the estimation processing parameters (S<b>405</b>) are W1 to W8 and B1 to B8.</p><p id="p-0109" num="0108">Other configurations may be used as the configuration of the neural network described above. Although a structure of the U-net having a maximum depth of 3 is described in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, for example, the depth may be changed, and a network having a maximum depth of 4 or more may be used.</p><p id="p-0110" num="0109">The process (S<b>607</b>) of calculating the estimation error is a process of evaluating a difference (error) between the estimation result F (Y) and the partial learning reference image, and the parameters are updated so that the estimation error obtained in this process becomes small. As a method for quantifying a difference (error) between images, a mean square error or the like may be used.</p><p id="p-0111" num="0110">In the process (S<b>608</b>) of updating the estimation processing parameters, a general error back propagation algorithm may be used in learning of the neural network. In addition, when the estimation error is calculated, all the pairs of the acquired learning defect image and learning reference image may be used, or a mini-batch method may be used. That is, a plurality of image pairs may be randomly extracted from the pairs of the learning defect image and the learning reference image, and the parameters may be repeatedly updated. Further, a patch image may be randomly cut out from one image pair and used as the input image Y of the neural network. As a result, the learning can be efficiently performed.</p><heading id="h-0020" level="2">&#x3c;GUI&#x3e;</heading><p id="p-0112" num="0111">Next, the GUI displayed on the user interface <b>111</b> will be described.</p><p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a GUI <b>900</b> for setting a size of a learning image in S<b>401</b>. On this GUI <b>900</b>, an inspection defect image size <b>901</b> and an imaging field of view <b>902</b> included in the recipe read in S<b>202</b> are displayed. Further, an input unit <b>903</b> for setting a learning image size is displayed on the GUI <b>900</b>, and after the learning image size is set through the GUI <b>900</b>, an &#x201c;OK&#x201d; button <b>904</b> is pressed to execute the process of S<b>402</b> and the subsequent processes.</p><p id="p-0114" num="0113">Here, a method for determining the learning image size will be described. In the neural network shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the input image is reduced to a size (XY direction) of 1/(2{circumflex over (&#x2003;)}d) at a depth d. Therefore, when a neural network having a maximum depth D is used, the input image is required to have a size of (2{circumflex over (&#x2003;)}D)&#xd7;(2{circumflex over (&#x2003;)}D) or more. Sizes of the partial learning defect image and the partial learning reference image obtained by aligning the learning defect image and the learning reference image in S<b>602</b> are equal to or smaller than sizes of the learning defect image and the learning reference image, respectively. Specifically, the image size is reduced by the alignment amounts AX and AY. In order to ensure that the sizes of the partial learning defect image and the partial learning reference image are equal to or larger than a size of (2{circumflex over (&#x2003;)}D)&#xd7;(2{circumflex over (&#x2003;)}D), it is necessary to acquire the learning defect image and the learning reference image each having a size of (2{circumflex over (&#x2003;)}D+AX)&#xd7;(2{circumflex over (&#x2003;)}D+AY), but the alignment amounts AX and AY cannot be known before the acquisition of the learning defect image and the learning reference image. Since the alignment amounts AX and AY depend on the stage movement error, the electron beam irradiation positioning error, or the like, maximum values MX and MY of the alignment amounts may be obtained based on the stage movement error, the electron beam irradiation positioning error, or the like, and a size of (2{circumflex over (&#x2003;)}D+MX)&#xd7;(2{circumflex over (&#x2003;)}D+MY) may be set as the learning image size. For example, when the maximum depth of the neural network is 7, MX=50, and MY=30, 178&#xd7;158 is set as the learning image size.</p><p id="p-0115" num="0114">In addition, without acquiring the size of the learning image in S<b>401</b>, the learning defect image and the learning reference image are acquired in S<b>502</b> and S<b>504</b> with the same size as the inspection defect image, the learning defect image and the learning reference image are aligned in S<b>602</b>, and after the partial learning defect image and the partial learning reference image are cut out in S<b>603</b>, it is checked whether at least one size of the partial learning defect image and the partial learning reference image is equal to or larger than a predetermined size, and when the size is smaller than the predetermined size, this image pair may not be used in the processes of S<b>605</b> to S<b>608</b>. The predetermined size is the size of (2{circumflex over (&#x2003;)}D)&#xd7;(2{circumflex over (&#x2003;)}D) when the neural network having the maximum depth D is used.</p><p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a GUI <b>1000</b> for setting the learning end condition in S<b>406</b>. The GUI <b>1000</b> displays an input unit <b>1001</b> for setting the number of times of the repetitions MR of the processes of S<b>605</b> to S<b>608</b>, an input unit <b>1002</b> for setting the estimation error threshold value TH, and an input unit <b>1003</b> for setting whether to receive a learning end operation by the user. After the learning end condition is set through the GUI <b>1000</b>, a &#x201c;start learning&#x201d; button <b>1004</b> is pressed to execute the calculation of the estimation processing parameters in S<b>407</b>. When a &#x201c;cancel&#x201d; button <b>1006</b> is pressed during the calculation of the estimation processing parameters, the calculation of the estimation processing parameters can be interrupted. When a &#x201c;check progress&#x201d; button <b>1005</b> is pressed during the calculation of the estimation processing parameters, a GUI <b>1100</b> for checking a progress of the update process of the learning estimation processing parameters as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref> is switched. In the GUI <b>1100</b>, the number of times of the repetitions of the update of the estimation parameters and a transition of the estimation error are displayed in a graph <b>1101</b>. When an &#x201c;end learning&#x201d; button <b>1103</b> of the GUI <b>1100</b> is pressed, it is regarded that an operation of ending the learning is received from the user, the update of the learning estimation processing parameters is ended, that is, it is determined in S<b>608</b> that the learning end condition is satisfied. When a &#x201c;check estimated image&#x201d; button <b>1102</b> of the GUI <b>1100</b> is pressed, the GUI <b>1100</b> is switched to a GUI <b>1200</b> as shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>. By pressing an image ID selection button <b>1201</b> on the GUI <b>1200</b> to designate the number of an image to be displayed, and selecting a type of an image to be displayed such as a secondary electron image (SE) or a backscattered electron image (BSE) by a channel selection unit <b>1202</b>, a pseudo defect image estimation process is executed on the designated image ID by using the learning estimation parameters, and a partial learning defect image <b>1203</b>, an estimated pseudo defect image <b>1204</b>, and a partial learning reference image <b>1205</b> of the designated image ID are displayed. When an &#x201c;OK&#x201d; button <b>1206</b> of the GUI <b>1200</b> is pressed, the GUI <b>1200</b> is switched to the display of original GUI <b>1100</b> as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0117" num="0116">According to the present embodiment, by acquiring the learning defect image and the learning reference image, calculating the estimation processing parameters by using the learning defect image and the learning reference image, and estimating the pseudo reference image based on the inspection defect image in the sample observation, it is possible to omit acquisition of a reference image, and it is possible to improve a throughput of the sample observation.</p><heading id="h-0021" level="2">&#x3c;Comparison of Sequences of Defect Observation&#x3e;</heading><p id="p-0118" num="0117"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows sequences when defects to be observed (1) and (2) are sequentially observed in a sample observation system of related art. A horizontal axis represents a time, and a vertical axis represents a defect to be observed.</p><p id="p-0119" num="0118">First, a sequence of a process <b>1301</b> related to the observation of the defect to be observed (1) includes:</p><p id="p-0120" num="0119">(S) moving the stage so that a reference region corresponding to the defect to be observed (1) is included in the imaging field of view of the SEM <b>101</b>;</p><p id="p-0121" num="0120">(RI) acquiring a learning reference image by the SEM <b>101</b> imaging the reference region;</p><p id="p-0122" num="0121">(S) moving the stage so that a region including the defect to be observed (1) is included in the imaging field of view of the SEM <b>101</b>;</p><p id="p-0123" num="0122">(DI) acquiring a learning defect image by the SEM <b>101</b> imaging a relatively wide region including the defect to be observed (1);</p><p id="p-0124" num="0123">(D) identifying a defect portion in the learning defect image by using the learning defect image and the learning reference image; and</p><p id="p-0125" num="0124">(HI) acquiring an observation defect image by the SEM <b>101</b> imaging a relatively narrow region including the identified defect portion.</p><p id="p-0126" num="0125">A same sequence is also included in a process <b>1302</b> related to observation of the next defect to be observed (2). Here, it is essential that the stage movement (S) in the process <b>1302</b> is performed after the acquisition (HI) of the observation defect image of the defect to be observed (1) is completed. This is because the defect (1) to be observed remains in the imaging field of view of the SEM <b>101</b> until the acquisition (HI) of the observation defect image of the defect (1) to be observed is completed in the process <b>1301</b>.</p><p id="p-0127" num="0126"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a sequence particularly related to the processes of S<b>207</b> to S<b>213</b> of the present embodiment. A relationship between a horizontal axis and a vertical axis is the same as that in <figref idref="DRAWINGS">FIG. <b>13</b></figref>. In the sequence of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, estimation (P) of a pseudo reference image presents as a time not shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, while the acquisition (RI) of the reference image is not present. In addition, the number of the stage movements (S) during each process is one.</p><p id="p-0128" num="0127">First, a sequence of a process <b>1401</b> related to the observation of the defect to be observed (1) includes:</p><p id="p-0129" num="0128">(S) moving the stage so that the region including the defect to be observed (1) is included in the imaging field of view of the SEM <b>101</b>;</p><p id="p-0130" num="0129">(DI) acquiring an inspection defect image by the SEM <b>101</b> imaging the relatively wide region including the defect to be observed (1);</p><p id="p-0131" num="0130">(P) estimating a pseudo reference image from the inspection defect image based on estimation processing parameters;</p><p id="p-0132" num="0131">(D) identifying a defect portion in the inspection defect image by using the inspection defect image and the pseudo reference image; and</p><p id="p-0133" num="0132">(HI) acquiring an observation defect image by the SEM <b>101</b> imaging a relatively narrow region including the identified defect portion.</p><p id="p-0134" num="0133">Next, in a process <b>1402</b> related to the observation of the defect to be observed (2), the same processes are performed on the defect to be observed (2).</p><p id="p-0135" num="0134">As described above, in the sequence of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, by estimating (P) the pseudo reference image based on the defect image, the first stage movement (S) and the acquisition (RI) of the reference image of the sequence described in <figref idref="DRAWINGS">FIG. <b>13</b></figref> become unnecessary. Accordingly, it is possible to reduce the number of times of the stage movements to &#xbd;, and further, it is possible to omit the imaging of the reference image, and it is possible to improve the throughput when a plurality of defects to be observed on the sample wafer <b>104</b> are sequentially observed using the sample observation system.</p><heading id="h-0022" level="1">Second Embodiment</heading><p id="p-0136" num="0135">In the first embodiment, the method for improving the throughput of the sample observation using the SEM to image the defect region and the reference region, acquiring the learning defect image and the learning reference image, using the learning defect image and the learning reference image to calculate the estimation processing parameters, and estimating the pseudo reference image based on the inspection defect image in the sample observation is described. During the calculation of the estimation processing parameters, the learning can be performed more efficiently as the number of pairs of the learning defect image and the learning reference image increases. In the present embodiment, a method for calculating the estimation processing parameters by assigning a pseudo defect to a learning reference image to generate a learning defect image and using a pair of the learning reference image and the generated learning defect image even when there is no defect to be learned or a small number of defects to be learned will be described.</p><p id="p-0137" num="0136">A configuration of a sample observation system according to the present embodiment is basically the same as the configuration shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> described in the first embodiment. Differences are in a processing flow of the learning sequence, and a sample observation flow other than the learning sequence has the same processing flow as the processing flow shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> described in the first embodiment. A GUI of the sample observation system according to the present embodiment includes interfaces equivalent to those shown in <figref idref="DRAWINGS">FIGS. <b>9</b> to <b>12</b></figref> described in the first embodiment. Hereinafter, only portions different from those of the first embodiment will be described.</p><heading id="h-0023" level="2">&#x3c;Learning Sequence&#x3e;</heading><p id="p-0138" num="0137">The learning sequence of S<b>205</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0139" num="0138">First, a region to be learned is set (S<b>1501</b>). This may be one or more regions designated by the user on the sample wafer, or one or more regions on the sample wafer <b>104</b> may be set randomly. However, the region to be learned does not include defect coordinates output from the defect inspection device. Next, the stage <b>105</b> is controlled and moved so that the set region to be learned is included in the imaging field of view of the SEM <b>101</b> (S<b>1502</b>), the region to be learned on the sample wafer <b>104</b> is irradiated and scanned with the electron beam <b>106</b>, the generated secondary electrons <b>108</b> and backscattered electrons <b>109</b> are detected by the detector <b>110</b> to image the region to be learned, and a detection signal from the detector <b>110</b> obtained by the imaging is processed by the detector control unit <b>118</b> to acquire a learning reference image (S<b>1503</b>).</p><p id="p-0140" num="0139">Next, a pseudo defect is assigned to the learning reference image, and a learning defect image is generated, such that the learning defect image is acquired (S<b>1504</b>), accessory information is added so that the learning defect image and the learning reference image are paired, and the image pair is stored in the image storage area <b>119</b> (S<b>1505</b>). A center position and a size (width and height) of a region PR to which the pseudo defect is assigned may be set randomly in a plane of the learning reference image.</p><p id="p-0141" num="0140">A certain offset may be added to light and shade of the region PR as the pseudo defect. Alternatively, the region PR may be set so as to include an edge of a circuit pattern, and deformation may be applied to the circuit pattern based on an edge strength. As described above, minute defects are simulated, but a huge defect that covers an entire surface of an image may be generated. A type of the pseudo defect is not limited thereto, and various defects may be used as long as the various defects are modeled and generated.</p><p id="p-0142" num="0141">The processes of S<b>1502</b> to S<b>1505</b> described above are repeatedly executed for all of the regions to be learned of the sample wafer <b>104</b>. Next, a learning end condition of the estimation processing parameters is acquired (S<b>1506</b>: corresponding to S<b>406</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), and the estimation processing parameter calculation unit <b>122</b> calculates the estimation processing parameters for estimating a pseudo reference image (S<b>1507</b>: corresponding to S<b>407</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0143" num="0142">In the above description, the method in which the image including the pseudo defect generated in S<b>1504</b> is used as the learning defect image for the calculation of the estimation processing parameters is described, but in addition to the image including the pseudo defect generated in S<b>1504</b>, an image including all the defect coordinates read in S<b>203</b> or the defect coordinates sampled based on the user designation conditions may be used as the learning defect image, similarly to S<b>403</b> described in the first embodiment. That is, in S<b>1507</b>, the estimation processing parameters may be calculated using a pair of an image (first defect image) obtained by imaging a region including a defect on the sample wafer <b>104</b> and a reference image (first reference image) corresponding to the first defect image, and a pair of a second reference image and an image (second defect image) including a pseudo defect generated based on the second reference image.</p><p id="p-0144" num="0143">According to the present embodiment, the estimation processing parameters can be calculated by assigning a pseudo defect to a learning reference image to generate a learning defect image and using a pair of the learning reference image and the generated learning defect image even when there is no defect to be learned or a small number of defects to be learned. By estimating the pseudo reference image based on the inspection defect image in the sample observation, it is possible to omit the acquisition of the reference image, and it is possible to improve the throughput of the sample observation.</p><heading id="h-0024" level="1">Third Embodiment</heading><p id="p-0145" num="0144">The first and second embodiments describe the method for identifying the defect portion in the inspection defect image by using the estimation processing parameters obtained by learning a correspondence relationship between the learning defect image and the learning reference image, estimating the pseudo reference image from the inspection defect image in the sample observation, and comparing the inspection defect image with the pseudo reference image. In the present embodiment, a method of calculating estimation processing parameters for estimating a defect portion in a defect image and estimating the defect portion in an inspection defect image based on the estimation processing parameters will be described.</p><heading id="h-0025" level="2">&#x3c;System Configuration&#x3e;</heading><p id="p-0146" num="0145">With respect to a configuration of the sample observation system <b>100</b> according to the present embodiment, the SEM <b>101</b> and the defect inspection device <b>102</b> have the same configuration as those shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> described in the first embodiment, and a difference is in the configuration of the calculator <b>103</b>. Hereinafter, only portions different from those of the first embodiment will be described.</p><p id="p-0147" num="0146">The calculator <b>103</b> of the sample observation system according to the present embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref>. The storage unit <b>114</b> includes the image storage area <b>119</b> for storing the digital image generated by the detector control unit <b>118</b> together with the accessory information, the recipe storage area <b>120</b> for storing the recipe including the information on the device or the manufacturing process, the imaging conditions of the image, and the like, and an estimation processing parameter storage area <b>1601</b> for storing parameters related to an estimation process of a defect portion in a defect image. The processing unit <b>115</b> includes an estimation processing parameter calculation unit <b>1602</b> that calculates estimation processing parameters for estimating a defect portion in a defect image, and a defect portion estimation unit <b>1603</b> that estimates the defect portion based on the estimation processing parameters.</p><heading id="h-0026" level="2">&#x3c;Sample Observation Method&#x3e;</heading><p id="p-0148" num="0147">A sample observation method will be described with reference to <figref idref="DRAWINGS">FIGS. <b>17</b></figref>. S<b>1701</b> to S<b>1703</b> are the same as S<b>201</b> to S<b>203</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0149" num="0148">In S<b>1704</b>, it is checked whether the estimation processing parameters corresponding to a process in which the sample wafer <b>104</b> is processed is stored in the estimation processing parameter storage area <b>1601</b>, and when the estimation processing parameters are not stored (&#x201c;ABSENT&#x201d; in <figref idref="DRAWINGS">FIG. <b>17</b></figref>), the estimation processing parameter calculation unit <b>1602</b> calculates and stores the estimation processing parameters by a learning sequence described later (S<b>1705</b>). Next, the estimation processing parameters are read from the estimation processing parameter storage area <b>1601</b> (S<b>1706</b>).</p><p id="p-0150" num="0149">Next, defects to be observed on the sample wafer <b>104</b> are sequentially imaged using the SEM <b>101</b>, and the following series of observations are performed. Since S<b>1707</b> to S<b>1709</b> are the same as S<b>207</b> to S<b>209</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a description will be omitted. Next, the defect portion estimation unit <b>1603</b> estimates the defect portion in the inspection defect image subjected to the image preprocessing by using the estimation processing parameters (S<b>1710</b>). In the estimation of the defect portion, the design data is not necessary, and the estimation processing parameters and the inspection defect image subjected to the image preprocessing are used. Next, the estimated defect portion is imaged at a large magnification by narrowing the field of view, an observation defect image is acquired (S<b>1711</b>), and the inspection defect image and the observation defect image are stored in the image storage area <b>119</b> (S<b>1712</b>). The processes of S<b>1707</b> to S<b>1712</b> described above are repeatedly executed for all of the defects to be observed of the sample wafer <b>104</b>.</p><heading id="h-0027" level="2">&#x3c;&#x3c;Learning Sequence&#x3e;&#x3e;</heading><p id="p-0151" num="0150">The learning sequence of S<b>1705</b> includes a sequence equivalent to the processing flow shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> described in the first embodiment except for S<b>407</b>.</p><p id="p-0152" num="0151">A method for calculating the estimation processing parameters corresponding to S<b>407</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> will be described with reference to <figref idref="DRAWINGS">FIG. <b>18</b></figref>. In the calculation of the estimation processing parameters, the design data is not necessary, and the learning defect image and the learning reference image are used.</p><p id="p-0153" num="0152">Since S<b>1801</b> to S<b>1803</b> are the same as S<b>601</b> to S<b>603</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a description will be omitted. Next, a partial learning defect image is compared with a partial learning reference image, a defect portion in the partial learning defect image is identified, and an identified defect portion image indicating the defect portion is obtained (S<b>1804</b>: corresponding to S<b>211</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>). In S<b>1804</b>, the learning defect image may be displayed on the GUI without identifying the defect portion by using the learning defect image and the learning reference image, the user may designate the defect portion, and an image in which the designated defect portion and a region other than the designated defect portion can be distinguished may be set as the identified defect portion image. Next, learning estimation processing parameters are initialized (S<b>1805</b>: corresponding to S<b>604</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). At this time, previously learned estimation processing parameters may be read from the estimation processing parameter storage area <b>1601</b> and used as initial values of the learning estimation processing parameters.</p><p id="p-0154" num="0153">Next, the defect portion estimation unit <b>1603</b> estimates the defect portion from the partial learning defect image based on the learning estimation processing parameters (S<b>1806</b>), obtains an estimated defect portion image, calculates an estimation error using the estimated defect portion image and the identified defect portion image (S<b>1807</b>), and the learning estimation processing parameters are updated so that the estimation error becomes small (S<b>1808</b>: corresponding to S<b>607</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). Next, it is checked whether the learning end condition acquired in S<b>406</b> is satisfied (S<b>1809</b>: corresponding to S<b>608</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>), and when the learning end condition is satisfied, the learning estimation processing parameters are stored in the estimation processing parameter storage area <b>1601</b> as the estimation processing parameters (S<b>1810</b>). The learning end condition is as described in the first embodiment. When the learning end condition is not satisfied, the process returns to S<b>1806</b> again. This is the end of the flow of <figref idref="DRAWINGS">FIG. <b>18</b></figref>.</p><p id="p-0155" num="0154">As a method of estimating the defect portion from the partial learning defect image in S<b>1806</b>, a neural network shown in <figref idref="DRAWINGS">FIG. <b>19</b></figref> may be used, and the input image Y is the partial learning defect image and F (Y) is the estimated defect portion image. The estimated defect portion image F (Y) is calculated by Equation 12 in <figref idref="DRAWINGS">FIG. <b>21</b></figref>. In Equation 12, F (Y) (x, y, c) represents a value of a pixel in which a coordinate value of the estimated defect portion image F (Y) in an X direction is x, a coordinate value in a Y direction is y, and a channel is c, and the same applies to F15 (Y) (x, y, c). F11 (Y), F12 (Y), F21 (Y), F22 (Y), F23 (Y), F24 (Y), F31 (Y), and F32 (Y) are calculated by Equations 1 to 8 in <figref idref="DRAWINGS">FIG. <b>20</b></figref> described in the first embodiment. In addition, when the estimation error of the defect portion is calculated in S<b>1807</b>, an identified defect portion image DD (single-channel binary image) is converted into a two-channel image MSK by Equation 13 of <figref idref="DRAWINGS">FIG. <b>21</b></figref>, and a difference (error) between the image MSK and the estimated defect portion image F (Y) is calculated. As a method for quantifying a difference (error) between images, a mean square error or the like may be used as in S<b>607</b>.</p><p id="p-0156" num="0155">Here, W9 is two filters of (c1&#xd7;2)&#xd7;f9&#xd7;f9 size, and B9 is a two-dimensional vector. f9 is a hyperparameter determined by the user before the learning sequence, and may be set to, for example, f9=3. The parameters calculated by the calculation processes of the estimation processing parameters (S<b>1801</b> to S<b>1810</b>) are W1 to W6, W9, B1 to B6, and B9.</p><p id="p-0157" num="0156">According to the present embodiment, using the learning defect image to calculate the estimation processing parameters for estimating the defect portion in the defect image, and estimating the defect portion in the inspection defect image based on the estimation processing parameters in the sample observation, it is possible to omit the acquisition of the reference image, and it is possible to improve the throughput of the sample observation.</p><heading id="h-0028" level="2">&#x3c;Summary&#x3e;</heading><p id="p-0158" num="0157">The following description is made in the above first to third embodiments. Numbers and alphabets covered with parentheses assigned to the following description do not indicate an execution order of the processes.</p><heading id="h-0029" level="2">&#x3c;&#x3c;Point of View 1&#x3e;&#x3e;</heading><p id="p-0159" num="0158">A sample observation system includes a scanning electron microscope and a calculator. In the sample observation system, the calculator:</p><p id="p-0160" num="0159">(1) acquires a plurality of images captured by the scanning electron microscope;</p><p id="p-0161" num="0160">(2) acquires, from the plurality of images, a learning defect image including a defect portion and a learning reference image not including the defect portion;</p><p id="p-0162" num="0161">(3) calculates an estimation processing parameter by using the learning defect image and the learning reference image;</p><p id="p-0163" num="0162">(4) acquires an inspection defect image including a defect portion; and</p><p id="p-0164" num="0163">(5) estimates a pseudo reference image by using the estimation processing parameter and the inspection defect image.</p><heading id="h-0030" level="2">&#x3c;&#x3c;Point of View 2&#x3e;&#x3e;</heading><p id="p-0165" num="0164">In the sample observation system according to point of view 1, the calculator:</p><p id="p-0166" num="0165">(6) compares the pseudo reference image with the inspection defect image, and identifies the defect portion of the inspection defect image.</p><heading id="h-0031" level="2">&#x3c;&#x3c;Point of View 3&#x3e;&#x3e;</heading><p id="p-0167" num="0166">In the sample observation system according to point of view 1, as the process (3), the calculator:</p><p id="p-0168" num="0167">(3A) aligns the learning defect image and the learning reference image based on a predetermined evaluation value to acquire an alignment amount;</p><p id="p-0169" num="0168">(3B) cuts out a partial learning defect image from the learning defect image based on the alignment amount;</p><p id="p-0170" num="0169">(3C) cuts out a partial learning reference image from the learning reference image based on the alignment amount; and</p><p id="p-0171" num="0170">(3D) calculates the estimation processing parameter by using the partial learning defect image and the partial learning reference image.</p><heading id="h-0032" level="2">&#x3c;&#x3c;Point of View 4&#x3e;&#x3e;</heading><p id="p-0172" num="0171">In the sample observation system according to point of view 1,</p><p id="p-0173" num="0172">the estimation processing parameter is a parameter of a neural network,</p><p id="p-0174" num="0173">in the neural network, a minimum size of an image input to an input layer is a first size, and</p><p id="p-0175" num="0174">as the process (1), the calculator:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0175">acquires the plurality of images each having a size equal to or larger than the first size.</li>    </ul>    </li></ul></p><heading id="h-0033" level="2">&#x3c;&#x3c;Point of View 5&#x3e;&#x3e;</heading><p id="p-0176" num="0176">In the sample observation system according to point of view 3,</p><p id="p-0177" num="0177">the estimation processing parameter is a parameter of a neural network,</p><p id="p-0178" num="0178">in the neural network, a minimum size of an image input to an input layer is a first size, and</p><p id="p-0179" num="0179">as the process (3), the calculator:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0180">(3E) checks whether at least one size of the partial learning defect image and the partial learning reference image is equal to or larger than the first size.</li>    </ul>    </li></ul></p><heading id="h-0034" level="2">&#x3c;&#x3c;Point of View 6&#x3e;&#x3e;</heading><p id="p-0180" num="0181">In the sample observation system according to point of view 1,</p><p id="p-0181" num="0182">the calculator acquires an end condition of the calculation process of the estimation processing parameter, and</p><p id="p-0182" num="0183">as the process (3), the calculator:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0184">(3F) ends an update of the estimation processing parameter when it is detected that the end condition is satisfied.</li>    </ul>    </li></ul></p><heading id="h-0035" level="2">&#x3c;&#x3c;Point of View 7&#x3e;&#x3e;</heading><p id="p-0183" num="0185">In the sample observation system according to point of view 6,</p><p id="p-0184" num="0186">in parallel with the calculation of the estimation processing parameter, the calculator:<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0187">(7) compares the learning defect image with the learning reference image to identify a defect portion of the learning defect image.</li>    </ul>    </li></ul></p><heading id="h-0036" level="2">&#x3c;&#x3c;Point of View 8&#x3e;&#x3e;</heading><p id="p-0185" num="0188">In the sample observation system according to point of view 1,</p><p id="p-0186" num="0189">the calculator omits acquisition of a reference image corresponding to the inspection defect image.</p><heading id="h-0037" level="2">&#x3c;&#x3c;Point of View 9&#x3e;&#x3e;</heading><p id="p-0187" num="0190">A sample observation system includes: a scanning electron microscope and a calculator. In the sample observation system, the calculator:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0191">acquires a plurality of images captured by the scanning electron microscope;</li>        <li id="ul0010-0002" num="0192">acquires, from the plurality of images, a learning defect image including a defect portion;</li>        <li id="ul0010-0003" num="0193">calculates an estimation processing parameter by using the learning defect image;</li>        <li id="ul0010-0004" num="0194">acquires an inspection defect image including a defect portion; and</li>        <li id="ul0010-0005" num="0195">estimates the defect portion in the inspection defect image by using the estimation processing parameter and the inspection defect image.</li>    </ul>    </li></ul></p><p id="p-0188" num="0196">It has been described that the processes described above may be implemented by the processing unit executing the image processing program. The image processing program may be distributed by a calculator-readable storage medium or may be distributed from a distribution server calculator. Here, the distribution server calculator includes a storage unit, a calculation unit, and the network interface <b>112</b>. A specific example of each unit may be the same as that of the calculator <b>103</b>. The image processing program may be stored in the storage unit of the distribution server calculator having such a configuration, and the processing unit may read the image processing program in response to a distribution request from the calculator <b>103</b> and transmit the image processing program to the calculator <b>103</b> via the network interface <b>112</b>.</p><heading id="h-0038" level="1">REFERENCE SIGNS LIST</heading><p id="p-0189" num="0197"><b>100</b>: sample observation system</p><p id="p-0190" num="0198"><b>101</b>: SEM</p><p id="p-0191" num="0199"><b>102</b>: defect inspection device</p><p id="p-0192" num="0200"><b>103</b>: calculator</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A sample observation system comprising:<claim-text>a scanning electron microscope; and</claim-text><claim-text>a calculator, wherein</claim-text><claim-text>the calculator:<claim-text>(1) acquires a plurality of images captured by the scanning electron microscope;</claim-text><claim-text>(2) acquires, from the plurality of images, a learning defect image including a defect portion and a learning reference image not including the defect portion;</claim-text><claim-text>(3) calculates an estimation processing parameter by using the learning defect image and the learning reference image;</claim-text><claim-text>(4) acquires an inspection defect image including a defect portion; and</claim-text><claim-text>(5) estimates a pseudo reference image by using the estimation processing parameter and the inspection defect image, and</claim-text></claim-text><claim-text>the process (3) includes:<claim-text>(3A) aligning the learning defect image and the learning reference image based on a predetermined evaluation value to acquire an alignment amount;</claim-text><claim-text>(3B) cutting out a partial learning defect image from the learning defect image based on the alignment amount;</claim-text><claim-text>(3C) cutting out a partial learning reference image from the learning reference image based on the alignment amount; and</claim-text><claim-text>(3D) calculating the estimation processing parameter by using the partial learning defect image and the partial learning reference image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The sample observation system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the calculator:<claim-text>(6) compares the pseudo reference image with the inspection defect image, and identifies the defect portion of the inspection defect image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. (canceled)</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The sample observation system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the estimation processing parameter is a parameter of a neural network,</claim-text><claim-text>in the neural network, a minimum size of an image input to an input layer is a first size, and</claim-text><claim-text>as the process (1), the calculator:<claim-text>acquires the plurality of images each having a size equal to or larger than the first size.</claim-text></claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The sample observation system according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the estimation processing parameter is a parameter of a neural network,</claim-text><claim-text>in the neural network, a minimum size of an image input to an input layer is a first size,</claim-text><claim-text>the scanning electron microscope sets a size of a captured image based on the first size and a maximum value of the alignment amount, and captures a plurality of images based on the set size, and</claim-text><claim-text>the maximum value of the alignment amount is obtained based on (A1) a stage movement error and (A2) an electron beam irradiation positioning error of the scanning electron microscope.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The sample observation system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the calculator acquires an end condition of the calculation process of the estimation processing parameter, and</claim-text><claim-text>as the process (3), the calculator:<claim-text>(3F) ends an update of the estimation processing parameter when it is detected that the end condition is satisfied.</claim-text></claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The sample observation system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>in parallel with the calculation of the estimation processing parameter, the calculator:<claim-text>(7) compares the learning defect image with the learning reference image to identify the defect portion of the learning defect image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The sample observation system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the calculator omits acquisition of a reference image corresponding to the inspection defect image.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. (canceled)</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A sample observation system comprising:<claim-text>a scanning electron microscope; and</claim-text><claim-text>a calculator, wherein</claim-text><claim-text>the calculator:<claim-text>acquires a plurality of images captured by the scanning electron microscope;</claim-text><claim-text>acquires, from the plurality of images, a learning defect image including a defect portion;</claim-text><claim-text>calculates an estimation processing parameter by using the learning defect image;</claim-text><claim-text>acquires an inspection defect image including a defect portion; and</claim-text><claim-text>estimates the defect portion in the inspection defect image by using the estimation processing parameter and the inspection defect image, and</claim-text></claim-text><claim-text>in the calculation of the estimation processing parameter,<claim-text>the learning defect image and the learning reference image are aligned based on a predetermined evaluation value to acquire an alignment amount,</claim-text><claim-text>a partial learning defect image is cut out from the learning defect image based on the alignment amount,</claim-text><claim-text>a partial learning reference image is cut out from the learning reference image based on the alignment amount, and</claim-text><claim-text>the estimation processing parameter is calculated by using the partial learning defect image and the partial learning reference image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An image processing method executed by a calculator, the image processing method comprising:<claim-text>(M1) acquiring a plurality of images captured by a scanning electron microscope;</claim-text><claim-text>(M2) acquiring, from the plurality of images, a learning defect image including a defect portion and a learning reference image not including the defect portion;</claim-text><claim-text>(M3) calculating an estimation processing parameter by using the learning defect image and the learning reference image;</claim-text><claim-text>(M4) acquiring an inspection defect image including a defect candidate portion; and</claim-text><claim-text>(M5) estimating a pseudo reference image by using the estimation processing parameter and the inspection defect image, wherein</claim-text><claim-text>the (M3) includes:<claim-text>(M3A) aligning the learning defect image and the learning reference image based on a predetermined evaluation value to acquire an alignment amount;</claim-text><claim-text>(M3B) cutting out a partial learning defect image from the learning defect image based on the alignment amount;</claim-text><claim-text>(M3C) cutting out a partial learning reference image from the learning reference image based on the alignment amount; and</claim-text><claim-text>(M3D) calculating the estimation processing parameter by using the partial learning defect image and the partial learning reference image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The image processing method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>(M6) comparing the pseudo reference image with the inspection defect image, and identifying a defect portion of the inspection defect image.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. (canceled)</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The image processing method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>the estimation processing parameter is a weight value of a neural network,</claim-text><claim-text>in the neural network, a minimum size of an image input to an input layer is a first size, and</claim-text><claim-text>the (M1) includes:<claim-text>acquiring the plurality of images each having a size equal to or larger than the first size.</claim-text></claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The image processing method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein<claim-text>the estimation processing parameter is a weight value of a neural network,</claim-text><claim-text>in the neural network, a minimum size of an image input to an input layer is a first size,</claim-text><claim-text>the scanning electron microscope sets a size of a captured image based on the first size and a maximum value of the alignment amount, and captures a plurality of images based on the set size, and</claim-text><claim-text>the maximum value of the alignment amount is obtained based on (A1) a stage movement error and (A2) an electron beam irradiation positioning error of the scanning electron microscope.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The image processing method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>an end condition of the estimation process of the estimation processing parameter is acquired, and</claim-text><claim-text>the (M3) includes:<claim-text>(M3F) ending an update of the estimation processing parameter when it is detected that the end condition is satisfied.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The image processing method according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising:<claim-text>(M7) comparing the learning defect image with the learning reference image to identify the defect portion of the learning defect image in parallel with the calculation of the estimation processing parameter.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The image processing method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>acquisition of a reference image corresponding to the inspection defect image is omitted.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. (canceled)</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The sample observation system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the learning reference image is an image of a region designed to form a circuit pattern same as a circuit pattern captured in the learning defect image, and</claim-text><claim-text>a coordinate difference between the learning reference image and the learning defect image when the alignment is performed by the process (3A) is acquired as the alignment amount.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The sample observation system according to <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein<claim-text>the learning reference image is an image of a region of a circuit pattern formed on a chip shifted by one chip from a chip, on which a circuit pattern captured in the learning defect image is formed, in a plurality of same chips disposed on a wafer imaged by the scanning electron microscope.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The image processing method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>the learning reference image is an image of a region designed to form a circuit pattern same as a circuit pattern captured in the learning defect image, and</claim-text><claim-text>a coordinate difference between the learning reference image and the learning defect image when the alignment is performed by the process (M3A) is acquired as the alignment amount.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The sample observation system according to <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein<claim-text>the learning reference image is an image of a region of a circuit pattern formed on a chip shifted by one chip from a chip, on which a circuit pattern captured in the learning defect image is formed, in a plurality of same chips disposed on a wafer imaged by the scanning electron microscope.</claim-text></claim-text></claim></claims></us-patent-application>