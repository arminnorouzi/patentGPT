<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007216A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007216</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17853003</doc-number><date>20220629</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2021-109336</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>18</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>183</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>174</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGING APPARATUS, CONTROL METHOD, AND STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>CANON KABUSHIKI KAISHA</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Ishizu</last-name><first-name>Akihiko</first-name><address><city>Kanagawa</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An imaging apparatus includes an imaging unit configured to image a subject, a transmission unit configured to transmit image data to an external apparatus, a subject search unit configured to automatically search for a subject and detect the subject, a determination unit configured to determine whether to image a found subject, and a control unit configured to control transmission processing for transmitting the image data and imaging processing by the imaging unit not to be performed in parallel, wherein the subject search unit searches for a subject even during transmission of the image data, and wherein, in a case where the determination unit determines to image the found subject while the image data is transmitted, the transmission unit suspends the transmission of the image data, and the imaging unit images the found subject.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="222.33mm" wi="117.52mm" file="US20230007216A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="215.05mm" wi="86.19mm" file="US20230007216A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="238.00mm" wi="152.91mm" orientation="landscape" file="US20230007216A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="57.74mm" wi="132.25mm" file="US20230007216A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="142.24mm" wi="118.87mm" file="US20230007216A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="196.17mm" wi="103.72mm" file="US20230007216A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="188.04mm" wi="146.73mm" orientation="landscape" file="US20230007216A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="235.63mm" wi="119.55mm" file="US20230007216A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND OF THE DISCLOSURE</heading><heading id="h-0002" level="1">Field of the Disclosure</heading><p id="p-0002" num="0001">The present disclosure relates to an imaging apparatus capable of communicating with an external apparatus.</p><heading id="h-0003" level="1">Description of the Related Art</heading><p id="p-0003" num="0002">In recent years, an imaging apparatus that can image surroundings without receiving an instruction from a user has been discussed. In addition, the user can transmit an image from the imaging apparatus to an external apparatus such as a server, so that the user can view the image via the server.</p><p id="p-0004" num="0003">Japanese Patent Application Laid-Open No. 2019-106694 discusses an imaging apparatus that transmits a captured image to an external apparatus. This imaging apparatus has an automatic image capturing mode of automatically searching for a subject and capturing an image of the subject by driving pan/tilt and zoom mechanisms, and an image transmission mode of automatically extracting an image expected to meet preference of a user and transmitting the extracted image to the external apparatus.</p><p id="p-0005" num="0004">In Japanese Patent Application Laid-Open No. 2019-106694, however, searching for a subject and automatic image capturing are not performed in the image transmission mode, and thus the imaging apparatus may fail to image a subject.</p><heading id="h-0004" level="1">SUMMARY OF THE DISCLOSURE</heading><p id="p-0006" num="0005">According to an aspect of the present disclosure, an imaging apparatus includes an imaging unit configured to image a subject, a transmission unit configured to transmit image data to an external apparatus, a subject search unit configured to automatically search for a subject and detect the subject, a determination unit configured to determine whether to image a subject found by the subject search unit, and a control unit configured to control transmission processing for transmitting the image data by the transmission unit and imaging processing by the imaging unit not to be performed in parallel, wherein the subject search unit searches for a subject even during transmission of the image data by the transmission unit, and wherein, in a case where the determination unit determines to image the subject found by the subject search unit while the image data is transmitted by the transmission unit, the transmission unit suspends the transmission of the image data, and the imaging unit images the subject found by the subject search unit.</p><p id="p-0007" num="0006">Further features of the present disclosure will become apparent from the following description of exemplary embodiments with reference to the attached drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a diagram illustrating an example of an external appearance of an imaging apparatus according to an exemplary embodiment, and <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a diagram illustrating an operation of the imaging apparatus according to the exemplary embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating a configuration of the imaging apparatus according to the exemplary embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating a configuration including the imaging apparatus and an external apparatus according to the exemplary embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating a configuration of the external apparatus according to the exemplary embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart illustrating automatic imaging processing according to the exemplary embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. <b>6</b>A to <b>6</b>D</figref> are diagrams illustrating area segmentation within an image according to the exemplary embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating automatic transmission processing according to the exemplary embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0015" num="0014">Exemplary embodiments of the present disclosure will be described below in detail with reference to the attached drawings.</p><p id="p-0016" num="0015">The exemplary embodiments to be described below are examples of a way of implementing the present disclosure, and may be modified or changed depending on a configuration of an apparatus to which the present disclosure is applied and various conditions. In addition, the exemplary embodiments can be combined as appropriate.</p><heading id="h-0007" level="2">&#x3c;Configuration of Imaging Apparatus&#x3e;</heading><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are diagrams illustrating a configuration of an imaging apparatus <b>101</b> according to an exemplary embodiment.</p><p id="p-0018" num="0017">The imaging apparatus <b>101</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is provided with operation members including a power switch that can be operated to switch between on and off of the power. The operation members also include a touch panel.</p><p id="p-0019" num="0018">A lens barrel <b>102</b> is a housing including an optical lens group and an image sensor. The lens barrel <b>102</b> is attached to the imaging apparatus <b>101</b>. A tilt rotation unit <b>104</b> and a pan rotation unit <b>105</b> form a rotation mechanism that can rotate the lens barrel <b>102</b> with respect to a fixed portion <b>103</b>. The tilt rotation unit <b>104</b> is a motor that can rotate the lens barrel <b>102</b>, for example, in a pitch direction illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>. The pan rotation unit <b>105</b> is a motor that can rotate the lens barrel <b>102</b>, for example, in a yaw direction illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>. The tilt rotation unit <b>104</b> and the pan rotation unit <b>105</b> can rotate the lens barrel <b>102</b> about one or more axes. In the present exemplary embodiment, a Y-axis illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a rotation axis of the pan rotation unit <b>105</b>. Further, in the present exemplary embodiment, the positive direction of a Z-axis illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is the frontward direction of the imaging apparatus <b>101</b>.</p><p id="p-0020" num="0019">An angular velocity meter <b>106</b> and an accelerometer <b>107</b> are, for example, a gyroscope sensor and an acceleration sensor, respectively, and are disposed at the fixed portion <b>103</b> of the imaging apparatus <b>101</b>. The imaging apparatus <b>101</b> detects vibration of the imaging apparatus <b>101</b>, based on a velocity measured by each of the angular velocity meter <b>106</b> and the accelerometer <b>107</b>. The imaging apparatus <b>101</b> can generate an image in which shake and tilt in the lens barrel <b>102</b> are corrected by rotating the tilt rotation unit <b>104</b> and the pan rotation unit <b>105</b>, based on the detected vibration of the imaging apparatus <b>101</b>.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a configuration of the imaging apparatus <b>101</b> of the present exemplary embodiment.</p><p id="p-0022" num="0021">A first control unit <b>223</b> is composed of a processor (such as a central processing unit (CPU), a graphics processing unit (GPU), a microprocessor, or a micro processing unit (MPU)), a memory (such as a dynamic random access memory (DRAM) or a static random access memory (SRAM)), and the like. The first control unit <b>223</b> executes various types of processing, thereby controlling each block of the imaging apparatus <b>101</b> and controlling data transfer between the blocks. The first control unit <b>223</b> is an example of each of a control unit and a determination unit.</p><p id="p-0023" num="0022">A nonvolatile memory <b>216</b> is a memory in which data can be recorded and from which data can be erased. Constants, programs, and the like for operating the first control unit <b>223</b> are recorded in the nonvolatile memory <b>216</b>.</p><p id="p-0024" num="0023">A zoom unit <b>201</b> is an optical lens group forming a zoom lens for changing the zoom ratio. A zoom drive control unit <b>202</b> is a controller for controlling driving of the optical lenses of the zoom unit <b>201</b>. A focus unit <b>203</b> is an optical lens group that adjusts focus.</p><p id="p-0025" num="0024">A focus drive control unit <b>204</b> controls driving of the optical lenses of the focus unit <b>203</b>. An imaging unit <b>206</b> receives incident light via each of the optical lens groups by using the image sensor, and outputs information of electric charges corresponding to the amount of the received light to an image processing unit <b>207</b>, as analog image data. The zoom unit <b>201</b>, the zoom drive control unit <b>202</b>, the focus unit <b>203</b>, the focus drive control unit <b>204</b>, and the imaging unit <b>206</b> are included in the lens barrel <b>102</b>.</p><p id="p-0026" num="0025">The image processing unit <b>207</b> subjects image data input from the imaging unit <b>206</b> to image processing such as distortion correction, white balance adjustment, and color interpolation processing, and outputs digital image data. An image recording unit <b>208</b> converts the digital image data output from the image processing unit <b>207</b>, based on an image file format such as a Joint Photographic Experts Group (JPEG) format or a moving image file format such as a Moving Picture Experts Group (MPEG) format. The converted digital image data is transmitted to a memory <b>215</b> or a video output unit <b>217</b> to be described below. In a case where digital image data recorded in the memory <b>215</b> is to be recorded, the first control unit <b>223</b> outputs the digital image data to a recording/reproduction unit <b>220</b>.</p><p id="p-0027" num="0026">A lens barrel rotation drive unit <b>205</b> drives the lens barrel <b>102</b> in a tilt direction and a pan direction by driving the tilt rotation unit <b>104</b> and the pan rotation unit <b>105</b>. The lens barrel rotation drive unit <b>205</b> is an example of a drive unit.</p><p id="p-0028" num="0027">An apparatus shake detection unit <b>209</b> is mounted with, for example, the angular velocity meter <b>106</b> for detecting an angular velocity in the 3-axis directions of the imaging apparatus <b>101</b>, and the accelerometer <b>107</b> for detecting an acceleration in the 3-axis directions of the imaging apparatus <b>101</b>. The apparatus shake detection unit <b>209</b> calculates a rotation angle of the imaging apparatus <b>101</b> and a shift amount of the imaging apparatus <b>101</b>, based on signals detected by the angular velocity meter <b>106</b> and the accelerometer <b>107</b>.</p><p id="p-0029" num="0028">An audio input unit <b>213</b> has a plurality of microphones. Further, the audio input unit <b>213</b> performs analog-to-digital (A/D) conversion of audio signals input from the microphones, and outputs the converted signals to an audio processing unit <b>214</b>.</p><p id="p-0030" num="0029">The audio processing unit <b>214</b> can detect the direction of sound on a flat surface where the plurality of microphones is installed. The detected direction of the sound can be used in a search or automatic imaging to be described below. Further, the audio processing unit <b>214</b> can recognize a specific audio command. In the present exemplary embodiment, there are two types of specific audio command, which are a trigger word and a command word. The trigger word is a command to be a trigger for starting recognition of the command word. For example, the trigger word is a command including a specific keyword said by a user, such as &#x201c;OK, camera&#x201d;. The command word is a command for instructing the imaging apparatus <b>101</b> to perform predetermined processing. Examples of this predetermined processing include imaging processing for still image, imaging start processing for moving image, imaging termination processing for moving image, sleep processing, changing processing for a subject, and automatic imaging processing. For example, the command word is a command including a keyword varying depending on the predetermined processing, such as &#x201c;take a still image&#x201d; if the imaging processing for still image is the intended processing, and &#x201c;take a moving image&#x201d; if the imaging start processing for moving image is the intended processing. These audio commands are recorded beforehand in the memory <b>215</b> of the imaging apparatus <b>101</b>. The imaging apparatus <b>101</b> may be configured to register an audio command of a user for executing any processing, in addition to the audio commands recorded beforehand.</p><p id="p-0031" num="0030">Further, the audio processing unit <b>214</b> performs audio processing such as optimization processing and encoding for an input audio signal. The audio signal processed by the audio processing unit <b>214</b> is transmitted to the memory <b>215</b> by the first control unit <b>223</b>. The memory <b>215</b> temporarily stores the data input from the image recording unit <b>208</b> and the audio signal input from the audio processing unit <b>214</b>. In a case where this audio signal is to be recorded, the first control unit <b>223</b> outputs the audio signal from the memory <b>215</b> to the recording/reproduction unit <b>220</b>.</p><p id="p-0032" num="0031">The recording/reproduction unit <b>220</b> records image data, an audio signal, control data about imaging, and the like in a recording medium <b>221</b>. The recording medium <b>221</b> may be a recording medium built in the imaging apparatus <b>101</b>, or may be a removable recording medium. Various kinds of data such as image data and an audio signal can be recorded in the recording medium <b>221</b>. In the present exemplary embodiment, the recording medium <b>221</b> has a capacity larger than that of the nonvolatile memory <b>216</b>. Examples of the recording medium <b>221</b> include a hard disk, an optical disc, a magneto-optical disk, a compact disc recordable (CD-R), a digital versatile disc recordable (DVD-R), a magnetic tape, a nonvolatile semiconductor memory, and a flash memory.</p><p id="p-0033" num="0032">Further, the recording/reproduction unit <b>220</b> can read out (reproduce) image data, audio signals, various data, and programs recorded in the recording medium <b>221</b>. To reproduce image data and an audio signal recorded in the recording medium <b>221</b>, the first control unit <b>223</b> operates as follows. The first control unit <b>223</b> outputs image data and an audio signal read out by the recording/reproduction unit <b>220</b> to the image processing unit <b>207</b> and the audio processing unit <b>214</b>, respectively. The image processing unit <b>207</b> and the audio processing unit <b>214</b> decode the image data and the audio signal, respectively. The image processing unit <b>207</b> and the audio processing unit <b>214</b> output the decoded data and the decoded signal to the video output unit <b>217</b> and an audio output unit <b>218</b>, respectively.</p><p id="p-0034" num="0033">A second control unit <b>211</b> controls supply of power to the first control unit <b>223</b>. For example, the second control unit <b>211</b> consists of a processor (such as a CPU, a microprocessor, or an MPU), and a memory (such as a DRAM or an SRAM). In the present exemplary embodiment, the second control unit <b>211</b> is disposed separately from the first control unit <b>223</b> that controls the entire main system of the imaging apparatus <b>101</b>.</p><p id="p-0035" num="0034">A first power supply unit <b>210</b> and a second power supply unit <b>212</b> supply power for operating the first control unit <b>223</b> and power for operating the second control unit <b>211</b>, respectively. In the present exemplary embodiment, the power supplied by the first power supply unit <b>210</b> is greater than the power supplied by the second power supply unit <b>212</b>. In the present exemplary embodiment, the first power supply unit <b>210</b> and the second power supply unit <b>212</b> are selected depending on the amount of power to be supplied. For example, the first power supply unit <b>210</b> is a switch for supplying power to the first control unit <b>223</b>, and the second power supply unit <b>212</b> is a lithium battery or an alkaline battery. When the power switch of the imaging apparatus <b>101</b> is pressed, at first, the power is supplied to the second control unit <b>211</b>, and subsequently the power is supplied to the first control unit <b>223</b>.</p><p id="p-0036" num="0035">The imaging apparatus <b>101</b> has a sleep state. In the sleep state, the second control unit <b>211</b> controls the first power supply unit <b>210</b> to turn off the power supply to the first control unit <b>223</b>. Even in the sleep state where the power is not supplied to the first control unit <b>223</b>, the second control unit <b>211</b> operates, and acquires information from the apparatus shake detection unit <b>209</b> and the audio processing unit <b>214</b>. Based on such input information, the second control unit <b>211</b> performs processing for determining whether to activate the first control unit <b>223</b>.</p><p id="p-0037" num="0036">In a case where the second control unit <b>211</b> determines to activate the first control unit <b>223</b> (cancel the sleep state), the second control unit <b>211</b> controls the first power supply unit <b>210</b> to supply the power to the first control unit <b>223</b>.</p><p id="p-0038" num="0037">The audio output unit <b>218</b> outputs, for example, an audio signal such as an electronic shutter sound from a speaker built in the imaging apparatus <b>101</b> at the time of imaging. A light emitting diode (LED) control unit <b>224</b> controls, for example, an LED disposed in the imaging apparatus <b>101</b> to glow or blink in a pattern set beforehand, at the time of imaging.</p><p id="p-0039" num="0038">The video output unit <b>217</b> consists of for example, a video output terminal, and outputs an image signal for displaying a video image on a connected external display or the like. The audio output unit <b>218</b> and the video output unit <b>217</b> may be an integrated single terminal, e.g., an interface such as a High-Definition Multimedia Interface (HDMI&#xae;) terminal.</p><p id="p-0040" num="0039">A communication unit <b>222</b> is an interface for communication between the imaging apparatus <b>101</b> and an external apparatus. The communication unit <b>222</b> has wireless communication modules such as a Bluetooth&#xae; communication module, a wireless local area network (LAN) communication module, a wireless universal serial bus (USB) module, and a global positioning system (GPS) receiver.</p><p id="p-0041" num="0040">A subject detection unit <b>225</b> reads out image data output from the image processing unit <b>207</b>, from the memory <b>215</b>, and recognizes subjects such as a person and an object. For example, in a case where the subject detection unit <b>225</b> recognizes a person, the subject detection unit <b>225</b> detects the face of the subject. A pattern for determining the face of a subject is registered beforehand in the imaging apparatus <b>101</b>. This pattern is given an identifier for distinguishing each subject. In subject face detection processing, the subject detection unit <b>225</b> detects the face of a subject by detecting a point included in a captured image and matching with the pattern for determining the face of the subject. The subject detection unit <b>225</b> can distinguish each of a plurality of registered persons.</p><p id="p-0042" num="0041">At the same time, the subject detection unit <b>225</b> also calculates a degree of reliability indicating the probability of the detected face of the subject. The degree of reliability is calculated from, for example, the size of a face area in the image, the level of similarity to a face pattern, and the like. In addition, the subject detection unit <b>225</b> can detect face information indicating whether the detected face is a smile, whether eyes are open, the direction of the face, and the like, by performing pattern matching with respect to the face of the subject within the image. The method of detecting the face information is not limited to the pattern matching, and a conventional technique such as a method of utilizing deep learning can be used. The subject detection unit <b>225</b> is an example of a detection unit.</p><p id="p-0043" num="0042">Further, in object recognition processing, the subject detection unit <b>225</b> can recognize an object by determining whether the object matches with a pattern registered beforehand. In addition, the subject detection unit <b>225</b> can recognize an object by extracting a feature amount of a subject, using a histogram of hue, color saturation, or the like within a captured image.</p><p id="p-0044" num="0043">In the above-described methods, the first control unit <b>223</b> can detect a subject from captured image data, using the subject detection unit <b>225</b>.</p><p id="p-0045" num="0000">&#x3c;System Configuration with External Apparatus&#x3e;</p><p id="p-0046" num="0044"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an example of a system configuration for the imaging apparatus <b>101</b> to communicate with a server <b>401</b> via a network and record image data. A configuration of the server <b>401</b> will be described below. In the present exemplary embodiment, the address of the server <b>401</b> is recorded beforehand in the imaging apparatus <b>101</b>.</p><p id="p-0047" num="0045">In the present exemplary embodiment, the imaging apparatus <b>101</b> and the server <b>401</b> are connected via a network router <b>301</b> to communicate with each other. In wireless communication <b>302</b>, the imaging apparatus <b>101</b> and the network router <b>301</b> take the following connection form. The network router <b>301</b> becomes a wireless LAN access point and builds a LAN. The imaging apparatus <b>101</b> becomes a wireless LAN client and joins the LAN built by the network router <b>301</b>. In public communication <b>303</b>, the network router <b>301</b> and the server <b>401</b> communicate using a public line such as a fiber-to-the-home (FTTH) line.</p><p id="p-0048" num="0046">In the present exemplary embodiment, the wireless communication <b>302</b> is described to be the communication based on the wireless LAN as an example, but the wireless communication <b>302</b> may be replaced with wired communication such as Ethernet. In this way, the imaging apparatus <b>101</b> connects to and communicates with the server <b>401</b> via the wireless communication <b>302</b> and the public communication <b>303</b>.</p><heading id="h-0008" level="2">&#x3c;Configuration of External Apparatus&#x3e;</heading><p id="p-0049" num="0047">An example of the configuration of the server <b>401</b> serving as an example of the external apparatus will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Here, the server <b>401</b> will be described as an example of the external apparatus, but the external apparatus is not limited thereto. The external apparatus may be any type of apparatus if the apparatus has a function of receiving image data from the imaging apparatus <b>101</b>. For example, the external apparatus may a tablet device, a personal computer, or the like.</p><p id="p-0050" num="0048">A control unit <b>402</b> is a processor that controls each component of the server <b>401</b> based on an input signal and a program to be described below. The entire apparatus may be controlled by processing shared by a plurality of pieces of hardware (such as a CPU, a GPU, a microprocessor, and an MPU), instead of being controlled by the control unit <b>402</b>.</p><p id="p-0051" num="0049">A nonvolatile memory <b>403</b> is an electrically erasable recordable nonvolatile memory. The nonvolatile memory <b>403</b> stores an operating system (OS) that is basic software to be executed by the control unit <b>402</b>, and an application that implements a practical function in cooperation with the OS. In the present exemplary embodiment, the nonvolatile memory <b>403</b> also stores an application for communicating with the imaging apparatus <b>101</b>.</p><p id="p-0052" num="0050">A work memory <b>404</b> is a buffer to be used as a work area or the like of the control unit <b>402</b>.</p><p id="p-0053" num="0051">A connection unit <b>405</b> is a communication interface for connecting to the imaging apparatus <b>101</b>.</p><heading id="h-0009" level="2">&#x3c;Automatic Imaging Processing&#x3e;</heading><p id="p-0054" num="0052">The automatic imaging processing is processing in which the first control unit <b>223</b> determines timing for imaging and automatically images a subject. In this automatic imaging processing, the first control unit <b>223</b> automatically repeats determination of an imaging target subject and imaging of the subject, in a case where the first control unit <b>223</b> determines that a satisfactory still image or a satisfactory moving image can be obtained, or in a case where a certain length of time has elapsed. This makes it possible for the user to keep a pleasant scene that suddenly appears in daily life or a casual change in daily life, by imaging such a scene or change using the imaging apparatus <b>101</b>, without manually performing imaging.</p><p id="p-0055" num="0053"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart illustrating the automatic imaging processing of the imaging apparatus <b>101</b> in the present exemplary embodiment.</p><p id="p-0056" num="0054">The processing of this flowchart starts when the power switch of the imaging apparatus <b>101</b> is turned on by the user. In the present exemplary embodiment, connection has been established between the imaging apparatus <b>101</b> and the server <b>401</b>. The first control unit <b>223</b> controls each component of the imaging apparatus <b>101</b>, so that each step of the following flowchart is implemented.</p><p id="p-0057" num="0055">In step S<b>501</b>, the first control unit <b>223</b> determines whether the current state is a state where the automatic imaging processing is stopped. The stoppage of the automatic imaging processing will be described in the description of voice recognition processing. In a case where the automatic imaging processing is stopped, the first control unit <b>223</b> waits until the stoppage of the automatic imaging processing is canceled. In other words, in a case where the automatic imaging processing is stopped (YES in step S<b>501</b>), step S<b>501</b> is repeated until the stoppage of the automatic imaging processing is canceled. In a case where the automatic imaging processing is not stopped (NO in step S<b>501</b>), the processing proceeds to step S<b>502</b>.</p><p id="p-0058" num="0056">In step S<b>502</b>, the first control unit <b>223</b> causes the image processing unit <b>207</b> to perform the image processing on a signal captured by the imaging unit <b>206</b>, and to generate an image for subject recognition. Further, the first control unit <b>223</b> controls the subject detection unit <b>225</b> to perform subject recognition such as person recognition and animal recognition, from the generated image for subject recognition. For example, in a case where the subject recognition is performed, a pattern for determining a subject is held beforehand, and the first control unit <b>223</b> determines the subject, using the subject detection unit <b>225</b>, based on the level of similarity between this held pattern and a pattern included in the image for subject recognition. The subject detection unit <b>225</b> can thereby identify the subject. The first control unit <b>223</b> determines the subject, and also detects the position of the subject in an angle of view.</p><p id="p-0059" num="0057">In step S<b>503</b>, the first control unit <b>223</b> calculates an image shake correction amount. Specifically, at first, the first control unit <b>223</b> calculates the absolute angle of the imaging apparatus <b>101</b>, based on angular velocity and acceleration information acquired in the apparatus shake detection unit <b>209</b>. Subsequently, the first control unit <b>223</b> determines an image stabilization angle for moving the tilt rotation unit <b>104</b> and the pan rotation unit <b>105</b> in an angular direction for cancelling the absolute angle, as the image shake correction amount.</p><p id="p-0060" num="0058">In step S<b>504</b>, the first control unit <b>223</b> performs subject search processing, using a subject search unit that automatically searches for a subject and detects the subject. The subject search processing is composed of the following processes.</p><heading id="h-0010" level="2">(1) Area Segmentation</heading><p id="p-0061" num="0059">Area segmentation will be described with reference to <figref idref="DRAWINGS">FIGS. <b>6</b>A to <b>6</b>C</figref>. In <figref idref="DRAWINGS">FIGS. <b>6</b>A to <b>6</b>C</figref>, an area on a spherical surface is segmented using the position of the imaging apparatus <b>101</b> as the origin O. In the example in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, the area is segmented at an interval of 22.5 degrees in each of the tilt direction and the pan direction.</p><p id="p-0062" num="0060">In the segmentation in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, as the angle in the tilt direction increases from the zero degree, the circumference in the horizontal direction decreases, and the size of one area also decreases. Therefore, in the imaging apparatus <b>101</b> of the present exemplary embodiment, in a case where the tilt angle is 45 degrees or more, an area range in the horizontal direction is set to be divided at an interval of an angle greater than 22.5 degrees, as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>.</p><p id="p-0063" num="0061">Next, an area in the angle of view of an image captured by the imaging apparatus <b>101</b> will be described with reference to <figref idref="DRAWINGS">FIGS. <b>6</b>C and <b>6</b>D</figref>. An axis <b>1301</b> is a reference direction for the imaging direction of the imaging apparatus <b>101</b>, and area segmentation is performed based on this direction. The axis <b>1301</b> is, for example, an imaging direction when the imaging apparatus <b>101</b> is activated, or a direction determined beforehand as a direction to be a reference for an imaging direction. An area <b>1302</b> is an angle-of-view area of an image captured by the imaging unit <b>206</b>. <figref idref="DRAWINGS">FIG. <b>6</b>D</figref> illustrates an example of a live view image captured by the imaging unit <b>206</b> in the area <b>1302</b>. In the angle of view of the live view image in <figref idref="DRAWINGS">FIG. <b>6</b>D</figref>, the area of the image is segmented into areas <b>1303</b> to <b>1318</b>, based on the area segmentation illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>C</figref>.</p><heading id="h-0011" level="2">(2) Calculation of Importance Level for Each Area</heading><p id="p-0064" num="0062">For each area obtained by the segmentation described above, an importance level indicating the priority order in searching for a subject is calculated based on a subject present within the area or the situation of the scene of the area. The importance level based on the situation of the subject is calculated based on, for example, the number of subjects present within the area, the size of the face of the subject, the direction of the face of the subject, the probability of the detected face of the subject, the facial expression of the subject, and the result of identifying the subject. Examples of the importance level based on the situation of the scene include a general object recognition result, a scene determination result (such as blue sky, backlight, and evening view), the level of sound from the direction of the area or a voice recognition result, and information about motion detection within the area. Here, the first control unit <b>223</b> performs driving to search all around the imaging apparatus <b>101</b>.</p><p id="p-0065" num="0063">In addition, for example, in a case where the face of a subject is registered, the first control unit <b>223</b> raises the importance level of an area where the face of the registered subject is detected. For example, the face of the subject is recorded in the nonvolatile memory <b>216</b>, as a pattern for determining the subject. In a case where the importance level of the area where the face of the subject is detected is raised, the first control unit <b>223</b> returns the importance level of the area to the original importance level, in response to a lapse of a predetermined time, or the execution of imaging a predetermined number of times.</p><heading id="h-0012" level="2">(3) Determination of Search Area</heading><p id="p-0066" num="0064">After determining the importance level of each area as described above, the first control unit <b>223</b> determines to focus on a search in an area where the importance level is high. Subsequently, the first control unit <b>223</b> calculates a pan angle and a tilt angle desirable for imaging of one of the areas where the importance level is high.</p><p id="p-0067" num="0065">In step S<b>505</b>, the first control unit <b>223</b> performs pan drive and tilt drive. Specifically, based on the image shake correction amount and the pan angle as well as the tilt angle calculated in step S<b>504</b>, a pan drive amount and a tilt drive amount are calculated. The first control unit <b>223</b> then controls the lens barrel rotation drive unit <b>205</b> to drive the tilt rotation unit <b>104</b> and the pan rotation unit <b>105</b>, based on the calculated tilt drive amount and the calculated pan drive amount, respectively. In the present exemplary embodiment, the first control unit <b>223</b> is described to detect a subject in an area where the importance level is high, and starts imaging the subject, by performing the drive in step S<b>505</b>. Subsequently, the first control unit <b>223</b> controls the lens barrel rotation drive unit <b>205</b> to track the subject (continue to keep the subject within the angle of view).</p><p id="p-0068" num="0066">In step S<b>506</b>, the first control unit <b>223</b> controls the zoom unit <b>201</b> to perform zoom drive. For example, the zoom drive is performed based on the state of the subject for which imaging has started in step S<b>505</b>. For example, in a case where the imaged face of the subject is very small in the angle of view, the first control unit <b>223</b> performs zooming to the telephoto side to execute imaging so that the face of the subject has an appropriate (larger) size in the angle of view. On the other hand, in a case where the imaged face of the subject is very large in the angle of view, the first control unit <b>223</b> performs zooming to the wide-angle side to execute imaging so that the face of the subject has an appropriate (smaller) size in the angle of view. The state suitable for tracking the subject can be maintained by thus performing the zoom control.</p><p id="p-0069" num="0067">In step S<b>504</b> to step S<b>506</b>, the method of performing the subject search based on the pan drive, the tilt drive, and the zoom drive is described. However, the subject search may be performed in an imaging system that performs omni-directional imaging at a time, by using a plurality of wide-angle lenses. In this case, if the image processing such as subject detection is performed using all the signals obtained by the omni-directional imaging, as an input image, the overall processing load is large. Therefore, in this case, there is provided a configuration in which part of the image obtained by the omni-directional imaging is clipped, and the subject search processing is performed in the range of the clipped image.</p><p id="p-0070" num="0068">In this configuration, the first control unit <b>223</b> calculates an importance level for each area in a manner similar to the above-described method, changes the clipping position based on the importance level, and performs determination for automatic imaging to be described below.</p><p id="p-0071" num="0069">This makes it possible to perform a high-speed subject search while reducing the power consumed by the image processing.</p><p id="p-0072" num="0070">In step S<b>507</b>, the first control unit <b>223</b> determines whether to perform the automatic imaging using an imaging unit that images a subject.</p><p id="p-0073" num="0071">Here, the determination as to whether to perform the automatic imaging will be described. Whether to perform the automatic imaging is determined based on whether an imaging score exceeds a predetermined value. The imaging score is a parameter to be used for the determination as to whether to perform the automatic imaging. The imaging score is the number of points added based on the situation of detection of a subject and the lapse of time. For example, suppose the automatic imaging is designed to be performed when the imaging score exceeds 2000 points. In this case, at first, the initial value of the imaging score is 0, and points are added as time proceeds, starting from the time when an automatic imaging mode begins. For example, the imaging score increases at an increasing rate to reach 2000 points after a lapse of 120 seconds. In a case where 120 seconds have elapsed without detection of a subject, the imaging score reaches 2000 points based on the point addition due to the lapse of time, and imaging is performed. In addition, 1000 points are added when a subject having high priority is detected by a subject determination unit that determines whether a subject found by the subject search unit is a priority subject, while the time proceeds. For example, the subject having high priority is a subject set by the user as a priority subject to be imaged prior to others, among subjects whose faces are registered in the imaging apparatus <b>101</b>. In a state where the subject having high priority is detected, 2000 points are easily reached, and consequently, the frequency of imaging readily increases.</p><p id="p-0074" num="0072">Further, for example, in a case where a smile of a subject is recognized, 800 points are added. The points based on a smile are added even if the subject is not given high priority. In the present exemplary embodiment, the case where the number of points to be added based on a smile is constant regardless of whether the detected subject is a subject given high priority is described as an example, but the present exemplary embodiment is not limited to this case. For example, points to be added based on detection of a smile of a subject given high priority may be more than points to be added based on detection of a smile of a subject given low priority. This makes it possible to perform imaging more in line with the intention of a user. If the imaging score exceeds 2000 points owing to the point addition based on facial expression changes such as emotions of these subjects, the automatic imaging is performed. Even if the imaging score does not exceed 2000 points even though points are added based on the facial expression changes, the imaging score quickly reaches 2000 points owing to the point addition based on the lapse of time thereafter.</p><p id="p-0075" num="0073">As for the point addition based on the lapse of time, the case where 2000 points are added in 120 seconds is described as an example. In other words, the case where 2000/120 points are added per second, i.e., points are linearly added with respect to the time, is described. However, the present exemplary embodiment is not limited thereto. For example, the imaging score may be increased as follows. No point is added until 110 seconds of 120 seconds, and 200 points are added per second during 10 seconds from 110 seconds to 120 seconds so that 2000 points are reached. This can prevent the imaging score from reaching the number of points for performing imaging because of the point addition based on facial expression changes of subjects regardless of the priority. In the point addition method in which the number of points linearly increases as the time elapses, the state where points are added based on the lapse of time is long. Therefore, in this method, the number of points for performing imaging is frequently reached even by the point addition based on a change to a smile of a subject having low priority, and the difference in priority is not sufficiently reflected. Nevertheless, if the number of points to be added based on facial expression changes is decreased, the timing of a facial expression change is missed, and thus, it is desirable to avoid taking a measure to decrease the number of points to be added. Therefore, no point is added until 110 seconds. Thus, 110 seconds pass without addition of points for the subject having low priority. Meanwhile, 1000 points are supposed to be added when the subject having high priority is detected, and thus, in a state when the subject having high priority is detected, 1000 points are added even if there is no point addition based on the lapse of time until 110 seconds.</p><p id="p-0076" num="0074">Therefore, in a case where the point addition based on facial expression changes is executed, the possibility of reaching the number of points for performing imaging for the subject having low priority can be reduced as compared with that for the subject having high priority, and the difference in priority easily functions. The facial expression change is described above as an example, but other examples of the criterion for point addition, such as a case where voice becomes louder and a case where a gesture becomes large, are conceivable. For these cases as well, the difference in point addition method described above may be provided so that the difference in priority easily functions.</p><p id="p-0077" num="0075">In addition, even if 2000 points are not exceeded by an action of a subject, imaging is ensured to take place in 120 seconds because of the lapse of time, and therefore, a situation where imaging is not performed at all for a certain period of time is unlikely to occur.</p><p id="p-0078" num="0076">In a case where a subject is detected halfway, the time to start the increase may be moved up, within 120 seconds. In other words, for example, in a case where the subject having high priority is detected when 60 seconds have elapsed, 2000 points are not exceeded even if 1000 points are added thereby, but the linear increase may start after a lapse of 30 seconds following the detection of the subject, instead of stopping the increase until 110 seconds. Alternatively, the linear increase may start 20 seconds before the elapse of 120 seconds, instead of starting 10 seconds before. This increases the possibility of imaging the subject having high priority, so that imaging more in line with the intention of a user is implemented.</p><p id="p-0079" num="0077">When the automatic imaging is performed, the imaging score is reset to zero. The automatic imaging is not performed until 2000 points are exceeded again.</p><p id="p-0080" num="0078">This concludes the description of the determination as to whether to perform the automatic imaging. In a case where the first control unit <b>223</b> determines to perform the automatic imaging (YES in step S<b>507</b>), the processing proceeds to step S<b>508</b>. In a case where the first control unit <b>223</b> determines not to perform the automatic imaging (NO in step S<b>507</b>), the processing returns to step S<b>501</b>.</p><p id="p-0081" num="0079">In step S<b>508</b>, the first control unit <b>223</b> executes the imaging processing. The imaging processing is, for example, still image capturing or moving image capturing.</p><p id="p-0082" num="0080">Next, a classification method for the captured images will be described. The image captured based on the above-described determination as to whether to perform the automatic imaging is not necessarily an image in line with the intention of the user. Therefore, the images are classified by an image classification unit, into a high score (high rating), a middle score (middle rating), and a low score (low rating), based on the time taken before the imaging score exceeds the predetermined value. This classification is used in the calculation and the transmission order of automatic transmission target images, in automatic transmission processing to be described below. The automatic transmission processing will be described in detail below.</p><p id="p-0083" num="0081">As with the above-described case, for example, suppose the automatic imaging is designed to be performed when the imaging score exceeds 2000 points. In this case, at first, the initial value of the imaging score is 0, and points are added as time proceeds, starting from the time when the automatic imaging mode begins. For example, the imaging score increases at an increasing rate to reach 2000 points after a lapse of 120 seconds. In a case where 120 seconds have elapsed without detection of a subject, the imaging score reaches 2000 points based on the point addition due to the lapse of time, and imaging is performed. At this moment, no subject is detected, and thus it is difficult to regard the obtained image as an image in line with the intention of the user. Therefore, the image is classified as the low score, in a case where the time taken before the imaging score exceeds the predetermined value is 120 seconds or more. For example, when a subject having high priority is detected while the time proceeds, 1000 points are added. In a case where a specific facial expression such as a smile or a crying face of a subject is recognized, 800 points are added. On the other hand, in a case where a subject has a facial expression lacking emotion such as an expressionless face, no point is added for rating. Even if the subject having high priority is detected, if there is no point addition based on the detection of a smile or the like, 2000 points are reached after a lapse of 60 seconds by the point addition based on the lapse of time, and imaging is performed. It is conceivable that, even if the subject having high priority is imaged, if a scene with a poor change is obtained, the image is not sufficiently in line with the intention of the user. Therefore, in a case where the time taken before the imaging score exceeds the predetermined value is 60 seconds or more and less than 120 seconds, the image is classified as the middle score.</p><p id="p-0084" num="0082">In a case where the subject having high priority is detected while the time proceeds, and there is point addition based on other factor, the predetermined value is exceeded in less than 60 seconds. It is conceivable that the larger the number of points added based on various factors is, the shorter the time taken before the imaging score exceeds the predetermined value is. Therefore, in a case where the time taken before the imaging score exceeds the predetermined value is less than 60 seconds, the image is classified as the high score.</p><p id="p-0085" num="0083">This concludes the description of the classification method for the captured images. In the present exemplary embodiment, the image is classified as a higher rating, as the time taken before the imaging score exceeds 2000 points is shorter, but the present exemplary embodiment is not limited thereto. The imaging apparatus <b>101</b> may determine the image to be a high score image, in a case where imaging is performed in a state where the subject having high priority is detected. Further, the imaging apparatus <b>101</b> may determine the image to be a high score image, in a case where the imaging score exceeds the predetermined value because of point addition based on a facial expression of a subject or a change in facial expression, and imaging is performed. Furthermore, these may be combined, and any type of classification method may be employed if an image in line with the intention of a user is classified as the high score.</p><p id="p-0086" num="0084">This concludes the description of the automatic imaging processing of the imaging apparatus <b>101</b> in the present exemplary embodiment. Such processing for automatically imaging a subject enables the imaging apparatus <b>101</b> to capture an image of a scene desired by a user or a moving image, without receiving an imaging instruction from the user.</p><heading id="h-0013" level="2">&#x3c;Automatic Transmission Processing&#x3e;</heading><p id="p-0087" num="0085"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating the automatic transmission processing of the imaging apparatus <b>101</b> in the present exemplary embodiment.</p><p id="p-0088" num="0086">The processing of this flowchart starts when the recording of image data to be transmitted to the server <b>401</b> is detected. Further, in the present exemplary embodiment, the processing of this flowchart is executed in parallel with the automatic imaging processing illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. The first control unit <b>223</b> executes a program recorded in the nonvolatile memory <b>216</b>, so that this processing is implemented.</p><p id="p-0089" num="0087">In step S<b>701</b>, the first control unit <b>223</b> determines whether image data to be transmitted to the server <b>401</b> is recorded in the recording medium <b>221</b>. Here, the transmission target image data is image data classified as the high score or the middle score by the above-described image classification unit, among image data that has not been transmitted to the server <b>401</b>. The reason why the image of the low score is not transmitted here is that the image of the low score is, conceivably, more likely to be unnecessary image data for the user. In a case where the transmission target image data is recorded (YES in step S<b>701</b>), the processing proceeds to step S<b>702</b>. In a case where the transmission target image data is not recorded (NO, in step S<b>701</b>), step S<b>701</b> is repeated.</p><p id="p-0090" num="0088">In a case where the automatic imaging processing illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> is executed in parallel, and this step is executed after a new transmission target image is captured, the image data of this image is detected as the transmission target image data.</p><p id="p-0091" num="0089">In step S<b>702</b>, the first control unit <b>223</b> joins the LAN built by the network router <b>301</b>, and connects to the server <b>401</b> serving as the external apparatus, via the communication unit <b>222</b>. The first control unit <b>223</b> then determines whether the imaging apparatus <b>101</b> is successfully connected to the server <b>401</b>.</p><p id="p-0092" num="0090">In a case where the connection between the imaging apparatus <b>101</b> and the server <b>401</b> is completed (YES in step S<b>702</b>), the processing proceeds to step S<b>703</b>. In a case where the imaging apparatus <b>101</b> fails to connect to the LAN built by the network router <b>301</b> or fails to establish connection with the server <b>401</b> (NO in step S<b>702</b>), the processing of this flowchart ends.</p><p id="p-0093" num="0091">In step S<b>703</b>, the first control unit <b>223</b> starts transmission of the image data to the server <b>401</b> via the communication unit <b>222</b>. After this step, the first control unit <b>223</b> keeps transmitting the image data until the transmission of the transmission target image data is completed, except when the transmission of the image data is suspended as will be described below.</p><p id="p-0094" num="0092">Here, the order in which the first control unit <b>223</b> transmits the image data will be described. In the present exemplary embodiment, the first control unit <b>223</b> transmits the image data in descending order of score. The image data recorded in the recording medium <b>221</b> is classified into at least the high score, the middle score, and the low score by the above-described image classification unit. The image data is transmitted in such an order for the following reason. For example, in a case where imaging based on the automatic imaging processing is performed for a long time in a state where connection to the server <b>401</b> is not established, this results in a state where a large amount of transmission target image data is recorded in the recording medium <b>221</b>. In a case where the imaging apparatus <b>101</b> transmits the image data in order based on the image capturing date and time in this state, this can lead to a situation where image data of the high score, which is likely to be image data the user wants to view, is not quickly transmitted to the server <b>401</b>. However, naturally, in a case where the user views image data recorded in the server <b>401</b> before the transmission of all the image data is completed, the user cannot view the image data that has not been transmitted to the server <b>401</b>. Therefore, in a case where image data having the latest (or oldest) image capturing date and time is the image data the user most wants to view, the user is forced to wait until the transmission of all the image data of the imaging apparatus <b>101</b> is completed, in the method of transmitting the image data in order based on the image capturing date and time. On the other hand, in the case where the image data is transmitted in descending order of score as described above, the user is more likely to acquire the image data the user wants to view, from the server <b>401</b>, without waiting for the completion of the transmission of the image data to the server <b>401</b> by the imaging apparatus <b>101</b>. Therefore, in the present exemplary embodiment, the first control unit <b>223</b> can transmit the image data the user wants to view to the external apparatus prior to the rest, by transmitting the image data starting from the image data of the high score. In addition, in the present exemplary embodiment, the first control unit <b>223</b> does not transmit the image of the low score. This is because the image of the low score is unlikely to be an image desired by the user.</p><p id="p-0095" num="0093">Further, in the present exemplary embodiment, the first control unit <b>223</b> transmits a still image prior to a moving image. The reason for transmitting the image data in such an order is as follows. The moving image is likely to have a larger file size than that of the still image. In addition, the file size of the moving image increases as the playback time increases. In other words, it is likely that it takes more time to transmit the moving image than to transmit the still image. Therefore, in a case where the moving image is transmitted prior to the still image, since it takes time to transmit the moving image, a situation where none of images including the moving image and the still image is recorded in the server <b>401</b> can occur over a long period of time, even if the automatic transmission processing has started. On the other hand, the file size of the still image is uniform, and thus, in a case where the still image is transmitted prior to the moving image, it is less likely that a situation where no still image is recorded in the server <b>401</b> continues over a long period of time after start of the automatic transmission processing. Therefore, in the present exemplary embodiment, the first control unit <b>223</b> transmits the still image to the server <b>401</b> prior to the moving image.</p><p id="p-0096" num="0094">In a case where there is a plurality of image data of the same score, the first control unit <b>223</b> transmits the image data, starting from image data having a later image capturing date and time. This is because, if images are sequentially transmitted in the order of image capturing date and time from oldest to newest, a state where an image captured most recently is not transmitted for a long time can occur in a case where a large number of transmission target images are recorded.</p><p id="p-0097" num="0095">In summary, in the present exemplary embodiment, the first control unit <b>223</b> transmits the image data in the following order. The image data is transmitted in order of a still image of the high score, a still image of the middle score, a moving image of the high score, and a moving image of the middle score (in a case where there are images of the same score, an image having a later capturing date and time is given higher priority). Since the next imaging processing is not executed, in particular, immediately after imaging, by transmitting the image data in this order, the first control unit <b>223</b> can transmit a high-score image desired by the user to the server <b>401</b>, without suspending the transmission. In step S<b>703</b>, the first control unit <b>223</b> determines image data to be transmitted next in the above-described priority order among the image data recorded in the recording medium <b>221</b>, and starts the transmission of the determined image data.</p><p id="p-0098" num="0096">Incidentally, in the present exemplary embodiment, the imaging apparatus <b>101</b> does not perform the imaging processing and the image data transmission processing, simultaneously. This is because a processing load for the imaging processing and a processing load for the image data transmission processing are both large for the first control unit <b>223</b>, and thus it is efficient for the first control unit <b>223</b> to avoid executing these types of processing in parallel. Therefore, in the present exemplary embodiment, subsequent steps S<b>704</b> to step S<b>707</b> are executed.</p><p id="p-0099" num="0097">In step S<b>704</b>, the first control unit <b>223</b> determines whether to execute imaging in the automatic imaging processing described with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. In a case where imaging in the automatic imaging processing is to be executed (YES in step S<b>704</b>), the processing proceeds to step S<b>705</b>. In other words, in a case where imaging in the automatic imaging processing is to be executed during the transmission of the image data, step S<b>705</b> is performed as interruption processing. The determination as to whether to execute imaging in this step is performed based on a condition similar to that in step S<b>507</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. In the present exemplary embodiment, the first control unit <b>223</b> gives higher priority to the imaging processing than to the image data transmission processing. This is because, when imaging is determined to be performed in the automatic imaging processing, a situation at this moment is conceivably suitable for imaging of a subject, and thus so-called missing a shot can occur if imaging is not performed at this moment. In a case where imaging in the automatic imaging processing is not to be executed (NO in step S<b>704</b>), the processing proceeds to step S<b>707</b>.</p><p id="p-0100" num="0098">In step S<b>705</b>, the first control unit <b>223</b> suspends the transmission of the image data to the server <b>401</b>.</p><p id="p-0101" num="0099">In step S<b>706</b>, the first control unit <b>223</b> executes imaging processing in the automatic imaging processing. The imaging processing is, for example, still image capturing or moving image capturing. In other words, this step is similar to step S<b>508</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0102" num="0100">In step S<b>707</b>, the first control unit <b>223</b> resumes the suspended transmission of the image data, via the communication unit <b>222</b>, and the processing proceeds to step S<b>708</b>.</p><p id="p-0103" num="0101">Here, suspending the transmission of the image data results in a state where the image data being transmitted to the server <b>401</b> is held in an imperfect state in the server <b>401</b>. The server <b>401</b> enters a state of waiting for reception of the image data, but in response to a lapse of a predetermined time, the server <b>401</b> determines that a communication error has occurred, and disconnects from the imaging apparatus <b>101</b>. When being disconnected, the server <b>401</b> cannot receive the image data, and thus deletes the image data held in the imperfect state. In this case, it is desirable for the imaging apparatus <b>101</b> to transmit the image data, which has been in the middle of the transmission, to the server <b>401</b> again. Therefore, in order to maintain the connection with the server <b>401</b>, it is desirable for the imaging apparatus <b>101</b> to minimize the time of suspending the transmission of the image data.</p><p id="p-0104" num="0102">For example, in the automatic transmission processing, in a case where there is no increase in the imaging score by the point addition based on detection of a priority subject or a change in facial expression of a subject, the imaging processing is not performed even if the imaging score exceeds the predetermined value based on the lapse of time.</p><p id="p-0105" num="0103">Further, for example, because the imaging apparatus <b>101</b> suspends the image data transmission processing during recording of the moving image, the imaging apparatus <b>101</b> reduces an imaging duration for the moving image. As a method of reducing the imaging duration, there is a method of setting an upper limit on the imaging duration for the moving image, or reducing the upper limit of the imaging duration when the image transmission is suspended. There is also a method of reducing the upper limit of the imaging duration for the next moving image, in response to disconnection from the server <b>401</b> during imaging of the moving image. There is also a method of stopping the automatic imaging processing, when the priority subject is not detected anymore during recording of the moving image.</p><p id="p-0106" num="0104">In this way, the imaging apparatus <b>101</b> may change the processing content of the automatic imaging processing, in order to prevent disconnection from the server <b>401</b> while the image transmission is suspended.</p><p id="p-0107" num="0105">In step S<b>708</b>, the first control unit <b>223</b> determines whether the transmission of the image data being transmitted is completed. In a case where the transmission of the image data being transmitted is completed (YES in step S<b>708</b>), the processing proceeds to step S<b>709</b>. In a case where the transmission of the image data being transmitted is not completed (NO in step S<b>708</b>), the processing returns to step S<b>704</b>, and the automatic transmission of the image data continues.</p><p id="p-0108" num="0106">In step S<b>709</b>, the first control unit <b>223</b> further determines whether transmission target image data is recorded in the recording medium <b>221</b>. In a case where the transmission target image data is recorded in the recording medium <b>221</b> (YES in step S<b>709</b>), the processing returns to step S<b>703</b>. In a case where the transmission target image data is not recorded (NO in step S<b>709</b>), the processing proceeds to step S<b>710</b>.</p><p id="p-0109" num="0107">In step S<b>710</b>, the first control unit <b>223</b> breaks the connection with the server <b>401</b> established via the communication unit <b>222</b>, and the processing ends.</p><p id="p-0110" num="0108">This concludes the description of the automatic transmission processing for transmitting the image in the present exemplary embodiment.</p><p id="p-0111" num="0109">In the present exemplary embodiment, the example in which the image other than the image of the low score is the transmission target is described, but the present exemplary embodiment is not necessarily limited thereto. For example, the imaging apparatus <b>101</b> may transmit the image of the low score to the server <b>401</b>. In this case, the server <b>401</b> classifies the image data according to the score and displays the image data, so that difficulty when the user views the images is reduced. Further, for example, the imaging apparatus <b>101</b> may use only the image data of the high score as the image transmission target.</p><heading id="h-0014" level="1">OTHER EXEMPLARY EMBODIMENTS</heading><p id="p-0112" num="0110">The present disclosure can also be implemented by processing for supplying a program for implementing one or more functions in the above-described exemplary embodiment to a system or apparatus via a network or a storage medium and causing one or more processors in a computer of the system or apparatus to read and execute the program. The present disclosure can also be implemented by a circuit that implements the one or more functions (for example, an application specific integrated circuit (ASIC)).</p><p id="p-0113" num="0111">The present disclosure is not limited to the above-described exemplary embodiments in unchanged condition, and can be embodied by modifying a component without departing from the gist thereof in an execution phase. In addition, various inventions can be formed by appropriately combining a plurality of components discussed in the above-described exemplary embodiments. For example, some components may be deleted from all the components described in the exemplary embodiments. Further, components of different exemplary embodiments may be combined as appropriate.</p><p id="p-0114" num="0112">According to the exemplary embodiments of the present disclosure, the possibility of failing to image a subject can be reduced in the imaging apparatus that does not execute the imaging processing and the transmission processing in parallel.</p><p id="p-0115" num="0113">While the present disclosure has been described with reference to exemplary embodiments, it is to be understood that the present disclosure is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.</p><p id="p-0116" num="0114">This application claims the benefit of Japanese Patent Application No. 2021-109336, filed Jun. 30, 2021, which is hereby incorporated by reference herein in its entirety.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An imaging apparatus comprising:<claim-text>an imaging unit configured to image a subject;</claim-text><claim-text>a transmission unit configured to transmit image data to an external apparatus;</claim-text><claim-text>a subject search unit configured to automatically search for a subject and detect the subject;</claim-text><claim-text>a determination unit configured to determine whether to image a subject found by the subject search unit; and</claim-text><claim-text>a control unit configured to control transmission processing for transmitting the image data by the transmission unit and imaging processing by the imaging unit not to be performed in parallel,</claim-text><claim-text>wherein the subject search unit searches for a subject even during transmission of the image data by the transmission unit, and</claim-text><claim-text>wherein, in a case where the determination unit determines to image the subject found by the subject search unit while the image data is transmitted by the transmission unit, the transmission unit suspends the transmission of the image data, and the imaging unit images the subject found by the subject search unit.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, when the subject found by the subject search unit is a priority subject, the determination unit determines to image the priority subject.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the determination unit determines to perform imaging in response to a lapse of a predetermined time, when the image data is not transmitted by the transmission unit, and</claim-text><claim-text>wherein the determination unit determines not to perform imaging even after the lapse of the predetermined time, when the image data is transmitted by the transmission unit.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The imaging apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the imaging unit executes the imaging processing in response to the lapse of the predetermined time, when the image data is not transmitted by the transmission unit.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The imaging apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising a measurement unit configured to measure the predetermined time,<claim-text>wherein the measurement unit resets the measured time, in response to imaging by the imaging unit.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, when the transmission of the image data is suspended based on determination by the determination unit, the transmission unit resumes the suspended transmission of the image data, in response to completion of imaging by the imaging unit.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a classification unit configured to classify an image captured by the imaging unit; and</claim-text><claim-text>a control unit configured to control an order of image transmission by the transmission unit, to transmit images starting from an image determined to have a high rating by the classification unit.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The imaging apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the classification unit classifies the image captured by the imaging unit as a higher rating, as a time taken to search for a subject by the subject search unit is shorter.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The imaging apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein, when a subject is detected from the image captured by the imaging unit, the classification unit classifies the image as a higher rating, than when no subject is detected from the image captured by the imaging unit.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The imaging apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein, when a specific facial expression is recognized from a subject detected from the image captured by the imaging unit, the classification unit classifies the image captured by the imaging unit as a higher rating.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The imaging apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein, when images are classified as a same score by the classification unit, the control unit controls transmission of an image having a later image capturing date and time to precede.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The imaging apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the control unit controls a still image to be transmitted prior to a moving image.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The imaging apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the control unit controls an image classified as a low rating by the classification unit not to be transmitted to the external apparatus.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the transmission unit transmits image data generated by the imaging unit to the external apparatus, during a search performed by the subject search unit.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the imaging unit sets an upper limit on an imaging duration for a moving image, when the transmission processing for transmitting image data by the transmission unit is to be performed during a search performed by the subject search unit.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The imaging apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, when the transmission processing for transmitting image data by the transmission unit is to be performed during a search performed by the subject search unit, the imaging unit sets a shorter upper limit on an imaging duration for a moving image, than when the transmission processing for transmitting image data by the transmission unit is not to be performed during a search performed by the subject search unit.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A control method of an imaging apparatus, the control method comprising:<claim-text>imaging a subject;</claim-text><claim-text>transmitting image data to an external apparatus;</claim-text><claim-text>performing a subject search of automatically searching for a subject and detecting the subject;</claim-text><claim-text>determining whether to image a subject found in the subject search; and</claim-text><claim-text>controlling transmission processing for transmitting the image data and imaging processing for imaging the subject not to be performed in parallel,</claim-text><claim-text>wherein, in the subject search, a subject is searched for even during transmission of the image data, and</claim-text><claim-text>wherein, in a case where imaging the subject found by the subject search unit is determined while the image data is transmitted, the transmission of the image data is suspended, and the subject found in the subject search is imaged.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A non-transitory computer-readable storage medium which stores a program for causing a computer to execute a method, the method comprising:<claim-text>imaging a subject;</claim-text><claim-text>transmitting image data to an external apparatus;</claim-text><claim-text>performing a subject search of automatically searching for a subject and detecting the subject;</claim-text><claim-text>determining whether to image a subject found in the subject search; and</claim-text><claim-text>controlling transmission processing for transmitting the image data and imaging processing for imaging the subject not to be performed in parallel,</claim-text><claim-text>wherein, in the subject search, a subject is searched for even during transmission of the image data, and</claim-text><claim-text>wherein, in a case where imaging the subject found by the subject search unit is determined while the image data is transmitted, the transmission of the image data is suspended, and the subject found in the subject search is imaged.</claim-text></claim-text></claim></claims></us-patent-application>