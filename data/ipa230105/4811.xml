<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004812A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004812</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17808949</doc-number><date>20220624</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>082</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6218</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>628</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">HIERARCHICAL SUPERVISED TRAINING FOR NEURAL NETWORKS</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63214940</doc-number><date>20210625</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>QUALCOMM Incorporated</orgname><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>BORSE</last-name><first-name>Shubhankar Mangesh</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>CAI</last-name><first-name>Hong</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Yizhe</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>PORIKLI</last-name><first-name>Fatih Murat</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Certain aspects of the present disclosure provide techniques for training neural networks using hierarchical supervision. An example method generally includes training a neural network with a plurality of stages using a training data set and an initial number of classification clusters into which data in the training data set can be classified. A cluster-validation set performance metric is generated for each stage based on a reduced number of classification clusters relative to the initial number of classification clusters and a validation data set. A number of classification clusters to implement at each stage is selected based on the cluster-validation set performance metric and an angle selected relative to the cluster-validation set performance metric for a last stage of the neural network. The neural network is retrained based on the training data set and the selected number of classification clusters for each stage, and the trained neural network is deployed.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="188.81mm" wi="154.43mm" file="US20230004812A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="244.60mm" wi="163.15mm" orientation="landscape" file="US20230004812A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="220.56mm" wi="156.46mm" file="US20230004812A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="204.72mm" wi="156.38mm" file="US20230004812A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="210.48mm" wi="162.64mm" orientation="landscape" file="US20230004812A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="232.58mm" wi="158.58mm" orientation="landscape" file="US20230004812A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="226.82mm" wi="138.01mm" orientation="landscape" file="US20230004812A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="236.56mm" wi="152.82mm" file="US20230004812A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="236.56mm" wi="152.82mm" file="US20230004812A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit of and priority to U.S. Provisional Patent Application Ser. No. 63/214,940, entitled &#x201c;Hierarchical Supervised Training for Neural Networks,&#x201d; filed Jun. 25, 2021, and assigned to the assignee hereof, the contents of which are hereby incorporated by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">INTRODUCTION</heading><p id="p-0003" num="0002">Aspects of the present disclosure relate to machine learning.</p><p id="p-0004" num="0003">Some applications of machine learning may involve the use of neural networks to classify input data. These neural networks may be used, for example, in various scenarios where semantic information about the data to be classified may be used in the classification process, such as in semantic segmentation of data (e.g., for data compression), augmented reality or virtual reality, in controlling autonomous vehicles, in operations based on domain-specific data (e.g., medical imaging), or the like. Generally, semantic segmentation attempts to classify (or assign a label) to each of a plurality of subcomponents in data input into a neural network for classification. For example, a neural network used to classify different segments of an image can assign one of a plurality of labels to each pixel of the image so that different regions of the image may be correlated to different categories of data.</p><p id="p-0005" num="0004">In some examples, deep neural networks may be trained and deployed to perform various classification tasks using semantic segmentation. A deep neural network generally includes an input layer, one or more intermediate layers, and an output layer which together attempts to perform various tasks, such as classifying an input into one of a plurality of categories, tracking objects across a spatial area, translation, prediction, and so on. However, supervised learning techniques used to train these deep neural networks may not accurately classify data for various reasons.</p><p id="p-0006" num="0005">Accordingly, what is needed are improved techniques for training deep neural networks.</p><heading id="h-0003" level="1">BRIEF SUMMARY</heading><p id="p-0007" num="0006">Certain aspects provide a method for training a neural network. The method generally includes training a neural network with a plurality of stages using a training data set and an initial number of classification clusters into which data in the training data set can be classified. A cluster-validation set performance metric is generated for each stage of the plurality of stages of the neural network based on a reduced number of classification clusters relative to the initial number of classification clusters and a validation data set separate from the training data set. A number of classification clusters to implement at each stage of the plurality of stages of the neural network is selected based on the cluster-validation set performance metric and an angle selected relative to the cluster-validation set performance metric for a last stage of the neural network. The neural network is retrained based on the training data set and the selected number of classification clusters for each stage of the plurality of stages, and the trained neural network is deployed.</p><p id="p-0008" num="0007">Other aspects provide a method for classifying data using a trained neural network. The method generally includes receiving an input for classification. The input is classified using a neural network having a plurality of stages. Generally, each stage of the plurality of stages classifies the input using a different number of classification clusters. One or more actions are taken based on the classification of the input.</p><p id="p-0009" num="0008">Other aspects provide processing systems configured to perform the aforementioned methods as well as those described herein; non-transitory, computer-readable media comprising instructions that, when executed by one or more processors of a processing system, cause the processing system to perform the aforementioned methods as well as those described herein; a computer program product embodied on a computer readable storage medium comprising code for performing the aforementioned methods as well as those further described herein; and a processing system comprising means for performing the aforementioned methods as well as those further described herein.</p><p id="p-0010" num="0009">The following description and the related drawings set forth in detail certain illustrative features of one or more embodiments.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010">The appended figures depict certain aspects of the one or more embodiments and are therefore not to be considered limiting of the scope of this disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an example architecture of a neural network used in generating an inference from a received input.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates example operations that may be performed by a computing system to train a neural network using hierarchical supervision, according to aspects of the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates example operations that may be performed by a computing device to classify data using a neural network trained using hierarchical supervision, according to aspects of the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example plot of cluster-validation set performance for each stage of a plurality of stages in a neural network as a function of a number of classification clusters in each stage in the neural network, according to aspects of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example architecture of a neural network trained using hierarchical supervision, according to aspects of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example architecture of a neural network trained using hierarchical supervision including segmentation transformers associated with each stage of the neural network, according to aspects of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example implementation of a processing system in which a neural network can be trained using hierarchical supervision, according to aspects of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example implementation of a processing system in which data can be classified using a neural network trained using hierarchical supervision, according to aspects of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0020" num="0019">To facilitate understanding, identical reference numerals have been used, where possible, to designate identical elements that are common to the drawings. It is contemplated that elements and features of one embodiment may be beneficially incorporated in other embodiments without further recitation.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">Aspects of the present disclosure provide apparatuses, methods, processing systems, and computer-readable mediums for training neural networks using hierarchical supervision and varying numbers of classification clusters at each stage of the neural network.</p><p id="p-0022" num="0021">Neural networks used in various data classification tasks generally include a number of stages, or layers, which may perform discrete classification tasks in order to classify data input into these neural networks. These neural networks may include encoder-decoder architectures in which an encoder encodes an input into a latent space (or otherwise compressed) representation of the input, a decoder generates a reconstruction of the input, and a classification task is performed based on the latent space representation of the input. These neural networks may also include multi-stage neural networks in which each stage of the neural network is configured to perform task with respect to the input.</p><heading id="h-0006" level="1">Example Neural Network Architecture</heading><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an architecture of a neural network used in generating an inference from a received input. Generally, the neural network <b>100</b> may include any number N of stages through which an input, or data derived by a stage from the input, is processed in order to generate an inference as the output of the neural network <b>100</b>. As illustrated, the neural network <b>100</b> includes a plurality of stages <b>110</b>, <b>120</b>, <b>130</b>, and <b>140</b>, designated as Stage 1, Stage 2, Stage N&#x2212;1, and Stage N, respectively. To generate an inference with respect to an input&#x2014;for example, to classify the input, or portions thereof, into one of a plurality of categories&#x2014;the input may be fed into Stage 1 <b>110</b>. The output of Stage 1 <b>110</b> (e.g., a feature map) may serve as input into Stage 2 <b>120</b>. More generally, for any stage after an initial stage of the neural network <b>100</b> (e.g., for stages <b>120</b>, <b>130</b>, and <b>140</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>), the input for that stage generally includes the output of a previous stage. The output of Stage <b>140</b> (e.g., the N<sup>th </sup>and final stage of the neural network <b>100</b>) may be the inference generated for the input. Though not depicted, in various embodiments, &#x201c;skip&#x201d; connections (also known as residual or shortcut connections) may also be used in neural network <b>100</b> to skip over certain stages, or to accumulate a stage's output with its input, to name just a few examples.</p><p id="p-0024" num="0023">Neural network <b>100</b> may be affected by various complications that result in degraded accuracy of the output of these neural networks. As neural networks become deeper (e.g., as neural networks include more intermediate stages between an input stage and an output stage), neural networks may be increasingly affected by the vanishing gradient problem. The vanishing gradient problem generally refers to a situation arising when, in optimizing a loss function at each stage of the neural network, the gradient of the loss function approaches zero. Thus, in neural networks affected by the vanishing gradient problem, the weights and biases at each stage of the neural network may not be updated effectively, and the resulting neural network may not be able to make accurate inferences against input data. In another example, intermediate stages in these neural networks may not be able to identify sensible patterns in an input that would allow for an accurate output to be generated by the neural network for a given input.</p><p id="p-0025" num="0024">To address the vanishing gradient problem and the inability of intermediate stages in neural networks to identify sensible patterns in an input, and thus to improve the accuracy of neural networks, direct supervision of intermediate stages in the neural network has been proposed. In directly supervising the training of each intermediate stage in the neural network <b>100</b> (e.g., stages 2 through N&#x2212;1 illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>), intermediate stages may be trained using auxiliary loss functions that add a loss term and attempts to mitigate the vanishing gradient problem that deep neural networks can experience. Each intermediate stage may also be trained based on ground truth data, such as ground truth maps representing a desired classification for different portions of an input image. However, because intermediate stages of a neural network may have limited abilities to accurately classify data (e.g., have weaker representation power than the final stage of the neural network), these intermediate stages may also be unable to identify coherent patterns from the input data and the ground truth maps, thus adversely affecting the accuracy of inferences generated by the neural network. Further, in training the intermediate stages in the neural network, differences in the representation power of the intermediate stages and the final stage may be disregarded.</p><heading id="h-0007" level="1">Example Methods for Training Neural Networks Using Hierarchical Supervision</heading><p id="p-0026" num="0025">To improve the accuracy of deep neural networks, aspects of the present disclosure describe techniques by which neural networks can be trained using hierarchical supervision. In using hierarchical supervision to train a neural network, intermediate stages of the neural network may be trained using a reduced number of classification clusters relative to the number of classification clusters into which data can be classified at the final stage of the neural network. Generally, a classification cluster may represent a class into which data can be classified. As discussed in further detail herein, the classification clusters may be used to classify data on a more granular basis at later stages in the neural network and on a more generalized basis at earlier stages in the neural network. By doing so, aspects of the present disclosure may simplify training of intermediate stages of the neural network so that the intermediate stages of the neural network can be trained using fewer computing resources (e.g., processing power, processing time, memory, etc.) than would be used in training the neural network using direct supervision of the intermediate stages of the neural network in which each stage of the neural network is trained using the of classification clusters into which data can be classified at the final stage of the neural network. Further, aspects of the present disclosure may provide for neural networks that more accurately generate inferences for an input than neural networks in which the intermediate stages are trained using direct supervision.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates example operations <b>200</b> that may be performed for training a neural network using hierarchical supervision, according to certain aspects of the present disclosure. Operations <b>200</b> may be performed, for example, by a physical or virtual computing device or cluster of physical and/or virtual computing devices on which neural networks can be trained.</p><p id="p-0028" num="0027">As illustrated, operations <b>200</b> begin at block <b>210</b>, where a neural network is trained. The neural network generally includes a plurality of stages. The neural network may be trained using a training data set and an initial number of classification clusters into which data in the training data set can be classified. Generally, training the neural network may include training a new neural network from a training data set, further training a partially trained model, or fine tuning an already trained model (e.g., by performing retraining, incremental training, training in a federated learning scheme, and the like).</p><p id="p-0029" num="0028">Generally, the neural network may be trained using supervised learning techniques in which each element in the training data set is labeled with information identifying a category to which the element belongs. The training data set may be generated as a portion of a larger data set from which the training data set and a validation data set may be generated. Generally, the training data set may be significantly larger than the validation data set. For example, the training data set may be ninety percent of the overall data set, and the validation data set may be the remaining ten percent of the overall data set.</p><p id="p-0030" num="0029">At block <b>220</b>, a cluster-validation set performance metric is generated for each stage of the plurality of stages. The cluster-validation set performance metric may be based on a reduced number of classification clusters relative to the initial number of classification clusters and a validation data set separate from the training data set. Generally, reducing the number of classification clusters may result in classification clusters that encompass broader classes of data. By doing so, earlier stages in a neural network, which may have less robust abilities to classify data at a granular level, can be trained to classify data into broader classes. This may improve the performance of neural networks used in classifying data, such as by increasing the accuracy of predictions made using these neural networks and reducing compute resources used in training these neural networks.</p><p id="p-0031" num="0030">In some aspects, the reduced number of classification clusters may be defined a priori. The set of classification clusters into which data in the training data set can be classified may include a number of specific species of classifications that can be grouped into an overall genus. For example, assume that the set of classification clusters includes the classifications &#x201c;train,&#x201d; &#x201c;car,&#x201d; &#x201c;bus,&#x201d; and &#x201c;bicycle.&#x201d; Based on human knowledge and an a priori defined reduction in the set of classification clusters, the classifications of &#x201c;train,&#x201d; &#x201c;car,&#x201d; &#x201c;bus,&#x201d; and &#x201c;bicycle&#x201d; may be consolidated into a single cluster representing, for example, wheeled transportation devices as an overall group, or the like.</p><p id="p-0032" num="0031">In some aspects, the reduced number of classification clusters may be generated using agglomerative clustering techniques. As discussed above, at block <b>210</b>, the neural network may be trained using direct supervision on the training data set. Two confusion matrices can be generated using the trained neural network: a first confusion matrix, C<sub>out</sub><sup>T </sup>calculated for the training data set and a second confusion matrix C<sub>out </sub>calculated for the validation data set. Generally, the confusion matrices identifies, for each class in the set of classification clusters into which data can be classified, a number of true positive predictions, a number of false positive predictions, and a number of false negative predictions. Subsequently, an adjacency matrix A<sub>out </sub>can be calculated over the set of classification clusters according to the equation:</p><p id="p-0033" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>A</mi>      <mrow>       <mi>o</mi>       <mo>&#x2062;</mo>       <mi>u</mi>       <mo>&#x2062;</mo>       <mi>t</mi>      </mrow>     </msub>     <mo>=</mo>     <mfrac>      <mrow>       <msub>        <mi>C</mi>        <mi>out</mi>       </msub>       <mo>+</mo>       <msubsup>        <mi>C</mi>        <mi>out</mi>        <mi>T</mi>       </msubsup>      </mrow>      <mrow>       <mo>&#xf605;</mo>       <mrow>        <msub>         <mi>C</mi>         <mi>out</mi>        </msub>        <mo>+</mo>        <msubsup>         <mi>C</mi>         <mi>out</mi>         <mi>T</mi>        </msubsup>       </mrow>       <mo>&#xf606;</mo>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0034" num="0032">An adjacency matrix can be generated for each stage i through N of an N-stage neural network such that A<sub>out,i</sub>&#x2200;i&#x2208;[1, N]. At each stage, agglomerative clustering can be used to combine clusters in the calculated adjacency matrix such that a plurality of neighboring clusters are reduced into a single cluster. This single cluster generally represents a broader classification of data than the classification associated with any one of the plurality of neighboring clusters that were consolidated into the single cluster.</p><p id="p-0035" num="0033">In still another example, spectral clustering can be used to reduce the set of classification clusters into which data can be classified into a smaller set of classification clusters. Generally, spectral clustering allows for groups of classification clusters to be consolidated into a single larger group based on a graph representation and edges connecting nodes in the graph, where each classification cluster is represented by a node in the graph. To spectrally cluster classification clusters, the adjacency matrix for a given stage i, A<sub>out,i</sub>, may be calculated according to Equation (1) above. One or more orthogonal eigenvectors can be identified within the adjacency matrix and clustered into a number of clusters. Data points within the adjacency matrix, representing different clusters in the set of classification clusters, may be consolidated into a single, broader, cluster based on determining that the data point is located in a row also assigned to a given cluster.</p><p id="p-0036" num="0034">In still another example, each stage of the neural network may include a segmentation transformer module (also referred to as an object-contextual representation (OCR) module). Generally, a segmentation transformer module (or OCR module) characterizes data based on the relationship between the data and data in a surrounding region in an image, based on assumptions that a data point surrounded by data points of a given classification is likely to be similarly classified. In such an example, the segmentation transformer can extract a one-dimensional embedding for each classification cluster. Class-wise embeddings may be extracted by executing inferences on the validation data set, and k-means clustering may be applied to these embeddings in order to generate the reduced number of classification clusters.</p><p id="p-0037" num="0035">At block <b>230</b>, a number of classification clusters to implement at each stage of the plurality of stages of the neural network is selected. The number of classification clusters may be selected based on the calculated cluster-validation set performance metric and an angle selected relative to the cluster-validation set performance metric for a last stage of the neural network, as discussed in further detail below and illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0038" num="0036">In some aspects, to select the number of classification clusters to implement at each stage of the plurality of stages of the neural network, the generated cluster-validation set performance metric for each stage of the plurality of stages of the neural network can be plotted to show a relationship between inference performance and the number of clusters implemented at each stage of the plurality of stages. The generated cluster-validation set performance metric for the last stage of the neural network and the initial number of classification clusters may be selected as an origin point. From a vertical axis drawn from this origin point, an angle &#x3b8; for a line drawn from the origin point may be selected to identify the number of classification clusters to implement at each intermediate stage of the neural network. In some aspects, the angle &#x3b8; may range between 0&#xb0; and 90&#xb0;. A selected angle of &#x3b8;=0&#xb0; generally indicates that the neural network may be trained using direct supervision (e.g., using the same number of classification clusters), as each stage in the neural network may be trained using a same (or similar) number of classification clusters. A selected angle of &#x3b8;=90&#xb0; generally indicates that each stage in training should converge on a same or similar performance level (e.g., by using a number of classification clusters resulting in inference accuracy for any given stage being within a threshold amount of inference accuracy at the origin point). A selected angle &#x3b8; between 0&#xb0; and 90&#xb0; may result in a progressive increase in the number of classification clusters used in each successive stage of the neural network. In some aspects, some angle between 0&#xb0; and 90&#xb0; may result in a highest inference performance (e.g., classification accuracy) for the neural network.</p><p id="p-0039" num="0037">Generally, the selected angle &#x3b8; may be used to identify the performance level of each stage in the neural network and the corresponding number of classification clusters to implement at each stage in the neural network. Various techniques can be used to identify the selected angle &#x3b8; on the plot of the generated cluster-validation set performance metrics for each stage of the neural network. In one example, the selected angle &#x3b8; may be selected based on a largest increase in performance between different stages in the neural network. In one example, a hyper-parameter search may be conducted to identify the angle &#x3b8; resulting in a highest performance (e.g., accuracy) for the neural network. In some aspects, the angle may be selected such that successive stages in the neural network use increasing numbers of classification clusters relative to preceding stages in the neural network (e.g., the number of classification clusters monotonically increases as the layer number increases).</p><p id="p-0040" num="0038">At <b>240</b>, the neural network is retrained based on the training data set and the selected number of classification clusters for each stage of the plurality of stages.</p><p id="p-0041" num="0039">In some aspects, the neural network may be retained using single-stage training or multi-stage training. In single-stage training, there may not be a priori knowledge of the capabilities of the neural network. To compensate, the selected number of classification clusters used in each stage may be defined a priori. For example, for a number of stages N in the neural network, the number of classification clusters at the i<sup>th </sup>stage, where 1&#x2264;i&#x2264;N, may be defined as</p><p id="p-0042" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mfrac>   <mn>1</mn>   <msup>    <mn>2</mn>    <mrow>     <mi>N</mi>     <mo>-</mo>     <mi>i</mi>    </mrow>   </msup>  </mfrac>  <mo>*</mo>  <mrow>   <mi>TotalClassificationClusters</mi>   <mo>.</mo>  </mrow> </mrow></math></maths></p><p id="p-0043" num="0040">In multi-stage training, the number of classification clusters for each stage may be selected using a plot and a selected angle &#x3b8; relative to a defined origin point, as discussed above. At each stage, the classification clusters can be combined using various clustering techniques (as discussed above) so that the number of classification clusters equals a smaller number than the total number of classification clusters and equals the number of clusters at a point on the plot at which performance for the stage and a line drawn from the origin point using the selected angle &#x3b8; intersect.</p><p id="p-0044" num="0041">Generally, re-training the neural network may be performed by minimizing a loss function over each stage in the neural network. Where stage N represents the output stage of the neural network, and any given stage i has K<sub>i </sub>classification clusters (where K<sub>i </sub>represents a subset of the K classification clusters into which the neural network, the loss associated with the output stage N may be represented by the equation:</p><p id="p-0045" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>L</mi>      <msub>       <mi>K</mi>       <mi>N</mi>      </msub>     </msub>     <mo>=</mo>     <mfrac>      <mrow>       <msubsup>        <mo>&#x2211;</mo>        <mrow>         <mi>n</mi>         <mo>=</mo>         <mn>0</mn>        </mrow>        <mrow>         <msub>          <mi>K</mi>          <mi>N</mi>         </msub>         <mo>-</mo>         <mn>1</mn>        </mrow>       </msubsup>       <msub>        <mi>L</mi>        <mi>n</mi>       </msub>      </mrow>      <msub>       <mi>K</mi>       <mi>N</mi>      </msub>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0046" num="0000">where L<sub>n </sub>represents the binary loss term associated with a classification cluster n. The overall loss term, over the trained neural network, may be represented by the equation:</p><p id="p-0047" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>L</mi>      <mi>total</mi>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>N</mi>       </munderover>       <mrow>        <msub>         <mi>&#x3b3;</mi>         <mi>i</mi>        </msub>        <mo>&#x2062;</mo>        <msub>         <mi>L</mi>         <msub>          <mi>K</mi>          <mi>i</mi>         </msub>        </msub>       </mrow>      </mrow>      <mo>=</mo>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>N</mi>       </munderover>       <mrow>        <msub>         <mi>&#x3b3;</mi>         <mi>i</mi>        </msub>        <mo>&#x2062;</mo>        <mfrac>         <mrow>          <msubsup>           <mo>&#x2211;</mo>           <mrow>            <mi>n</mi>            <mo>=</mo>            <mn>0</mn>           </mrow>           <mrow>            <msub>             <mi>K</mi>             <mi>i</mi>            </msub>            <mo>-</mo>            <mn>1</mn>           </mrow>          </msubsup>          <msub>           <mi>L</mi>           <mi>n</mi>          </msub>         </mrow>         <msub>          <mi>K</mi>          <mi>i</mi>         </msub>        </mfrac>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0048" num="0000">where &#x3b3;<sub>i </sub>is the weight hyper-parameter associated with stage i of the neural network.</p><p id="p-0049" num="0042">At block <b>250</b>, the trained neural network is deployed. The neural network may be deployed to an endpoint device on which inferences can be performed locally, such as a mobile phone, a desktop or laptop computer, a vehicle user equipment (UE), or the like. In some aspects, the neural network may be deployed to a networked computing system (e.g., a server or cluster of servers). The networked computing system may be configured to receive, from a remote computing device, a request for an inference to be performed on a given input, use the neural network to generate the inference for the input, and output the inference to the remote computing device for the remote computing device to use in executing one or more actions on an application executing on the remote computing device.</p><p id="p-0050" num="0043">In training a neural network using hierarchical supervision, a backbone network may be trained by imposing auxiliary supervision through segmentation heads attached to intermediate (or transitional) layers of the neural network. For a set of S ground truth predictions, a smaller set of semantic labels may be generated at each stage of the neural network, such that i, &#x2200;i&#x2208;{1, . . . , N}, where i represents an intermediate stage in the neural network, and N&#x3c;S. The resulting loss function may be represented by the equation</p><p id="p-0051" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>            <mi>total</mi>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>-</mo>         <mn>1</mn>        </mrow>        <mi>N</mi>       </munderover>       <mrow>        <msub>         <mi>&#x3b3;</mi>         <mi>i</mi>        </msub>        <msubsup>                  <mi>i</mi>         <msub>          <mi>S</mi>          <mi>i</mi>         </msub>        </msubsup>       </mrow>      </mrow>      <mo>+</mo>      <msub>              <mi>final</mi>      </msub>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0052" num="0000">where <img id="CUSTOM-CHARACTER-00001" he="3.22mm" wi="2.46mm" file="US20230004812A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>i</sub><sup>S</sup><sup><sub2>i </sub2></sup>is the segmentation loss for the i<sup>th </sup>intermediate stage, &#x3b3;<sub>i </sub>is the weight of the i<sup>th </sup>intermediate stage, and <img id="CUSTOM-CHARACTER-00002" he="3.22mm" wi="2.46mm" file="US20230004812A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>final </sub>represents the segmentation loss at the final stage of the neural network. Unlike neural networks trained using the same set of classes at each stage of the neural network, aspects of the present disclosure train a neural network by supervising each intermediate layer with an optimal task complexity in terms of the set of semantic classes.</p><p id="p-0053" num="0044">As discussed, during training, a reduced number of classifications relative to the full set of classifications may be used to train each intermediate stage of the neural network. In doing so, learning tasks may be customized for each stage in the neural network so that training is neither too complex nor too simple, both of which may lead to unoptimized inference performance (e.g., accuracy) for the neural network. In some aspects, some intermediate stages may be trained to perform classification tasks on very broad categories, while other (later) intermediate stages may be trained to perform classification tasks on narrower categories. For example, in an object detection system, an intermediate layer of the neural network might be trained to classify objects into either a stationary object or a moving object class, and later intermediate layers of the neural network may be trained to classify data more granularly. For example, for stationary objects, an intermediate layer may be trained to classify these objects as either living or non-living objects, a further intermediate layer may be trained to classify living objects into one of a plurality of species, and so on.</p><p id="p-0054" num="0045">Generally, to allow for the final segmentation layer to use the hierarchy of features in generating an inference, various fusing techniques may be used to provide a set of semantic data to the last layer for use in segmentation. For example, for each intermediate layer, the segmentation features for that layer may be input into an Object Contextual Representation (OCR) block, which enhances the features via relational context attention. These enhanced intermediate features are then fused and provided to the final segmentation layer. To reduce computational cost with the task complexity, the number of channels defined for an intermediate OCR block may be set to a smaller number of channels than of the number of channels in the next stage we set the number of channels (e.g., &#xbd; of the number of channels in the next stage).</p><heading id="h-0008" level="1">Example Methods for Classifying Data Using Neural Networks Trained Using Hierarchical Supervision</heading><p id="p-0055" num="0046"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates example operations that may be performed by a computing device to classify data using a neural network trained using hierarchical supervision, according to certain aspects of the present disclosure. Operations <b>300</b> may be performed, for example, by a physical or virtual computing device or cluster of physical and/or virtual computing devices on which neural networks can be deployed and used to classify an input and take one or more actious based on the classification of the input.</p><p id="p-0056" num="0047">As illustrated, operations <b>300</b> begin at block <b>310</b>, where an input is received for classification. The input may include, for example, an image captured by one or more cameras or other imaging devices communicatively coupled with the computing device on which the neural network is deployed and executing. For example, the input may include domain-specific imaging data, such as images captured by a medical imaging device (e.g., X-ray machines, computed tomography machines, magnetic resonance imaging machines, etc.). In another example, the input may include information to be used in real-time decision making, such as camera or other imaging data from one or more imaging devices used by a vehicle user equipment (UE) operating autonomously or semi-autonomously.</p><p id="p-0057" num="0048">At block <b>320</b>, the input is classified using a neural network having a plurality of stages. Each stage of the plurality of stages generally classifies the input using a different number of classification clusters. For example, each stage of the plurality of stages downstream from the final stage (e.g., stages prior to a final stage of the neural network) in the neural network may be trained to generate an inference using a reduced number of classification clusters relative to a number of classification clusters used by the final stage. In some aspects, the stages may use a monotonically increasing number of classification clusters as a function of the stage number such that a first stage of the neural network classifies the input into x classification clusters, a second stage of the neural network classifies the input into y classification clusters, a third stage of the neural network classifies the input into z classification clusters, and so on, where x&#x3c;y&#x3c;z. The number of classification clusters used at each stage of the neural network may be defined a priori according to an equation defining the number of classification clusters as a function of the stage number or may be selected based on a cluster-validation set performance metric for the final stage of the neural network and the number of classification clusters used by the final stage and an angle selected for a line drawn from a point on a plot corresponding to the cluster-validation set performance metric for the final stage of the neural network.</p><p id="p-0058" num="0049">At block <b>330</b>, one or more actions are taken based on the classification of the input. Generally, the one or more actions may be associated with a specific application for which data is being classified. In a medical application, in which domain-specific imagery is classified using the neural network, the one or more actions may include identifying portions of an image, corresponding to areas in a human body, in which a disease is present. In an autonomous vehicle or semi-autonomous vehicle application, the one or more actions may include identifying a direction of travel and applying a steering input to cause the vehicle to travel in the identified direction, accelerating or decelerating the vehicle, or otherwise controlling the vehicle to avoid obstacles or harm to persons or property in the vicinity of the vehicle.</p><heading id="h-0009" level="1">Example Cluster-Validation Set Performance Metric Plot for Selecting a Number of Classification Clusters Used in Stages of a Neural Network</heading><p id="p-0059" num="0050"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example plot <b>400</b> of cluster-validation set performance for each stage of a plurality of stages in a neural network as a function of a number of classification clusters in the neural network.</p><p id="p-0060" num="0051">In particular, plot <b>400</b> includes first stage inference performance line <b>402</b>, second stage inference performance line <b>404</b>, and third stage inference performance line <b>406</b> for the different stages of a three-stage neural network. In plot <b>400</b>, inference accuracy, represented on the vertical axis by a mean intersection over union (mIoU) measurement for each number of classification clusters from a defined minimum to a defined maximum number of classification clusters. Generally, inference accuracy increases as the number of classification clusters decreases (at the expense of the usefulness of any given inference, as broad classifications may be less useful than more granular classifications). The mIoU value for each stage of the neural network and each number of classification clusters generally represents an accuracy of classifications made by the neural network based on a ratio of true positives to the number of true positives, false negatives, and false negatives identified by the neural network.</p><p id="p-0061" num="0052">To identify a number of classification clusters to use in retraining the intermediate stages of the neural network (e.g., stages other than an input stage and a final stage&#x2014;in this example, Stage 3&#x2014;of the neural network), inference performance of the final stage of the neural network for the maximum number of classification clusters into which data can be classified may be selected as origin point <b>410</b>. An angle &#x3b8; may be selected for drawing a line <b>420</b> in the plot <b>400</b> from the origin point <b>410</b>, with an angle measured from the vertical axis to the horizontal axis. As discussed, when &#x3b8;=0&#xb0;, the neural network may be trained using direct supervision and the same number of classification clusters in each stage of the neural network. Meanwhile, when &#x3b8;=90&#xb0;, inference performance for each stage may converge to a value within a threshold amount from the performance of the final stage at the origin point <b>410</b>.</p><p id="p-0062" num="0053">Various techniques may be used to select the angle &#x3b8; used in drawing line <b>420</b> from origin point <b>410</b>. In some aspects, &#x201c;greedy&#x201d; techniques may be used to attempt to identify the angle resulting in the largest overall gain in inference performance between one or more of the intermediate stages of the neural network and the final stage of the neural network.</p><p id="p-0063" num="0054">After angle &#x3b8; is identified, and line <b>420</b> is drawn on plot <b>400</b>, the number of classification clusters to use at each intermediate stage of the neural network may be identified. Generally, the number of classification clusters to use at any given intermediate stage of the neural network may be the number of classification clusters at the point where the inference performance line intersects with the line <b>420</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, thus, the second stage of the neural network may be retrained to classify data into the number of clusters at point K2 <b>430</b>, and the first stage of the neural network may be retrained to classify data into the number of clusters at point K1 <b>440</b>. In this manner, hierarchical supervision of the neural network may be achieved by using smaller numbers of classification clusters in earlier stages of a neural network and increasing the number of classification clusters used in later stages of the neural network until the maximum number of classification clusters are used by the final stage of the neural network.</p><heading id="h-0010" level="1">Example Architectures for Neural Networks Trained Using Hierarchical Supervision</heading><p id="p-0064" num="0055"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example architecture of a neural network <b>500</b> trained using hierarchical supervision, according to aspects of the present disclosure. Neural network <b>500</b> includes an input stage <b>510</b>, a first intermediate stage <b>520</b>, a second intermediate stage <b>530</b>, and an output stage <b>540</b>. Within each stage, as illustrated conceptually, an input from a prior stage may be further compressed into another representation, and the data generated by a prior stage may be an input into a current stage of the neural network.</p><p id="p-0065" num="0056">Input stage <b>510</b> represents a stage of neural network <b>500</b> that is configured to receive an input of data to be classified through neural network <b>500</b>. Input stage <b>510</b> generally dispatches the received input to a first intermediate stage <b>520</b>, which generates first stage output <b>522</b> using a first number of classification clusters that is less than the number of classification clusters into which data can be classified at output stage <b>540</b> of the neural network <b>500</b>. In the example illustrated herein, the input received at input stage <b>510</b> may be an image captured by an imaging device in an autonomous vehicle, and the first stage output <b>522</b> may include a classification of different pixels in the input image, representing different portions of the environment in which the autonomous vehicle is operating, into one of a plurality of object classifications (e.g., road, buildings, other vehicles, etc.)</p><p id="p-0066" num="0057">The output of the first intermediate stage <b>520</b> may be input into second intermediate stage <b>530</b>. Similar to first intermediate stage <b>520</b>, second intermediate stage <b>530</b> may be configured to classify the data input from first intermediate stage <b>520</b> using a second number of classification clusters. The second number of classification clusters may be greater than the first number of classification clusters and may be less than the number of classification clusters into which data can be classified at output stage <b>540</b> of the neural network <b>500</b>. For example, intermediate stage <b>520</b> may classify data using the number of classification clusters associated with point K1 <b>440</b>, while intermediate stage <b>530</b> may classify data using the number of classification clusters associated with point K2 <b>430</b>. In the example illustrated herein, the second stage output <b>532</b> also includes a classification of different pixels in the received image into one of a plurality of classes. Different representations of these pixels, such as different color values, generally represent different classifications into which data is classified. In this example, relative to output <b>522</b> in which all vehicles in the image are classified similarly, second intermediate stage <b>530</b> may be configured to recognize differences between different types of vehicles. Instead of classifying all vehicles in the image into the generic class of vehicles, second intermediate stage <b>530</b> can classify vehicles into a first category of four-wheeled vehicles and a second category of two-wheeled vehicles.</p><p id="p-0067" num="0058">The output of second intermediate stage <b>530</b> may be provided as input into the output stage <b>540</b>, which is configured to generate a final classification of data in the image and output the final classification <b>542</b> for use in identifying an action to perform based on the final classification. Output stage <b>540</b>, as discussed, is generally trained to classify data into the number of classification clusters, which is larger than the number of classification clusters implemented at first intermediate stage <b>520</b> and second intermediate stage <b>530</b>. In this example, further granular detail has been identified at output stage <b>540</b> such that different portions of a flat surface are delineated between road surfaces and non-road surfaces.</p><p id="p-0068" num="0059">Each of first intermediate stage <b>520</b>, second intermediate stage <b>530</b>, and output stage <b>540</b> may be trained using supervised learning techniques. As discussed, the supervised learning techniques may be hierarchical, such that earlier stages in the neural network <b>500</b> are trained to classify data into fewer classification clusters than later stages in the neural network. By doing so, aspects of the present disclosure may improve the accuracy of the neural network <b>500</b> while taking into account the computational power available to perform inferences at any given stage of neural network <b>500</b>.</p><p id="p-0069" num="0060"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example architecture of a neural network <b>600</b> trained using hierarchical supervision in which the neural network includes segmentation transformers associated with each stage of the neural network, according to aspects of the present disclosure.</p><p id="p-0070" num="0061">As illustrated, neural network <b>600</b> includes an input stage <b>610</b>, a plurality of intermediate stages <b>620</b> and <b>630</b>, and an output stage <b>640</b>. Each intermediate stage <b>620</b> and <b>630</b> is associated with a respective segmentation transformer (or OCR module) <b>622</b> and <b>632</b>, respectively, and output stage <b>640</b> may be associated with an output segmentation transformer <b>642</b>. These segmentation transformers, as discussed, allow for a one-dimensional embedding to be extracted for each class.</p><p id="p-0071" num="0062">As illustrated, each segmentation transformer <b>622</b>, <b>632</b>, and <b>642</b> may be configured to classify data into a number of classification clusters selected as a function of the stage of the neural network in which the segmentation transformers are deployed. For a neural network where the number of stages N=3, segmentation transformer <b>642</b> (associated with the final stage <b>640</b> of the neural network <b>600</b>) may be trained to classify data into</p><p id="p-0072" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mrow>  <mfrac>   <mn>1</mn>   <msup>    <mn>2</mn>    <mrow>     <mn>3</mn>     <mo>-</mo>     <mn>3</mn>    </mrow>   </msup>  </mfrac>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <msup>     <mn>2</mn>     <mn>0</mn>    </msup>   </mfrac>   <mo>=</mo>   <mrow>    <mn>1</mn>    <mo>&#x2062;</mo>    <mi>x</mi>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0073" num="0000">the total number of classification clusters. Intermediate stage <b>630</b>, being the second stage in the neural network <b>600</b>, may be trained to classify data into</p><p id="p-0074" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mrow>  <mfrac>   <mn>1</mn>   <msup>    <mn>2</mn>    <mrow>     <mn>3</mn>     <mo>-</mo>     <mn>2</mn>    </mrow>   </msup>  </mfrac>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <msup>     <mn>2</mn>     <mn>1</mn>    </msup>   </mfrac>   <mo>=</mo>   <mrow>    <mfrac>     <mn>1</mn>     <mn>2</mn>    </mfrac>    <mo>&#x2062;</mo>    <mi>x</mi>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0075" num="0000">the total number of classification clusters. Finally, intermediate stage <b>620</b>, being the first stage in the neural network <b>600</b>, may be trained to classify data into</p><p id="p-0076" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mrow>  <mfrac>   <mn>1</mn>   <msup>    <mn>2</mn>    <mrow>     <mn>3</mn>     <mo>-</mo>     <mn>1</mn>    </mrow>   </msup>  </mfrac>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <msup>     <mn>2</mn>     <mn>2</mn>    </msup>   </mfrac>   <mo>=</mo>   <mrow>    <mfrac>     <mn>1</mn>     <mn>4</mn>    </mfrac>    <mo>&#x2062;</mo>    <mi>x</mi>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0077" num="0000">the total number of classification clusters.</p><p id="p-0078" num="0063">To provide additional information in training neural network <b>600</b>, the outputs of the segmentation transformers associated with stages in the neural network <b>600</b> other than final stage <b>640</b> (e.g., as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the outputs of segmentation transformers <b>622</b> and <b>632</b>) may be concatenated at concatenator <b>650</b>. That is, for an N stage neural network, the outputs of the segmentation transformers associated with stages 1 through N&#x2212;1 of the neural network may be concatenated. The output of concatenator <b>650</b> may be input into the final stage <b>640</b> of the neural network (e.g., stage 3 of a neural network where N=3) to train the final stage. Concatenation of the outputs of the segmentation transformers may impose an additional processing overhead in training in generating inferences through the neural network <b>600</b>, but may allow for additional information to be used in training and generating inferences using the neural network <b>600</b> and improve the accuracy of inferences generated by the neural network <b>600</b>.</p><p id="p-0079" num="0064">The performance of the neural networks trained using the techniques discussed herein generally result in increased inference performance relative to training multi-stage neural networks using direct supervision. For example, inference accuracy, measured by mean intersection over union (mIoU) is generally higher for neural networks trained using the hierarchical supervision techniques discussed herein than for neural networks trained using direct supervision techniques. In some aspects, various techniques (such as incorporating segmentation transformers into each stage of the neural network) may result in both increased inference accuracy and increased throughput (e.g., as measured in billions of multiply-and-accumulate operations (MACs)). The hierarchical supervision techniques discussed herein, when controlled for constant throughput (e.g., a similar number of MACs), may still result in increased inference accuracy for the same or similar computational cost.</p><heading id="h-0011" level="1">Example Processing Systems for Training Machine Learning Models Using Hierarchical Supervision</heading><p id="p-0080" num="0065"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts an example processing system <b>700</b> for training a neural network using hierarchical supervision, such as described herein for example with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0081" num="0066">Processing system <b>700</b> includes a central processing unit (CPU) <b>702</b>, which in some examples may be a multi-core CPU. Instructions executed at the CPU <b>702</b> may be loaded, for example, from a program memory associated with the CPU <b>702</b> or may be loaded from a memory <b>724</b> or memory partition.</p><p id="p-0082" num="0067">Processing system <b>700</b> also includes additional processing components tailored to specific functions, such as a graphics processing unit (GPU) <b>704</b>, a digital signal processor (DSP) <b>706</b>, a neural processing unit (NPU) <b>708</b>, and a wireless connectivity component <b>712</b>.</p><p id="p-0083" num="0068">An NPU, such as <b>708</b>, is generally a specialized circuit configured for implementing all the necessary control and arithmetic logic for executing machine learning algorithms, such as algorithms for processing artificial neural networks (ANNs), deep neural networks (DNNs), random forests (RFs), and the like. An NPU may sometimes alternatively be referred to as a neural signal processor (NSP), tensor processing units (TPU), neural network processor (NNP), intelligence processing unit (IPU), vision processing unit (VPU), or graph processing unit.</p><p id="p-0084" num="0069">NPUs, such as <b>708</b>, are configured to accelerate the performance of common machine learning tasks, such as image classification, machine translation, object detection, and various other predictive models. In some examples, a plurality of NPUs may be instantiated on a single chip, such as a system on a chip (SoC), while in other examples they may be part of a dedicated neural-network accelerator.</p><p id="p-0085" num="0070">NPUs may be optimized for training or inference, or in some cases configured to balance performance between both. For NPUs that are capable of performing both training and inference, the two tasks may still generally be performed independently.</p><p id="p-0086" num="0071">NPUs designed to accelerate training are generally configured to accelerate the optimization of new models, which is a highly compute-intensive operation that involves inputting an existing dataset (often labeled or tagged), iterating over the dataset, and then adjusting model parameters, such as weights and biases, in order to improve model performance. Generally, optimizing based on a wrong prediction involves propagating back through the layers of the model and determining gradients to reduce the prediction error.</p><p id="p-0087" num="0072">NPUs designed to accelerate inference are generally configured to operate on complete models. Such NPUs may thus be configured to input a new piece of data and rapidly process it through an already trained model to generate a model output (e.g., an inference).</p><p id="p-0088" num="0073">In one implementation, NPU <b>708</b> is a part of one or more of CPU <b>702</b>, GPU <b>704</b>, and/or DSP <b>706</b>.</p><p id="p-0089" num="0074">Processing system <b>700</b> may also include one or more input and/or output devices <b>722</b>, such as screens, touch-sensitive surfaces (including touch-sensitive displays), physical buttons, speakers, microphones, and the like.</p><p id="p-0090" num="0075">In some examples, one or more of the processors of processing system <b>700</b> may be based on an ARM or RISC-V instruction set.</p><p id="p-0091" num="0076">Processing system <b>700</b> also includes memory <b>724</b>, which is representative of one or more static and/or dynamic memories, such as a dynamic random access memory, a flash-based static memory, and the like. In this example, memory <b>724</b> includes computer-executable components, which may be executed by one or more of the aforementioned processors of processing system <b>700</b>.</p><p id="p-0092" num="0077">In particular, in this example, memory <b>724</b> includes neural network training component <b>724</b>A, cluster-validation set performance metric generator component <b>724</b>B, classification cluster selecting component <b>724</b>C, neural network retraining component <b>724</b>D, and neural network deploying component <b>724</b>E. The depicted components, and others not depicted, may be configured to perform various aspects of the methods described herein.</p><p id="p-0093" num="0078">Generally, processing system <b>700</b> and/or components thereof may be configured to perform the methods described herein. Notably, aspects of processing system <b>700</b> may be distributed.</p><p id="p-0094" num="0079"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts an example processing system <b>800</b> for classifying data using a multi-stage neural network trained using supervised learning techniques, such as described herein for example with respect to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0095" num="0080">Processing system <b>800</b> includes a central processing unit (CPU) <b>802</b>, which in some examples may be a multi-core CPU. Instructions executed at the CPU <b>802</b> may be loaded, for example, from a program memory associated with the CPU <b>802</b> or may be loaded from a memory <b>824</b> or memory partition.</p><p id="p-0096" num="0081">Processing system <b>800</b> also includes additional processing components tailored to specific functions, such as a graphics processing unit (GPU) <b>804</b>, a digital signal processor (DSP) <b>806</b>, a neural processing unit (NPU) <b>808</b>, a multimedia processing unit <b>810</b>, a multimedia processing unit <b>810</b>, and a wireless connectivity component <b>812</b>.</p><p id="p-0097" num="0082">An NPU, such as <b>808</b>, is generally a specialized circuit configured for implementing all the necessary control and arithmetic logic for executing machine learning algorithms, such as algorithms for processing artificial neural networks (ANNs), deep neural networks (DNNs), random forests (RFs), and the like. An NPU may sometimes alternatively be referred to as a neural signal processor (NSP), tensor processing units (TPU), neural network processor (NNP), intelligence processing unit (IPU), vision processing unit (VPU), or graph processing unit.</p><p id="p-0098" num="0083">NPUs, such as <b>808</b>, may be configured similarly to NPU <b>708</b> described above with respect to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. In one implementation, NPU <b>808</b> is a part of one or more of CPU <b>802</b>, GPU <b>804</b>, and/or DSP <b>806</b>.</p><p id="p-0099" num="0084">In some examples, wireless connectivity component <b>812</b> may include subcomponents, for example, for third generation (3G) connectivity, fourth generation (4G) connectivity (e.g., 4G LTE), fifth generation connectivity (e.g., 5G or NR), Wi-Fi connectivity, Bluetooth connectivity, and other wireless data transmission standards. Wireless connectivity component <b>812</b> is further connected to one or more antennas <b>814</b>.</p><p id="p-0100" num="0085">Processing system <b>800</b> may also include one or more sensor processing units <b>816</b> associated with any manner of sensor, one or more image signal processors (ISPs) <b>818</b> associated with any manner of image sensor, and/or a navigation processor <b>820</b>, which may include satellite-based positioning system components (e.g., GPS or GLONASS) as well as inertial positioning system components.</p><p id="p-0101" num="0086">Processing system <b>800</b> may also include one or more input and/or output devices <b>822</b>, such as screens, touch-sensitive surfaces (including touch-sensitive displays), physical buttons, speakers, microphones, and the like.</p><p id="p-0102" num="0087">In some examples, one or more of the processors of processing system <b>800</b> may be based on an ARM or RISC-V instruction set.</p><p id="p-0103" num="0088">Processing system <b>800</b> also includes memory <b>824</b>, which is representative of one or more static and/or dynamic memories, such as a dynamic random access memory, a flash-based static memory, and the like. In this example, memory <b>824</b> includes computer-executable components, which may be executed by one or more of the aforementioned processors of processing system <b>800</b>.</p><p id="p-0104" num="0089">In particular, in this example, memory <b>824</b> includes input receiving component <b>824</b>A, input classifying component <b>824</b>B, and action taking component <b>824</b>C. The depicted components, and others not depicted, may be configured to perform various aspects of the methods described herein.</p><p id="p-0105" num="0090">Generally, processing system <b>800</b> and/or components thereof may be configured to perform the methods described herein.</p><p id="p-0106" num="0091">Notably, in other embodiments, aspects of processing system <b>800</b> may be omitted, such as where processing system <b>800</b> is a server computer or the like. For example, multimedia processing unit <b>810</b>, wireless connectivity component <b>812</b>, sensor processing units <b>816</b>, ISPs <b>818</b>, and/or navigation processor <b>820</b> may be omitted in other embodiments. Further, aspects of processing system <b>800</b> may be distributed, such as training a model and using the model to generate inferences.</p><heading id="h-0012" level="1">Example Clauses</heading><p id="p-0107" num="0092">Implementation details of various aspects of the present disclosure are described in the following numbered clauses.</p><p id="p-0108" num="0093">Clause 1: A method, comprising: training a neural network with a plurality of stages using a training data set and an initial number of classification cluster into which data in the training data set can be classified; generating a cluster-validation set performance metric for each stage of the plurality of stages of the neural network based on a reduced number of classification clusters relative to the initial number of classification clusters and a validation data set separate from the training data set; selecting a number of classification clusters to implement at each stage of the plurality of stages of the neural network based on the cluster-validation set performance metric and an angle selected relative to the cluster-validation set performance metric for a last stage of the neural network; retraining the neural network based on the training data set and the selected number of classification clusters for each stage of the plurality of stages; and deploying the trained neural network.</p><p id="p-0109" num="0094">Clause 2: The method of Clause 1, further comprising, for each stage of the plurality of stages: calculating a confusion matrix for the training data set and a confusion matrix for the validation data set, wherein discrete elements in one dimension of the confusion matrices represent one of a plurality of classification clusters; calculating an adjacency matrix based on the confusion matrix calculated for the training data set and the confusion matrix calculated for the validation data set; and generating the reduced number of classification clusters using agglomerative clustering of neighboring clusters in the calculated adjacency matrix such that a plurality of neighboring clusters are reduced into a single cluster representing a broader classification of data than each of the plurality of neighboring clusters.</p><p id="p-0110" num="0095">Clause 3: The method of any one of Clauses 1 or 2, wherein generating the cluster-validation set performance metric comprises calculating a performance metric for each stage in the plurality of stages for cluster sizes up to and including the initial number of classification clusters.</p><p id="p-0111" num="0096">Clause 4: The method of Clause 3, wherein the performance metric comprises a mean intersection over union (mIoU) metric calculated as a function of a number of clusters in each stage of the plurality of stages in the neural network.</p><p id="p-0112" num="0097">Clause 5: The method of any one of Clauses 1 through 4, wherein: the selected angle comprises a zero degree angle, and training the neural network based on the training data set and the selected number of classification clusters at each stage comprises training the plurality of stages in the neural network using direct supervision.</p><p id="p-0113" num="0098">Clause 6: The method of any one of Clauses 1 through 4, wherein: the selected angle comprises a ninety degree angle, and training the neural network based on the training data set and the selected number of classification clusters at each stage comprises training the plurality of stages in the neural network such that performance of each stage of the neural network converges to a performance level within a threshold value.</p><p id="p-0114" num="0099">Clause 7: The method of any one of Clauses 1 through 6, wherein retraining the neural network based on the training data set and the selected number of classification clusters at each stage comprises minimizing a total loss function, wherein: the total loss function comprises a sum of a loss function for each respective stage of the plurality of stages weighted by a value associated with each respective stage of the plurality of stages, and the loss function for the respective stage of the plurality of stages is based on a number of classification clusters selected for the respective stage.</p><p id="p-0115" num="0100">Clause 8: The method of any one of Clauses 1 through 7, wherein retraining the neural network based on the training data set and the selected number of classification clusters at each stage comprises: aggregating an output of each stage of the plurality of stages other than a final stage of the neural network; and training the final stage of the neural network based on an input of the aggregated output of the plurality of stages other than the final stage of the neural network into a segmentation transformer module associated with the final stage of the neural network.</p><p id="p-0116" num="0101">Clause 9: A method, comprising: receiving an input for classification; classifying the input using a neural network having a plurality of stages, wherein each stage of the plurality of stages classifies the input using a different number of classification clusters; and taking one or more actions based on the classification of the input.</p><p id="p-0117" num="0102">Clause 10: The method of Clause 9, wherein classifying the output comprises classifying the input at a stage of the plurality of stages based on an inference generated by a prior stage of the plurality of stages.</p><p id="p-0118" num="0103">Clause 11: The method of any one of Clauses 9 or 10, wherein: the neural network comprises a neural network including segmentation transformers at each stage of the neural network, output of each stage of the neural network other than the final stage of the neural network is aggregated, and the aggregated output is input into a segmentation transformer associated with a final stage of the neural network to generate the classification of the input.</p><p id="p-0119" num="0104">Clause 12: The method of any one of Clauses 9 through 11, wherein each stage of the plurality of stages classifies the input using a larger number of classification clusters than a preceding stage of the plurality of stages.</p><p id="p-0120" num="0105">Clause 13: An apparatus, comprising: a memory having executable instructions stored thereon; and a processor configured to execute the executable instructions to cause the apparatus to perform a method in accordance with of any one of Clauses 1 through 12.</p><p id="p-0121" num="0106">Clause 14: An apparatus, comprising: means for performing a method in accordance with of any one of Clauses 1 through 12.</p><p id="p-0122" num="0107">Clause 15: A non-transitory computer-readable medium having instructions stored thereon which, when executed by a processor, performs a method in accordance with of any one of Clauses 1 through 12.</p><p id="p-0123" num="0108">Clause 16: A computer program product embodied on a computer-readable storage medium comprising code for performing a method in accordance with of any one of Clauses 1 through 12.</p><heading id="h-0013" level="1">ADDITIONAL CONSIDERATIONS</heading><p id="p-0124" num="0109">The preceding description is provided to enable any person skilled in the art to practice the various embodiments described herein. The examples discussed herein are not limiting of the scope, applicability, or embodiments set forth in the claims. Various modifications to these embodiments will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other embodiments. For example, changes may be made in the function and arrangement of elements discussed without departing from the scope of the disclosure. Various examples may omit, substitute, or add various procedures or components as appropriate. For instance, the methods described may be performed in an order different from that described, and various steps may be added, omitted, or combined. Also, features described with respect to some examples may be combined in some other examples. For example, an apparatus may be implemented or a method may be practiced using any number of the aspects set forth herein. In addition, the scope of the disclosure is intended to cover such an apparatus or method that is practiced using other structure, functionality, or structure and functionality in addition to, or other than, the various aspects of the disclosure set forth herein. It should be understood that any aspect of the disclosure disclosed herein may be embodied by one or more elements of a claim.</p><p id="p-0125" num="0110">As used herein, the word &#x201c;exemplary&#x201d; means &#x201c;serving as an example, instance, or illustration.&#x201d; Any aspect described herein as &#x201c;exemplary&#x201d; is not necessarily to be construed as preferred or advantageous over other aspects.</p><p id="p-0126" num="0111">As used herein, a phrase referring to &#x201c;at least one of&#x201d; a list of items refers to any combination of those items, including single members. As an example, &#x201c;at least one of: a, b, or c&#x201d; is intended to cover a, b, c, a-b, a-c, b-c, and a-b-c, as well as any combination with multiples of the same element (e.g., a-a, a-a-a, a-a-b, a-a-c, a-b-b, a-c-c, b-b, b-b-b, b-b-c, c-c, and c-c-c or any other ordering of a, b, and c).</p><p id="p-0127" num="0112">As used herein, the term &#x201c;determining&#x201d; encompasses a wide variety of actions. For example, &#x201c;determining&#x201d; may include calculating, computing, processing, deriving, investigating, looking up (e.g., looking up in a table, a database or another data structure), ascertaining and the like. Also, &#x201c;determining&#x201d; may include receiving (e.g., receiving information), accessing (e.g., accessing data in a memory) and the like. Also, &#x201c;determining&#x201d; may include resolving, selecting, choosing, establishing and the like.</p><p id="p-0128" num="0113">The methods disclosed herein comprise one or more steps or actions for achieving the methods. The method steps and/or actions may be interchanged with one another without departing from the scope of the claims. In other words, unless a specific order of steps or actions is specified, the order and/or use of specific steps and/or actions may be modified without departing from the scope of the claims. Further, the various operations of methods described above may be performed by any suitable means capable of performing the corresponding functions. The means may include various hardware and/or software component(s) and/or module(s), including, but not limited to a circuit, an application specific integrated circuit (ASIC), or processor. Generally, where there are operations illustrated in figures, those operations may have corresponding counterpart means-plus-function components with similar numbering.</p><p id="p-0129" num="0114">The following claims are not intended to be limited to the embodiments shown herein, but are to be accorded the full scope consistent with the language of the claims. Within a claim, reference to an element in the singular is not intended to mean &#x201c;one and only one&#x201d; unless specifically so stated, but rather &#x201c;one or more.&#x201d; Unless specifically stated otherwise, the term &#x201c;some&#x201d; refers to one or more. No claim element is to be construed under the provisions of 35 U.S.C. &#xa7; 112(f) unless the element is expressly recited using the phrase &#x201c;means for&#x201d; or, in the case of a method claim, the element is recited using the phrase &#x201c;step for.&#x201d; All structural and functional equivalents to the elements of the various aspects described throughout this disclosure that are known or later come to be known to those of ordinary skill in the art are expressly incorporated herein by reference and are intended to be encompassed by the claims. Moreover, nothing disclosed herein is intended to be dedicated to the public regardless of whether such disclosure is explicitly recited in the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004812A1-20230105-M00001.NB"><img id="EMI-M00001" he="7.03mm" wi="76.20mm" file="US20230004812A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004812A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US20230004812A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230004812A1-20230105-M00003.NB"><img id="EMI-M00003" he="7.79mm" wi="76.20mm" file="US20230004812A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230004812A1-20230105-M00004.NB"><img id="EMI-M00004" he="8.81mm" wi="76.20mm" file="US20230004812A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230004812A1-20230105-M00005.NB"><img id="EMI-M00005" he="8.13mm" wi="76.20mm" file="US20230004812A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230004812A1-20230105-M00006.NB"><img id="EMI-M00006" he="5.67mm" wi="76.20mm" file="US20230004812A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230004812A1-20230105-M00007.NB"><img id="EMI-M00007" he="5.67mm" wi="76.20mm" file="US20230004812A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230004812A1-20230105-M00008.NB"><img id="EMI-M00008" he="5.67mm" wi="76.20mm" file="US20230004812A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method for generating an inference using a machine learning model, comprising:<claim-text>receiving an input for classification;</claim-text><claim-text>classifying the input using a neural network having a plurality of stages, wherein each stage of the plurality of stages classifies the input using a different number of classification clusters; and</claim-text><claim-text>taking one or more actions based on the classification of the input.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein classifying the output comprises classifying the input at a stage of the plurality of stages based on an inference generated by a prior stage of the plurality of stages.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the neural network comprises a neural network including segmentation transformers at each stage of the neural network,</claim-text><claim-text>output of each stage of the neural network other than a final stage of the neural network is aggregated, and</claim-text><claim-text>the aggregated output is input into a segmentation transformer associated with the final stage of the neural network to generate the classification of the input.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each stage of the plurality of stages classifies the input using a larger number of classification clusters than a preceding stage of the plurality of stages.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. A computer-implemented method for training a machine learning model, comprising:<claim-text>training a neural network with a plurality of stages using a training data set and an initial number of classification cluster into which data in the training data set can be classified;</claim-text><claim-text>generating a cluster-validation set performance metric for each stage of the plurality of stages of the neural network based on a reduced number of classification clusters relative to the initial number of classification clusters and a validation data set separate from the training data set;</claim-text><claim-text>selecting a number of classification clusters to implement at each stage of the plurality of stages of the neural network based on the cluster-validation set performance metric and an angle selected relative to the cluster-validation set performance metric for a last stage of the neural network;</claim-text><claim-text>retraining the neural network based on the training data set and the selected number of classification clusters for each stage of the plurality of stages; and</claim-text><claim-text>deploying the trained neural network.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising, for each stage of the plurality of stages:<claim-text>calculating a confusion matrix for the training data set and a confusion matrix for the validation data set, wherein discrete elements in one dimension of the confusion matrices represent one of a plurality of classification clusters;</claim-text><claim-text>calculating an adjacency matrix based on the confusion matrix calculated for the training data set and the confusion matrix calculated for the validation data set; and</claim-text><claim-text>generating the reduced number of classification clusters using agglomerative clustering of neighboring clusters in the calculated adjacency matrix such that a plurality of neighboring clusters are reduced into a single cluster representing a broader classification of data than each of the plurality of neighboring clusters.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein generating the cluster-validation set performance metric comprises calculating a performance metric for each stage in the plurality of stages for cluster sizes up to and including the initial number of classification clusters.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the performance metric comprises a mean intersection over union (mIoU) metric calculated as a function of a number of clusters in each stage of the plurality of stages in the neural network.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein:<claim-text>the selected angle comprises a zero degree angle, and</claim-text><claim-text>training the neural network based on the training data set and the selected number of classification clusters at each stage comprises training the plurality of stages in the neural network using direct supervision.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein:<claim-text>the selected angle comprises a ninety degree angle, and</claim-text><claim-text>training the neural network based on the training data set and the selected number of classification clusters at each stage comprises training the plurality of stages in the neural network such that performance of each stage of the neural network converges to a performance level within a threshold value.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein retraining the neural network based on the training data set and the selected number of classification clusters at each stage comprises minimizing a total loss function, wherein:<claim-text>the total loss function comprises a sum of a loss function for each respective stage of the plurality of stages weighted by a value associated with each respective stage of the plurality of stages, and</claim-text><claim-text>the loss function for the respective stage of the plurality of stages is based on a number of classification clusters selected for the respective stage.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein retraining the neural network based on the training data set and the selected number of classification clusters at each stage comprises:<claim-text>aggregating an output of each stage of the plurality of stages other than a final stage of the neural network; and</claim-text><claim-text>training the final stage of the neural network based on an input of the aggregated output of the plurality of stages other than the final stage of the neural network into a segmentation transformer module associated with the final stage of the neural network.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A processing system, comprising:<claim-text>a memory having computer-executable instructions stored thereon; and</claim-text><claim-text>a processor configured to execute the computer-executable instructions to cause the processing system to:<claim-text>receive an input for classification;</claim-text><claim-text>classify the input using a neural network having a plurality of stages, wherein each stage of the plurality of stages classifies the input using a different number of classification clusters; and</claim-text><claim-text>take one or more actions based on the classification of the input.</claim-text></claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The processing system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein in order to classify the output, the processor is configured to cause the processing system to classify the input at a stage of the plurality of stages based on an inference generated by a prior stage of the plurality of stages.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The processing system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein:<claim-text>the neural network comprises a neural network including segmentation transformers at each stage of the neural network,</claim-text><claim-text>output of each stage of the neural network other than a final stage of the neural network is aggregated, and</claim-text><claim-text>the aggregated output is input into a segmentation transformer associated with the final stage of the neural network to generate the classification of the input.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The processing system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein each stage of the plurality of stages classifies the input using a larger number of classification clusters than a preceding stage of the plurality of stages.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A processing system, comprising:<claim-text>a memory having computer-executable instructions stored thereon; and</claim-text><claim-text>a processor configured to execute the computer-executable instructions to cause the processing system to:<claim-text>train a neural network with a plurality of stages using a training data set and an initial number of classification cluster into which data in the training data set can be classified;</claim-text><claim-text>generate a cluster-validation set performance metric for each stage of the plurality of stages of the neural network based on a reduced number of classification clusters relative to the initial number of classification clusters and a validation data set separate from the training data set;</claim-text><claim-text>select a number of classification clusters to implement at each stage of the plurality of stages of the neural network based on the cluster-validation set performance metric and an angle selected relative to the cluster-validation set performance metric for a last stage of the neural network;</claim-text><claim-text>retrain the neural network based on the training data set and the selected number of classification clusters for each stage of the plurality of stages; and</claim-text><claim-text>deploy the trained neural network.</claim-text></claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The processing system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the processor is further configured to cause the processing system to:<claim-text>calculate a confusion matrix for the training data set and a confusion matrix for the validation data set, wherein discrete elements in one dimension of the confusion matrices represent one of a plurality of classification clusters;</claim-text><claim-text>calculate an adjacency matrix based on the confusion matrix calculated for the training data set and the confusion matrix calculated for the validation data set; and</claim-text><claim-text>generate the reduced number of classification clusters using agglomerative clustering of neighboring clusters in the calculated adjacency matrix such that a plurality of neighboring clusters are reduced into a single cluster representing a broader classification of data than each of the plurality of neighboring clusters.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The processing system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein in order to generate the cluster-validation set performance metric, the processor is configured to cause the processing system to calculate a performance metric for each stage in the plurality of stages for cluster sizes up to and including the initial number of classification clusters.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The processing system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the performance metric comprises a mean intersection over union (mIoU) metric calculated as a function of a number of clusters in each stage of the plurality of stages in the neural network.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The processing system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the selected angle comprises a zero degree angle, and</claim-text><claim-text>in order to train the neural network based on the training data set and the selected number of classification clusters at each stage, the processor is configured to cause the processing system to train the plurality of stages in the neural network using direct supervision.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The processing system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the selected angle comprises a ninety degree angle, and</claim-text><claim-text>in order to train the neural network based on the training data set and the selected number of classification clusters at each stage, the processor is configured to cause the processing system to train the plurality of stages in the neural network such that performance of each stage of the neural network converges to a performance level within a threshold value.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The processing system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein in order to retrain the neural network based on the training data set and the selected number of classification clusters at each stage, the processor is configured to cause the processing system to minimize a total loss function, wherein:<claim-text>the total loss function comprises a sum of a loss function for each respective stage of the plurality of stages weighted by a value associated with each respective stage of the plurality of stages, and</claim-text><claim-text>the loss function for the respective stage of the plurality of stages is based on a number of classification clusters selected for the respective stage.</claim-text></claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The processing system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein in order to retrain the neural network based on the training data set and the selected number of classification clusters at each stage, the processor is configured to cause the processing system to:<claim-text>aggregate an output of each stage of the plurality of stages other than a final stage of the neural network; and</claim-text><claim-text>train the final stage of the neural network based on an input of the aggregated output of the plurality of stages other than the final stage of the neural network into a segmentation transformer module associated with the final stage of the neural network.</claim-text></claim-text></claim></claims></us-patent-application>