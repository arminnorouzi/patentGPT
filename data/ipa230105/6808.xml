<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230006809A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230006809</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940847</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>9</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>9</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>9</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>9</main-group><subgroup>32</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>9</main-group><subgroup>008</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>9</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>9</main-group><subgroup>0618</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>602</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>63</main-group><subgroup>0428</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>9</main-group><subgroup>3218</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">HOMOMORPHIC COMPUTATIONS ON ENCRYPTED DATA WITHIN A DISTRIBUTED COMPUTING ENVIRONMENT</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16751792</doc-number><date>20200124</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11469878</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17940847</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62797665</doc-number><date>20190128</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>The Toronto-Dominion Bank</orgname><address><city>Toronto</city><country>CA</country></address></addressbook><residence><country>CA</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SHPUROV</last-name><first-name>Alexey</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>HODGE</last-name><first-name>Lovell</first-name><address><city>Burlington</city><country>CA</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>LAM</last-name><first-name>Brian Andrew</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>THOMAS</last-name><first-name>Leslie Carol</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The disclosed exemplary embodiments include computer-implemented systems, apparatuses, and processes that perform homomorphic computations on encrypted third-party data within a distributed computing environment. For example, an apparatus receives a homomorphic public key and encrypted transaction data characterizing an exchange of data from a computing system, and encrypts modelling data associated with a first predictive model using the homomorphic public key. The apparatus may perform homomorphic computations that apply the first predictive model to the encrypted transaction data in accordance with the encrypted first modelling data, and transmit an encrypted first output of the homomorphic computations to the computing system, which may decrypt the encrypted first output using a homomorphic private key and generate decrypted output data indicative of a predicted likelihood that the data exchange represents fraudulent activity.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="107.36mm" wi="158.50mm" file="US20230006809A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="228.35mm" wi="155.11mm" orientation="landscape" file="US20230006809A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="220.22mm" wi="167.47mm" orientation="landscape" file="US20230006809A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="212.17mm" wi="159.51mm" orientation="landscape" file="US20230006809A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="227.67mm" wi="166.45mm" orientation="landscape" file="US20230006809A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="230.38mm" wi="159.51mm" orientation="landscape" file="US20230006809A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="181.61mm" wi="112.18mm" file="US20230006809A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims the benefit of priority to U.S. Provisional Application No. 62/797,665, filed Jan. 28, 2019, the disclosure of which is expressly incorporated by reference herein to its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The disclosed embodiments generally relate to computer-implemented systems and processes that, among other things, perform homomorphic computations on encrypted confidential data within a distributed computing environment.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Today, many institutions maintain confidential data characterizing various interactions with their customers. The confidential data may, for example, include elements of transaction data that characterize transactions involving one or more accounts held by the customers and identify instances of fraudulent activity associated with certain ones of these transactions. Given the scope of the maintained data, these institutions can develop and train predictive fraud models that, when applied to selected elements of input data, generate output data characterizing a likelihood that a particular transaction represents fraudulent activity.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">In some examples, an apparatus includes a communications interface, a memory storing instructions, and at least one processor coupled to the communications interface and the memory. The at least one processor is configured to execute the instructions to receive, via the communications interface, a first signal from a computing system that includes a homomorphic public key and encrypted transaction data characterizing an exchange of data, and to encrypt first modelling data associated with a first predictive model using the homomorphic public key. The at least one processor is further configured to perform homomorphic computations on the encrypted transaction data, and the homomorphic computations apply the first predictive model to the encrypted transaction data in accordance with the encrypted first modelling data. The at least one processor is further configured to generate and transmit, via the communications interface, a second signal to the computing system that includes an encrypted first output of the homomorphic computations. The computing system is configured to decrypt the encrypted first output using a homomorphic private key and generate decrypted output data indicative of a predicted likelihood that the data exchange represents fraudulent activity.</p><p id="p-0006" num="0005">In other examples, a computer-implemented method includes receiving, by at least one processor, a first signal from a computing system that includes a homomorphic public key and encrypted transaction data characterizing an exchange of data and by the at least one processor, encrypting first modelling data associated with a first predictive model using the homomorphic public key. The computer-implemented method also includes, by the at least one processor, performing homomorphic computations on the encrypted transaction data. The homomorphic computations apply the first predictive model to the encrypted transaction data in accordance with the encrypted first modelling data. The computer-implemented method also includes generating and transmitting, by the at least one processor, a second signal to the computing system that includes an encrypted first output of the homomorphic computations. The computing system is configured to decrypt the encrypted first output using a homomorphic private key and generate decrypted output data indicative of a predicted likelihood that the data exchange represents fraudulent activity.</p><p id="p-0007" num="0006">Additionally, in some instances, a tangible, non-transitory computer-readable medium stores instructions that, when executed by at least one processor, cause the at least one processor to perform a method that includes receiving a first signal from a computing system that includes a homomorphic public key and encrypted transaction data characterizing an exchange of data. The method also includes encrypting first modelling data associated with a first predictive model using the homomorphic public key, and performing homomorphic computations on the encrypted transaction data. The homomorphic computations apply the first predictive model to the encrypted transaction data in accordance with the encrypted first modelling data. The method also includes generating and transmitting a second signal to the computing system that includes an encrypted first output of the homomorphic computations. The computing system is configured to decrypt the encrypted first output using a homomorphic private key and generate decrypted output data indicative of a predicted likelihood that the data exchange represents fraudulent activity.</p><p id="p-0008" num="0007">It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory only and are not restrictive of the invention, as claimed. Further, the accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate aspects of the present disclosure and together with the description, serve to explain principles of the disclosed embodiments as set forth in the accompanying claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>A, <b>2</b>B, <b>3</b>A, and <b>3</b>B</figref> are diagrams illustrating portions of an exemplary computing environment, consistent with disclosed embodiments.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of an exemplary process for applying a privately trained predictive model to encrypted third-party data using verifiable homomorphic computations, consistent with the disclosed embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0011" num="0010">Many institutions, organizations, and business capture and maintain confidential data characterizing interactions with various customers, stakeholders, or interested parties. By way of example, a financial institution may issue payment instruments, such as credit card accounts or debit card accounts, to various customers, and computing systems operated by these financial institutions may receive and maintain elements of confidential transaction data that establish a time-evolving record of transactions involving these payment instruments. For instance, the elements of confidential transaction data may specify, for a corresponding one of the transactions, not only values of certain transaction parameters (e.g., a transaction value or a transaction time) but also counterparty data that identifies one or more counterparties and positional information characterizing a geographical position associated with the corresponding transaction (e.g., a geographic position of an initiating device, a point-of-sale device, etc.).</p><p id="p-0012" num="0011">Further, the elements of confidential transaction data may also identify instances of fraudulent activity involving corresponding ones of the transactions. For example, the instances of fraudulent activity include an unauthorized use of a payment instrument in a corresponding purchase transaction (e.g., through a loss or theft of a physical payment card, etc.), and the elements of transaction data may correlate each instance of fraudulent activity with a corresponding one of the payment instruments and with the parameter values, counterparty data, and positional data that characterize the corresponding purchase transaction. In other examples, the transaction data elements may also include information that associates a particular counterparty to one or more of the transactions with fraudulent activity (e.g., that associates a particular physical retailer with reported instances of fraudulent activity), or that associates a particular geographic region with a heightened risk of fraudulent activity.</p><p id="p-0013" num="0012">In view of the confidential transaction data available to the financial institutions, the computing systems associated with one or more these financial institutions may leverage portions of the maintained transaction data to adaptively train and improve predictive fraud models that, when applied to selected elements of structured input transaction data, generate output data characterizing a likelihood that a particular transaction or exchange of data represents an instance of fraudulent activity. Examples of these predictive fraud models may include, but are not limited to, or more regression models characterized by derived model coefficients, tuned parameters, and/or threshold values (such as, but not limited to a LASSO model or a ridge regression), one or more artificial neural network models, and one or more machine learning models (such as, but not limited to, a logistic regression model or certain decision tree models. Further, the output of these predictive fraud models may be binary, e.g., unity or zero, or a scaled value compared against a specified threshold value, e.g., selected to distinguish between fraudulent or acceptable activities.</p><p id="p-0014" num="0013">In some instances, each of the financial institutions are subject to certain restrictions imposed by governmental entities, regulatory entities, or industry groups, which mandate these financial institutions not only maintain the security and confidentiality of the maintained transaction data, but also obtain customer consent to a distribution of the confidential transaction data, either within the financial institutions or to unrelated, third-party financial institutions or organizations. Additionally, as the exemplary predictive fraud models described herein are trained against, and adaptively improved using, elements of the confidential transaction data, and as the model coefficients, parameters, and thresholds are derived on the basis of selected portions of the confidential transaction data, certain of the imposed restrictions may also mandate that the financial institutions maintain a security and confidentiality of the derived model coefficients, parameters, and thresholds.</p><p id="p-0015" num="0014">For example, an unauthorized (or accidental) disclosure of the derived model coefficients, parameters, and/or thresholds by one or more of the financial institutions could enable an unrelated third party, such as another financial institution, a malicious third party, etc., to back-compute the values of the model coefficients, parameters, and thresholds, and portions of the confidential transaction data, based on repetitive computation. Thus, and in additional to maintaining a strict confidentiality of the transaction data elements, each of these financial institutions, and corresponding ones of the computing systems operated by the financial institutions, may implement policies that maintain a confidentiality of the derived model coefficients, parameters, and thresholds, and that exclude any application of these adaptively trained models to third-party transaction data collected by unrelated financial institutions.</p><p id="p-0016" num="0015">In other instances, however, one or more of the financial institutions may benefit from an ability to access a predictive fraud model privately trained and by a computing system operated by another, unrelated financial institution, and to apply the privately trained predictive fraud models to locally accessible elements of confidential transaction data, e.g., to characterize a likelihood that one or more transactions represent fraudulent or illicit activity. For example, the computing systems operated by one or more of these financial institutions may privately train, and adaptively improve, a predictive fraud model based on sets of training data that characterize transactions involving groups of demographically or geographically different customers. When applied to input transaction data associated with a pending or initiated transaction, each of these predictive fraud models may generate a distinct element of output data representative of a likelihood that the pending or initiated transaction is associated with fraudulent or illicit activity, e.g., in accordance with the transaction behavior of a corresponding group of the demographically or geographically different customers</p><p id="p-0017" num="0016">Certain of the exemplary processes described herein, which implement predictive fraud models in conjunction with homomorphic encryption schemes that facilitate verifiable computations on encrypted data, enable a computing system associated with a financial institution to apply a privately trained predictive fraud model to encrypted elements of confidential, third-party transaction data. By way of example, and as described herein, the computing system may receive a third-party public key and the third-party transaction data encrypted using a corresponding third-party private key, e.g., from a third-party computing system. The computing system may encrypt the model coefficients, parameters, or thresholds using the third-party public key, and based on an application of the predictive fraud model to the encrypted third-party transaction data using verifiable homomorphic computations, may generate encrypted output data for transmission to the third-party computing system, which may decrypt the encrypted output data (e.g., using the corresponding private key) and generate output data indicative of the likelihood of fraudulent activity.</p><p id="p-0018" num="0017">In some exemplary embodiments, described herein, the homomorphic computations involving the encrypted third-party transaction data maintain the confidentiality of the privately trained model, as the underlying model coefficients, parameters, or thresholds are encrypted using the third-party public key. Further, certain of these exemplary processes, as described herein, allow the third-party financial institution to maintain the confidentiality of not only the third-party transaction data provided to the computing system, but also the output generated by the predictive fraud model, e.g., as the input and output data are encrypted using the third-party public cryptographic key. Moreover, by generating not only encrypted output data based on the application of the predictive fraud model (and the encrypted model coefficients, parameters, or models) to the encrypted input data, but also a zero-knowledge proof of the encrypted output data, the third-party computing system may perform operations that independently verify an accuracy of the computation based on the underlying input data and the zero-knowledge proof.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates components of an exemplary computing environment <b>100</b>, which perform computerized processes that establish one or more predictive fraud models capable of performing verifiable, homomorphic computations on encrypted input data, and that privately train one or more established predictive fraud models using locally accessible elements of confidential data, in accordance with some exemplary implementations. For example, and referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, environment <b>100</b> includes a first computing system <b>102</b> and one or more additional computing systems <b>200</b>, including a second computing system <b>202</b> and a third computing system <b>302</b>, each of which may be interconnected through one or more communications networks, such as communications network <b>120</b>. Examples of communications network <b>120</b> include, but are not limited to, a wireless local area network (LAN), e.g., a &#x201c;Wi-Fi&#x201d; network, a network utilizing radio-frequency (RF) communication protocols, a Near Field Communication (NFC) network, a wireless Metropolitan Area Network (MAN) connecting multiple wireless LANs, and a wide area network (WAN), e.g., the Internet.</p><p id="p-0020" num="0019">As described herein, each of first computing system <b>102</b> and additional computing systems <b>200</b>, including second computing system <b>202</b> and third computing system <b>302</b>, may correspond to a computing system that includes one or more servers and tangible, non-transitory memory devices storing executable code and application modules. The one or more servers may each include one or more processors, which may be configured to execute portions of the stored code or application modules to perform operations consistent with the disclosed embodiments. Further, in some instances, first computing system <b>102</b> or one or more of additional computing systems <b>200</b> (including second computing system <b>202</b> and third computing system <b>302</b>) can be incorporated into a single computing system, although in other instances, one or more of first computing system <b>102</b> or additional computing systems <b>200</b> (including second computing system <b>202</b> and third computing system <b>302</b>) can correspond to a distributed system that includes computing components distributed across communications network <b>120</b>, such as those described herein, or those provided or maintained by cloud-service providers (e.g., Google Cloud&#x2122;, Microsoft Azure&#x2122;, etc.). The disclosed embodiments are, however, not limited to these exemplary distributed systems, and in other instances, first computing system <b>102</b> and additional computing systems <b>200</b>, including second computing system <b>202</b> and third computing system <b>302</b>, may include computing components disposed within any additional or alternate number or type of computing systems or across any appropriate network.</p><p id="p-0021" num="0020">In some instances, each of first computing system <b>102</b> and second computing system <b>202</b> may be associated with, or operated by, a financial institution or other business entity that provides financial services to one or more customers (e.g., respective ones of a first financial institution and a second financial institution). Further, one or more of additional computing systems <b>200</b> may also be associated with, or operated by, an additional financial institution that provides financial services to one or more additional customers. Examples of the provisioned financial services may include, but are not limited to, establishing and maintaining financial services accounts on behalf of corresponding customers (e.g., a deposit account, a brokerage account, a credit card account or a revolving line of credit, etc.) and/or initiating payment transactions involving corresponding ones of the financial services accounts and counterparties.</p><p id="p-0022" num="0021">Further, third computing system <b>302</b> may be associated with, or operated by, a centralized authority, such as, but not limited to, an industry group, industry consortium, or other provider of financial services having affiliates or members. In some instances, as described herein, the centralized authority, and third computing system <b>302</b>, may be trusted by the first financial institution, the second financial institution, and additional financial institutions to apply privately trained predictive fraud models to encrypted transaction data based on locally maintained model coefficients, model parameters, and threshold values. In other instances, third computing system <b>302</b> may be associated with a third financial institution that provides any of the exemplary financial services described herein to corresponding customers.</p><p id="p-0023" num="0022">To facilitate a performance of any of the exemplary processes described herein, first computing system <b>102</b> may establish and maintain, within the one or more tangible, non-tangible memories, one or more structured or unstructured data repositories or databases, such as data repository <b>104</b>. By way of example, data repository <b>104</b> may include, but is not limited to, a transaction database <b>106</b>, a cryptographic library <b>108</b>, and a trained model data store <b>110</b>.</p><p id="p-0024" num="0023">Transaction database <b>106</b> may include data records that identify and characterize one or more exchanges of data, e.g., transactions involving corresponding payment instrument, initiated by, or on behalf of, one or more customers of the first financial institution during prior temporal intervals. For example, and for a corresponding one of the initiated data exchanges, such as a purchase transaction, the data records of transaction database <b>106</b> may include a unique identifier of the data exchange (e.g., a correlation identifier assigned to the purchase transaction, etc.), counterparty data that identifies each of the counterparties to the data exchange (e.g., an IP address of customer device that initiated the purchase transaction, an IP address of a point-of-sale device or interface, data identifying a retailer, etc.), and positional information characterizing a geographical position associated with the data exchange (e.g., a geographic position of the customer device, the POS device or interface, etc.).</p><p id="p-0025" num="0024">Further, the data records of transaction database may also include a value of one or more parameters that characterize the corresponding one of the initiated data exchanges, e.g., the purchase transaction describe herein. Examples of these parameter values include, but are not limited to, a transaction amount, a transaction date or time, and an identifier of the corresponding payment instrument (e.g., a tokenized or actual account number, etc.). The disclosed embodiments are, however, not limited to the examples of counterparty information, positional information, and transaction parameter values, and in other instances, the data records of transaction database <b>106</b> may maintain any additional or alternate counterparty information, positional information, parameter values, or elements of other data, that identify and characterize transactions and further, that are suitable for training the exemplary predictive fraud models described herein.</p><p id="p-0026" num="0025">Cryptographic library <b>108</b> may maintain, among other things, an asymmetric cryptographic key pair associated with or assigned to associated with first computing system <b>102</b>. As described herein, the asymmetric cryptographic key pair may include a homomorphic private cryptographic key and a corresponding homomorphic public cryptographic key, which may be generated in accordance with one or more homomorphic encryption schemes. In some instances, the one or more homomorphic encryption schemes may include a partially homomorphic encryption scheme, such as, but not limited to, an unpadded RSA encryption scheme, an El-Gamal encryption scheme, or a Pailler encryption scheme.</p><p id="p-0027" num="0026">In other instances, an as described herein, the one or more homomorphic encryption schemes may include a fully homomorphic encryption scheme, which facilities arbitrary computations on ciphertext and generates encrypted results that, when decrypted, match the results of the arbitrary computations performed on corresponding elements of plaintext. Examples of these homomorphic encryption schemes include but are not limited to, a TFHE scheme that facilitates verifiable computations on integer ciphertext and a SEAL encryption scheme or a PALISADE encryption scheme that facilitates verifiable computations on floating-point ciphertext.</p><p id="p-0028" num="0027">Referring back to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, trained model data store <b>110</b> may include data that identifies and characterizes one or more of the privately trained predictive fraud models described herein. In some instances, when applied to elements of homomorphically encrypted transaction data characterizing a pending or executed transaction, each of the privately trained predictive fraud models may generate output data indicative of a likelihood that the pending or executed transaction represents an instance of fraudulent activity, e.g., based on a comparison of the output data with a corresponding threshold value. Examples of the one or more predictive fraud models include, but are not limited to, a linear or nonlinear regression model, a Ridge regression model, a Least Absolute Shrinkage Selector Operator (LASSO) model, a classification scheme, such as a logistic regression model, a decision-tree model or other machine learning model, and an artificial neural network model, such as an artificial feed-forward neural network model.</p><p id="p-0029" num="0028">By way of example, each of the privately trained models may be characterized, and specified, by a corresponding set of model coefficients or model parameters (e.g., one or more of the regression models described herein) and the corresponding threshold value, as described herein. Further, the determined model coefficients, model parameters, and threshold values that specify the privately trained predictive fraud models may correspond to floating point values (e.g., suitable for application to transaction data encrypted in the SEAL or PALISADE encryption schemes described herein), or alternatively, may be scaled to integer values (e.g., suitable for application to transaction data encrypted in the TFHE encryption scheme described herein). In some instances, trained model data store <b>110</b> may maintain, for each of the privately trained predictive fraud models, a unique model identifier and corresponding elements of modelling data that include the adaptively determined model coefficients, model parameters, and threshold value.</p><p id="p-0030" num="0029">Further, each of additional computing systems <b>200</b> may also establish and maintain, within the one or more tangible, non-tangible memories, one or more structured or unstructured data repositories or databases that include, among other things, a transaction database, a cryptographic library, and a trained model data store. For example, second computing system <b>202</b> may maintain, within the one or more tangible, non-transitory memories, a data repository <b>204</b> that includes, but is not limited to, a transaction database <b>206</b>, a cryptographic library <b>208</b>, and a trained model data store <b>210</b>. In some instances, transaction database <b>206</b>, cryptographic library <b>208</b>, and trained model data store <b>210</b> may include elements of structured or unstructured data similar to that described above in reference to transaction database <b>106</b>, cryptographic library <b>108</b>, and trained model data store <b>110</b>, e.g., as maintained in data repository <b>104</b> of first computing system <b>102</b>.</p><p id="p-0031" num="0030">Further, and by way of example, third computing system <b>302</b> may maintain, within the one or more tangible, non-transitory memories, a data repository <b>304</b> that includes, but is not limited to, a transaction database <b>306</b>, a cryptographic library <b>308</b>, and a trained model data store <b>310</b>. In some instances, transaction database <b>306</b>, cryptographic library <b>308</b>, and trained model data store <b>310</b> may include elements of structured or unstructured data similar to that described above in reference to transaction database <b>106</b>, cryptographic library <b>108</b>, and trained model data store <b>110</b>, e.g., as maintained in data repository <b>104</b> of first computing system <b>102</b>.</p><p id="p-0032" num="0031">Referring back to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, first computing system <b>102</b> may perform any of the exemplary processes described herein to adaptively train and improve one or more of the predictive fraud models based on selected elements of the confidential transaction data maintained within transaction database <b>106</b>, and based on an outcome of these exemplary adaptive training and improvement processes, generate corresponding ones of the model coefficients, model parameters, and thresholds that specify the one or more predictive fraud models. In some instances, as described herein, first computing system <b>102</b> may package the model coefficients, model parameters, and thresholds into corresponding elements of modelling data, which may be stored in trained model data store <b>110</b> in conjunction with the corresponding model identifier.</p><p id="p-0033" num="0032">When executed by the one or more processors of first computing system <b>102</b>, a training engine <b>112</b> may access the data records of transaction database <b>106</b>. Executed training engine <b>112</b> may perform further operations that split or decompose the accessed data records into a first subset suitable for adaptively training the one or more predictive fraud models described herein (e.g., training data <b>114</b>) and a second subset suitable for testing and characterizing an accuracy of each of the adaptively trained predictive fraud models described herein (e.g., testing data <b>116</b>). By way of example, and without limitation, the first and second subsets may include confidential transaction data that characterizes initiated purchase transactions involving payment instruments (e.g., credit card accounts) issued to customers of the first financial institution, and a specified number of the elements of transaction data packaged into each of training data <b>114</b> and testing data <b>116</b> represent instances of fraudulent activity. By way of example, and without limitation, training data <b>114</b> may include transaction data characterizing 230,000 discrete purchase transactions, of which 394 represent instances of fraud, and testing data <b>116</b> may include transaction data characterizing 57,000 discrete purchase transactions, of which 98 represent instances of fraud, etc.).</p><p id="p-0034" num="0033">In some instances, executed training engine <b>112</b> may perform operations that normalize portions of the transaction data included within training data <b>114</b> (e.g., that normalize each transaction amount within the transaction data to range from zero to unity, etc.) and that adaptively train each of the one or more predictive fraud models against the elements of the transaction data included within training data <b>114</b>. Based on the performance of these adaptive training processes, executed training engine <b>112</b> may compute the model coefficients or model parameters for each of the one or more predictive fraud models, and determine the threshold value for each of the one or more predictive fraud models (e.g., based on a determined relationship between predicted true positive rates and false positive rates, etc.). As described herein, each of the determined threshold values may distinguish fraudulent from non-fraudulent activity when compared against corresponding elements of output data</p><p id="p-0035" num="0034">Training engine <b>112</b> may perform further operations that, based on the determined model coefficients, model parameters, and threshold value, apply each of the one or more predictive fraud models to corresponding elements of testing data <b>116</b> and determine a value indicative of an accuracy of each of the one or more predictive fraud models based on, for example, a percentage of true positives (e.g., a percentage of the total number of transactions characterized by testing data <b>116</b> identified correctly as fraudulent activity) or a percentage of true negatives (e.g., a percentage of the total number of transactions characterized by testing data <b>116</b> identified correctly as non-fraudulent activity). If training engine <b>112</b> were to establish that the metric value falls below a predetermined value (e.g., 90%) for a corresponding one of the predictive fraud models, training engine <b>112</b> may perform additional operations that continue to iteratively train, improve, and text the corresponding predictive fraud model using any of the exemplary processes described herein.</p><p id="p-0036" num="0035">Alternatively, if training engine <b>112</b> were to establish that the metric value exceeds the predetermined value for a corresponding one of the predictive fraud models, training engine <b>112</b> may deem, the corresponding predictive fraud model trained and suitable for deployment. Training engine <b>112</b> may also compute components of a confusion matrix for the newly trained predictive fraud model, and may perform operations that, for the newly trained predictive fraud model, package the determined model coefficients or parameters and the determined threshold into corresponding portions of modelling data <b>118</b>. In one example, the determined model coefficients or parameters and the determined threshold for the newly trained predictive fraud model may include floating-point values, and training engine <b>112</b> may provide modelling data <b>118</b> as an input to a scaling module <b>121</b> of first computing system <b>102</b>. When executed by the one or more processors of first computing system <b>102</b>, scaling module <b>121</b> may perform operations that scale each of the floating-point values to corresponding integer values, and generate scaled modelling data <b>122</b> that includes the scaled model coefficients or parameters and the scaled threshold.</p><p id="p-0037" num="0036">In some instances, the integer-valued model coefficients, the integer-valued model parameters, and/or the integer-valued threshold maintained within scaled modelling data <b>122</b> may be consistent with certain of the homomorphic encryption schemes described herein, such as the TFHE scheme described herein, and scaling module <b>121</b> may store scaled modelling data <b>122</b> within a corresponding portion of trained model data store <b>110</b>, e.g., in conjunction with unique model identifier <b>124</b>. In other instances, not depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the floating-point model coefficients, model parameters, and/or threshold values maintained within modelling data <b>118</b> may be consistent with additional ones of the homomorphic encryption schemes described herein, such as the SEAL or PALISADE encryption schemes, and training engine <b>112</b> may perform operations that store portions of modelling data <b>118</b> within trained model data store <b>110</b> directly and without scaling.</p><p id="p-0038" num="0037">Further, although not illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, training engine <b>112</b> may perform any of the exemplary processes described herein, either alone or in conjunction with scaling module <b>121</b>, to adaptively train, test, and improve any additional or alternate one of the predictive fraud models described herein, which may be available for application to corresponding elements of transaction data characterizing suspect transactions. Further, one or more of additional computing systems <b>200</b>, such as second computing system <b>202</b> or third computing system <b>302</b>, may also perform any of the exemplary processes described herein to adaptively train, test, and improve any of the exemplary predictive fraud models described herein, e.g., based on locally accessible and confidential elements of transaction data.</p><p id="p-0039" num="0038">In some exemplary embodiments, second computing system <b>202</b> may receive transaction data characterizing one or more pending transactions involving corresponding payment instruments, e.g., from one or more point-of-sale (POS) devices or from other devices operated by customers of the second financial institution. Prior to submitting the received elements of transaction data to one or more appropriate transaction processing networks (e.g., a payment rail, etc.), second computing system <b>202</b> may perform operations that apply one or more of the now-trained predictive fraud models to the received transaction data. As described herein, examples of the predictive fraud models include, but are not limited to, a linear or nonlinear regression model, a Ridge regression model, a LASSO model, a classification scheme, such as a logistic regression model, a decision-tree model or other machine learning model, and an artificial neural network model.</p><p id="p-0040" num="0039">For example, second computing system <b>202</b> may perform operations that access trained model data store <b>210</b>, and access modelling data that includes model coefficients or parameters and a threshold for a corresponding one of the now-trained predictive fraud models. Second computing system <b>202</b> may apply the corresponding predictive fraud model to the received transaction data in accordance with the accessed model coefficients or parameters and generate corresponding elements of output data characterizing a likelihood that each elements of the received transaction data, and corresponding ones of the pending or initiated transactions, represent instances of fraudulent activity. Based on a comparison between the accessed threshold value and the corresponding elements of the output data, second computing system <b>202</b> may determine that a particular element of the received transaction data, and a particular one of the transactions, represents potentially fraudulent activity.</p><p id="p-0041" num="0040">In one instance, second computing system <b>202</b> may determine that the particular transaction represents potentially fraudulent activity when a value of the corresponding output data element exceeds the threshold value. In other instances, second computing system <b>202</b> may determine that the particular transaction represents potentially fraudulent activity when the value of the corresponding output data element fails to exceed the threshold value, and when certain of the parameter values that characterize the particular transaction are inconsistent with expected parameter values (e.g., when a transaction amount of the particular transaction exceeds an average transaction value during a prior time period, or when a transaction velocity resulting from the particular transaction exceeds an average transaction velocity during the prior time period). In some instances, and responsive to determination that the particular transaction represents potentially fraudulent activity, second computing system <b>202</b> may decline to submit the transaction data elements characterizing the particular transaction to an appropriate transaction processing network, and may perform operations that either discard that transaction data elements or request additional confirmation from a device operated by a customer (e.g., through data transmitted across network <b>120</b> to the device).</p><p id="p-0042" num="0041">In other exemplary embodiments, described below in reference to <figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref>, second computing system <b>202</b> may generate homomorphically encrypted transaction data that characterizes the particular transaction, and may poll one or more additional computing systems operating within environment <b>100</b>, such as first computing system <b>102</b>, to obtain homomorphically encrypted output data characterizing a likelihood that the particular transaction represents fraudulent activity. As described herein, first computing system <b>102</b> may receive the homomorphically encrypted transaction data, may apply one or more predictive fraud models through a performance of verifiable homomorphic computations on portions of the homomorphically encrypted transaction data, and based on the application of the one or more predictive fraud models, generate elements of the homomorphically encrypted output data for transmission across network <b>120</b> to second computing system <b>202</b>. In some instances, second computing system <b>202</b> may receive the elements of the homomorphically encrypted output data from first computing system <b>102</b>, and from any additional or alternate ones of the computing systems within environment <b>100</b>, and may decrypt the homomorphically encrypted output data to determine the likelihood that the particular transaction represents fraudulent activity, either on an individual basis or through ensemble-based decision protocols.</p><p id="p-0043" num="0042">Through these exemplary processes, second computing system <b>202</b> may maintain the confidentiality of not only the underlying transaction data, e.g., that characterizes the particular transaction, but also the output data indicative of the likelihood that the particular transaction represents fraudulent activity, while preserving an ability to access predictive models privately trained and maintained by other computing systems operation within environment <b>100</b>. Further, certain of these exemplary processes enable first computing system <b>102</b> to maintain the confidentiality of the model coefficients, model parameters, or threshold values that characterize each of the privately trained predictive models, while facilitating interaction with confidential third-party data maintained by other computing systems operating within environment <b>100</b>.</p><p id="p-0044" num="0043">Referring to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, second computing system <b>202</b> may perform operations that generate an asymmetric cryptographic key pair using any appropriate one or more appropriate key-generation algorithms. For example, the asymmetric cryptographic key pair may include a homomorphic private cryptographic key <b>212</b> and a corresponding homomorphic public cryptographic key <b>214</b>, each of which may be generated in accordance with any of the homomorphic encryption schemes described herein. Examples of these homomorphic encryption schemes include, but are not limited to, the TFHE scheme that facilitates verifiable computations on integer ciphertext and the SEAL or PALISADE encryption schemes that facilitate verifiable computations on floating-point ciphertext. In some instances, second computing system <b>202</b> may perform operations that store homomorphic private cryptographic key <b>212</b> and homomorphic public cryptographic key <b>214</b> within corresponding portions of cryptographic library <b>208</b>, e.g., as maintained within data repository <b>204</b>.</p><p id="p-0045" num="0044">In some instances, the one or more processors of second computing system <b>202</b> may execute an encryption module <b>216</b>, which may perform operations that access the structured or unstructured data records of transaction database <b>206</b> (e.g., as maintained within data repository <b>204</b>), and access transaction data <b>218</b> that identifies and characterizes a particular pending or initiated transaction. The particular transaction may, for example, represent a potential instance of fraudulent activity, which second computing system <b>22</b> may identify using any of the processes described herein.</p><p id="p-0046" num="0045">By way of example, transaction data <b>218</b> may include, among other things, a correlation identifier assigned to the particular transaction, counterparty data that identifies each of the counterparties to the particular transaction (e.g., an IP address of a customer device that initiated the particular transaction, an IP address of a point-of-sale device or interface, data identifying a retailer), and positional information characterizing a geographical position associated with the particular transaction (e.g., a geographic position of the customer device, the POS device or interface, etc.). Further, and as described herein, transaction data <b>218</b> may also include values of transaction parameters that characterize the particular transaction, such as, but not limited to, a transaction amount, a transaction date or time, and an identifier of the corresponding payment instrument (e.g., a tokenized or actual account number, etc.).</p><p id="p-0047" num="0046">Further, and as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, encryption module <b>216</b> may also access cryptographic library <b>208</b> and extract homomorphic public cryptographic key <b>214</b>. In some instances, encryption module <b>216</b> may encrypt transaction data <b>218</b> using homomorphic public cryptographic key <b>214</b> (e.g., to generate encrypted transaction data <b>220</b>), and may package encrypted transaction data <b>220</b> and homomorphic public cryptographic key <b>214</b> into corresponding portions of a polling request <b>222</b>. In some instances, and prior to encryption using homomorphic public cryptographic key <b>214</b>, encryption module <b>216</b> may perform operations that scale each floating-point value within transaction data <b>218</b> to a corresponding integer value, e.g., for consistency with the homomorphic encryption scheme associated with homomorphic public encryption key <b>214</b>.</p><p id="p-0048" num="0047">Encryption module <b>216</b> may also perform operations that cause second computing system <b>202</b> to transmit polling request <b>222</b> across network <b>120</b> to first computing system <b>102</b>, e.g., using any appropriate communications protocol. In some instances, first computing system may perform any of the exemplary processes described herein to apply one or more predictive fraud models through a performance of verifiable, homomorphic computations on portions of encrypted transaction data <b>220</b>, and to generate homomorphically encrypted output data that, when decrypted by second computing system <b>202</b>, indicates a likelihood that the particular transaction represents fraudulent activity.</p><p id="p-0049" num="0048">For example, a secure programmatic interface of first computing system <b>102</b>, e.g., application programming interface (API) <b>224</b>, may receive polling request <b>222</b> and may provide polling request <b>222</b> as an input to a management module <b>226</b> of first computing system <b>102</b>. When executed by the one or more processors of first computing system <b>102</b>, management module <b>226</b> may perform operations that parse polling request <b>222</b> and extract corresponding ones of encrypted transaction data <b>220</b> and homomorphic public cryptographic key <b>214</b>. Further, management module <b>226</b> may provide homomorphic public cryptographic key <b>214</b> as an input to a local encryption module <b>228</b> of first computing system <b>102</b> and additionally, may provide all or a portion of encrypted transaction data <b>220</b> as an input to a homomorphic computation module <b>232</b> of first computing system <b>102</b>.</p><p id="p-0050" num="0049">When executed by the one or more processors of first computing system <b>102</b>, local encryption module <b>228</b> may receive homomorphic public cryptographic key <b>214</b>, and may extract, from scaled modelling data <b>122</b>, the model coefficients, the model parameters, and/or the threshold value that specify and characterize a corresponding one or the predictive fraud models. In some instances, local encryption module <b>228</b> may encrypt the model coefficients (and/or the model parameters) and the threshold value using homomorphic public cryptographic key <b>214</b> (e.g., to generate homomorphically encrypted model coefficients and/or parameters and a homomorphically encrypted threshold value). Further, local encryption module <b>228</b> may perform operations that package the homomorphically encrypted model coefficients (and/or the homomorphically encrypted model parameters) and the homomorphically encrypted threshold value into corresponding portions of encrypted modelling data <b>230</b>, which local encryption module <b>228</b> may provide as an input to homomorphic computation module <b>232</b>.</p><p id="p-0051" num="0050">Further, and when executed by the one or more processors of first computing system <b>102</b>, homomorphic computation module <b>232</b> may receive encrypted modelling data <b>230</b>, which includes the homomorphically encrypted model coefficients (and/or homomorphically encrypted model parameters) and the homomorphically encrypted threshold value that characterize the corresponding one of the predictive fraud models, and may also receive encrypted transaction data <b>220</b>, which includes the homomorphically encrypted transaction parameter values that characterize the pending or initiated transaction. In some instances, homomorphic computation module <b>232</b> may perform homomorphic computations that apply the corresponding predictive fraud model to each element of encrypted transaction data <b>220</b> in accordance with the homomorphically encrypted model coefficients (and/or model parameters). Based on the homomorphically encrypted threshold value, homomorphic computation module <b>232</b> may generate homomorphically encrypted output data <b>234</b> that indicates of a predicted likelihood that the particular transaction (e.g., as characterized by encrypted transaction data <b>220</b>) represents an instance of fraudulent activity.</p><p id="p-0052" num="0051">For example, homomorphically encrypted output data <b>234</b> may include binary output data that characterizes the predicted likelihood that the particular transaction represents fraudulent activity, e.g., a homomorphically encrypted value of zero for a predicted occurrence of non-fraudulent activity, and a homomorphically encrypted value of unity for a predicted occurrence of fraudulent activity. Further, in some instances, first computing system <b>102</b> may perform the homomorphic computations that apply the corresponding predictive fraud model to encrypted transaction data <b>220</b> may be implemented in conjunction within one or more additional computing systems operations within environment <b>100</b>, e.g., in parallel across a distributed computing system, such as, but not limited to a cloud-based network.</p><p id="p-0053" num="0052">In some exemplary embodiments, the homomorphic computations that facilitate the application of the corresponding one of the predictive fraud models to encrypted transaction data <b>220</b> may be tailored or selected for consistency with a homomorphic encryption scheme associated with encrypted transaction data <b>220</b> and encrypted modelling data <b>230</b>. For example, certain of the homomorphic encryption schemes described herein, which facilitate verifiable computations on integer ciphertext (e.g., the TFHE scheme), support operations that include, but are not limited to: integer-based addition operations or subtraction operations (e.g., via an additional of a signed integer); integer-based multiplication operations; integer-based comparison operations (e.g., a minimization across two integers); integer-based matrix multiplication; integer-based scalar or dot products; and integer-based operations involving decision tree algorithms (e.g., based on a pre-computation of all decision paths for a given class and a corresponding summation for that given class).</p><p id="p-0054" num="0053">To facilitate a consistency with these supported operations, certain of the floating-point model coefficients, model parameters, or thresholds that characterize the predictive models (e.g., the LASSO model or the ridge regression model) can be scaled to corresponding integer values prior to the application of the corresponding predictive fraud model to the encrypted transaction data <b>220</b> by homomorphic computation module <b>232</b>. While many of the predictive models described herein are specified in terms of one or more of these operations (e.g., the linear or non-linear regression models, which rely on addition and multiplication operations, and certain comparison operations between integer values), certain of these predictive fraud models require a computation of, one or more non-linear, exponential, or logarithmic functions, which can be approximated as polynomial functions (e.g., representative of combinations of the supported operations) prior to the application of the corresponding predictive fraud model to encrypted transaction data <b>220</b> by homomorphic computation module <b>232</b>.</p><p id="p-0055" num="0054">For example, certain of the predictive fraud models described herein, such as a logistic regression model, may evaluate a sigmoid function, e.g., y(x)=1/1+e<sup>&#x2212;x</sup>. In some instances, and prior to the performance of the homomorphic computations described herein, the sigmoid function that specifies one or more of the predictive fraud models may be approximated by a Taylor series expansion having a specified degree, e.g., y(x)=1/2+x/4+x<sup>3</sup>/48+x<sup>5</sup>/480. In other examples, one or more of the artificial neural network models described herein may define a transfer function in terms of the sigmoid function (e.g., y(x)=1/1+e<sup>&#x2212;x</sup>) or in terms of one or more additional or alternate exponential functions (e.g., y(x)=e<sup>&#x3b2;</sup><sup><sub2>0</sub2></sup><sup>+&#x3b2;</sup><sup><sub2>1</sub2></sup><sup>x</sup><sup><sub2>1</sub2></sup><sup>+&#x3b2;</sup><sup><sub2>2</sub2></sup><sup>x</sup><sup><sub2>2</sub2></sup>). Prior to the performance of the homomorphic computations described herein, the sigmoid transfer function may be approximated by the Taylor series expansion described herein, and the additional or alternate exponential functions can be approximated as polynomials using appropriate Taylor or Maclaurin expansions (e.g., that require only integer-based addition or multiplication processes).</p><p id="p-0056" num="0055">In other instances, additional ones of the homomorphic encryption schemes described herein facilitate verifiable computations on floating-point ciphertext (e.g., the SEAL or PALISADES encryption scheme). Although these additional homomorphic encryption schemes may support operations similar to those that facilitate verifiable computations on integer ciphertext, these additional homomorphic encryption schemes may be incapable of supporting certain of the comparison operations implemented by the artificial neural network models, e.g., through minimization or maximization operations implemented the pooling or rectification processes, or the operations that compare an output of the predictive fraud models against corresponding threshold values.</p><p id="p-0057" num="0056">For example, and prior to the performance of the homomorphic computations described herein, certain of these maximization or minimization operations may be approximated as a scalar multiple of an output of the pooling or rectification functions evaluated within one or more artificial neural network models. In other examples, and prior to the comparison of the predictive fraud models against corresponding threshold values, homomorphic computation module <b>232</b> may perform operations that add additional random noise to selective decrypted, and re-encrypted, portions of binary output data, and may implement the comparison operations based on a multi-step process that subtracts the homomorphically encrypted output data (which includes additional noise) from the homomorphically encrypted threshold value, and then identify a positive or negative sign that characterizes the difference.</p><p id="p-0058" num="0057">Referring back to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, homomorphic computation module <b>232</b> may perform operations that cause first computing system <b>102</b> to transmit homomorphically encrypted output data <b>234</b> across network <b>120</b> to second computing system <b>202</b>, e.g., using any appropriate communications protocol and as a response to polling request <b>222</b>. In other instances, homomorphic computation module <b>232</b> may also provide homomorphically encrypted output data <b>234</b> as an input to a verification module <b>236</b> that, when executed by the one or more processors of first computing system <b>102</b>, generates homomorphically encrypted proof data <b>238</b> representing a zero-knowledge proof of homomorphically encrypted output data <b>234</b>.</p><p id="p-0059" num="0058">By way of example, executed verification module <b>236</b> may receive homomorphically encrypted output data <b>234</b>. In some instances, executed verification module <b>236</b> may perform operations that: (i) generate a proving key (e.g., proving key P<sub>K</sub>) and a verifying key (e.g., verifying key S<sub>K</sub>) based on an implementation of a randomized key generation algorithm (e.g., KeyGen(F, &#x3bb;)&#x2192;(P<sub>K</sub>, S<sub>K</sub>)) in accordance with the corresponding one of the predictive fraud models (e.g., predictive fraud model F) and a security parameter (e.g., security parameter &#x3bb;); and (ii) compute homomorphically encrypted proof data <b>238</b> (e.g., encoded value &#x3c3;y) representing the zero knowledge proof of homomorphically encrypted output data <b>234</b>. By way of example, executed verification module <b>236</b> may compute homomorphically encrypted proof data <b>238</b> (e.g., encoded value &#x3c3;y) based on the proving key P<sub>K</sub>, the homomorphically encrypted values of the model coefficients, parameters, and/or thresholds (e.g., as specified within encrypted modelling data <b>230</b>), and encrypted transaction data <b>220</b> (e.g., as Compute(P<sub>K</sub>, x, Z<sub>ENC</sub>)&#x2192;&#x3c3;y). The encoded value &#x3c3;y (e.g., as specified by homomorphically encrypted proof data <b>238</b>) may correspond to the zero-knowledge proof that y=F(x,Z<sub>ENC</sub>).</p><p id="p-0060" num="0059">In some instances, verification module <b>236</b> may perform operations that cause first computing system <b>102</b> to transmit homomorphically encrypted proof data <b>238</b>, which includes the encoded value &#x3c3;y corresponding to the zero-knowledge proof, across network <b>120</b> to second computing system <b>202</b>, either alone or combination with homomorphically encrypted output data <b>234</b>. In other instances, verification module <b>236</b> may perform additional operations that cause first computing system <b>102</b> to transmit homomorphically encrypted proof data <b>238</b> across network <b>120</b> to one or more peer computing systems, which may perform consensus-based operations that records homomorphically encrypted proof data <b>238</b> into an additional ledger block of a distributed ledger, which may be accessible to participants in a distributed-ledger network, such as first computing system <b>102</b>, second computing system <b>202</b>, and one or more of additional computing systems <b>200</b>.</p><p id="p-0061" num="0060">Referring to <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, a secure programmatic interface of second computing system <b>202</b>, such as application programming interface (API) <b>240</b> may receive homomorphically encrypted output data <b>234</b>. API <b>240</b> may also receive homomorphically encrypted proof data <b>238</b> that includes the encoded value &#x3c3;y corresponding to the zero-knowledge proof, e.g., along or in combination with homomorphically encrypted output data <b>234</b>.</p><p id="p-0062" num="0061">In some instances, API <b>240</b> may route homomorphically encrypted output data <b>234</b> to a decryption module <b>242</b> of second computing system <b>202</b>. When executed by the one or more processors or second computing system <b>202</b>, decryption module <b>242</b> may access cryptographic library <b>208</b> (e.g., as maintained within data repository <b>204</b>) and extract homomorphic private cryptographic key <b>212</b>. Executed decryption module <b>242</b> may perform operations that decrypt all or a portion of homomorphically encrypted output data <b>234</b> using homomorphic private cryptographic key <b>212</b>, and that generate decrypted output data <b>244</b> indicative of a predicted likelihood that the pending or initiated transaction represents fraudulent activity. For example, and as described herein, decrypted output data <b>244</b> may include a binary integer value indicative of a likelihood that the pending or initiated transaction represents fraudulent activity, e.g., a value of unity for likely fraud, or a value of zero of no fraud.</p><p id="p-0063" num="0062">Further, although not illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, an additional application module executed by the one or more processors of second computing system <b>202</b> may receive homomorphically encrypted proof data <b>238</b> (e.g., that includes the encoded value &#x3c3;y), and execute a verification function based on actual transaction data <b>218</b> characterizing the pending or initiated transaction (e.g., actual values Z) and on the encoded value ay (e.g., Verify<sub>SK</sub>(Z, &#x3c3;y)&#x2192;true/false). In other instances, also not illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, the additional executed application program may access a local copy of the distributed ledger (e.g., as maintained within data repository <b>204</b>), and extract the encoded value ay representative of the zero-knowledge proof from the additional ledger block of the distributed ledger. Second computing system <b>202</b> may perform any of the exemplary processes described herein to verify the zero-knowledge proof based on the now-extracted encoded value &#x3c3;y and on transaction data <b>218</b>.</p><p id="p-0064" num="0063">In some exemplary embodiments, and through the application the homomorphic encryption schemes and the implementation of the verifiable, homomorphic computations described herein, first computing system <b>102</b> may publish, and render accessible to other computing systems operating within environment <b>100</b>, a privately trained predictive fraud model while maintaining not only the confidentiality of the trained model coefficients or parameters and corresponding threshold value, but also the confidentiality of any sensitive transaction data provided as an input to the published predictive fraud model by the other computing systems, such as second computing system <b>202</b>. In other exemplary embodiments, one or more additional computing systems operating within environment <b>100</b>, e.g., a subset of additional computing systems <b>200</b>, may perform any of the exemplary processes described herein to privately train an additional predictive fraud model based on locally maintained elements of confidential transaction data, and to publish confidentially and render these additional predictive fraud models accessible to over participants in environment <b>100</b> using any of the homomorphic encryption schemes and verifiable, homomorphic computations described herein.</p><p id="p-0065" num="0064">For example, although not illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>A or <b>2</b>B</figref>, second computing system <b>202</b> may transmit polling request <b>222</b> (e.g., that includes encrypted transaction data <b>220</b> and homomorphic public cryptographic key <b>214</b>) across network <b>120</b> to the subset of additional computing systems <b>200</b>. Each of the subset of additional computing systems <b>200</b> may perform any of the exemplary processes described herein to apply a corresponding privately trained predictive fraud model (e.g., which may be different from the predictive fraud model privately trained by first computing system <b>102</b>) to encrypted transaction data <b>220</b> in accordance with a corresponding set of homomorphically encrypted model coefficients or model parameters. Further, based on a corresponding homomorphically encrypted threshold value, the subset of additional computing systems <b>200</b> may each generate a corresponding element of homomorphically encrypted output data, and transmit the corresponding element of homomorphically encrypted output data across network <b>120</b> to second computing system <b>202</b> (e.g., either alone or in conjunction with homomorphically encrypted proof data).</p><p id="p-0066" num="0065">Second computing system <b>202</b> may receive each of the elements of homomorphically encrypted output data from the subset of additional computing systems <b>200</b>, e.g., through API <b>240</b>. In some instances, executed decryption module <b>242</b> may perform any of the exemplary processes described herein to decrypt each of the elements of homomorphically encrypted output data using homomorphic private cryptographic key <b>212</b>, and to generate additional elements of decrypted output data indicative of a likelihood that the particular transaction represents fraudulent activity, e.g., based on the application of the each of the corresponding privately trained predictive fraud models to encrypted transaction data <b>220</b>.</p><p id="p-0067" num="0066">By way of example, second computing system <b>202</b> may establish that the pending or initiated transaction represents fraudulent or non-fraudulent activity based on a collective analysis of decrypted output data <b>244</b> in conjunction with each of the additional elements of decrypted output data, e.g., based on an implementation of ensemble-based decision protocols. For instance, based on these ensemble-based decision protocols, second computing system <b>202</b> may establish that the pending or initiated transaction represents fraudulent activity (or non-fraudulent activity) when (i) a majority of first computing system <b>102</b> and the additional computing systems predict that pending or initiated transaction represents fraudulent (or non-fraudulent) activity, or (ii) when a predetermined fraction, such as 75%, of first computing system <b>102</b> and the additional computing systems predict that pending or initiated transaction represents fraudulent (or non-fraudulent) activity.</p><p id="p-0068" num="0067">In some instances, the repeated generation and transmission of elements of homomorphically encrypted output data (e.g., homomorphically encrypted output data <b>234</b> of <figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref>) to second computing system <b>202</b> by first computing system <b>102</b> and each of the subset of additional computing systems <b>200</b> may be computationally inefficient, and can decrease a network bandwidth characterizing network <b>120</b> and other communications networks that interconnect systems and devices within environment <b>100</b>, especially when the subset of additional computing systems <b>200</b> includes a large number of computing systems. Further, the repeated transmission of elements of homomorphically encrypted output data across network <b>120</b>, and the repeated receipt of these elements of homomorphically encrypted output data by second computing system <b>202</b>, may increase a likelihood of fraudulent activity or unauthorized access by malicious third parties (e.g., man-in-the-middle attacks, etc.).</p><p id="p-0069" num="0068">Further, the reliance of second computing system <b>202</b> on the elements of homomorphically encrypted output data generated by first computing system <b>102</b> and each of the subset of additional computing systems <b>200</b> implies an established level of trust between the second financial institution and each of the first financial institution and the additional financial institutions or related entities (e.g., that the privately generated models accurately and effectively predict likelihood of fraud, etc.). Although the established trust may be appropriate for models generated by large financial institutions or well-known related entities, such as a regulators or certain consortia, the second financial institution may be susceptible to certain biases or inaccuracies derived from an incomplete or improper application of the predictive models by less reputable entities, or from an inaccurately derived or trained predictive models.</p><p id="p-0070" num="0069">In view of these deficiencies, certain of the exemplary embodiments enable a network-connected computing system operated by a centralized authority, such as third computing system <b>302</b>, to receive discrete elements of modelling data that identify and characterize predictive fraud models privately trained by each of first computing system <b>102</b> and the subset of additional computing systems <b>200</b>, and to store the discrete elements of modelling data within a corresponding data repository. The centralized authority may, for example include a governmental entity, a regulatory entity, a consortium of financial institutions, or a service provider associated with the financial services industry. Further, the centralized authority may establish certain policies that, when applied to a potentially unknown financial institution that submits elements of modelling data to third computing system <b>302</b>, establishes trust between the second financial institution and the potentially unknown financial institution and ensure a reputability of that entity and a robustness of the underlying predictive fraud model generated and trained privately by the potentially unknown financial institution.</p><p id="p-0071" num="0070">Referring to <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, third computing system <b>302</b> may receive discrete elements of modelling data from first computing system <b>102</b> and from the subset of additional computing systems <b>200</b>, and may perform operations that store the discrete elements of modelling data within corresponding portions of trained model data store <b>310</b>. For example, third computing system <b>302</b> may receive all or a portion of modelling data <b>122</b> from first computing system <b>102</b> and as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, third computing system <b>302</b> may store modelling data <b>122</b> within a corresponding portion of trained model data store <b>310</b>. As described herein, modelling data <b>122</b> may include scaled (or unscaled) modeled coefficients or parameters and a threshold value specifying a first predictive fraud model privately trained by first computing system <b>102</b>, e.g., based on locally maintained elements of confidential transaction data using any of the exemplary processes described herein.</p><p id="p-0072" num="0071">Third computing system <b>302</b> may also receive modelling data <b>312</b> and modelling data <b>314</b> from corresponding ones of additional computing systems <b>200</b> and, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, third computing system <b>302</b> may store modelling data <b>312</b> and modelling data <b>314</b> within a corresponding portion of trained model data store <b>310</b>. As described herein, modelling data <b>312</b> and modelling data <b>314</b> may include scaled (or unscaled) modeled coefficients or parameters and a threshold value specifying respective ones of a second and a third predictive fraud model privately trained by the corresponding ones of additional computing systems <b>200</b>, e.g., based on locally maintained elements of confidential transaction data using any of the exemplary processes described herein.</p><p id="p-0073" num="0072">The disclosed embodiments are, however, not limited to, processes by which third computing system <b>302</b> receive modelling data <b>122</b>, modelling data <b>312</b>, and modelling data <b>314</b>, which specify corresponding ones of the first, second, and third privately trained predictive fraud models described herein. In other instances, and consistent with the disclosed embodiments, third computing system <b>302</b> may receive, store, and perform operations consistent with any additional or alternate sets of modelling data, which correspond to any additional or alternate ones of the privately trained predictive fraud models described herein.</p><p id="p-0074" num="0073">Additionally, in some instances, one or more modelling data <b>122</b>, modelling data <b>312</b>, modelling data <b>314</b> may be encrypted using a public cryptographic key associated with the centralized authority. By way of example, a computing system operated by the centralized authority (e.g., third computing system <b>302</b>) may broadcast the public cryptographic key associated with the centralized authority to first computing system <b>102</b>, the subset of additional computing systems <b>200</b>, and any other computing system that participates in the distributed-ledger network described herein.</p><p id="p-0075" num="0074">In further examples, not illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, third computing system <b>302</b> may also maintain, within data repository <b>304</b>, elements of policy data that identify and characterize one or more model retention and application policies, which establish enforce certain institution- or model-specific restrictions on any financial institution that submits modelling data for storage and ultimate application to homomorphically encrypted transaction data by third computing system <b>302</b>. For instance, the one or more model retention and application policies may specify restrictions on a type of the predictive fraud model (e.g., that excludes a predictive fraud model shown unreliable through academic study or practical application, etc.) or on a degree of private training (e.g., that a tested accuracy of the privately trained model exceeds a particular accuracy, such as 90%, etc.). In other instances, the one or more model retention or application policies may impose certain restrictions on a financial institution that submits modelling data for retention and application, such as, but not limited to, restrictions on an institution size, a geographic restriction, or restrictions related to a regulatory status of the financial institution. For example, the one or more model retention or application policies may establish a list of financial institution permitted to submit modelling data, and a corresponding list of financial institution not permitted to submit modelling data.</p><p id="p-0076" num="0075">In some instances, third computing system <b>302</b> may perform operations that access the stored policy data and apply the one or more model retention or application policies to each of the received elements of modelling data, e.g., modelling data <b>122</b>, modelling data <b>312</b>, and modelling data <b>314</b> described herein. If third computing system <b>302</b> were to determine that one of the received elements of modelling data fails to comply with the applied model retention and application policies, third computing system <b>302</b> may reject that elements of modelling data that a transmit an error message to the computing system that submitted the modelling data. Alternatively, if third computing system <b>302</b> were to establish that the received element of modelling data complies with each of the applied model retention and application policies, third computing system <b>302</b> may perform operations that store the received element of modelling data within a corresponding portion of trained model data store <b>310</b>.</p><p id="p-0077" num="0076">Referring back to <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, second computing system <b>202</b> may perform any of the exemplary processes described herein to package encrypted transaction data <b>220</b> (e.g., transaction data <b>218</b> representative of the pending or initiated transaction encrypted using homomorphic public cryptographic key <b>214</b>) and homomorphic public cryptographic key <b>214</b> into corresponding portions of polling request <b>222</b>, and to transmit polling request <b>222</b> across network <b>120</b> to third computing system <b>302</b>, e.g., using any appropriate communications protocol. In some instances, and through any of the exemplary processes described herein, third computing system <b>302</b> may perform homomorphic processes that apply each of the first, second, and third privately trained, predictive fraud models to portions of encrypted transaction data <b>220</b>, and that generate model-specific elements homomorphically encrypted output data. As described herein, and when decrypted by second computing system <b>202</b>, each of the model-specific elements homomorphically encrypted output data indicate a likelihood that the pending or initiated transaction represents fraudulent activity, and facilitate an implementation of any of the ensemble-based decision protocols described herein by second computing system <b>202</b>.</p><p id="p-0078" num="0077">For example, a secure programmatic interface of third computing system <b>302</b>, e.g., application programming interface (API) <b>316</b>, may receive polling request <b>222</b> and may provide polling request <b>222</b> as an input to a management module <b>226</b> of third computing system <b>302</b>. When executed by the one or more processors of third computing system <b>302</b>, management module <b>318</b> may perform operations that parse polling request <b>222</b> and extract corresponding ones of encrypted transaction data <b>220</b> and homomorphic public cryptographic key <b>214</b>. Further, management module <b>318</b> may provide homomorphic public cryptographic key <b>214</b> as an input to a local encryption module <b>320</b> of third computing system <b>302</b> and additionally, may provide all or a portion of encrypted transaction data <b>220</b> as an input to a homomorphic computation module <b>321</b> of third computing system <b>302</b>.</p><p id="p-0079" num="0078">When executed by the one or more processors of third computing system <b>302</b>, local encryption module <b>320</b> may receive homomorphic public cryptographic key <b>214</b>, and may access each of modelling data <b>122</b>, modelling data <b>312</b>, and modelling data <b>314</b>, and extract sets of the model coefficients and/or parameters and the threshold value that specifies and characterizes the corresponding one of the first, second, and third predictive fraud models. In one example, and as described herein, one or more of modelling data <b>122</b>, modelling data <b>312</b>, and modelling data <b>314</b> may be encrypted using the public cryptographic key of the centralized authority, and local encryption module <b>320</b> may perform operations that obtain a private cryptographic key associated with the centralized authority (e.g., as maintained within a software- or hardware-based secure element) and decrypt one or more of modelling data <b>122</b>, modelling data <b>312</b>, and modelling data <b>314</b> using the private cryptographic key of the centralized authority.</p><p id="p-0080" num="0079">In some instances, local encryption module <b>320</b> may perform operations that encrypt the model coefficients and/or parameters and the threshold value within each of the extracted sets using homomorphic public cryptographic key <b>214</b> (e.g., to generate sets of homomorphically encrypted model coefficients (and/or model parameters) and threshold value), and to package the sets of homomorphically encrypted model coefficients (and/or model parameters) and the homomorphically encrypted threshold value corresponding ones of encrypted modelling data <b>322</b>, encrypted modelling data <b>324</b>, and encrypted modelling data <b>326</b>. In some instances, encrypted modelling data <b>322</b>, encrypted modelling data <b>324</b>, and encrypted modelling data <b>326</b> may facilitate an application of corresponding ones of the first, second, and third predictive fraud modules through homomorphic computations on encrypted transaction data <b>220</b>, and local encryption module <b>320</b> may provide each of encrypted modelling data <b>322</b>, encrypted modelling data <b>324</b>, and encrypted modelling data <b>326</b> as inputs to homomorphic computation module <b>321</b>.</p><p id="p-0081" num="0080">When executed by the one or more processors of first computing system <b>102</b>, homomorphic computation module <b>232</b> may receive encrypted modelling data <b>322</b>, encrypted modelling data <b>324</b>, and encrypted modelling data <b>326</b>, which includes the homomorphically encrypted model coefficients and/or parameters and the homomorphically encrypted threshold value that characterize corresponding ones of the first, second, and third predictive fraud models, and may also receive encrypted transaction data <b>220</b>, which includes the homomorphically encrypted transaction parameter values that characterize the pending or initiated transaction. In some instances, homomorphic computation module <b>321</b> may perform any of the exemplary homomorphic computations described herein that apply each of the first, second, and third predictive fraud models to each element of encrypted transaction data <b>220</b>, in accordance with the homomorphically encrypted model coefficients or model parameters specified within corresponding ones of encrypted modelling data <b>322</b>, encrypted modelling data <b>324</b>, and encrypted modelling data <b>326</b>.</p><p id="p-0082" num="0081">Further, and based on the homomorphically encrypted threshold value specified within corresponding ones of encrypted modelling data <b>322</b>, encrypted modelling data <b>324</b>, and encrypted modelling data <b>326</b>, homomorphic computation module <b>232</b> may perform any of the exemplary homomorphic computations described herein to generate, for each of the first, second, and third predictive fraud models, a corresponding one of homomorphically encrypted output data <b>328</b>, homomorphically encrypted output data <b>330</b>, and homomorphically encrypted output data <b>332</b> indicative of a predicted likelihood that the pending or initiated transaction (e.g., as characterized by encrypted transaction data <b>220</b>) represents fraudulent activity.</p><p id="p-0083" num="0082">Each of homomorphically encrypted output data <b>328</b>, <b>330</b>, and <b>332</b> may, in some instances, include binary output data that characterized the predicted likelihood that the pending or initiated transaction represents fraudulent activity, e.g., a homomorphically encrypted value of zero for a predicted occurrence of non-fraudulent activity, and a homomorphically encrypted value of unity for a predicted occurrence of fraudulent activity. Further, in some instances, third computing system <b>302</b> may perform the homomorphic computations, which apply first, second, and third predictive fraud models to encrypted transaction data <b>220</b>, in conjunction within one or more additional computing systems operations within environment <b>100</b>, e.g., in parallel across a distributed computing system, such as, but not limited to a cloud-based network.</p><p id="p-0084" num="0083">In some instances, homomorphic computation module <b>321</b> may package each of homomorphically encrypted output data <b>328</b>, <b>330</b>, and <b>332</b> into a corresponding portion of polling response <b>334</b>, and may perform operations that cause third computing system <b>302</b> to transmit polling response <b>334</b> across network <b>120</b> to second computing system <b>202</b>, e.g., using any appropriate communications protocol and as a response to polling request <b>222</b>. In other instances, homomorphic computation module <b>321</b> may also provide each of homomorphically encrypted output data <b>328</b>, <b>330</b>, and <b>332</b> as an input to a verification module <b>336</b> that, when executed by the one or more processors of first computing system <b>102</b>, perform any of the exemplary processes described herein to generate encoded values <b>338</b>, <b>340</b>, and <b>342</b> (e.g., homomorphically encrypted values), each of which represent a zero-knowledge proof of a corresponding one of homomorphically encrypted output data <b>328</b>, <b>330</b>, and <b>332</b>.</p><p id="p-0085" num="0084">In some instances, verification module <b>336</b> may package each of encoded values <b>338</b>, <b>340</b>, and <b>342</b> into corresponding portions of homomorphically encrypted proof data <b>344</b>, and may perform operations that cause third computing system <b>302</b> to transmit homomorphically encrypted proof data <b>344</b> across network <b>120</b> to second computing system <b>202</b>, either alone or combination with polling response <b>334</b>. In other instances, verification module <b>336</b> may perform additional operations that cause third computing system <b>302</b> to transmit homomorphically encrypted proof data <b>344</b> across network <b>120</b> to one or more peer computing systems, which may perform consensus-based operations that records homomorphically encrypted proof data <b>344</b>, including encoded values <b>338</b>, <b>340</b>, and <b>342</b>, into an additional ledger block of a distributed ledger, which may be accessible to participants in a distributed-ledger network, such as first computing system <b>102</b>, second computing system <b>202</b>, third computing system <b>302</b>, and one or more of additional computing systems <b>200</b>.</p><p id="p-0086" num="0085">Referring to <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, a secure programmatic interface of second computing system <b>202</b>, such as API <b>240</b> may receive polling response <b>334</b>, which includes homomorphically encrypted output data <b>328</b>, <b>330</b>, and <b>332</b>. In some instances, each of homomorphically encrypted output data <b>328</b>, <b>330</b>, and <b>332</b> may include homomorphically encrypted binary output data characterizing the predicted likelihood that the pending or initiated transaction represents fraudulent activity, e.g., based on an application of the first, second, and third predictive models to encrypted transaction data <b>220</b>. Further, API <b>240</b> may also receive homomorphically encrypted proof data that includes encoded value <b>338</b>, <b>340</b>, and <b>342</b> corresponding to the zero-knowledge proof described herein, e.g., alone or in combination with polling response <b>334</b>.</p><p id="p-0087" num="0086">In some instances, API <b>240</b> may route polling response <b>334</b> to decryption module <b>242</b>, which may perform any of the exemplary processes described herein to decrypt each of homomorphically encrypted output data <b>328</b>, <b>330</b>, and <b>332</b> using homomorphic private cryptographic key <b>212</b> (e.g., as maintained within cryptographic library <b>208</b> of data repository <b>204</b>), and to generate corresponding ones of model-specific elements <b>348</b>, <b>350</b>, and <b>352</b> of decrypted output data <b>346</b>. For example, and as described herein, each of elements <b>348</b>, <b>350</b>, and <b>352</b> of decrypted output data <b>346</b> may include a binary integer value indicative of a likelihood that the pending or initiated transaction represents fraudulent activity, e.g., a value of unity for likely fraud, or a value of zero of no fraud, based on an application of a corresponding one of the first, second, and third predictive fraud model to encrypted transaction data <b>220</b>.</p><p id="p-0088" num="0087">Further, although not illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, an additional application module executed by the one or more processors of second computing system <b>202</b> may receive homomorphically encrypted proof data <b>238</b>, which includes encoded values <b>338</b>, <b>340</b>, and <b>342</b>, and perform any of the exemplary processes described herein to verify the zero-knowledge proofs represents by <b>338</b>, <b>340</b>, and <b>342</b>, e.g., based on a computation of a verification function based on actual transaction data <b>218</b> characterizing the pending or initiated transaction and on each of encoded values <b>338</b>, <b>340</b>, and <b>342</b>.</p><p id="p-0089" num="0088">In other instances, also not illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, the additional executed application program may access a local copy of the distributed ledger (e.g., as maintained within data repository <b>204</b>), and extract each of encoded values <b>338</b>, <b>340</b>, and <b>342</b> from the additional ledger block of the distributed ledger. Second computing system <b>202</b> may perform any of the exemplary processes described herein to verify each of the zero-knowledge proofs (e.g., which are represented by corresponding ones of encoded values <b>338</b>, <b>340</b>, and <b>342</b>) based on the now-extracted encoded values <b>338</b>, <b>340</b>, and <b>342</b> and on transaction data <b>218</b>.</p><p id="p-0090" num="0089">In some exemplary embodiments, a computing system operated by a centralized authority (e.g., a governmental entity, a regulatory entity, a consortium of financial institutions, or a service provider, etc.) may receive and store discrete elements of modelling data that identify and characterize predictive fraud models privately trained by each of first computing system <b>102</b> and the subset of additional computing systems <b>200</b>. The computing system of the centralized authority, e.g., third computing system <b>302</b>, may apply any of the model retention and application policies described herein to each of the discrete elements of modelling data, and perform any of the exemplary processes described herein to apply each of the privately trained predictive fraud models to a received element of homomorphically encrypted transaction data associated with a particular transaction, and for each of the applied predictive fraud models, to generate an element of homomorphically encrypted output data that characterize a predicted likelihood that the particular transaction represents fraudulent activity.</p><p id="p-0091" num="0090">In other examples, and consistent with the disclosed embodiments, the centralized authority may establish and maintain a distributed ledger having ledger blocks that immutably record discrete elements of modelling data specifying the predictive fraud models privately trained by each of first computing system <b>102</b> and the subset of additional computing systems <b>200</b>, such as, but not limited to, the model coefficients, model parameters, or thresholds described herein. Further, the ledger blocks may also immutably record executable code (e.g., within &#x201c;smart contract&#x201d; blocks) that, when executed by one or more peer computing systems of a distributed-ledger network, implement consensus-based operations that perform any of the exemplary homomorphic computations described herein, which apply each of the predictive fraud models to homomorphically encrypted transaction data provided by second computing system <b>202</b>.</p><p id="p-0092" num="0091">As described herein, each of the peer systems may correspond to a computing system that includes one or more servers and tangible, non-transitory memory devices storing executable code and application modules. The one or more servers may each include one or more processors, which may be configured to execute portions of the stored code or application modules to perform operations consistent with the disclosed embodiments. Further, each of the peer systems may be interconnected with first computing system <b>102</b>, second computing system <b>202</b>, third computing system <b>302</b>, and additional computing systems <b>200</b> across one or more of the communications networks described herein, such as network <b>120</b>.</p><p id="p-0093" num="0092">By way of example, each of the peer computing systems may receive, through a secure, programmatic interface, discrete elements of modelling data <b>122</b>, which identifies and characterized the first predictive fraud model privately trained by first computing system <b>102</b> using any of the exemplary processes described herein. Further, each of the peer computing systems may also receive, through the secure, programmatic interface, discrete elements of modelling data <b>312</b> and modelling data <b>314</b>, which identify and characterize respective ones of the second and third predictive fraud models privately trained by the subset of additional computing systems <b>200</b> using any of the exemplary processes described herein.</p><p id="p-0094" num="0093">In some instances, and as described herein, each of modelling data <b>122</b>, modelling data <b>312</b>, and modelling data <b>314</b> may include scaled (or unscaled) modeled coefficients or parameters and a threshold value specifying a corresponding one of the first predictive fraud model, the second predictive fraud model, and the third predictive fraud model. Further, one or more of modelling data <b>122</b>, modelling data <b>312</b>, and modelling data <b>314</b> may include counter data characterizing a number of discrete operations, e.g., homomorphic computations, required to apply corresponding ones of the first, second, and third predictive fraud models to an element of homomorphically encrypted transaction data. The counter data associated with each of the first, second, and third predictive models may be generated by corresponding ones of first computing system <b>102</b> and the subset of additional computing systems <b>200</b> through the implementation of any of the exemplary model training processes described herein.</p><p id="p-0095" num="0094">Additionally, in some instances, one or more modelling data <b>122</b>, modelling data <b>312</b>, modelling data <b>314</b> may be encrypted using a public cryptographic key associated with the centralized authority. By way of example, one or more of the peer computing systems, or an additional computing system operated by the centralized authority (e.g., third computing system <b>302</b>) may broadcast the public cryptographic key associated with the centralized authority to first computing system <b>102</b>, the subset of additional computing systems <b>200</b>, and any additional or alternate computing system that participates in the distributed-ledger network described herein.</p><p id="p-0096" num="0095">In some examples, the peer computing systems may perform consensus-based operations that immutably record each modelling data <b>122</b>, modelling data <b>312</b>, modelling data <b>314</b> within one or more additional ledger blocks of the distributed ledger, and that append the one or more additional ledger blocks to a prior version of the distributed ledger to generate an updated version of the distributed ledger for among the peer computing systems and any additional or alternate computing systems associated with the distributed-ledger network. In addition to the discrete elements of modelling data <b>122</b>, modelling data <b>312</b>, modelling data <b>314</b>, the one or more ledger blocks may also include information that identifies corresponding ones of the first, second, and third-predictive models (e.g., the model identifiers described herein) and information that identifies corresponding ones of first computing system <b>102</b> and the subset of additional computing systems <b>200</b> (e.g., an IP address, and identifier of a corresponding financial institution, etc.), along with an applied digital signature and a corresponding hash value.</p><p id="p-0097" num="0096">In further instances, and prior to recording modelling data <b>122</b>, modelling data <b>312</b>, modelling data <b>314</b> within the one or more ledger blocks, the peer computing systems may perform consensus-based operations that execute certain instructions recorded onto the distributed ledger to verify a consistency of each of modelling data <b>122</b>, modelling data <b>312</b>, modelling data <b>314</b> with each of the model retention and application policies described herein (e.g., that ensure model robustness, etc.). For example, and as described herein, if the peer computing systems were to determine that one of the received elements of modelling data fails to comply with the applied model retention and application policies, the peer computing systems may collectively reject those elements of modelling data, and transmit an error message to the computing system that submitted the rejected elements of modelling data.</p><p id="p-0098" num="0097">By way of example, second computing system <b>202</b> may perform any of the exemplary processes described herein to package encrypted transaction data <b>220</b> (e.g., transaction data <b>218</b> representative of the pending or initiated transaction encrypted using homomorphic public cryptographic key <b>214</b>) and homomorphic public cryptographic key <b>214</b> into corresponding portions of polling request <b>222</b>, and to transmit polling request <b>222</b> across network <b>120</b> to each of the peer computing systems, e.g., using any appropriate communications protocol. Each of the peer computing systems may receive polling request <b>222</b> through a secure programmatic interface, such as an application programming interface, and upon receipt of polling request, each of the peer computing system may access the updated version of the distributed ledger, which records modelling data <b>122</b>, modelling data <b>312</b>, and modelling data <b>314</b> within the one or more additional ledger blocks, and which also records elements of executable code within corresponding ones of the smart contract blocks (e.g., discrete application modules, etc.).</p><p id="p-0099" num="0098">Further, and responsive to the receipt of polling request <b>222</b>, each of the peer computing systems may execute the accessed code elements. In some instances, the execution of the accessed code elements may cause each of the peer computing systems to perform, through consensus-based operations, any of the exemplary homomorphic computations described herein to apply each of the first, second, and third privately trained, predictive fraud models to portions of encrypted transaction data <b>220</b>, and to generate model-specific elements of homomorphically encrypted output data that, when decrypted by second computing system <b>202</b>, indicates a likelihood that the particular transaction represents fraudulent activity.</p><p id="p-0100" num="0099">By way of example, for the first predictive model, the consensus-based operations performed by the peer computing systems can include, among other things: (i) extracting modelling data <b>122</b> from a ledger block of the distributed ledger, and encrypting the corresponding model coefficients or parameters and the threshold value using the homomorphic public cryptographic key <b>214</b>; (ii) perform any of the exemplary homomorphic computations described herein to apply the first predictive fraud model to encrypted transaction data <b>220</b> in accordance with the homomorphically encrypted model coefficients or model parameters; (iii) based on the homomorphically encrypted threshold value, perform any the exemplary processes described herein to generate an element of homomorphically encrypted output data indicative of a predicted likelihood that the particular transaction represents fraudulent activity; and (iv) perform any of the exemplary processes described herein to compute an encoded value that represents a zero knowledge proof for each element of the homomorphically encrypted output data and the associated homomorphic computations. Further, and responsive to the receipt of polling request <b>222</b>, each of the peer computing systems may implement the exemplary consensus-based processes to apply the second predictive fraud model to encrypted transaction data <b>220</b> (e.g., based on modelling data <b>312</b>) and to apply the third predictive fraud model to encrypted transaction data <b>220</b> (e.g., based on modelling data <b>314</b>).</p><p id="p-0101" num="0100">In some instances, described herein, one or more elements of modelling data <b>122</b>, modelling data <b>312</b>, and modelling data <b>314</b> may be encrypted using the public cryptographic key of the centralized authority. Prior to performing any of the exemplary consensus-based processes described herein, each of the peer computing systems may perform operations that access a secure, permissioned portion of the distributed ledger, extract a private cryptographic key associated with the distributed ledger, and decrypt each of the elements of modelling data <b>122</b>, modelling data <b>312</b>, and modelling data <b>314</b> using the private cryptographic key of the centralized authority.</p><p id="p-0102" num="0101">Further, and as described herein, one or more of modelling data <b>122</b>, modelling data <b>312</b>, and modelling data <b>314</b> may include the counter data characterizing the number of discrete operations, e.g., homomorphic computations, required to apply corresponding ones of the first, second, and third predictive fraud models to encrypted transaction data <b>220</b>. In some instances, the peer computing systems may perform consensus-based operations described herein to track a number of discrete homomorphic computations associated with the application of each of the first, second, and third predictive fraud models to encrypted transaction data <b>220</b>, and to verify a consistency between the tracked numbers of homomorphic computations and each elements of the counter data. If, for example, one or more of the peer computing systems were to detect an inconsistency between the counter data and the tracked numbers of homomorphic computations for a corresponding one of the predictive fraud models, the peer computing systems may decline, through consensus-based operations, to generate homomorphically encrypted output data indicative of the application of that predictive fraud model to encrypted transaction data <b>220</b>.</p><p id="p-0103" num="0102">Through the implementation of the consensus-based operations, the peer computing systems may perform any of the exemplary processes described herein to package each element of the homomorphically encrypted output data (e.g., that correspond to the application of respective ones of the first, second, and third predictive models to encrypted transaction data <b>220</b>) into a corresponding portion of a polling response, may be transmitted across network <b>120</b> to second computing system <b>202</b>. Additionally, and through the implementation of these consensus-based operations, the peer computing systems may also package the encoded values (e.g., that represent the zero-knowledge proof for corresponding elements of the homomorphically encrypted output data) into a corresponding portion of homomorphically encrypted proof data, which may also be transmitted across network <b>120</b> to second computing system <b>202</b>.</p><p id="p-0104" num="0103">In some examples, a secure programmatic interface of second computing system <b>202</b>, such as API <b>240</b> described herein, may receive the polling response, which includes each element of the homomorphically encrypted output data, and may also receive the homomorphically encrypted proof data, which includes the encoded values representative of the zero-knowledge proof for corresponding elements of the homomorphically encrypted output data. One or more application modules executed by second computing system <b>202</b> may perform any of the exemplary processes described herein to decrypt each element of the homomorphically encrypted output data (e.g., using homomorphic private cryptographic key <b>212</b>), and to obtain binary output data characterizing the predicted likelihood that the pending or initiated transaction represents fraudulent activity, e.g., based on an application of corresponding ones of the first, second, and third predictive models to encrypted transaction data <b>220</b>.</p><p id="p-0105" num="0104">Second computing system <b>202</b> may also perform any of the exemplary processes described herein to determine whether the pending or initiated transaction represents fraudulent activity, e.g., based on an application of any of the ensemble-based decision protocols to the binary output data. Further, an additional application module executed by second computing system <b>202</b> may also perform any of the exemplary processes described herein to verify the zero-knowledge proofs represents by the encoded values packaged into the received homomorphically encrypted proof data, e.g., based on a computation of a verification function based on actual transaction data <b>218</b> characterizing the pending or initiated transaction and on each of the encoded values.</p><p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of an exemplary process <b>400</b> for applying a privately trained predictive model to encrypted third-party data using verifiable homomorphic computations, in accordance with some exemplary embodiments. In some examples, one or more network-connected computing systems operating within environment <b>100</b>, such as, but not limited to, first computing system <b>102</b>, third computing system <b>302</b>, and one or more of the peer computing systems described herein, may perform one or more of the exemplary steps of process <b>400</b>.</p><p id="p-0107" num="0106">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may obtain a homomorphic public cryptographic key and elements of homomorphically encrypted transaction data that identify and characterize a particular transaction (e.g., in step <b>402</b>). For example, in step <b>402</b>, first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may receive a polling request that includes the homomorphic public cryptographic key and the elements of the homomorphically encrypted transaction data from an additional network-connected system operating within environment <b>100</b>, such as second computing system <b>202</b>. Further, in some instances, second computing system <b>202</b> may generate the homomorphically encrypted transaction data by encrypting one or more elements of locally maintained transaction data, which identify and characterize the pending or initiated transaction, using the homomorphic public cryptographic key.</p><p id="p-0108" num="0107">In some instances, first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may access modelling data that specifies a privately trained predictive model (e.g., in step <b>404</b>). Examples of these privately trained predictive-fraud models include, but are not limited to, one or more regression models characterized by derived model coefficients, tuned parameters, and/or threshold values (such as, but not limited to a LASSO model or a ridge regression), one or more artificial neural network models, and one or more machine learning models (such as, but not limited to, a logistic regression model or certain decision tree models. The accessed modelling data may include adaptively trained values of model coefficients (and/or model parameters) and a threshold value that specify the privately trained predictive-fraud model, and the modelling data may be maintained within one or more tangible, non-transitory memories (e.g., within trained model data store <b>110</b> of data repository <b>104</b> of first computing system <b>102</b>), or may be recorded onto one or more ledger blocks of a permissioned distributed ledger.</p><p id="p-0109" num="0108">Further, first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may perform any of the exemplary processes described herein to encrypt each element of the accessed modelling data using the homomorphic public cryptographic key (e.g., in step <b>406</b>). By way of example, in step <b>406</b>, first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may generate homomorphically encrypted modelling data that include homomorphically encrypted values of the model coefficients (and/or the model parameters) and the threshold value that characterizes the privately trained predictive-fraud model.</p><p id="p-0110" num="0109">Using any of the exemplary processes described herein, first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may perform homomorphic computations that apply the privately trained predictive-fraud model to the homomorphically encrypted transaction data in accordance with the homomorphically encrypted model coefficients or model parameters (e.g., in step <b>408</b>). Further, and based on homomorphically encrypted threshold value, first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may generate homomorphically encrypted output data indicative of a predicted likelihood that the particular transaction represents fraudulent activity (e.g., in step <b>410</b>).</p><p id="p-0111" num="0110">In some instances, the homomorphically encrypted output data may include binary output data that characterized the predicted likelihood that the pending or initiated transaction represents fraudulent activity, e.g., a homomorphically encrypted value of zero for a predicted occurrence of non-fraudulent activity, and a homomorphically encrypted value of unity for a predicted occurrence of fraudulent activity. Further, first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may perform any of the exemplary processes described herein to generate an encoded value, such as, but not limited to, a homomorphically encrypted value, that represents a zero-knowledge proof of the homomorphically encrypted output data (e.g., in step <b>412</b>).</p><p id="p-0112" num="0111">In step <b>414</b>, first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may perform additional operations that determine whether additional privately trained predictive-fraud models (and corresponding elements of modelling data) are available for application to the homomorphically encrypted transaction data. If first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) were to establish that one or more additional privately trained predictive-fraud models are available for application (e.g., step <b>414</b>; YES), exemplary process <b>400</b> may pass back to step <b>404</b>, and first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may perform any of the exemplary processes described herein to access modelling data that specifies an additional one of the privately-trained predictive models.</p><p id="p-0113" num="0112">Alternatively, if first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) were to establish that no further privately trained predictive-fraud models are available for application (e.g., step <b>414</b>; NO), first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may perform operations that package each element of the homomorphically encrypted output data into a polling response, and that transmit the polling response across network <b>120</b> to second computing system <b>202</b> (e.g., in step <b>416</b>). As described herein, second computing system <b>202</b> may receive the polling response, e.g., through a secure programmatic interface, and may perform any of the exemplary processes described herein to decrypt each element of the homomorphically encrypted output data using a homomorphic private cryptographic key, and determine whether the pending or initiated transaction based on all or a portion of the decrypted output data.</p><p id="p-0114" num="0113">Further, first computing system <b>102</b> (or third computing system <b>302</b> and/or one or more of the peer computing systems) may perform additional operations that package each of the encoded values into a corresponding portion of homomorphically encrypted proof data, and that transmit the homomorphically encrypted proof data across network <b>120</b> to second computing system <b>202</b> (e.g., in step <b>418</b>). As described herein, second computing system <b>202</b> may receive the homomorphically encrypted proof data, e.g., through a secure programmatic interface, and may perform any of the exemplary processes described herein verify the zero-knowledge proof associated with each of the encoded values based on actual transaction data characterizing the pending or initiated transaction. Exemplary process <b>400</b> is then complete in step <b>420</b>.</p><p id="p-0115" num="0114">Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification, including, but not limited to, training engine <b>112</b>, scaling module <b>121</b>, encryption module <b>216</b>, APIs <b>224</b>, <b>240</b>, and <b>316</b>, management modules <b>226</b> and <b>318</b>, local encryption modules <b>228</b> and <b>320</b>, homomorphic computation module <b>232</b> and <b>321</b>, verification modules <b>236</b> and <b>336</b>, decryption module <b>242</b>, can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non-transitory program carrier for execution by, or to control the operation of, a data processing apparatus (or a computer system).</p><p id="p-0116" num="0115">Additionally, or alternatively, the program instructions can be encoded on an artificially generated propagated signal, such as a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.</p><p id="p-0117" num="0116">The terms &#x201c;apparatus,&#x201d; &#x201c;device,&#x201d; and &#x201c;system&#x201d; refer to data processing hardware and encompass all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus, device, or system can also be or further include special purpose logic circuitry, such as an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus, device, or system can optionally include, in addition to hardware, code that creates an execution environment for computer programs, such as code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.</p><p id="p-0118" num="0117">A computer program, which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, such as one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, such as files that store one or more modules, sub-programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.</p><p id="p-0119" num="0118">The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, such as an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).</p><p id="p-0120" num="0119">Computers suitable for the execution of a computer program include, by way of example, general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random-access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, such as magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, such as a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a universal serial bus (USB) flash drive, to name just a few.</p><p id="p-0121" num="0120">Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.</p><p id="p-0122" num="0121">To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display unit, such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, such as a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser.</p><p id="p-0123" num="0122">Implementations of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, such as a data server, or that includes a middleware component, such as an application server, or that includes a front-end component, such as a computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, such as a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), such as the Internet.</p><p id="p-0124" num="0123">The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, a server transmits data, such as an HTML page, to a user device, such as for purposes of displaying data to and receiving user input from a user interacting with the user device, which acts as a client. Data generated at the user device, such as a result of the user interaction, can be received from the user device at the server.</p><p id="p-0125" num="0124">While this specification includes many specifics, these should not be construed as limitations on the scope of the invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of the invention. Certain features that are described in this specification in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment may also be implemented in multiple embodiments separately or in any suitable sub-combination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination may in some cases be excised from the combination, and the claimed combination may be directed to a sub-combination or variation of a sub-combination.</p><p id="p-0126" num="0125">Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems may generally be integrated together in a single software product or packaged into multiple software products.</p><p id="p-0127" num="0126">In each instance where an HTML file is mentioned, other file types or formats may be substituted. For instance, an HTML file may be replaced by an XML, JSON, plain text, or other types of files. Moreover, where a table or hash table is mentioned, other data structures (such as spreadsheets, relational databases, or structured files) may be used.</p><p id="p-0128" num="0127">Various embodiments have been described herein with reference to the accompanying drawings. It will, however, be evident that various modifications and changes may be made thereto, and additional embodiments may be implemented, without departing from the broader scope of the disclosed embodiments as set forth in the claims that follow.</p><p id="p-0129" num="0128">Further, other embodiments will be apparent to those skilled in the art from consideration of the specification and practice of one or more embodiments of the present disclosure. It is intended, therefore, that this disclosure and the examples herein be considered as exemplary only, with a true scope and spirit of the disclosed embodiments being indicated by the following listing of exemplary claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-01-20" num="01-20"><claim-text><b>1</b>-<b>20</b>. (canceled)</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. An apparatus, comprising:<claim-text>a communications interface;</claim-text><claim-text>a memory storing instructions; and</claim-text><claim-text>at least one processor coupled to the communications interface and the memory, the at least one processor being configured to execute the instructions to:<claim-text>receive encrypted information from a computing system via the communications interface, the encrypted information characterizing an exchange of data;</claim-text><claim-text>perform homomorphic computations on the encrypted information, the homomorphic computations comprising generating encrypted first output indicative of a predicted likelihood that the data exchange represents fraudulent activity based on an application of a first predictive model to the encrypted information; and</claim-text><claim-text>transmit the encrypted first output of the homomorphic computations to the computing system, the computing system being configured to decrypt the encrypted first output and generate decrypted output data indicative of the predicted likelihood that the data exchange represents fraudulent activity.</claim-text></claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The apparatus of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein:<claim-text>the at least one processor is further configured to execute the instructions to encrypt first modelling data associated with the first predictive model using a homomorphic public key;</claim-text><claim-text>the homomorphic computations further comprise applying the first predictive model to the encrypted information in accordance with the encrypted first modelling data; and</claim-text><claim-text>the computing system is further configured to decrypt the encrypted information using a homomorphic private key.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The apparatus of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the at least one processor is further configured to execute the instructions to receive the homomorphic encryption key and the encrypted information from the computing system via the communications interface.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The apparatus of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein:<claim-text>the encrypted information comprises confidential transaction data encrypted using the homomorphic public key; and</claim-text><claim-text>the computing system is further configured to access the confidential transaction data that characterizes the data exchange and encrypt the confidential transaction data using the homomorphic public key.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The apparatus of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein:<claim-text>the first modelling data comprises at least one of a model coefficient, a model parameter, or a threshold value associated with the first predictive model; and</claim-text><claim-text>the at least one processor is further configured to execute the instructions to encrypt the at least one of the model coefficient, the model parameter, or the threshold value using the homomorphic public key.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The apparatus of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the homomorphic computations further comprise applying the first predictive model to the encrypted information in accordance with the at least one of the encrypted model coefficient, the encrypted model parameter, or the encrypted threshold value.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The apparatus of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein:<claim-text>the homomorphic public key and the homomorphic private key are consistent with a homomorphic encryption scheme, the homomorphic encryption scheme supporting verifiable computations on integer ciphertext; and</claim-text><claim-text>the first modelling data comprises at least one of a scaled model coefficient, a scaled model parameter, or a scaled threshold value that are consistent with the verifiable computations on the integer ciphertext.</claim-text></claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The apparatus of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the at least one processor is further configured to:<claim-text>obtain second modelling data associated with a second predictive model and encrypt the second modelling data using the homomorphic public key;</claim-text><claim-text>perform additional homomorphic computations on the encrypted information, the additional homomorphic computations applying the second predictive model to the encrypted information in accordance with the encrypted second modelling data; and</claim-text><claim-text>transmit, via the communications interface, the encrypted first output and an encrypted second output of the additional homomorphic computations to the computing system, the second computing system being configured to decrypt the encrypted first and second outputs using the homomorphic private key.</claim-text></claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The apparatus of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the at least one processor is further configured to execute the instructions to:<claim-text>access one or more ledger blocks of a distributed ledger, the distributed ledger being maintained within the memory; and</claim-text><claim-text>extract at least a portion of the first modelling data from the one or more ledger blocks of the distributed ledger.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The apparatus of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the at least one processor is further configured to execute the instructions to:<claim-text>based on the encrypted first output, generate an encoded value representative of a zero-knowledge proof, the zero-knowledge proof confirming an accuracy of the homomorphic computations performed on the encrypted information; and</claim-text><claim-text>transmit the encoded value to the computing system via the communications interface, the computing system being configured to verify the zero-knowledge proof based on decrypted output data characterizing the data exchange.</claim-text></claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The apparatus of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the at least one processor is further configured to execute the instructions to transmit the encoded value to an additional computing system via the communications interface, the additional computing system being configured to perform operations that record the encoded value within a ledger block of a distributed ledger.</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The apparatus of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the decrypted output data comprises binary output data indicative of the predicted likelihood that the data exchange represents fraudulent activity.</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. A computer-implemented method, comprising:<claim-text>receiving, using at least one processor, encrypted information from a computing system, the encrypted information characterizing an exchange of data;</claim-text><claim-text>performing, using the at least one processor, homomorphic computations on the encrypted information, the homomorphic computations comprising generating encrypted first output indicative of a predicted likelihood that the data exchange represents fraudulent activity based on an application of a first predictive model to the encrypted information; and</claim-text><claim-text>transmitting, using the at least one processor, the encrypted first output of the homomorphic computations to the computing system, the computing system being configured to decrypt the encrypted first output and generate decrypted output data indicative of the predicted likelihood that the data exchange represents fraudulent activity.</claim-text></claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The computer-implemented method of <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein:<claim-text>the computer-implemented method further comprises encrypting, using the at least one processor, first modelling data associated with the first predictive model using a homomorphic public key;</claim-text><claim-text>the homomorphic computations further comprise applying the first predictive model to the encrypted information in accordance with the encrypted first modelling data; and</claim-text><claim-text>the computing system is further configured to decrypt the encrypted information using a homomorphic private key.</claim-text></claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The computer-implemented method of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the receiving comprises receiving the homomorphic encryption key and the encrypted information from the computing system.</claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. The computer-implemented method of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein:<claim-text>the first modelling data comprises at least one of a model coefficient, a model parameter, or a threshold value associated with the first predictive model; and</claim-text><claim-text>the encrypting further comprises encrypting the at least one of the model coefficient, the model parameter, or the threshold value using the homomorphic public key; and</claim-text><claim-text>the homomorphic computations further comprise applying the first predictive model to the encrypted information in accordance with the at least one of the encrypted model coefficient, the encrypted model parameter, or the encrypted threshold value.</claim-text></claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The computer-implemented method of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein:<claim-text>the homomorphic public key and the homomorphic private key are consistent with a homomorphic encryption scheme, the homomorphic encryption scheme supporting verifiable computations on integer ciphertext; and</claim-text><claim-text>the first modelling data comprises at least one of a scaled model coefficient, a scaled model parameter, or a scaled threshold value that are consistent with the verifiable computations on the integer ciphertext.</claim-text></claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. The computer-implemented method of <claim-ref idref="CLM-00034">claim 34</claim-ref>, further comprising:<claim-text>accessing, using the at least one processor, one or more ledger blocks of a distributed ledger, the distributed ledger being maintained within a data repository; and</claim-text><claim-text>extracting, using the at least one processor, at least a portion of the first modelling data from the one or more ledger blocks of the distributed ledger.</claim-text></claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. The computer-implemented method of <claim-ref idref="CLM-00033">claim 33</claim-ref>, further comprising:<claim-text>based on the encrypted first output, generating, using the at least one processor, an encoded value representative of a zero-knowledge proof, the zero-knowledge proof confirming an accuracy of the homomorphic computations performed on the encrypted information; and</claim-text><claim-text>transmitting, using the at least one processor, the encoded value to the computing system, the computing system being configured to verify the zero-knowledge proof based on decrypted output data characterizing the data exchange.</claim-text></claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. A tangible, non-transitory computer-readable medium storing instructions that, when executed by at least one processor, cause the at least one processor to perform a method, comprising:<claim-text>receiving encrypted information from a computing system, the encrypted information characterizing an exchange of data;</claim-text><claim-text>performing homomorphic computations on the encrypted information, the homomorphic computations comprising generating encrypted first output indicative of a predicted likelihood that the data exchange represents fraudulent activity based on an application of a first predictive model to the encrypted information; and</claim-text><claim-text>transmitting the encrypted first output of the homomorphic computations to the computing system, the computing system being configured to decrypt the encrypted first output and generate decrypted output data indicative of the predicted likelihood that the data exchange represents fraudulent activity.</claim-text></claim-text></claim></claims></us-patent-application>