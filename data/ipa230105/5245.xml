<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005246A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005246</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17853914</doc-number><date>20220630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>776</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>776</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30196</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">TRAINING METHOD OF NEURAL NETWORK MODEL AND ASSOCIATED DEVICE</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217769</doc-number><date>20210702</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Realtek Semiconductor Corp.</orgname><address><city>HsinChu</city><country>TW</country></address></addressbook><residence><country>TW</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Cheng</last-name><first-name>Chia-Chun</first-name><address><city>HsinChu</city><country>TW</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Realtek Semiconductor Corp.</orgname><role>03</role><address><city>HsinChu</city><country>TW</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present invention provides a training method of a neural network model, wherein the training method includes the steps of: receiving image data including a plurality of frames, and for first frames in the frames, the image data further includes detection data, and the detection data includes position of at least one person within the corresponding first frame; and for second frames in the frames, the image data further includes person search data, and the person search data includes position and serial number of at least one person within the corresponding second frame; using the neural network model to perform a person recognition operation on the frames to generate a recognition result; and using loss functions to process the recognition result of each frame, the detection result of each first frame and the person search data of each second frame, for adjusting parameters of the neural network model.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="86.78mm" wi="158.75mm" file="US20230005246A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="206.59mm" wi="137.41mm" orientation="landscape" file="US20230005246A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="210.99mm" wi="122.34mm" file="US20230005246A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="221.83mm" wi="135.38mm" orientation="landscape" file="US20230005246A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Application No. 63/217,769, filed on Jul. 2, 2021. The content of the application is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><heading id="h-0003" level="1">1. Field of the Invention</heading><p id="p-0003" num="0002">The present invention relates to a training method of a neural network model.</p><heading id="h-0004" level="1">2. Description of the Prior Art</heading><p id="p-0004" num="0003">Video understanding is currently widely used in many levels of society, for example, it can be used in remote video conferences, and person recognition or face recognition is an important technical content in the application of video understanding. In the operation of person recognition, deep learning or neural network methods are usually used to process each frame in the image data to identify whether there is a person in the image data, and even identify each person's identity. However, before performing the person identification through deep learning or neural network, the related neural network model needs to be trained to optimize the parameters in the model, and how to efficiently train the neural network model to get the most suitable parameters is an important topic.</p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0005" num="0004">It is therefore an objective of the present invention to provide a training method of a neural network model, which can form a set of image data from an image containing only the position information of each person and an image containing the position information and a serial number of each person for training the neural network model efficiently to obtain the most suitable parameters, to solve the above-mentioned problems.</p><p id="p-0006" num="0005">According to one embodiment of the present invention, a training method of a neural network model comprises the steps of: receiving image data, wherein the image data comprises a plurality of frames, and for a plurality of first frames in the plurality of frames, the image data further comprises detection data of each first frame, and the detection data comprises position of at least one person within the corresponding first frame; and for a plurality of second frames in the plurality of frames, the image data further comprises person search data of each second frame, and the person search data comprises position and serial number of at least one person within the corresponding second frame; using the neural network model to perform a person recognition operation on the plurality of frames to generate a recognition result; and using a plurality of loss functions to process the recognition result of each frame, the detection result of each first frame and the person search data of each second frame, for adjusting parameters of the neural network model.</p><p id="p-0007" num="0006">According to one embodiment of the present invention, a device comprising a person search module and a calculation and control module is disclosed. The person search module is configured to receive image data, and use a neural network model to perform a person recognition operation on the plurality of frames to generate a recognition result, wherein the image data comprises a plurality of frames, and for a plurality of first frames in the plurality of frames, the image data further comprises detection data of each first frame, and the detection data comprises position of at least one person within the corresponding first frame; and for a plurality of second frames in the plurality of frames, the image data further comprises person search data of each second frame, and the person search data comprises position and serial number of at least one person within the corresponding second frame. The calculation and control module is configured to use a plurality of loss functions to process the recognition result of each frame, the detection result of each first frame and the person search data of each second frame, for adjusting parameters of the neural network model.</p><p id="p-0008" num="0007">These and other objectives of the present invention will no doubt become obvious to those of ordinary skill in the art after reading the following detailed description of the preferred embodiment that is illustrated in the various figures and drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of a device for training a neural network model according to one embodiment of the present invention.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of a training method of the neural network model according to one embodiment of the present invention.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating detection data or person search data included in image data according to one embodiment of the present invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of a device <b>100</b> for training a neural network model according to one embodiment of the present invention, wherein the device <b>100</b> comprises a person search module <b>110</b> and a calculation and control module <b>120</b>, and the person search module <b>110</b> and the calculation and control module <b>120</b> can be implemented by hardware circuits. In this embodiment, the device <b>100</b> is used to train the neural network model in the person search module <b>110</b> to obtain optimal parameters. Specifically, the engineer will provide a lot of image data including person images to the person search module <b>110</b>, wherein the image data also include the position and/or serial number of the person in each frame of the image data. Then, the person search module <b>110</b> including a neural network model processes each frame in the image data, for example, the person search module <b>110</b> uses multiple different convolution filters to perform multiple convolution operations on the frame, so as to identify the position and characteristic value of the person in the frame. Then, the calculation and control module <b>120</b> compares position and characteristic value of the person in the frame generated by the person search module <b>110</b> with the position and/or the serial number of the person inputted by the engineer, to generate multiple loss functions for adjusting the parameters of multiple convolution filters of the neural network model included in the person search module <b>110</b>, to optimize the neural network model. It should be noted that since the main operations of the person search module <b>110</b> are well known to those skilled in the art, and the present invention focuses on the contents of the image data processed by the person search module <b>110</b> and the corresponding loss function, the following content does not describe the detailed operation of person search module <b>110</b>.</p><p id="p-0013" num="0012">In the operation of the device <b>100</b>, referring to the flowchart shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> together, in Step <b>200</b>, the process starts, and the device <b>100</b> is powered on and the initialization operation is completed. In Step <b>202</b>, the person search module <b>110</b> receives image data, wherein the image data includes a plurality of frames, such as <b>32</b> frames, and the image data further includes the detection data or person search data corresponding to each frame. In detail, referring to a frame <b>300</b> shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, assuming that the frame <b>300</b> includes three persons, the detection data corresponding to the frame <b>300</b> is the position information of the persons, such as the positions of the regions <b>310</b>, <b>320</b> and <b>330</b> shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>; and the person search data corresponding to the frame <b>300</b> not only includes the position information of the person (for example, the regions <b>310</b>, <b>320</b>, and <b>330</b>), but also includes the serial number of each person, wherein the serial number of the person is marked based on all person search data to distinguish whether it is the same person, wherein the same serial number means the same person, and different serial numbers correspond to different persons. For example, the persons in the regions <b>310</b>, <b>320</b>, and <b>330</b> are all different persons, so the serial number in the region <b>310</b> may be &#x2018;0&#x2019;, the serial number in the region <b>320</b> may be &#x2018;1&#x2019;, and the serial number in the region <b>330</b> may be &#x2018;2&#x2019;. In other words, the above-mentioned detection data can be regarded as a part of the person search data, that is, the detection data does not include the serial numbers of the persons in the regions <b>310</b>, <b>320</b>, and <b>330</b>. In addition, in this embodiment, each frame only corresponds to one of the detection data and the person search data. For example, in the image data, the first frame corresponds to the detection data, the second frame corresponds to the detection data, the third frame corresponds to the person search data, . . . etc.</p><p id="p-0014" num="0013">It should be noted that the detection data and person search data in the above-mentioned image data can be regarded as the correct content of the frame after being processed by the neural network model, that is, the detection data and the person search data include correct position information and correct serial number of the person in the frame.</p><p id="p-0015" num="0014">In Step <b>204</b>, the person search module <b>110</b> sequentially performs person recognition on each frame in the image data to generate a recognition result. In this embodiment, the recognition result includes the image classification, the position information of the person and the characteristic value of the person, wherein the position information of the person is similar to the regions <b>310</b>, <b>320</b>, and <b>330</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, and the characteristic value of the person is similar to the characteristic value of the image contents in the regions <b>310</b>, <b>320</b> and <b>330</b>. It should be noted that since the neural network model in the person search module <b>110</b> does not have optimal parameters, the recognition result of each frame is not exactly the same as the detection data/person search data.</p><p id="p-0016" num="0015">In Step <b>206</b>, the calculation and control module <b>120</b> uses a classification loss function to process the image classification in the recognition result to calculate a classification loss. In this embodiment, since this embodiment involves person recognition, the image classification is person, and the classification loss is used to indicate the accuracy with which the person search module <b>110</b> identifies that the frame comprises at least one person.</p><p id="p-0017" num="0016">In Step <b>208</b>, the device <b>100</b> has a characteristic value database that is recorded according to the serial number of each person, and the database is updated along with the training process. For the characteristic value of the person of the recognition result, the serial number of the person of the person search data and the characteristic value database, the calculation and control module <b>120</b> uses a loss function, such as contrastive loss function and triplet loss function, to calculate the difference of the characteristic values in the recognition result, to obtain a re-identification loss. Meanwhile, the characteristic values of the person are added into the characteristic value database with the serial number of the person. In this embodiment, since only some of the frames have person search data, the calculation and control module <b>120</b> only calculates the re-identification loss for the frames with person search data.</p><p id="p-0018" num="0017">In addition, for the frame with detection data, since the multiple persons in the same frame are not the same person, ideally, the characteristic values of the multiple persons in the same frame will have larger difference. Therefore, the calculation and control module <b>120</b> uses a triplet loss function to process multiple characteristic values in the same frame to obtain a triplet loss, wherein the triplet loss can reflect the difference between the characteristic values of the multiple persons in the same frame. It should be noted that, since the details of the triplet loss function are well known to those with ordinary knowledge in the art, the key point of Step <b>208</b> is to use the triplet loss function to process the characteristic value of multiple persons in the same frame, so the details of the triplet loss function are omitted here.</p><p id="p-0019" num="0018">In the operations of Step <b>204</b> and Step <b>208</b>, for the frame with detection data, the calculation and control module <b>120</b> only calculates the triplet loss; and for the frame with person search data, the calculation and control module <b>120</b> only calculates the re-identification loss. In other embodiments, however, for the frame with person search data, the calculation and control module <b>120</b> can calculate the re-identification loss and the triplet loss, and this alternative design should belong to the scope of the present invention.</p><p id="p-0020" num="0019">In Step <b>210</b>, the calculation and control module <b>120</b> uses a regression loss function to process the position of the person in the recognition result to calculate a regression loss. For example, the calculation and control module <b>120</b> may use the mean square error (MSE) and the mean absolute value error to calculate the difference between the position of the person in the recognition result with the position of the person in the detection data/person search data, to obtain the regression loss.</p><p id="p-0021" num="0020">In Step <b>212</b>, the calculation and control module <b>120</b> adjusts the parameters of the neural network model in the person search module <b>110</b> according to the classification loss, triplet loss, re-identification loss and regression loss to optimize the neural network models. For example, the calculation and control module <b>120</b> can adjust the parameters in the person search module <b>110</b> so that the class loss, re-identification loss and regression loss have lower values, and the triplet loss can reflect the large difference between the characteristic values of multiple persons in the same frame.</p><p id="p-0022" num="0021">Then, the flow goes back to Step <b>202</b>, and the person search module <b>110</b> receives next image data for similar processing.</p><p id="p-0023" num="0022">In the above embodiment, by mixing the frames with detection data and the frames with person search data in the image data, the device <b>100</b> can have generalization ability when training a neural network model. Furthermore, since the frames with detection data have a large number of samples, the device can have a lot of image data for training, so that the neural network model is more effective in optimization. In addition, by calculating the triplet loss for the frame with detection data, and calculating the re-identification loss for the frame with person search data, to adjust the parameters of the neural network model, the neural network model can be optimized more efficiently</p><p id="p-0024" num="0023">Those skilled in the art will readily observe that numerous modifications and alterations of the device and method may be made while retaining the teachings of the invention. Accordingly, the above disclosure should be construed as limited only by the metes and bounds of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A training method of a neural network model, comprising:<claim-text>receiving image data, wherein the image data comprises a plurality of frames, and for a plurality of first frames in the plurality of frames, the image data further comprises detection data of each first frame, and the detection data comprises position of at least one person within the corresponding first frame; and for a plurality of second frames in the plurality of frames, the image data further comprises person search data of each second frame, and the person search data comprises position and serial number of at least one person within the corresponding second frame;</claim-text><claim-text>using the neural network model to perform a person recognition operation on the plurality of frames to generate a recognition result; and</claim-text><claim-text>using a plurality of loss functions to process the recognition result of each frame, the detection result of each first frame and the person search data of each second frame, for adjusting parameters of the neural network model.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The training method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the detection data does not comprise any serial number of the person.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The training method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of at least a portion of the first frames comprises a plurality of persons.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The training method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of using the neural network model to perform the person recognition operation on the plurality of frames to generate the recognition result comprises:<claim-text>using the neural network model to perform the person recognition operation on the plurality of first frames to generate a plurality of first recognition results, respectively; and</claim-text><claim-text>using the neural network model to perform the person recognition operation on the plurality of second frames to generate a plurality of second recognition results, respectively; and</claim-text><claim-text>using the plurality of loss functions to process the recognition result of each frame, the detection result of each first frame and the person search data of each second frame, for adjusting the parameters of the neural network model comprises:</claim-text><claim-text>using a first loss function to process the plurality of first recognition results and the position of at least one person within the corresponding first frame to generate a triplet loss of each first frame;</claim-text><claim-text>using a second loss function to process the plurality of second recognition results and the serial number of at least one person within the corresponding second frame to generate a re-identification loss of each second frame; and</claim-text><claim-text>adjusting the parameters of the neural network model according to the triplet loss of each first frame and the re-identification loss of each second frame.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The training method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of using the neural network model to perform the person recognition operation on the plurality of frames to generate the recognition result comprises:<claim-text>using the neural network model to perform the person recognition operation on the plurality of first frames to generate a plurality of first recognition results, respectively; and</claim-text><claim-text>using the neural network model to perform the person recognition operation on the plurality of second frames to generate a plurality of second recognition results, respectively; and</claim-text><claim-text>using the plurality of loss functions to process the recognition result of each frame, the detection result of each first frame and the person search data of each second frame, for adjusting the parameters of the neural network model comprises:</claim-text><claim-text>using a first loss function to process the plurality of first recognition results and the position of at least one person within the corresponding first frame to generate a triplet loss of each first frame;</claim-text><claim-text>using the first loss function and a second loss function to process the plurality of second recognition results and the position and the serial number of at least one person within the corresponding second frame to generate the triplet loss of each second frame and a re-identification loss of each second frame; and</claim-text><claim-text>adjusting the parameters of the neural network model according to the triplet loss of each first frame, the triplet loss of each second frame and the re-identification loss of each second frame.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. A device, comprising:<claim-text>a person search module, configured to receive image data, and use a neural network model to perform a person recognition operation on the plurality of frames to generate a recognition result, wherein the image data comprises a plurality of frames, and for a plurality of first frames in the plurality of frames, the image data further comprises detection data of each first frame, and the detection data comprises position of at least one person within the corresponding first frame; and for a plurality of second frames in the plurality of frames, the image data further comprises person search data of each second frame, and the person search data comprises position and serial number of at least one person within the corresponding second frame; and</claim-text><claim-text>a calculation and control module, configured to use a plurality of loss functions to process the recognition result of each frame, the detection result of each first frame and the person search data of each second frame, for adjusting parameters of the neural network model.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The device of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the detection data does not comprise any serial number of the person.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The device of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein each of at least a portion of the first frames comprises a plurality of persons.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The device of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the person search module uses the neural network model to perform the person recognition operation on the plurality of first frames to generate a plurality of first recognition results, respectively, and uses the neural network model to perform the person recognition operation on the plurality of second frames to generate a plurality of second recognition results, respectively; and the calculation and control module uses a first loss function to process the plurality of first recognition results and the position of at least one person within the corresponding first frame to generate a triplet loss of each first frame, uses a second loss function to process the plurality of second recognition results and the serial number of at least one person within the corresponding second frame to generate a re-identification loss of each second frame, and adjusts the parameters of the neural network model according to the triplet loss of each first frame and the re-identification loss of each second frame.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The device of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the person search module uses the neural network model to perform the person recognition operation on the plurality of first frames to generate a plurality of first recognition results, respectively, and uses the neural network model to perform the person recognition operation on the plurality of second frames to generate a plurality of second recognition results, respectively; and the calculation and control module uses a first loss function to process the plurality of first recognition results and the position of at least one person within the corresponding first frame to generate a triplet loss of each first frame, uses the first loss function and a second loss function to process the plurality of second recognition results and the position and the serial number of at least one person within the corresponding second frame to generate the triplet loss of each second frame and a re-identification loss of each second frame, and adjusts the parameters of the neural network model according to the triplet loss of each first frame, the triplet loss of each second frame and the re-identification loss of each second frame.</claim-text></claim></claims></us-patent-application>