<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004793A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004793</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17446020</doc-number><date>20210826</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2021-0087202</doc-number><date>20210702</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0445</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">MULTI-MODAL FEW-SHOT LEARNING DEVICE FOR USER IDENTIFICATION USING WALKING PATTERN BASED ON DEEP LEARNING ENSEMBLE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Industry-Academic Cooperation Foundation, Dankook University</orgname><address><city>Yongin-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>CHOI</last-name><first-name>Sang-Il</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Industry-Academic Cooperation Foundation, Dankook University</orgname><role>03</role><address><city>Yongin-si</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Disclosed is multi-modal few-shot learning device for user identification using a walking pattern based on deep learning ensemble. The device includes: a walking data collector configured to collect walking data of a user from a smart insole including any one or more of a pressure sensor, an acceleration sensor, and a gyro sensor; a preprocessor configured to convert a series of time series walking data obtained from each of the sensors included in the smart insole into a unit format data set; and an ensemble learner configured to apply an ensemble learning model that provides one final prediction by training CNN series learning and RNN series learning respectively and independently based on the unit-format data set generated by the preprocessor.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="64.26mm" wi="87.38mm" file="US20230004793A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="212.34mm" wi="132.93mm" file="US20230004793A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="90.17mm" wi="106.51mm" file="US20230004793A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="179.66mm" wi="165.61mm" orientation="landscape" file="US20230004793A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="82.38mm" wi="89.41mm" file="US20230004793A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="219.79mm" wi="113.37mm" orientation="landscape" file="US20230004793A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="223.52mm" wi="109.98mm" orientation="landscape" file="US20230004793A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO PRIOR APPLICATION</heading><p id="p-0002" num="0001">This application claims priority to Korean Patent Application No. 10-2021-0087202 (filed on Jul. 2, 2021), which is hereby incorporated by reference in its entirety.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The present disclosure relates to a multi-modal few-shot learning device for user identification using a walking pattern based on deep learning ensemble, and more particularly, a multi-modal few-shot learning device for user identification using a walking pattern based on deep learning ensemble, the device which is capable of collecting a user's walking data from a smart insole and identifying the user based on an ensemble learning model.</p><p id="p-0004" num="0003">Walking is one of the typical behaviors of human beings, and analysis of a walking pattern contains a lot of information on a person's physical activities. Since the same type of normal walking exhibits different features depending on an individual, a walking pattern may also be used for purpose of biometrics such as face recognition and fingerprint recognition.</p><p id="p-0005" num="0004">In addition, even in the same walking motion, there are differences in pattern between walking on flat ground and walking on a hill, and these variations make it difficult to extract features so as to classify a walking type which is a &#x201c;step.&#x201d; Thus, a walking data feature extracting technology for identifying an individual using walking data in a situation where there is variation having being developed.</p><p id="p-0006" num="0005">Meanwhile, a walking type classifying system is composed of a sensor module for acquiring sensor data and an application module for calculating a classification result based on the acquired data. Sensors used for walking gait classification include a video sensor, an electromyoraphic (EMG) sensor, a plantar pressure sensor, an acceleration sensor, a gyro sensor, and the like.</p><p id="p-0007" num="0006">In addition, the recent development of wearable sensor technologies has led to weight reduction and simplification of equipment which is used to measure walking data. That is, various wearable devices such as smart watches, sports bands, and smart insoles have been being developed as sensor modules are miniaturized and low-power sensor technologies are developed. Since the use of wearable sensors has fewer environmental restrictions in collecting data, the data may be collected relatively easily in daily life, and, since such data has a small capacity compared to video data such as optical flow or heat map, there is an advantage in that data storage and processing are less burdensome.</p><p id="p-0008" num="0007">Meanwhile, techniques for collecting walking gait information using a smart insole provided with various types of sensors, performing neural network analysis for each sensor type, and classifying a walking type using the analyzed information have been proposed. However, there is a problem that such techniques derive a result including an error value rather than a desired result value due to meaningless walking data according to open set gait recognition data. Such a walking walking pattern classification technique of the related art has a problem in that there is a limit in terms of accuracy. In addition, there is a demand for development of a new device capable of increasing the accuracy of walking pattern classification.</p><heading id="h-0003" level="1">PRIOR ART LITERATURE</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0009" num="0008">Korea Patent No. 10-2175191 (Oct. 30, 2020)</p><p id="p-0010" num="0009">Korea Number No. 10-2061810 (Dec. 26, 2019)</p><p id="p-0011" num="0010">Korea Number No. 10-2194313 (Dec. 16, 2020)</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0012" num="0011">An aspect of the present disclosure provides a multi-modal few-shot learning device for user identification using a walking pattern based on deep learning ensemble, the device which is capable of collecting walking data of a user from a smart insole, recognizing and excluding meaningless walking information through a neural network trained with an ensemble learning model, and extracting only feature data that enables identification of the user.</p><p id="p-0013" num="0012">According to an aspect of the present disclosure, there is provided a multi-modal few-shot learning device for user identification using a walking pattern based on deep learning ensemble, and the device includes: a walking data collector configured to collect walking data of a user from a smart insole including any one or more of a pressure sensor, an acceleration sensor, and a gyro sensor; a preprocessor configured to convert a series of time series walking data obtained from each of the sensors included in the smart insole into a unit format data set; and an ensemble learner configured to apply an ensemble learning model that provides one final prediction by training CNN series learning and RNN series learning respectively and independently based on the unit-format data set generated by the preprocessor.</p><p id="p-0014" num="0013">According to embodiments of the present disclosure, the walking data collector may include any one or more of n pressure sensors, the acceleration sensor, and the gyro sensor included in the smart insole, and collects the walking data of the user measured from the sensors.</p><p id="p-0015" num="0014">According to embodiments of the present disclosure, the n pressure sensors each may measure a measurement level of foot pressure of both feet of the user is as 0, 1, or 2.</p><p id="p-0016" num="0015">According to embodiments of the present disclosure, the preprocessor may process a sampling rate of the smart insole to 100 Hz.</p><p id="p-0017" num="0016">According to embodiments of the present disclosure, the preprocessor may further include a unit vectorizer configured to vectorize a unit format of each series of time series walking data obtained from the pressure sensor, the acceleration sensor, and the gyro sensors included in the smart insole, a unit minimum length vectorizer configured to find data having a minimum length in unit-format vectorized data for each of the pressure sensor, the acceleration sensor, and the gyro sensors, and equalize a length of the unit-format vectorized data to the minimum length, and a unit vector set part configured to construct a minimum unit format data set from minimum unit format data equally processed to the minimum length.</p><p id="p-0018" num="0017">According to embodiments of the present disclosure, the unit vectorizer may perform a convolution operation using N pressure values and an average of a Gaussian function in order to vectorize the time series walking data in a unit format, and the convolution operation may be calculated according to</p><p id="p-0019" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mi>Z</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mi>t</mi>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mrow>     <mo>(</mo>     <mrow>      <msup>       <mrow>        <msup>         <mi>X</mi>         <mi>t</mi>        </msup>        <mo>(</mo>        <mi>t</mi>        <mo>)</mo>       </mrow>       <mo>*</mo>      </msup>      <mo>&#x2062;</mo>      <mi>y</mi>     </mrow>     <mo>)</mo>    </mrow>    <mo>&#x2062;</mo>    <mrow>     <mo>(</mo>     <mi>t</mi>     <mo>)</mo>    </mrow>   </mrow>   <mo>=</mo>   <mrow>    <msubsup>     <mo>&#x222b;</mo>     <mn>0</mn>     <mi>t</mi>    </msubsup>    <mrow>     <mi>X</mi>     <mstyle><mtext>?</mtext></mstyle>     <mrow>      <mo>(</mo>      <mi>&#x3c4;</mi>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mrow>      <mi>y</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>t</mi>       <mo>-</mo>       <mi>&#x3c4;</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mi>d</mi>     <mo>&#x2062;</mo>     <mi>r</mi>    </mrow>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0020" num="0000">(where <img id="CUSTOM-CHARACTER-00001" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates an average of N pressure values,</p><p id="p-0021" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <mi>y</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mi>t</mi>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <msqrt>     <mrow>      <mn>2</mn>      <mo>&#x2062;</mo>      <mi>&#x3c0;</mi>      <mo>&#x2062;</mo>      <mi>&#x3c3;</mi>     </mrow>    </msqrt>   </mfrac>   <mo>&#x2062;</mo>   <msup>    <mi>e</mi>    <mrow>     <mo>-</mo>     <mfrac>      <msup>       <mi>t</mi>       <mn>2</mn>      </msup>      <mn>2</mn>     </mfrac>    </mrow>   </msup>   <mstyle><mtext>?</mtext></mstyle>  </mrow> </mrow></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0022" num="0000">indicates the N pressure values, and &#x3c3;=0.2 s).</p><p id="p-0023" num="0018">According to embodiments of the present disclosure, in a case where a sorted list for each foot is [t<b>0</b>,t<b>1</b>, . . . , ti . . . ],</p><p id="p-0024" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mrow>   <mfrac>    <mi>d</mi>    <mi>dx</mi>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <mi>z</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mi>t</mi>    <mo>)</mo>   </mrow>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mn>0</mn>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>and</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mfrac>     <msup>      <mi>d</mi>      <mn>2</mn>     </msup>     <msup>      <mi>dt</mi>      <mn>2</mn>     </msup>    </mfrac>    <mo>&#x2062;</mo>    <mrow>     <mi>z</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mi>t</mi>     <mo>)</mo>    </mrow>   </mrow>   <mo>&#x3e;</mo>   <mn>0</mn>  </mrow> </mrow></math></maths></p><p id="p-0025" num="0000">are applied with respect to every time t, and <img id="CUSTOM-CHARACTER-00002" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> the unit vectorizer may define the time series walking data as a vectorized time series in a unit format <img id="CUSTOM-CHARACTER-00003" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> and a discontinuous variable may be calculated according to <img id="CUSTOM-CHARACTER-00004" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> because a sample speed of the insole is 100 Hz and a standard length is defined as <img id="CUSTOM-CHARACTER-00005" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/>.</p><p id="p-0026" num="0019">According to embodiments of the present disclosure, the ensemble learner may apply the ensemble learning model to a fully connected network and may include a CNN set constructor configured to construct a CNN series learning_vector_data set derived through the CNN series learning based on a minimum unit-format data set, and an RNN set constructor configured to construct an RNN series learning_vector_data set derived through the RNN series learning based on the minimum unit format data set.</p><p id="p-0027" num="0020">According to embodiments of the present disclosure, the ensemble learner may further include a CNN-RNN set constructor configured to, in a test stage, construct an average data vector set of the CNN-series learning_vector_data set and the RNN-series learning_vector_data set to construct a final walking data set for identifying the user from the average data vector set. According to embodiments of the present disclosure, the CNN-based learning and the RNN-based learning may be respectively and independently trained using the CNN-series learning_vector_data set and the RNN-series learning_vector_data set, and, in a test stage, an individual's softmax scores may be calculated by taking an average of soft max scores in CNN and RNN.</p><p id="p-0028" num="0021">According to embodiments of the present disclosure, the CNN series learning and the RNN series learning may be defined as <img id="CUSTOM-CHARACTER-00006" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> (for tri-modal sensing) where a unit step of <img id="CUSTOM-CHARACTER-00007" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> in a standard format, an acceleration <img id="CUSTOM-CHARACTER-00008" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> and rotation <img id="CUSTOM-CHARACTER-00009" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> is used as inputs an output of a model is a vector of a soft max probability u.</p><p id="p-0029" num="0022">According to embodiments of the present disclosure, the CNN series learning and the RNN series learning may construct an average ensemble model to aggregating CNN and RNN predictions and provide one final prediction, and an average probability of CNN and RNN may be calculated according to</p><p id="p-0030" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <mrow>   <mi>M</mi>   <mstyle><mtext>?</mtext></mstyle>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mn>2</mn>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <mo>(</mo>    <mrow>     <mrow>      <mi>M</mi>      <mstyle><mtext>?</mtext></mstyle>     </mrow>     <mo>+</mo>     <mrow>      <mi>M</mi>      <mstyle><mtext>?</mtext></mstyle>     </mrow>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00004-2" num="00004.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0031" num="0000">(where <img id="CUSTOM-CHARACTER-00010" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates a case where only CNN is activated, <img id="CUSTOM-CHARACTER-00011" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates a case where only RNN is activated, and <img id="CUSTOM-CHARACTER-00012" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates a case where CNN and RNN are all activated).</p><p id="p-0032" num="0023">According to embodiments of the present disclosure, the ensemble learning model may be composed of vectors of 128 units in a dimension of embedding the CNN-series learning_vector_data set or the RNN-series learning_vector_data set, a CNN-based learning_vector and an RNN-based learning_vector may be connected to a fully connected network layer to form embedding vectors of 256 units, and the embedding vectors may be normalized to a same value.</p><p id="p-0033" num="0024">According to embodiments of the present disclosure, the device may further include an output part configured to output, through the network trained with the ensemble learning model, walking feature data of the user from each unit format data set obtained from the sensors so as to identify (authenticate) the user from the walking data.</p><p id="p-0034" num="0025">According to the multi-modal few-shot learning device for user identification using a walking pattern based on a deep learning ensemble as described above, the following effects are obtained.</p><p id="p-0035" num="0026">First, by recognizing and excluding meaningless walking information data through a neural network trained with an ensemble learning model, it is possible to extract only feature data that enables identification of a user.</p><p id="p-0036" num="0027">Second, by extracting the user's walking feature data without the meaningless information, it is possible to improve a probability of identifying the user.</p><p id="p-0037" num="0028">Third, the learning effect is enhanced by using an ensemble model network that connects CNN and RNN networks.</p><p id="p-0038" num="0029">Fourth, it is possible to extract feature walking data from walking data that enables identification of an individual through walking data in a situation where there is variation.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0039" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of the present disclosure.</p><p id="p-0040" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of a preprocessor according to an embodiment of the present disclosure.</p><p id="p-0041" num="0032"><figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref> are a block diagram and a schematic diagram of an ensemble learner, according to an embodiment of the present disclosure.</p><p id="p-0042" num="0033"><figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref> are a block diagram and a schematic diagram of a few-shot learner according to an embodiment of the present disclosure.</p><p id="p-0043" num="0034"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a graph showing a comparison of learning results of an ensemble model, a CNN model, and an RNN model according to an embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0044" num="0035">A multi-modal few-shot learning device for user identification using a walking pattern based on deep learning ensemble according to embodiments of the present disclosure will be described in detail with reference to the accompanying drawings. The present disclosure may make various changes and have various forms, and specific embodiments will be illustrated in drawings and will be described in detail in the present specification. However, these are not intended to limit the present disclosure to specific disclosure forms, and it should be understood that the present disclosure includes all changes, equivalents, or substitutions falling within the spirit and scope of the present disclosure. In describing the drawings, like reference numerals are used for like elements. In the accompanying drawings, the dimensions of the structures might be shown exaggerated for clarity of the disclosure or abridged for a schematic representation of the configurations of some embodiments.</p><p id="p-0045" num="0036">Also, the terms such as &#x201c;first&#x201d; and &#x201c;second&#x201d; may be used to describe various components, but those components should not be limited by the terms. The terms are merely used to distinguish one component from other components. For example, without departing from the scope of the present disclosure, a first component may be referred to as a second component, and similarly, a second component may also be referred to as a first component. Meanwhile, unless otherwise defined, all terms used herein, including technical or scientific terms, have the same meaning as commonly understood by those of ordinary skill in the art to which the present disclosure belongs. Unless otherwise defined, all terms, including technical and scientific terms, commonly used and defined in dictionaries are to be interpreted as is customary in the art to which the present disclosure belongs. It will be further understood that terms in common usage should also be interpreted as is customary in the related art and not in an idealized or overly formal sense unless expressly so defined herein.</p><p id="p-0046" num="0037"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of the present disclosure. Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a multi-modal few-shot learning device for user identification using a walking pattern may include a walking data collector <b>100</b>, a preprocessor <b>200</b>, an ensemble learner <b>300</b>, a few-shot identifier <b>400</b> and an output part <b>500</b>.</p><p id="p-0047" num="0038">In one embodiment of the present disclosure, the walking data collector <b>100</b> collects a user's walking data from a smart insole including any one or more of a pressure sensor, an acceleration sensor, and a gyro sensor, and collects the user's walking data measured by n pressure sensors, acceleration sensors, and gyro sensors included in the smart insole. In addition, in one embodiment of the present disclosure, the n pressure sensors measure a measurement level of foot pressure of both feet of the user as 0 or 1 or 2, and a sampling rate of the smart insole may be 100 Hz.</p><p id="p-0048" num="0039">In addition, in one embodiment of the present disclosure, the preprocessor <b>200</b> converts a series of time series walking data obtained from each of the sensors included in the smart insole into a unit data set, and the ensemble learner <b>300</b> applies an ensemble learning model that provides one final prediction by training CNN series learning and RNN series learning respectively and independently based on the unit-format data set generated by the preprocessor <b>200</b>. In addition, the few-shot identifier <b>400</b> according to an embodiment of the present disclosure recognizes and excludes walking data having inadequate information (few-shot data) related to the user's walking data. Lastly, the output part <b>500</b> outputs a walking data feature for identifying the user, by using a value output from a fully connected network learned through the ensemble learner <b>300</b>. That is, through the network which has learned the ensemble learning model for identifying (authenticating) the user from the walking data, the user's walking feature data is extracted from each of the unit data set obtained from the sensors. In more detail, in order to accurately extract feature data related to the user with a small amount of walking data related to the user, a function of the few-shot identifier <b>400</b> for discriminating whether the collected walking data is walking data related to the user or walking data related to a non-user and a function of the few-shot identifier <b>400</b> for learning the ensemble model in the network by inputting the walking data related to the user as an input may be performed in parallel.</p><p id="p-0049" num="0040"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of a preprocessor according to an embodiment of the present disclosure.</p><p id="p-0050" num="0041">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the preprocessor <b>200</b> may include a unit vectorizer <b>205</b>, a unit minimum length vectorizer <b>210</b>, and a unit vector set part <b>215</b>.</p><p id="p-0051" num="0042">In more detail, the unit vectorizer <b>205</b> vectorizes a unit format of a series of time series walking data obtained from the pressure sensor, the acceleration sensor, and the gyro sensors included in the smart insole, and the unit minimum length vectorizer <b>210</b> finds data having a minimum length from among the unit-format vectorized data for each of the pressure sensor, the acceleration sensor, and the gyro sensor, and equalizes a length of the unit-format vectorized data to a minimum length. Next, the unit vector set part <b>215</b> may construct a minimum unit format data set according to the pressure sensor, the acceleration sensor, or the gyro sensor from a series of unit format data having the minimum length.</p><p id="p-0052" num="0043">That is, in an optimal embodiment (best mode), the unit vectorizer <b>205</b> performs a convolution operation using N pressure values and an average of a Gaussian function to vectorize the time series walking data in a unit format. The convolution operation is calculated as</p><p id="p-0053" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mrow>  <mrow>   <mi>Z</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mi>t</mi>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mrow>     <mo>(</mo>     <mrow>      <mi>X</mi>      <mstyle><mtext>?</mtext></mstyle>      <msup>       <mrow>        <mo>(</mo>        <mi>t</mi>        <mo>)</mo>       </mrow>       <mo>*</mo>      </msup>      <mo>&#x2062;</mo>      <mi>y</mi>     </mrow>     <mo>)</mo>    </mrow>    <mo>&#x2062;</mo>    <mrow>     <mo>(</mo>     <mi>t</mi>     <mo>)</mo>    </mrow>   </mrow>   <mo>=</mo>   <mrow>    <msubsup>     <mo>&#x222b;</mo>     <mn>0</mn>     <mi>t</mi>    </msubsup>    <mrow>     <mi>X</mi>     <mstyle><mtext>?</mtext></mstyle>     <mrow>      <mo>(</mo>      <mi>&#x3c4;</mi>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mrow>      <mi>y</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>t</mi>       <mo>-</mo>       <mi>&#x3c4;</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mi>d</mi>     <mo>&#x2062;</mo>     <mi>r</mi>    </mrow>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00005-2" num="00005.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0054" num="0000">(where <img id="CUSTOM-CHARACTER-00013" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates an average of N pressure values,</p><p id="p-0055" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mrow>  <mrow>   <mi>y</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mi>t</mi>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <msqrt>     <mrow>      <mn>2</mn>      <mo>&#x2062;</mo>      <mi>&#x3c0;</mi>      <mo>&#x2062;</mo>      <mi>&#x3c3;</mi>     </mrow>    </msqrt>   </mfrac>   <mo>&#x2062;</mo>   <msup>    <mi>e</mi>    <mrow>     <mo>-</mo>     <mfrac>      <msup>       <mi>t</mi>       <mn>2</mn>      </msup>      <mn>2</mn>     </mfrac>    </mrow>   </msup>   <mstyle><mtext>?</mtext></mstyle>  </mrow> </mrow></math></maths><maths id="MATH-US-00006-2" num="00006.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0056" num="0000">indicates N pressure values, and <img id="CUSTOM-CHARACTER-00014" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/>). In addition, in a case where a sorted list for each foot is [t<b>0</b>, t<b>1</b> . . . , ti . . . ],</p><p id="p-0057" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mrow>  <mrow>   <mfrac>    <mi>d</mi>    <mrow>     <mi>d</mi>     <mstyle><mtext>?</mtext></mstyle>    </mrow>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <mi>z</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mi>t</mi>    <mo>)</mo>   </mrow>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mn>0</mn>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>and</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mfrac>     <msup>      <mi>d</mi>      <mn>2</mn>     </msup>     <msup>      <mi>dt</mi>      <mn>2</mn>     </msup>    </mfrac>    <mo>&#x2062;</mo>    <mrow>     <mi>z</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mi>t</mi>     <mo>)</mo>    </mrow>   </mrow>   <mo>&#x3e;</mo>   <mn>0</mn>  </mrow> </mrow></math></maths><maths id="MATH-US-00007-2" num="00007.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0058" num="0000">are applied with respect to every time t, and <img id="CUSTOM-CHARACTER-00015" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> the time series walking data is defined as a vectorized time series in a unit format <img id="CUSTOM-CHARACTER-00016" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> (where <img id="CUSTOM-CHARACTER-00017" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/>). Since a sampling rate of the insole is 100 Hz and a standard length is defined as <img id="CUSTOM-CHARACTER-00018" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/>, a discontinuous variable is calculated according to <img id="CUSTOM-CHARACTER-00019" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> (where <img id="CUSTOM-CHARACTER-00020" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates a unit step of two feet of every participant).</p><p id="p-0059" num="0044"><figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref> are a block diagram and a schematic diagram of an ensemble learner, according to an embodiment of the present disclosure. Referring to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>, the ensemble learner <b>300</b> may include a CNN set constructor <b>305</b>, an RNN set constructor <b>310</b>, and a CNN-RNN set constructor <b>315</b>.</p><p id="p-0060" num="0045">In more detail, in order to accurately extract a user's walking feature data, big data related to the user's walking data is required. However, there is a limitation in learning CNN or RNN by collecting in advance the walking data according to the user's environmental condition. Therefore, through CNN learning and RNN learning only with a small amount of the walking data related to the user, the network is learned by applying an ensemble model, so that feature data of walking data related to the user can be derived for each environmental condition with various changes.</p><p id="p-0061" num="0046">In more detail, according to an embodiment of the present disclosure, the ensemble learner <b>300</b> applies and learns an ensemble learning model to a fully connected network, and the CNN set constructor <b>305</b> constructs a CNN-series learning_vector_data set derived through the CNN series learning based on a minimum unit format data set generated by the preprocess <b>200</b>. The RNN set constructor <b>310</b> constructs an RNN-series learning_vector_data set derived through the RNN series learning based on the minimum unit format data set. In addition, the CNN-RNN set constructor <b>310</b> constructs an average data vector set of the CNN-series learning_vector_data set and the RNN-series learning_vector_data set in a test stage to construct a final walking data set for identifying the user from the average data vector set.</p><p id="p-0062" num="0047">In an optimal embodiment (best mode) of the present disclosure, the CNN series learning and RNN series learning are respectively and independently trained using the CNN-series learning_vector_data set and the RNN-series learning_vector_data set, and, in a test stage, an individual's softmax scores are calculated by taking an average of the softmax scores in CNN and RNN. That is, the CNN series learning or the RNN series learning is defined as <img id="CUSTOM-CHARACTER-00021" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> (for tri-modal sensing) where an input is a unit step of <img id="CUSTOM-CHARACTER-00022" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> in a standard format, an acceleration <img id="CUSTOM-CHARACTER-00023" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/>, and rotation <img id="CUSTOM-CHARACTER-00024" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> and an output of a model is a soft max probability u. In addition, the CNN series learning and the RNN series learning aggregate CNN and RNN predictions, construct an average ensemble model to provide one final prediction, and calculate an average probability of CNN and RNN according to</p><p id="p-0063" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mrow>  <mrow>   <mi>M</mi>   <mstyle><mtext>?</mtext></mstyle>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mn>2</mn>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <mo>(</mo>    <mrow>     <mrow>      <mi>M</mi>      <mstyle><mtext>?</mtext></mstyle>     </mrow>     <mo>+</mo>     <mrow>      <mi>M</mi>      <mstyle><mtext>?</mtext></mstyle>     </mrow>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00008-2" num="00008.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0064" num="0000">(where <img id="CUSTOM-CHARACTER-00025" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates a case where only CNN is activated, <img id="CUSTOM-CHARACTER-00026" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates a case where only RNN is activated, and <img id="CUSTOM-CHARACTER-00027" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates a case where CNN and RNN are all activated). In addition, the ensemble learning model is composed of vectors of 128 units in a dimension of embedding the CNN-series learning_vector_data set or the RNN-series learning_vector_data set, and a CNN series learning_vector or a RNN series learning_vector is connected to a fully connected network layer to form embedding vectors of 256 units, and the embedding vectors are normalized to the same value.</p><p id="p-0065" num="0048"><figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref> are a block diagram and a schematic diagram of a few-shot learner according to an embodiment of the present disclosure. Referring to <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref>, the few-shot identifier <b>400</b> includes a few-shot learner <b>405</b>.</p><p id="p-0066" num="0049">In embodiments of the present disclosure, the few-shot identifier <b>400</b> includes a few-shot learner that utilizes a Support Vector Machine (SVM) to exclude walking data irrelevant to the user's walking data based on an inadequate information (few-shot data) set related to the user's walking data.</p><p id="p-0067" num="0050">Here, the few-shot learner <b>405</b> includes the inadequate information (few-shot data) set related to the user's walking data, and the inadequate information (few-shot data) set related to the user's walking data includes data which is not walking data (unknown known data) and non-user data (unknown unknown data) in user-related data. Therefore, the few-shot learner <b>400</b> utilizes the SVM to exclude inadequate information (few-shot data) sets related to the user's walking data and the non-user data (unknown unknown data).</p><p id="p-0068" num="0051">In an optimal embodiment of the few-shot learner <b>405</b>, the few-shot learner sets a vector set <img id="CUSTOM-CHARACTER-00028" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> in a unit format randomly selected from the non-user data (unknown unknown data) (where <img id="CUSTOM-CHARACTER-00029" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/>) and calculates a center of n embedding vectors from an embedding vector set <img id="CUSTOM-CHARACTER-00030" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> generated by any one model of CNN and RNN networks (where the center of the embedding vector is</p><p id="p-0069" num="0000"><maths id="MATH-US-00009" num="00009"><math overflow="scroll"> <mrow>  <mrow>   <mi>M</mi>   <mstyle><mtext>?</mtext></mstyle>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mi>n</mi>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>n</mi>    </munderover>    <mrow>     <mi>V</mi>     <mstyle><mtext>?</mtext></mstyle>    </mrow>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00009-2" num="00009.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0070" num="0000">). The center of the embedding vectors and {V<sub>i,a</sub>|1&#x2264;i&#x2264;n} are input conditions, and, in order to solve the optimization,</p><p id="p-0071" num="0000"><maths id="MATH-US-00010" num="00010"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>min</mi>    <mi>o</mi>   </msub>   <mo>=</mo>   <mrow>    <mfrac>     <mn>1</mn>     <mn>2</mn>    </mfrac>    <mo>&#x2062;</mo>    <mrow>     <mo>&#x2211;</mo>     <mrow>      <mstyle><mtext>?</mtext></mstyle>      <mrow>       <mo>&#x2211;</mo>       <mstyle><mtext>?</mtext></mstyle>      </mrow>     </mrow>    </mrow>   </mrow>  </mrow>  <mo>,</mo>  <mrow>   <msub>    <mi>&#x3b1;</mi>    <mi>i</mi>   </msub>   <mo>&#x2062;</mo>   <msub>    <mi>&#x3b1;</mi>    <mi>i</mi>   </msub>  </mrow> </mrow></math></maths><maths id="MATH-US-00010-2" num="00010.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0072" num="0000">and K(<img id="CUSTOM-CHARACTER-00031" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/>) are applied to the SVM (where</p><p id="p-0073" num="0000"><maths id="MATH-US-00011" num="00011"><math overflow="scroll"> <mrow>  <mrow>   <mn>0</mn>   <mo>&#x2264;</mo>   <msub>    <mi>&#x3b1;</mi>    <mi>i</mi>   </msub>   <mo>&#x2264;</mo>   <mfrac>    <mn>1</mn>    <mstyle><mtext>?</mtext></mstyle>   </mfrac>  </mrow>  <mo>,</mo>  <mrow>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>n</mi>    </munderover>    <msub>     <mi>&#x3b1;</mi>     <mi>i</mi>    </msub>   </mrow>   <mo>=</mo>   <mn>1</mn>  </mrow>  <mo>,</mo> </mrow></math></maths><maths id="MATH-US-00011-2" num="00011.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0074" num="0000">K(V,V&#x2032;)=<img id="CUSTOM-CHARACTER-00032" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates a radial bias kernel function, indicates a Lagrange multiplier, and <img id="CUSTOM-CHARACTER-00033" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> and V indicate hyperparameters).</p><p id="p-0075" num="0052"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a graph showing a comparison of learning results of an ensemble model, a CNN model, and an RNN model according to an embodiment of the present disclosure. Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a distribution of ACC as a function of &#x3b3; and V for CNN, RNN, and ensemble models is shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. Selection of &#x3b3; and V is an important condition for the overall recognition accuracy. <figref idref="DRAWINGS">FIG. <b>7</b></figref> according to an optimal embodiment of the present disclosure shows a comparison of regions (light green to yellow regions) that accounts for 90% or more in area and, shows that an area of the ensemble model is wider than areas of the CNN and RNN. This indicates that the ensemble model has a weak dependence when selecting &#x3b3; and V, which affects robustness of a recognition result.</p><p id="p-0076" num="0053">In embodiments of the present disclosure, a method for the multi-modal few-shot learning device for user identification using a walking pattern based on a deep learning ensemble includes collecting a user's walking data from a smart insole including any one or more of a pressure sensor, an acceleration sensor, and a gyro sensor in operation, converting a series of time series walking data obtained from each of the sensors included in the smart insole into each unit format data set in operation, training a network based on an ensemble learning model for extracting the user's features based on the unit format data set in operation, recognizing and excluding walking data having information irrelevant to the user in operation, and extracting user feature walking data to identify the user from the walking data through the network trained with the ensemble learning model in operation.</p><p id="p-0077" num="0054">According to the multi-modal few-shot learning device for user identification using a walking pattern based on deep learning ensemble as described above, the following effects are obtained. First, by recognizing and excluding meaningless walking information data through a neural network trained with an ensemble learning model, it is possible to extract only feature data that enables identification of a user. Second, by extracting the user's walking feature data without the meaningless information, it is possible to improve a probability of identifying the user. Third, the learning effect is enhanced by using an ensemble model network that connects CNN and RNN networks. Fourth, it is possible to extract feature walking data from walking data that enables identification of an individual through walking data in a situation where there is variation.</p><p id="p-0078" num="0055">Although the preferred embodiments of the present disclosure have been disclosed for illustrative purposes, those skilled in the art will appreciate that various modifications, additions and substitutions are possible, without departing from the scope and spirit of the invention as disclosed in the accompanying claims. Therefore, the above descriptions and the attached drawings should be interpreted as exemplifying the present disclosure rather than limiting the spirit of the present disclosure.</p><p id="p-0079" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>[Detailed Description of Main Elements]</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="105pt" align="left"/><colspec colname="2" colwidth="112pt" align="left"/><tbody valign="top"><row><entry>100: walking data collector</entry><entry>200: preprocessor</entry></row><row><entry>205: unit vectorizer</entry><entry>210: unit minimum length vectorizer</entry></row><row><entry>215: unit vector set part</entry><entry>300: Ensemble learner</entry></row><row><entry>305: CNN set constructor</entry><entry>310: RNN set constructor</entry></row><row><entry>315: CNN-RNN set constructor</entry><entry>400: few-shot identifier</entry></row><row><entry>405: few-shot learner</entry><entry>500: output part</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230004793A1-20230105-M00001.NB"><img id="EMI-M00001" he="10.24mm" wi="76.20mm" file="US20230004793A1-20230105-M00001.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US20230004793A1-20230105-M00002.NB"><img id="EMI-M00002" he="11.26mm" wi="76.20mm" file="US20230004793A1-20230105-M00002.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230004793A1-20230105-M00003.NB"><img id="EMI-M00003" he="6.35mm" wi="76.20mm" file="US20230004793A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004 MATH-US-00004-2" nb-file="US20230004793A1-20230105-M00004.NB"><img id="EMI-M00004" he="9.48mm" wi="76.20mm" file="US20230004793A1-20230105-M00004.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005 MATH-US-00005-2" nb-file="US20230004793A1-20230105-M00005.NB"><img id="EMI-M00005" he="10.24mm" wi="76.20mm" file="US20230004793A1-20230105-M00005.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006 MATH-US-00006-2" nb-file="US20230004793A1-20230105-M00006.NB"><img id="EMI-M00006" he="11.26mm" wi="76.20mm" file="US20230004793A1-20230105-M00006.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007 MATH-US-00007-2" nb-file="US20230004793A1-20230105-M00007.NB"><img id="EMI-M00007" he="10.92mm" wi="76.20mm" file="US20230004793A1-20230105-M00007.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008 MATH-US-00008-2" nb-file="US20230004793A1-20230105-M00008.NB"><img id="EMI-M00008" he="9.48mm" wi="76.20mm" file="US20230004793A1-20230105-M00008.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00009 MATH-US-00009-2" nb-file="US20230004793A1-20230105-M00009.NB"><img id="EMI-M00009" he="12.02mm" wi="76.20mm" file="US20230004793A1-20230105-M00009.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00010 MATH-US-00010-2" nb-file="US20230004793A1-20230105-M00010.NB"><img id="EMI-M00010" he="9.48mm" wi="76.20mm" file="US20230004793A1-20230105-M00010.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00011 MATH-US-00011-2" nb-file="US20230004793A1-20230105-M00011.NB"><img id="EMI-M00011" he="12.02mm" wi="76.20mm" file="US20230004793A1-20230105-M00011.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00012 MATH-US-00012-2" nb-file="US20230004793A1-20230105-M00012.NB"><img id="EMI-M00012" he="10.24mm" wi="76.20mm" file="US20230004793A1-20230105-M00012.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00013 MATH-US-00013-2" nb-file="US20230004793A1-20230105-M00013.NB"><img id="EMI-M00013" he="11.26mm" wi="76.20mm" file="US20230004793A1-20230105-M00013.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00014 MATH-US-00014-2" nb-file="US20230004793A1-20230105-M00014.NB"><img id="EMI-M00014" he="10.92mm" wi="76.20mm" file="US20230004793A1-20230105-M00014.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00015 MATH-US-00015-2" nb-file="US20230004793A1-20230105-M00015.NB"><img id="EMI-M00015" he="9.48mm" wi="76.20mm" file="US20230004793A1-20230105-M00015.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A multi-modal few-shot learning device for user identification using a walking pattern based on deep learning ensemble, the device comprising:<claim-text>a walking data collector configured to collect walking data of a user from a smart insole including any one or more of a pressure sensor, an acceleration sensor, and a gyro sensor;</claim-text><claim-text>a preprocessor configured to convert a series of time series walking data obtained from each of the sensors included in the smart insole into a unit format data set; and</claim-text><claim-text>an ensemble learner configured to apply an ensemble learning model that provides one final prediction by training CNN series learning and RNN series learning respectively and independently based on the unit-format data set generated by the preprocessor.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the walking data collector comprises any one or more of n pressure sensors, the acceleration sensor, and the gyro sensor included in the smart insole, and collects the walking data of the user measured from the sensors.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The device of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the n pressure sensors each measure a measurement level of foot pressure of both feet of the user is as 0, 1, or 2.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the preprocessor processes a sampling rate of the smart insole to 100 Hz.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the preprocessor further comprises:<claim-text>a unit vectorizer configured to vectorize a unit format of each series of time series walking data obtained from the pressure sensor, the acceleration sensor, and the gyro sensors included in the smart insole;</claim-text><claim-text>a unit minimum length vectorizer configured to find data having a minimum length in unit-format vectorized data for each of the pressure sensor, the acceleration sensor, and the gyro sensors, and equalize a length of the unit-format vectorized data to the minimum length; and</claim-text><claim-text>a unit vector set part configured to construct a minimum unit format data set from minimum unit format data equally processed to the minimum length.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The device of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the unit vectorizer performs a convolution operation using N pressure values and an average of a Gaussian function in order to vectorize the time series walking data in a unit format,<claim-text>wherein the convolution operation is calculated according to</claim-text></claim-text><claim-text><maths id="MATH-US-00012" num="00012"><math overflow="scroll"> <mrow>  <mrow>   <mi>Z</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mi>t</mi>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mrow>     <mo>(</mo>     <mrow>      <mi>X</mi>      <mstyle><mtext>?</mtext></mstyle>      <msup>       <mrow>        <mo>(</mo>        <mi>t</mi>        <mo>)</mo>       </mrow>       <mo>*</mo>      </msup>      <mo>&#x2062;</mo>      <mi>y</mi>     </mrow>     <mo>)</mo>    </mrow>    <mo>&#x2062;</mo>    <mrow>     <mo>(</mo>     <mi>t</mi>     <mo>)</mo>    </mrow>   </mrow>   <mo>=</mo>   <mrow>    <msubsup>     <mo>&#x222b;</mo>     <mn>0</mn>     <mi>t</mi>    </msubsup>    <mrow>     <mi>X</mi>     <mstyle><mtext>?</mtext></mstyle>     <mrow>      <mo>(</mo>      <mi>&#x3c4;</mi>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mrow>      <mi>y</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>t</mi>       <mo>-</mo>       <mi>&#x3c4;</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mi>d</mi>     <mo>&#x2062;</mo>     <mi>r</mi>    </mrow>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00012-2" num="00012.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></claim-text><claim-text>(where <img id="CUSTOM-CHARACTER-00034" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates an average of N pressure values,</claim-text><claim-text><maths id="MATH-US-00013" num="00013"><math overflow="scroll"> <mrow>  <mrow>   <mi>y</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mi>t</mi>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <msqrt>     <mrow>      <mn>2</mn>      <mo>&#x2062;</mo>      <mi>&#x3c0;</mi>      <mo>&#x2062;</mo>      <mi>&#x3c3;</mi>     </mrow>    </msqrt>   </mfrac>   <mo>&#x2062;</mo>   <msup>    <mi>e</mi>    <mrow>     <mo>-</mo>     <mfrac>      <msup>       <mi>t</mi>       <mn>2</mn>      </msup>      <mn>2</mn>     </mfrac>    </mrow>   </msup>   <mstyle><mtext>?</mtext></mstyle>  </mrow> </mrow></math></maths><maths id="MATH-US-00013-2" num="00013.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></claim-text><claim-text>indicates the N pressure values, and <img id="CUSTOM-CHARACTER-00035" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/>).</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The device of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein, in a case where a sorted list for each foot is [t<b>0</b>, t<b>1</b>, . . . , ti . . . ],</claim-text><claim-text><maths id="MATH-US-00014" num="00014"><math overflow="scroll"> <mrow>  <mrow>   <mfrac>    <mi>d</mi>    <mrow>     <mi>d</mi>     <mstyle><mtext>?</mtext></mstyle>    </mrow>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <mi>z</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mi>t</mi>    <mo>)</mo>   </mrow>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mn>0</mn>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>and</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mfrac>     <msup>      <mi>d</mi>      <mn>2</mn>     </msup>     <msup>      <mi>dt</mi>      <mn>2</mn>     </msup>    </mfrac>    <mo>&#x2062;</mo>    <mrow>     <mi>z</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mi>t</mi>     <mo>)</mo>    </mrow>   </mrow>   <mo>&#x3e;</mo>   <mn>0</mn>  </mrow> </mrow></math></maths><maths id="MATH-US-00014-2" num="00014.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></claim-text><claim-text>are applied with respect to every time t, and <img id="CUSTOM-CHARACTER-00036" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> the unit vectorizer defines the time series walking data as a vectorized time series in a unit format <img id="CUSTOM-CHARACTER-00037" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/><claim-text>wherein a discontinuous variable is calculated according to <img id="CUSTOM-CHARACTER-00038" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> because a sample speed of the insole is 100 Hz and a standard length is defined as <img id="CUSTOM-CHARACTER-00039" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/>.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ensemble learner applies the ensemble learning model to a fully connected network and comprises:<claim-text>a CNN set constructor configured to construct a CNN series learning_vector_data set derived through the CNN series learning based on a minimum unit-format data set; and</claim-text><claim-text>an RNN set constructor configured to construct an RNN series learning_vector_data set derived through the RNN series learning based on the minimum unit format data set.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the ensemble learner further comprises a CNN-RNN set constructor configured to, in a test stage, construct an average data vector set of the CNN-series learning_vector_data set and the RNN-series learning_vector_data set to construct a final walking data set for identifying the user from the average data vector set</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the CNN series learning and the RNN series learning are respectively and independently trained using the CNN-series learning_vector_data set and the RNN-series learning_vector_data set, and<claim-text>wherein, in a test stage, an individual's softmax scores are calculated by taking an average of soft max scores in CNN and RNN.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the CNN series learning or the RNN series learning is defined as <img id="CUSTOM-CHARACTER-00040" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> (for tri-modal sensing) where a unit step of <img id="CUSTOM-CHARACTER-00041" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> in a standard format, an acceleration <img id="CUSTOM-CHARACTER-00042" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/>, and rotation <img id="CUSTOM-CHARACTER-00043" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> is used as inputs an output of a model is a vector of a soft max probability u.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the CNN series learning and the RNN series learning construct an average ensemble model to aggregating CNN and RNN predictions and provide one final prediction, and an average probability of CNN and RNN is calculated according to</claim-text><claim-text><maths id="MATH-US-00015" num="00015"><math overflow="scroll"> <mrow>  <mrow>   <mi>M</mi>   <mstyle><mtext>?</mtext></mstyle>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mn>2</mn>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <mo>(</mo>    <mrow>     <mrow>      <mi>M</mi>      <mstyle><mtext>?</mtext></mstyle>     </mrow>     <mo>+</mo>     <mrow>      <mi>M</mi>      <mstyle><mtext>?</mtext></mstyle>     </mrow>    </mrow>    <mo>)</mo>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00015-2" num="00015.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></claim-text><claim-text>(where <img id="CUSTOM-CHARACTER-00044" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates a case where only CNN is activated, <img id="CUSTOM-CHARACTER-00045" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates a case where only RNN is activated, and <img id="CUSTOM-CHARACTER-00046" he="2.96mm" wi="53.51mm" file="US20230004793A1-20230105-P00999.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> indicates a case where CNN and RNN are all activated).</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the ensemble learning model is composed of vectors of <b>128</b> units in a dimension of embedding the CNN-series learning_vector_data set or the RNN-series learning_vector_data set,<claim-text>wherein a CNN-series learning_vector or an RNN-series learning_vector is connected to a fully connected network layer to form embedding vectors of 256 units, and</claim-text><claim-text>wherein the embedding vectors are normalized to a same value.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>an output part configured to output, through the network trained with the ensemble learning model, walking feature data of the user from each unit format data set obtained from the sensors so as to identify (authenticate) the user from the walking data.</claim-text></claim-text></claim></claims></us-patent-application>