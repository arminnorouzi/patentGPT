<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005171A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005171</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17865260</doc-number><date>20220714</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202111147751.2</doc-number><date>20210929</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>564</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>564</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20116</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">VISUAL POSITIONING METHOD, RELATED APPARATUS AND COMPUTER PROGRAM PRODUCT</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Beijing Baidu Netcom Science Technology Co., Ltd.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>YANG</last-name><first-name>Wei</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>YE</last-name><first-name>Xiaoqing</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>TAN</last-name><first-name>Xiao</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>SUN</last-name><first-name>Hao</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A visual positioning method and apparatus, an electronic device, a computer readable storage medium, and a computer program product are provided. The method includes: performing contour enhancement processing on an actual building image included in a image for positioning to obtain an actual building contour, and determining location information of a target building matching the actual building contour from a preset contour map, the contour map being obtained by blurring non-building contour information in a real panoramic map, and finally generating a visual positioning result based on the location information.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="68.07mm" wi="114.89mm" file="US20230005171A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="186.86mm" wi="116.92mm" file="US20230005171A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="229.11mm" wi="131.57mm" file="US20230005171A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="229.79mm" wi="88.31mm" file="US20230005171A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="168.40mm" wi="107.61mm" file="US20230005171A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">The present application claims the priority of Chinese Patent Application No. 202111147751.2, titled &#x201c;VISUAL POSITIONING METHOD, RELATED APPARATUS AND COMPUTER PROGRAM PRODUCT&#x201d;, filed on Sep. 29, 2021, the content of which is incorporated herein by reference in its entirety.</p><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to the field of computer technology, in particular to the fields of artificial intelligence technologies such as computer vision and deep learning, may be applied to visual positioning and three-dimensional visual scenarios, and more particular relates to a visual positioning method and apparatus, an electronic device, a computer readable storage medium, and a computer program product.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In order to better present scenario information to users, so that the users may acquire actual information in the scenarios and acquire navigation services based on the actual information, in existing technologies, more and more panoramic maps are used to provide related services to the users.</p><p id="p-0005" num="0004">Panoramic map scenarios contain a large number of artificial geometric contours and textures, so they provide relatively good conditions for visual positioning tasks.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">Embodiments of the present disclosure propose a visual positioning method and apparatus, an electronic device, a computer readable storage medium, and a computer program product.</p><p id="p-0007" num="0006">Some embodiments of the present disclosure provide a visual positioning method, including: performing contour enhancement processing on an actual building image included in an image for positioning to obtain an actual building contour; determining location information of a target building matching the actual building contour from a preset contour map, the contour map being obtained by blurring non-building contour information in a real panoramic map; and generating a visual positioning result based on the location information.</p><p id="p-0008" num="0007">Some embodiments of the present disclosure provide a visual positioning apparatus, including: an actual building contour acquiring unit, configured to perform contour enhancement processing on an actual building image included in an image for positioning to obtain an actual building contour; a location information determining unit, configured to determine location information of a target building matching the actual building contour from a preset contour map, the contour map being obtained by blurring non-building contour information in a real panoramic map; and a visual positioning result generating unit, configured to generate a visual positioning result based on the location information.</p><p id="p-0009" num="0008">Some embodiments of the present disclosure provide a non-transitory computer readable storage medium storing computer instructions, where the computer instructions are used to cause the computer to perform the above visual positioning method.</p><p id="p-0010" num="0009">It should be understood that the content described in this section is not intended to identify key or important features of the embodiments of the present disclosure, nor is it intended to limit the scope of the present disclosure. Other features of the present disclosure will become readily understood from the following specification.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010">Other features, objectives and advantages of the present disclosure will become more apparent upon reading the detailed description of non-limiting embodiment with reference to the following accompanying drawings.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an exemplary system architecture to which the present disclosure may be applied;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of a visual positioning method according to an embodiment of the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of another visual positioning method according to an embodiment of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIGS. <b>4</b>-<b>1</b>, <b>4</b>-<b>2</b>, <b>4</b>-<b>3</b>, and <b>4</b>-<b>4</b></figref> are schematic diagrams of effects of the visual positioning method in an application scenario according to an embodiment of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a structural block diagram of a visual positioning apparatus according to an embodiment of the present disclosure; and</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic structural diagram of an electronic device suitable for performing the visual positioning method according to an embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading><p id="p-0018" num="0017">Example embodiments of the present disclosure are described below with reference to the accompanying drawings, where various details of the embodiments of the present disclosure are included to facilitate understanding, and should be considered merely as examples. Therefore, those of ordinary skills in the art should realize that various changes and modifications can be made to the embodiments described herein without departing from the scope and spirit of the present disclosure. Similarly, for clearness and conciseness, descriptions of well-known functions and structures are omitted in the following description. It should be noted that the embodiments of the present disclosure and features of the embodiments may be combined with each other on a non-conflict basis.</p><p id="p-0019" num="0018">In addition, in the technical solution of the present disclosure, the acquisition, storage, use, processing, transmission, provision and disclosure of the user personal information involved are all in compliance with the relevant laws and regulations, and do not violate public order and good customs.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an exemplary system architecture <b>100</b> to which embodiments of a visual positioning method and apparatus, an electronic device, and a computer readable storage medium of the present disclosure may be applied.</p><p id="p-0021" num="0020">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the system architecture <b>100</b> may include terminal devices <b>101</b>, <b>102</b>, <b>103</b>, a network <b>104</b> and a server <b>105</b>. The network <b>104</b> serves as a medium for providing a communication link between the terminal devices <b>101</b>, <b>102</b>, <b>103</b> and the server <b>105</b>. The network <b>104</b> may include various types of connections, such as wired or wireless communication links, or optical fiber cables.</p><p id="p-0022" num="0021">A user may use the terminal devices <b>101</b>, <b>102</b>, and <b>103</b> to interact with the server <b>105</b> through the network <b>104</b> to receive or send messages and the like. Various applications for implementing information communication between the terminal devices <b>101</b>, <b>102</b>, <b>103</b> and the server <b>105</b> may be installed on the terminal devices <b>101</b>, <b>102</b>, <b>103</b> and the server <b>105</b>, such as visual positioning applications, image matching applications, or instant messaging applications.</p><p id="p-0023" num="0022">The terminal devices <b>101</b>, <b>102</b>, and <b>103</b> may be hardware or software. When the terminal devices <b>101</b>, <b>102</b>, and <b>103</b> are hardware, they may be electronic devices having display screens, including but not limited to smart phones, tablet computers, laptop computers and desktop computers, etc.; when the terminal devices <b>101</b>, <b>102</b>, and <b>103</b> are software, they may be installed in the above-listed electronic devices. They may be implemented as a plurality of software or software modules, and may also be implemented as a single software or software module, which is not limited herein. When the server <b>105</b> is hardware, it may be implemented as a distributed server cluster composed of a plurality of servers, or may be implemented as a single server; when the server is software, it may be implemented as a plurality of software or software modules, or may be implemented as a single software or software module, which is not limited herein.</p><p id="p-0024" num="0023">The server <b>105</b> may provide various services through various built-in applications. Taking a visual positioning application that can provide visual positioning services as an example, the server <b>105</b> may achieve the following effects when running the visual positioning application: first, acquiring an image for positioning from the terminal devices <b>101</b>, <b>102</b>, <b>103</b> through the network <b>104</b>, and performing contour enhancement processing on an actual building image included in the image for positioning to obtain an actual building contour; then, the server <b>105</b> determines location information of a target building matching the actual building contour from a preset contour map, the contour map being obtained by blurring non-building contour information in a real panoramic map; and finally, the server <b>105</b> generates a visual positioning result based on the location information.</p><p id="p-0025" num="0024">It should be noted that the image for positioning may be pre-stored locally in the server <b>105</b> in various methods in addition to being acquired from the terminal devices <b>101</b>, <b>102</b> and <b>103</b> through the network <b>104</b>. Therefore, when the server <b>105</b> detects that such data is already stored locally (for example, a to-be-processed visual positioning task remained before starting processing), it may choose to acquire the data directly from the local, and in this case, the exemplary system architecture <b>100</b> may also exclude the terminal devices <b>101</b>, <b>102</b>, <b>103</b> and the network <b>104</b>.</p><p id="p-0026" num="0025">Since storing the contour map, performing contour enhancement processing on content in the image, and matching between contours require lots of computing resources and strong computing power, the visual positioning method provided by subsequent embodiments of the present disclosure is generally executed by the server <b>105</b> with strong computing power and more computing resources, correspondingly, the visual positioning apparatus is generally also provided in the server <b>105</b>. But at the same time, it should be noted that when the terminal devices <b>101</b>, <b>102</b>, and <b>103</b> also have computing power and computing resources that meet the requirements, the terminal devices <b>101</b>, <b>102</b>, and <b>103</b> may also complete the above various operations performed by the server <b>105</b> through the visual positioning applications installed thereon, and further output the same results as the server <b>105</b>. Especially when there are simultaneously a plurality of terminal devices with different computing powers, and the terminal devices where the visual positioning applications are judge that they have strong computing powers and more computing resources left, the terminal devices may be allowed to perform the above operations, so as to appropriately reduce the computing pressure of the server <b>105</b>, correspondingly, the visual positioning apparatus may also be provided in the terminal devices <b>101</b>, <b>102</b> and <b>103</b>. In this case, the exemplary system architecture <b>100</b> may also exclude the server <b>105</b> and the network <b>104</b>.</p><p id="p-0027" num="0026">It should be appreciated that the number of terminal devices, networks and servers in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is merely illustrative. Any number of terminal devices, networks and servers may be provided depending on the implementation needs.</p><p id="p-0028" num="0027">With reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of a visual positioning method according to an embodiment of the present disclosure, where a flow <b>200</b> includes the following steps.</p><p id="p-0029" num="0028">Step <b>201</b>, performing contour enhancement processing on an actual building image included in an image for positioning to obtain an actual building contour.</p><p id="p-0030" num="0029">In the present embodiment, after acquiring the image for positioning, an executing body of the visual positioning method (for example, the server <b>105</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) may perform contour enhancement processing on the actual building image included in the image for positioning to obtain the actual building contour. Generally, a contour of an actual building image in the image for positioning may be recognized by using a radial basis function (abbreviated as RBF) neural network, a back propagation (abbreviated as BP) multi-layer feedforward neural network or other neural networks, and after a recognition result is acquired, the contour enhancement processing is performed on the contour of the actual building image to obtain the actual building contour.</p><p id="p-0031" num="0030">In practice, the contour of the actual building image included in the image for positioning may also be highlighted by sharpening content in the image for positioning or adjusting a contrast of the image for positioning, so as to achieve a purpose of enhancing and extracting the actual building contour.</p><p id="p-0032" num="0031">It should be noted that the image for positioning may be acquired by the executing body directly from a local storage device, or may be acquired from a non-local storage device (for example, the terminal devices <b>101</b>, <b>102</b>, and <b>103</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>). The local storage device may be a data storage module module in the executing body, such as a server hard disk. In this case, the image for positioning may be quickly read locally. The non-local storage device may also be any other electronic device configured to store data, such as some user terminals. In this case, the executing body may acquire the required image for positioning by sending an acquisition command to the electronic device.</p><p id="p-0033" num="0032">Step <b>202</b>, determining location information of a target building matching the actual building contour from a preset contour map.</p><p id="p-0034" num="0033">In the present embodiment, after the actual building contour is obtained based on the above step <b>201</b>, the preset contour map may be called, and the contour map is obtained by blurring non-building contour information in a real panoramic map. The real panoramic map is an image formed by de-duplicating and splicing images obtained by photographing a real scenario. The real panoramic map corresponds to the real scenario. Therefore, the real panoramic map is also called a 360-degree panoramic map, a panoramic look-around map, etc. After acquiring the contour map, the actual building contour obtained based on the above step <b>201</b> is matched with content in the contour map to determine the target building, recorded in the contour map, that matches the actual building contour, and determine the location information of the target building from the contour map.</p><p id="p-0035" num="0034">After acquiring the content in the real panoramic map, the content (features of each content) included in the real panoramic map is identified, and then resolution-reducing, blurring and other processing are performed on the content to blur a contour of non-building content in the real panoramic map, to obtain the contour map. Preferably, the content in the real panoramic map is blurred to an extent that only contour information belonging to a building can be extracted from the obtained contour map, so as to reduce interference of content other than the building, reduce features in the contour map that are used to provide visual positioning for the image for positioning, and avoid dis-matching with the image for positioning due to too many features in the contour map.</p><p id="p-0036" num="0035">Step <b>203</b>, generating a visual positioning result based on the location information.</p><p id="p-0037" num="0036">In the present embodiment, after the location information of the target building is determined from the contour map, a corresponding visual positioning result is generated, so as to determine a photographing location of the image for positioning based on the visual positioning result.</p><p id="p-0038" num="0037">In the visual positioning method provided by an embodiment of the present disclosure, the contour map obtained by blurring non-building contour information in the real panoramic map may be used to provide a visual positioning service, which reduces resolution requirements for the uploaded image for positioning in the visual positioning process, so that the matching and positioning work may still be completed in the case of poor quality or few features of the image uploaded by a user.</p><p id="p-0039" num="0038">In some optional implementations of the present embodiment, the performing contour enhancement processing on an actual building image included in an image for positioning to obtain an actual building contour, includes: extracting the actual building image included in the image for positioning; improving a contrast of an edge of an actual building in the actual building image by sharpening to obtain a sharpened image; and extracting contour information corresponding to the actual building in the sharpened image to generate the actual building contour.</p><p id="p-0040" num="0039">In particular, after the image for positioning is acquired, the actual building image included in the image for positioning may be extracted, and the contrast of the edge of the actual building in the actual building image may be improved by sharpening. Sharpening, also known as image sharpening, is a processing method for compensating a contour of an image, enhancing an edge and grayscale transition part of an image, making the image clear. Sharpening may be divided into spatial domain processing and frequency domain processing. Image sharpening is intended to highlight the edges, contours of ground objects, or features of some linear-target features on images. This filtering method improves a contrast between a ground object edge and surrounding pixels. After improving the contrast of the edge of the actual building in the actual building image, the sharpened image may be obtained, and the actual building contour may be generated from the contour information corresponding to the actual building in the sharpened image. In this method, after the image for positioning is sharpened, the actual building contour may be acquired from the image for positioning more accurately, quickly and conveniently, thereby reducing a difficulty of acquiring the actual building contour.</p><p id="p-0041" num="0040">In some optional implementations of the present embodiment, the visual positioning method further includes: generating photographing pose information of the image for positioning based on difference information between the actual building contour and a standard contour of the target building.</p><p id="p-0042" num="0041">In particular, after the visual positioning result is generated based on the location information, a building corresponding to the actual building contour is determined as the target building, and the standard contour of the target building is extracted. The standard contour may be determined based on content such as the a real panoramic map photographed in advance, and the standard contour may be used to indicate a contour obtained by photographing in a standard pose.</p><p id="p-0043" num="0042">After comparing the standard contour with the actual building contour, the difference information between the standard contour and the actual building contour is obtained. For example, the difference information may be an angle between the actual building contour and the standard contour belonging to the same spatial point, or location. After the difference information is acquired, pose information such as a photographing angle when acquiring the image for positioning may be determined based on the difference information, so as to improve the quality of visual positioning.</p><p id="p-0044" num="0043">With reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart for generating a contour map, in a visual positioning method according to an embodiment of the present disclosure, where a flow <b>300</b> includes the following steps.</p><p id="p-0045" num="0044">Step <b>301</b>, acquiring a real panoramic map, and determining a reference building image included in the real panoramic map.</p><p id="p-0046" num="0045">In the present embodiment, after the real panoramic map is acquired, content included in the real panoramic map may be identified, and the building image included in the real panoramic map may be determined.</p><p id="p-0047" num="0046">Step <b>302</b>, performing Gaussian blurring on the real panoramic map to obtain a blurred panoramic map.</p><p id="p-0048" num="0047">In the present embodiment, a Gaussian convolution kernel may be used to perform blurring on the real panoramic map to obtain the blurred panoramic map.</p><p id="p-0049" num="0048">In some optional implementations of the present embodiment, a size of the corresponding Gaussian convolution kernel may also be pre-determined based on resolution requirements for the blurred panoramic map obtained after blurring, and corresponding processing templates may be set corresponding to the sizes of the Gaussian convolution kernels, so as to call a corresponding configuration of the corresponding Gaussian convolution kernel by calling the processing template, and the real panoramic map is processed to obtain the blurred panoramic map of the corresponding resolution.</p><p id="p-0050" num="0049">Further, when generating the contour map based on the blurred panoramic map subsequently, one of the blurred panoramic maps with a resolution may be selected according to an actual configuration requirement, so as to obtain the contour map corresponding to the resolution. Here, preferably, contour maps with respective clarities are generated corresponding to the blurred panoramic maps with the respective clarities. After that, based on the image for visual positioning input by the user, a contour map with the closest resolution may be selected based on the resolution of the image, so as to balance between the quality of the visual positioning service and completion of the visual positioning service.</p><p id="p-0051" num="0050">Step <b>303</b>, extracting contour information in the blurred panoramic map, and generating the contour map based on contour information corresponding to the reference building image in the blurred panoramic map.</p><p id="p-0052" num="0051">In the present embodiment, after obtaining the blurred panoramic map based on the above step <b>302</b>, a feature extractor may be used to process the blurred panoramic map to extract features included in the blurred panoramic map. After obtaining the features, contour features belonging to a reference building are kept, the contour information corresponding to the reference building image is generated, and the contour information of each reference building existing in the blurred panoramic map is summarized to form the complete contour map.</p><p id="p-0053" num="0052">In some optional implementations of the present embodiment, the extracting contour information in the blurred panoramic map, and generating the contour map based on contour information corresponding to the reference building image in the blurred panoramic map, includes: performing edge extraction on the blurred panoramic map to obtain a binarized edge panoramic map that only includes an edge portion defined as 1 and a non-edge portion defined as 0; and generating the contour map by multiplying feature information corresponding to the reference building image in a form of a matrix and the binarized edge map.</p><p id="p-0054" num="0053">Specifically, after extracting the features included in the blurred panoramic map, pixels corresponding to the features are marked as 1, and the rest is marked as 0, to obtain the binarized edge panoramic map, and then the feature information, corresponding to the reference building image, in the form of a matrix is used to multiply the binarized edge panoramic map, to complete the extraction of the contour information belonging to the reference building image in the binarized edge panoramic map, and finally generate the contour map. The extraction of the contour belonging to the reference building image in the binarized edge panoramic map is completed in combination with binarization, which reduces an error effect caused by the use of the blurred panoramic map for contour analysis alone, and improves the quality of the generated contour map.</p><p id="p-0055" num="0054">In the present embodiment, the real panoramic map may be blurred based on Gaussian blurring to obtain the blurred panoramic map, and other noise contours may not be additionally generated without affecting the contour information of the original reference building image, to ensure the quality of the generated blurred panoramic map.</p><p id="p-0056" num="0055">In order to deepen understanding, the present disclosure also provides an implementation scheme in combination with an application scenario, which is specifically described as follows.</p><p id="p-0057" num="0056">First, after acquiring a real panoramic map I<sub>i</sub>, in which some images in the real panoramic map may be as shown in <figref idref="DRAWINGS">FIG. <b>4</b>-<b>1</b></figref>, the real panoramic map I<sub>i </sub>is blurred by using a Gaussian convolution kernel with a fixed template size to obtain a blurred panoramic map I<sub>i</sub><sup>blur </sup>and a feature extractor F<sub>extractor </sub>is used to perform feature extraction on the blurred panoramic map after blurring, and feature information F<sub>extractor</sub>(I<sub>i</sub><sup>blur</sup>) belonging to a reference building image is determined from obtained features.</p><p id="p-0058" num="0057">Next, edge extraction is performed on the blurred panoramic map I<sub>i</sub><sup>blur</sup>, to obtain a binarized edge panoramic map I<sub>i</sub><sup>countor </sup>that only includes an edge portion defined as 1 and a non-edge portion defined as 0, and I<sub>i</sub><sup>countor </sup>is used to multiply F<sub>extractor</sub>(I<sub>i</sub><sup>blur</sup>) to obtain a contour map f<sub>i</sub><sup>counter </sup>that only contains a contour corresponding to the reference building image. In the contour map, the content corresponding to the part shown in <figref idref="DRAWINGS">FIG. <b>4</b>-<b>1</b></figref> above may be as shown in <figref idref="DRAWINGS">FIG. <b>4</b>-<b>2</b></figref>.</p><p id="p-0059" num="0058">Further, after a image for positioning I<sub>query </sub>is acquired, the image for positioning may be as shown in <figref idref="DRAWINGS">FIG. <b>4</b>-<b>3</b></figref>, contour enhancement processing is performed on an actual building image included in the image for positioning I<sub>query </sub>to obtain an actual building contour I<sub>query</sub><sup>sharp</sup>, and the actual building contour may be as shown in <figref idref="DRAWINGS">FIG. <b>4</b>-<b>4</b></figref>. Further, the actual building contour is input into the contour map f<sub>i</sub><sup>counter </sup>for matching, to determine location information of a target building matching the actual building contour.</p><p id="p-0060" num="0059">With further reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, as an implementation of the method shown in the above figures, the present disclosure provides an embodiment of a visual positioning apparatus. The apparatus embodiment corresponds to the method embodiment as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The apparatus may be applied to various electronic devices.</p><p id="p-0061" num="0060">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the visual positioning apparatus <b>500</b> of the present embodiment may include: an actual building contour acquiring unit <b>501</b>, a location information determining unit <b>502</b>, and a visual positioning result generating unit <b>503</b>. The actual building contour acquiring unit <b>501</b> is configured to perform contour enhancement processing on an actual building image included in a image for positioning to obtain an actual building contour. The location information determining unit <b>502</b> is configured to determine location information of a target building matching the actual building contour from a preset contour map, the contour map being obtained by blurring non-building contour information in a real panoramic map. The visual positioning result generating unit <b>503</b> is configured to generate a visual positioning result based on the location information.</p><p id="p-0062" num="0061">In the present embodiment, in the visual positioning apparatus <b>500</b>: for specific processing and technical effects of the actual building contour acquiring unit <b>501</b>, the location information determining unit <b>502</b>, and the visual positioning result generating unit <b>503</b>, reference may be made to the relevant descriptions of steps <b>201</b>-<b>203</b> in the corresponding embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, respectively, and detailed description thereof will be omitted.</p><p id="p-0063" num="0062">In some alternative implementations of the present embodiment, the apparatus further includes a contour map generating unit configured to generate the contour map, and the contour map generating unit includes: a reference building image determining subunit, configured to acquire the real panoramic map, and determine a reference building image included in the real panoramic map; a blurred panoramic map generating subunit, configured to perform Gaussian blurring on the real panoramic map to obtain a blurred panoramic map; and a contour map generating subunit, configured to extract contour information in the blurred panoramic map, and generate the contour map based on contour information corresponding to the reference building image in the blurred panoramic map.</p><p id="p-0064" num="0063">In some alternative implementations of the present embodiment, the blurred panoramic map generating subunit is further configured to: perform Gaussian blurring on the real panoramic map by using Gaussian convolution kernels of different sizes respectively, to obtain blurred panoramic maps of different blurring degrees correspondingly.</p><p id="p-0065" num="0064">In some alternative implementations of the present embodiment, the contour map generating subunit includes: a binarization processing module, configured to perform edge extraction on the blurred panoramic map to obtain a binarized edge panoramic map that only includes an edge portion defined as 1 and a non-edge portion defined as 0; and a contour map generating module, configured to generate the contour map by multiplying feature information corresponding to the reference building image in a form of a matrix and the binarized edge map.</p><p id="p-0066" num="0065">In some alternative implementations of the present embodiment, the actual building contour acquiring unit includes: an actual building image extracting subunit, configured to extract the actual building image included in the image for positioning; a sharpened image generating subunit, configured to improve a contrast of an edge of an actual building in the actual building image by sharpening to obtain a sharpened image; and an actual building contour generating subunit, configured to extract contour information corresponding to the actual building in the sharpened image to generate the actual building contour.</p><p id="p-0067" num="0066">In some alternative implementations of the present embodiment, the visual positioning apparatus, further includes: a pose information generating unit, configured to generate photographing pose information of the image for positioning based on difference information between the actual building contour and a standard contour of the target building.</p><p id="p-0068" num="0067">The present embodiment serves as the apparatus embodiment corresponding to the foregoing method embodiment, in the visual positioning apparatus provided by the present embodiment, the contour map obtained by blurring non-building contour information in the real panoramic map may be used to provide a visual positioning service, which reduces resolution requirements for the uploaded image for positioning in the visual positioning process, so that the matching and positioning work may still be completed in the case of poor image quality or few features uploaded by a user.</p><p id="p-0069" num="0068">According to an embodiment of the present disclosure, the present disclosure also provides an electronic device, a readable storage medium, and a computer program product.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a schematic block diagram of an example electronic device <b>600</b> that may be used to implement embodiments of the present disclosure. The electronic device is intended to represent various forms of digital computers, such as laptop computers, desktop computers, workbenches, personal digital assistants, servers, blade servers, mainframe computers, and other suitable computers. The electronic device may also represent various forms of mobile apparatuses, such as personal digital processors, cellular phones, smart phones, wearable devices, and other similar computing apparatuses. The components shown herein, their connections and relationships, and their functions are merely examples, and are not intended to limit the implementation of the present disclosure described and/or claimed herein.</p><p id="p-0071" num="0070">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the device <b>600</b> includes a computing unit <b>601</b>, which may perform various appropriate actions and processing, based on a computer program stored in a read-only memory (ROM) <b>602</b> or a computer program loaded from a storage unit <b>608</b> into a random access memory (RAM) <b>603</b>. In the RAM <b>603</b>, various programs and data required for the operation of the device <b>600</b> may also be stored. The computing unit <b>601</b>, the ROM <b>602</b>, and the RAM <b>603</b> are connected to each other through a bus <b>604</b>. An input/output (I/O) interface <b>605</b> is also connected to the bus <b>604</b>.</p><p id="p-0072" num="0071">A plurality of parts in the device <b>600</b> are connected to the I/O interface <b>605</b>, including: an input unit <b>606</b>, for example, a keyboard and a mouse; an output unit <b>607</b>, for example, various types of displays and speakers; the storage unit <b>608</b>, for example, a disk and an optical disk; and a communication unit <b>606</b>, for example, a network card, a modem, or a wireless communication transceiver. The communication unit <b>606</b> allows the device <b>600</b> to exchange information/data with other devices over a computer network such as the Internet and/or various telecommunication networks.</p><p id="p-0073" num="0072">The computing unit <b>601</b> may be various general-purpose and/or dedicated processing components having processing and computing capabilities. Some examples of the computing unit <b>601</b> include, but are not limited to, central processing unit (CPU), graphics processing unit (GPU), various dedicated artificial intelligence (AI) computing chips, various computing units running machine learning model algorithms, digital signal processors (DSP), and any appropriate processors, controllers, microcontrollers, etc. The computing unit <b>601</b> performs the various methods and processes described above, such as the visual positioning method. For example, in some embodiments, the visual positioning method may be implemented as a computer software program, which is tangibly included in a machine readable medium, such as the storage unit <b>608</b>. In some embodiments, part or all of the computer program may be loaded and/or installed on the device <b>600</b> via the ROM <b>602</b> and/or the communication unit <b>609</b>. When the computer program is loaded into the RAM <b>603</b> and executed by the computing unit <b>601</b>, one or more steps of the visual positioning method described above may be performed. Alternatively, in other embodiments, the computing unit <b>601</b> may be configured to perform the visual positioning method by any other appropriate means (for example, by means of firmware).</p><p id="p-0074" num="0073">Various embodiments of the systems and technologies described in this article may be implemented in digital electronic circuit systems, integrated circuit systems, field programmable gate arrays (FPGA), application specific integrated circuits (ASIC), application-specific standard products (ASSP), system-on-chip (SOC), complex programmable logic device (CPLD), computer hardware, firmware, software, and/or their combinations. These various embodiments may include: being implemented in one or more computer programs, the one or more computer programs may be executed and/or interpreted on a programmable system including at least one programmable processor, the programmable processor may be a dedicated or general-purpose programmable processor that may receive data and instructions from a storage system, at least one input apparatus, and at least one output apparatus, and transmit the data and instructions to the storage system, the at least one input apparatus, and the at least one output apparatus.</p><p id="p-0075" num="0074">Program codes for implementing the method of the present disclosure may be written in any combination of one or more programming languages. These program codes may be provided to a processor or controller of a general purpose computer, special purpose computer or other programmable data processing apparatus such that the program codes, when executed by the processor or controller, enables the functions/operations specified in the flowcharts and/or block diagrams being implemented. The program codes may execute entirely on the machine, partly on the machine, as a stand-alone software package partly on the machine and partly on the remote machine, or entirely on the remote machine or server.</p><p id="p-0076" num="0075">In the context of the present disclosure, the machine readable medium may be a tangible medium that may contain or store programs for use by or in connection with an instruction execution system, apparatus, or device. The machine readable medium may be a machine readable signal medium or a machine readable storage medium. The machine readable medium may include, but is not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples of the machine readable storage medium may include an electrical connection based on one or more wires, portable computer disk, hard disk, random access memory (RAM), read only memory (ROM), erasable programmable read only memory (EPROM or flash memory), optical fiber, portable compact disk read only memory (CD-ROM), optical storage device, magnetic storage device, or any suitable combination of the foregoing.</p><p id="p-0077" num="0076">In order to provide interaction with a user, the systems and technologies described herein may be implemented on a computer, the computer has: a display apparatus (e.g., CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user; and a keyboard and a pointing apparatus (for example, a mouse or trackball), the user may use the keyboard and the pointing apparatus to provide input to the computer. Other kinds of apparatuses may also be used to provide interaction with the user; for example, the feedback provided to the user may be any form of sensory feedback (for example, visual feedback, auditory feedback, or tactile feedback); and may use any form (including acoustic input, voice input, or tactile input) to receive input from the user.</p><p id="p-0078" num="0077">The systems and technologies described herein may be implemented in a computing system (e.g., as a data server) that includes back-end components, or a computing system (e.g., an application server) that includes middleware components, or a computing system (for example, a user computer with a graphical user interface or a web browser, through which the user may interact with the embodiments of the systems and technologies described herein) that includes front-end components, or a computing system that includes any combination of such back-end components, middleware components, or front-end components. The components of the system may be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of the communication network include: local area network (LAN), wide area network (WAN), and Internet.</p><p id="p-0079" num="0078">The computer system may include a client and a server. The client and the server are generally far from each other and usually interact through a communication network. The client and server relationship is generated by computer programs operating on the corresponding computer and having client-server relationship with each other. The server can be a cloud server, a server for a distributed system, or a server combined with blockchain.</p><p id="p-0080" num="0079">In the technical solution according to the embodiments of the present disclosure, the contour map obtained by blurring non-building contour information in the real panoramic map may be used to provide a visual positioning service, which reduces resolution requirements for the uploaded image for positioning in the visual positioning process, so that the matching and positioning work may still be completed in the case of poor quality or few features of the image uploaded by a user.</p><p id="p-0081" num="0080">It should be understood that various forms of processes shown above may be used to reorder, add, or delete steps. For example, the steps described in the present disclosure may be performed in parallel, sequentially, or in different orders, as long as the desired results of the technical solution disclosed in embodiments of the present disclosure can be achieved, no limitation is made herein.</p><p id="p-0082" num="0081">The above specific embodiments do not constitute a limitation on the protection scope of the present disclosure. Those skilled in the art should understand that various modifications, combinations, sub-combinations and substitutions can be made according to design requirements and other factors. Any modification, equivalent replacement and improvement made within the spirit and principle of the present disclosure shall be included in the protection scope of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A visual positioning method, the method comprising:<claim-text>performing contour enhancement processing on an actual building image comprised in an image for positioning to obtain an actual building contour;</claim-text><claim-text>determining location information of a target building matching the actual building contour from a preset contour map, the contour map being obtained by blurring non-building contour information in a real panoramic map; and</claim-text><claim-text>generating a visual positioning result based on the location information.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the contour map is generated by steps comprising:<claim-text>acquiring the real panoramic map, and determining a reference building image comprised in the real panoramic map;</claim-text><claim-text>performing Gaussian blurring on the real panoramic map to obtain a blurred panoramic map; and</claim-text><claim-text>extracting contour information in the blurred panoramic map, and generating the contour map based on contour information corresponding to the reference building image in the blurred panoramic map.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein performing Gaussian blurring on the real panoramic map to obtain the blurred panoramic map, comprises:<claim-text>performing Gaussian blurring on the real panoramic map by using Gaussian convolution kernels of different sizes respectively, to obtain blurred panoramic maps of different blurring degrees correspondingly.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein extracting contour information in the blurred panoramic map, and generating the contour map based on contour information corresponding to the reference building image in the blurred panoramic map, comprises:<claim-text>performing edge extraction on the blurred panoramic map to obtain a binarized edge panoramic map that only comprises an edge portion defined as 1 and a non-edge portion defined as 0; and</claim-text><claim-text>generating the contour map by multiplying feature information, in a form of a matrix, corresponding to the reference building image by the binarized edge map.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing contour enhancement processing on an actual building image comprised in a image for positioning to obtain an actual building contour, comprises:<claim-text>extracting the actual building image comprised in the image for positioning;</claim-text><claim-text>improving a contrast of an edge of an actual building in the actual building image by sharpening, to obtain a sharpened image; and</claim-text><claim-text>extracting contour information corresponding to the actual building in the sharpened image to generate the actual building contour.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises:<claim-text>generating photographing pose information of the image for positioning based on difference information between the actual building contour and a standard contour of the target building.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A visual positioning apparatus, the apparatus comprising:<claim-text>at least one processor; and</claim-text><claim-text>a memory storing instructions, wherein the instructions when executed by the at least one processor, cause the at least one processor to perform operations, the operations comprising:</claim-text><claim-text>performing contour enhancement processing on an actual building image comprised in an image for positioning to obtain an actual building contour;</claim-text><claim-text>determining location information of a target building matching the actual building contour from a preset contour map, the contour map being obtained by blurring non-building contour information in a real panoramic map; and</claim-text><claim-text>generating a visual positioning result based on the location information.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the contour map is generated by steps comprising:<claim-text>acquiring the real panoramic map, and determining a reference building image comprised in the real panoramic map;</claim-text><claim-text>performing Gaussian blurring on the real panoramic map to obtain a blurred panoramic map; and</claim-text><claim-text>extracting contour information in the blurred panoramic map, and generating the contour map based on contour information corresponding to the reference building image in the blurred panoramic map.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the steps further comprise: performing Gaussian blurring on the real panoramic map by using Gaussian convolution kernels of different sizes respectively, to obtain blurred panoramic maps of different blurring degrees correspondingly.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the steps further comprise:<claim-text>performing edge extraction on the blurred panoramic map to obtain a binarized edge panoramic map that only comprises an edge portion defined as 1 and a non-edge portion defined as 0; and</claim-text><claim-text>generating the contour map by multiplying feature information corresponding to the reference building image in a form of a matrix and the binarized edge map.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the operations further comprise:<claim-text>extracting the actual building image comprised in the image for positioning;</claim-text><claim-text>improving a contrast of an edge of an actual building in the actual building image by sharpening to obtain a sharpened image; and</claim-text><claim-text>extracting contour information corresponding to the actual building in the sharpened image to generate the actual building contour.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the operations further comprise:<claim-text>generating photographing pose information of the image for positioning based on difference information between the actual building contour and a standard contour of the target building.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A non-transitory computer readable storage medium storing computer instructions, wherein, the computer instructions are used to cause the computer to perform operations comprising:<claim-text>performing contour enhancement processing on an actual building image comprised in an image for positioning to obtain an actual building contour;</claim-text><claim-text>determining location information of a target building matching the actual building contour from a preset contour map, the contour map being obtained by blurring non-building contour information in a real panoramic map; and</claim-text><claim-text>generating a visual positioning result based on the location information.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the contour map is generated by steps comprising:<claim-text>acquiring the real panoramic map, and determining a reference building image comprised in the real panoramic map;</claim-text><claim-text>performing Gaussian blurring on the real panoramic map to obtain a blurred panoramic map; and</claim-text><claim-text>extracting contour information in the blurred panoramic map, and generating the contour map based on contour information corresponding to the reference building image in the blurred panoramic map.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, performing Gaussian blurring on the real panoramic map to obtain the blurred panoramic map, comprises:<claim-text>performing Gaussian blurring on the real panoramic map by using Gaussian convolution kernels of different sizes respectively, to obtain blurred panoramic maps of different blurring degrees correspondingly.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein extracting contour information in the blurred panoramic map, and generating the contour map based on contour information corresponding to the reference building image in the blurred panoramic map, comprises:<claim-text>performing edge extraction on the blurred panoramic map to obtain a binarized edge panoramic map that only comprises an edge portion defined as 1 and a non-edge portion defined as 0; and</claim-text><claim-text>generating the contour map by multiplying feature information, in a form of a matrix, corresponding to the reference building image by the binarized edge map.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein performing contour enhancement processing on an actual building image comprised in a image for positioning to obtain an actual building contour, comprises:<claim-text>extracting the actual building image comprised in the image for positioning;</claim-text><claim-text>improving a contrast of an edge of an actual building in the actual building image by sharpening, to obtain a sharpened image; and</claim-text><claim-text>extracting contour information corresponding to the actual building in the sharpened image to generate the actual building contour.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the operations further comprise:<claim-text>generating photographing pose information of the image for positioning based on difference information between the actual building contour and a standard contour of the target building.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the operations further comprise:<claim-text>generating photographing pose information of the image for positioning based on difference information between the actual building contour and a standard contour of the target building.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the operations further comprise:<claim-text>generating photographing pose information of the image for positioning based on difference information between the actual building contour and a standard contour of the target building.</claim-text></claim-text></claim></claims></us-patent-application>