<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004567A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004567</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17866852</doc-number><date>20220718</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>2457</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>2452</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>253</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>289</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>24575</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>24522</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>253</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>289</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">VOICE QUERY REFINEMENT TO EMBED CONTEXT IN A VOICE QUERY</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16206385</doc-number><date>20181130</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11468071</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17866852</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Rovi Guides, Inc.</orgname><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Pichaimurthy</last-name><first-name>Rajendran</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Seetharam</last-name><first-name>Madhusudhan</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Sreekanth</last-name><first-name>Harshith Kumar Gejjegondanahally</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods are described for providing contextual search results. The system may receive a search query during presentation of a video. If the query is ambiguous, the system accesses some of the frames of the video. The frames are analyzed to identify a performed action depicted in the frames. The system retrieves a keyword related to the identified action.</p><p id="p-0002" num="0000">The ambiguous query is augmented with the keyword. The augmented search query is used to search for and output relevant search results.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="234.95mm" wi="141.14mm" file="US20230004567A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="242.91mm" wi="144.53mm" file="US20230004567A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="228.26mm" wi="161.88mm" orientation="landscape" file="US20230004567A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="232.07mm" wi="147.32mm" orientation="landscape" file="US20230004567A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="194.31mm" wi="171.28mm" file="US20230004567A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="240.03mm" wi="176.70mm" file="US20230004567A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="213.28mm" wi="175.34mm" file="US20230004567A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="212.43mm" wi="177.21mm" file="US20230004567A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="226.57mm" wi="176.61mm" file="US20230004567A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="236.98mm" wi="165.27mm" orientation="landscape" file="US20230004567A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0003" num="0001">The present disclosure relates to improved computerized search, and more particularly, to methods and systems for providing contextual search results to an ambiguous query by identifying an action being performed in a concurrently presented video, and modifying the query based on the identified action.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0004" num="0002">Modern computerized search systems often receive user queries that are ambiguous. The search systems are often unable to return appropriate results in response to a receipt of such a query. For example, queries like &#x201c;what is this?&#x201d;, &#x201c;what is she doing&#x201d; or &#x201c;where is he going&#x201d; are very difficult for search systems to interpret because they are too general or missing key information. In particular, pronouns like &#x201c;he&#x201d; or &#x201c;she&#x201d; or auxiliary verbs like &#x201c;do&#x201d; would return too many results unrelated to a topic that is actually relevant to the request. In one approach, a search system may attempt to supplement the ambiguous search query with contextual information. For example, such a search system may extract information about the media asset that is being presented to the user when the search query was received. In one example, if a certain movie was being shown on TV, the search system may supplement the search query with information about objects that are being shown. However, such an approach does not improve search results for a query related to an action that is being performed in video. For example, if the search query is an ambiguous query &#x201c;what is she doing,&#x201d; a system mentioned above would be unable to improve such a query simply by adding information about objects because information about statistic objects does help resolve the ambiguity related to an action.</p><p id="p-0005" num="0003">Accordingly, to overcome such problems, methods and systems are disclosed herein for providing contextual search results to an ambiguous query by augmenting that query to include metadata (e.g., a keyword) related to an action that occurred in a video that was presented concurrently with receiving the search query (e.g., &#x201c;What is she doing&#x201d;). In one embodiment, a search application analyzes the query to determine that it is ambiguous. For example, the search application determines that that the query includes an auxiliary verb or a term with multiple possible meanings. In response, the search application accesses a plurality of frames from the video that were presented concurrently with receiving the search query (e.g., by extracting frames of a video that was played on a computer screen in a vicinity of the user). By analyzing frames of a concurrently presented video, the search application can acquire context for the user's ambiguous query and provide significantly improved search results that are more relevant to the query.</p><p id="p-0006" num="0004">For example, the search application captures a predetermined number of frames that were shown on a screen in a vicinity of the user when the search query was received. The accessed frames are analyzed to identify an action that was depicted by these frames. Once the action is identified, the search application augments the search query with a keyword related to the action. For example, if the search application detected that a video depicted a character who was rappelling from a mountain, the search application may augment the query to include a keyword &#x201c;rappelling.&#x201d; The system may then perform a search using the augmented query and output the results. Because the ambiguous query was supplemented with a keyword associated with an action that that occurred in a concurrently presented video, the search application can acquire search results that are significantly more relevant to the query than results that would be generated in response to an ambiguous query.</p><p id="p-0007" num="0005">In one illustrative embodiment, the search application may identify the performed action by identifying a character (e.g., a human body) in each of the plurality of frames. The search application generates a model for the movement of the identified character. For example, the search application may identify body parts of the character in the frame and calculate angles between body parts of that character. In some embodiments, the search application calculates angles between the body trunk and the arms, between the body trunk and the legs, as well as bend angles at the elbows and knees. The system may also identify changes between such angles between frames of the plurality of frames. The calculated angles (or changes in angles) may then be compared to the angle values (or angle change values) stored in a template for specific types of an action. If the calculated angle sufficiently matches the stored angle values of a template, the search application may determine that the action that was shown in the plurality of frames corresponds to the action of that template. For example, the search application may retrieve a keyword of the template and use it to augment the query.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0006">The above and other objects and advantages of the disclosure will be apparent upon consideration of the following detailed description, taken in conjunction with the accompanying drawings, in which like reference characters refer to like parts throughout, and in which:</p><p id="p-0009" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an illustrative example for providing contextual search results to an ambiguous query, in accordance with some embodiments of the disclosure;</p><p id="p-0010" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an illustrative example for identifying a performed action based on frames of a video, in accordance with some embodiments of the disclosure;</p><p id="p-0011" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of an illustrative user equipment device in accordance with some embodiments of the disclosure;</p><p id="p-0012" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of an illustrative search system in accordance with some embodiments of the disclosure;</p><p id="p-0013" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of a detailed illustrative process for providing contextual search results to an ambiguous query, in accordance with some embodiments of the disclosure;</p><p id="p-0014" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is flowchart of another detailed illustrative process providing contextual search results to an ambiguous query, in accordance with some embodiments of the disclosure;</p><p id="p-0015" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of a detailed illustrative process for identifying a performed action, in accordance with some embodiments of the disclosure;</p><p id="p-0016" num="0014"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart of a detailed illustrative process for accessing a plurality of frames, in accordance with some embodiments of the disclosure; and</p><p id="p-0017" num="0015"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart of a detailed illustrative process for analyzing features of relevant frames to refine a query, in accordance with some embodiments of the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0018" num="0016"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an illustrative example of a search application for providing contextual search results, in accordance with some embodiments of the disclosure. In particular, <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a scenario <b>100</b> where a query <b>104</b> (e.g., query &#x201c;What is she doing&#x201d;) is received via user input/output device <b>105</b> (e.g., a digital voice assistant). In some embodiments, the query is received as voice input from user <b>102</b>. The search application may determine that the query <b>104</b> is ambiguous. For example, the search application may determine that query <b>104</b> comprises an auxiliary verb, and ambiguous term, or a pronoun. The search application may determine that query <b>104</b> is ambiguous because it includes auxiliary verb &#x201c;doing&#x201d; and no other verbs.</p><p id="p-0019" num="0017">In response to the determination, the search application may leverage a presentation of a video on screen <b>106</b> in a vicinity of user <b>102</b> to augment the search query. In some embodiments, the search application extracts several frames of a video (e.g., a movie or a TV show) that is being presented on display <b>106</b> concurrently with a receipt of the query. For example, the search application may capture 10 frames of the video after the receipt of the query or retrieve all frames presented for 2 seconds before and after the receipt of the query.</p><p id="p-0020" num="0018">In some embodiments, the search application analyzes the frames of the video to identify a performed action depicted in those frames. For example, the search application may analyze a first frame <b>110</b> and a second frame <b>112</b>. The search application mat identify a human character present in frames <b>110</b> and <b>112</b>. For example, a human character may be identified by a computer vision algorithm trained to look for typical human shapes. The search application may then generate movement model <b>130</b> of the character. For example, the search application may generate vector repreparation of the character's body in each analyzed frame to create movement model <b>130</b>.</p><p id="p-0021" num="0019">In some embodiments, the search application compares movement model <b>130</b> with templates from movement template database <b>132</b>. For example, the search application may access movement template database <b>132</b> that includes three templates (or any other number of templates). Each template may be associated with an activity and comprise a keyword identifying the activity (e.g., &#x201c;running,&#x201d; &#x201c;swimming, &#x201c;rappelling&#x201d;). Each template may also comprise a model (e.g. a vector model) of character movement normally associated with the respective activity, and each model may compromise vector graphics (as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>), or a list of angles defined by the vectors.</p><p id="p-0022" num="0020">In some embodiments, the search application compares movement model <b>130</b> with each template of template database <b>132</b>. For example, the search application may compare the vectors, or stored angles between the vector components. The search application may determine that movement model <b>130</b> matches a template when vector graphics of the template movement model <b>130</b> are sufficiently similar (e.g., if the least square analysis of vector similarity returns a value that is below a threshold.). In the example shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the search application determines that the movement model <b>130</b> is sufficiently similar to the &#x201c;rappelling&#x201d; template of movement template database <b>132</b>.</p><p id="p-0023" num="0021">In some embodiments, after the search application determines that movement model <b>130</b> matches a template of movement template database <b>132</b>, the search application may extract a keyword of the matching template. In the example shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the search application extracts the keyword &#x201c;rappelling.&#x201d; The search application may augment query <b>104</b> with the extracted keyword. For example, the search application may remove pronouns and auxiliary verbs from query <b>104</b> (&#x201c;What is she doing&#x201d;) and replace them with the exacted keyword resulting in an augmented query &#x201c;What is rappelling?&#x201d; The search application may perform a search (e.g., Internet search, local database search, etc.,) and output the results of the search. In some embodiments, results <b>144</b> may be displayed on a display of user device <b>140</b>. The search application may also use the keyword to generate an answer <b>142</b> to query <b>104</b> which may also be displayed on a display of user device <b>140</b>. The search application may output the results via audio using input/output device <b>105</b> (e.g., a digital voice assistant).</p><p id="p-0024" num="0022"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an illustrative example of a search application for identifying a performed action based on frames of a video. In particular, <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a scenario <b>200</b> where a scene extracted from a video is analyzed to identify a performed action. In some embodiments, scenario <b>200</b> is performed as part of Scenario of <figref idref="DRAWINGS">FIG. <b>1</b></figref> where frames <b>110</b> and <b>120</b> were analyzed. For example, the search application may extract frame <b>202</b> (e.g., one of frame <b>110</b> or frame <b>120</b>). The search application may identify character <b>202</b> in that frame (e.g., character <b>202</b> may be a human rappelling down a mountain). The search application mat vectorize the identified character <b>202</b> by drawing vectors along body parts (e.g., trunk, legs and fees) of the character. The resulting vector model <b>220</b> is further analyzed by the search application. For example, vector model <b>220</b> may include vectors representing body torso, left arm, left forearm, right arm, right forearm, left thigh, right thigh, left ankle, and right ankle.</p><p id="p-0025" num="0023">In some embodiments, the search application determines angles between multiple vectors that represent multiple body parts. For example, the search application may determine left elbow angle <b>230</b>, right elbow angle <b>223</b>, left leg torso angle <b>234</b>, left knee angle <b>236</b>, and right knee angle <b>238</b>. In some embodiments, other angles may also be measured. The search application may store the angles <b>240</b> as part of a movement template. The search application may also store angles detected using the process above for other extracted frames. The search application may calculate angle changes across the planarity of analyzed frames.</p><p id="p-0026" num="0024">In some embodiments, the search application may compare <b>244</b> the detected angles <b>240</b> or angle changes to template angles <b>242</b> (e.g., angles stored as part of a movement template). If the angles (or angle changes) are sufficiently similar, the search application may identify the performed action based on the metadata of the matching template. For example, if template angles <b>242</b> are part of the template with a keyword &#x201c;rappelling,&#x201d; the search application may identify action performed in frame <b>202</b> (and surrounding frames) as &#x201c;rappelling.&#x201d;</p><p id="p-0027" num="0025"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows generalized embodiments of a system that can host a search application. For example, the system may include user equipment device <b>300</b>. User equipment device <b>300</b> may be one of a user smartphone device, user computer equipment, or user television equipment. User television equipment system may include a set-top box <b>316</b>. Set-top box <b>316</b> may be communicatively connected to speaker <b>314</b> and display <b>312</b>. In some embodiments, display <b>312</b> may be a television display or a computer display. Set top box <b>316</b> may be communicatively connected to user interface input <b>310</b>. In some embodiments, user interface input <b>310</b> may be a remote-control device. User interface input <b>310</b> may be a voice controlled digital assistant device (e.g., Amazon Echo&#x2122;). Set-top box <b>316</b> may include one or more circuit boards. In some embodiments, the circuit boards may include processing circuitry, control circuitry, and storage (e.g., RAM, ROM, Hard Disk, Removable Disk, etc.). Such circuit boards may include an input/output path. More specific implementations of user equipment devices are discussed below in connection with <figref idref="DRAWINGS">FIG. <b>4</b></figref>. User equipment device <b>300</b> may receive content and data via input/output (hereinafter &#x201c;I/O&#x201d;) path <b>302</b>. I/O path <b>302</b> may provide content (e.g., broadcast programming, on-demand programming, Internet content, content available over a local area network (LAN) or wide area network (WAN), and/or other content) and data to control circuitry <b>304</b>, which includes processing circuitry <b>306</b> and storage <b>308</b>. Control circuitry <b>304</b> may be used to send and receive commands, requests, and other suitable data using I/O path <b>302</b>. I/O path <b>302</b> may connect control circuitry <b>304</b> (and specifically processing circuitry <b>306</b>) to one or more communications paths (described below). I/O functions may be provided by one or more of these communications paths but are shown as a single path in <figref idref="DRAWINGS">FIG. <b>3</b></figref> to avoid overcomplicating the drawing.</p><p id="p-0028" num="0026">Control circuitry <b>304</b> may be based on any suitable processing circuitry such as processing circuitry <b>306</b>. As referred to herein, processing circuitry should be understood to mean circuitry based on one or more microprocessors, microcontrollers, digital signal processors, programmable logic devices, field-programmable gate arrays (FPGAs), application-specific integrated circuits (ASICs), etc., and may include a multi-core processor (e.g., dual-core, quad-core, hexa-core, or any suitable number of cores) or supercomputer. In some embodiments, processing circuitry may be distributed across multiple separate processors or processing units. For example, the search application may provide instructions to control circuitry <b>304</b> to generate the media guidance displays. In some implementations, any action performed by control circuitry <b>304</b> may be based on instructions received from the search application.</p><p id="p-0029" num="0027">Memory may be an electronic storage device provided as storage <b>308</b> that is part of control circuitry <b>304</b>. As referred to herein, the phrase &#x201c;electronic storage device&#x201d; or &#x201c;storage device&#x201d; should be understood to mean any device for storing electronic data, computer software, or firmware, such as random-access memory, read-only memory, hard drives, optical drives, digital video recorders (DVR, sometimes called a personal video recorder, or PVR), solid state devices, quantum storage devices, gaming consoles, gaming media, or any other suitable fixed or removable storage devices, and/or any combination of the same. Storage <b>308</b> may be used to store various types of content described herein as well as media guidance data described above. Nonvolatile memory may also be used (e.g., to launch a boot-up routine and other instructions). Cloud-based storage, described in relation to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, may be used to supplement storage <b>308</b> or instead of storage <b>308</b>.</p><p id="p-0030" num="0028">Control circuitry <b>304</b> may include video generating circuitry and tuning circuitry, such as one or more analog tuners, one or more MPEG-2 decoders or other digital decoding circuitry, high-definition tuners, or any other suitable tuning or video circuits or combinations of such circuits. Encoding circuitry (e.g., for converting over-the-air, analog, or digital signals to MPEG signals for storage) may also be provided. Control circuitry <b>304</b> may also include scaler circuitry for upconverting and downconverting content into the preferred output format of the user equipment <b>300</b>.</p><p id="p-0031" num="0029">A user may send instructions to control circuitry <b>304</b> using user input interface <b>310</b>. User input interface <b>310</b> may be any suitable user interface, such as a remote control, mouse, trackball, keypad, keyboard, touch screen, touchpad, stylus input, joystick, voice recognition interface, or other user input interfaces. Display <b>312</b> may be provided as a stand-alone device or integrated with other elements of user equipment device <b>300</b>. For example, display <b>312</b> may be a touchscreen or touch-sensitive display. In such circumstances, user input interface <b>310</b> may be integrated with or combined with display <b>312</b>. Display <b>312</b> may be one or more of a monitor, a television, a liquid crystal display (LCD) for a mobile device, amorphous silicon display, low temperature poly silicon display, electronic ink display, electrophoretic display, active matrix display, electro-wetting display, electrofluidic display, cathode ray tube display, light-emitting diode display, electroluminescent display, plasma display panel, high-performance addressing display, thin-film transistor display, organic light-emitting diode display, surface-conduction electron-emitter display (SED), laser television, carbon nanotubes, quantum dot display, interferometric modulator display, or any other suitable equipment for displaying visual images. Speakers <b>314</b> may be provided as integrated with other elements of user equipment device <b>300</b> or may be stand-alone units. The audio component of videos and other content displayed on display <b>312</b> may be played through speakers <b>314</b>. In some embodiments, the audio may be distributed to a receiver (not shown), which processes and outputs the audio via speakers <b>314</b>.</p><p id="p-0032" num="0030">The search application may be implemented using any suitable architecture. For example, it may be a stand-alone application wholly-implemented on user equipment device <b>300</b>. In such an approach, instructions of the search application are stored locally (e.g., in storage <b>308</b>), and data for use by the application is downloaded on a periodic basis (e.g., from an out-of-band feed, from an Internet resource, or using another suitable approach). Control circuitry <b>304</b> may retrieve instructions of the search application from storage <b>308</b> and process the instructions to generate any of the displays discussed herein. Based on the processed instructions, control circuitry <b>304</b> may determine what action to perform when input is received from input interface <b>310</b>. For example, movement of a cursor on a display up/down may be indicated by the processed instructions when input interface <b>310</b> indicates that an up/down button was selected.</p><p id="p-0033" num="0031">In some embodiments, the search application is a client-server based application. Data for use by a thick or thin client implemented on user equipment device <b>300</b> is retrieved on-demand by issuing requests to user equipment device <b>300</b>. In one example of a client-server based guidance application, control circuitry <b>304</b> runs a web browser that interprets web pages provided by a remote server. For example, the remote server may store the instructions for the search application in a storage device. The remote server may process the stored instructions using circuitry (e.g., control circuitry <b>304</b>) and generate the displays discussed above and below. The client device may receive the displays generated by the remote server and may display the content of the displays locally on equipment device <b>300</b>. This way, the processing of the instructions is performed remotely by the server while the resulting displays are provided locally on equipment device <b>300</b>. Equipment device <b>300</b> may receive inputs from the user via input interface <b>310</b> and transmit those inputs to the remote server for processing and generating the corresponding displays. For example, equipment device <b>300</b> may transmit a communication to the remote server indicating a search query received from a user. The remote server may process instructions in accordance with that input and generate an output corresponding to the input (e.g., search results). The generated display is then transmitted to equipment device <b>300</b> for presentation to the user.</p><p id="p-0034" num="0032">User equipment device <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> can be implemented in system <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> as part of processor <b>404</b>. Processor <b>404</b> may include numerous types of equipment (and more than one of) such as user television equipment, user computer equipment, wireless user communications devices, and/or any other type of user equipment suitable for accessing content, such as a non-portable gaming machine. For simplicity, these devices may be referred to herein collectively as user equipment or user equipment devices and may be substantially similar to user equipment devices described above. User equipment devices, on which a search application may be implemented, may function as a standalone device or may be part of a network of devices. Likewise, user equipment and processor <b>404</b> may be separate devices or a single device. Various network configurations of devices may be implemented and are discussed in more detail below.</p><p id="p-0035" num="0033">In system <b>400</b>, there is typically more than one of each type of user equipment device but only one of each is shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> to avoid overcomplicating the drawing. In addition, each user may utilize more than one type of user equipment device and also more than one of each type of user equipment device.</p><p id="p-0036" num="0034">In some embodiments, system <b>400</b> may include a display or output device <b>402</b>. Output device <b>402</b> may be referred to as a &#x201c;second screen device.&#x201d; For example, a second screen device may supplement content presented on a first user equipment device. The content presented on the second screen device may be any suitable content that supplements the content presented on the first device. In some embodiments, or output device <b>402</b> may be a voice output device (e.g., a digital voice assistant <b>105</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) configured to generate voice output. Output device <b>402</b> may include at least one of a video display, speakers, headphones, other media consumption device, or an output service such as e-mail interface, social-media interface or text messaging interface. For example, system <b>40</b> may provide output (e.g., search results) via mail interface, social-media interface or text messaging interface of output device <b>402</b>.</p><p id="p-0037" num="0035">The various parts of system <b>400</b> (e.g., processor <b>404</b>, output device <b>402</b>, sampling buffer <b>406</b>, and external Internet source <b>462</b>) may be coupled together by communications networks <b>408</b>, <b>410</b>, and <b>412</b> (referred to herein collectively as communications network). Communications network may be one or more networks including the Internet, a mobile phone network, mobile voice or data network (e.g., a 4G or LTE network), cable network, public switched telephone network, or other types of communications network or combinations of communications networks. Paths <b>408</b> may separately or together include one or more communications paths, such as, a satellite path, a fiber-optic path, a cable path, a path that supports Internet communications (e.g., IPTV), free-space connections (e.g., for broadcast or other wireless signals), or any other suitable wired or wireless communications path or combination of such paths.</p><p id="p-0038" num="0036">Although communications paths are not drawn between output device <b>402</b> and sampling buffer <b>406</b>, these devices may communicate directly with each other via communication paths, such as those described above in connection with paths <b>408</b>, <b>410</b>, and <b>412</b>, as well as other short-range point-to-point communication paths, such as USB cables, IEEE 1394 cables, wireless paths (e.g., Bluetooth, infrared, IEEE 402-11x, etc.), or other short-range communication via wired or wireless paths. BLUETOOTH is a certification mark owned by Bluetooth SIG, INC. Processor <b>404</b> may also communicate with AI service <b>460</b> via communications network <b>414</b>. Additionally, voice input <b>452</b>, which may correspond to user input interface <b>310</b>) as well as video source <b>454</b> and audio source <b>456</b>, may communicate directly with each other via communication paths as well as the other components described above.</p><p id="p-0039" num="0037">Sampling buffer <b>406</b> may be a region of a physical memory storage used to temporarily store data while it is being moved from one place to another. In some embodiments, sampling buffer <b>406</b> may be incorporated into processor <b>404</b> or user equipment <b>402</b>. Typically, the data is stored in a buffer as it is retrieved from an input such as video source <b>454</b> and audio source <b>456</b>. Sampling buffer <b>406</b> can be implemented in a fixed memory location in hardware (e.g., storage <b>308</b>)&#x2014;or by using a virtual data buffer in software, pointing at a location in the physical memory. In some embodiments, sampling buffer <b>406</b> may be used to store several past frames of a video that is being provided via video sources <b>454</b> or that is being shown on output device <b>402</b>. The sampling buffer can thus be used by processor <b>404</b> to access frames of a video that was recently presented.</p><p id="p-0040" num="0038">Processor <b>404</b> includes local media <b>416</b> and metadata source <b>418</b>. Processor <b>404</b> is also coupled to AI service via communications network <b>414</b>. For example, AI service <b>460</b> may be used to perform any search application function describe herein. For example, AI service may be able to perform speech to text and text to speech conversion and analyze frames of a video to identify a performed action. Processor <b>404</b> may be a headend system or coupled to and/or integrated into a local device (e.g., as a set-top box). Communications with the local media <b>416</b> and metadata source <b>418</b> may be exchanged over one or more communications paths discussed herein. In addition, there may be more than one of each of local media <b>416</b> and metadata source <b>418</b>, but only one of each is shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> to avoid overcomplicating the drawing. If desired, local media <b>416</b> and metadata source <b>418</b> may be integrated as one source device. Sources <b>416</b> and <b>418</b> may communicate with output device <b>402</b> and sampling buffer <b>406</b> directly or through processor <b>404</b> via communication paths such as those described above in connection with paths <b>408</b>, <b>410</b>, and <b>412</b>.</p><p id="p-0041" num="0039">Local media <b>416</b> may receive and store data from one or more types of content distribution equipment including a television distribution facility, cable system headend, satellite distribution facility, programming sources (e.g., television broadcasters, such as NBC, ABC, HBO, etc.), intermediate distribution facilities and/or servers, Internet providers, on-demand media servers, and other content providers. Local media <b>416</b> may receive and store data from sources that include cable sources, satellite providers, on-demand providers, Internet providers, over-the-top content providers, or other providers of content. Local media <b>416</b> may also include a remote media server used to store different types of content (including video content selected by a user), in a location remote from any of the user equipment devices.</p><p id="p-0042" num="0040">Processor <b>404</b> may be communicatively coupled to external Internet source <b>462</b>, e.g., via network <b>410</b>. In some embodiments, processor <b>404</b> may send and receive data to external internet source <b>462</b>. For example, search request generated by a search application may be sent to external internet source <b>462</b>. Processor <b>404</b> may receive the search results from external internet sources <b>462</b> and process the search results for output to output device <b>402</b>.</p><p id="p-0043" num="0041">In some embodiments, system <b>400</b> may include remote computing sites such as AI service <b>460</b>. AI service <b>460</b> may include any service where intelligence is supplied by technology that makes feasible the execution of algorithms that mimic cognitive functions. For example, learning functions created by AI, allow the execution of algorithms mimicking human activities related with problem solving, recommendations, and/or decision making to the computational level. AI services may generate a consistent increase of the efficiency, quality and efficacy through predictions, recommendations and classifications. For example, machine learning can consider data that influence recommendation engine performance, leading to more accurate or timely recommendations and calibrations by spotting patterns in large volumes of data.</p><p id="p-0044" num="0042"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of an illustrative process for providing contextual search results to an ambiguous query, in accordance with some embodiments of the disclosure. In some embodiments, each step of process <b>500</b> can be performed by user device <b>300</b> (e.g. via control circuitry <b>304</b>) or any of the system components shown in <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>4</b></figref>.</p><p id="p-0045" num="0043">Process <b>500</b> begins at block <b>502</b> where control circuitry <b>304</b> receives a search query. For example, the search query may be received via user input interface <b>310</b>. For example, control circuitry <b>304</b> may receive the search query as audio signal via voice input <b>452</b>. In another embodiment, control circuitry <b>304</b> receives search input as text. In some embodiments, control circuitry <b>304</b> may receive the search query (e.g., &#x201c;what is she doing&#x201d;) via digital assistant <b>105</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0046" num="0044">At <b>504</b>, control circuitry <b>304</b> processes the search query to determine whether it is ambiguous. For example, control circuitry <b>304</b> may evaluate each word of the query and check if it contains a pronoun, an auxiliary verb or a word (e.g., a verb) that has multiple possible meanings. If control circuitry <b>304</b> determines that the search query is ambiguous, control circuitry <b>304</b> proceeds to block <b>508</b>, otherwise, control circuitry <b>304</b> proceeds to <b>506</b>.</p><p id="p-0047" num="0045">At <b>506</b>, control circuitry <b>304</b> may perform a search using the search query (as it was received). For example, control circuitry <b>304</b> may send a query to an internet source <b>462</b> or to AI service <b>460</b> via network <b>414</b> or network <b>410</b>.</p><p id="p-0048" num="0046">At <b>508</b>, control circuitry <b>304</b> accesses a plurality of frames of a video that was presented concurrently with the time when the search query was received at <b>502</b>. For example, control circuitry <b>304</b> may access one or more frames from a buffer (e.g., sampling buffer <b>406</b>) which stores several frames of the video that is being presented (e.g., on screen display <b>312</b>, output device <b>402</b>, or any other display). In some embodiments, control circuitry <b>304</b> may extract a predetermined number of frames that are presented after the search quarry as received at block <b>502</b> or before the search query was received at <b>502</b>. In one implementation, control circuitry <b>304</b> extracts frames for a predetermined time period after the search quarry as received at <b>502</b> or before the search query was received at <b>502</b>. In some embodiments, control circuitry <b>304</b> may receive the frames from a remote source (e.g., AI service <b>460</b> or Internet source <b>462</b>). In another implementation, control circuitry <b>304</b> may receive the frames from local media sources <b>416</b>.</p><p id="p-0049" num="0047">At <b>510</b>, control circuitry <b>304</b> may analyze the plurality of frames to identify a performed action. For example, control circuitry <b>304</b> may generate a movement model and find a matching movement template (e.g., as shown with respect to element <b>130</b> and <b>132</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). In one example, control circuitry <b>304</b> may determine that the plurality of frames depict a person rappelling down a mountain (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>). At <b>512</b>, control circuitry <b>304</b> retrieves a keyword associated with the identified action (e.g., &#x201c;rappelling&#x201d;). For example, the keyword may be retrieved from the matching movement template.</p><p id="p-0050" num="0048">At <b>514</b>, control circuitry <b>304</b> may augment the search query (e.g., &#x201c;What is she doing&#x201d;). In some embodiments, control circuitry <b>304</b> simply adds the keyword to the query. For example, control circuitry <b>304</b> replaces pronouns, (e.g., &#x201c;she&#x201d;) and auxiliary verbs (e.g., &#x201c;doing&#x201d;) with the keyword. For example, search query &#x201c;What is she doing?&#x201d; may become &#x201c;What is she &#x3c;&#x3e;&#x201d; as pronouns and auxiliary verbs are removed. The search query may then become &#x201c;What is rappelling?&#x201d; as it is augmented with the keyword.</p><p id="p-0051" num="0049">At <b>516</b>, control circuitry <b>304</b> may perform a search using the augmented search query (as it was augmented in block <b>514</b>). For example, control circuitry <b>304</b> may send the modified query to Internet source <b>462</b> or to AI service <b>460</b> via network <b>414</b> or network <b>410</b>. Control circuitry <b>304</b> may then receive search results via network <b>414</b> or network <b>410</b>.</p><p id="p-0052" num="0050">At <b>518</b>, control circuitry <b>304</b> may output the results of the search received in block <b>506</b> or in block <b>516</b>. For example, search results may be displayed as text on display <b>140</b> or <b>312</b>. In some embodiments, control circuitry <b>304</b> may generate speech output based on the search results and output the results using output device <b>402</b>.</p><p id="p-0053" num="0051"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of another illustrative process for providing contextual search results to an ambiguous query, in accordance with some embodiments of the disclosure. In some embodiments, each step of process <b>600</b> can be performed by user device <b>300</b> (e.g. via control circuitry <b>304</b>) or any of the system components shown in <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>4</b></figref>.</p><p id="p-0054" num="0052">At <b>602</b>, control circuitry <b>304</b> may receive a voice search query. For example, control circuitry <b>304</b> may receive the voice search query via voice input <b>452</b>. At <b>604</b>, control circuitry <b>304</b> may perform speech to text processing to generate a text. In some embodiments, control circuitry <b>304</b> may send the voice search query to a remote processor, (e.g. AI service <b>460</b>), which returns the text of the query via network <b>414</b>. Control circuitry <b>304</b> may use any known speech to text processing algorithm.</p><p id="p-0055" num="0053">At <b>606</b>, control circuitry <b>304</b>, may extract a word from the text of the search query (e.g., control circuitry <b>304</b> may start by extracting a first word, and moving to a subsequent word every time step <b>606</b> is performed). At <b>608</b>, control circuitry <b>304</b> may determine whether the extracted word is a pronoun, an auxiliary verb, or an ambiguous word. This determination may be made by comparing the extracted word to a dictionary of pronouns, auxiliary verbs, and ambiguous words. In some embodiments, control circuitry <b>304</b> generates its own dictionary over time by identifying words that have failed to generate good search results. If the extracted word is a pronoun, an auxiliary verb, or an ambiguous word, process <b>600</b> proceeds to <b>612</b>, otherwise process <b>600</b> proceeds back to <b>610</b>. At <b>610</b>, if there are more words to analyze, process <b>600</b> returns to <b>606</b> and extracts a next word, otherwise process <b>600</b> ends at <b>622</b>.</p><p id="p-0056" num="0054">At <b>612</b>, control circuitry <b>304</b> extracts a plurality of frames of a video that was being played concurrently with receipt of the voice query (e.g., on user computer equipment <b>300</b>). Frames may be extracted as described with respect to steps <b>508</b>.</p><p id="p-0057" num="0055">At <b>614</b>, control circuitry <b>304</b> may identify a character in each of the frames. For example, a human shape can be discovered using an AI (e.g., AI service <b>460</b>) trained to recognize human shapes. At <b>616</b>, control circuitry <b>304</b> may generate a movement model based on the character in each of the frames. For example, control circuitry <b>304</b> may create vectorized representations of body parts and measure angles between the angles (e.g., as showing in <figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>2</b></figref>)</p><p id="p-0058" num="0056">At <b>618</b>, control circuitry <b>304</b> may compare the generated movement model to movement template (e.g., one templates <b>132</b> or <b>242</b>). For example, control circuitry <b>304</b> may check whether the difference between angles of vectorized human shape are within a threshold from the angles listed in the template. If no matching template is found, process <b>600</b> ends at <b>622</b>. If a matching template is found, process <b>600</b> proceeds to <b>620</b>. At <b>620</b>, control circuitry <b>304</b> may augment the search quart with metadata (e.g., the title) of the matching template. For example, the value of &#x201c;title&#x201d; field of a matching template is retrieved and added to the search query. At <b>624</b>, control circuitry <b>304</b> may perform a search (e.g., an Internet search via internet source <b>462</b>) using the augmented query. At <b>626</b>, control circuitry <b>304</b> may output the results of the search on a screen (e.g., display <b>312</b>) or as a voice output (e.g., via output device <b>402</b>).</p><p id="p-0059" num="0057"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of another illustrative process for identifying a performed action, in accordance with some embodiments of the disclosure. In some embodiments, each step of process <b>700</b> can be performed by user device <b>300</b> (e.g. via control circuitry <b>304</b>) or any of the system components shown in <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>4</b></figref>. Process <b>700</b> may be performed as part of step <b>618</b> after a plurality of frames of a video is accessed.</p><p id="p-0060" num="0058">At <b>702</b>, control circuitry <b>304</b> may identify a character in the frame. In some embodiments, control circuitry <b>304</b> may use any known computer vision technique or AI human body search (e.g., using AI service <b>460</b>) to identify pixels of a frame that define a shape of a human body.</p><p id="p-0061" num="0059">At <b>704</b>, control circuitry <b>304</b> may identify body parts of the identified character. For example, control circuitry <b>304</b> may use any known computer vision technique or AI search to identify, torso, legs and arms. In some embodiments, insert something here? may generate a vector representation of each body part (e.g., as shown in element <b>230</b>-<b>238</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0062" num="0060">At <b>706</b>, control circuitry <b>304</b> may access a body part combination of the identified body parts. For example, the body part combination may include: {torso, left arm}, {torso, right arm}, {upper left arm, lower left arm}, {upper right arm, lower right arm}, {torso, left leg}, {torso, right leg }, {upper left leg, lower left leg}; {upper right leg, lower right leg}. At <b>708</b>, control circuitry <b>304</b> may calculate an angle for the selected body part combination. The resulting angle may be stored in memory <b>308</b> as part of a movement model (e.g., movement model <b>240</b>). At <b>710</b>, control circuitry <b>304</b>, may check if some body part combinations are not yet analyzed. If so, process <b>700</b> returns to <b>706</b>. Otherwise, process <b>700</b> proceeds to <b>712</b>.</p><p id="p-0063" num="0061">At <b>712</b>, control circuitry <b>304</b> may determine whether computed angles match expected angles listed in a movement template (e.g., table <b>242</b>.) For example, control circuitry <b>304</b> may check if the angles are within the range specified by the movement template or within threshold of an angle value specified by the movement template. If the angles match, process <b>700</b> may procced to step <b>714</b>. In some embodiments, process <b>700</b> may procced to step <b>714</b> only if the match succeeds for angles generated for each frame of a plurality of the plurality of frames extracted at step <b>612</b>. If the match fails, process <b>700</b> proceeds to <b>716</b>.</p><p id="p-0064" num="0062">At <b>714</b>, control circuitry <b>304</b> determines that the movement model matches the template. At <b>716</b>, control circuitry <b>304</b> determines that the movement model does not match the template. This determination may be used by process <b>600</b> to procced differently during step <b>618</b>.</p><p id="p-0065" num="0063"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart of an illustrative process for accessing a plurality of frames, in accordance with some embodiments of the disclosure. In some embodiments, each step of process <b>800</b> can be performed by user device <b>300</b> (e.g. via control circuitry <b>304</b>) or any of the system components shown in <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>4</b></figref>. Process <b>800</b> may be performed as part of step <b>612</b> to access a plurality of frames of a video. Step <b>800</b> is performed as an alternative to local extraction of frames using sampling buffer <b>406</b>, for example, if the user is watching a video on a smartphone with limited memory.</p><p id="p-0066" num="0064">At <b>802</b>, control circuitry <b>304</b> may receive a search query as described in step <b>502</b>. At <b>804</b>, control circuitry <b>304</b> may also receive an audio sample received concurrently with the search query. For example, voice input <b>452</b> may capture user voice and a sample of an audio track of the video that was being presented at the time (e.g., via audio source <b>456</b>).</p><p id="p-0067" num="0065">At <b>806</b>, control circuitry <b>304</b> checks if the received sample matches a sample from a database of video programming. For example, control circuitry <b>304</b> may calculate a freqeuncy signature of the sample (e.g., by using a Fourier transform) and compare it to a signature of videos stored in a database (e.g., via metadata sources <b>418</b>). For example, control circuitry <b>304</b> may determine that the signature matches a signature of a TV show &#x201c;Climbing the Eiger.&#x201d;</p><p id="p-0068" num="0066">At <b>810</b>, control circuitry <b>304</b> may perform the speech to text analysis of the audio sample. For example, control circuitry <b>304</b> may determine that the sample includes the dialogue line &#x201c;she is in a middle of a dangerous rappel.&#x201d; At <b>812</b>, control circuitry <b>304</b> may search the metadata of the matched video (e.g., timestamped metadata of TV show &#x201c;Climbing the Eiger&#x201d;) to identify a time location where the sample occurred. For example, control circuitry <b>304</b> may determine that the sample occurred at the 23:50 time mark of the TV show &#x201c;Climbing the Eiger.&#x201d;</p><p id="p-0069" num="0067">At steps <b>814</b>, <b>816</b>, and <b>820</b>, control circuitry <b>304</b> may extract frames of a remote copy of the identified video (e.g., &#x201c;Climbing the Eiger.&#x201d;). For example, control circuitry <b>304</b> may extract frames from a remote copy stored at an Internet location <b>462</b> or at metadata sources <b>418</b>. At <b>814</b>, control circuitry <b>304</b> may extract frames from a predetermined time period (e.g., 2 second) prior to the time location where the sample occurred (e.g., from 23:47-23:49). At <b>816</b>, control circuitry <b>304</b> may extract frames from the time location where the sample occurred (e.g., from 23:50). At <b>830</b>, control circuitry <b>304</b> may extract frames from a predetermined time period (e.g., 2 second) after the time location where the sample occurred (e.g., from 23:51-23:53). The extracted frames may then be accessed as described with respect to steps <b>508</b> and <b>612</b>.</p><p id="p-0070" num="0068"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart of a detailed illustrative process for analyzing features of relevant frames to refine a query, in accordance with some embodiments of the disclosure. In some embodiments, each step of process <b>900</b> can be performed by user device <b>300</b> (e.g. via control circuitry <b>304</b>) or any of the system components shown in <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>4</b></figref>. Process <b>900</b> may be performed as part of steps <b>510</b>-<b>518</b> or instead of the steps <b>510</b>-<b>518</b>.</p><p id="p-0071" num="0069">At <b>904</b>, control circuitry <b>304</b> may detect that a user paying attention to presentation of frames 1-N <b>902</b> while making a query (e.g., voice query as described in step <b>502</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>). For example, control circuitry <b>304</b> may use remote control signal to gage the level of engagement. In another example, control circuitry <b>304</b> may utilize camera input to ascertain that the user is engaged with presentation of frames <b>902</b>. When control circuitry <b>304</b> determine that the user is paying attention, process <b>900</b> proceeds to frame analysis <b>906</b>.</p><p id="p-0072" num="0070">At <b>906</b>, control circuitry <b>304</b> analyzes each of the frames <b>902</b> to identify objects that are displayed in each frame. For example, control circuitry <b>304</b> may use object recognition techniques to identify objects in each frame (e.g., actors, trees, cars, geographical features, buildings, etc.). For example, control circuitry <b>304</b> may create a table of objects that maps the objects to frames in which they appear. For example, control circuitry <b>304</b> may generate Table 1 (as shown below) based on frames <b>902</b>.</p><p id="p-0073" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="42pt" align="left"/><colspec colname="1" colwidth="84pt" align="left"/><colspec colname="2" colwidth="91pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="2" rowsep="1">TABLE 1</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Object</entry><entry>Frames</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Person A</entry><entry>Frames 1-10</entry></row><row><entry/><entry>Car</entry><entry>Frames 1-K</entry></row><row><entry/><entry>Tree</entry><entry>Frames I-K</entry></row><row><entry/><entry>Cityscape</entry><entry>Frames I-N</entry></row><row><entry/><entry>Person B</entry><entry>Frames K-N</entry></row><row><entry/><entry>Table</entry><entry>Frames 15-35</entry></row><row><entry/><entry>Chair</entry><entry>Frames 15-35</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><br/>Once objects are identified for each frame, process <b>900</b> proceeds to feature generation <b>908</b>.</p><p id="p-0074" num="0071">At <b>908</b>, control circuitry <b>304</b> may generate context (e.g., generate context data structures) for sets of frames. For example, control circuitry <b>304</b> may generate one context data structure for time period defined by frames 1-K and another context data structure for time period defined by frames 15-35. In some embodiments, control circuitry <b>304</b> generates feature keywords for the context data structure by analyzing objects present in certain frames. In one implementation, control circuitry <b>304</b> uses machine learning model that is trained to classify detected objects (e.g., objects of Table 1) to generate feature keyworks. In some embodiments, feature generation may include identification of actions performed in certain frames. For example, once a character is identified in frames <b>110</b> and <b>112</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, control circuitry <b>304</b> may use feature generation techniques to generate a feature keyword &#x201c;rappelling&#x201d; (e.g., as described with respect to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>). In some embodiments, control circuitry <b>304</b> may generate Table 2 (as shown below) based on Table 1.</p><p id="p-0075" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="21pt" align="left"/><colspec colname="1" colwidth="56pt" align="left"/><colspec colname="2" colwidth="140pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="2" rowsep="1">TABLE 2</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Time period</entry><entry>Features</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Frames 1-K</entry><entry>{Outdoors, Car Chase, Rome, Italy)</entry></row><row><entry/><entry>Frames 15-35</entry><entry>{Indoors, Kitchen, Cooking Pasta)</entry></row><row><entry/><entry>Frames K-N</entry><entry>{Outdoors, Mountains, Woman, Rappelling,</entry></row><row><entry/><entry/><entry>Eiger)</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0076" num="0072">In some embodiments, the detected features can be used to provide context to a user query was received during the presentation of frames <b>902</b> (e.g., a query received at step <b>502</b>). For example, a query (e.g., a voice query) received in step <b>502</b> may be received at some point during the presentation of frames 1-N <b>902</b>, but it may not be immediately apparent which frames of frames <b>902</b> are referenced by the query. To solve this problem, control circuitry <b>304</b> may search the features of Table 2 for matching contextual keywords.</p><p id="p-0077" num="0073">For example, control circuitry <b>304</b> may determine that the query includes the word &#x201c;car&#x201d; (e.g., when the query is &#x201c;what car is it?&#x201d;) and that a car was depicted in frames 1-K. In this case, control circuitry <b>304</b> may determine that the query was referencing frames 1-K. In another example control circuitry <b>304</b> may determine that the query includes the word &#x201c;doing&#x201d; (e.g., when the query is &#x201c;what is she doing?&#x201d;) and an action or rappelling was shown in frames K-N. In this case, control circuitry <b>304</b> may determine that the query was referencing frames K-N.</p><p id="p-0078" num="0074">It should be noted that while Tables 1 and 2 (or similar data structures) may be generated locally (e.g., by control circuitry <b>304</b>), in some embodiments, such data structures may be pre-generated and included in the video stream data (e.g., video stream from video source <b>454</b>). In some embodiments, the data structures may be included in Hypertext Transfer Protocol Live Streaming (HLS) playlist file. In some embodiments, the features of Table 1 or 2 may be encoded into each of the frames <b>902</b>.</p><p id="p-0079" num="0075">At <b>910</b>, control circuitry <b>304</b> may refine the query based on the context data generated at step <b>908</b>. For example, if the query referenced a car, control circuitry <b>304</b> may investigate frames 1-K to refine the query. In some embodiments, control circuitry <b>304</b> may know the position of the car in each frame such that only the relevant part of the image is analyzed. For example, if the query was &#x201c;what kind of car is this?&#x201d;, control circuitry <b>304</b> may determine that the car shown in frames 1-K is a Mercedes <b>500</b>, and modify the query to be &#x201c;Information about Mercedes <b>500</b>?&#x201d; In another example, if the query is &#x201c;How can I get there?&#x201d;, control circuitry <b>304</b> may analyze frames 1-K and determine that Rome cityscape is shown. In this case, control circuitry <b>304</b> may modify the query to be &#x201c;How can I get to Rome, Italy?&#x201d; In yet another embodiment, if the query is &#x201c;what is she doing?&#x201d; control circuitry <b>304</b> may analyze frames K-N and determine that a rappelling action was shown. In this case, control circuitry <b>304</b> may modify the query to be &#x201c;what is rappelling&#x201d; (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.).</p><p id="p-0080" num="0076">At <b>912</b>, control circuitry <b>304</b> may send the refined query to a voice service (e.g., AI Service <b>460</b>) via network (e.g., network <b>414</b>). In some embodiments, control circuitry <b>304</b> may receive search results from the voice service and output the received results (e.g., via speakers <b>314</b> or via display <b>312</b>).</p><p id="p-0081" num="0077">It should be noted that processes <b>500</b>-<b>900</b> or any step thereof could be performed on, or provided by, any of the devices shown in <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>3</b></figref>. For example, the processes may be executed by control circuitry <b>304</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>) as instructed by a search application. In addition, one or more steps of a process may be omitted, modified, and/or incorporated into or combined with one or more steps of any other process or embodiment (e.g., steps from process <b>600</b> may be combined with steps from processes <b>700</b>, <b>800</b>, and <b>900</b>). In addition, the steps and descriptions described in relation to <figref idref="DRAWINGS">FIGS. <b>4</b>-<b>9</b></figref> may be done in alternative orders or in parallel to further the purposes of this disclosure. For example, each of these steps may be performed in any order or in parallel or substantially simultaneously to reduce lag or increase the speed of the system or method.</p><p id="p-0082" num="0078">It will be apparent to those of ordinary skill in the art that methods involved in the present invention may be embodied in a computer program product that includes a computer-usable and/or -readable medium. For example, such a computer-usable medium may consist of a read-only memory device, such as a CD-ROM disk or conventional ROM device, or a random-access memory, such as a hard drive device or a computer diskette, having a computer-readable program code stored thereon. It should also be understood that methods, techniques, and processes involved in the present disclosure may be executed using processing circuitry.</p><p id="p-0083" num="0079">The processes discussed above are intended to be illustrative and not limiting. More generally, the above disclosure is meant to be exemplary and not limiting. Only the claims that follow are meant to set bounds as to what the present invention includes. Furthermore, it should be noted that the features and limitations described in any one embodiment may be applied to any other embodiment herein, and flowcharts or examples relating to one embodiment may be combined with any other embodiment in a suitable manner, done in different orders, or done in parallel. In addition, the systems and methods described herein may be performed in real time. It should also be noted, the systems and/or methods described above may be applied to, or used in accordance with, other systems and/or methods.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-01-30" num="01-30"><claim-text><b>1</b>-<b>30</b>. (canceled)</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. A method for providing contextual search results to ambiguous queries, the method comprising:<claim-text>receiving a search query during a presentation of a portion of a video;</claim-text><claim-text>determining whether at least one word in the search query is ambiguous;</claim-text><claim-text>in response to determining that at least one word in the search query is ambiguous, identifying a performed action from the portion of the video that was presented concurrently with receiving the search query;</claim-text><claim-text>determining a keyword associated with the identified performed action;</claim-text><claim-text>performing a text-based search based on the search query and the keyword; and</claim-text><claim-text>outputting results of the search.</claim-text></claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>,</claim-text><claim-text>wherein identifying the performed action comprises:<claim-text>identifying a character in the portion of the video;</claim-text><claim-text>generating a model of the identified character's movements;</claim-text><claim-text>determining that the generated model matches a movement template; and</claim-text></claim-text><claim-text>wherein retrieving the keyword associated with the identified performed action comprises retrieving metadata of the movement template.</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The method of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein generating a model of the identified character's movements comprises:<claim-text>identifying body parts of the identified character; and</claim-text><claim-text>calculating an angle between two body parts of the identified character.</claim-text></claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The method of <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein determining that the generated model matches the movement template comprises:<claim-text>comparing the calculated angle with a reference angle of the movement template; and</claim-text><claim-text>in response to determining that the calculated angle matches a reference angle determining that the generated model matches the movement template.</claim-text></claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein receiving the search query comprises:<claim-text>detecting user voice input; and</claim-text><claim-text>performing speech to text analysis of the user voice input to derive the search query.</claim-text></claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein determining that at least one word in the search query is ambiguous comprises:<claim-text>determining that the search query comprises at least one of: a pronoun and an auxiliary verb.</claim-text></claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein identifying the performed action from the portion of the video that was presented concurrently with receiving the search query further comprises:<claim-text>receiving an audio sample of the portion of the video that was presented concurrently with receiving the search query;</claim-text><claim-text>identifying a time location in the video where the audio sample occurred; and</claim-text><claim-text>extracting frames corresponding to the time location.</claim-text></claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. The method of <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein extracting frames corresponding to the time location comprises:<claim-text>extracting frames from a predetermined time period prior to the time location; and</claim-text><claim-text>extracting frames from a predetermined time period after the time location.</claim-text></claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein identifying the performed action from the portion of the video that was presented concurrently with receiving the search query further comprises capturing an extended portion of the video for a predetermined time period after receiving the search query.</claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein identifying the performed action from the portion of the video that was presented concurrently with receiving the search query further comprises capturing a predetermined number of displayed frames after receiving the search query.</claim-text></claim><claim id="CLM-00041" num="00041"><claim-text><b>41</b>. A system for providing contextual search results to ambiguous queries, the system comprising;<claim-text>input/output circuitry of a device configured to:<claim-text>receive a search query during a presentation of a video; and</claim-text></claim-text><claim-text>processing circuitry of the device configured to:<claim-text>determine whether at least one word in the search query is ambiguous;</claim-text><claim-text>in response to determining that at least one word in the search query is ambiguous, identify a performed action from the portion of the video that was presented concurrently with receiving the search query;</claim-text><claim-text>determine a keyword associated with the identified performed action;</claim-text><claim-text>perform a text-based search based on the search query and the keyword; and</claim-text><claim-text>output results of the search.</claim-text></claim-text></claim-text></claim><claim id="CLM-00042" num="00042"><claim-text><b>42</b>. The system of <claim-ref idref="CLM-00041">claim 41</claim-ref>,</claim-text><claim-text>wherein, when identifying the performed action, the processing circuitry is further configured to:<claim-text>identify a character in the portion of the video;</claim-text><claim-text>generate a model of the identified character's movements;</claim-text><claim-text>determine that the generated model matches a movement template; and</claim-text></claim-text><claim-text>wherein, when retrieving the keyword associated with the identified performed action, the processing circuitry is configured to retrieve metadata of the movement template.</claim-text></claim><claim id="CLM-00043" num="00043"><claim-text><b>43</b>. The system of <claim-ref idref="CLM-00042">claim 42</claim-ref>, wherein, when generating a model of the identified character's movements, the processing circuitry is further configured to:<claim-text>identify body parts of the identified character; and</claim-text><claim-text>calculate an angle between two body parts of the identified character.</claim-text></claim-text></claim><claim id="CLM-00044" num="00044"><claim-text><b>44</b>. The system of <claim-ref idref="CLM-00043">claim 43</claim-ref>, wherein, when determining that the generated model matches the movement template, the processing circuitry is further configured to:<claim-text>compare the calculated angle with a reference angle of the movement template; and</claim-text><claim-text>in response to determining that the calculated angle matches a reference angle, determine that the generated model matches the movement template.</claim-text></claim-text></claim><claim id="CLM-00045" num="00045"><claim-text><b>45</b>. The system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein, when receiving the search query, the processing circuitry is further configured to:<claim-text>detect user voice input; and</claim-text><claim-text>perform speech to text analysis of the user voice input to derive the search query.</claim-text></claim-text></claim><claim id="CLM-00046" num="00046"><claim-text><b>46</b>. The system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein, when determining that at least one word in the search query is ambiguous, the processing circuitry is further configured to:<claim-text>determine that the search query comprises at least one of: a pronoun and an auxiliary verb.</claim-text></claim-text></claim><claim id="CLM-00047" num="00047"><claim-text><b>47</b>. The system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein, when identifying the performed action from the portion of the video that was presented concurrently with receiving the search query further comprises, the processing circuitry is further configured to:<claim-text>receive an audio sample of the portion of the video that was presented concurrently with receiving the search query;</claim-text><claim-text>identify a time location in the video where the audio sample occurred; and</claim-text><claim-text>extract frames corresponding to the time location.</claim-text></claim-text></claim><claim id="CLM-00048" num="00048"><claim-text><b>48</b>. The system of <claim-ref idref="CLM-00047">claim 47</claim-ref>, wherein, when extracting frames corresponding to the time location, the processing circuitry is further configured to:<claim-text>extract frames from a predetermined time period prior to the time location; and</claim-text><claim-text>extract frames from a predetermined time period after the time location.</claim-text></claim-text></claim><claim id="CLM-00049" num="00049"><claim-text><b>49</b>. The system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein, when identifying the performed action from the portion of the video that was presented concurrently with receiving the search query further, the processing circuitry is further configured to capture an extended portion of the video for a predetermined time period after receiving the search query.</claim-text></claim><claim id="CLM-00050" num="00050"><claim-text><b>50</b>. The system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein, when identifying the performed action from the portion of the video that was presented concurrently with receiving the search query further, the processing circuitry is further configured to capture a predetermined number of displayed frames after receiving the search query.</claim-text></claim></claims></us-patent-application>