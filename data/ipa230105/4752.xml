<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004753A9-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004753</doc-number><kind>A9</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17209051</doc-number><date>20210322</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><us-publication-filing-type><us-republication-corrected/></us-publication-filing-type><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>CN202010399048.X</doc-number><date>20200512</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6257</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6276</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD, APPARATUS, ELECTRONIC DEVICE AND STORAGE MEDIUM FOR TRAINING SEMANTIC SIMILARITY MODEL</invention-title><us-related-documents><related-publication><document-id><country>US</country><doc-number>20220300763</doc-number><kind>A1</kind><date>20220922</date></document-id></related-publication></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE AND TECHNOLOGY CO.,LTD.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Li</last-name><first-name>Zhen</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Li</last-name><first-name>Yukun</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Sun</last-name><first-name>Yu</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure provides a method, apparatus, electronic device and storage medium for training a semantic similarity model, which relates to the field of artificial intelligence. A specific implementation solution is as follows: obtaining a target field to be used by a semantic similarity model to be trained; calculating respective correlations between the target field and application fields corresponding to each of training datasets in known multiple training datasets; training the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets. According to the technical solution of the present disclosure, it is possible to, in the fine-tuning phase, more purposefully train the semantic similarity model with the training datasets with reference to the correlations between the target field and the application fields corresponding to the training datasets, thereby effectively improving the learning capability of the sematic similarity model and effectively improving the accuracy of the trained semantic similarity model.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="82.04mm" wi="99.65mm" file="US20230004753A9-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="101.43mm" wi="101.68mm" file="US20230004753A9-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="169.59mm" wi="102.28mm" file="US20230004753A9-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="97.45mm" wi="69.51mm" file="US20230004753A9-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="145.29mm" wi="69.51mm" file="US20230004753A9-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="120.40mm" wi="148.93mm" file="US20230004753A9-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">The present application claims the priority of Chinese Patent Application No. 202010399048.X, filed on May 12, 2020, with the title of &#x201c;Method, apparatus, electronic device and storage medium for training semantic similarity model&#x201d;. The disclosure of the above application is incorporated herein by reference in its entirety.</p><heading id="h-0001" level="1">FIELD OF THE DISCLOSURE</heading><p id="p-0003" num="0002">The present disclosure relates to computer technology, particularly to artificial intelligence technology, and specifically to a method, apparatus, electronic device and storage medium for training a semantic similarity model.</p><heading id="h-0002" level="1">BACKGROUND OF THE DISCLOSURE</heading><p id="p-0004" num="0003">In the prior art, performing self-supervised pre-training learning of a language model using a lot of unsupervised texts and then performing parameter fine-tuning for the language by using supervised task data is an advanced model training technique in the current field of Natural Language Processing (NLP).</p><p id="p-0005" num="0004">For example, a semantic matching task in the field of NLP aims to judge whether two texts are similar semantically. At present, there are few annotation data available for text matching, and the fields of the annotation data are also largely different. Therefore, in the fine-tuning phase, the current high-quality annotation data needs to be better used to fine tune a pre-trained semantic similarity model to mine the potential of the model and improve the model effect. Sets of high-quality annotation data often used in the prior art include lcqmc, bq_corpus etc. To improve the training effect, multiple types of datasets are integrated together in the prior art to train the semantic similarity model in the fine-tuning phase.</p><p id="p-0006" num="0005">However, simply integrating multiple types of datasets to train the semantic similarity model in the fine-tuning phase in the prior art causes extremity and undesirable accuracy of the trained semantic similarity model.</p><heading id="h-0003" level="1">SUMMARY OF THE DISCLOSURE</heading><p id="p-0007" num="0006">To address the above technical problems, the present disclosure provides a method, apparatus, electronic device and storage medium for training a semantic similarity model.</p><p id="p-0008" num="0007">According to a first aspect of the present disclosure, there is provided a method for training a semantic similarity model, comprising:</p><p id="p-0009" num="0008">obtaining a target field to be used by a semantic similarity model to be trained;</p><p id="p-0010" num="0009">calculating respective correlations between the target field and application fields corresponding to each of training datasets in known multiple training datasets;</p><p id="p-0011" num="0010">training the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets.</p><p id="p-0012" num="0011">According to a second aspect of the present disclosure, there is provided an electronic device, comprising:</p><p id="p-0013" num="0012">at least one processor; and</p><p id="p-0014" num="0013">a memory communicatively connected with the at least one processor;</p><p id="p-0015" num="0014">wherein the memory stores instructions executable by the at least one processor, and the instructions are executed by the at least one processor to enable the at least one processor to perform a method for training a semantic similarity model, wherein the method comprises:</p><p id="p-0016" num="0015">obtaining a target field to be used by a semantic similarity model to be trained;</p><p id="p-0017" num="0016">calculating respective correlations between the target field and application fields corresponding to each of training datasets in known multiple training datasets;</p><p id="p-0018" num="0017">training the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets.</p><p id="p-0019" num="0018">According to a third aspect of the present disclosure, there is provided anon-transitory computer readable storage medium with computer instructions stored thereon, wherein the computer instructions are used for causing a computer to perform a method for training a semantic similarity model, wherein the method comprises:</p><p id="p-0020" num="0019">obtaining a target field to be used by a semantic similarity model to be trained;</p><p id="p-0021" num="0020">calculating respective correlations between the target field and application fields corresponding to each of training datasets in known multiple training datasets;</p><p id="p-0022" num="0021">training the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets.</p><p id="p-0023" num="0022">According to the technology of the present application, the problem about the extremity of the semantic similarity model trained by integrating multiple types of training datasets in the prior art is solved, and it is possible to, in the fine-tuning phase, more purposefully train the semantic similarity model with the training datasets with reference to the correlations between the target field and the application fields corresponding to the training datasets, thereby effectively improving the learning capability of the sematic similarity model and effectively improving the accuracy of the trained semantic similarity model.</p><p id="p-0024" num="0023">It is to be understood that the summary section is not intended to identify key or essential features of embodiments of the present disclosure, nor is it intended to be used to limit the scope of the present disclosure. Other features of the present disclosure will become easily comprehensible through the following description.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0025" num="0024">The figures are intended to facilitate understanding the solutions, not to limit the present disclosure. In the figures,</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a schematic diagram of a first embodiment according to the present disclosure;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a schematic diagram of a second embodiment according to the present disclosure;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a schematic diagram of a third embodiment according to the present disclosure;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a schematic diagram of a fourth embodiment according to the present disclosure;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a block diagram of an electronic device for implementing a method for training a semantic similarity model according to embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS</heading><p id="p-0031" num="0030">Exemplary embodiments of the present disclosure are described below with reference to the accompanying drawings, include various details of the embodiments of the present disclosure to facilitate understanding, and should be considered as being only exemplary. Therefore, those having ordinary skill in the art should recognize that various changes and modifications can be made to the embodiments described herein without departing from the scope and spirit of the application. Also, for the sake of clarity and conciseness, depictions of well-known functions and structures are omitted in the following description.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a schematic diagram of a first embodiment according to the present disclosure; as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a method for training a semantic similarity model according to the present embodiment may specifically include the following steps:</p><p id="p-0033" num="0032">S<b>101</b>: obtaining a target field to be used by a semantic similarity model to be trained;</p><p id="p-0034" num="0033">S<b>102</b>: calculating respective correlations between the target field and application fields corresponding to each of training datasets in known multiple training datasets;</p><p id="p-0035" num="0034">S<b>103</b>: training the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets.</p><p id="p-0036" num="0035">A subject for executing the method for training a semantic similarity model according to the present embodiment is an apparatus for training the semantic similarity model. The apparatus may be an electronic entity similar to a computer, or may be an application integrated with software, the application, upon use, running on the computer device to train the semantic similarity model.</p><p id="p-0037" num="0036">The method of training the sematic similarity model according to the present embodiment is applied in the training in the parameter fine-tuning phase. In a semantic matching task of the NLP field, a preliminary structure of the semantic similarity model is already obtained in the pre-training phase. Then, in the fine-turning phase, parameter fine-tuning may be performed for the pre-trained semantic similarity model using corresponding high-quality task datasets. However, there are less training datasets in the current fine-tuning phase, for example, a lcqmc dataset constituted by similarity data in the field of Baidu Know-how, a bq_corpus dataset constituted by similarity data in the banking and financial field, etc. likewise, it is further possible to mine to get datasets constituted by similarity data in other fields such as E-commerce, medical care, education, economy, sports and music. However, in the solution in the prior art, these datasets are directly integrated together to train the pre-trained semantic similarity model, which causes a poor accuracy of the trained sematic similarity model.</p><p id="p-0038" num="0037">In the present embodiment, to improve the accuracy of the semantic similarity model, the fine-tuning in the fine-tuning phase needs to be performed with reference to the target field to be used by the semantic similarity model. Specifically, the target field to be used by the semantic similarity model to be trained is obtained first. In the present embodiment, the semantic similarity model may be preset to correspond to fields. A plurality of fields may be set according to actual needs. In addition, in the present embodiment, known training datasets and application fields corresponding to the training datasets need to be acquired, for example, the lcqmc dataset in the field of Baidu Know-know serves as a training dataset, and the bq_corpus dataset in the banking and financial field may also serve as a training dataset, etc. Then, correlations between the target field and the application fields corresponding to the known training datasets are calculated.</p><p id="p-0039" num="0038">It needs to be appreciated that the correlations here may be calculated based on the semantic similarity between the target field and the application fields corresponding to the training datasets. For example, specifically a duly-trained word vector model may be used to obtain a word vector of the target field and word vectors of the application fields corresponding to the training datasets, and then the correlations between the target field and the application fields corresponding to the training datasets may be obtained by calculating a similarity between word vectors. For example, the word vector model here may be implemented using a duy-trained Word2vec model.</p><p id="p-0040" num="0039">Finally, the semantic similarity model is trained with the training datasets in turn with reference to the correlations between the target field and the application fields corresponding to the known training datasets, to improve the accuracy of the semantic similarity model.</p><p id="p-0041" num="0040">According to the method of training the semantic similarity model of the present embodiment, the target field to be used by a semantic similarity model to be trained is obtained; the correlations between the target field and application fields corresponding to training datasets in known multiple training datasets are calculated; the semantic similarity model is trained with the training datasets in turn, according to the correlations between the target field and the application fields corresponding to the training datasets. According to the technical solution of the present embodiment, it is possible to, in the fine-tuning phase, more purposefully train the semantic similarity model with the training datasets with reference to the correlations between the target field and the application fields corresponding to the training datasets, thereby overcoming the problem about the extremity of the semantic similarity model trained by integrating multiple types of training datasets in the prior art, effectively improving the learning capability of the sematic similarity model and effectively improving the accuracy of the trained semantic similarity model.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a schematic diagram of a second embodiment according to the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the technical solution of the method for training the semantic similarity model according to the present embodiment will be further introduced in more detail on the basis of the above technical solution of the embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the method for training the semantic similarity model according to the present embodiment may specifically include the following steps:</p><p id="p-0043" num="0042">S<b>201</b>: obtaining a target field to be used by a semantic similarity model to be trained;</p><p id="p-0044" num="0043">S<b>202</b>: calculating respective correlations between the target field and application fields corresponding to each of training datasets;</p><p id="p-0045" num="0044">Steps S<b>201</b> and S<b>202</b> are the same as steps S<b>101</b> and S<b>102</b> in the embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, respectively. For details, reference may be made to relevant depictions of the embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Detailed depictions will not be presented any more here.</p><p id="p-0046" num="0045">S<b>203</b>: according to the respective correlations between the target field and the application fields corresponding to each of the training datasets, dividing a plurality of training datasets into a set of high-correlation training datasets and a set of low-correlation training datasets;</p><p id="p-0047" num="0046">In the present embodiment, an example is taken in which a plurality of training datasets are divided into a set of high-correlation training datasets and a set of low-correlation training datasets. For example, upon dividing, training datasets of N application fields with the highest correlation with the target field may be directed to the set of high-correlation training datasets; other training datasets may be directed to the set of low-correlation training datasets. The value of N may be determined according to actual experience, for example, 1 may be taken as the value of N. At this time, the set of the high-correlation training datasets only include one training dataset, namely, a training dataset corresponding to the application field with the highest correlation with the target field, among the plurality of training datasets. Other training datasets among the plurality of training datasets are directed to the set of low-correlation training datasets. N may also take other values such as 2 or 3 according to actual experience.</p><p id="p-0048" num="0047">Optionally, the above dividing is exemplified as dividing the plurality of training datasets into two sets of training datasets. In practical application, if there are more training datasets, the plurality of training datasets may be divided into three or more sets of training datasets according actual needs. The correlations between application fields of the training datasets in each set of training dataset and the target field are close. For example, an example is taken in which the training datasets are divided into three sets of training datasets. Two correlation thresholds, namely, a first correlation threshold and a second correlation threshold, may be set, with the first correlation threshold being greater than the second correlation threshold. Upon dividing, the training datasets corresponding to the application fields with the correlation with the target field being greater than or equal to the first correlation threshold may be directed into a first-grade set of training datasets; the training datasets corresponding to the application fields with the correlation with the target field being greater than or equal to the second correlation threshold and smaller than the first correlation threshold may be directed into a second-grade set of training datasets; the training datasets corresponding to the application fields with the correlation with the target field being smaller than the second correlation threshold may be directed into a third-grade set of training datasets. Likewise, multi-grade sets of training datasets may also be provided according to actual needs, which will not be detailed any more here.</p><p id="p-0049" num="0048">S<b>204</b>: training the semantic similarity model with training datasets in the set of low-correlation training datasets;</p><p id="p-0050" num="0049">Optionally, when the step is implemented, any of the following manners may be employed for implementation:</p><p id="p-0051" num="0050">(1) training the semantic similarity model respectively with the corresponding training datasets in turn in an ascending order of correlations between the target field and the application fields corresponding to training datasets in the set of low-correlation training datasets; or</p><p id="p-0052" num="0051">(2) randomly ranking the training datasets in the set of low-correlation training datasets, and training the semantic similarity model with the corresponding training datasets in turn in the random ranking order.</p><p id="p-0053" num="0052">The semantic similarity model may be trained in any of the above manners with the training datasets in the set of low-correlation training datasets. Since the semantic similarity model is trained with the set of low-correlation training datasets before with the set of high-correlation training datasets, and the training datasets in the set of low-correlation training datasets exerts a small impact on the semantic similarity model, the above manner (2) randomly ranking the training datasets in the set of low-correlation training datasets and training the semantic similarity model with the corresponding training datasets in turn in the random ranking order may be employed.</p><p id="p-0054" num="0053">However, preferably, the correlations between the application fields corresponding to the training datasets in the set of low-correlation training datasets and the target field are at different levels, the above manner (1) training the semantic similarity model respectively with the corresponding training datasets in turn in an ascending order of correlations between the target field and the application fields corresponding to training datasets in the set of low-correlation training datasets may be employed so that the semantic similarity model gradually learns information in the training datasets of the application fields more related to the target field, thereby making the expression thereof in the target field more accurate.</p><p id="p-0055" num="0054">S<b>205</b>: training the semantic similarity model with the training datasets in the set of high-correlation training datasets.</p><p id="p-0056" num="0055">Optionally, if N=1 when the set of high-correlation training datasets is obtained by dividing, the semantic similarity model is trained directly with the training datasets in the set of high-correlation training datasets. If N&#x3e;1, the semantic similarity model is trained respectively with the corresponding training datasets in turn in an ascending order of correlations between the target field and the application fields corresponding to training datasets in the set of high-correlation training datasets, so that the semantic similarity model gradually learns information in the training datasets of the application fields more related to the target field, thereby making the expression thereof in the target field more accurate.</p><p id="p-0057" num="0056">According to the above technical solution of the present embodiment, the set of low-correlation training datasets and the set of high-correlation training datasets are divided, the semantic similarity model is trained first with the training datasets in the set of low-correlation training datasets, and the semantic similarity model is trained first with the training datasets in the set of high-correlation training datasets. In this way, the semantic similarity model first learns the information of low-correlation training datasets, and then learns the information of high-correlation training datasets so that the accuracy of the semantic similarity model in the target field is made higher.</p><p id="p-0058" num="0057">In addition, optionally, in the above manner stated above, if there are multiple grades of sets of training datasets with the correlation, the semantic similarity model is trained first with the training datasets in the set of low-correlation training datasets in turn in an ascending order of correlations between the training datasets in multi-grade sets of training datasets, and then the semantic similarity model is trained with the training datasets in the set of high-correlation training datasets, so that when the semantic similarity model of the target field is trained, a gradual learning process in an ascending order of correlations is performed. This is a better learning process, so that the learning effect of the semantic similarity model is made better, and the expression of the semantic similarity model of the target field obtained from the learning is more accurate.</p><p id="p-0059" num="0058">According to the above technical solution of the method of training the semantic similarity model in the present embodiment, the plurality of training datasets may be divided into the set of high-correlation training datasets and the set of low-correlation training datasets, and then the semantic similarity model is trained in turn with the training datasets in the set of low-correlation training datasets and the set of high-correlation training datasets, so that the semantic similarity model gradually learns information in the training datasets with a higher correlation with the target field, thereby making the accuracy of the semantic similarity model in the target field higher. Hence, the technical solution of the present embodiment can effectively improve the accuracy of the trained semantic similarity model.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a schematic diagram of a third embodiment according to the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the present embodiment provides an apparatus <b>300</b> for training a semantic similarity model, comprising:</p><p id="p-0061" num="0060">an obtaining module <b>301</b> configured to obtain a target field to be used by a semantic similarity model to be trained;</p><p id="p-0062" num="0061">a calculating module <b>302</b> configured to calculate respective correlations between the target field and application fields corresponding to each of training datasets in known multiple training datasets;</p><p id="p-0063" num="0062">a training module <b>303</b> configured to train the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets.</p><p id="p-0064" num="0063">Principles employed by the apparatus <b>300</b> for training the semantic similarity model of the present embodiment to implement the training of the semantic similarity model by using the above modules and the resultant technical effects are the same as those of the above relevant method embodiments. For particulars, please refer to the depictions of the aforesaid relevant method embodiments, and no detailed depictions will be presented here.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a schematic diagram of a fourth embodiment according to the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the apparatus <b>300</b> for training the semantic similarity model of the present embodiment will be further described in more detail on the basis of the technical solution of the embodiment shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0066" num="0065">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in the apparatus <b>300</b> for training the semantic similarity model of the present embodiment, the training module <b>303</b> comprises:</p><p id="p-0067" num="0066">a dividing unit <b>3031</b> configured to, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets, divide a plurality of training datasets into a set of high-correlation training datasets and a set of low-correlation training datasets;</p><p id="p-0068" num="0067">a first training unit <b>3032</b> configured to train the semantic similarity model in turn with training datasets in the set of low-correlation training datasets;</p><p id="p-0069" num="0068">a second training unit <b>3033</b> configured to train the semantic similarity model in turn with the training datasets in the set of high-correlation training datasets.</p><p id="p-0070" num="0069">Further optionally, the first training unit <b>3032</b> is configured to:</p><p id="p-0071" num="0070">train the semantic similarity model respectively with the corresponding training datasets in turn in an ascending order of correlations between the target field and the application fields corresponding to training datasets in the set of low-correlation training datasets; or</p><p id="p-0072" num="0071">randomly rank the training datasets in the set of low-correlation training datasets, and train the semantic similarity model with the corresponding training datasets in turn in the random ranking order.</p><p id="p-0073" num="0072">Further optionally, the dividing unit <b>3031</b> is configured to:</p><p id="p-0074" num="0073">direct training datasets of N application fields with the highest correlation with the target field to the set of high-correlation training datasets;</p><p id="p-0075" num="0074">direct other training datasets to the set of low-correlation training datasets.</p><p id="p-0076" num="0075">Further optionally, the second training unit <b>3033</b> is configured to:</p><p id="p-0077" num="0076">if N&#x3e;1, train the semantic similarity model respectively with the corresponding training datasets in turn in an ascending order of correlations between the target field and the application fields corresponding to training datasets in the set of high-correlation training datasets.</p><p id="p-0078" num="0077">Principles employed by the apparatus <b>300</b> for training the semantic similarity model of the present embodiment to implement the training of the semantic similarity model by using the above modules and the resultant technical effects are the same as those of the above relevant method embodiments. For particulars, please refer to the depictions of the aforesaid relevant method embodiments, and no detailed depictions will be presented here.</p><p id="p-0079" num="0078">According to embodiments of the present disclosure, the present disclosure further provides an electronic device and a readable storage medium.</p><p id="p-0080" num="0079">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, it shows a block diagram of an electronic device for implementing the method for training the semantic similarity model according to embodiments of the present disclosure. The electronic device is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The electronic device is further intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, wearable devices and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in the text here.</p><p id="p-0081" num="0080">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the electronic device comprises: one or more processors <b>501</b>, a memory <b>502</b>, and interfaces configured to connect components and including a high-speed interface and a low speed interface. Each of the components are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor can process instructions for execution within the electronic device, including instructions stored in or on the memory to display graphical information for a GUI on an external input/output device, such as a display device coupled to the interface. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple electronic devices may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). One processor <b>501</b> is taken as an example in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0082" num="0081">The memory <b>502</b> is a non-transitory computer-readable storage medium provided by the present disclosure. The memory stores instructions executable by at least one processor, so that the at least one processor executes the method for training the semantic similarity model according to the present disclosure. The non-transitory computer-readable storage medium of the present disclosure stores computer instructions, which are used to cause a computer to execute the method for training the semantic similarity model according to the present disclosure.</p><p id="p-0083" num="0082">The memory <b>502</b> is a non-transitory computer-readable storage medium and can be used to store non-transitory software programs, non-transitory computer executable programs and modules, such as program instructions/modules (e.g., relevant modules shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> through <figref idref="DRAWINGS">FIG. <b>4</b></figref>) corresponding to the method for training the semantic similarity model in embodiments of the present disclosure. The processor <b>501</b> executes various functional applications and data processing of the server, i.e., implements the method for training the semantic similarity model in the above method embodiments, by running the non-transitory software programs, instructions and modules stored in the memory <b>502</b>.</p><p id="p-0084" num="0083">The memory <b>502</b> may include a storage program region and a storage data region, wherein the storage program region may store an operating system and an application program needed by at least one function; the storage data region may store data created by the use of the electronic device for implementing the method for training the semantic similarity model. In addition, the memory <b>502</b> may include a high-speed random access memory, and may also include a non-transitory memory, such as at least one magnetic disk storage device, a flash memory device, or other non-transitory solid-state storage device. In some embodiments, the memory <b>502</b> may optionally include a memory remotely arranged relative to the processor <b>501</b>, and these remote memories may be connected to the electronic device for implementing the method for training the semantic similarity model through a network. Examples of the above network include, but are not limited to, the Internet, an intranet, a local area network, a mobile communication network, and combinations thereof.</p><p id="p-0085" num="0084">The electronic device for implementing the method for training the semantic similarity model may further include an input device <b>503</b> and an output device <b>504</b>. The processor <b>501</b>, the memory <b>502</b>, the input device <b>503</b> and the output device <b>504</b> may be connected through a bus or in other manners. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the connection through the bus is taken as an example.</p><p id="p-0086" num="0085">The input device <b>503</b> may receive inputted numeric or character information and generate key signal inputs related to user settings and function control of the electronic device for implementing the method for training the semantic similarity model, and may be an input device such as a touch screen, keypad, mouse, trackpad, touchpad, pointing stick, one or more mouse buttons, trackball and joystick. The output device <b>504</b> may include a display device, an auxiliary lighting device (e.g., an LED), a haptic feedback device (for example, a vibration motor), etc. The display device may include but not limited to a Liquid Crystal Display (LCD), a Light Emitting Diode (LED) display, and a plasma display. In some embodiments, the display device may be a touch screen.</p><p id="p-0087" num="0086">Various implementations of the systems and techniques described here may be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (Application Specific Integrated Circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations may include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to send data and instructions to, a storage system, at least one input device, and at least one output device.</p><p id="p-0088" num="0087">These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms &#x201c;machine-readable medium&#x201d; and &#x201c;computer-readable medium&#x201d; refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term &#x201c;machine-readable signal&#x201d; refers to any signal used to provide machine instructions and/or data to a programmable processor.</p><p id="p-0089" num="0088">To provide for interaction with a user, the systems and techniques described here may be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user may provide input to the computer. Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user may be received in any form, including acoustic, speech, or tactile input.</p><p id="p-0090" num="0089">The systems and techniques described here may be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a proxies component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the systems and techniques described here), or any combination of such back end, proxies, or front end components. The components of the system may be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (&#x201c;LAN&#x201d;), a wide area network (&#x201c;WAN&#x201d;), and the Internet.</p><p id="p-0091" num="0090">The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p><p id="p-0092" num="0091">According to the technical solutions of embodiments of the present disclosure, the target field to be used by the semantic similarity model to be trained is obtained; the correlations between the target field and application fields corresponding to training datasets in known multiple training datasets are calculated; the semantic similarity model is trained with the training datasets in turn, according to the correlations between the target field and the application fields corresponding to the training datasets. According to the technical solution of the present embodiment, it is possible to, in the fine-tuning phase, more purposefully train the semantic similarity model with the training datasets with reference to the correlations between the target field and the application fields corresponding to the training datasets, thereby overcoming the problem about the extremity of the semantic similarity model trained by integrating multiple types of training datasets in the prior art and effectively improving the accuracy of the trained semantic similarity model.</p><p id="p-0093" num="0092">According to the technical solutions of embodiments of the present disclosure, the plurality of training datasets may be divided into the set of high-correlation training datasets and the set of low-correlation training datasets, and then the semantic similarity model is trained in turn with the training datasets in the set of low-correlation training datasets and the set of high-correlation training datasets, so that the semantic similarity model gradually learns information in the training datasets with a higher correlation with the target field, thereby making the accuracy of the semantic similarity model in the target field higher. Hence, the technical solution of the present embodiment can effectively improve the accuracy of the trained semantic similarity model.</p><p id="p-0094" num="0093">It should be understood that the various forms of processes shown above can be used to reorder, add, or delete steps. For example, the steps described in the present disclosure can be performed in parallel, sequentially, or in different orders as long as the desired results of the technical solutions disclosed in the present disclosure can be achieved, which is not limited herein.</p><p id="p-0095" num="0094">The foregoing specific implementations do not constitute a limitation on the protection scope of the present disclosure. It should be understood by those skilled in the art that various modifications, combinations, sub-combinations and substitutions can be made according to design requirements and other factors. Any modification, equivalent replacement and improvement made within the spirit and principle of the present disclosure shall be included in the protection scope of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for training a semantic similarity model, wherein the method comprises:<claim-text>obtaining a target field to be used by a semantic similarity model to be trained;</claim-text><claim-text>calculating respective correlations between the target field and application fields corresponding to each of training datasets in known multiple training datasets;</claim-text><claim-text>training the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the training the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets comprises:<claim-text>according to the respective correlations between the target field and the application fields corresponding to each of the training datasets, dividing the plurality of training datasets into a set of high-correlation training datasets and a set of low-correlation training datasets;</claim-text><claim-text>training the semantic similarity model in turn with training datasets in the set of low-correlation training datasets;</claim-text><claim-text>training the semantic similarity model in turn with the training datasets in the set of high-correlation training datasets.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the training the semantic similarity model in turn with training datasets in the set of low-correlation training datasets comprises:<claim-text>training the semantic similarity model respectively with the corresponding training datasets in turn in an ascending order of correlations between the target field and the application fields corresponding to training datasets in the set of low-correlation training datasets; or</claim-text><claim-text>randomly ranking the training datasets in the set of low-correlation training datasets; training the semantic similarity model with the corresponding training datasets in turn in the random ranking order.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the step of, according to the correlations between the target field and the application fields corresponding to the training datasets, dividing the plurality of training datasets into a set of high-correlation training datasets and a set of low-correlation training datasets comprises:<claim-text>directing the training datasets of N application fields with the highest correlation with the target field to the set of high-correlation training datasets;</claim-text><claim-text>directing other training datasets to the set of low-correlation training datasets.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the training the semantic similarity model in turn with the training datasets in the set of high-correlation training datasets comprises:<claim-text>if N&#x3e;1, training the semantic similarity model respectively with the corresponding training datasets in turn in an ascending order of correlations between the target field and the application fields corresponding to training datasets in the set of high-correlation training datasets.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. An electronic device, comprising:<claim-text>at least one processor; and</claim-text><claim-text>a memory communicatively connected with the at least one processor;</claim-text><claim-text>wherein the memory stores instructions executable by the at least one processor, and the instructions are executed by the at least one processor to enable the at least one processor to perform a method for training a semantic similarity model, wherein the method comprises:</claim-text><claim-text>obtaining a target field to be used by a semantic similarity model to be trained;</claim-text><claim-text>calculating respective correlations between the target field and application fields corresponding to each of training datasets in known multiple training datasets;</claim-text><claim-text>training the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The electronic device according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the training the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets comprises:<claim-text>according to the respective correlations between the target field and the application fields corresponding to each of the training datasets, dividing the plurality of training datasets into a set of high-correlation training datasets and a set of low-correlation training datasets;</claim-text><claim-text>training the semantic similarity model in turn with training datasets in the set of low-correlation training datasets;</claim-text><claim-text>training the semantic similarity model in turn with the training datasets in the set of high-correlation training datasets.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The electronic device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the training the semantic similarity model in turn with training datasets in the set of low-correlation training datasets comprises:<claim-text>training the semantic similarity model respectively with the corresponding training datasets in turn in an ascending order of correlations between the target field and the application fields corresponding to training datasets in the set of low-correlation training datasets; or</claim-text><claim-text>randomly ranking the training datasets in the set of low-correlation training datasets, and training the semantic similarity model with the corresponding training datasets in turn in the random ranking order.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The electronic device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the step of, according to the correlations between the target field and the application fields corresponding to the training datasets, dividing the plurality of training datasets into a set of high-correlation training datasets and a set of low-correlation training datasets comprises:<claim-text>directing the training datasets of N application fields with the highest correlation with the target field to the set of high-correlation training datasets;</claim-text><claim-text>directing other training datasets to the set of low-correlation training datasets.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the training the semantic similarity model in turn with the training datasets in the set of high-correlation training datasets comprises:<claim-text>if N&#x3e;1, training the semantic similarity model respectively with the corresponding training datasets in turn in an ascending order of correlations between the target field and the application fields corresponding to training datasets in the set of high-correlation training datasets.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A non-transitory computer readable storage medium with computer instructions stored thereon, wherein the computer instructions are used for causing a computer to perform a method for training a semantic similarity model, wherein the method comprises:<claim-text>obtaining a target field to be used by a semantic similarity model to be trained;</claim-text><claim-text>calculating respective correlations between the target field and application fields corresponding to each of training datasets in known multiple training datasets;</claim-text><claim-text>training the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the training the semantic similarity model with the training datasets in turn, according to the respective correlations between the target field and the application fields corresponding to each of the training datasets comprises:<claim-text>according to the respective correlations between the target field and the application fields corresponding to each of the training datasets, dividing the plurality of training datasets into a set of high-correlation training datasets and a set of low-correlation training datasets;</claim-text><claim-text>training the semantic similarity model in turn with training datasets in the set of low-correlation training datasets;</claim-text><claim-text>training the semantic similarity model in turn with the training datasets in the set of high-correlation training datasets.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the training the semantic similarity model in turn with training datasets in the set of low-correlation training datasets comprises:<claim-text>training the semantic similarity model respectively with the corresponding training datasets in turn in an ascending order of correlations between the target field and the application fields corresponding to training datasets in the set of low-correlation training datasets; or</claim-text><claim-text>randomly ranking the training datasets in the set of low-correlation training datasets; training the semantic similarity model with the corresponding training datasets in turn in the random ranking order.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the step of, according to the correlations between the target field and the application fields corresponding to the training datasets, dividing the plurality of training datasets into a set of high-correlation training datasets and a set of low-correlation training datasets comprises:<claim-text>directing the training datasets of N application fields with the highest correlation with the target field to the set of high-correlation training datasets;</claim-text><claim-text>directing other training datasets to the set of low-correlation training datasets.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the training the semantic similarity model in turn with the training datasets in the set of high-correlation training datasets comprises:<claim-text>if N&#x3e;1, training the semantic similarity model respectively with the corresponding training datasets in turn in an ascending order of correlations between the target field and the application fields corresponding to training datasets in the set of high-correlation training datasets.</claim-text></claim-text></claim></claims></us-patent-application>