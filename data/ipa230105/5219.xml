<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005220A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005220</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943415</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-059676</doc-number><date>20200330</date></priority-claim><priority-claim sequence="02" kind="national"><country>JP</country><doc-number>2020-179647</doc-number><date>20201027</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>13</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>13</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>761</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30244</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e79">SHOOTING METHOD, SHOOTING INSTRUCTION METHOD, SHOOTING DEVICE, AND SHOOTING INSTRUCTION DEVICE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2021/012156</doc-number><date>20210324</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17943415</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Panasonic Intellectual Property Management Co., Ltd.</orgname><address><city>Osaka</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>YOSHIKAWA</last-name><first-name>Satoshi</first-name><address><city>Hyogo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>MATSUNOBU</last-name><first-name>Toru</first-name><address><city>Osaka</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>TERANISHI</last-name><first-name>Kensho</first-name><address><city>Osaka</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>FUKUDA</last-name><first-name>Masaki</first-name><address><city>Osaka</city><country>JP</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>KATSURA</last-name><first-name>Ukyou</first-name><address><city>Osaka</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A shooting method executed by a shooting device includes: shooting first images of a target space; generating a first three-dimensional point cloud of the target space, based on the first images and a first shooting position and a first shooting orientation of each of the first images; and determining a first region of the target space for which generating a second three-dimensional point cloud which is denser than the first three-dimensional point cloud is difficult, using the first three-dimensional point cloud and without generating the second three-dimensional point cloud. The determining includes generating a mesh using the first three-dimensional point cloud, and determining the region other than a second region of the target space for which the mesh is generated.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="151.55mm" wi="103.21mm" file="US20230005220A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="223.52mm" wi="124.04mm" orientation="landscape" file="US20230005220A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="235.37mm" wi="163.07mm" orientation="landscape" file="US20230005220A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="184.57mm" wi="93.90mm" file="US20230005220A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="238.25mm" wi="136.31mm" file="US20230005220A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="234.36mm" wi="124.71mm" file="US20230005220A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="250.53mm" wi="105.24mm" file="US20230005220A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="207.35mm" wi="83.23mm" orientation="landscape" file="US20230005220A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="252.05mm" wi="123.36mm" file="US20230005220A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="133.52mm" wi="94.06mm" file="US20230005220A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="224.03mm" wi="101.68mm" file="US20230005220A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="232.41mm" wi="140.04mm" file="US20230005220A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="226.99mm" wi="143.34mm" file="US20230005220A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="209.30mm" wi="158.24mm" file="US20230005220A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="160.36mm" wi="107.44mm" file="US20230005220A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="112.78mm" wi="89.75mm" file="US20230005220A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="152.48mm" wi="100.08mm" file="US20230005220A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="84.41mm" wi="101.52mm" file="US20230005220A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="112.18mm" wi="158.33mm" file="US20230005220A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="126.32mm" wi="95.59mm" file="US20230005220A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="123.44mm" wi="99.40mm" file="US20230005220A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="212.85mm" wi="140.97mm" file="US20230005220A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="105.75mm" wi="142.07mm" file="US20230005220A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="217.68mm" wi="143.34mm" file="US20230005220A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="77.64mm" wi="123.53mm" file="US20230005220A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="88.73mm" wi="108.46mm" file="US20230005220A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="100.67mm" wi="68.50mm" file="US20230005220A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="187.54mm" wi="79.16mm" file="US20230005220A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="187.79mm" wi="79.42mm" file="US20230005220A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="137.41mm" wi="98.13mm" file="US20230005220A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This is a continuation application of PCT International Application No. PCT/JP2021/012156 filed on Mar. 24, 2021, designating the United States of America, which is based on and claims priority of Japanese Patent Application No. 2020-059676 filed on Mar. 30, 2020 and Japanese Patent Application No. 2020-179647 filed on Oct. 27, 2020. The entire disclosures of the above-identified applications, including the specifications, drawings and claims are incorporated herein by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to shooting instruction methods, shooting methods, shooting instruction devices, and shooting devices.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Patent Literature (PTL) 1 discloses a technique of generating a three-dimensional model of a subject, using images obtained by shooting the subject from a plurality of viewpoints.</p><heading id="h-0004" level="1">CITATION LIST</heading><heading id="h-0005" level="1">Patent Literature</heading><p id="p-0005" num="0004">PTL 1: Japanese Unexamined Patent Application Publication No. 2017-130146</p><heading id="h-0006" level="1">SUMMARY</heading><p id="p-0006" num="0005">A shooting method according to an aspect of the present disclosure is a shooting method executed by a shooting device, and includes: shooting first images of a target space; generating a first three-dimensional point cloud of the target space, based on the first images and a first shooting position and a first shooting orientation of each of the first images; and determining a first region of the target space for which generating a second three-dimensional point cloud is difficult, using the first three-dimensional point cloud and without generating the second three-dimensional point cloud, the second three-dimensional point cloud being denser than the first three-dimensional point cloud, wherein the determining includes: generating a mesh using the first three-dimensional point cloud; and determining the region other than a second region of the target space, the second region being a region for which the mesh is generated.</p><p id="p-0007" num="0006">A shooting instruction method according to an aspect of the present disclosure is a shooting instruction method executed by a shooting instruction device, and includes: displaying a first image of a subject on which recognition of attributes is performed; getting input to specify a first attribute among the attributes to specify a first region in order to generate a three-dimensional model of the subject, based on second images generated by shooting the subject and on a shooting position and a shooting orientation of each of the second images; and outputting at least one of a shooting position or a shooting orientation so that a third image to be used in generating a first three-dimensional model of the first region specified is shot.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0007" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0008" num="0007">These and other advantages and features will become apparent from the following description thereof taken in conjunction with the accompanying Drawings, by way of non-limiting examples of embodiments disclosed herein.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a terminal device according to Embodiment 1.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a sequence diagram of a terminal device according to Embodiment 1.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of initial processing according to Embodiment 1.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of an initial display according to Embodiment 1.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating an example of a way of selecting a priority-assigned part according to Embodiment 1.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating an example of a way of selecting a priority-assigned part according to Embodiment 1.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of position and orientation estimation processing according to Embodiment 1.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart of shooting position candidate determination processing according to Embodiment 1.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating cameras and objects viewed from above according to Embodiment 1.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating an example of an image obtained by each camera according to Embodiment 1.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram for describing an example of shooting position candidate determination according to Embodiment 1.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic diagram for describing an example of shooting position candidate determination according to Embodiment 1.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a schematic diagram for describing an example of shooting position candidate determination according to Embodiment 1.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart of a three-dimensional reconstruction processing according to Embodiment 1.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart of in-shooting display processing according to Embodiment 1.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram illustrating an example of a way of visually presenting a shooting position candidate according to Embodiment 1.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram illustrating an example of a way of visually presenting a shooting position candidate according to Embodiment 1.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram illustrating an example display of an alert according to Embodiment 1.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a flowchart of shooting instruction processing according to Embodiment 1.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram illustrating a configuration of a three-dimensional reconstruction system according to according to Embodiment 2.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a block diagram illustrating a terminal device according to Embodiment 2.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a flowchart illustrating an operation of a shooting device according to Embodiment 2.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a flowchart of position/orientation estimation processing according to Embodiment 2.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flowchart of position and orientation integration processing according to Embodiment 2.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a plan view illustrating shooting in a target space according to Embodiment 2.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a diagram illustrating example images and example comparison processing according to Embodiment 2.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a flowchart of region detection processing according to Embodiment 2.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a flowchart of display processing according to Embodiment 2.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a diagram illustrating an example of an UI screen displayed according to Embodiment 2.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a diagram illustrating an example of region information according to Embodiment 2.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a diagram illustrating an example display in a case where the position/orientation estimation has failed according to Embodiment 2.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a diagram illustrating an example display in the case where a low-precision region is detected, according to Embodiment 2.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>33</b></figref> is a diagram illustrating an example of the instruction to a user according to Embodiment 2.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>34</b></figref> is a diagram illustrating examples of instructions (arrows) according to Embodiment 2.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>35</b></figref> is a diagram illustrating an example of region information according to Embodiment 2.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a plan view illustrating a shooting of a target region according to Embodiment 2.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>37</b></figref> is a diagram illustrating an example of shot regions in a case where three-dimensional points are used according to Embodiment 2.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>38</b></figref> is a diagram illustrating an example of shot regions in a case where a mesh is used according to Embodiment 2.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>39</b></figref> is a diagram illustrating an example of a depth image according to Embodiment 2.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>40</b></figref> is a diagram illustrating an example of shot regions in a case where a depth image is used according to Embodiment 2.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>41</b></figref> is a flowchart illustrating a shooting method according to Embodiment 2.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0008" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0050" num="0049">A shooting instruction method according to an aspect of the present disclosure is a shooting instruction method executed by a shooting instruction device, and includes: getting input to specify a first region in order to generate a three-dimensional model of a subject, based on images generated by shooting the subject and on a shooting position and a shooting orientation of each of the images; and instructing at least one of a shooting position or a shooting orientation so that an image to be used in generating a three-dimensional model of the first region specified is shot.</p><p id="p-0051" num="0050">Accordingly, since the precision of the three-dimensional model of the region required by the user can be preferentially improved, the precision of the three-dimensional model can be improved.</p><p id="p-0052" num="0051">For example, the shooting instruction method includes: detecting a second region for which generating of the three-dimensional model is difficult, based on the images, the shooting positions, and the shooting orientations; and instructing at least one of a shooting position or a shooting orientation so that an image which facilitates generating of a three-dimensional model of the second region is generated. In the instructing corresponding to the first region, the at least one of the shooting position or the shooting orientation is instructed shoots that the image which facilitates the generating of the three-dimensional model of the first region is shot.</p><p id="p-0053" num="0052">The shooting instruction method further includes: displaying an image of the subject on which recognition of an attribute has been performed. In the getting of input to specify the first region, input of the attribute is gotten.</p><p id="p-0054" num="0053">For example, the detecting of the second region may include: (i) calculating an edge, on a two-dimensional image, for which an angular difference with an epipolar line based on the shooting position and the shooting orientation is smaller than a predetermined value; and (ii) detecting, as the second region, a three-dimensional region corresponding to the edge calculated. In the instructing corresponding to the second region, the at least one of the shooting position or the shooting orientation may be instructed shoots that an image for which the angular difference is bigger than the predetermined value is shot.</p><p id="p-0055" num="0054">For example, the images may be frames included in a moving image that is currently being shot and displayed, and the instructing corresponding to the second region may be performed in real time.</p><p id="p-0056" num="0055">Accordingly, shooting instruction can be performed in real time, and thus user convenience can be improved.</p><p id="p-0057" num="0056">For example, in the instructing corresponding to the second region, a shooting direction may be instructed.</p><p id="p-0058" num="0057">Accordingly, the user can easily perform ideal shooting following the instruction.</p><p id="p-0059" num="0058">For example, in the instructing corresponding to the second region, a shooting region may be instructed.</p><p id="p-0060" num="0059">Accordingly, the user can easily perform ideal shooting following the instruction.</p><p id="p-0061" num="0060">A shooting instruction device according to an aspect of the present disclosure includes: a processor; and memory, wherein input to specify a first region is gotten in order to generate a three-dimensional model of a subject, based on images generated by shooting the subject and on a shooting position and a shooting orientation of each of the images; and at least one of a shooting position or a shooting orientation is instructed shoots that an image to be used in generating a three-dimensional model of the first region specified is shot.</p><p id="p-0062" num="0061">Accordingly, since the precision of the three-dimensional model of the region required by the user can be preferentially improved, the precision of the three-dimensional model can be improved.</p><p id="p-0063" num="0062">A shooting instruction method according to an aspect of the present disclosure includes: detecting, based on images generated by shooting a subject and a shooting position and a shooting orientation of each of the images, a region for which generating of a three-dimensional model of the subject which uses the images is difficult; and instructing at least one of a shooting position or a shooting orientation shoots that an image which facilitates generating of a three-dimensional model of the region detected is shot.</p><p id="p-0064" num="0063">Accordingly, the precision of the three-dimensional model can be improved.</p><p id="p-0065" num="0064">For example, the shooting instruction method may further include: getting input to specify a priority region, and, in the instructing, the at least one of the shooting position or the imaging orientation may be instructed so that an image which facilitates generating of a three-dimensional model of the priority region specified is shot.</p><p id="p-0066" num="0065">Accordingly, the precision of the three-dimensional model of the region required by the user can be preferentially improved.</p><p id="p-0067" num="0066">A shooting method according to an aspect of the present disclosure is a shooting method executed by a shooting device, and includes: shooting first images of a target space; generating first three-dimensional position information of the target space, based on the first images and a first shooting position and a first shooting orientation of each of the first images; and determining a second region of the target space for which generating second three-dimensional position information is difficult, using the first three-dimensional position information and without generating the second three-dimensional position information, the second three-dimensional position information being more detailed than the first three-dimensional position information.</p><p id="p-0068" num="0067">According to the shooting method, since the second region for which generating the second three-dimensional position information is difficult can be determined using the first three-dimensional position information without generating the second three-dimensional position information, the efficiency of shooting of images used for generating the second three-dimensional position information can be improved.</p><p id="p-0069" num="0068">For example, the second region may be at least one of a region in which shooting of an image is not performed or a region for which precision of the second three-dimensional position information is estimated to be lower than a predetermined standard.</p><p id="p-0070" num="0069">For example, the first three-dimensional position information may include a first three-dimensional point cloud, and the second three-dimensional position information may include a second three-dimensional point cloud which is denser than the first three-dimensional point cloud.</p><p id="p-0071" num="0070">For example, in the determining, a third region of the target space which corresponds to a region in a vicinity of the first three-dimensional point cloud may be determined, and a region other than the third region may be determined as the second region.</p><p id="p-0072" num="0071">For example, the determining may include: generating a mesh using the first three-dimensional point cloud; and determining a region other than the third region of the target space as the second region, the third region corresponding to a region in which the mesh is generated.</p><p id="p-0073" num="0072">For example, in the determining, the second region may be determined based on a reprojection error of the first three-dimensional point cloud.</p><p id="p-0074" num="0073">For example, the first three-dimensional position information may include a depth image, and, in the determining, a region within a predetermined distance from a shooting viewpoint may be determined as a third region, and a region other than the third region may be determined as the second region.</p><p id="p-0075" num="0074">For example, the shooting method may further include combining, using second images that have already been shot, a second shooting position and a second shooting orientation of each of the second images, the first images, and the first shooting positions and the first shooting orientations, coordinate systems of the first shooting positions and the first shooting orientations with coordinate systems of the second shooting positions and the second shooting orientations.</p><p id="p-0076" num="0075">Accordingly, determining of the second region can be performed using information obtained by a plurality of shootings.</p><p id="p-0077" num="0076">For example, the shooting method may further include displaying the second region or a third region other than the second region during shooting of the target space.</p><p id="p-0078" num="0077">Accordingly, the second region can be presented to the user.</p><p id="p-0079" num="0078">For example, in the displaying, information indicating the second region or the third region may be displayed superimposed on any one of the images.</p><p id="p-0080" num="0079">Accordingly, the position of the second region inside the image can be presented to the user, and thus the user can easily know the position of the second region.</p><p id="p-0081" num="0080">For example, in the displaying, information indicating the second region or the third region may be displayed superimposed on a map of the target space.</p><p id="p-0082" num="0081">Accordingly, the position of the second region in the environment can be presented to the user, and thus the user can easily know the position of the second region.</p><p id="p-0083" num="0082">For example, the displaying may include displaying the second region and a reconstruction precision of each region included in the second region.</p><p id="p-0084" num="0083">Accordingly, since the user can know the restoration precision of each region, in addition to the second region, the user can perform appropriate shooting based on this.</p><p id="p-0085" num="0084">For example, the shooting method may further include presenting, to the user, an instruction for causing the user to shoot an image of the second region.</p><p id="p-0086" num="0085">Accordingly, the user can efficiently perform appropriate shooting.</p><p id="p-0087" num="0086">For example, the instruction may include at least one of a direction or a distance from a current position to the second region.</p><p id="p-0088" num="0087">Accordingly, the user can efficiently perform appropriate shooting.</p><p id="p-0089" num="0088">Furthermore, a shooting device according to an aspect of the present disclosure includes: a processor; and memory. Using the memory, the processor: shoots first images of a target space; generates first three-dimensional position information of the target space based on the first images and a first shooting position and a first shooting orientation of each of the first images; and determines a second region of the target space for which generating second three-dimensional position information using the first images is difficult, using the first three-dimensional position information and without generating the second three-dimensional position information, the second three-dimensional position information being more detailed than the first three-dimensional position information.</p><p id="p-0090" num="0089">According to the shooting device, since the second region for which generating the second three-dimensional position information is difficult can be determined using the first three-dimensional position information without generating the second three-dimensional position information, the efficiency of shooting of images used for generating the second three-dimensional position information can be improved.</p><p id="p-0091" num="0090">Note that these generic or specific aspects may be implemented as a system, a method, an integrated circuit, a computer program, or a computer-readable recording medium such as a CD-ROM, or may be implemented as any combination of a system, a method, an integrated circuit, a computer program, and a recording medium.</p><p id="p-0092" num="0091">Hereinafter, embodiments will be described in detail with reference to the drawings. Note that each of the embodiments described below shows a specific example of the present disclosure. The numerical values, shapes, materials, structural components, the arrangement and connection of the structural components, steps, the processing order of the steps, etc., shown in the following embodiments are mere examples, and thus are not intended to limit the present disclosure. Furthermore, among the structural components described in the following embodiments, structural components not recited in any one of the independent claims are described as optional structural components.</p><heading id="h-0009" level="1">Embodiment 1</heading><p id="p-0093" num="0092">A three-dimensional map or the like can be more easily generated by generating a three-dimensional model using images shot by a camera than in a method of generating a three-dimensional model using laser measurement. Therefore, a method of generating a three-dimensional model using images is used when measuring distances in the constructive management at construction sites, for example. Here, the three-dimensional model is a representation of a shot measurement subject on a computer. The three-dimensional model has position information on three-dimensional parts of the measurement subject, for example.</p><p id="p-0094" num="0093">However, when generating a three-dimensional model using images, a plurality of images of the same part having a parallax needs to be shot. Furthermore, the more the images, the denser the generated three-dimensional model can be. Furthermore, since the parallax has an influence on the precision of reproduction, the subject should be shot while appropriately moving when measuring distances. However, it is difficult to know an appropriate shooting position during shooting while coping with the status of the subject.</p><p id="p-0095" num="0094">According to the present embodiment, a user interface (UI) or the like will be described which detects a region that is difficult to three-dimensionally reconstruct (generate a three-dimensional model) and issues an instruction as to the shooting position or orientation based on the detection result. Such an UI can reduce the possibility of failure or the decrease of precision of the three-dimensional model reconstruction. In addition, since the occurrence of re-shootings can be reduced, so that the efficiency of the operation can be improved.</p><p id="p-0096" num="0095">First, a configuration of terminal device <b>100</b>, which is an example of a shooting instruction device according to the present embodiment, will be described. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating terminal device <b>100</b> according to the present embodiment. Terminal device <b>100</b> has a shooting function, a function of estimating a three-dimensional position and orientation during shooting, a function of determining a candidate for a next shooting position from a shot image, and a function of presenting, to a user, the estimated shooting position candidate. Terminal device <b>100</b> may also have a function of performing a three-dimensional reconstruction using an estimated three-dimensional position and orientation to generate a three-dimensional model, which is a three-dimensional point cloud of the shooting environment, a function of determining a candidate for a next shooing position using the three-dimensional model, a function of presenting, to the user, the estimated shooting position candidate, and a function of transmitting and receiving at least one of a shot video, a three-dimensional position and orientation, and a three-dimensional model to and from another terminal device, a management server or the like. Terminal device <b>100</b> includes shooting unit <b>101</b>, controller <b>102</b>, position and orientation estimator <b>103</b>, three-dimensional reconstructor <b>104</b>, image analyzer <b>105</b>, point cloud analyzer <b>106</b>, communication unit <b>107</b>, UI <b>108</b>, video storage <b>111</b>, camera orientation storage <b>112</b>, and three-dimensional model storage <b>113</b>.</p><p id="p-0097" num="0096">Shooting unit <b>101</b> is a shooting device, such as a camera, and obtains a video (moving image). Although examples in which video is used will be mainly described below, a plurality of static images may be used instead of the video. Shooting unit <b>101</b> stores the obtained video in video storage <b>111</b>. Shooting unit <b>101</b> may shoot a visible light image or an infrared image. When the infrared image is used, shooting is possible even in a dark environment, such as in the nighttime. Shooting unit <b>101</b> may be a monocular camera or may include a plurality of cameras such as in a stereo camera. By using a calibrated stereo camera, the precision of the three-dimensional position and orientation can be improved. Even if the stereo camera has not been calibrated, parallax images having a parallax can be obtained.</p><p id="p-0098" num="0097">Controller <b>102</b> controls the whole of the shooting processing and the like of terminal device <b>100</b>. Position and orientation estimator <b>103</b> estimates the three-dimensional position and orientation of the camera shooting the video using the video stored in video storage <b>111</b>. Position and orientation estimator <b>103</b> also stores the estimated three-dimensional position and orientation in camera orientation storage <b>112</b>. For example, position and orientation estimator <b>103</b> uses image processing, such as simultaneous localization and mapping (SLAM), to estimate the position and orientation of the camera. Alternatively, position and orientation estimator <b>103</b> may calculate the position and orientation of the camera using information obtained by various sensors (GPS or acceleration sensor) provided in terminal device <b>100</b>. In the former case, the position and orientation can be estimated from information from shooting unit <b>101</b>. In the latter case, the image processing can be achieved with low processing load.</p><p id="p-0099" num="0098">Three-dimensional reconstructor <b>104</b> generates a three-dimensional model by performing three-dimensional reconstruction using the video stored in video storage <b>111</b> and the three-dimensional position and orientation stored in camera orientation storage <b>112</b>. Three-dimensional reconstructor <b>104</b> stores the generated three-dimensional model in three-dimensional model storage <b>113</b>. For example, three-dimensional reconstructor <b>104</b> performs the three-dimensional reconstruction using image processing, such as structure from motion (SfM). Alternatively, when using video obtained with a calibrated camera, such as a stereo camera, three-dimensional reconstructor <b>104</b> may use the stereo parallax. In the former case, a precise three-dimensional model can be generated by using many images. In the latter case, a three-dimensional model can be quickly generated with light processing load.</p><p id="p-0100" num="0099">When SLAM is used by position and orientation estimator <b>103</b>, a three-dimensional model of the environment is also generated at the same time as the three-dimensional position and orientation being estimated. This three-dimensional model can also be used.</p><p id="p-0101" num="0100">Image analyzer <b>105</b> analyzes from what position the shooting should be performed and determines a shooting position candidate based on the analysis result, in order to perform precise three-dimensional reconstruction using the video stored in video storage <b>111</b> and the three-dimensional position and orientation stored in camera orientation storage <b>112</b>. Information indicating the determined shooting position candidate is output to UI <b>108</b> and presented to the user.</p><p id="p-0102" num="0101">Point cloud analyzer <b>106</b> determines the density or sparseness of a point cloud included in the three-dimensional model using the video stored in video storage <b>111</b>, the three-dimensional position and orientation stored in camera orientation storage <b>112</b>, and the three-dimensional model in three-dimensional model storage <b>113</b>. Point cloud analyzer <b>106</b> determines a shooting position candidate at which a sparse region can be shot. Point cloud analyzer <b>106</b> also detects a point cloud region that is generated using a peripheral region of an image where lens distortion is likely to occur, and determines a shooting position candidate at which the region is located at the center of the field of view of the camera. The determined shooting position candidate is output to UI <b>108</b> and presented to the user.</p><p id="p-0103" num="0102">Communication unit <b>107</b> transmits and receives the shot video and the calculated three-dimensional orientation and three-dimensional model to and from a cloud server or another terminal device via communication.</p><p id="p-0104" num="0103">UI <b>108</b> presents, to the user, the shot video and the shooting position candidate determined by image analyzer <b>105</b> and point cloud analyzer <b>106</b>. UI <b>108</b> also has an input function of receiving, from the user, input of a shooting start instruction, a shooting end instruction, and a priority processing part.</p><p id="p-0105" num="0104">Next, an operation of terminal device <b>100</b> according to the present embodiment will be described. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a sequence diagram illustrating exchanges of information and the like in terminal device <b>100</b>. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the hatched area shows that shooting unit <b>101</b> is continuing shooting. In order to generate a three-dimensional model of high quality, terminal device <b>100</b> analyzes a video and a shooting position and orientation in real time and issues a shooting instruction to the user. Here, analyzing in real time means performing analysis while shooting. Alternatively, analyzing in real time means performing analysis without generating a three-dimensional model. Specifically, terminal device <b>100</b> estimates the position and orientation of the camera during shooting, and determines, based on the estimation result and the shot video, a region that is difficult to reproduce. Terminal device <b>100</b> predicts a shooting position and orientation where a parallax that facilitates reproduction of the region is ensured, and presents the predicted shooting position and orientation on the UI. Although a sequence for moving image shooting is presented here, similar processing may be performed in shooting each single static image.</p><p id="p-0106" num="0105">First, UI <b>108</b> performs initial processing (S<b>101</b>). By this, UI <b>108</b> transmits a shooting start signal to shooting unit <b>101</b>. Start processing is performed by the user clicking a &#x201c;shooting start&#x201d; button on a display of terminal device <b>100</b>, for example. UI <b>108</b> then performs in-shooting display processing (S<b>102</b>). Specifically, UI <b>108</b> performs presentation of a video being shot and an instruction to the user.</p><p id="p-0107" num="0106">Upon receiving the shooting start signal, shooting unit <b>101</b> performs shooting of a video, and transmits image information, which is the shot video, to position and orientation estimator <b>103</b>, three-dimensional reconstructor <b>104</b>, image analyzer <b>105</b>, and point cloud analyzer <b>106</b>. For example, shooting unit <b>101</b> may perform streaming transmission, in which a video is transmitted as appropriate during shooting, or may transmit a batch of video in a certain length of time. That is, the image information is one or more images (frames) included in a video. In the former case, the processing can be performed as appropriate, so that the waiting time for the three-dimensional model generation can be reduced. In the latter case, much shooting information can be used, so that the processing can be achieved with high precision.</p><p id="p-0108" num="0107">Position and orientation estimator <b>103</b> first performs input waiting processing at the start of shooting to enter a state where position and orientation estimator <b>103</b> waits for image information from shooting unit <b>101</b>. When image information is input from shooting unit <b>101</b>, position and orientation estimator <b>103</b> performs position and orientation estimation processing (S<b>103</b>). That is, the position and orientation estimation processing is performed on a basis of one or more frames. When the position and orientation estimation processing has failed, position and orientation estimator <b>103</b> transmits an estimation failure signal to UI <b>108</b> to present the failure to the user. When the position and orientation estimation processing has succeeded, position and orientation estimator <b>103</b> transmits position and orientation information, which is the estimation result of the three-dimensional position and orientation, to UI <b>108</b> in order to output the current three-dimensional position and orientation. Position and orientation estimator <b>103</b> also transmits position and orientation information to image analyzer <b>105</b> and three-dimensional reconstructor <b>104</b>.</p><p id="p-0109" num="0108">Image analyzer <b>105</b> first performs input waiting processing at the start of shooting to enter a state where image analyzer <b>105</b> waits for image information from shooting unit <b>101</b> and position and orientation information from position and orientation estimator <b>103</b>. When image information and position and orientation information are input, image analyzer <b>105</b> performs shooting position candidate determination processing (S<b>104</b>). The shooting position candidate determination processing may be performed for each frame or for a certain length of time (a plurality of frames) (such as every 5 seconds). Image analyzer <b>105</b> also determines whether terminal device <b>100</b> is moving toward the shooting position candidate generated by the shooting position candidate determination processing, and need not perform another shooting position candidate determination processing if terminal device <b>100</b> is moving toward the shooting position candidate. For example, image analyzer <b>105</b> determines that terminal device <b>100</b> is moving toward the shooting position candidate if the current position and orientation are on the line connecting the position and orientation in the image based on which the shooting position candidate was determined and the position and orientation of the calculated candidate.</p><p id="p-0110" num="0109">Three-dimensional reconstructor <b>104</b> first performs input waiting processing at the start of shooting to enter a state where three-dimensional reconstructor <b>104</b> waits for image information from shooting unit <b>101</b> and position and orientation information from position and orientation estimator <b>103</b>. When image information and position and orientation information are input, three-dimensional reconstructor <b>104</b> performs three-dimensional reconstruction processing (S<b>105</b>) to calculate a three-dimensional model. Three-dimensional reconstructor <b>104</b> transmits point cloud information, which is the calculated three-dimensional model, to point cloud analyzer <b>106</b>.</p><p id="p-0111" num="0110">Point cloud analyzer <b>106</b> first performs input waiting processing at the start of shooting to enter a state where point cloud analyzer <b>106</b> waits for point cloud information from three-dimensional reconstructor <b>104</b>. When point cloud information is input, point cloud analyzer <b>106</b> performs shooting position candidate determination processing (S<b>106</b>). For example, point cloud analyzer <b>106</b> determines the state of density or sparseness of the whole point cloud, and detects a sparse region. Point cloud analyzer <b>106</b> determines a shooting position candidate at which much of the sparse region can be shot. Point cloud analyzer <b>106</b> may determine the shooting position candidate using not only point cloud information but also image information or position and orientation information.</p><p id="p-0112" num="0111">Next, the initial processing (S<b>101</b>) will be described. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of the initial processing (S<b>101</b>). First, UI <b>108</b> displays a current shot image (S<b>201</b>). UI <b>108</b> then obtains whether there is a priority part, which is a part the user wants to preferentially reproduce (S<b>202</b>). For example, UI <b>108</b> displays a button that designates a priority mode, and determines that there is a priority part if the button is pressed.</p><p id="p-0113" num="0112">When there is a priority part (Yes in S<b>203</b>), UI <b>108</b> displays a priority part selection screen (S<b>204</b>), and obtains information on a priority part selected by the user (S<b>205</b>). After step S<b>205</b>, or when there is no priority part (No in S<b>203</b>), UI <b>108</b> then outputs a shooting start signal to shooting unit <b>101</b>. In this way, shooting is started (S<b>206</b>). For example, shooting may be started by the user pressing a button or may be automatically started when a predetermined time has elapsed.</p><p id="p-0114" num="0113">Here, for the set priority part, the degree of priority of reproduction is set to be higher when issuing an instruction to move the camera or the like. As a result, no instruction is issued to move the camera to reproduce a region that is difficult to reproduce and is not required by the user, and an instruction as to a region required by the user can be issued.</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of an initial display in the initial processing (S<b>101</b>). Shot image <b>201</b>, priority assignment button <b>202</b> for selecting whether to select a priority-assigned part, and shooting start button <b>203</b> for starting shooting are displayed. Shot image <b>201</b> may be a static image or a video (moving image) currently being shot.</p><p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating an example of the way of selecting a priority-assigned part when assigning a priority. In this drawing, attribute recognition is performed for a target object, such as a window frame, and a desired target object is selected, in selection field <b>204</b>, from a list of target objects included in the image. For example, a label, such as window frame, desk, or wall, is assigned to each pixel in the image in an approach, such as semantic segmentation, and a batch of target pixels is selected by selecting a label.</p><p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating another example of the way of selecting a priority-assigned part when assigning a priority. In this drawing, a priority part is selected by specifying an arbitrary region (a region surrounded by a rectangle in the drawing) by means of a pointer or through a touch operation. Any means can be used as far as the user can select a particular region. For example, the selection may be made by specifying a color or by rough specification, such as a right half region of the image.</p><p id="p-0118" num="0117">Other ways of input than the operations on the screen can also be used. For example, the selection may be made through audio input. In that case, the input operation can be readily performed even by a user who is wearing gloves in cold weather and can hardly input by hand.</p><p id="p-0119" num="0118">Although an example has been shown here in which a priority part is selected in the initial processing before shooting, a priority part may be added as required during shooting. Therefore, a part that is not in the image at the time of the initial processing can be selected.</p><p id="p-0120" num="0119">Next, the position and orientation estimation processing (S<b>103</b>) will be described. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of the position and orientation estimation processing (<b>103</b>). First, position and orientation estimator <b>103</b> obtains one or more images or videos from video storage <b>111</b> (S<b>301</b>). Position and orientation estimator <b>103</b> then calculates or obtains, for an input image, position and orientation information including a camera parameter including the three-dimensional position and orientation (direction) of the camera, lens information and the like (S<b>302</b>). For example, position and orientation estimator <b>103</b> calculates position and orientation information by performing image processing, such as SLAM or SfM, on an image obtained in S<b>301</b>.</p><p id="p-0121" num="0120">Position and orientation estimator <b>103</b> then stores the position and orientation information obtained in S<b>302</b> in camera orientation storage <b>112</b> (S<b>303</b>).</p><p id="p-0122" num="0121">The image input in S<b>301</b> is an image sequence for a predetermined length of time that is formed by a plurality of frames, and the process of S<b>302</b> and the following steps may be performed on the image sequence (a plurality of images). Alternatively, images may be successively input in streaming or the like, and the process of S<b>302</b> and the following steps may be repeatedly performed on each image. In the former case, information at different points in time is used, and the precision can be improved. In the latter case, since images are successively input, a fixed length of input delay can be ensured, and the waiting time of the three-dimensional model generation can be reduced.</p><p id="p-0123" num="0122">Next, the shooting position candidate determination processing (S<b>104</b>) by image analyzer <b>105</b> will be described. <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart of the shooting position candidate determination processing (S<b>104</b>). First, image analyzer <b>105</b> obtains images or videos from video storage <b>111</b> (S<b>401</b>). One of the obtained images is set as a key image. Here, the key image is an image that serves as a reference in the subsequent three-dimensional reconstruction. For example, the depth for each pixel of the key image is estimated using information on the other images than the key image, and the three-dimensional reconstruction is performed using the estimated depths. Image analyzer <b>105</b> then obtains position and orientation information on each of the images (S<b>402</b>).</p><p id="p-0124" num="0123">Image analyzer <b>105</b> then uses the position and orientation information on each image to calculate an epipolar line between images (between cameras) (S<b>403</b>). Image analyzer <b>105</b> then detects an edge in each image (S<b>404</b>). For example, image analyzer <b>105</b> detects an edge through filtering processing, such as the Sobel filter.</p><p id="p-0125" num="0124">Image analyzer <b>105</b> then calculates an angle between the epipolar line and the edge in each image (S<b>405</b>). Image analyzer <b>105</b> then calculates the degree of difficulty of reproduction of each pixel in the key image based on the angle obtained in S<b>405</b> (S<b>406</b>). Specifically, the closer to parallel the epipolar line and the edge, the more difficult the three-dimensional reconstruction becomes, so that the degree of difficulty of reproduction is set to be higher as the angle becomes smaller. The degree of difficulty of reproduction may be set in two steps, such as high and low, or in more steps. For example, the degree of difficulty of reproduction may be set to be high when the angle is smaller than a predetermined value (such as 5 degrees) and to be low when the angle is greater than the predetermined value.</p><p id="p-0126" num="0125">Image analyzer <b>105</b> then estimates, based on the degree of difficulty of reproduction calculated in S<b>406</b>, a shooting position where a region of a high degree of difficulty of reproduction is easily reproduced, and determines the estimated position as a shooting position candidate (S<b>407</b>). Specifically, a region of a high degree of difficulty of reproduction means that the region is on the plane including the direction of displacement of the cameras, and the epipolar line and the edge come out of parallelism as the camera moves in a direction perpendicular to the plane. Therefore, when the camera is moving forward, the degree of difficulty of reproduction of the region of high degree of difficulty of reproduction can be reduced by moving the camera in the up-down direction or the left-right direction.</p><p id="p-0127" num="0126">Although the degree of difficulty of reproduction is described as being determined through the process from S<b>401</b> to S<b>406</b>, the present disclosure is not limited to this as far as the degree of difficulty of reproduction can be calculated. For example, the degradation of image quality due to lens distortion is more significant at the edges of the image than at the center of the image. Therefore, image analyzer <b>105</b> may determine an object that is captured only at an edge of each image, and set the degree of difficulty of reproduction of the region of the object to be high. For example, image analyzer <b>105</b> may determine the fields of view of the cameras in the three-dimensional space from the position and orientation information, and determine a region that is captured only at an edge of the screen based on the overlap of the fields of view of the cameras.</p><p id="p-0128" num="0127">In the following, that the difficulty of reproduction varies with the angle between the epipolar line and the edge will be described. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating cameras and objects viewed from above. <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating an example of an image obtained by each camera in the situation shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0129" num="0128">In this case, when searching an image from camera B for a corresponding point to a point in an image from camera A, an epipolar line that can be calculated from the camera geometry is searched. For point A, for which the epipolar line and the edge of the object in the image are parallel to each other, matching using pixel information, such as normalized cross correlation (NCC), is difficult, and it is difficult to determine a correct corresponding point.</p><p id="p-0130" num="0129">On the other hand, for point B, for which the epipolar line and the edge of the object in the image are perpendicular to each other, matching is easy, and a correct corresponding point can be determined. That is, by calculating the angle between the epipolar line and the edge in the image, the degree of difficulty of determination of a corresponding point can be determined. Whether a correct corresponding point can be determined depends on the precision of the three-dimensional reconstruction. Therefore, the angle between the epipolar line and the edge in the image can be used as the degree of difficulty of the three-dimensional reconstruction. Any information equivalent to angle can be used. For example, the epipolar line and the edge may be regarded as vectors, and the inner product of the epipolar line and the edge may be used.</p><p id="p-0131" num="0130">The epipolar line can be calculated using a fundamental matrix between camera A and camera B. The fundamental matrix can be calculated from the position and orientation information on camera A and camera B. Provided that intrinsic matrices of camera A and camera B are KA and KB, a relative rotation matrix of camera B viewed from camera A is R, and a relative movement vector is T, the epipolar line can be determined in the manner described below.</p><p id="p-0132" num="0131">For a pixel (x, y) on camera A, (a, b, c) are calculated according to the following formula, and the epipolar line on camera B can be represented as a straight line that satisfies a relation: ax+by+c=0.</p><p id="p-0133" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>F</mi>      <mo>=</mo>      <mrow>       <msubsup>        <mi>K</mi>        <mi>B</mi>        <mrow>         <mo>-</mo>         <mi>T</mi>        </mrow>       </msubsup>       <mo>*</mo>       <mrow>        <mo>(</mo>        <mrow>         <mrow>          <mo>[</mo>          <msub>           <mi>t</mi>           <mi>X</mi>          </msub>          <mo>]</mo>         </mrow>         <mo>*</mo>         <mi>R</mi>        </mrow>        <mo>)</mo>       </mrow>       <mo>*</mo>       <msubsup>        <mi>K</mi>        <mi>A</mi>        <mrow>         <mo>-</mo>         <mn>1</mn>        </mrow>       </msubsup>      </mrow>     </mrow>     <mo>&#x2062;</mo>     <mtext></mtext>     <mrow>      <mrow>       <mo>[</mo>       <msub>        <mi>t</mi>        <mi>X</mi>       </msub>       <mo>]</mo>      </mrow>      <mo>=</mo>      <mrow>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <mn>0</mn>         </mtd>         <mtd>          <mrow>           <mo>-</mo>           <msub>            <mi>T</mi>            <mi>z</mi>           </msub>          </mrow>         </mtd>         <mtd>          <msub>           <mi>T</mi>           <mi>Y</mi>          </msub>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>T</mi>           <mi>Z</mi>          </msub>         </mtd>         <mtd>          <mn>0</mn>         </mtd>         <mtd>          <mrow>           <mo>-</mo>           <msub>            <mi>T</mi>            <mi>X</mi>           </msub>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mo>-</mo>           <msub>            <mi>T</mi>            <mi>Y</mi>           </msub>          </mrow>         </mtd>         <mtd>          <msub>           <mi>T</mi>           <mi>X</mi>          </msub>         </mtd>         <mtd>          <mn>0</mn>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>     </mrow>     <mo>&#x2062;</mo>     <mtext></mtext>     <mrow>      <mrow>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <mi>a</mi>         </mtd>        </mtr>        <mtr>         <mtd>          <mi>b</mi>         </mtd>        </mtr>        <mtr>         <mtd>          <mi>c</mi>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mrow>       <mi>F</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mtable>        <mtr>         <mtd>          <mi>x</mi>         </mtd>        </mtr>        <mtr>         <mtd>          <mi>y</mi>         </mtd>        </mtr>        <mtr>         <mtd>          <mn>1</mn>         </mtd>        </mtr>       </mtable>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Math</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>1</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0134" num="0132">In the following, examples of the shooting position candidate determination will be described. <figref idref="DRAWINGS">FIG. <b>11</b></figref> to <figref idref="DRAWINGS">FIG. <b>13</b></figref> are schematic diagrams for describing examples of the shooting position candidate determination. As illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, edges of high degree of difficulty of reproduction are often a straight line or line segment on a plane in the three-dimensional space that passes through a straight line connecting the three-dimensional positions of camera A and camera B. Conversely, for a straight line perpendicular to a plane passing through such a straight line, the angle between the epipolar line and the edge is greater in the matching between camera A and camera B, and the degree of difficulty of reproduction is lower.</p><p id="p-0135" num="0133">That is, when determining a shooting position candidate, if shooting is performed at a position of camera C displaced from the target edge perpendicularly to the plane that includes the edge and satisfies the above condition as shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the degree of difficulty of reproduction for the target edge can be reduced in the matching between camera C and camera A or B. Therefore, image analyzer <b>105</b> determines camera C as the shooting position candidate.</p><p id="p-0136" num="0134">When there is no target edge, image analyzer <b>105</b> may determines a shooting position candidate in the manner described above by designating the edge of the highest degree of difficulty of reproduction as a candidate. Alternatively, image analyzer <b>105</b> may randomly select an edge from ten edges of the highest degrees of difficulty of reproduction.</p><p id="p-0137" num="0135">Although camera C is described here as being calculated from only the information on the pair of cameras (camera A and camera B), when there is a plurality of edges of high degree of difficulty of reproduction, image analyzer <b>105</b> may determine a shooting position candidate (camera C) for a first edge and a shooting position candidate (camera D) for a second edge, and output a route connecting camera C and camera D as shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0138" num="0136">The way of shooting position candidate determination is not limited to this. Considering that the influence of a distortion or the like on the image quality is smaller in a region closer to the center of the image, when an edge captured at an edge of the image from camera A is designated as a target, image analyzer <b>105</b> may determine, as a shooting position candidate, a position where the edge is captured at the center of the image.</p><p id="p-0139" num="0137">Next, the three-dimensional reconstruction processing (S<b>105</b>) will be described. <figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart of the three-dimensional reconstruction processing (S<b>105</b>). First, three-dimensional reconstructor <b>104</b> obtains a plurality of images or videos from video storage <b>111</b> (S<b>501</b>). Three-dimensional reconstructor <b>104</b> then obtains position and orientation information (camera parameter) on each of the images from camera orientation storage <b>112</b> (S<b>502</b>).</p><p id="p-0140" num="0138">Three-dimensional reconstructor <b>104</b> then performs three-dimensional reconstruction using the obtained images and the obtained position and orientation information to generate a three-dimensional model (S<b>503</b>). For example, three-dimensional reconstructor <b>104</b> performs the three-dimensional reconstruction using the silhouette volume intersection or SfM. Finally, three-dimensional reconstructor <b>104</b> stores the generated three-dimensional model in three-dimensional model storage <b>113</b> (S<b>504</b>).</p><p id="p-0141" num="0139">The processing of S<b>503</b> need not be performed by terminal device <b>100</b>. For example, terminal device <b>100</b> transmits the images and camera parameters to a cloud server or the like. The cloud server performs three-dimensional reconstruction to generate a three-dimensional model. Terminal device <b>100</b> receives the three-dimensional model from the cloud server. In this way, terminal device <b>100</b> can use a three-dimensional model of high quality, regardless of the capability of terminal device <b>100</b>.</p><p id="p-0142" num="0140">Next, the in-shooting display processing (S<b>102</b>) will be described. <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart of the in-shooting display processing (<b>102</b>). First, UI <b>108</b> displays a UI screen (S<b>601</b>). UI <b>108</b> then obtains and displays a shot image, which is an image being shot (S<b>602</b>). UI <b>108</b> then determines whether a shooting position candidate or estimation failure signal has been received (S<b>603</b>). Here, the estimation failure signal is a signal that is issued by position and orientation estimator <b>103</b> when the position and orientation estimation by position and orientation estimator <b>103</b> fails. The shooting position candidate is transmitted from three-dimensional reconstructor <b>104</b> or point cloud analyzer <b>106</b>.</p><p id="p-0143" num="0141">When a shooting position candidate is received (Yes in S<b>603</b>), UI <b>108</b> displays that there is a shooting position candidate (S<b>604</b>), and presents the shooting position candidate (S<b>605</b>). For example, UI <b>108</b> may visually display the shooting position candidate via the UI or may present the shooting position candidate by audio from a mechanism that outputs audio, such as a speaker. Specifically, an audio instruction &#x201c;raise the terminal device 20 cm&#x201d; may be issued in order to move terminal device <b>100</b> upward, or an audio instruction &#x201c;turn the terminal device 45&#xb0; to the right&#x201d; in order to shoot an area to the right. In that case, the user need not look at the screen of terminal device <b>100</b> when shooting while moving, and therefore can safely perform shooting.</p><p id="p-0144" num="0142">When terminal device <b>100</b> is provided with an oscillator, such as a vibrator, the presentation may be made by vibration. For example, rules may be previously established, such as that two short series of vibrations indicate to move the terminal device upward and one long series of vibrations indicates to turn the terminal device to the right, and the presentation may be made according to the rules. In that case, again, the user need not look at the screen and therefore can safely perform shooting.</p><p id="p-0145" num="0143">When receiving the estimation failure signal, UI <b>108</b> displays that the estimation has failed in S<b>604</b>.</p><p id="p-0146" num="0144">After S<b>605</b>, or when no shooting position candidate has been received (No in S<b>603</b>), UI <b>108</b> then determines whether there is a shooting end instruction (S<b>606</b>). The shooting end instruction may be an operation on the UI screen or may be audio. Alternatively, the shooting end instruction may be a gesture input, such as shaking terminal device <b>100</b> twice.</p><p id="p-0147" num="0145">When there is a shooting end instruction (Yes in S<b>606</b>), UI <b>108</b> issues a shooting end signal that makes shooting unit <b>101</b> to end shooting (S<b>607</b>). When there is no shooting end instruction (No in S<b>606</b>), UI <b>108</b> performs S<b>601</b> and the following steps again.</p><p id="p-0148" num="0146"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram illustrating an example of the way of visually presenting a shooting position candidate. An example is shown here in which the shooting position candidate is above the current position, and an instruction is issued to shoot from a position above the current position. In this example, up arrow <b>211</b> is presented on the screen. UI <b>108</b> may change the way of display (such as color or size) of the arrow depending on the distance from the current position to the shooting position candidate. For example, UI <b>108</b> may display a large red arrow when the current position is distant from the shooting position candidate, and change the arrow into a small green arrow as the current position comes closer to the shooting position candidate. When there is no shooting position candidate (that is, there is no region of high degree of difficulty of reproduction in the current shooting), UI <b>108</b> need not display an arrow or may present a circle symbol to indicate that the current shooting is going well.</p><p id="p-0149" num="0147"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram illustrating another example of the way of visually presenting a shooting position candidate. Here, UI <b>108</b> displays dotted frame <b>212</b> that is located at the center of the screen when the camera has moved to the shooting position candidate, and instructs the user to move the camera so as to bring dotted frame <b>212</b> closer to the center of the screen. The distance between the current position and the shooting position candidate may be indicated by the color or thickness of the frame.</p><p id="p-0150" num="0148">When the user fails to follow the instruction to bring the camera closer to the presented shooting position candidate, UI <b>108</b> may display message <b>213</b>, such as an alert, as illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>. UI <b>108</b> may also switch the way of instruction depending on the situation. For example, UI <b>108</b> may display the instruction with a small size immediately after starting the instruction, and enlarge the displayed instruction once a predetermined time has elapsed. Alternatively, UI <b>108</b> may issue an alert based on the time elapsed since the start of the instruction. For example, UI <b>108</b> may issue an alert if the instruction is not followed when one minute has elapsed since the start of the instruction.</p><p id="p-0151" num="0149">UI <b>108</b> may present rough information when the current position is distant from the shooting position candidate, and display the frame when the current position has come close to the shooting position candidate enough that the shooting position candidate can be shown in the screen.</p><p id="p-0152" num="0150">Other ways of displaying a shooting position candidate than those illustrated can also be used. For example, UI <b>108</b> may display a shooting position candidate on (two-dimensional or three-dimensional) map information. In that case, the user can intuitively know the direction in which the user should move.</p><p id="p-0153" num="0151">Although examples have been described here in which instructions are issued to the user, instructions may be issued to a moving body provided with a camera, such as a robot or a drone. In that case, the functionality of terminal device <b>100</b> may be included in the moving body. That is, the moving body may move to the shooting position candidate and shoot. In that case, a precise three-dimensional model can be stably generated by automatically controlled equipment.</p><p id="p-0154" num="0152">Information on a pixel determined to be high in degree of difficulty of reproduction may be used for three-dimensional reconstruction by terminal device <b>100</b> or a server. For example, terminal device <b>100</b> or a server may determine a three-dimensional point reconstructed using a pixel determined to be high in degree of difficulty of reproduction as a region or point of low precision. Furthermore, metadata representing the region or point of low precision may be imparted to the three-dimensional model or three-dimensional point. In that case, whether precision of the generated three-dimensional point is high or low can be determined in the subsequent processing. For example, the extent of correction in the filtering processing for three-dimensional points can be changed depending on the precision.</p><p id="p-0155" num="0153">As described above, the shooting instruction device according to the present embodiment performs the processes shown in <figref idref="DRAWINGS">FIG. <b>19</b></figref>. The shooting instruction device (for example, terminal device <b>100</b>) detects, based on images generated by shooting a subject and a shooting position and a shooting orientation of each of the images, a region (second region) for which generating of a three-dimensional model of the subject using the images is difficult (S<b>701</b>). Next, the shooting instruction device instructs at least one of a shooting position or a shooting orientation so that an image which facilitates generating of a three-dimensional model of the region detected is shot (S<b>702</b>). Accordingly, the precision of the three-dimensional model can be improved.</p><p id="p-0156" num="0154">Here, the image which facilitates generating of a three-dimensional model includes at least one of (1) an image of any region that is not shot from some of a plurality of shooting viewpoints, (2) an image of a region having small amount of blurring, (3) an image of a region that includes many feature points because of its higher contrast than the other regions, (4) an image of a region that is closer to a shooting viewpoint than the other regions and has a smaller error between the actual position and a calculated three-dimensional position, and (5) an image of a region that is less affected by a lens distortion than the other regions.</p><p id="p-0157" num="0155">For example, the shooting instruction device further gets input to specify a priority region (first region), and, in the instructing (S<b>702</b>), the at least one the shooting position or the imaging orientation is instructed so that an image which facilitates generating of a three-dimensional model of the priority region specified is shot. Accordingly, the precision of the three-dimensional model of the region required by the user can be preferentially improved.</p><p id="p-0158" num="0156">For example, the shooting instruction device: gets input to specify a first region (for example, a priority region) in order to generate a three-dimensional model of a subject, based on images generated by shooting the subject and on a shooting position and a shooting orientation of each of the images (S<b>205</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>); and instructs at least one of a shooting position or a shooting orientation so that an image to be used in generating a three-dimensional model of the first region specified is shot. Accordingly, since the precision of the three-dimensional model of the region required by the user can be preferentially improved, the precision of the three-dimensional model can be improved.</p><p id="p-0159" num="0157">For example, the shooting instruction device further: detects a second region for which generating of the three-dimensional model is difficult, based on the images, the shooting positions, and the shooting orientations (S<b>701</b>); and instructs at least one of a shooting position or a shooting orientation so that an image which facilitates generating of a three-dimensional model of the second region is generated (S<b>702</b>). In the instructing corresponding to the first region (S<b>702</b>), the at least one of the shooting position or the shooting orientation is instructed so that the image which facilitates the generating of the three-dimensional model of the first region is shot.</p><p id="p-0160" num="0158">For example, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, and so on, the shooting instruction device displays an image of the subject on which recognition of an attribute has been performed, and, in the getting of input to specify the first region, gets input of the attribute.</p><p id="p-0161" num="0159">For example, the detecting of the second region includes: (i) calculating an edge, on a two-dimensional image, for which an angular difference with an epipolar line based on the shooting position and the shooting orientation is smaller than a predetermined value; and (ii) detecting, as the second region, a three-dimensional region corresponding to the edge calculated. In the instructing corresponding to the second region, the shooting instruction device instructs the at least one of the shooting position or the shooting orientation so that an image for which the angular difference is bigger than the predetermined value is shot.</p><p id="p-0162" num="0160">For example, the images are frames included in a moving image that is currently being shot and displayed, and, the instructing corresponding to the second region (S<b>702</b>) is performed in real time. Accordingly, shooting instruction can be performed in real time, and thus user convenience can be improved.</p><p id="p-0163" num="0161">For example, in the instructing corresponding to the second region (S<b>702</b>), a shooting direction is instructed, as shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, for example. Accordingly, the user can easily perform appropriate shooting following the instruction. For example, the direction in which the next shooting position is located with respect to the current position is presented.</p><p id="p-0164" num="0162">For example, in the instructing corresponding to the second region (S<b>702</b>), a shooting region is instructed, as shown in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, for example. Accordingly, the user can easily perform ideal shooting following the instruction.</p><p id="p-0165" num="0163">For example, the shooting instruction device includes a processor and memory, and using the memory, the processor performs the above-described processes.</p><heading id="h-0010" level="1">Embodiment 2</heading><p id="p-0166" num="0164">A three-dimensional map or the like can be more easily generated by generating a three-dimensional model using images shot by a camera than in a method of generating a three-dimensional model using laser measurement. Here, the three-dimensional model is a representation of a shot measurement subject on a computer. The three-dimensional model has position information on three-dimensional parts of the measurement subject, for example.</p><p id="p-0167" num="0165">However, it is not easy to determine an image that is required by the user (the person who is shooting the image) when shooting images used for generating a three-dimensional model of a target space to be measured (referred to as a target space, hereinafter). Therefore, there is a possibility that no appropriate image is obtained, and as a result, no three-dimensional model can be reconstructed, or the precision of the three-dimensional model may decrease. Here, the precision means the error between position information of the three-dimensional model and the actual position. In the present embodiment, information for assisting the user to shoot is presented during shooting. This allows the user to efficiently shoot an appropriate image. In addition, the precision of the three-dimensional model generated can be improved.</p><p id="p-0168" num="0166">According to the present embodiment, specifically, a region that has not been shot is detected during shooting of a target space, and the detected region is presented to the user (the person who is shooting the image). Here, the region that has not been shot may include a region that has not been shot at that point in time in the shooting of the target space (such as a region hidden by another object) and a region that has been shot but has not resulted in any three-dimensional point. In addition, a region that is difficult to three-dimensionally reconstruct (generate a three-dimensional model) is detected, and the detected region is presented to the user. In this way, the efficiency of shooting can be improved, and the possibility of failure or the decrease of precision of the three-dimensional model reconstruction can be reduced.</p><p id="p-0169" num="0167">First an example configuration of a three-dimensional reconstruction system according to the present embodiment will be described. <figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram illustrating a configuration of the three-dimensional reconstruction system according to the present embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the three-dimensional reconstruction system includes shooting device <b>301</b> and reconstructing device <b>302</b>.</p><p id="p-0170" num="0168">Shooting device <b>301</b> is a terminal device used by the user and, for example, is a mobile terminal, such as a tablet terminal, a smartphone, or a notebook computer. Shooting device <b>301</b> has a shooting function, a function of estimating the position and orientation (referred to as a position/orientation, hereinafter) of a camera, and a function of displaying a shot region, for example. Furthermore, shooting device <b>301</b> transmits a shot image and a position/orientation to reconstructing device <b>302</b> during or after shooting. Here, the image is a moving image, for example. The image may be a plurality of static images. Shooting device <b>301</b> estimates a position/orientation during shooting, determines a shot region using at least one of the position/orientation and a three-dimensional point cloud, and presents the shot region to the user.</p><p id="p-0171" num="0169">Reconstructing device <b>302</b> is a server connected to shooting device <b>301</b> via a network or the like, for example. Reconstructing device <b>302</b> obtains an image shot by shooting device <b>301</b>, and generates a three-dimensional model using the obtained image. For example, reconstructing device <b>302</b> may use the camera position/orientation estimated by shooting device <b>301</b> or estimate the camera position from the obtained image.</p><p id="p-0172" num="0170">Data exchange between shooting device <b>301</b> and reconstructing device <b>302</b> may occur offline via a hard disk drive (HDD) or may constantly occur over a network.</p><p id="p-0173" num="0171">The three-dimensional model generated by reconstructing device <b>302</b> may be a dense three-dimensional point cloud of a three-dimensional space or a set of three-dimensional meshes. The three-dimensional point cloud generated by shooting device <b>301</b> is a sparse set of three-dimensional points formed by three-dimensionally reproducing feature points, such as a corner of an object in the space. That is, the three-dimensional model (three-dimensional point cloud) generated by shooting device <b>301</b> is a model having a lower spatial resolution than the three-dimensional model generated by reconstructing device <b>302</b>. In other words, the three-dimensional model (three-dimensional point cloud) generated by shooting device <b>301</b> is a simpler model than the three-dimensional model generated by reconstructing device <b>302</b>. The simpler model is a model having less amount of information, a model that is more easily generated, or a model having lower precision. For example, the three-dimensional model generated by shooting device <b>301</b> is a sparser three-dimensional point cloud than the three-dimensional model generated by reconstructing device <b>302</b>.</p><p id="p-0174" num="0172">Next, a configuration of shooting device <b>301</b> will be described. <figref idref="DRAWINGS">FIG. <b>21</b></figref> is a block diagram of shooting device <b>301</b>. Shooting device <b>301</b> includes shooting unit <b>311</b>, position and orientation estimator <b>312</b>, position and orientation integrator <b>313</b>, region detector <b>314</b>, UI <b>315</b>, controller <b>316</b>, image storage <b>317</b>, position and orientation storage <b>318</b>, and region information storage <b>319</b>.</p><p id="p-0175" num="0173">Shooting unit <b>311</b> is a shooting device, such as a camera, and obtains an image (moving image). Although examples in which a moving image is used will be mainly described below, a plurality of static images may be used instead of the moving image. Shooting unit <b>311</b> stores the obtained image in image storage <b>317</b>. Shooting unit <b>311</b> may shoot a visible light image or non-visible light image (such as an infrared image). When the infrared image is used, shooting is possible even in a dark environment, such as in the nighttime. Shooting unit <b>311</b> may be a monocular camera or a stereo camera or other camera having a plurality of lenses. By using a calibrated stereo camera, the precision of the three-dimensional position and orientation can be improved. Shooting unit <b>311</b> may be equipment capable of shooting a depth image, such as an RGB-D sensor. In that case, since a depth image serving as three-dimensional information can be obtained, the precision of estimation of the camera position/orientation can be improved. In addition, the depth image can be used as information for alignment in the integration of three-dimensional orientations described later.</p><p id="p-0176" num="0174">Controller <b>316</b> controls the whole of the shooting processing and the like of shooting device <b>301</b>. Position and orientation estimator <b>312</b> estimates the three-dimensional position and orientation (position/orientation) of the camera shooting the image using the image stored in image storage <b>317</b>. Position and orientation estimator <b>312</b> also stores the estimated position/orientation in position and orientation storage <b>318</b>. For example, position and orientation estimator <b>312</b> uses image processing, such as simultaneous localization and mapping (SLAM), to estimate the position/orientation. Alternatively, position and orientation estimator <b>312</b> may calculate the position and orientation of the camera using information obtained by various sensors (GPS or acceleration sensor) provided in shooting device <b>301</b>. In the former case, the position and orientation can be estimated from information from shooting unit <b>311</b>. In the latter case, the image processing can be achieved with low processing load.</p><p id="p-0177" num="0175">When performing a plurality of shootings in one environment, position and orientation integrator <b>313</b> integrates the positions/orientations of the camera estimated in the shootings to calculate a position/orientation that can be handled in one space. Specifically, position and orientation integrator <b>313</b> uses, as reference coordinate axes, three-dimensional coordinate axes of the position/orientation obtained in the first shooting. Position and orientation integrator <b>313</b> then converts the coordinates of the positions/orientations obtained in the second and following shootings into coordinates in the space defined by the reference coordinate axes.</p><p id="p-0178" num="0176">Region detector <b>314</b> detects a region that cannot be three-dimensionally reconstructed in the target space or a region having a low three-dimensional reconstruction precision using the images stored in image storage <b>317</b> and the positions/orientations stored in position and orientation storage <b>318</b>. The region that cannot be three-dimensionally reconstructed in the target space is a region no image of which has been shot, for example. The region having a low three-dimensional reconstruction precision is a region the number of images of which is small (smaller than a predetermined number), for example. When three-dimensional position information is generated, the region having a low precision may also be a region the error between the actual position and the generated three-dimensional position information is great. Region detector <b>314</b> stores information on the detected region in region information storage <b>319</b>. Region detector <b>314</b> may detect a region that can be three-dimensionally reconstructed, and determine the other regions than the detected region as regions that cannot be three-dimensionally reconstructed.</p><p id="p-0179" num="0177">The information stored in region information storage <b>319</b> may be two-dimensional information superimposed on an image or three-dimensional information, such as three-dimensional coordinate information.</p><p id="p-0180" num="0178">UI <b>315</b> presents, to the user, the shot image and the region information detected by region detector <b>314</b>. UI <b>315</b> also has an input function that allows the user to input a shooting start instruction and a shooting end instruction. For example, UI <b>315</b> is a display with a touch panel.</p><p id="p-0181" num="0179">Next, an operation of shooting device <b>301</b> according to the present embodiment will be described. <figref idref="DRAWINGS">FIG. <b>22</b></figref> is a flowchart illustrating an operation of shooting device <b>301</b>. Shooting device <b>301</b> starts and ends shooting in response to an instruction from the user. Specifically, shooting is started by a shooting start button on the UI being depressed. When a shooting start instruction is input (Yes in S<b>801</b>), shooting unit <b>311</b> starts shooting. The shot image is stored in image storage <b>317</b>.</p><p id="p-0182" num="0180">Position and orientation estimator <b>312</b> then calculates a position/orientation each time an image is added (S<b>802</b>). The calculated position/orientation is stored in position and orientation storage <b>318</b>. When there is a three-dimensional point cloud generated by SLAM or the like in addition to the position/orientation, the generated three-dimensional point cloud is also stored.</p><p id="p-0183" num="0181">Position and orientation integrator <b>313</b> then integrates the positions/orientations (S<b>803</b>). Specifically, position and orientation integrator <b>313</b> determines, using the estimation result of positions/orientations and the images, whether the three-dimensional coordinate space of the positions/orientations for the previously shot images and the three-dimensional coordinate space of the position/orientation for a new shot image can be integrated, and integrates the three-dimensional coordinate spaces if they can be integrated. That is, position and orientation integrator <b>313</b> converts the coordinates of the position/orientation for a new shot image into a coordinate system of the previous positions/orientations. As a result, a plurality of positions/orientations is expressed in one three-dimensional coordinate space. Therefore, data obtained by a plurality of shootings can be commonly used, and the precision of estimation of positions/orientations can be improved.</p><p id="p-0184" num="0182">Region detector <b>314</b> then detects a shot region or the like (S<b>804</b>). Specifically, region detector <b>314</b> generates three-dimensional position information (such as a three-dimensional point cloud, a three-dimensional model, or a depth image) using the estimation result of positions/orientations and the images, and detects a region that cannot be three-dimensionally reconstructed or a region having a low three-dimensional reconstruction precision using the generated three-dimensional position information. Region detector <b>314</b> stores information on the detected region in region information storage <b>319</b>. UI <b>315</b> then displays the information on the region obtained in the process described above (S<b>805</b>).</p><p id="p-0185" num="0183">The series of processing is repeatedly performed until the shooting ends (S<b>806</b>). For example, the series of processing is repeatedly performed each time one or more frames of images are obtained.</p><p id="p-0186" num="0184"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a flowchart of position/orientation estimation processing (S<b>802</b>). First, position and orientation estimator <b>312</b> obtains images from image storage <b>317</b> (S<b>811</b>). Position and orientation estimator <b>312</b> then calculates the position/orientation of the camera in each image using the obtained images (S<b>812</b>). For example, position and orientation estimator <b>312</b> uses image processing, such as SLAM or structure from motion (SfM), to calculate the position/orientation. When shooting device <b>301</b> has a sensor, such as an inertial measurement unit (IMU), position and orientation estimator <b>312</b> may estimate the position/orientation using information obtained by the sensor.</p><p id="p-0187" num="0185">Position and orientation estimator <b>312</b> may use, as a camera parameter such as a lens focal length, a result obtained by a calibration previously performed. Alternatively, position and orientation estimator <b>312</b> may calculate a camera parameter at the same time as the position/orientation estimation.</p><p id="p-0188" num="0186">Position and orientation estimator <b>312</b> then stores the information on the calculated positions/orientations in position and orientation storage <b>318</b> (S<b>813</b>). When the calculation of the information on the position/orientation has failed, information indicating the failure may be stored in position and orientation storage <b>318</b>. This allows the user to know the location and time of the failure and in what kind of image the failure has occurred, and such information can be used for re-shooting or the like.</p><p id="p-0189" num="0187"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flowchart of the position and orientation integration processing (S<b>803</b>). First, position and orientation integrator <b>313</b> obtains an image from image storage <b>317</b> (S<b>821</b>). Position and orientation integrator <b>313</b> then obtains the current position/orientation (S<b>822</b>). Position and orientation integrator <b>313</b> then obtains the image and position/orientation for at least one past shooting route that is different from the current route (S<b>823</b>). For example, the past shooting route can be generated from time-series information on the positions/orientations of the camera obtained by SLAM. The information on the past shooting route is stored in position and orientation storage <b>318</b>, for example. Specifically, the result of SLAM is stored in each trial of shooting, and in the N-th shooting (along the current route), the three-dimensional coordinate axes of the N-th shooting are integrated with the three-dimensional coordinate axes of the first to N&#x2212;1-th results (along the past routes). Instead of the result of SLAM, position information obtained by GPS or Bluetooth (registered trademark) may be used.</p><p id="p-0190" num="0188">Position and orientation integrator <b>313</b> then determines whether the integration is possible (S<b>824</b>). Specifically, position and orientation integrator <b>313</b> determines whether the current position/orientation and image are similar to the position/orientation and image for a past shooting route obtained, and determines that they can be integrated if they are similar to each other and that they cannot be integrated if they are not similar to each other. More specifically, position and orientation integrator <b>313</b> calculates, from each image, a feature quantity representing characteristics of the whole image, and compares the feature quantities to determine whether the relevant images are shot from similar viewpoints. When shooting device <b>301</b> has GPS, and the absolute position of shooting device <b>301</b> is known, position and orientation integrator <b>313</b> may use the information to determine an image that is the same as the current image or shot from a similar position to the current image.</p><p id="p-0191" num="0189">When the integration is possible (Yes in S<b>824</b>), position and orientation integrator <b>313</b> performs route integration processing (S<b>825</b>). Specifically, position and orientation integrator <b>313</b> calculates three-dimensional relative positions of the current image with respect to a reference image obtained by shooting a region similar to the region of the current image. Position and orientation integrator <b>313</b> calculates the coordinates of the current image by adding the calculated three-dimensional relative positions to the coordinates of the reference image.</p><p id="p-0192" num="0190">In the following, a specific example of the route integration processing will be described. <figref idref="DRAWINGS">FIG. <b>25</b></figref> is a plan view illustrating shooting in a target space. Route C is a route of camera A used for shooting in the past whose position/orientation is already estimated. This drawing illustrates a case where camera A is located at a predetermined position on route C. Route D is a route of camera B being currently used for shooting. In this situation, current camera B and camera A have similar fields of view for shooting. Although an example is shown here in which two images are obtained with different cameras, two images shot by the same camera at different points in time may be used. <figref idref="DRAWINGS">FIG. <b>26</b></figref> is a diagram illustrating example images and comparison processing in this case.</p><p id="p-0193" num="0191">As illustrated in <figref idref="DRAWINGS">FIG. <b>26</b></figref>, position and orientation integrator <b>313</b> extracts feature quantities, such as Oriented FAST and Rotated BRIEF (ORB) feature quantities, from each image, and extracts feature quantities of the whole image based on the distribution or number thereof. For example, position and orientation integrator <b>313</b> clusters feature quantities occurring in the image like in the bag of words model, and uses a histogram for each class as a feature quantity.</p><p id="p-0194" num="0192">Position and orientation integrator <b>313</b> compares the feature quantities of the whole image between the images. When it is determined that the images are images of the same part, a relative three-dimensional positional relationship between the cameras is calculated by performing feature point matching between the images. In other words, position and orientation integrator <b>313</b> searches a plurality of images for the past shooting route for an image the feature quantities of the whole of which are similar to those of the current image. Based on the positional relationship, position and orientation integrator <b>313</b> converts the three-dimensional position of route D into the coordinate system of route C. In this way, a plurality of routes can be expressed in one coordinate system. In this way, positions/orientations for a plurality of routes can be integrated.</p><p id="p-0195" num="0193">When shooting device <b>301</b> has a sensor capable of detecting an absolute position, such as GPS, position and orientation integrator <b>313</b> may use the detection result to perform the integration processing. For example, position and orientation integrator <b>313</b> may perform processing using the detection result from the sensor instead of performing the processing using images described above, or may use detection result from the sensor in addition to the images. For example, position and orientation integrator <b>313</b> may use GPS information to narrow down the images to be used for comparison. Specifically, position and orientation integrator <b>313</b> designate, as being used for comparison, images the latitude and longitude of the positions/orientations for which fall within a range of &#xb1;0.001 degrees from the latitude and longitude of the current camera position according to GPS. In this way, the processing amount can be reduced.</p><p id="p-0196" num="0194"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a flowchart of the region detection processing (S<b>804</b>). First, region detector <b>314</b> obtains an image from image storage <b>317</b> (S<b>831</b>). Region detector <b>314</b> then obtains a position/orientation from position and orientation storage <b>318</b> (S<b>832</b>). Region detector <b>314</b> also obtains a three-dimensional point cloud representing the three-dimensional position of a feature point generated by SLAM or the like from position and orientation storage <b>318</b>.</p><p id="p-0197" num="0195">Region detector <b>314</b> then detects a region that has not been shot yet using the obtained image, position/orientation, and three-dimensional point cloud (S<b>833</b>). Specifically, region detector <b>314</b> projects the three-dimensional point cloud onto the image, and determines a vicinity of a pixel on which a three-dimensional point is projected (within a predetermined distance from the pixel) as a region that can be reproduced (a shot region). The predetermined distance described above may be increased as the distance of the projected three-dimensional point from the shooting position increases. When a stereo camera is used, region detector <b>314</b> may estimate the region that can be reproduced from a parallax image. When an RGB-D camera is used, the determination may be made using the obtained depth value. Details of the processing of determining a shot region will be described later. Region detector <b>314</b> may not only determine whether a three-dimensional model can be generated but also estimate the precision of the three-dimensional reconstruction.</p><p id="p-0198" num="0196">Finally, region detector <b>314</b> outputs the obtained region information (S<b>834</b>). The region information may be region information superimposed on the shot image or may be region information arranged in a three-dimensional space, such as a three-dimensional map.</p><p id="p-0199" num="0197"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a flowchart of the display processing (S<b>805</b>). First, UI <b>315</b> checks whether there is information to be displayed (S<b>841</b>). Specifically, UI <b>315</b> checks whether there is a newly added image in image storage <b>317</b>, and determines that there is information to be displayed if there is a newly added image in image storage <b>317</b>. UI <b>315</b> also checks whether there is newly added information in region information storage <b>319</b>, and determines that there is information to be displayed if there is newly added information in region information storage <b>319</b>.</p><p id="p-0200" num="0198">When there is information to be displayed (Yes in S<b>841</b>), UI <b>315</b> obtains the information to be displayed, such as an image or region information (S<b>842</b>). UI <b>315</b> then displays the obtained information to be displayed (S<b>843</b>).</p><p id="p-0201" num="0199"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a diagram illustrating an example of an UI screen displayed by UI <b>315</b>. The UI screen includes shooting start/shooting stop button <b>321</b>, currently-shot image <b>322</b>, region information <b>323</b>, and text display region <b>324</b>.</p><p id="p-0202" num="0200">Shooting start/shooting stop button <b>321</b> is an operation unit for the user to instruct to start and stop shooting. Currently-shot image <b>322</b> is an image being currently shot. Region information <b>323</b> shows a shot region, a low-precision region and the like. Here, the low-precision region is a region the precision of the three-dimensional model generated using the shot image of which is low. Text display region <b>324</b> shows, by text, information on a region that has not been shot yet or a low-precision region. Audio or the like may be used instead of text.</p><p id="p-0203" num="0201"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a diagram illustrating an example of region information <b>323</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>30</b></figref>, for example, information indicating each region is superimposed on a currently-shot image. As the shooting viewpoint moves, the displayed region information <b>323</b> is changed in real time. This allows the user to easily know the shot region and the like while seeing the image shot at the current camera viewpoint. For example, the shot region and the low-precision region are displayed in different colors. The region shot in the current route and the region shot in another route are also displayed in different colors. The information indicating each region may be information superimposed on the image as illustrated in <figref idref="DRAWINGS">FIG. <b>30</b></figref>, or may be a text or symbol. That is, the information can be any information that allows the user to visually discriminate each region. In this way, the user can know that the user should shoot a region that is not colored and a low-precision region, and therefore can avoid missing shooting. In addition, to perform the route integration processing, a region that should be shot can be presented to the user so that the shot region is continuous to the region shot in another route.</p><p id="p-0204" num="0202">A region to which the user's attention needs to be attracted, such as a low-precision region, can be made to blink, for example, so as to attract the user's attention. The image on which the region information is superimposed may be a past image.</p><p id="p-0205" num="0203">Although currently-shot image <b>322</b> and region information <b>323</b> are separately displayed here, the region information may be superimposed on currently-shot image <b>322</b>. In that case, the area required for the display can be reduced, so that the image and information are more visible on a small terminal, such as a smartphone. Therefore, the way of display may be switched depending on the type of the terminal. For example, the region information may be superimposed on currently-shot image <b>322</b> on a terminal having a small screen size, such as a smartphone, and currently-shot image <b>322</b> and region information <b>323</b> may be separately displayed on a terminal having a large screen size, such as a tablet terminal.</p><p id="p-0206" num="0204">UI <b>315</b> presents, by text or audio, how many meters back from the current position a low-precision region occurred, for example. The distance can be calculated from the result of position estimation. By using text information, the user can be correctly notified of the information. When using audio, the user can be safely notified, since the user can keep paying attention to shooting.</p><p id="p-0207" num="0205">The region information may be superimposed on the real space by means of an augmented reality (AR) glasses or a head-up display (HUD), rather than being two-dimensionally displayed on the image. In that case, the compatibility with the video obtained from the actual viewpoint increases, and the part that should be shot can be intuitively presented to the user.</p><p id="p-0208" num="0206">When the position/orientation estimation has failed, shooting device <b>301</b> may notify the user of the failure of the position/orientation estimation. <figref idref="DRAWINGS">FIG. <b>31</b></figref> is a diagram illustrating an example display in that case.</p><p id="p-0209" num="0207">For example, an image for which the position/orientation estimation failed may be displayed in region information <b>323</b>. Furthermore, a text or audio that prompts the user to redo shooting from that position may be presented. This allows the user to quickly redo shooting after failure.</p><p id="p-0210" num="0208">Shooting device <b>301</b> may predict the position of the failure from the elapsed time since the failure and the moving speed, and present audio or text information that &#x201c;go back 5 m&#x201d; or the like. Alternatively, shooting device <b>301</b> may display a two-dimensional map or three-dimensional map, and present the position of the shooting failure on the map.</p><p id="p-0211" num="0209">Shooting device <b>301</b> may detect that the user has come back to the position of the failure, and present that to the user by text, image, sound, vibration or the like. For example, that the user has come back to the position of the failure can be detected using the feature quantities of the whole image.</p><p id="p-0212" num="0210">When shooting device <b>301</b> detects a low-precision region, shooting device <b>301</b> may instruct the user to redo shooting and indicate a way of shooting. Here, the way of shooting is to zoom in on the region, for example. For example, shooting device <b>301</b> may give this instruction by text, image, sound, vibration or the like.</p><p id="p-0213" num="0211">In this way, the quality of the data obtained can be improved, and the precision of the generated three-dimensional model can be improved.</p><p id="p-0214" num="0212"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a diagram illustrating an example display in the case where a low-precision region is detected. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>32</b></figref>, text is used to give instructions to the user. In this example, in text display region <b>324</b>, that a low-precision region is detected is displayed. In addition, an instruction for the user to shoot the low-precision region, such as &#x201c;go back 5 m&#x201d;, is displayed. Shooting device <b>301</b> may dismiss these messages when shooting device <b>301</b> detects that the user has moved to the indicated position or has shot the indicated region.</p><p id="p-0215" num="0213"><figref idref="DRAWINGS">FIG. <b>33</b></figref> is a diagram illustrating another example of the instruction to the user. For example, arrow <b>325</b> illustrated in <figref idref="DRAWINGS">FIG. <b>33</b></figref> indicates, to the user, the direction and distance of movement. <figref idref="DRAWINGS">FIG. <b>34</b></figref> is a diagram illustrating examples of this arrow. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>34</b></figref>, the direction of movement is indicated by the angle of the arrow, and the distance to the destination is indicated by the size of the arrow. The way of display of the arrow may be changed depending on the distance. For example, the way of display may be color, or the presence or absence or the magnitude of an effect. The effect is blinking, motion, or zooming in and out, for example. Alternatively, the darkness of the arrow may be changed. For example, as the distance becomes shorter, the magnitude of the effect may increase, or the color of the arrow may become darker. A plurality of these may be used in combination.</p><p id="p-0216" num="0214">Any indication other than the arrow, such as a triangle or a finger icon, that can indicate direction can be used.</p><p id="p-0217" num="0215">Region information <b>323</b> may be superimposed on an image from a third party's viewpoint, such as a plan view or perspective view, rather than on the image being currently shot. For example, in an environment in which a three-dimensional map is available, a shot region or the like may be superimposed and displayed on the three-dimensional map.</p><p id="p-0218" num="0216"><figref idref="DRAWINGS">FIG. <b>35</b></figref> is a diagram illustrating an example of region information <b>323</b> in that case. <figref idref="DRAWINGS">FIG. <b>35</b></figref> is a diagram showing a three-dimensional map in plan view, and shows a shot region and a low-precision region. Furthermore, current camera position <b>331</b> (the position and direction of shooting device <b>301</b>), current route <b>332</b>, and past route <b>333</b> are also shown.</p><p id="p-0219" num="0217">When there is a map, such as CAD, of the target space at the construction site or the like, or when a three-dimensional model was generated in the past at the same place and three-dimensional map information has already been generated, shooting device <b>301</b> can use the map or map information. When GPS or the like is available, shooting device <b>301</b> can also create map information based on the latitude and longitude information obtained by GPS.</p><p id="p-0220" num="0218">As described above, by superimposing the result of estimation of the position/orientation of the camera and the shot region on the three-dimensional map and displaying them as a bird's eye view, the user can easily know which region has been shot and which route was used for shooting, for example. For example, in the example illustrated in FIG. <b>35</b>, the user can be made aware of a part missing in the shooting, such as that the backsides of the pillars have not been shot, of which the user can hardly be made aware by presentation of an image during shooting, and therefore, the shooting can be efficiently performed.</p><p id="p-0221" num="0219">In an environment where CAD or the like is not available, similarly, a view from a third party's viewpoint can be used without a map. In that case, since there is no object serving as a reference, the viewability is low, although the user can know the positional relationship between the current shooting position and the low-precision region. The user can also confirm that a region where an object is expected to exist has not been shot. Therefore, in this case, again, the efficiency of the shooting can be improved.</p><p id="p-0222" num="0220">Although an example where a plan view is used has been shown here, map information from other viewpoints can also be used. Shooting device <b>301</b> may have a function of changing the viewpoint of the three-dimensional map. For example, an UI that allows the user to change the viewpoint may be used.</p><p id="p-0223" num="0221">Shooting device <b>301</b> may display both region information <b>323</b> illustrated in <figref idref="DRAWINGS">FIG. <b>29</b></figref> or the like described above and region information illustrated in <figref idref="DRAWINGS">FIG. <b>35</b></figref>, or may have a function of switching between these screens.</p><p id="p-0224" num="0222">Next, examples of the method of determining a shot region and the method of presenting the shot region will be described. When SLAM is used in the estimation of the camera position, a three-dimensional point cloud relating to a feature point, such as a corner of an object in the image, is generated in addition to the position and orientation information on the camera. The region in which the three-dimensional point is generated can be determined as a region whose three-dimensional model can be generated, so that a shot region can be presented on the image by projecting the three-dimensional point onto the image.</p><p id="p-0225" num="0223"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a plan view illustrating a shooting of a target region. Black dots in the drawing represent three-dimensional points (feature points) generated. <figref idref="DRAWINGS">FIG. <b>37</b></figref> is a diagram illustrating an example where three-dimensional points are projected onto an image, and regions in vicinities of the three-dimensional points are determined as shot regions.</p><p id="p-0226" num="0224">Shooting device <b>301</b> may generate a mesh by connecting the three-dimensional points, and determine a shot region based on the mesh. <figref idref="DRAWINGS">FIG. <b>38</b></figref> is a diagram illustrating an example of shot regions in that case. As illustrated in <figref idref="DRAWINGS">FIG. <b>38</b></figref>, shooting device <b>301</b> may determine regions of meshes generated as shot regions.</p><p id="p-0227" num="0225">When shooting device <b>301</b> uses a stereo camera or RGB-D sensor, shooting device <b>301</b> may determine a shot region using a parallax image or depth image obtained by the camera or sensor. In that case, dense three-dimensional information can be obtained with light processing, so that shot regions can be more precisely determined.</p><p id="p-0228" num="0226">Although SLAM is used for the self-position estimation (position/orientation estimation) in this example, the present disclosure is not limited to this, as far as the position/orientation of the camera can be estimated during shooting.</p><p id="p-0229" num="0227">Shooting device <b>301</b> may predict the precision of the reproduction and display the predicted precision, in addition to presenting the shot region. Specifically, a three-dimensional point is calculated from a feature point in the image. When the three-dimensional point is projected onto the image, the projected three-dimensional point may deviate from the reference feature point. The deviation is referred to as a reprojection error, and the precision can be evaluated using the reprojection error. Specifically, the precision is determined to be lower as the reprojection error increases.</p><p id="p-0230" num="0228">For example, shooting device <b>301</b> may indicate the precision by the color of the shot region. For example, a high-precision region is indicated in blue, and a low-precision region is indicated in red. The precision may be indicated stepwise by the difference or darkness of the color. This allows the user to easily know the precision of each region.</p><p id="p-0231" num="0229">Shooting device <b>301</b> may use a depth image obtained by an RGB-D sensor or the like to determine whether the reproduction is possible or evaluate the precision. <figref idref="DRAWINGS">FIG. <b>39</b></figref> is a diagram illustrating an example of the depth image. In this drawing, the darker the color (or the denser the hatching), the farther the part is.</p><p id="p-0232" num="0230">Here, the precision of the three-dimensional model generated tends to increase as the distance from the camera decreases, and to decrease as the distance increases. Therefore, for example, shooting device <b>301</b> may determine that pixels in a predetermined depth range (such as up to a depth of 5 m) as being within a shot range. <figref idref="DRAWINGS">FIG. <b>40</b></figref> is a diagram illustrating an example where a region in a predetermined depth range is determined as a shot region. In this drawing, the hatched region is determined as a shot region</p><p id="p-0233" num="0231">Shooting device <b>301</b> may determine the precision of a region based on the distance to the region. That is, shooting device <b>301</b> may determine the precision to be higher as the distance decreases. For example, the relationship between the distance and the precision may be linearly or otherwise defined</p><p id="p-0234" num="0232">When using a stereo camera, shooting device <b>301</b> can generate a depth image from a parallax image, and therefore can determine the shot region and the precision in the same manner as when using a depth image. In that case, shooting device <b>301</b> may determine, as a region that cannot be reproduced (a region that has not been shot), a region for which the depth value cannot be calculated from the parallax image. Alternatively, shooting device <b>301</b> may estimate the depth value based on pixels in the vicinity thereof for a region for which the depth value cannot be calculated. For example, shooting device <b>301</b> calculates an average value of 5 by 5 pixels centered about a target pixel.</p><p id="p-0235" num="0233">The three-dimensional positions of the shot regions determined as described above and the information indicating the precision of each three-dimensional position are accumulated. Since the coordinates of the positions/orientations of the camera are integrated as described above, the coordinates of the obtained region information can also be integrated. In this way, the three-dimensional positions of the shot regions in the target space and the information indicating the precision of each three-dimensional position are generated.</p><p id="p-0236" num="0234">As described above, the shooting instruction device according to the present embodiment performs the process shown in <figref idref="DRAWINGS">FIG. <b>41</b></figref>. Shooting device <b>301</b>: shoots first images of a target space (S<b>851</b>); generates first three-dimensional position information (for example, a sparse three-dimensional point cloud or a depth image) of the target space, based on the first images and a first shooting position and a first shooting orientation of each of the first images (S<b>852</b>); and determines a second region of the target space for which generating second three-dimensional position information (for example, a dense three-dimensional point cloud) using the first images is difficult, using the first three-dimensional position information and without generating the second three-dimensional position information, the second three-dimensional position information being more detailed than the first three-dimensional position information (S<b>853</b>).</p><p id="p-0237" num="0235">Here, the region for which generating the three-dimensional position is difficult includes at least one of a region for which the three-dimensional position cannot be calculated and a region for which the error between the three-dimensional position and the actual position is greater than a predetermined threshold. Specifically, the second region includes at least one of (1) a region that is not shot from some of a plurality of shooting viewpoints, (2) a region that has been shot but has a large amount of blurring, (3) a region that has been shot but includes less feature points because of its lower contrast than the other regions, (4) a region that has been shot but is farther from a shooting viewpoint than the other regions, and is estimated to have a great error between the calculated three-dimensional position and the actual position even if the three-dimensional position is calculated, and (5) a region that is more affected by a lens distortion than the other regions. The blurring can be detected by determining a temporal change in position of the feature point, for example.</p><p id="p-0238" num="0236">The detailed three-dimensional position information is three-dimensional position information having high spatial resolution, for example. The spatial resolution of the three-dimensional position information means the distance between two adjacent three-dimensional positions that can be discriminated as different three-dimensional positions. High spatial resolution means that the distance between two adjacent three-dimensional positions is short. That is, three-dimensional position information having higher spatial resolution has information on more three-dimensional positions in a space of a predetermined size. Three-dimensional position information having high spatial resolution may be referred to as dense three-dimensional position information, and three-dimensional position information having low spatial resolution may be referred to as sparse three-dimensional position information.</p><p id="p-0239" num="0237">The detailed three-dimensional position information may be three-dimensional information having a large amount of information. For example, the first three-dimensional position information may be distance information from one viewpoint, such as a depth image, and the second three-dimensional position information may be a three-dimensional model, such as a three-dimensional point cloud, from which distance information from an arbitrary viewpoint can be obtained.</p><p id="p-0240" num="0238">The target space and the subject are the same concept and both mean a region to be shot, for example.</p><p id="p-0241" num="0239">According to shooting device <b>301</b>, since the second region for which generating the second three-dimensional position information is difficult can be determined using the first three-dimensional position information without generating the second three-dimensional position information, the efficiency of shooting of images used for generating the second three-dimensional position information can be improved.</p><p id="p-0242" num="0240">For example, the second region is at least one of a region in which shooting of an image is not performed or a region for which precision of the second three-dimensional position information is estimated to be lower than a predetermined standard. Here, the reference is a threshold of the distance between two different three-dimensional positions, for example. That is, the second region is a region for which the difference between the generated three-dimensional position information and the actual position is greater than a predetermined threshold when the three-dimensional position information is generated.</p><p id="p-0243" num="0241">For example, the first three-dimensional position information includes a first three-dimensional point cloud, and the second three-dimensional position information includes a second three-dimensional point cloud which is denser than the first three-dimensional point cloud.</p><p id="p-0244" num="0242">For example, in the determining (S<b>853</b>), shooting device <b>301</b> determines a third region of the target space which corresponds to a region in a vicinity of the first three-dimensional point cloud (a region that is within a predetermined distance from the first three-dimensional point cloud), and determines a region other than the third region as the second region (for example, <figref idref="DRAWINGS">FIG. <b>37</b></figref>).</p><p id="p-0245" num="0243">For example, in the determining (S<b>853</b>), shooting device <b>301</b> generates a mesh using the first three-dimensional point cloud, and determines a region other than the third region of the target space as the second region, the third region corresponding to a region in which the mesh is generated (for example, <figref idref="DRAWINGS">FIG. <b>38</b></figref>).</p><p id="p-0246" num="0244">For example, in the determining (S<b>853</b>), shooting device <b>301</b> determines the second region based on a reprojection error of the first three-dimensional point cloud.</p><p id="p-0247" num="0245">For example, the first three-dimensional position information includes a depth image. For example, as shown in <figref idref="DRAWINGS">FIG. <b>40</b></figref>, shooting device <b>301</b> determines a region within a predetermined distance from a shooting viewpoint as a third region, and determines a region other than the third region as the second region.</p><p id="p-0248" num="0246">For example, shooting device <b>301</b> further combines, using second images that have already been shot, a second shooting position and a second shooting orientation of each of the second images, the first images, and the first shooting positions and the first shooting orientations, coordinate systems of the first shooting positions and the first shooting orientations with coordinate systems of the second shooting positions and the second shooting orientations. Accordingly, shooting device <b>301</b> can determine the second region using the information obtained by a plurality of shootings.</p><p id="p-0249" num="0247">For example, shooting device <b>301</b> further displays the second region or a third region (for example, a shot region) other than the second region during shooting of the target space (for example, <figref idref="DRAWINGS">FIG. <b>30</b></figref>). Accordingly, shooting device <b>301</b> can present the second region to the user.</p><p id="p-0250" num="0248">For example, in the displaying, shooting device <b>301</b> displays information indicating the second region or the third region, superimposed on any one of the images (for example, <figref idref="DRAWINGS">FIG. <b>30</b></figref>). Accordingly, shooting device <b>301</b> can present the position of the second region inside the image to the user, and thus the user can easily know the position of the second region.</p><p id="p-0251" num="0249">For example, in the displaying, shooting device <b>301</b> displays information indicating the second region or the third region, superimposed on a map of the target space (for example, <figref idref="DRAWINGS">FIG. <b>35</b></figref>). Accordingly, shooting device <b>301</b> can present the position of the second region in the environment to the user, and thus the user can easily know the position of the second region.</p><p id="p-0252" num="0250">For example, shooting device <b>301</b> displays the second region and a reconstruction precision (three-dimensional reconstruction precision) of each region included in the second region. Accordingly, since the user can know the restoration precision of each region, in addition to the second region, the user can perform appropriate shooting based on this.</p><p id="p-0253" num="0251">For example, shooting device <b>301</b> further presents, to the user, an instruction for causing the user to shoot an image of the second region (for example, <figref idref="DRAWINGS">FIG. <b>32</b></figref> and <figref idref="DRAWINGS">FIG. <b>33</b></figref>). Accordingly, the user can efficiently perform appropriate shooting.</p><p id="p-0254" num="0252">For example, the instruction may include at least one of a direction or a distance from a current position to the second region (for example, <figref idref="DRAWINGS">FIG. <b>33</b></figref> and <figref idref="DRAWINGS">FIG. <b>34</b></figref>). Accordingly, the user can efficiently perform appropriate shooting.</p><p id="p-0255" num="0253">For example, the shooting device includes a processor and memory, and, using the memory, the processor performs the above-described processes.</p><p id="p-0256" num="0254">Although a shooting instruction device, a shooting device, and so on, according to embodiments of the present disclosure have been described thus above, the present disclosure is not limited to these embodiments.</p><p id="p-0257" num="0255">For example, the shooting instruction device according to Embodiment 1 and the shooting device according to Embodiment 2 may be combined. For example, the shooting instruction device may include at least some of the processing units included in the shooting device. Furthermore, the shooting device may include at least some of the processing units included in the shooting instruction device. Moreover, at least some of the processing units included in the shooting instruction device and at least some of the processing units included in the shooting device may be combined. In other words, the shooting instruction method according to Embodiment 1 may include at least some of the processes included in the shooting method according to Embodiment 2. Furthermore, the shooting method according to Embodiment 2 may include at least some of the processes included in the shooting instruction method according to Embodiment 1. Moreover, at least some of the processes included in the shooting instruction method according to Embodiment 1 and at least some of the processes included in the shooting method according to Embodiment 2 may be combined.</p><p id="p-0258" num="0256">Furthermore, each of the processing units included in the shooting instruction device, the shooting device, etc., according to the foregoing embodiments is implemented typically as an LSI, which is an integrated circuit (IC). They may take the form of individual chips, or one or more or all of them may be encapsulated into a single chip.</p><p id="p-0259" num="0257">Furthermore, the integrated circuit is not limited to an LSI, and thus may be implemented as a dedicated circuit or a general-purpose processor. Alternatively, a field programmable gate array (FPGA) that allows for programming after the manufacture of an LSI, or a reconfigurable processor that allows for reconfiguration of the connection and the setting of circuit cells inside an LSI may be employed.</p><p id="p-0260" num="0258">Moreover, in each of the above embodiments, the respective structural components may be implemented as dedicated hardware or may be realized by executing a software program suited to such structural components. Alternatively, the respective structural components may be implemented by a program executor such as a CPU or a processor reading out and executing the software program recorded in a recording medium such as a hard disk or a semiconductor memory.</p><p id="p-0261" num="0259">Furthermore, the present disclosure may be implemented as a shooting instruction method executed by a shooting instruction device.</p><p id="p-0262" num="0260">Also, the divisions of the functional blocks shown in the block diagrams are mere examples, and thus a plurality of functional blocks may be implemented as a single functional block, or a single functional block may be divided into a plurality of functional blocks, or one or more functions may be moved to another functional block. Also, the functions of a plurality of functional blocks having similar functions may be processed by single hardware or software in a parallelized or time-divided manner.</p><p id="p-0263" num="0261">Also, the processing order of executing the steps shown in the flowcharts is a mere illustration for specifically describing the present disclosure, and thus may be an order other than the shown order. Also, one or more of the steps may be executed simultaneously (in parallel) with another step</p><p id="p-0264" num="0262">Although the imaging instruction device, etc., according to one or more aspects has been described based on the foregoing embodiments, the present disclosure is not limited to such embodiments. Forms obtained by making various modifications to the above embodiments that can be conceived by those skilled in the art, as well as forms obtained by combining structural components in different embodiments, without materially departing from the spirit of the present disclosure, may be included in the scope of the one or more aspects.</p><heading id="h-0011" level="1">INDUSTRIAL APPLICABILITY</heading><p id="p-0265" num="0263">The present disclosure can be applied to shooting instruction devices.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005220A1-20230105-M00001.NB"><img id="EMI-M00001" he="23.96mm" wi="76.20mm" file="US20230005220A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A shooting method executed by a shooting device, the shooting method comprising:<claim-text>shooting first images of a target space;</claim-text><claim-text>generating a first three-dimensional point cloud of the target space, based on the first images and a first shooting position and a first shooting orientation of each of the first images; and</claim-text><claim-text>determining a first region of the target space for which generating a second three-dimensional point cloud is difficult, using the first three-dimensional point cloud and without generating the second three-dimensional point cloud, the second three-dimensional point cloud being denser than the first three-dimensional point cloud, wherein</claim-text><claim-text>the determining includes: generating a mesh using the first three-dimensional point cloud; and determining the region other than a second region of the target space, the second region being a region for which the mesh is generated.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The shooting method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>combining, using second images that have already been shot, a second shooting position and a second shooting orientation of each of the second images, the first images, and the first shooting positions and the first shooting orientations, coordinate systems of the first shooting positions and the first shooting orientations with coordinate systems of the second shooting positions and the second shooting orientations.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The shooting method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>displaying the first region or the second region during shooting of the target space.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The shooting method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>in the displaying, information indicating the first region or the second region is displayed superimposed on any one of the first images.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The shooting method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>in the displaying, information indicating the first region or the second region is displayed superimposed on a map of the target space.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The shooting method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the displaying includes displaying the first region and a reconstruction precision of each region included in the first region.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A shooting instruction method executed by a shooting instruction device, the shooting instruction method comprising:<claim-text>displaying a first image of a subject on which recognition of attributes is performed;</claim-text><claim-text>getting input to specify a first attribute among the attributes to specify a first region in order to generate a three-dimensional model of the subject, based on second images generated by shooting the subject and on a shooting position and a shooting orientation of each of the second images; and</claim-text><claim-text>outputting at least one of a shooting position or a shooting orientation so that a third image to be used in generating a first three-dimensional model of the first region specified is shot.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The shooting instruction method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>detecting a second region for which generating of the three-dimensional model is difficult, based on the second images, the shooting positions, and the shooting orientations; and</claim-text><claim-text>outputting at least one of a shooting position or a shooting orientation so that a fourth image which facilitates generating of a second three-dimensional model of the second region is generated, wherein</claim-text><claim-text>in the outputting corresponding to the first region, the at least one of the shooting position or the shooting orientation is outputted so that the third image which facilitates the generating of the first three-dimensional model of the first region is generated.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The shooting instruction method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein<claim-text>the detecting of the second region includes:<claim-text>(i) calculating an edge, on a two-dimensional image, for which an angular difference with an epipolar line based on the shooting position and the shooting orientation is smaller than a predetermined value; and</claim-text><claim-text>(ii) detecting, as the second region, a three-dimensional region corresponding to the edge calculated, and</claim-text></claim-text><claim-text>in the outputting corresponding to the second region, the at least one of the shooting position or the shooting orientation is outputted so that the fourth image for which the angular difference is bigger than the predetermined value is shot.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The shooting instruction method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein<claim-text>the second images are frames included in a moving image that is currently being shot and displayed, and</claim-text><claim-text>the outputting corresponding to the second region is performed in real time.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The shooting instruction method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>in the outputting corresponding to the second region, a shooting direction is outputted.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The shooting instruction method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>in the outputting corresponding to the second region, a shooting region is instructed.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A shooting device comprising:<claim-text>a processor; and</claim-text><claim-text>memory, wherein</claim-text><claim-text>using the memory, the processor:<claim-text>shoots first images of a target space;</claim-text><claim-text>generates a first three-dimensional point cloud of the target space based on the first images and a first shooting position and a first shooting orientation of each of the first images; and</claim-text><claim-text>determines a first region of the target space for which generating a second three-dimensional point cloud is difficult, using the first three-dimensional point cloud and without generating the second three-dimensional point cloud, the second three-dimensional point cloud being denser than the first three-dimensional point cloud, and</claim-text></claim-text><claim-text>the determining includes: generating a mesh using the first three-dimensional point cloud; and determining the region other than a second region of the target space, the second region being a region for which the mesh is generated.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A shooting instruction device comprising:<claim-text>a processor; and</claim-text><claim-text>memory, wherein</claim-text><claim-text>using the memory, the processor:<claim-text>displays a first image of a subject on which recognition of attributes is performed;</claim-text><claim-text>gets input to specify a first attribute among the attributes to specify a first region in order to generate a three-dimensional model of the subject, based on second images generated by shooting the subject and on a shooting position and a shooting orientation of each of the second images; and</claim-text><claim-text>outputs at least one of a shooting position or a shooting orientation so that a third image to be used in generating a first three-dimensional model of the first region specified is shot.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>