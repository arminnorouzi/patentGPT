<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007396A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007396</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942663</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2018-194777</doc-number><date>20181016</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>303</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>307</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>2430</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>2420</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">SIGNAL PROCESSING APPARATUS AND METHOD, AND PROGRAM TO REDUCE CALCULATION AMOUNT BASED ON MUTE INFORMATION</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17284419</doc-number><date>20210409</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11445296</doc-number></document-id></parent-grant-document><parent-pct-document><document-id><country>WO</country><doc-number>PCT/JP2019/038846</doc-number><date>20191002</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17942663</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Sony Group Corporation</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Honma</last-name><first-name>Hiroyuki</first-name><address><city>Chiba</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Chinen</last-name><first-name>Toru</first-name><address><city>Kanagawa</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Oikawa</last-name><first-name>Yoshiaki</first-name><address><city>Kanagawa</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Sony Group Corporation</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present technology relates to a signal processing apparatus and method, and a program that make it possible to reduce an arithmetic operation amount.</p><p id="p-0002" num="0000">The signal processing apparatus performs, on the basis of audio object mute information indicative of whether or not a signal of an audio object is a mute signal, at least either one of a decoding process or a rendering process of an object signal of the audio object. The present technology can be applied to a signal processing apparatus.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="97.71mm" wi="158.75mm" file="US20230007396A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="256.29mm" wi="166.54mm" orientation="landscape" file="US20230007396A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="193.29mm" wi="165.69mm" file="US20230007396A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="213.19mm" wi="166.37mm" orientation="landscape" file="US20230007396A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="251.63mm" wi="160.10mm" orientation="landscape" file="US20230007396A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="120.57mm" wi="141.90mm" file="US20230007396A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="253.66mm" wi="166.29mm" orientation="landscape" file="US20230007396A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="185.42mm" wi="141.65mm" file="US20230007396A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="207.35mm" wi="168.15mm" file="US20230007396A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="101.26mm" wi="145.37mm" file="US20230007396A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="257.13mm" wi="167.22mm" orientation="landscape" file="US20230007396A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="257.30mm" wi="166.62mm" orientation="landscape" file="US20230007396A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="218.44mm" wi="144.86mm" orientation="landscape" file="US20230007396A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="254.59mm" wi="166.54mm" orientation="landscape" file="US20230007396A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0003" num="0001">The present application claims the benefit under 35 U.S.C. &#xa7; 120 as a continuation application of U.S. application Ser. No. 17/284,419, filed on Apr. 9, 2021, which claims the benefit under 35 U.S.C. &#xa7; 371 as a U.S. National Stage Entry of International Application No. PCT/JP2019/038846, filed in the Japanese Patent Office as a Receiving Office on Oct. 2, 2019, which claims priority to Japanese Patent Application Number JP2018-194777, filed in the Japanese Patent Office on Oct. 16, 2018, each of which applications is hereby incorporated by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0004" num="0002">The present technology relates to a signal processing apparatus and method, and a program, and particularly to a signal processing apparatus and method, and a program that make it possible to reduce an arithmetic operation amount.</p><heading id="h-0003" level="1">BACKGROUND ART</heading><p id="p-0005" num="0003">In the past, an object audio technology has been used in a movie, a game and so forth, and an encoding method capable of handling an object audio has also been developed. In particular, for example, the MPEG (Moving Picture Experts Group)-H Part 3:3D audio standard that is an international standard and like standards are known (for example, refer to NPL <b>1</b>).</p><p id="p-0006" num="0004">Together with an existing 2-channel stereo method or multichannel stereo method for 5.1 channels or the like, in such an encoding method as described above, it is possible to treat a moving sound source or the like as an independent audio object and to encode position information of an object as metadata together with signal data of the audio object.</p><p id="p-0007" num="0005">This makes it possible to perform reproduction in various viewing environments in which the number or the arrangement of speakers is different. Further, it makes it possible to easily process, upon reproduction of sound of a specific sound source, the sound of the specific sound source in volume adjustment of the sound of the specific sound source or addition of an effect to the sound of the specific sound source, which have been difficult by the existing encoding methods.</p><p id="p-0008" num="0006">In such encoding methods as described above, decoding of a bit stream is performed by the decoding side such that an object signal that is an audio signal of an audio object and metadata including object position information indicative of the position of the audio object in a space are obtained.</p><p id="p-0009" num="0007">Then, a rendering process for rendering the object signal to a plurality of virtual speakers that is virtually arranged in the space is performed on the basis of the object position information. For example, in the standard of NPL 1, a method called three-dimensional VBAP (Vector Based Amplitude Panning) (hereinafter referred to simply as VBAP) is used for the rendering process.</p><p id="p-0010" num="0008">Further, after a virtual speaker signal corresponding to each virtual speaker is obtained by the rendering process, an HRTF (Head Related Transfer Function) process is performed on the basis of the virtual speaker signals. In the HRTF process, an output audio signal for allowing sound to be outputted from an actual headphone or speaker such that it sounds as if the sound were reproduced from the virtual speakers is generated.</p><heading id="h-0004" level="1">CITATION LIST</heading><heading id="h-0005" level="1">Non Patent Literature</heading><heading id="h-0006" level="2">[NPL 1]</heading><p id="p-0011" num="0009">INTERNATIONAL STANDARD ISO/IEC 23008-3 First edition Oct. 15, 2015 Information technology&#x2014;High efficiency coding and media delivery in heterogeneous environments&#x2014;Part 3: 3D audio</p><heading id="h-0007" level="1">SUMMARY</heading><heading id="h-0008" level="1">Technical Problem</heading><p id="p-0012" num="0010">Incidentally, if the rendering process and the HRTF process are performed for the virtual speakers regarding the audio object described above, then audio reproduction can be implemented such that the sound sounds as if it were reproduced from the virtual speakers, and therefore, a high sense of presence can be obtained.</p><p id="p-0013" num="0011">However, in the object audio, a great amount of arithmetic operation is required for a process for audio reproduction such as a rendering process and an HRTF process.</p><p id="p-0014" num="0012">Especially, in the case where it is tried to reproduce an object audio with a device such as a smartphone, since increase of the arithmetic operation amount accelerates consumption of a battery, it is demanded to reduce the arithmetic operation amount without impairing the sense of presence.</p><p id="p-0015" num="0013">The present technology has been made in view of such a situation as described above and makes it possible to reduce the arithmetic operation amount.</p><heading id="h-0009" level="1">Solution to Problem</heading><p id="p-0016" num="0014">In a signal processing apparatus according to one aspect of the present technology, on the basis of audio object mute information indicative of whether or not a signal of an audio object is a mute signal, at least either one of a decoding process or a rendering process of an object signal of the audio object is performed.</p><p id="p-0017" num="0015">A signal processing method or a program according to the one aspect of the present technology includes a step of performing, on the basis of audio object mute information indicative of whether or not a signal of an audio object is a mute signal, at least either one of a decoding process or a rendering process of an object signal of the audio object.</p><p id="p-0018" num="0016">In the one aspect of the present technology, at least either one of a decoding process or a rendering process of an object signal of the audio object is performed on the basis of the audio object mute information indicative of whether or not the signal of the audio object is a mute signal.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0010" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0019" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a view illustrating a process for an input bit stream.</p><p id="p-0020" num="0018"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view illustrating VBAP.</p><p id="p-0021" num="0019"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a view illustrating an HRTF process.</p><p id="p-0022" num="0020"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a view depicting an example of a configuration of a signal processing apparatus.</p><p id="p-0023" num="0021"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart illustrating an output audio signal generation process.</p><p id="p-0024" num="0022"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a view depicting an example of a configuration of a decoding processing section.</p><p id="p-0025" num="0023"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flow chart illustrating an object signal generation process.</p><p id="p-0026" num="0024"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a view depicting an example of a configuration of a rendering processing section.</p><p id="p-0027" num="0025"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flow chart illustrating a virtual speaker signal generation process.</p><p id="p-0028" num="0026"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flow chart illustrating a gain calculation process.</p><p id="p-0029" num="0027"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flow chart illustrating a smoothing process.</p><p id="p-0030" num="0028"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a view depicting an example of metadata.</p><p id="p-0031" num="0029"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a view depicting an example of a configuration of a computer.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0011" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0032" num="0030">In the following, embodiments to which the present technology are applied are described with reference to the drawings.</p><heading id="h-0012" level="1">First Embodiment</heading><heading id="h-0013" level="2">&#x3c;Present Technology&#x3e;</heading><p id="p-0033" num="0031">The present technology makes it possible to reduce an arithmetic operation amount without causing an error of an output audio signal by omitting at least part of processing during a mute interval or by outputting a predetermined value determined in advance as a value corresponding to an arithmetic operation result without actually performing arithmetic operation during a mute interval. This makes it possible to obtain a high sense of presence while reducing the arithmetic operation amount.</p><p id="p-0034" num="0032">First, a general process is described which is performed when decoding (decoding) is performed for a bit stream obtained by encoding using an encoding method of the MPEG-H Part 3:3D audio standard to generate an output audio signal of an object audio.</p><p id="p-0035" num="0033">For example, if an input bit stream obtained by encoding is inputted as depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, then a decoding process is performed for the input bit stream.</p><p id="p-0036" num="0034">By the decoding process, an object signal that is an audio signal for reproducing sound of an audio object and metadata including object position information indicative of a position in a space of the audio object are obtained.</p><p id="p-0037" num="0035">Then, a rendering process for rendering an object signal to virtual speakers virtually arranged in the space on the basis of the object position information included in the metadata is performed such that a virtual speaker signal for reproducing sound to be outputted from each virtual speaker is generated.</p><p id="p-0038" num="0036">Further, an HRTF process is performed on the basis of the virtual speaker signal for each virtual speaker, and an output audio signal for causing sound to be outputted from a headphone set mounted on the user or a speaker arranged in the actual space is generated.</p><p id="p-0039" num="0037">If sound is outputted from the actual headphone or speaker on the basis of the output audio signal obtained in such a manner as described above, then audio reproduction can be implemented such that the sound sounds as if it were reproduced from the virtual speaker. It is to be noted that, in the following description, a speaker actually arranged in an actual space is specifically referred to also as an actual speaker.</p><p id="p-0040" num="0038">When such an object audio as described above is to be reproduced actually, in the case where a great number of actual speakers can be arranged in a space, an output of the rendering process can be reproduced as it is from the actual speakers. In contrast, in the case where a great number of actual speakers cannot be arranged in a space, the HRTF process is performed such that reproduction is performed by a small number of actual speakers such as a headphone or a sound bar. Generally, in most cases, reproduction is performed by a headphone or a small number of actual speakers.</p><p id="p-0041" num="0039">Here, the general rendering process and HRTF process are further described.</p><p id="p-0042" num="0040">For example, at the time of rendering, a rendering process of a predetermined method such as VBAP described above is performed. The VBAP is one of rendering methods generally called panning, and a gain is distributed, from among virtual speakers existing on a spherical surface having the origin at a position of a user, to three virtual speakers positioned nearest to an audio object existing on the same spherical surface to perform rendering.</p><p id="p-0043" num="0041">It is assumed that, for example, as depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a user U<b>11</b> who is a hearing person is in a three-dimensional space and three virtual speakers SP<b>1</b> to SP<b>3</b> are arranged in front of the user U<b>11</b>.</p><p id="p-0044" num="0042">Here, it is assumed that a position of the head of the user U<b>11</b> is determined as an origin O and the virtual speakers SP<b>1</b> to SP<b>3</b> are positioned on the surface of a sphere centered at the origin O.</p><p id="p-0045" num="0043">It is assumed now that an audio object exists in a region TR<b>11</b> surrounded by the virtual speakers SP<b>1</b> to SP<b>3</b> on the spherical surface and a sound image is localized at a position VSP<b>1</b> of the audio object.</p><p id="p-0046" num="0044">In such a case as just described, according to the VBAP, a gain regarding the audio object is distributed to the virtual speakers SP<b>1</b> to SP<b>3</b> existing around the position VSP<b>1</b>.</p><p id="p-0047" num="0045">In particular, in a three-dimensional coordinate system whose reference (origin) is the origin O, the position VSP<b>1</b> is represented by a three-dimensional vector P that starts from the origin O and ends at the position VSP<b>1</b>.</p><p id="p-0048" num="0046">Further, if three-dimensional vectors starting from the origin and ending at positions of the virtual speakers SP<b>1</b> to SP<b>3</b> are determined as vectors L<sub>1 </sub>to L<sub>3</sub>, respectively, then the vector P can be represented by a linear sum of the vectors L<sub>1 </sub>to L<sub>3 </sub>as indicated by the following expression (1).</p><p id="p-0049" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Math. 1]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0050" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P=g</i><sub>1</sub><i>L</i><sub>1</sub><i>+g</i><sub>2</sub><i>L</i><sub>2</sub><i>+g</i><sub>3</sub><i>L</i><sub>3</sub>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0051" num="0047">Here, if coefficients g<sub>1 </sub>to g<sub>3 </sub>multiplied to the vectors L<sub>1 </sub>to L<sub>3 </sub>in the expression (1) are calculated and such coefficients g<sub>1 </sub>to g<sub>3 </sub>are determined as gains of sound to be outputted from the virtual speakers SP<b>1</b> to SP<b>3</b>, respectively, then a sound image can be localized at the position VSP<b>1</b>.</p><p id="p-0052" num="0048">For example, if a vector having the coefficients g<sub>1 </sub>to g<sub>3 </sub>as elements thereof is given as g<sub>123</sub>=[g<sub>1</sub>, g<sub>2</sub>, g<sub>3</sub>] and a vector having vectors L<sub>1 </sub>to L<sub>3 </sub>as elements thereof is given as L<sub>123</sub>=[L<sub>1</sub>, L<sub>2</sub>, L<sub>3</sub>], then the following expression (2) can be obtained by transforming the expression (1) given hereinabove.</p><p id="p-0053" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Math. 2]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0054" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>g</i><sub>123</sub><i>=P</i><sup>T</sup><i>L</i><sup>&#x2212;1</sup><sub>123</sub>&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0055" num="0049">If sound based on the object signal is outputted from the virtual speakers SP<b>1</b> to SP<b>3</b> by using, as gains, the coefficients g<sub>1 </sub>to g<sub>3 </sub>obtained by calculation of such an expression (2) as given above, then a sound image can be localized at the position VSP<b>1</b>.</p><p id="p-0056" num="0050">It is to be noted that, since the arrangement positions of the virtual speakers SP<b>1</b> to SP<b>3</b> are fixed and information indicative of the positions of the virtual speakers is already known, L<sub>123</sub><sup>&#x2212;1 </sup>that is an inverse matrix can be determined in advance.</p><p id="p-0057" num="0051">A triangular region TR<b>11</b> surrounded by three virtual speakers on the spherical surface depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is called mesh. By combining a great number of virtual speakers arranged in a space to configure plural meshes, sound of an audio object can be localized at any position in the space.</p><p id="p-0058" num="0052">In such a manner, if a gain for the virtual speaker is determined with respect to each audio object, then a virtual speaker signal for each virtual speaker can be obtained by performing arithmetic operation of the following expression (3).</p><p id="p-0059" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mtext>                </mtext>     <mrow>      <mo>[</mo>      <mrow>       <mi>Math</mi>       <mo>.</mo>       <mtext>   </mtext>       <mn>3</mn>      </mrow>      <mo>]</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <mo>[</mo>       <mtable>        <mtr>         <mtd>          <mtable>           <mtr>            <mtd>             <mtable>              <mtr>               <mtd>                <mrow>                 <mi>SP</mi>                 <mo>&#x2061;</mo>                 <mo>(</mo>                 <mrow>                  <mn>0</mn>                  <mo>,</mo>                  <mi>t</mi>                 </mrow>                 <mo>)</mo>                </mrow>               </mtd>              </mtr>              <mtr>               <mtd>                <mrow>                 <mi>SP</mi>                 <mo>&#x2061;</mo>                 <mo>(</mo>                 <mrow>                  <mn>1</mn>                  <mo>,</mo>                  <mi>t</mi>                 </mrow>                 <mo>)</mo>                </mrow>               </mtd>              </mtr>             </mtable>            </mtd>           </mtr>           <mtr>            <mtd>             <mo>&#x22ee;</mo>            </mtd>           </mtr>          </mtable>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mi>SP</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mrow>             <mi>M</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>,</mo>            <mi>t</mi>           </mrow>           <mo>)</mo>          </mrow>         </mtd>        </mtr>       </mtable>       <mo>]</mo>      </mrow>      <mo>=</mo>      <mrow>       <mo>[</mo>       <mtable>        <mtr>         <mtd>          <mrow>           <mi>G</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mn>0</mn>            <mo>,</mo>            <mn>0</mn>           </mrow>           <mo>)</mo>          </mrow>         </mtd>         <mtd>          <mrow>           <mi>G</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mn>0</mn>            <mo>,</mo>            <mn>1</mn>           </mrow>           <mo>)</mo>          </mrow>         </mtd>         <mtd>          <mo>&#x2026;</mo>         </mtd>         <mtd>          <mrow>           <mi>G</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mn>0</mn>            <mo>,</mo>            <mrow>             <mi>N</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>           </mrow>           <mo>)</mo>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mi>G</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mn>1</mn>            <mo>,</mo>            <mn>0</mn>           </mrow>           <mo>)</mo>          </mrow>         </mtd>         <mtd>          <mrow>           <mi>G</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mn>1</mn>            <mo>,</mo>            <mn>1</mn>           </mrow>           <mo>)</mo>          </mrow>         </mtd>         <mtd>          <mtext> </mtext>         </mtd>         <mtd>          <mrow>           <mi>G</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mn>1</mn>            <mo>,</mo>            <mrow>             <mi>N</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>           </mrow>           <mo>)</mo>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>         <mtd>          <mtext> </mtext>         </mtd>         <mtd>          <mo>&#x22ee;</mo>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mi>G</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mrow>             <mi>M</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>,</mo>            <mn>0</mn>           </mrow>           <mo>)</mo>          </mrow>         </mtd>         <mtd>          <mrow>           <mi>G</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mrow>             <mi>M</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>,</mo>            <mn>1</mn>           </mrow>           <mo>)</mo>          </mrow>         </mtd>         <mtd>          <mo>&#x2026;</mo>         </mtd>         <mtd>          <mrow>           <mi>G</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mrow>             <mi>M</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>,</mo>            <mrow>             <mi>N</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>           </mrow>           <mo>)</mo>          </mrow>         </mtd>        </mtr>       </mtable>       <mo>]</mo>      </mrow>     </mrow>     <mo>&#x2062;</mo>     <mtext>&#x2028;</mtext>     <mrow>      <mo>[</mo>      <mtable>       <mtr>        <mtd>         <mtable>          <mtr>           <mtd>            <mtable>             <mtr>              <mtd>               <mrow>                <mi>S</mi>                <mo>&#x2061;</mo>                <mo>(</mo>                <mrow>                 <mn>0</mn>                 <mo>,</mo>                 <mi>t</mi>                </mrow>                <mo>)</mo>               </mrow>              </mtd>             </mtr>             <mtr>              <mtd>               <mrow>                <mi>S</mi>                <mo>&#x2061;</mo>                <mo>(</mo>                <mrow>                 <mn>1</mn>                 <mo>,</mo>                 <mi>t</mi>                </mrow>                <mo>)</mo>               </mrow>              </mtd>             </mtr>            </mtable>           </mtd>          </mtr>          <mtr>           <mtd>            <mo>&#x22ee;</mo>           </mtd>          </mtr>         </mtable>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mi>S</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <mi>N</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>           <mo>,</mo>           <mi>t</mi>          </mrow>          <mo>)</mo>         </mrow>        </mtd>       </mtr>      </mtable>      <mo>]</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0060" num="0053">It is to be noted that, in the expression (3), SP(m,t) indicates a virtual speaker signal at time t of an mth (where, m=0, 1, . . . , M&#x2212;1) virtual speaker from among M virtual speakers. Further, in the expression (3), S(n,t) indicates an object signal at time t of an nth (where, n=0, 1, . . . , N&#x2212;1) audio object from among N audio objects.</p><p id="p-0061" num="0054">Further, in the expression (3), G(m,n) indicates a gain to be multiplied to the object signal S(n,t) of the nth audio object for obtaining the virtual speaker signal SP(m,t) regarding the mth virtual speaker. In particular, the gain G(m,n) indicates a gain distributed to the mth virtual speaker regarding the nth audio object calculated in accordance with the expression (2) given hereinabove.</p><p id="p-0062" num="0055">In the rendering process, calculation of the expression (3) is a process that requires the highest calculation cost. In other words, arithmetic operation of the expression (3) is a process in which the arithmetic operation amount is greatest.</p><p id="p-0063" num="0056">Now, an example of the HRTF process performed in the case where sound based on the virtual speaker signal obtained by the arithmetic operation of the expression (3) is reproduced by a headphone or a small number of actual speakers is described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. It is to be noted that, in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the virtual speakers are arranged on a two-dimensional horizontal plane in order to simplify the description.</p><p id="p-0064" num="0057">In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, five virtual speakers SP<b>11</b>-<b>1</b> to SP<b>11</b>-<b>5</b> are arranged side by side on a circular line in a space. In the following description, in the case where there is no necessity to specifically distinguish the virtual speakers SP<b>11</b>-<b>1</b> to SP<b>11</b>-<b>5</b> from one another, each of the virtual speakers SP<b>11</b>-<b>1</b> to SP<b>11</b>-<b>5</b> is sometimes referred to simply as virtual speaker SP<b>11</b>.</p><p id="p-0065" num="0058">Further, in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a user U<b>21</b> who is a sound receiving person is positioned at a position surrounded by the five virtual speakers SP<b>11</b>, namely, at a central position of the circular line on which the virtual speakers SP<b>11</b> are arranged. Accordingly, In the HRTF process, an output audio signal for implementing audio reproduction is generated such that the sound sounds as if the user U<b>21</b> were enjoying the sound outputted from the respective virtual speakers SP<b>11</b>.</p><p id="p-0066" num="0059">Especially, it is assumed that, in the present example, a listening position is given by the position at which the user U<b>21</b> is and sound based on the virtual speaker signals obtained by rendering to the five virtual speakers SP<b>11</b> is reproduced by a headphone.</p><p id="p-0067" num="0060">In such a case as just described, for example, sound outputted (emitted) from the virtual speaker SP<b>11</b>-<b>1</b> on the basis of the virtual speaker signal follows a path indicated by an arrow mark Q<b>11</b> and reaches the eardrum of the left ear of the user U<b>21</b>. Therefore, the characteristic of sound outputted from the virtual speaker SP<b>11</b>-<b>1</b> should be varied by the spatial transfer characteristic from the virtual speaker SP<b>11</b>-<b>1</b> to the left ear of the user U<b>21</b>, the shape of the face or the ear of the user U<b>21</b>, the reflection absorption characteristic and so forth.</p><p id="p-0068" num="0061">Therefore, if a transfer function H_L_SP<b>11</b> obtained by taking a spatial transfer characteristic from the virtual speaker SP<b>11</b>-<b>1</b> to the left ear of the user U<b>21</b>, a shape of the face or the ear of the user U<b>21</b>, a reflection absorption characteristic and so forth into account is convoluted into a virtual speaker signal for the virtual speaker SP<b>11</b>-<b>1</b>, then an output audio signal for reproducing sound from the virtual speaker SP<b>11</b>-<b>1</b> to be heard by the left ear of the user U<b>21</b> can be obtained.</p><p id="p-0069" num="0062">Similarly, sound outputted from the virtual speaker SP<b>11</b>-<b>1</b> on the basis of a virtual speaker signal follows a path indicated by an arrow mark Q<b>12</b> and reaches the eardrum of the right ear of the user U<b>21</b>. Accordingly, if a transfer function H_R_SP<b>11</b> obtained by taking a spatial transfer characteristic from the virtual speaker SP<b>11</b>-<b>1</b> to the right ear of the user U<b>21</b>, a shape of the face or the ear of the user U<b>21</b>, a reflection absorption characteristic and so forth into account is convoluted into a virtual speaker signal for the virtual speaker SP<b>11</b>-<b>1</b>, then an output audio signal for reproducing sound from the virtual speaker SP<b>11</b>-<b>1</b> to be heard by the right ear of the user U<b>21</b> can be obtained.</p><p id="p-0070" num="0063">From those, when sound based on virtual speaker signals for the five virtual speakers SP<b>11</b> is finally reproduced by a headphone, it is sufficient if, for the left channel, a transfer function for the left ear for the respective virtual speakers is convoluted into the virtual speaker signals and signals obtained as a result of the convolution are added to form an output audio signal for the left channel.</p><p id="p-0071" num="0064">Similarly, for the right channel, it is sufficient if a transfer function for the right ear for the respective virtual speakers is convoluted into the virtual speaker signals and signals obtained as a result of the convolution are added to form an output audio signal for the right channel.</p><p id="p-0072" num="0065">It is to be noted that, also in the case where the device to be used for reproduction is not a headphone but an actual speaker, an HRTF process similar to that in the case of a headphone is performed. However, in this case, since sound from the speaker reaches the left and right ears of the user by spatial propagation, a process that takes crosstalk into consideration is performed as an HRTF process. Such an HRTF process as just described is also called transaural processing.</p><p id="p-0073" num="0066">Generally, if a frequency-expressed output audio signal for the left ear, namely, for the left channel, is represented by L(&#x3c9;) and a frequency-expressed output audio signal for the right ear, namely, for the right channel, is represented by R(&#x3c9;), then L(&#x3c9;) and R(&#x3c9;) can be obtained by calculating the following expression (4).</p><p id="p-0074" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mtext>                </mtext>     <mrow>      <mo>[</mo>      <mrow>       <mi>Math</mi>       <mo>.</mo>       <mtext>   </mtext>       <mn>4</mn>      </mrow>      <mo>]</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mrow>      <mo>[</mo>      <mtable>       <mtr>        <mtd>         <mrow>          <mi>L</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>&#x3c9;</mi>          <mo>)</mo>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mi>R</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>&#x3c9;</mi>          <mo>)</mo>         </mrow>        </mtd>       </mtr>      </mtable>      <mo>]</mo>     </mrow>     <mo>=</mo>     <mtext>&#x2028;</mtext>     <mrow>      <mrow>       <mo>[</mo>       <mtable>        <mtr>         <mtd>          <mrow>           <mi>H_L</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mn>0</mn>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>         </mtd>         <mtd>          <mrow>           <mi>H_L</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mn>1</mn>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mo>&#x2026;</mo>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mi>H_L</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mrow>              <mi>M</mi>              <mo>-</mo>              <mn>1</mn>             </mrow>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mi>H_R</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mn>0</mn>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>         </mtd>         <mtd>          <mrow>           <mi>H_R</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mn>1</mn>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mo>&#x2026;</mo>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mi>H_R</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mrow>              <mi>M</mi>              <mo>-</mo>              <mn>1</mn>             </mrow>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>         </mtd>        </mtr>       </mtable>       <mo>]</mo>      </mrow>      <mo>[</mo>      <mtable>       <mtr>        <mtd>         <mtable>          <mtr>           <mtd>            <mtable>             <mtr>              <mtd>               <mrow>                <mi>SP</mi>                <mo>&#x2061;</mo>                <mo>(</mo>                <mrow>                 <mn>0</mn>                 <mo>,</mo>                 <mi>&#x3c9;</mi>                </mrow>                <mo>)</mo>               </mrow>              </mtd>             </mtr>             <mtr>              <mtd>               <mrow>                <mi>SP</mi>                <mo>&#x2061;</mo>                <mo>(</mo>                <mrow>                 <mn>1</mn>                 <mo>,</mo>                 <mi>&#x3c9;</mi>                </mrow>                <mo>)</mo>               </mrow>              </mtd>             </mtr>            </mtable>           </mtd>          </mtr>          <mtr>           <mtd>            <mo>&#x22ee;</mo>           </mtd>          </mtr>         </mtable>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mi>SP</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <mi>M</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>           <mo>,</mo>           <mi>&#x3c9;</mi>          </mrow>          <mo>)</mo>         </mrow>        </mtd>       </mtr>      </mtable>      <mo>]</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0075" num="0067">It is to be noted that, in the expression (4), &#x3c9; indicates a frequency, and SP(m,&#x3c9;) indicates a virtual speaker signal of the frequency &#x3c9; for the mth (where m=0, 1, . . . , M&#x2212;1) virtual speaker among M virtual speakers. The virtual speaker signal SP(m,&#x3c9;) can be obtained by time frequency conversion of the virtual speaker signal SP(m,t) described hereinabove.</p><p id="p-0076" num="0068">Further, in the expression (4), H_L(m,&#x3c9;) indicates a transfer function for the left ear that is multiplied to the virtual speaker signal SP(m,&#x3c9;) for the mth virtual speaker in order to obtain an output audio signal L(&#x3c9;) of the left channel. Similarly, H_R(m,&#x3c9;) indicates a transfer function for the right ear.</p><p id="p-0077" num="0069">In the case where such HRTF transfer function H_L(m,&#x3c9;) and transfer function H_R(m,&#x3c9;) are expressed as impulse responses in the time domain, at least approximately one second is required. Therefore, in the case where, for example, the sampling frequency of the virtual speaker signals is 48 kHz, convolution of 48000 taps must be performed, and even if a high-seed calculation method that uses FFT (Fast Fourier Transform) is used for convolution of the transfer functions, a lot of arithmetic operation amount is still required.</p><p id="p-0078" num="0070">In the case where a decoding process, a rendering process, and an HRTF process are performed to generate an output audio signal and an object audio is reproduced using a headphone or a small number of actual speakers, a lot of arithmetic operation amount is required as described above. Further, as the number of audio objects increases, this arithmetic operation amount increases that much.</p><p id="p-0079" num="0071">Incidentally, although a stereo bit stream includes a very small number of mute intervals, generally it is very rare that an audio object bit stream includes a signal in all intervals of all audio objects.</p><p id="p-0080" num="0072">In many audio object bit streams, approximately 30% of intervals are mute intervals, and in some cases, 60% of all intervals are mute intervals.</p><p id="p-0081" num="0073">Therefore, in the present technology, information an audio object in a bit stream has is used to make it possible to reduce the arithmetic operation amount of a decoding process, a rendering process, and an HRTF process during mute intervals with a small arithmetic operation amount without calculating the energy of an object signal.</p><heading id="h-0014" level="2">&#x3c;Example of Configuration of Signal Processing Apparatus&#x3e;</heading><p id="p-0082" num="0074">Now, an example of a configuration of a signal processing apparatus to which the present technology is applied is described.</p><p id="p-0083" num="0075"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a view depicting an example of a configuration of an embodiment of the signal processing apparatus to which the present technology is applied.</p><p id="p-0084" num="0076">A signal processing apparatus <b>11</b> depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref> includes a decoding processing section <b>21</b>, a mute information generation section <b>22</b>, a rendering processing section <b>23</b>, and an HRTF processing section <b>24</b>.</p><p id="p-0085" num="0077">The decoding processing section <b>21</b> receives and decodes (decodes) an input bit stream transmitted thereto and supplies an object signal and metadata of an audio object obtained as a result of the decoding to the rendering processing section <b>23</b>.</p><p id="p-0086" num="0078">Here, the object signal is an audio signal for reproducing sound of the audio object, and the metadata includes at least object position information indicative of a position of the audio objected in a space.</p><p id="p-0087" num="0079">More particularly, at the time of a decoding process, the decoding processing section <b>21</b> supplies information regarding a spectrum in each time frame extracted from the input bit stream and the like to the mute information generation section <b>22</b> and receives supply of information indicative of a mute or non-mute state from the mute information generation section <b>22</b>. Then, the decoding processing section <b>21</b> performs a decoding process while performing omission or the like of processing of a mute interval on the basis of the information indicative of a mute or non-mute state supplied from the mute information generation section <b>22</b>.</p><p id="p-0088" num="0080">The mute information generation section <b>22</b> receives supply of various kinds of information from the decoding processing section <b>21</b> and the rendering processing section <b>23</b>, generates information indicative of a mute or non-mute state on the basis of the information supplied thereto, and supplies the information to the decoding processing section <b>21</b>, the rendering processing section <b>23</b>, and the HRTF processing section <b>24</b>.</p><p id="p-0089" num="0081">The rendering processing section <b>23</b> performs transfer of information to and from the mute information generation section <b>22</b> and performs a rendering process based on an object signal and metadata supplied from the decoding processing section <b>21</b> according to the information indicative of a mute or non-mute state supplied from the mute information generation section <b>22</b>.</p><p id="p-0090" num="0082">In the rendering process, a process for a mute interval is omitted or the like on the basis of the information indicative of a mute or non-mute state. The rendering processing section <b>23</b> supplies a virtual speaker signal obtained by the rendering process to the HRTF processing section <b>24</b>.</p><p id="p-0091" num="0083">The HRTF processing section <b>24</b> performs an HRTF process on the basis of the virtual speaker single supplied from the rendering processing section <b>23</b> according to the information indicative of a mute or non-mute state supplied from the mute information generation section <b>22</b> and outputs an output audio signal obtained as a result of the HRTF process to a later stage. In the HRTF process, a process for a mute interval is omitted on the basis of the information indicative of a mute or non-mute state.</p><p id="p-0092" num="0084">It is to be noted that an example is described here in which omission or the like of arithmetic operation is performed for a portion of mute signal (mute interval) in the decoding process, the rendering process, and the HRTF process. However, only it is necessary that omission or the like of arithmetic operation (process) is performed in at least either one of the decoding process, the rendering process, or the HRTF process, and also in such a case as just described, the arithmetic operation amount can be reduced as a whole.</p><heading id="h-0015" level="2">&#x3c;Description of Output Audio Signal Generation Process&#x3e;</heading><p id="p-0093" num="0085">Now, operation of the signal processing apparatus <b>11</b> depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref> is described. In particular, an output audio signal generation process by the signal processing apparatus <b>11</b> is described below with reference to a flow chart of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0094" num="0086">In step S<b>11</b>, the decoding processing section <b>21</b> performs, while performing transmission and reception of information to and from the mute information generation section <b>22</b>, a decoding process for an input bit stream supplied thereto to generate an object signal and supplies the object signal and metadata to the rendering processing section <b>23</b>.</p><p id="p-0095" num="0087">For example, in step S<b>11</b>, the mute information generation section <b>22</b> generates spectral mute information indicative of whether or not each time frame (hereinafter referred to sometimes merely as frame) is mute, and the decoding processing section <b>21</b> executes a decoding process in which omission or the like of part of processing is performed on the basis of the spectral mute information. Further, in step S<b>11</b>, the mute information generation section <b>22</b> generates audio object mute information indicative of whether or not an object signal of each frame is a mute signal and supplies it to the rendering processing section <b>23</b>.</p><p id="p-0096" num="0088">In step S<b>12</b>, while the rendering processing section <b>23</b> performs transmission and reception of information to and from the mute information generation section <b>22</b>, it performs a rendering process on the basis of the object signal and the metadata supplied from the decoding processing section <b>21</b> to generate a virtual speaker signal and supplies the virtual speaker signal to the HRTF processing section <b>24</b>.</p><p id="p-0097" num="0089">For example, in step S<b>12</b>, virtual speaker mute information indicative of whether or not the virtual speaker signal of each frame is a mute signal is generated by the mute information generation section <b>22</b>. Further, a rendering process is performed on the basis of the audio object mute information and the virtual speaker mute information supplied from the mute information generation section <b>22</b>. Especially, in the rendering process, omission of processing is performed during a mute interval.</p><p id="p-0098" num="0090">In step S<b>13</b>, the HRTF processing section <b>24</b> generates an output audio signal by performing an HRTF process by which processing is omitted during a mute interval on the basis of the virtual speaker mute information supplied from the mute information generation section <b>22</b> and outputs the output audio signal to a later stage. After the output audio signal is outputted in such a manner, the output audio signal generation process is ended.</p><p id="p-0099" num="0091">The signal processing apparatus <b>11</b> generates spectral mute information, audio object mute information, and virtual speaker mute information as information indicative of a mute or non-mute state in such a manner as described and performs, on the basis of the information, a decoding process, a rendering process, and an HRTF process to generate an output audio signal. Especially here, the spectral mute information, the audio object mute information, and the virtual speaker mute information are generated on the basis of information that can be obtained directly or indirectly from an input bit stream.</p><p id="p-0100" num="0092">By this, the signal processing apparatus <b>11</b> performs omission or the like of processing during a mute interval and can reduce the arithmetic operation amount without damaging the presence. In other words, reproduction of an object audio can be performed with high presence while the arithmetic operation amount is reduced.</p><heading id="h-0016" level="2">&#x3c;Example of Configuration of Decoding Processing Section&#x3e;</heading><p id="p-0101" num="0093">Here, the decoding process, the rendering process, and the HRTF process are described in more detail.</p><p id="p-0102" num="0094">For example, the decoding processing section <b>21</b> is configured in such a manner as depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0103" num="0095">In the example depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the decoding processing section <b>21</b> includes a demultiplexing section <b>51</b>, a sub information decoding section <b>52</b>, a spectral decoding section <b>53</b>, and an IMDCT (Inverse Modified Discrete Cosine Transform) processing section <b>54</b>.</p><p id="p-0104" num="0096">The demultiplexing section <b>51</b> demultiplexes an input bit stream supplied thereto to extract (separate) audio object data and metadata from the input bit stream, and supplies the obtained audio object data to the sub information decoding section <b>52</b> and supplies the metadata to the rendering processing section <b>23</b>.</p><p id="p-0105" num="0097">Here, the audio object data is data for obtaining an object signal and includes sub information and spectral data.</p><p id="p-0106" num="0098">In the present embodiment, on the encoding side, namely, on the generation side of an input bit stream, MDCT (Modified Discrete Cosine Transform) is performed for an object signal that is a time signal, and an MDCT coefficient obtained as a result of the MDCT is spectral data that is a frequency component of the object signal.</p><p id="p-0107" num="0099">Further, on the encoding side, encoding of spectral data is performed by a context-based arithmetic encoding method. Then, the encoded spectral data and encoded sub information that is required for decoding of the spectral data are placed as audio object data into an input bit stream.</p><p id="p-0108" num="0100">Further, as described hereinabove, the metadata includes at least object position information that is spatial position information indicative of a position of an audio object in a space.</p><p id="p-0109" num="0101">It is to be noted that, generally, metadata is also encoded (compressed) frequently. However, since the present technology can be applied to metadata irrespective of whether or not the metadata is in an encoded state, namely, whether or not the metadata is in a compressed state, the description is continued here assuming that the metadata is not in an encoded state in order to simplify the description.</p><p id="p-0110" num="0102">The sub information decoding section <b>52</b> decodes sub information included in audio object data supplied from the demultiplexing section <b>51</b> and supplies the decoded sub information and spectral data included in the audio object data supplied thereto to the spectral decoding section <b>53</b>.</p><p id="p-0111" num="0103">In other words, the audio object data including the decoded sub information and the spectral data in an encoded state to the spectral decoding section <b>53</b>. Especially here, data other than spectral data from within data included in audio object data of each audio object included in a general input bit stream is the sub information.</p><p id="p-0112" num="0104">Further, the sub information decoding section <b>52</b> supplies max_sfb that is information regarding a spectrum of each frame from within the sub information obtained by the decoding to the mute information generation section <b>22</b>.</p><p id="p-0113" num="0105">For example, the sub information includes information required for an IMDCT process or decoding of spectral data such as information indicative of a type of a transform window selected at the time of MDCT processing for an object signal and the number of scale factor bands with which encoding of spectral data has been performed.</p><p id="p-0114" num="0106">In the MPEG-H Part 3:3D audio standard, in ics_info( ) max_sfb is encoded with 4 bits or 6 bits corresponding to a type of a transform window selected at the time of MDCT processing, namely, corresponding to window_sequence. This max_sfb is information indicative of a quantity of encoded spectral data, namely, information indicative of the number of scale factor bands with which encoding of spectral data has been performed. In other words, the audio object data includes spectral data by an amount corresponding to the number of scale factor bands indicated by max_sfb.</p><p id="p-0115" num="0107">For example, in the case where the value of max_sfb is 0, there is no encoded spectral data, and since all of spectral data in the frame are regarded as 0, the frame can be determined as a mute frame (mute interval).</p><p id="p-0116" num="0108">The mute information generation section <b>22</b> generates spectral mute information of each audio object for each frame on the basis of max_sfb of each audio object for each frame supplied from the sub information decoding section <b>52</b> and supplies the spectral mute information to the spectral decoding section <b>53</b> and the IMDCT processing section <b>54</b>.</p><p id="p-0117" num="0109">Especially here, in the case where the value of max_sfb is 0, spectral mute information is generated which indicates that the target frame is a mute interval, namely, that the object signal is a mute signal. In contrast, in the case where the value of max_sfb is not 0, spectral mute information indicating that the target frame is a sounded interval, namely, that the object signal is a sounded signal, is generated.</p><p id="p-0118" num="0110">For example, in the case where the value of the spectral mute information is 1, this indicates that the spectral mute information is a mute interval, but in the case where the value of the spectral mute information is 0, this indicates that the spectral mute information is a sounded interval, namely, that the spectral mute information is not a mute interval.</p><p id="p-0119" num="0111">In such a manner, the mute information generation section <b>22</b> performs detection of a mute interval (mute frame) on the basis of max_sfb that is sub information and generates spectral mute information indicative of a result of the detection. This makes it possible to specify a mute frame with a very small processing amount (arithmetic operation amount) with which it is decided whether or not max_sfb extracted from an input bit stream is 0 without the necessity for calculation for obtaining energy of the object signal.</p><p id="p-0120" num="0112">It is to be noted that, for example, &#x201c;U.S. Pat. No. 9,905,232 B2, Hatanaka et al.&#x201d; proposes an encoding method that does not use max_sfb and separately adds, in the case where a certain channel can be deemed mute, a flag such that encoding is not performed for the channel.</p><p id="p-0121" num="0113">According to the encoding method, the encoding efficiency can be improved by 30 to 40 bits per channel from that by encoding according to the MPEG-H Part 3:3D audio standard, and in the present technology, such an encoding method as just described may also be applied. In such a case as just described, the sub information decoding section <b>52</b> extracts a flag that is included as sub information and indicates whether or not a frame of an audio object can be deemed mute, namely, whether or not encoding of spectral data has been performed, and supplies the flag to the mute information generation section <b>22</b>. Then, the mute information generation section <b>22</b> generates spectral mute information on the basis of the flag supplied from the sub information decoding section <b>52</b>.</p><p id="p-0122" num="0114">Further, in the case where increase of the arithmetic operation amount at the time of decoding processing is permissible, the mute information generation section <b>22</b> may calculate the energy of spectral data to decide whether or not the frame is a mute frame and generate spectral mute information according to a result of the decision.</p><p id="p-0123" num="0115">The spectral decoding section <b>53</b> decodes spectral data supplied from the sub information decoding section <b>52</b> on the basis of sub information supplied from the sub information decoding section <b>52</b> and spectral mute information supplied from the mute information generation section <b>22</b>. Here, the spectral decoding section <b>53</b> performs decoding of the spectral data by a decoding method corresponding to the context-based arithmetic encoding method.</p><p id="p-0124" num="0116">For example, according to the MPEG-H Part 3:3D audio standard, context-based arithmetic encoding is performed for spectral data.</p><p id="p-0125" num="0117">Generally, according to arithmetic encoding, not one output encoded data exists for one input data, but final output encoded data is obtained by transition of a plurality of input data.</p><p id="p-0126" num="0118">For example, in non-context-based arithmetic encoding, since the appearance frequency table to be used for encoding of input data becomes huge or plural appearance frequency tables are switchably used, it is necessary to encode an ID representative of an appearance frequency table and transmit the ID to the decoding side separately.</p><p id="p-0127" num="0119">In contrast, context-based arithmetic encoding, a characteristic (contents) of a frame preceding frame to a noticed spectral data or a characteristic of spectral data of a frequency lower than the frequency of the noticed spectral data is obtained by calculation as a context. Then, an appearance frequency table to be used is automatically determined on the basis of a calculation result of the context.</p><p id="p-0128" num="0120">Therefore, in the context-based arithmetic encoding, although also the decoding side must always perform calculation of the context, there are advantages that the appearance frequency table can be made compact and besides that the ID of the appearance frequency table need not be transmitted to the decoding side.</p><p id="p-0129" num="0121">For example, in the case where the value of the spectral mute information supplied from the mute information generation section <b>22</b> is 0 and the frame of the processing target is a sounded interval, the spectral decoding section <b>53</b> performs calculation of a context suitably using sub information supplied from the sub information decoding section <b>52</b> and a result of decoding of other spectral data.</p><p id="p-0130" num="0122">Then, the spectral decoding section <b>53</b> selects an appearance frequency table indicated by a value determined with respect to a result of the calculation of a context, namely, by the ID, and uses the appearance frequency table to decode the spectral data. The spectral decoding section <b>53</b> supplies the decoded spectral data and the sub information to the IMDCT processing section <b>54</b>.</p><p id="p-0131" num="0123">In contrast, in the case where the spectral mute information is 1 and the frame of the processing target is a mute interval (interval of a mute signal), namely, in the case where the value of max_sfb described hereinabove is 0, since the spectral data in this frame is 0 (zero data), the ID indicative of an appearance frequency table obtained by the context calculation indicates a same value without fail. In other words, the same appearance frequency table is selected without fail.</p><p id="p-0132" num="0124">Therefore, in the case where the value of the spectral mute information is 1, the spectral decoding section <b>53</b> does not perform context calculation, but selects an appearance frequency table indicated by an ID of a specific value determined in advance and uses the appearance frequency table to decode spectral data. In this case, for spectral data determined as data of a mute signal, context calculation is not performed. Then, the ID of the specific value determined in advance as a value corresponding to a calculation result of a context, namely, as a value indicative of a calculation result of a context, is used as an output to select an appearance frequency table, and a subsequent process for decoding is performed.</p><p id="p-0133" num="0125">By not performing calculation of a context according to spectral mute information in such a manner, namely, by omitting calculation of a contest and outputting a value determined in advance as a value indicative of a calculation result, the arithmetic operation amount of processing at the time of decoding (decoding) can be reduced. Besides, in this case, as a decoding result of spectral data, a result quite same as that when the calculation of a context is not omitted can be obtained.</p><p id="p-0134" num="0126">The IMDCT processing section <b>54</b> performs IMDCT (inverse modified discrete cosine transform) on the basis of spectral data and sub information supplied from the spectral decoding section <b>53</b> according to the spectral mute information supplied from the mute information generation section <b>22</b> and supplies an object obtained as a result of the IMDCT to the rendering processing section <b>23</b>.</p><p id="p-0135" num="0127">For example, in the IMDCT, processing is performed in accordance with an expression described in &#x201c;INTERNATIONAL STANDARD ISO/IEC 23008-3 First edition Oct. 15, 2015 Information technology&#x2014;High efficiency coding and media delivery in heterogeneous environments&#x2014;Part 3: 3D audio.&#x201d;</p><p id="p-0136" num="0128">In the case where the value of max_sfb is 0 and the frame of the target is a mute interval, all of the values of samples of a time signal of an output (processing result) of the IMDCT are 0. That is, the signal obtained by the IMDCT is zero data.</p><p id="p-0137" num="0129">Therefore, in the case where the value of the spectral mute information supplied from the mute information generation section <b>22</b> is 1 and the target frame is a mute interval (interval of a mute signal), the IMDCT processing section <b>54</b> outputs zero data without performing IMDCT processing for the spectral data.</p><p id="p-0138" num="0130">In particular, IMDCT processing is not performed actually, and zero data is outputted as a result of the IMDCT processing. In other words, as a value indicative of a processing result of the IMDCT, &#x201c;0&#x201d; (zero data) that is a value determined in advance is outputted.</p><p id="p-0139" num="0131">More particularly, the IMDCT processing section <b>54</b> overlap synthesizes a time signal objected as a processing result of the IMDCT of the current frame of the processing target and a time signal obtained as a processing result of the IMDCT of a frame immediately preceding to the current frame to generate an object signal of the current frame and outputs the object signal.</p><p id="p-0140" num="0132">The IMDCT processing section <b>54</b> can reduce the overall arithmetic operation amount of the IMDCT without giving rise to any error of the object signal obtained as an output by omitting the IMDCT processing during a mute interval. In other words, while the overall arithmetic operation amount of the IMDCT is reduced, an object signal quite same as that in the case where the IMDCT processing is not omitted can be obtained.</p><p id="p-0141" num="0133">Generally, in the MPEG-H Part 3:3D audio standard, since decoding of spectral data and IMDCT processing in a decoding process of an audio object occupy most of the decoding process, that the IMDCT processing can be reduced leads to significant reduction of the arithmetic operation amount.</p><p id="p-0142" num="0134">Further, the IMDCT processing section <b>54</b> supplies mute frame information indicative of whether or not a time signal of the current frame obtained as a processing result of the IMDCT is zero data, that is, whether or not the time signal is a signal of a mute interval, to the mute information generation section <b>22</b>.</p><p id="p-0143" num="0135">Consequently, the mute information generation section <b>22</b> generates audio object mute information on the basis of mute frame information of the current frame of the processing target and mute frame information of a frame immediately preceding in time to the current frame supplied from the IMDCT processing section <b>54</b> and supplies the audio object mute information to the rendering processing section <b>23</b>. In other words, the mute information generation section <b>22</b> generates audio object mute information on the basis of mute frame information obtained as a result of the decoding process.</p><p id="p-0144" num="0136">Here, in the case where both the mute frame information of the current frame and the mute frame information of the immediately preceding frame are information that they are signals during a mute interval, the mute information generation section <b>22</b> generates audio object mute information representing that the object signal of the current frame is a mute signal.</p><p id="p-0145" num="0137">In contrast, in the case where at least either one of the mute frame information of the current frame or the mute frame information of the immediately preceding frame is information that it is not a signal during a mute interval, the mute information generation section <b>22</b> generates audio object mute information representing that the object signal of the current frame is a sounded signal.</p><p id="p-0146" num="0138">Especially, in this example, in the case where the audio object mute information is 1, it is determined that this indicates that the object signal of the current frame a mute signal, and in the case where the audio object mute information is 0, it is determined that this indicates that the object signal is a sounded signal, namely, is not a mute signal.</p><p id="p-0147" num="0139">As described hereinabove, the IMDCT processing section <b>54</b> generates an object signal of a current frame by overlapping synthesis with a time signal obtained as a processing result of the IMDCT of an immediately preceding frame. Accordingly, since the object signal of the current frame is influenced by the immediately preceding frame, at the time of generation of audio object mute information, it is necessary to take a result of the overlapping synthesis, namely, a processing result of the IMDCT of the immediately preceding frame, into account.</p><p id="p-0148" num="0140">Therefore, only in the case where the value of max_sfb is 0 in both the current frame and the immediately preceding frame, namely, only in the case where zero data is obtained as a processing result of the IMDCT, the mute information generation section <b>22</b> determines that the object signal of the current frame is a frame of a mute interval.</p><p id="p-0149" num="0141">By generating audio object mute information indicative of whether or not the object signal is mute taking the IMDCT processing into consideration in such a manner, the rendering processing section <b>23</b> at the later stage can correctly recognize whether the object signal of the frame of the processing target is mute.</p><heading id="h-0017" level="2">&#x3c;Description of Object Signal Generation Process&#x3e;</heading><p id="p-0150" num="0142">Now, the process in step S<b>11</b> in the output audio signal generation process described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> is described in more detail. In particular, the object signal generation process that corresponds to step S<b>11</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> and is performed by the decoding processing section <b>21</b> and the mute information generation section <b>22</b> is described below with reference to a flow chart of <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0151" num="0143">In step S<b>41</b>, the demultiplexing section <b>51</b> demultiplexes the input bit stream supplied thereto and supplies audio object data and metadata obtained as a result of the demultiplexing to the sub information decoding section <b>52</b> and the rendering processing section <b>23</b>, respectively.</p><p id="p-0152" num="0144">In step S<b>42</b>, the sub information decoding section <b>52</b> decodes sub information included in the audio object data supplied from the demultiplexing section <b>51</b> and supplies the sub information after the decoding and spectral data included in the audio object data supplied thereto to the spectral decoding section <b>53</b>. Further, the sub information decoding section <b>52</b> supplies max_sfb included in the sub information to the mute information generation section <b>22</b>.</p><p id="p-0153" num="0145">In step S<b>43</b>, the mute information generation section <b>22</b> generates spectral mute information on the basis of max_sfb supplied thereto from the sub information decoding section <b>52</b> and supplies the spectral mute information to the spectral decoding section <b>53</b> and the IMDCT processing section <b>54</b>. For example, in the case where the value of max_sfb is 0, spectral mute information whose value is 1 is generated, but in the case where the value of max_sfb is not 0, spectral mute information whose value is 0 is generated.</p><p id="p-0154" num="0146">In step S<b>44</b>, the spectral decoding section <b>53</b> decodes the spectral data supplied from the sub information decoding section <b>52</b> on the basis of the sub information supplied from the sub information decoding section <b>52</b> and the spectral mute information supplied from the mute information generation section <b>22</b>.</p><p id="p-0155" num="0147">At this time, although the spectral decoding section <b>53</b> performs decoding of the spectral data by a decoding method corresponding to a context-based arithmetic encoding method, in the case where the value of the spectral mute information is 1, the spectral decoding section <b>53</b> omits the calculation of a context at the time of decoding and performs decoding of the spectral data by using a specific appearance frequency table. The spectral decoding section <b>53</b> supplies the decoded spectral data and sub information to the IMDCT processing section <b>54</b>.</p><p id="p-0156" num="0148">In step S<b>45</b>, the IMDCT processing section <b>54</b> performs IMDCT on the basis of the spectral data and the sub information supplied from the spectral decoding section <b>53</b> according to the spectral mute information supplied from the mute information generation section <b>22</b> and supplies an object signal obtained as a result of the IMDCT to the rendering processing section <b>23</b>.</p><p id="p-0157" num="0149">At this time, when the value of the spectral mute information supplied from the mute information generation section <b>22</b> is 1, the IMDCT processing section <b>54</b> does not perform the IMDCT process but performs an overlap synthesis by using the zero data to generate an object signal. Further, the IMDCT processing section <b>54</b> generates mute frame information according to whether or not the processing result of the IMDCT is zero data and supplies the mute frame information to the mute information generation section <b>22</b>.</p><p id="p-0158" num="0150">The processes of demultiplexing, decoding of the sub information, decoding of the spectral data, and IMDCT described above are performed as a decoding process for the input bit stream.</p><p id="p-0159" num="0151">In step S<b>46</b>, the mute information generation section <b>22</b> generates audio object mute information on the basis of the mute frame information supplied from the IMDCT processing section <b>54</b> and supplies the audio object mute information to the rendering processing section <b>23</b>.</p><p id="p-0160" num="0152">Here, audio object mute information of a current frame is generated on the basis of the mute frame information of a current frame and an immediately preceding frame. After the audio object mute information is generated, the object signal generation process is ended.</p><p id="p-0161" num="0153">The decoding processing section <b>21</b> and the mute information generation section <b>22</b> decode an input bit stream to generate an object signal in such a manner as described above. At this time, by generating spectral mute information such that calculation of a context or a process of IMDCT is not performed suitably, the arithmetic operation amount of the decoding process can be reduced without giving rise to an error in an object signal obtained as a decoding result. This makes it possible to obtain high presence even with a small amount of arithmetic operation.</p><heading id="h-0018" level="2">&#x3c;Example of Configuration of Rendering Processing Section&#x3e;</heading><p id="p-0162" num="0154">Subsequently, a configuration of the rendering processing section <b>23</b> is described. For example, the rendering processing section <b>23</b> is configured in such a manner as depicted in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0163" num="0155">The rendering processing section <b>23</b> depicted in <figref idref="DRAWINGS">FIG. <b>8</b></figref> includes a gain calculation section <b>81</b> and a gain application section <b>82</b>.</p><p id="p-0164" num="0156">The gain calculation section <b>81</b> calculates, on the basis of object position information included in metadata supplied from the demultiplexing section <b>51</b> of the decoding processing section <b>21</b>, a gain corresponding to each virtual speaker, namely, for each object signal, and supplies the gains to the gain application section <b>82</b>. Further, the gain calculation section <b>81</b> supplies, to the mute information generation section <b>22</b>, search mesh information indicative of meshes in each of which all of gains for the virtual speakers configuring the mesh, namely, the virtual speakers located at the three apexes of the mesh, have values equal to or higher than a predetermined value from among plural meshes.</p><p id="p-0165" num="0157">The mute information generation section <b>22</b> generates virtual speaker mute information for each virtual speaker on the basis of the search mesh information supplied from the gain calculation section <b>81</b> for each audio object, namely, for each object signal, in each frame and the audio object mute information.</p><p id="p-0166" num="0158">The value of the virtual speaker mute information is 1 in the case where the virtual speaker signal is a signal during a mute interval (mute signal) but is 0 in the case where the virtual speaker signal is not a signal during a mute interval, namely, in the case where the virtual speaker signal is a signal during a sounded interval (sounded signal).</p><p id="p-0167" num="0159">To the gain application section <b>82</b>, audio object mute information and virtual speaker mute information are supplied from the mute information generation section <b>22</b> and a gain is supplied from the gain calculation section <b>81</b> while an object signal is supplied from the IMDCT processing section <b>54</b> of the decoding processing section <b>21</b>.</p><p id="p-0168" num="0160">The gain application section <b>82</b> multiplies, on the basis of the audio object mute information and the virtual speaker mute information, an object signal by a gain from the gain calculation section <b>81</b> for each virtual speaker and adds the object signal multiplied by the gain to generate a virtual speaker signal.</p><p id="p-0169" num="0161">At this time, the gain application section <b>82</b> does not perform an arithmetic operation process for generating a virtual speaker signal for a mute object signal or a mute virtual speaker signal according to the audio object mute information and the virtual speaker mute information. In other words, arithmetic operation of at least part of the arithmetic operation process for generating a virtual speaker signal is omitted. The gain application section <b>82</b> supplies the obtained virtual speaker signal to the HRTF processing section <b>24</b>.</p><p id="p-0170" num="0162">In such a manner, the rendering processing section <b>23</b> performs a process, which includes a gain calculation process for obtaining by calculating a gain for a virtual speaker, more particularly, for part of a gain calculation process hereinafter described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref> and a gain application process for generating a virtual speaker signal, as a rendering process.</p><heading id="h-0019" level="2">&#x3c;Description of Virtual Speaker Signal Generation Process&#x3e;</heading><p id="p-0171" num="0163">Here, the process in step S<b>12</b> in the output audio signal generation process described hereinabove with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> is described in more detail. In particular, the virtual speaker signal generation process that corresponds to step S<b>12</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> and is performed by the rendering processing section <b>23</b> and the mute information generation section <b>22</b> is described with reference to a flow chart of <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0172" num="0164">In step S<b>71</b>, the gain calculation section <b>81</b> and the mute information generation section <b>22</b> perform a gain calculation process.</p><p id="p-0173" num="0165">In particular, the gain calculation section <b>81</b> performs calculation of the expression (2) given hereinabove for each object signal on the basis of object position information included in metadata supplied from the demultiplexing section <b>51</b> to calculate a gain for each virtual speaker and supplies the gains to the gain application section <b>82</b>. Further, the gain calculation section <b>81</b> supplies search mesh information to the mute information generation section <b>22</b>.</p><p id="p-0174" num="0166">Further, the mute information generation section <b>22</b> generates, for each object signal, virtual speaker mute information on the basis of the search mesh information supplied from the gain calculation section <b>81</b> and the audio object mute information. The mute information generation section <b>22</b> supplies the audio object mute information and the virtual speaker mute information to the gain application section <b>82</b> and supplies the virtual speaker mute information to the HRTF processing section <b>24</b>.</p><p id="p-0175" num="0167">In step S<b>72</b>, the gain application section <b>82</b> generates a virtual speaker signal on the basis of the audio object mute information, the virtual speaker mute information, the gain from the gain calculation section <b>81</b>, and the object signal from the IMDCT processing section <b>54</b>.</p><p id="p-0176" num="0168">At this time, the gain application section <b>82</b> does not perform, namely, omits, at least part of the arithmetic operation process for generating a virtual speaker signal according to the audio object mute information and the virtual speaker mute information to reduce the arithmetic operation amount of the rendering process.</p><p id="p-0177" num="0169">In this case, since the process during an interval during which the object signal and the virtual speaker signal are mute is omitted, as a result, a virtual speaker signal quite same as that in the case where the process is not omitted is obtained. In other words, the arithmetic operation amount can be reduced without giving rise to an error of the virtual speaker signal.</p><p id="p-0178" num="0170">The calculation (computation) of a gain and the processes for generating a virtual speaker signal described above are performed as a rendering process by the rendering processing section <b>23</b>.</p><p id="p-0179" num="0171">The gain application section <b>82</b> supplies the obtained virtual speaker signal to the HRTF processing section <b>24</b>, and the virtual speaker signal generation process is ended.</p><p id="p-0180" num="0172">The rendering processing section <b>23</b> and the mute information generation section <b>22</b> generate virtual speaker mute information and generate a virtual speaker signal in such a manner as described above. At this time, by omitting at least part of the arithmetic operation process for generating a virtual speaker signal according to audio object mute information and virtual speaker mute information, the arithmetic operation amount of the rendering process can be reduced without giving rise to any error in a virtual speaker signal obtained as a result of the rendering process. Consequently, high presence can be obtained even with a small amount of arithmetic operation.</p><heading id="h-0020" level="2">&#x3c;Description of Gain Calculation Process&#x3e;</heading><p id="p-0181" num="0173">Further, the gain calculation process performed in step S<b>71</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref> is performed for each audio object. More particularly, processes depicted in <figref idref="DRAWINGS">FIG. <b>10</b></figref> are performed as the gain calculation process. In the following, the gain calculation process that corresponds to the process in step S<b>71</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref> and is performed by the rendering processing section <b>23</b> and the mute information generation section <b>22</b> is described with reference to a flow chart of <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0182" num="0174">In step S<b>101</b>, the gain calculation section <b>81</b> and the mute information generation section <b>22</b> initialize the value of an index obj_id indicative of an audio object that is a processing target to 0, and the mute information generation section <b>22</b> further initializes the values of virtual speaker mute information a_spk_mute[spk_id] for all virtual speakers to 1.</p><p id="p-0183" num="0175">Here, it is assumed that the number of object signals obtained from the input bit stream, namely, the total number of audio objects, is max_obj. Then, it is assumed that the object signals are determined as an audio object of the processing target in order beginning with the audio object indicated by the index obj_id=0 and ending with the audio object indicated by the index obj_id=max_obj-1.</p><p id="p-0184" num="0176">Further, spk_id is an index indicative of a virtual speaker, and a_spk_mute[spk_id] indicates virtual speaker mute information regarding the virtual speaker indicated by the index spk_id. As described hereinabove, in the case where the value of the virtual speaker mute information a_spk_mute[spk_id] is 1, this indicates that the virtual speaker mute signal corresponding to the virtual speaker is mute.</p><p id="p-0185" num="0177">Note that it is assumed that the total number of virtual speakers arranged in the space here is max_spk. Accordingly, in this example, totaling max_spk virtual speakers from the virtual speaker indicated by the index spk_id=0 to the virtual speaker indicated by the index spk_id=max_spk-1 exist.</p><p id="p-0186" num="0178">In step S<b>101</b>, the gain calculation section <b>81</b> and the mute information generation section <b>22</b> set the value of the index obj_id indicative of the audio object of the processing target to 0.</p><p id="p-0187" num="0179">Further, the mute information generation section <b>22</b> sets the value of the virtual speaker mute information a_spk_mute[spk_id] regarding each index spk_id (where 0&#x2264;spk_id&#x2264;max_spk-1) to 1. Here, it is assumed for the time being that virtual speaker signals of all virtual speakers are mute.</p><p id="p-0188" num="0180">In step S<b>102</b>, the gain calculation section <b>81</b> and the mute information generation section <b>22</b> set the value of an index mesh_id indicative of a mesh that is a processing target to 0.</p><p id="p-0189" num="0181">Here, it is assumed that max_mesh meshes are formed by the virtual speakers in the space. In other words, the total number of meshes existing in the space is max_mesh. Further, it is assumed here that the meshes are selected as a mesh of a processing target in order beginning with the mesh indicated by the index mesh_id=0, namely, in the ascending order of the value of the index mesh_id.</p><p id="p-0190" num="0182">In step S<b>103</b>, the gain calculation section <b>81</b> obtains gains of three virtual speakers configuring the mesh of the index mesh_id that is a processing target by calculating the expression (2) given hereinabove for the audio object of the index obj_id of the processing target.</p><p id="p-0191" num="0183">In step S<b>103</b>, object position information of the audio object of the index obj_id is used to perform calculation of the expression (2). Consequently, gains g<sub>1 </sub>to g<sub>3 </sub>of respective three virtual speakers are obtained.</p><p id="p-0192" num="0184">In step S<b>104</b>, the gain calculation section <b>81</b> decides whether or not all of the three gains g<sub>1 </sub>to g<sub>3 </sub>obtained by calculation in step S<b>103</b> are equal to or higher than a threshold value TH<b>1</b> determined in advance.</p><p id="p-0193" num="0185">Here, the threshold value TH<b>1</b> is a floating point number equal to or lower than 0 and is a value determined, for example, by arithmetic operation accuracy of an equipped apparatus. Generally, as the value of the threshold value TH<b>1</b>, a small value of approximately &#x2212;1&#xd7;10<sup>&#x2212;5 </sup>is frequently used.</p><p id="p-0194" num="0186">For example, in the case where all of the gains g<sub>1 </sub>to g<sub>3 </sub>regarding the audio object of the processing target are equal to or higher than the threshold value TH<b>1</b>, this indicates that the audio object exists (is located) in the mesh of the processing target. In contrast, in the case where any one of the gains g<sub>1 </sub>to g<sub>3 </sub>is lower than the threshold value TH<b>1</b>, this indicates that the audio object of the processing target does not exist (is not positioned) in the mesh of the processing target.</p><p id="p-0195" num="0187">In the case where it is intended to reproduce sound of the audio object of the processing target, only it is necessary that sound is outputted only from the three virtual speakers configuring the mesh in which the audio object is included, and it is sufficient if virtual speaker signals for the other virtual speakers are made a mute signal. Therefore, in the gain calculation section <b>81</b>, search for a mesh including an audio object of a processing target is performed, and the value of the virtual speaker mute information is determined according to a result of the search.</p><p id="p-0196" num="0188">In the case where it is decided in step S<b>104</b> that all of the three gains g<sub>1 </sub>to g<sub>3 </sub>are not equal to or higher than the threshold value TH<b>1</b>, the gain calculation section <b>81</b> decides in step S<b>105</b> that the value of the index mesh_id of the mesh of the processing target is lower than max_mesh, namely, whether or not mesh_id&#x3c;max_mesh is satisfied.</p><p id="p-0197" num="0189">In the case where it is decided in step S<b>105</b> that mesh_id&#x3c;max_mesh is not satisfied, the processing advances to step S<b>110</b>. It is to be noted that basically it is not presupposed in step S<b>105</b> that mesh_id&#x3c;max_mesh is satisfied.</p><p id="p-0198" num="0190">In contrast, in the case where it is decided in step S<b>105</b> that mesh_id&#x3c;max_mesh is satisfied, the processing advances to step S<b>106</b>.</p><p id="p-0199" num="0191">In step S<b>106</b>, the gain calculation section <b>81</b> and the mute information generation section <b>22</b> increment the value of the index mesh_id indicative of the mesh of the processing target by one.</p><p id="p-0200" num="0192">After the process in step S<b>106</b> is performed, the processing returns to step S<b>103</b> and the processes described above are performed repeatedly. In particular, the process for calculating a gain is performed repeatedly until a mesh that includes the audio object of the processing target is detected.</p><p id="p-0201" num="0193">On the other hand, in the case where it is decided in step S<b>104</b> that all of the three gains g<sub>1</sub>to g<sub>3 </sub>are equal to or higher than the threshold value TH<b>1</b>, the gain calculation section <b>81</b> generates search mesh information indicative of the mesh of the index mesh_id that is the processing target and supplies the search mesh information to the mute information generation section <b>22</b>. Thereafter, the processing advances to step S<b>107</b>.</p><p id="p-0202" num="0194">In step S<b>107</b>, the mute information generation section <b>22</b> decides whether or not the value of the audio object mute information a_obj_mute[obj_id] of the object signal of the audio object of the index obj_id of the processing target is 0.</p><p id="p-0203" num="0195">Here, a_obj_mute[obj_id] indicates audio object mute information of the audio object whose index is obj_id. As described hereinabove, in the case where the value of the audio object mute information a_obj_mute[obj_id] is 1, this indicates that the object signal of the audio object of the index obj_id is a mute signal.</p><p id="p-0204" num="0196">In contrast, in the case where the value of the audio object mute information a_obj_mute[obj_id] is 0, this indicates that the object signal of the audio object of the index obj_id is a sounded signal.</p><p id="p-0205" num="0197">In the case where it is decided in step S<b>107</b> that the value of the audio object mute information a_obj_mute[obj_id] is 0, namely, in the case where the object signal is a sounded signal, the processing advances to step S<b>108</b>.</p><p id="p-0206" num="0198">In step S<b>108</b>, the mute information generation section <b>22</b> sets the value of the virtual speaker mute information of the three virtual speakers configuring the mesh of the index mesh_id indicated by the search mesh information supplied from the gain calculation section <b>81</b> to 0.</p><p id="p-0207" num="0199">For example, for the mesh of the index mesh_id, the information indicative of the mesh is set to mesh information mesh_info[mesh_id]. This mesh information mesh_info[mesh_id] has indices spk_id=spk1, spk2, and spk3 indicative of the three virtual speakers configuring the mesh of the index mesh_id as member variables.</p><p id="p-0208" num="0200">Especially, the index spk_id indicative of the first virtual speaker configuring the mesh of the index mesh_id is represented specifically as spk_id =mesh_info[mesh_id].spk1.</p><p id="p-0209" num="0201">Similarly, the index spk_id indicative of the second virtual speaker configuring the mesh of the index mesh_id is represented as spk_id=mesh_info[mesh_id].spk2, and the index spk_id indicative of the third virtual speaker configuring the mesh of the index mesh_id is represented as spk_id=mesh_info[mesh_id].spk3.</p><p id="p-0210" num="0202">In the case where the value of the audio object mute information a_obj_mute[obj_id] is 0, since the object signal of the audio object is sounded, the sound outputted from the three virtual speakers configuring the mesh including the audio object is sounded.</p><p id="p-0211" num="0203">Therefore, the mute information generation section <b>22</b> changes each of the values of virtual speaker mute information a_spk_mute[mesh_info[mesh_id].spk1], virtual speaker mute information a_spk_mute[mesh_info[mesh_id].spk2], and virtual speaker mute information a_spk_mute[mesh_info[mesh_id].spk3] of the three virtual speakers configuring the mesh of the index mesh_id from 1 to 0.</p><p id="p-0212" num="0204">In such a manner, in the mute information generation section <b>22</b>, virtual speaker mute information is generated on the basis of a calculation result (computing result) of the gains for the virtual speakers and audio object mute information.</p><p id="p-0213" num="0205">After setting of virtual speaker mute information is performed in such a manner, the processing advances to step S<b>109</b>.</p><p id="p-0214" num="0206">On the other hand, in the case where it is decided in step S<b>107</b> that the audio object mute information a_obj_mute[obj_id] is not 0, namely, is 1, the process in step S<b>108</b> is not performed, and the processing advances to step S<b>109</b>.</p><p id="p-0215" num="0207">In this case, since the object signal of the audio object of the processing target is mute, the values of the virtual speaker mute information a_spk_mute[mesh_info[mesh_id].spk1], the virtual speaker mute information a_spk_mute[mesh_info[mesh_id].spk2], and the virtual speaker mute information a_spk_mute[mesh_info[mesh_id].spk3] of the virtual speakers are left to be 1 as having been set in step S<b>101</b>.</p><p id="p-0216" num="0208">If the process in step S<b>108</b> is performed or if it is decided in step S<b>107</b> that the value of the audio object mute information is 1, then a process in step S<b>109</b> is performed.</p><p id="p-0217" num="0209">In particular, in step S<b>109</b>, the gain calculation section <b>81</b> sets the gains obtained by calculation in step S<b>103</b> as values of the gain of the three virtual speakers configuring the mesh of the index mesh_id of the processing target.</p><p id="p-0218" num="0210">For example, it is assumed that the gain of the virtual speaker of the index spk_id regarding the audio object of the index obj_id is represented as a_gain[obj_id][spk_id].</p><p id="p-0219" num="0211">Further, it is assumed that the gain of the virtual speaker corresponding to the index spk_id=mesh_info[mesh_id].spk1 from among the gains g<sub>1 </sub>to g<sub>3 </sub>obtained by calculation in step S<b>103</b> is g<sub>1</sub>. Similarly, it is assumed that the gain of the virtual speaker corresponding to the index spk_id=mesh_info[mesh_id].spk2 is g<sub>2 </sub>and the gain of the virtual speaker corresponding to the index spk_id=mesh_info[mesh_id].spk3 is g<sub>3</sub>.</p><p id="p-0220" num="0212">In such a case as just described, it is assumed that the gain calculation section <b>81</b> sets the gain a_gain[obj_id][mesh_info[mesh_id].spk1] of the virtual speaker=g<sub>1 </sub>on the basis of a result of the calculation in step S<b>103</b>. Similarly, the gain calculation section <b>81</b> sets the gain a_gain[obj_id][mesh_info[mesh_id].spk2]=g<sub>2 </sub>and sets the gain a_gain[obj_id][mesh_info[mesh_id].spk3]=g<sub>3</sub>.</p><p id="p-0221" num="0213">After the gains of the three virtual speakers configuring the mesh of the processing target are determined in such a manner, the processing advances to step S<b>110</b>.</p><p id="p-0222" num="0214">If it is decided in step S<b>105</b> that mesh_id&#x3c;max_mesh is not satisfied or if the process in step S<b>109</b> is performed, then the gain calculation section <b>81</b> decides in step S<b>110</b> whether or not obj_id&#x3c;max_obj is satisfied. In other words, it is decided whether or not the process has been performed for all audio objects as the processing target.</p><p id="p-0223" num="0215">In the case where it is decided in step S<b>110</b> that obj_id&#x3c;max_obj is satisfied, namely, that all of the audio objects have not been set as the processing target, the processing advances to step S<b>111</b>.</p><p id="p-0224" num="0216">In step S<b>111</b>, the gain calculation section <b>81</b> and the mute information generation section <b>22</b> increment the value of the index obj_id indicative of an audio object that is a processing target by 1. After the process in step S<b>111</b> is performed, the processing returns to step S<b>102</b> and the processes described above are performed repeatedly. In particular, for the audio object set as a processing target newly, a gain is calculated and setting of virtual speaker mute information is performed.</p><p id="p-0225" num="0217">On the other hand, in the case where it is decided in step S<b>110</b> that obj_id&#x3c;max_obj is not satisfied, since the processing has been performed for all audio objects set as a processing target, the gain calculation process is ended. When the gain calculation process ends, a state is established in which gains of each of the virtual speakers are obtained for all object signals and virtual speaker mute information is generated for each of the virtual speakers.</p><p id="p-0226" num="0218">The rendering processing section <b>23</b> and the mute information generation section <b>22</b> calculate gains of the virtual speakers and generate virtual speaker mute information in such a manner as described above. If the virtual speaker mute information is generated in such a manner, then since it can be recognized correctly whether a virtual speaker signal is mute, the gain application section <b>82</b> and the HRTF processing section <b>24</b> at the later stages can omit a process appropriately.</p><heading id="h-0021" level="2">&#x3c;Description of Smoothing Process&#x3e;</heading><p id="p-0227" num="0219">In step S<b>72</b> of the virtual speaker signal generation process described hereinabove with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the gains of virtual speakers and virtual speaker mute information obtained by the gain calculation process described hereinabove, for example, with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref> are used.</p><p id="p-0228" num="0220">However, in the case where, for example, the position of an audio object changes for each time frame, the gain sometimes fluctuates suddenly at a changing point of the position of the audio object. In such a case as just described, if the gains determined in step S<b>109</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref> are used as they are, then noise is generated in the virtual speaker signals, and therefore, it is possible to perform a smoothing process such as linear interpolation using not only the gains in the current frame but also the gains in the immediately preceding frame.</p><p id="p-0229" num="0221">In such a case as just described, the gain calculation section <b>81</b> performs a gain smoothing process on the basis of the gains in the current frame and the gains in the immediately preceding frames and supplies the gains after the smoothing (smoothing) as gains of the current frame obtained finally to the gain application section <b>82</b>.</p><p id="p-0230" num="0222">In the case where gain smoothing is performed in such a manner, it is necessary to perform the smoothing (smoothing) taking virtual speaker mute information in the current frame and the immediately preceding frame also into account. In this case, the mute information generation section <b>22</b> performs a smoothing process depicted, for example, in <figref idref="DRAWINGS">FIG. <b>11</b></figref> to smooth the virtual speaker mute information of each virtual speaker. In the following, the smoothing process by the mute information generation section <b>22</b> is described with reference to a flow chart of <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0231" num="0223">In step S<b>141</b>, the mute information generation section <b>22</b> sets the value of the index spk_id (where 0&#x2264;spk_id&#x2264;max_spk-1) indicative of a virtual speaker that is a processing target.</p><p id="p-0232" num="0224">Further, it is assumed that the virtual speaker mute information of the current frame obtained for the virtual speaker of the processing target indicated by the index spk_id here is represented as a_spk_mute[spk_id] and the virtual speaker mute information of the immediately preceding frame to the current frame is represented as a_prev_spk_mute[spk_id].</p><p id="p-0233" num="0225">In step S<b>142</b>, the mute information generation section <b>22</b> decides whether or not the virtual speaker mute information of the current frame and the immediately preceding frame is 1.</p><p id="p-0234" num="0226">In particular, it is decided whether or not both the value of the virtual speaker mute information a_spk_mute[spk_id] of the current frame and the virtual speaker mute information a_prev_spk_mute[spk_id] of the immediately preceding frame are 1.</p><p id="p-0235" num="0227">In the case where it is decided in step S<b>142</b> that the virtual speaker mute information is 1, the mute information generation section <b>22</b> determines, in step S<b>143</b>, the final value of the virtual speaker mute information a_spk_mute[spk_id] of the current frame as 1. Thereafter, the processing advances to step S<b>145</b>.</p><p id="p-0236" num="0228">On the other hand, in the case where it is decided in step S<b>142</b> that the virtual speaker mute information is not 1, namely, in the case where the virtual speaker mute information of at least either one of the current frame or the immediately preceding frame is 0, the processing advances to step S<b>144</b>. In this case, in at least either one of the current frame or the immediately preceding frame, the virtual speaker signal is sounded.</p><p id="p-0237" num="0229">In step S<b>144</b>, the mute information generation section <b>22</b> sets the final value of the virtual speaker mute information a_spk_mute[spk_id] of the current frame to 0, and then the processing advances to step S<b>145</b>.</p><p id="p-0238" num="0230">For example, in the case where the virtual speaker signal is sounded in at least either one of the current frame or the immediately preceding frame, by setting the value of the virtual speaker mute information of the current frame to 0, such a situation can be prevented that sound of a virtual speaker signal is interrupted and becomes mute or the sound of a virtual speaker signal becomes sounded suddenly.</p><p id="p-0239" num="0231">After the process in step S<b>143</b> or step S<b>144</b> is performed, the process in step S<b>145</b> is performed.</p><p id="p-0240" num="0232">In step S<b>145</b>, the mute information generation section <b>22</b> determines the virtual speaker mute information a_spk_mute[spk_id] obtained by the gain calculation process of <figref idref="DRAWINGS">FIG. <b>10</b></figref> regarding the current frame of the processing target as virtual speaker mute information a_prev_spk_mute[spk_id] of an immediately preceding frame to be used in the next smoothing process. In other words, the virtual speaker mute information a_spk_mute[spk_id] of the current frame is used as virtual speaker mute information a_prev_spk_mute[spk_id] in the smoothing process in a next cycle.</p><p id="p-0241" num="0233">In step S<b>146</b>, the mute information generation section <b>22</b> decides whether or not spk_id&#x3c;max_spk is satisfied. In other words, it is decided whether or not the process has been performed for all virtual speaker as the processing target.</p><p id="p-0242" num="0234">In the case where it is decided in step S<b>146</b> that spk_id&#x3c;max_spk is satisfied, since all of the virtual speakers have not been processed as the processing target as yet, the mute information generation section <b>22</b> increments the value of the index spk_id indicative of the virtual speaker of the processing target by 1 in step S<b>147</b>.</p><p id="p-0243" num="0235">After the process in step S<b>147</b> is performed, the processing returns to step S<b>142</b> and the processes described above are performed repeatedly. In other words, a process for smoothing the virtual speaker mute information a_spk_mute[spk_id] for the virtual speaker newly determined as a processing target.</p><p id="p-0244" num="0236">On the other hand, in the case where it is decided in step S<b>146</b> that spk_id&#x3c;max_spk is not satisfied, since the smoothing of the virtual speaker mute information has been performed for all virtual speakers in the current frame, the smoothing process is ended.</p><p id="p-0245" num="0237">The mute information generation section <b>22</b> performs the smoothing process for virtual speaker mute information taking the immediately preceding frame also into consideration in such a manner as described. By performing smoothing in such a manner, an appropriate virtual speaker signal with less sudden changes and noise can be obtained.</p><p id="p-0246" num="0238">In the case where the smoothing process depicted in <figref idref="DRAWINGS">FIG. <b>11</b></figref> is performed, this signifies that the final virtual speaker mute information obtained in step S<b>143</b> or step S<b>144</b> is used in the gain application section <b>82</b> and the HRTF processing section <b>24</b>.</p><p id="p-0247" num="0239">Further, in step S<b>72</b> of the virtual speaker signal generation process described hereinabove with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the virtual speaker mute information obtained by the gain calculation process of <figref idref="DRAWINGS">FIG. <b>10</b></figref> or the smoothing process of <figref idref="DRAWINGS">FIG. <b>11</b></figref> is used.</p><p id="p-0248" num="0240">In particular, the calculation of the expression (3) described hereinabove is generally performed to obtain a virtual speaker signal. In this case, all arithmetic operations are performed irrespective of whether not the object signal or the virtual speaker signal is a mute signal.</p><p id="p-0249" num="0241">In contrast, the gain application section <b>82</b> obtains a virtual speaker signal by calculation of the following expression (5) taking audio object mute information and virtual speaker mute information supplied from the mute information generation section <b>22</b> into account.</p><p id="p-0250" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mtext>                </mtext>     <mrow>      <mo>[</mo>      <mrow>       <mi>Math</mi>       <mo>.</mo>       <mtext>   </mtext>       <mn>5</mn>      </mrow>      <mo>]</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <mo>[</mo>       <mtable>        <mtr>         <mtd>          <mtable>           <mtr>            <mtd>             <mtable>              <mtr>               <mtd>                <mrow>                 <mi>SP</mi>                 <mo>&#x2061;</mo>                 <mo>(</mo>                 <mrow>                  <mn>0</mn>                  <mo>,</mo>                  <mi>t</mi>                 </mrow>                 <mo>)</mo>                </mrow>               </mtd>              </mtr>              <mtr>               <mtd>                <mrow>                 <mi>SP</mi>                 <mo>&#x2061;</mo>                 <mo>(</mo>                 <mrow>                  <mn>1</mn>                  <mo>,</mo>                  <mi>t</mi>                 </mrow>                 <mo>)</mo>                </mrow>               </mtd>              </mtr>             </mtable>            </mtd>           </mtr>           <mtr>            <mtd>             <mo>&#x22ee;</mo>            </mtd>           </mtr>          </mtable>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mi>SP</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mrow>             <mi>M</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>,</mo>            <mi>t</mi>           </mrow>           <mo>)</mo>          </mrow>         </mtd>        </mtr>       </mtable>       <mo>]</mo>      </mrow>      <mo>=</mo>      <mrow>       <mo>[</mo>       <mtable>        <mtr>         <mtd>          <mtable>           <mtr>            <mtd>             <mtable>              <mtr>               <mtd>                <mrow>                 <mi>a_spk</mi>                 <mo>&#x2062;</mo>                 <mi>_mute</mi>                 <mo>&#x2062;</mo>                 <mrow>                  <mo>(</mo>                  <mn>0</mn>                  <mo>)</mo>                 </mrow>                </mrow>               </mtd>              </mtr>              <mtr>               <mtd>                <mrow>                 <mi>a_spk</mi>                 <mo>&#x2062;</mo>                 <mi>_mute</mi>                 <mo>&#x2062;</mo>                 <mrow>                  <mo>(</mo>                  <mn>1</mn>                  <mo>)</mo>                 </mrow>                </mrow>               </mtd>              </mtr>             </mtable>            </mtd>           </mtr>           <mtr>            <mtd>             <mo>&#x22ee;</mo>            </mtd>           </mtr>          </mtable>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mi>a_spk</mi>           <mo>&#x2062;</mo>           <mi>_mute</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mi>M</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>)</mo>           </mrow>          </mrow>         </mtd>        </mtr>       </mtable>       <mo>]</mo>      </mrow>     </mrow>     <mo>&#x2062;</mo>     <mtext>&#x2028;</mtext>     <mrow>      <mo>[</mo>      <mtable>       <mtr>        <mtd>         <mrow>          <mi>G</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mn>0</mn>           <mo>,</mo>           <mn>0</mn>          </mrow>          <mo>)</mo>         </mrow>        </mtd>        <mtd>         <mrow>          <mi>G</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mn>0</mn>           <mo>,</mo>           <mn>1</mn>          </mrow>          <mo>)</mo>         </mrow>        </mtd>        <mtd>         <mo>&#x2026;</mo>        </mtd>        <mtd>         <mrow>          <mi>G</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mn>0</mn>           <mo>,</mo>           <mrow>            <mi>N</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>          </mrow>          <mo>)</mo>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mi>G</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mn>1</mn>           <mo>,</mo>           <mn>0</mn>          </mrow>          <mo>)</mo>         </mrow>        </mtd>        <mtd>         <mrow>          <mi>G</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mn>1</mn>           <mo>,</mo>           <mn>1</mn>          </mrow>          <mo>)</mo>         </mrow>        </mtd>        <mtd>         <mtext> </mtext>        </mtd>        <mtd>         <mrow>          <mi>G</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mn>1</mn>           <mo>,</mo>           <mrow>            <mi>N</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>          </mrow>          <mo>)</mo>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mo>&#x22ee;</mo>        </mtd>        <mtd>         <mo>&#x22ee;</mo>        </mtd>        <mtd>         <mtext> </mtext>        </mtd>        <mtd>         <mo>&#x22ee;</mo>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mi>G</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <mi>M</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>           <mo>,</mo>           <mn>0</mn>          </mrow>          <mo>)</mo>         </mrow>        </mtd>        <mtd>         <mrow>          <mi>G</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <mi>M</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>           <mo>,</mo>           <mn>1</mn>          </mrow>          <mo>)</mo>         </mrow>        </mtd>        <mtd>         <mo>&#x2026;</mo>        </mtd>        <mtd>         <mrow>          <mi>G</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <mi>M</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>           <mo>,</mo>           <mrow>            <mi>N</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>          </mrow>          <mo>)</mo>         </mrow>        </mtd>       </mtr>      </mtable>      <mo>]</mo>     </mrow>     <mo>&#x2062;</mo>     <mtext>&#x2028;</mtext>     <mrow>      <mo>[</mo>      <mtable>       <mtr>        <mtd>         <mtable>          <mtr>           <mtd>            <mtable>             <mtr>              <mtd>               <mrow>                <mi>a_obj</mi>                <mo>&#x2062;</mo>                <mi>_mute</mi>                <mo>&#x2062;</mo>                <mrow>                 <mo>(</mo>                 <mn>0</mn>                 <mo>)</mo>                </mrow>                <mo>&#x2062;</mo>                <mrow>                 <mi>S</mi>                 <mo>&#x2061;</mo>                 <mo>(</mo>                 <mrow>                  <mn>0</mn>                  <mo>,</mo>                  <mi>t</mi>                 </mrow>                 <mo>)</mo>                </mrow>               </mrow>              </mtd>             </mtr>             <mtr>              <mtd>               <mrow>                <mi>a_obj</mi>                <mo>&#x2062;</mo>                <mi>_mute</mi>                <mo>&#x2062;</mo>                <mrow>                 <mo>(</mo>                 <mn>1</mn>                 <mo>)</mo>                </mrow>                <mo>&#x2062;</mo>                <mrow>                 <mi>S</mi>                 <mo>&#x2061;</mo>                 <mo>(</mo>                 <mrow>                  <mn>1</mn>                  <mo>,</mo>                  <mi>t</mi>                 </mrow>                 <mo>)</mo>                </mrow>               </mrow>              </mtd>             </mtr>            </mtable>           </mtd>          </mtr>          <mtr>           <mtd>            <mo>&#x22ee;</mo>           </mtd>          </mtr>         </mtable>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mi>a_obj</mi>          <mo>&#x2062;</mo>          <mi>_mute</mi>          <mo>&#x2062;</mo>          <mrow>           <mo>(</mo>           <mrow>            <mi>N</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <mrow>           <mi>S</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mrow>             <mi>N</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>,</mo>            <mi>t</mi>           </mrow>           <mo>)</mo>          </mrow>         </mrow>        </mtd>       </mtr>      </mtable>      <mo>]</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0251" num="0242">It is to be noted that, in the expression (5), SP(m,t) indicates a virtual speaker signal at time t of the mth (where m=0, 1, . . . , M&#x2212;1) virtual speaker among M virtual speakers. Further, in the expression (5), S(n,t) indicates an object signal at time t of an nth (where n=0, 1, . . . , N&#x2212;1) audio object among N audio objects.</p><p id="p-0252" num="0243">Further, in the expression (5), G(m,n) indicates a gain to be multiplied to an object signal S(n,t) of the nth audio object for obtaining a virtual speaker signal SP(m,t) for the mth virtual speaker. In particular, the gain G(m,n) is a gain of each virtual speaker obtained in step S<b>109</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0253" num="0244">Further, in the expression (5), a_spk_mute[spk_id] indicates a coefficient that is determined by the virtual speaker mute information a_spk_mute[spk_id] for the mth virtual speaker. In particular, in the case where the value of the virtual speaker mute information a_spk_mute[spk_id] is 1, the value of the coefficient a_spk_mute(m) is set to 0, and in the case where the value of the virtual speaker mute information a_spk_mute[spk_id] is 0, the value of the coefficient a_spk_mute(m) is set to 1.</p><p id="p-0254" num="0245">Accordingly, in the case where the virtual speaker signal is mute (mute signal), the gain application section <b>82</b> does not perform arithmetic operation for the virtual speaker signal. In particular, the arithmetic operation for obtaining the virtual speaker signal SP(m,t) that is mute is not performed, and zero data is outputted as the virtual speaker signal SP(m,t). In other words, the arithmetic operation for the virtual speaker signal is omitted, and the arithmetic operation amount is reduced.</p><p id="p-0255" num="0246">Further, in the expression (5), a_obj_mute(n) indicates a coefficient determined by the audio object mute information a_obj_mute[obj_id] regarding the object signal of the nth audio object.</p><p id="p-0256" num="0247">In particular, in the case where the value of the audio object mute information a_obj_mute[obj_id] is 1, the value of the coefficient a_obj_mute(n) is set to 0, and in the case where the value of the audio object mute information a_obj_mute[obj_id] is 0, the value of the coefficient a_obj_mute(n) is set to 1.</p><p id="p-0257" num="0248">Accordingly, in the gain application section <b>82</b>, in the case where the object signal is mute (mute signal), the gain application section <b>82</b> does not perform arithmetic operation regarding the object signal. In particular, the product sum arithmetic operation of the term of the object signal S(n,t) that is mute is not performed. In other words, the arithmetic operation part based on the object signal is omitted, and the arithmetic operation amount is reduced.</p><p id="p-0258" num="0249">It is to be noted that, in the gain application section <b>82</b>, the arithmetic operation amount can be reduced if arithmetic operation of at least either one of the part of the object signal that is determined a mute signal or the part of the virtual speaker signal that is determined a mute signal is omitted. Accordingly, the example in which arithmetic operation of both the part of the object signal determined to be a mute signal and the part of the virtual speaker signal determined to be a mute signal are omitted is not restrictive, and arithmetic operation of one of them may be omitted.</p><p id="p-0259" num="0250">In step S<b>72</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the gain application section <b>82</b> performs arithmetic operation similar to that of the expression (5) on the basis of the audio object mute information and the virtual speaker mute information supplied from the mute information generation section <b>22</b>, gains supplied form the gain calculation section <b>81</b>, and object signals supplied from the IMDCT processing section <b>54</b> to obtain a virtual speaker signal for each virtual speaker. Especially here, for a part at which arithmetic operation is omitted, zero data is used as an arithmetic operation result. In other words, actual arithmetic operation is not performed, and zero data is outputted as a value corresponding to the arithmetic operation result.</p><p id="p-0260" num="0251">Generally, in the case where the calculation of the expression (3) is performed for certain time frames T, namely, during an interval during which the number of frames is T, arithmetic operation by M&#xd7;N&#xd7;T times is required.</p><p id="p-0261" num="0252">However, it is assumed here that audio objects that are determined mute by audio object mute information are 30% of all audio objects and the number of virtual speakers that are determined mute by the virtual speaker mute information is 30% of all virtual speakers.</p><p id="p-0262" num="0253">In such a case as just described, if the virtual speaker signal is obtained by calculation by the expression (5), then the arithmetic operation time is 0.7&#xd7;M&#xd7;0.7&#xd7;N&#xd7;T, and the arithmetic operation amount can be reduced approximately by 50% in comparison with that of the case of the expression (3). Besides, in this case, the virtual speaker signals obtained finally by the expression (3) and the expression (5) are same, and the omission of part of arithmetic operation does not give rise to an error.</p><p id="p-0263" num="0254">Generally, in the case where the number of audio objects is great and the number of virtual speakers is also great, in spatial arrangement of the audio objects by a content creator, mute audio objects or mute virtual speakers are more likely to appear. In other words, intervals during which the object signal is mute or intervals during which the virtual speaker signal is mute are likely to appear.</p><p id="p-0264" num="0255">Therefore, according to a method of omitting part of arithmetic operation like the expression (5), in such a case that the number of audio objects or the number of virtual speakers is great and the arithmetic operation amount is very grate, a higher reduction effect of the arithmetic operation amount can be achieved.</p><p id="p-0265" num="0256">Further, if a virtual speaker signal is generated by the gain application section <b>82</b> and supplied to the HRTF processing section <b>24</b>, then an output audio signal is generated in step S<b>13</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0266" num="0257">In particular, in step S<b>13</b>, the HRTF processing section <b>24</b> generates an output audio signal on the basis of the virtual speaker mute information supplied from the mute information generation section <b>22</b> and the virtual speaker signal supplied from the gain application section <b>82</b>.</p><p id="p-0267" num="0258">Generally, an output audio signal is obtained by a convolution process of a transfer function that is an HRTF coefficient as indicated by the expression (4) and a virtual speaker signal.</p><p id="p-0268" num="0259">However, in the HRTF processing section <b>24</b>, the virtual speaker mute information is used to obtain an output audio signal in accordance with the following expression (6).</p><p id="p-0269" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mtext>                </mtext>     <mrow>      <mo>[</mo>      <mrow>       <mi>Math</mi>       <mo>.</mo>       <mtext>   </mtext>       <mn>6</mn>      </mrow>      <mo>]</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <mo>[</mo>       <mtable>        <mtr>         <mtd>          <mrow>           <mi>L</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mi>&#x3c9;</mi>           <mo>)</mo>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mi>R</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mi>&#x3c9;</mi>           <mo>)</mo>          </mrow>         </mtd>        </mtr>       </mtable>       <mo>]</mo>      </mrow>      <mo>=</mo>      <mrow>       <mo>[</mo>       <mtable>        <mtr>         <mtd>          <mrow>           <mi>H_L</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mn>0</mn>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>         </mtd>         <mtd>          <mrow>           <mi>H_L</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mn>1</mn>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mo>&#x2026;</mo>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mi>H_L</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mrow>              <mi>M</mi>              <mo>-</mo>              <mn>1</mn>             </mrow>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mi>H_R</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mn>0</mn>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>         </mtd>         <mtd>          <mrow>           <mi>H_R</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mn>1</mn>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mo>&#x2026;</mo>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mi>H_R</mi>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mrow>              <mi>M</mi>              <mo>-</mo>              <mn>1</mn>             </mrow>             <mo>,</mo>             <mi>&#x3c9;</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>         </mtd>        </mtr>       </mtable>       <mo>]</mo>      </mrow>     </mrow>     <mo>&#x2062;</mo>     <mtext>&#x2028;</mtext>     <mrow>      <mo>[</mo>      <mtable>       <mtr>        <mtd>         <mtable>          <mtr>           <mtd>            <mtable>             <mtr>              <mtd>               <mrow>                <mi>a_spk</mi>                <mo>&#x2062;</mo>                <mi>_mute</mi>                <mo>&#x2062;</mo>                <mrow>                 <mo>(</mo>                 <mn>0</mn>                 <mo>)</mo>                </mrow>                <mo>&#x2062;</mo>                <mrow>                 <mi>SP</mi>                 <mo>&#x2061;</mo>                 <mo>(</mo>                 <mrow>                  <mn>0</mn>                  <mo>,</mo>                  <mi>&#x3c9;</mi>                 </mrow>                 <mo>)</mo>                </mrow>               </mrow>              </mtd>             </mtr>             <mtr>              <mtd>               <mrow>                <mi>a_spk</mi>                <mo>&#x2062;</mo>                <mi>_mute</mi>                <mo>&#x2062;</mo>                <mrow>                 <mo>(</mo>                 <mn>1</mn>                 <mo>)</mo>                </mrow>                <mo>&#x2062;</mo>                <mrow>                 <mi>SP</mi>                 <mo>&#x2061;</mo>                 <mo>(</mo>                 <mrow>                  <mn>1</mn>                  <mo>,</mo>                  <mi>&#x3c9;</mi>                 </mrow>                 <mo>)</mo>                </mrow>               </mrow>              </mtd>             </mtr>            </mtable>           </mtd>          </mtr>          <mtr>           <mtd>            <mo>&#x22ee;</mo>           </mtd>          </mtr>         </mtable>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mi>a_spk</mi>          <mo>&#x2062;</mo>          <mi>_mute</mi>          <mo>&#x2062;</mo>          <mrow>           <mo>(</mo>           <mrow>            <mi>M</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <mrow>           <mi>SP</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mrow>             <mi>M</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>,</mo>            <mi>&#x3c9;</mi>           </mrow>           <mo>)</mo>          </mrow>         </mrow>        </mtd>       </mtr>      </mtable>      <mo>]</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0270" num="0260">It is to be noted that, in the expression (6), &#x3c9; indicates a frequency, and SP(m,&#x3c9;) indicates a virtual speaker signal of the frequency &#x3c9; of the mth (where m=0, 1, . . . , M&#x2212;1) virtual speaker among M virtual speakers. The virtual speaker signal SP(m,&#x3c9;) can be obtained by time frequency conversion of the virtual speaker signal that is a time signal.</p><p id="p-0271" num="0261">Further, in the expression (6), H_L(m,&#x3c9;) indicates a transfer function for the left ear to be multiplied to the virtual speaker signal SP(m,&#x3c9;) for the mth virtual speaker for obtaining an output audio signal L(&#x3c9;) of the left channel. Similarly, H_R(m,&#x3c9;) indicates a transfer function for the right ear.</p><p id="p-0272" num="0262">Further, in the expression (6), a_spk_mute(m) indicates a coefficient determined by the virtual speaker mute information a_spk_mute[spk_id] regarding the mth virtual speaker. In particular, in the case where the value of the virtual speaker mute information a_spk_mute[spk_id] is 1, the value of the coefficient a_spk_mute(m) is set to 0, and in the case where the value of the virtual speaker mute information a_spk_mute[spk_id] is 0, the value of the coefficient a_spk_mute(m) is set to 1.</p><p id="p-0273" num="0263">Accordingly, in the case where the virtual speaker signal is mute (mute signal) from the virtual speaker mute information, the HRTF processing section <b>24</b> does not perform arithmetic operation regarding the virtual speaker signal. In particular, the product sum arithmetic operation of the term of the virtual speaker signal SP(m,&#x3c9;) that is mute is not performed. In other words, the arithmetic operation (process) for convoluting the virtual speaker signal that is mute and the transfer function is omitted, and the arithmetic operation amount is reduced.</p><p id="p-0274" num="0264">Consequently, it is possible, in a convolution process in which the arithmetic operation amount is very great, for convolution arithmetic operation to be performed restrictively only for sounded virtual speaker signals, by which the arithmetic operation amount can be reduced significantly. Besides, in this case, the output audio signals obtained finally in accordance with both the expression (4) and the expression (6) are same as each other, and the omission of part of arithmetic operation does not give rise to an error.</p><p id="p-0275" num="0265">As described above, according to the present technology, in the case where a mute interval (mute signal) exists in an audio object, by omitting processing of at least part of a decoding process, a rendering process, or an HRTF process, the arithmetic operation amount can be reduced without giving rise to any error in an output audio signal. In other words, high presence can be obtained even with a small amount of arithmetic operation.</p><p id="p-0276" num="0266">Accordingly, in the present technology, since an average processing amount is reduced to reduce the power usage of the processor, it is possible to continuously reproduce a content for a longer period of time even with a portable apparatus such as a smartphone.</p><heading id="h-0022" level="1">Second Embodiment</heading><heading id="h-0023" level="2">&#x3c;Use of Object Priority&#x3e;</heading><p id="p-0277" num="0267">Incidentally, in the MPEG-H Part 3:3D audio standard, a degree of priority of an audio object can be placed into metadata (bit stream) together with object position information indicative of a position of the audio object. It is to be noted that the degree of priority of an audio object is hereinafter referred to as object priority.</p><p id="p-0278" num="0268">In the case where an object priority is included in metadata in such a manner, the metadata has, for example, such a format as depicted in <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0279" num="0269">In the example depicted in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, &#x201c;num_objects&#x201d; indicates the total number of audio objects, and [object_priority] indicates the object priority.</p><p id="p-0280" num="0270">Further, &#x201c;position_azimuth&#x201d; indicates a horizontal angle of an audio object in a spherical coordinate system; &#x201c;position_elevation&#x201d; indicates a vertical angle of the audio object in the spherical coordinate system; and &#x201c;position_radius&#x201d; indicates a distance (radius) from the origin of the spherical coordinate system to the audio object. Here, information including the horizontal angle, vertical angle, and distance makes object position information indicative of a position of the audio object.</p><p id="p-0281" num="0271">Further, in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the object priority object_priority is information of 3 bits and can assume a value from a low priority degree 0 to a high priority degree 7. In other words, a higher value of a priority degree from the priority degree 0 to the priority degree 7 indicates an audio object having a higher object priority.</p><p id="p-0282" num="0272">For example, in the case where the decoding side cannot perform processing for all audio objects, it is possible to process only audio objects having high object priorities according to a resource of the decoding side.</p><p id="p-0283" num="0273">In particular, it is assumed that, for example, there are three audio objects and the object priority of the audio objects is 7, 6, and 5. Further, it is assumed that the load of the processing apparatus is so high that it is difficult to process all of the three audio objects.</p><p id="p-0284" num="0274">In such a case as just described, for example, it is possible not to execute a process for the audio object whose object priority is 5 but to execute a process only for the audio objects having the object priorities 7 and 6.</p><p id="p-0285" num="0275">In addition, in the present technology, audio objects to be actually processed may be selected taking it also into consideration whether the signal of the audio object is mute.</p><p id="p-0286" num="0276">In particular, for example, on the basis of spectral mute information or audio object mute information, any mute audio object is excluded from among plural audio objects in a frame of a processing target. Then, from among the remaining audio objects after the mute audio objects are excluded, the number of audio objects to be processed, which number is determined by a resource or the like, are selected in order in the descending order of the object priority.</p><p id="p-0287" num="0277">In other words, at least either one of the decoding process or the rendering process is performed, for example, on the basis of spectral mute information or audio object mute information and the object priority.</p><p id="p-0288" num="0278">For example, it is assumed that an input bit stream includes audio object data of five audio objects of an audio object AOB1 to an audio object AOB5, and the signal processing apparatus <b>11</b> has a room for processing only three audio objects.</p><p id="p-0289" num="0279">At this time, for example, it is assumed that the value of the spectral mute information of the audio object AOB5 is 1 and the values of the spectral mute information of the other audio objects are 0. Further, it is assumed that the respective object priority of the audio object AOB1 to the audio object AOB4 are 7, 7, 6, and 5.</p><p id="p-0290" num="0280">In such a case as just described, for example, the spectral decoding section <b>53</b> first excludes the audio object AOB5 that is mute from among the audio objects AOB1 to AOB5. Then, the spectral decoding section <b>53</b> selects the audio object AOB1 to the audio object AOB3 having high object priorities from among the remaining audio objects AOB1 to AOB4.</p><p id="p-0291" num="0281">Then, the spectral decoding section <b>53</b> performs decoding of spectral data only of the audio objects AOB1 to AOB3 selected finally.</p><p id="p-0292" num="0282">This makes it possible to reduce the number of audio objects to be substantially discarded even in such a case that the processing load of the signal processing apparatus <b>11</b> is so high that the signal processing apparatus <b>11</b> cannot perform processing of all of the audio objects.</p><heading id="h-0024" level="2">&#x3c;Example of Configuration of Computer&#x3e;</heading><p id="p-0293" num="0283">While the series of processes described above can be executed by hardware, it can otherwise also be executed by software. In the case where the series of processes is executed by software, a program that constructs the software is installed into a computer. The computer here includes a computer that is incorporated in hardware for exclusive use, a personal computer, for example, for universal use that can execute various functions by installing various programs into the personal computer and so forth.</p><p id="p-0294" num="0284"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram depicting an example of a hardware configuration of a computer that executes the series of processes described hereinabove in accordance with a program.</p><p id="p-0295" num="0285">In the computer, a CPU (Central Processing Unit) <b>501</b>, a ROM (Read Only Memory) <b>502</b>, and a RAM (Random Access Memory) <b>503</b> are connected to one another by a bus <b>504</b>.</p><p id="p-0296" num="0286">Further, an input/output interface <b>505</b> is connected to the bus <b>504</b>. An inputting section <b>506</b>, an outputting section <b>507</b>, a recording section <b>508</b>, a communication section <b>509</b>, and a drive <b>510</b> are connected to the input/output interface <b>505</b>.</p><p id="p-0297" num="0287">The inputting section <b>506</b> includes, for example, a keyboard, a mouse, a microphone, an imaging element and so forth. The outputting section <b>507</b> includes a display, a speaker and so forth. The recording section <b>508</b> includes, for example, a hard disk, a nonvolatile memory or the like. The communication section <b>509</b> includes a network interface and so forth. The drive <b>510</b> drives a removable recording medium <b>511</b> such as a magnetic disk, an optical disk, a magneto-optical disk, or a semiconductor memory.</p><p id="p-0298" num="0288">In the computer configured in such a manner as described above, the CPU <b>501</b> loads a program recorded, for example, in the recording section <b>508</b> into the RAM <b>503</b> through the input/output interface <b>505</b> and the bus <b>504</b> and executes the program to perform the series of processes described above.</p><p id="p-0299" num="0289">The program to be executed by the computer (CPU <b>501</b>) can be recorded on and provided as a removable recording medium <b>511</b> as, for example, a package medium. Further, the program can be provided through a wired or wireless transmission medium such as a local area network, the Internet, or a digital satellite broadcast.</p><p id="p-0300" num="0290">In the computer, a program can be installed into the recording section <b>508</b> through the input/output interface <b>505</b> by mounting the removable recording medium <b>511</b> on the drive <b>510</b>. Further, the program can be received by the communication section <b>509</b> through a wired or wireless transmission medium and installed into the recording section <b>508</b>. Further, it is possible to install the program in the ROM <b>502</b> or the recording section <b>508</b> in advance.</p><p id="p-0301" num="0291">It is to be noted that the program to be executed by a computer may be a program in which processes are performed in a time series in the order as described in the present specification or may be a program in which processes are executed in parallel or executed at necessary timings such as when the process is called.</p><p id="p-0302" num="0292">Further, the embodiment of the present technology is not limited to the embodiments described hereinabove and allows various alterations without departing from the subject matter of the present technology.</p><p id="p-0303" num="0293">For example, the present technology can assume a configuration for cloud computing by which one function is shared and cooperatively processed by plural apparatuses through a network.</p><p id="p-0304" num="0294">Further, the steps described hereinabove in connection with the flow charts not only can be executed by a single apparatus but also can be shared and executed by plural apparatuses.</p><p id="p-0305" num="0295">Further, in the case where plural processes are included in one step, the plural processes included in the one step not only can be executed by one apparatus but also can be shared and executed by plural apparatuses.</p><p id="p-0306" num="0296">Further, the present technology can take the following configurations.<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0297">(1)</li></ul></p><p id="p-0307" num="0298">A signal processing apparatus, in which,</p><p id="p-0308" num="0299">on the basis of audio object mute information indicative of whether or not a signal of an audio object is a mute signal, at least either one of a decoding process or a rendering process of an object signal of the audio object is performed.<ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0300">(2)</li></ul></p><p id="p-0309" num="0301">The signal processing apparatus according to (1), in which,</p><p id="p-0310" num="0302">in at least either one of the decoding process or the rendering process, either at least part of arithmetic operation is omitted or a value determined in advance is outputted as a value corresponding to a result of predetermined arithmetic operation according to the audio object mute information.<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0303">(3)</li></ul></p><p id="p-0311" num="0304">The signal processing apparatus according to (1) or (2), further including:</p><p id="p-0312" num="0305">an HRTF processing section that performs an HRTF process on the basis of a virtual speaker signal obtained by the rendering process and used to reproduce sound by a virtual speaker and virtual speaker mute information indicative of whether or not the virtual speaker signal is a mute signal.<ul id="ul0004" list-style="none">    <li id="ul0004-0001" num="0306">(4)</li></ul></p><p id="p-0313" num="0307">The signal processing apparatus according to (3), in which</p><p id="p-0314" num="0308">the HRTF processing section omits, from within the HRTF process, arithmetic operation for convoluting the virtual speaker signal determined to be a mute signal by the virtual speaker mute information and a transfer function.<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0309">(5)</li></ul></p><p id="p-0315" num="0310">The signal processing apparatus according to (3) or (4), further including:</p><p id="p-0316" num="0311">a mute information generation section configured to generate the audio object mute information on the basis of information regarding a spectrum of the object signal.<ul id="ul0006" list-style="none">    <li id="ul0006-0001" num="0312">(6)</li></ul></p><p id="p-0317" num="0313">The signal processing apparatus according to (5), further including:</p><p id="p-0318" num="0314">a decoding processing section configured to perform the decoding process including decoding of spectral data of the object signal encoded by a context-based arithmetic encoding method, in which</p><p id="p-0319" num="0315">the decoding processing section does not perform calculation of a context of the spectral data determined as a mute signal by the audio object mute information but decodes the spectral data by using a value determined in advance as a result of calculation of the context.<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0316">(7)</li></ul></p><p id="p-0320" num="0317">The signal processing apparatus according to (6), in which</p><p id="p-0321" num="0318">the decoding processing section performs the decoding process including decoding of the spectral data and an IMDCT process for the decoded spectral data and outputs zero data without performing the IMDCT process for the decoded spectral data determined as a mute signal by the audio object mute information.<ul id="ul0008" list-style="none">    <li id="ul0008-0001" num="0319">(8)</li></ul></p><p id="p-0322" num="0320">The signal processing apparatus according to any one of (5) to (7), in which</p><p id="p-0323" num="0321">the mute information generation section generates, on the basis of a result of the decoding process, another audio object mute information different from the audio object mute information used in the decoding process, and</p><p id="p-0324" num="0322">the signal processing apparatus further includes a rendering processing section configured to perform the rendering process on the basis of the another audio object mute information.<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0323">(9)</li></ul></p><p id="p-0325" num="0324">The signal processing apparatus according to (8), in which</p><p id="p-0326" num="0325">the rendering processing section performs a gain calculation process of obtaining a gain of the virtual speaker for each object signal obtained by the decoding process and a gain application process of generating the virtual speaker signal on the basis of the gain and the object signal as the rendering process.<ul id="ul0010" list-style="none">    <li id="ul0010-0001" num="0326">(10)</li></ul></p><p id="p-0327" num="0327">The signal processing apparatus according to (9), in which</p><p id="p-0328" num="0328">the rendering processing section omits, in the gain application process, at least either one of arithmetic operation of the virtual speaker signal determined as a mute signal by the virtual speaker mute information or arithmetic operation based on the object signal determined as a mute signal by the another audio object mute information.<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0329">(11)</li></ul></p><p id="p-0329" num="0330">The signal processing apparatus according to (9) or (10), in which</p><p id="p-0330" num="0331">the mute information generation section generates the virtual speaker mute information on the basis of a result of the calculation of the gain and the another audio object mute information.<ul id="ul0012" list-style="none">    <li id="ul0012-0001" num="0332">(12)</li></ul></p><p id="p-0331" num="0333">The signal processing apparatus according to any one of (1) to (11), in which</p><p id="p-0332" num="0334">at least either one of the decoding process or the rendering process is performed on the basis of a priority degree of the audio object and the audio object mute information.<ul id="ul0013" list-style="none">    <li id="ul0013-0001" num="0335">(13)</li></ul></p><p id="p-0333" num="0336">A signal processing method, in which</p><p id="p-0334" num="0337">a signal processing apparatus performs,</p><p id="p-0335" num="0338">on the basis of audio object mute information indicative of whether or not a signal of an audio object is a mute signal, at least either one of a decoding process or a rendering process of an object signal of the audio object.<ul id="ul0014" list-style="none">    <li id="ul0014-0001" num="0339">(14)</li></ul></p><p id="p-0336" num="0340">A program for causing a computer to process including a step of:</p><p id="p-0337" num="0341">performing, on the basis of audio object mute information indicative of whether or not a signal of an audio object is a mute signal, at least either one of a decoding process or a rendering process of an object signal of the audio object.</p><heading id="h-0025" level="1">REFERENCE SIGNS LIST</heading><p id="p-0338" num="0342"><b>11</b>: Signal processing apparatus</p><p id="p-0339" num="0343"><b>21</b>: Decoding processing section</p><p id="p-0340" num="0344"><b>22</b>: Mute information generation section</p><p id="p-0341" num="0345"><b>23</b>: Rendering processing section</p><p id="p-0342" num="0346"><b>24</b>: HRTF processing section</p><p id="p-0343" num="0347"><b>53</b>: Spectral decoding section</p><p id="p-0344" num="0348"><b>54</b>: IMDCT processing section</p><p id="p-0345" num="0349"><b>81</b>: Gain calculation section</p><p id="p-0346" num="0350"><b>82</b>: Gain application section</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007396A1-20230105-M00001.NB"><img id="EMI-M00001" he="30.31mm" wi="76.20mm" file="US20230007396A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230007396A1-20230105-M00002.NB"><img id="EMI-M00002" he="22.94mm" wi="76.20mm" file="US20230007396A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230007396A1-20230105-M00003.NB"><img id="EMI-M00003" he="45.47mm" wi="76.20mm" file="US20230007396A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230007396A1-20230105-M00004.NB"><img id="EMI-M00004" he="22.94mm" wi="76.20mm" file="US20230007396A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A signal processing apparatus, wherein,<claim-text>on a basis of audio object mute information indicative of whether or not a signal of an audio object is a mute signal, at least either one of a decoding process or a rendering process of an object signal of the audio object is performed.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The signal processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein,<claim-text>in at least either one of the decoding process or the rendering process, either at least part of arithmetic operation is omitted or a value determined in advance is outputted as a value corresponding to a result of predetermined arithmetic operation according to the audio object mute information.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The signal processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>an HRTF processing section that performs an HRTF process on a basis of a virtual speaker signal obtained by the rendering process and used to reproduce sound by a virtual speaker and virtual speaker mute information indicative of whether or not the virtual speaker signal is a mute signal.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The signal processing apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the HRTF processing section omits, from within the HRTF process, arithmetic operation for convoluting the virtual speaker signal determined to be a mute signal by the virtual speaker mute information and a transfer function.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The signal processing apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:<claim-text>a mute information generation section configured to generate the audio object mute information on a basis of information regarding a spectrum of the object signal.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The signal processing apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:<claim-text>a decoding processing section configured to perform the decoding process including decoding of spectral data of the object signal encoded by a context-based arithmetic encoding method, wherein</claim-text><claim-text>the decoding processing section does not perform calculation of a context of the spectral data determined as a mute signal by the audio object mute information but decodes the spectral data by using a value determined in advance as a result of calculation of the context.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The signal processing apparatus according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the decoding processing section performs the decoding process including decoding of the spectral data and an IMDCT process for the decoded spectral data and outputs zero data without performing the IMDCT process for the decoded spectral data determined as a mute signal by the audio object mute information.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The signal processing apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the mute information generation section generates, on a basis of a result of the decoding process, another audio object mute information different from the audio object mute information used in the decoding process, and</claim-text><claim-text>the signal processing apparatus further includes a rendering processing section configured to perform the rendering process on a basis of the another audio object mute information.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The signal processing apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein<claim-text>the rendering processing section performs a gain calculation process of obtaining a gain of the virtual speaker for each object signal obtained by the decoding process and a gain application process of generating the virtual speaker signal on a basis of the gain and the object signal as the rendering process.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The signal processing apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the rendering processing section omits, in the gain application process, at least either one of arithmetic operation of the virtual speaker signal determined as a mute signal by the virtual speaker mute information or arithmetic operation based on the object signal determined as a mute signal by the another audio object mute information.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The signal processing apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the mute information generation section generates the virtual speaker mute information on a basis of a result of the calculation of the gain and the another audio object mute information.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The signal processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>at least either one of the decoding process or the rendering process is performed on a basis of a priority degree of the audio object and the audio object mute information.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A signal processing method, wherein<claim-text>a signal processing apparatus performs,</claim-text><claim-text>on a basis of audio object mute information indicative of whether or not a signal of an audio object is a mute signal, at least either one of a decoding process or a rendering process of an object signal of the audio object.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A program for causing a computer to process comprising a step of:<claim-text>performing, on a basis of audio object mute information indicative of whether or not a signal of an audio object is a mute signal, at least either one of a decoding process or a rendering process of an object signal of the audio object.</claim-text></claim-text></claim></claims></us-patent-application>