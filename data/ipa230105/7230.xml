<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007231A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007231</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17778607</doc-number><date>20201006</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-216706</doc-number><date>20191129</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>368</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>398</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>341</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>111</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>368</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>398</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>341</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>111</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>2213</main-group><subgroup>008</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE PROCESSING DEVICE, IMAGE PROCESSING METHOD, AND IMAGE DISPLAY SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KADAM</last-name><first-name>Akshat</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>SONY GROUP CORPORATION</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/037926</doc-number><date>20201006</date></document-id><us-371c12-date><date>20220520</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">There is provided an image processing device that processes a projection image presented to a plurality of persons at the same time. The image processing device specifies an overlapping area in which fields of view of two or more users overlap based on information on each user, classifies objects included in the overlapping area into a first object group and a second object group, generates a common image common to all users, made up of the first object group, generates individual images different for each user, made up of the second object group, and determines an output protocol for displaying the individual images.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="96.77mm" wi="146.56mm" file="US20230007231A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="215.14mm" wi="126.32mm" file="US20230007231A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="232.07mm" wi="151.38mm" file="US20230007231A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="220.73mm" wi="152.32mm" file="US20230007231A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="140.80mm" wi="151.47mm" file="US20230007231A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="176.61mm" wi="152.91mm" file="US20230007231A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="183.05mm" wi="152.40mm" file="US20230007231A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="142.49mm" wi="151.38mm" file="US20230007231A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="184.74mm" wi="152.15mm" file="US20230007231A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="178.05mm" wi="152.15mm" file="US20230007231A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="177.55mm" wi="151.72mm" file="US20230007231A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="223.10mm" wi="136.23mm" file="US20230007231A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="226.57mm" wi="152.40mm" file="US20230007231A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="217.85mm" wi="141.90mm" orientation="landscape" file="US20230007231A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="143.68mm" wi="152.48mm" file="US20230007231A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="141.90mm" wi="152.15mm" file="US20230007231A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="146.90mm" wi="152.15mm" file="US20230007231A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="210.31mm" wi="152.99mm" file="US20230007231A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="217.34mm" wi="152.57mm" file="US20230007231A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="216.58mm" wi="152.57mm" file="US20230007231A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="195.75mm" wi="152.48mm" file="US20230007231A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="204.13mm" wi="152.91mm" file="US20230007231A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="198.04mm" wi="152.48mm" file="US20230007231A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="207.77mm" wi="152.15mm" file="US20230007231A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="208.87mm" wi="152.74mm" file="US20230007231A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="101.09mm" wi="150.45mm" file="US20230007231A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="221.91mm" wi="147.49mm" file="US20230007231A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="154.35mm" wi="131.57mm" file="US20230007231A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="154.43mm" wi="131.32mm" file="US20230007231A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="218.44mm" wi="152.15mm" file="US20230007231A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="227.58mm" wi="144.53mm" orientation="landscape" file="US20230007231A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="192.87mm" wi="150.71mm" file="US20230007231A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The technologies disclosed in the present specification (hereinafter referred to as &#x201c;the present disclosure&#x201d;) relate to an image processing device and an image processing method for processing a projection image, and an image display system.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">Projection technology for projecting images on screens has been known for a long time and is widely used in an educational field, conferences, and presentations. Since an image can be enlarged and displayed on a relatively large screen, there is an advantage that the same image can be presented to a plurality of persons at the same time. Recently, a projection mapping technology for projecting and displaying an image on the surface of a screen having an arbitrary shape such as a building has been frequently used. The projection mapping technology is realized, for example, by measuring a three-dimensional shape of a projection surface and projecting a distortion-free image by correcting the projection image according to the measurement result (see, for example, PTL 1).</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0004" num="0003">[PTL 1]<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0004">JP 2001-320652 A</li></ul></p><p id="p-0005" num="0005">[PTL 2]<ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0006">JP 2019-184734 A</li></ul></p><p id="p-0006" num="0007">[PTL 3]<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0008">JP 2011-71757 A</li></ul></p><heading id="h-0005" level="1">SUMMARY</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0007" num="0009">An object of the technology according to the present disclosure is to provide an image processing device and an image processing method for processing an image projected on a screen having an arbitrary shape, and an image display system.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0008" num="0010">A first aspect of the technology according to the present disclosure provides an image processing device that executes processing on content to be displayed to a plurality of users, the device including: a user information acquisition unit that acquires information on each user; a classification unit that specifies an overlapping area in which fields of view of two or more users overlap based on the information on each user and classifies objects included in the overlapping area into a first object group and a second object group; a generation unit that generates a common image common to all users, made up of the first object group and generates individual images different for each user, made up of the second object group; and a determination unit that determines an output protocol for displaying the individual images.</p><p id="p-0009" num="0011">The classification unit classifies the objects included in the overlapping area into the first object group present in a background and the second object group which is a foreground for at least some users, and the generation unit generates a background image including the first object group as a common image and generates a foreground image for each user including the second object group as the individual image. For example, the generation unit generates an individual foreground image in which each object included in the second object group has an effect of motion parallax for each user. Alternatively, the generation unit generates an individual image including an object assigned to each user in the second object group.</p><p id="p-0010" num="0012">The determination unit determines an output protocol related to a timing for outputting the individual images in a time-division manner. The image processing device according to the first aspect further includes a communication unit for notifying a device such as shutter glasses used by each user of the output protocol.</p><p id="p-0011" num="0013">A second aspect of the technology according to the present disclosure provides an image processing method of executing processing on content to be displayed to a plurality of users, the method including: a user information acquisition step of acquiring information on each user; a classification step of specifying an overlapping area in which fields of view of two or more users overlap based on the information on each user and classifying objects included in the overlapping area into a first object group and a second object group; a generation step of generating a common image common to all users, made up of the first object group and generating individual images different for each user, made up of the second object group; and a determination step of determining an output protocol for displaying the individual images.</p><p id="p-0012" num="0014">A third aspect of the technology according to the present disclosure provides an image display system including: an image processing device that specifies an overlapping area in which fields of view of two or more users overlap based on information on each user, classifies objects included in the overlapping area into a first object group and a second object group, generates a common image common to all users, made up of the first object group, generates individual images different for each user, made up of the second object group, and notifies shutter glasses used by each user of an output protocol for displaying the individual images; a first display device that outputs the common image; a second display device that outputs the individual images of each user; and the shutter glasses used by each user.</p><heading id="h-0008" level="1">Advantageous Effects of Invention</heading><p id="p-0013" num="0015">According to the technology according to the present disclosure, it is possible to provide an image processing device and an image processing method for processing a projection image presented to a plurality of persons at the same time, and an image display system.</p><p id="p-0014" num="0016">Note that the effects described in the present specification are merely examples, and the effects provided by the technology according to the present disclosure are not limited thereto. In addition, the technology according to the present disclosure may further provide additional effects in addition to the aforementioned effects.</p><p id="p-0015" num="0017">Other objects, features, and advantages of the technology of the present disclosure will become clear according to detailed description based on embodiments which will be described later and the attached drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0016" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram showing an operation example of projection mapping (a perspective view from above).</p><p id="p-0017" num="0019"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram showing an operation example of projection mapping (a top view from above).</p><p id="p-0018" num="0020"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram showing an operation example in which a plurality of users experience projection mapping at the same time.</p><p id="p-0019" num="0021"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram showing a mechanism for mapping VR images for users A and B on the wall surface of a room <b>301</b>.</p><p id="p-0020" num="0022"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing a state in which an overlapping area <b>503</b> is present between the view ranges <b>501</b> and <b>502</b> of users A and B who share a real environment.</p><p id="p-0021" num="0023"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram showing a mechanism for detecting an overlapping area between the views of users who share a real environment.</p><p id="p-0022" num="0024"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram showing a state in which three users share the same real environment.</p><p id="p-0023" num="0025"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram showing an operation example of projection mapping when three users share the same real environment.</p><p id="p-0024" num="0026"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram showing an operation example of projection mapping when three users share the same real environment.</p><p id="p-0025" num="0027"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram showing a state in which three users share the same real environment.</p><p id="p-0026" num="0028"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram showing an operation example of projection mapping when three users share the same real environment.</p><p id="p-0027" num="0029"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram showing an operation example of projection mapping when three users share the same real environment.</p><p id="p-0028" num="0030"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram showing an operation example of projection mapping when three users share the same real environment.</p><p id="p-0029" num="0031"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating an assumed virtual environment.</p><p id="p-0030" num="0032"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram showing an image of the virtual environment shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> observed from the viewpoint position of user A.</p><p id="p-0031" num="0033"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram showing an image of the virtual environment shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> observed from the viewpoint position of user B.</p><p id="p-0032" num="0034"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram showing an example of performing projector stacking in a real environment shared by users A and B.</p><p id="p-0033" num="0035"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram showing a background image separated from the virtual environment shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>.</p><p id="p-0034" num="0036"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram showing a foreground image in which an effect of motion parallax for user A is given to an object in the foreground separated from the virtual environment shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>.</p><p id="p-0035" num="0037"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram showing a foreground image in which an effect of motion parallax for user B is given to an object in the foreground separated from the virtual environment shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>.</p><p id="p-0036" num="0038"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram showing a state in which a background image is projected.</p><p id="p-0037" num="0039"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram showing a state in which a foreground image for user A is superimposed and projected on a background image.</p><p id="p-0038" num="0040"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram showing a state in which a foreground image for user B is projected on a background image.</p><p id="p-0039" num="0041"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a diagram showing an operation example of a first projector <b>2100</b>.</p><p id="p-0040" num="0042"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a diagram showing an operation example of a second projector <b>2200</b>.</p><p id="p-0041" num="0043"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a diagram showing an image of an automobile traveling from a distance toward the front.</p><p id="p-0042" num="0044"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a diagram showing an image of an automobile traveling from a distance toward the front.</p><p id="p-0043" num="0045"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a diagram showing an image for user A when the automobile reaches the foreground area.</p><p id="p-0044" num="0046"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a diagram showing an image for user A when the automobile reaches the foreground area.</p><p id="p-0045" num="0047"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a diagram showing an operation example of a second projector <b>2802</b> that projects a foreground image.</p><p id="p-0046" num="0048"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a diagram showing a display example of projector stacking in which individual foreground images for each user are superimposed on a common background image in a time-division manner.</p><p id="p-0047" num="0049"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a diagram showing a display example of projector stacking in which a foreground image for user A is superimposed on a common background image.</p><p id="p-0048" num="0050"><figref idref="DRAWINGS">FIG. <b>33</b></figref> is a diagram showing a display example of projector stacking in which a foreground image for user B is superimposed on a common background image.</p><p id="p-0049" num="0051"><figref idref="DRAWINGS">FIG. <b>34</b></figref> is a diagram showing a display example of projector stacking in which a foreground image for user C is superimposed on a common background image.</p><p id="p-0050" num="0052"><figref idref="DRAWINGS">FIG. <b>35</b></figref> is a diagram showing an operation example of a second projector <b>3112</b> that projects a foreground image.</p><p id="p-0051" num="0053"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a diagram showing a functional configuration example of an image processing device <b>3600</b>.</p><p id="p-0052" num="0054"><figref idref="DRAWINGS">FIG. <b>37</b></figref> is a flowchart showing a processing procedure executed by the image processing device <b>3600</b>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0053" num="0055">Hereinafter, embodiments of the technology according to the present disclosure will be described in detail with reference to the drawings.</p><heading id="h-0011" level="1">First Embodiment</heading><p id="p-0054" num="0056"><figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> schematically show an operation example of projection mapping. In the shown example, a user <b>100</b> experiencing a virtual reality (VR) space provided by projection mapping technology stands in a rectangular room <b>101</b>. The wall surface on all sides of the room <b>101</b> is the real environment. Further, a rectangular virtual screen <b>102</b> on which a VR image to be presented to the user <b>100</b> is drawn is arranged on the outer side of the room <b>101</b>. However, <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a perspective view showing the real environment and the virtual environment viewed from above, and <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a top view showing the real environment and the virtual environment viewed from above.</p><p id="p-0055" num="0057">When a VR image on the virtual screen <b>102</b> is presented to the user by projection mapping using the wall surface of the room <b>101</b>, the three-dimensional shape of the wall surface of the room <b>101</b> is measured by a camera or a distance sensor (not shown), and a distortion-free VR image is projected on the wall surface of the room <b>101</b> by correcting the projection image according to the measurement result. Conventionally, an image projected by a projection mapping technology is basically an image displayed only from a specific viewpoint (for example, a viewpoint at the time of creation). Therefore, even if the user <b>100</b> moves in the room <b>101</b> under the operating environment as shown in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, the image projected on the wall surface does not move, so that the user <b>100</b> cannot be completely immersed in the virtual world. Therefore, there is a need for a mechanism for tracking the position, posture, and line of sight of the user <b>100</b> and feeding back the tracking information so that the VR image follows the user <b>100</b>.</p><p id="p-0056" num="0058"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically shows an operation example in which a plurality of users experience projection mapping at the same time. However, the figure shows the real environment and the virtual environment viewed from above. For the sake of simplification, in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, there are two users, users A and B.</p><p id="p-0057" num="0059">Here, it is assumed that a VR image that follows the movement of each user is presented. Therefore, the positions, postures, and lines of sight of users A and B are tracked. Then, as the VR image on the virtual screen <b>302</b>, two types of images are generated including the VR image (View for User A) viewed from the position, posture, and line of sight of user A, and the VR image (View for User B) viewed from the position, posture, and line of sight of user B. The VR image for user A on the virtual screen <b>302</b> is projected on the area of the field of view (FoV limit) of user A, of the wall surface of the room <b>301</b> by projection mapping. The VR image for user B on the virtual screen <b>302</b> is projected on the area of the field of view (FoV limit) of user B, of the wall surface of the room <b>301</b> by projection mapping.</p><p id="p-0058" num="0060"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a mechanism for mapping VR images for users A and B on the wall surface of the room <b>301</b>. The positions, postures, and lines of sight of users A and B in the room <b>301</b> are tracked based on sensor information from a camera, a distance sensor, or other sensors (not shown).</p><p id="p-0059" num="0061">Then, as the VR image on the virtual screen <b>302</b>, two types of images are generated including a VR image (View for User A) <b>401</b> viewed from the position, posture, and line of sight of user A, and a VR image (View for User B) <b>402</b> viewed from the position, posture, and line of sight of user B. Both the VR image <b>401</b> and the VR image <b>402</b> are images mapped to the same virtual world coordinate system, but in reality, they are written to, for example, individual plane memories for each user.</p><p id="p-0060" num="0062">Next, an image <b>411</b> of the viewing area of user A is cut out from one VR image <b>401</b>, a three-dimensional shape of a viewing area <b>311</b> of user A, of the wall surface of the room <b>301</b> is measured, and the original VR image <b>411</b> is corrected according to the measurement result, whereby a distortion-free VR image is projected on the viewing area <b>311</b> of user A on the wall surface of the room <b>301</b>. Similarly, an image <b>412</b> of the viewing area of user B is cut out from the other VR image <b>402</b>, a three-dimensional shape of a viewing area <b>312</b> of user B, of the wall surface of the room <b>301</b> is measured, and the original VR image <b>412</b> is corrected according to the measurement result, whereby a distortion-free VR image is projected on the viewing area <b>312</b> of user B on the wall surface of the room <b>301</b>.</p><p id="p-0061" num="0063">On the wall surface of the room <b>301</b>, one image including the VR image <b>401</b> for user A mapped to the viewing area <b>311</b> of user A and the VR image <b>402</b> for user B mapped to the viewing area <b>312</b> of user B is generated and projected using projection mapping technology. Two or more projectors are used to project an image over a wide area of the wall surface of the room <b>301</b>. The VR image <b>401</b> for user A and the VR image <b>402</b> for user B may both be generated from the same content or may be generated from different contents.</p><p id="p-0062" num="0064"><figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref> show a display example of a VR image when one real environment is shared by two users, but the same processing is performed when one real environment is shared by three or more users.</p><p id="p-0063" num="0065">In <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>, for the sake of simplicity, an example in which the fields of view do not overlap among a plurality of users who share one real environment has been described. However, it is also assumed that the fields of view overlap among a plurality of users who share one real environment. When a VR image directed to another user is reflected in the field of view of one user, there is a problem that the VR experience of the user is disturbed.</p><p id="p-0064" num="0066"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a state in which an overlapping area <b>503</b> is present between the view ranges <b>501</b> and <b>502</b> of users A and B who share the real environment made up of the wall surface <b>500</b> of a room. A VR image for user A is generated from the position, posture, and line of sight of user A with respect to the wall surface of the room <b>500</b>, and is projected on the view range <b>501</b> of the wall surface <b>500</b> using the projection mapping technology. Further, a VR image for user B is generated from the position, posture, and line of sight of user B with respect to the wall surface of the room <b>500</b>, and is projected on the field of view range <b>502</b> of the wall surface <b>500</b> using the projection mapping technology. Then, since the VR image directed to user B is reflected in the overlapping area <b>503</b> of the view range <b>501</b> of user A, the VR experience of user A is disturbed. Similarly, since the VR image directed to user A is reflected in the overlapping area <b>503</b> of the view range <b>502</b> of user B, the VR experience of user B is also disturbed.</p><p id="p-0065" num="0067">Therefore, in the technology according to the present disclosure, it is precisely detected whether or not there is an overlapping area between the views of a plurality of users, and if the overlapping area is detected, the individual VR images directed to respective users are allocated to the overlapping area in a time-division manner so as to create an image suitable for each user. Each user observes the real environment by wearing shutter glasses (see, for example, PTL 3) that open and close in synchronization with switching to a VR image for himself/herself in the overlapping area. As a result, each user can view only the VR image for himself/herself, and the VR experience is not disturbed by the VR image directed to other users. On the other hand, when the overlapping area is not detected between the fields of view of users, one image including all the VR images for respective users is generated and projected on the real environment using the projection mapping technology.</p><p id="p-0066" num="0068">In the real environment shared by a plurality of users, it is necessary to precisely detect the overlapping area between the fields of view of users in order for each user to have perfect VR experience. A method of detecting the overlapping area between the fields of views of users will be described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. However, in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, for the sake of simplification of the explanation, the real environment is a flat wall surface <b>600</b>, which is shared by two users A and B.</p><p id="p-0067" num="0069">First, information on the positions, postures, and lines of sight of users A and B is acquired for each frame to estimate the view ranges (dynamic fields of view) of the respective users.</p><p id="p-0068" num="0070">Next, a projection range <b>601</b> of the VR image for user A is set according to the human view limit and the distance from user A to the wall surface <b>600</b> which is the projection surface. Similarly, a projection range <b>602</b> of the VR image for user B is set according to the human view limit and the distance from user B to the wall surface <b>600</b> which is the projection surface.</p><p id="p-0069" num="0071">In this way, when the projection ranges <b>601</b> and <b>602</b> on the wall surface <b>600</b> with respect to the dynamic fields of view of users A and B are obtained, it is possible to precisely detect whether or not the fields of view of users A and B overlap. In the example shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, it is assumed that the projection range <b>601</b> of user A and the projection range <b>602</b> of user B do not overlap.</p><p id="p-0070" num="0072"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a state in which three users A, B, and C share the same real environment. In the figure, the real environment is made up of a wall surface <b>700</b> of a room on which a VR image is projected. Then, the VR images directed to users A, B, and C are projected on projection ranges <b>701</b>, <b>702</b>, and <b>703</b> set individually for the users by the projection mapping technology. Here, it is assumed that an overlapping area <b>711</b> is detected between the projection range <b>701</b> of the VR image for user A and the projection range <b>702</b> of the VR image for the adjacent user B, and an overlapping area <b>712</b> is detected between the projection range <b>702</b> of the VR image for user B and the projection range <b>703</b> of the VR image for the adjacent user C.</p><p id="p-0071" num="0073">As described above, in the technology according to the present disclosure, when an overlapping area is detected between the fields of view of a plurality of users, individual VR images directed to respective users are allocated to the overlapping area in a time-division manner to create an image suitable for each user.</p><p id="p-0072" num="0074"><figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref> show an operation example of projecting individual VR images directed to users A, B, and C in a time-division manner under the real environment shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. Here, the frame period is T [milliseconds]. The frame period T is the reciprocal of the frame rate. The frame period T is divided into two segments, and an image to be projected on the wall surface <b>700</b> is sequentially switched between a segment of 0 to T/2 and a segment of T/2 to T for each frame period.</p><p id="p-0073" num="0075">First, in the segment of 0 to T/2 in the first half of the frame period T, as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a VR image <b>802</b> for user B is projected on the projection range <b>702</b> of user B set based on the position, posture, and line of sight of user B. User B observes the wall surface <b>700</b> in the line-of-sight direction by wearing shutter glasses that open in synchronization with the segment of 0 to T/2, whereby user B can view only the VR image for user B, and the VR experience will not be disturbed by the VR images displayed for other users.</p><p id="p-0074" num="0076">Subsequently, in the segment of T/2 to T in the latter half of the frame period T, as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the VR image <b>901</b> for user A is projected on the projection range <b>701</b> set for user A, and the VR image <b>903</b> for user C is projected on the projection range <b>703</b> set for user C. Users A and C observe the wall surface <b>700</b> in the line-of-sight direction by wearing shutter glasses that open in synchronization with the segment of T/2 to T, whereby users A and C can view only the VR images for users A and C, respectively, and the VR experience will not be disturbed by the VR images directed to other users.</p><p id="p-0075" num="0077"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows another example in which three users A, B, and C share the same real environment. In the example shown in <figref idref="DRAWINGS">FIGS. <b>7</b> to <b>9</b></figref>, there is no overlapping area between the projection range <b>701</b> of the VR image for user A and the projection range <b>703</b> of the VR image for user C. On the other hand, in the example shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, overlapping areas <b>1011</b> and <b>1012</b> are detected between a projection range <b>1001</b> of user A and a projection range <b>1002</b> of user B and between the projection range <b>1002</b> of user B and a projection range <b>1003</b> of user C, and an overlapping area <b>1013</b> is also detected between the projection range <b>1001</b> of user A and the projection range <b>1003</b> of user C.</p><p id="p-0076" num="0078"><figref idref="DRAWINGS">FIGS. <b>11</b> to <b>13</b></figref> show an operation example of projecting individual VR images directed to users A, B, and C in a time-division manner under the real environment shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. In this case, the frame period T is divided into three segments, and an image to be projected on a wall surface <b>1000</b> is sequentially switched between a segment of 0 to T/3, a segment of T/3 to 2 T/3, and a segment of 2 T/3 to T for each frame period.</p><p id="p-0077" num="0079">First, in the first segment of 0 to T/3 of the frame period T, as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a VR image <b>1102</b> for user B is projected on a projection range <b>1002</b> of user B set based on the position, posture, and line of sight of user B. User B observes the wall surface <b>1000</b> in the line-of-sight direction by wearing shutter glasses that open in synchronization with the segment of 0 to T/3, whereby user B can view only the VR image for user B, and the VR experience of user B will not be disturbed by the VR images directed to users A and C.</p><p id="p-0078" num="0080">In the subsequent segment of T/3 to 2 T/3, as shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, a VR image <b>1201</b> for user A is projected on the projection range <b>1001</b> of user A set based on the position, posture, and line of sight of user A. User A observes the wall surface <b>1000</b> in the line-of-sight direction by wearing shutter glasses that open in synchronization with the segment of T/3 to 2 T/3, whereby user A can view only the VR image for user A, and the VR experience of user A will not be disturbed by the VR images directed to users B and C.</p><p id="p-0079" num="0081">In the subsequent segment of 2 T/3 to T, as shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, a VR image <b>1303</b> for user C is projected on a projection range <b>1003</b> of user C set based on the position, posture, and line of sight of user C. User C observes the wall surface <b>1000</b> in the line-of-sight direction by wearing shutter glasses that open in synchronization with the segment of 2 T/3 to T, whereby user C can view only the VR image for user C, and the VR experience of user C will not be disturbed by the VR images directed to users A and B.</p><p id="p-0080" num="0082">Note that, if the number of users whose fields of view overlap among the users who share the same real environment increases, the number of divided frame segments increases, the time during which each user can observe the VR image in the frame segment becomes shorter, and the brightness increases.</p><heading id="h-0012" level="1">Second Embodiment</heading><p id="p-0081" num="0083">Projector stacking is known in which images are superimposed and projected on the same projection surface using a plurality of projectors (see, for example, PTL 2). For example, the brightness of the projection image can be improved by projecting the same image in a superimposed manner.</p><p id="p-0082" num="0084">In the technology according to the present disclosure, an original content of a projection image is separated into the foreground and the background, and the foreground and the background are projected using separate projectors, thereby maintaining the effect of motion parallax and allowing the user to have more perfect VR experience.</p><p id="p-0083" num="0085">Motion parallax is the parallax caused by the relative movement between an observer and an observed object, and the closer the observed object is, the faster it moves, and the farther the observed object is, the slower it moves. Therefore, a virtual environment is separated into the background and the foreground, the information on the position, posture, and line of sight of the user is acquired for each frame to generate the optimum foreground image and background image, and projector stacking is performed in a real environment using two projectors. In this way, it is possible to maintain the effect of motion parallax and allow the user to have more perfect VR experience.</p><p id="p-0084" num="0086">It is assumed that projector stacking is performed in a real environment shared by a plurality of users. Since the position, posture, and line of sight of each user with respect to the projection surface are different, the motion parallax is different for each user. When projector stacking is performed, if an image having the effect of motion parallax is projected so as to be suitable for a certain user, there is a problem that the motion parallax becomes unnatural for other users and the VR experience is disturbed.</p><p id="p-0085" num="0087">Objects far from the viewpoint have a small change due to motion parallax, but objects closer to the viewpoint have a large change due to motion parallax. For example, as shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, it is assumed that a virtual environment <b>1400</b> has a tree <b>1401</b> in a distance and a mailbox <b>1402</b> in the front in the area where the view ranges of users A and B overlap. In such a case, the motion parallax of the distant tree <b>1401</b> is small, but the motion parallax of the front mailbox <b>1402</b> is large between users A and B. Therefore, it is considered appropriate that user A observes a VR image in which the tree <b>1401</b> is seen in the back on the left side of the mailbox <b>1402</b> in the front as shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, while user B observes a VR image in which the tree <b>1401</b> is seen in the back on the right side of the mailbox <b>1402</b> in the front as shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>. However, if the image shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, in which only the effect of motion parallax on user A is maintained, is constantly projected on the real environment by projection mapping, the motion parallax becomes unnatural for user B who observes the image, and the VR experience is disturbed.</p><p id="p-0086" num="0088">Therefore, in the technology according to the present disclosure, the subject (object) included in the overlapping area where the fields of view of a plurality of users overlap is separated into the foreground and the background, the background image is treated as a common image since it has a small difference in motion parallax between the users, and optimum individual foreground images that reproduce the motion parallax according to the position, posture, and line of sight of each user are generated for respective users. Then, the background image common to the users is constantly projected and displayed using a first projector for background image projection, and the foreground images different for each user are projected and displayed using a second projector for foreground image projection in a time-division manner. Each user observes the real environment by wearing shutter glasses that open and close in synchronization with the switching to the foreground image for himself/herself. As a result, each user can view the VR image in which the optimum foreground image is superimposed on the background image common to the users. The optimum foreground image is a foreground image that maintains the effect of motion parallax for each user, and the user can have more perfect VR experience.</p><p id="p-0087" num="0089">Both the first projector and the second projector project images in the real environment using the projection mapping technology. The first projector and the second projector may each be composed of a plurality of projector devices.</p><p id="p-0088" num="0090"><figref idref="DRAWINGS">FIG. <b>17</b></figref> shows an example of performing projector stacking in a real environment shared by users A and B. In the figure, it is assumed that the real environment is made up of a wall surface <b>1700</b> of a room on which a VR image is projected, and the virtual environment shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref> is projected by projector stacking.</p><p id="p-0089" num="0091">First, when the information on the position, posture, and line of sight of users A and B is acquired and an overlapping area <b>1703</b> in which the view range <b>1701</b> of user A and the view range <b>1702</b> of user B overlap is specified, an object included in the overlapping area is classified into the foreground and the background. For example, the motion parallax of each object in the overlapping area <b>1703</b> may be calculated based on the position, posture, and line of sight of users A and B for each frame, an object having a small change due to the motion parallax may be classified as the background, and an object in which a change due to the motion parallax exceeds a predetermined value may be classified as the foreground. Here, an object in which the change due to motion parallax is small for one user but the change due to motion parallax is large for the other user may be classified as the foreground. In the example shown in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, it is assumed that the tree <b>1711</b> in a distance is classified as the background and the mailbox <b>1712</b> in the front is classified as the foreground.</p><p id="p-0090" num="0092">Next, as shown in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, a background image <b>1800</b> composed of an object <b>1711</b> classified as the background is generated. The background image <b>1800</b> is an image that is common to users A and B because the change due to the motion parallax is small. A foreground image <b>1702</b> for user A and a foreground image <b>1703</b> for user B are generated, respectively. Specifically, as shown in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, the information on the position, posture, and line of sight of user A is acquired for each frame to generate a foreground image <b>1900</b> for user A such that the object <b>1712</b> classified as the foreground maintains the effect of motion parallax appropriate for user A. Similarly, as shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the information on the position, posture, and line of sight of user B is acquired for each frame to generate a foreground image <b>2000</b> for user B such that the object <b>1712</b> classified as the foreground maintains the effect of motion parallax appropriate for user B.</p><p id="p-0091" num="0093">Then, the background object <b>1800</b> common to users A and B is constantly projected and displayed on the wall surface <b>1700</b> of the room using the first projector <b>2100</b> for background image projector as shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref>. Further, when the foreground image <b>1900</b> for user A is superimposed on the background image <b>1800</b> using a second projector <b>2200</b> for foreground image projection, a projector-stacked projection image <b>2201</b> as shown in <figref idref="DRAWINGS">FIG. <b>22</b></figref> is realized. The projection image <b>2201</b> is an image that maintains the effect of motion parallax that is appropriate for user A.</p><p id="p-0092" num="0094">Further, when the foreground image <b>2000</b> for user B is superimposed on the background image <b>1800</b> using the second projector <b>2200</b> for foreground image projection, a projector-stacked projection image <b>2300</b> as shown in <figref idref="DRAWINGS">FIG. <b>23</b></figref> is realized. The projection image <b>2301</b> is an image that maintains the effect of motion parallax that is appropriate for user B.</p><p id="p-0093" num="0095">Therefore, the background image <b>1800</b> common to users A and B is constantly projected and displayed on the wall surface <b>1700</b> of the room using the first projector <b>2100</b> for background image projection. The foreground image <b>1900</b> or <b>2000</b>, which is different for users A and B, is projected on the wall surface <b>1700</b> of the room in a time-division manner using the second projector <b>2200</b> for foreground image projection, and is alternately superimposed and displayed on the background image <b>1800</b>.</p><p id="p-0094" num="0096">In <figref idref="DRAWINGS">FIGS. <b>21</b> to <b>23</b></figref>, for the sake of simplicity, only one first projector <b>2100</b> and one second projector <b>2200</b> are depicted, but in the case of displaying an image over a wide angle, the first projector <b>2100</b> and the second projector <b>2200</b> may each be composed of a plurality of projectors.</p><p id="p-0095" num="0097"><figref idref="DRAWINGS">FIG. <b>24</b></figref> shows an operation example of the first projector <b>2100</b>. <figref idref="DRAWINGS">FIG. <b>25</b></figref> shows an operation example of the second projector <b>2200</b>. As shown in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the first projector <b>2100</b> constantly projects and displays the background image <b>1800</b> generated for each frame on the wall surface <b>1700</b> of the room. On the other hand, as shown in <figref idref="DRAWINGS">FIG. <b>25</b></figref>, the second projector <b>1712</b> divides the frame period T into two segments, and an image to be projected on the wall surface <b>1700</b> is sequentially switched between a segment of 0 to T/2 and a segment of T/2 to T for each frame period. In the segment of 0 to T/2 in the first half of the frame period T, the foreground image <b>1900</b> that maintains the effect of motion parallax corresponding to the position, posture, and line of sight of user A is projected. In the segment of T/2 to T in the latter half of the frame period T, the foreground image <b>2000</b> that maintains the effect of motion parallax corresponding to the position, posture, and line of sight of user B is projected.</p><p id="p-0096" num="0098">Users A and B observe the real environment by wearing shutter glasses that open and close in synchronization with switching to the foreground image <b>1900</b> or <b>2000</b> for himself/herself. As a result, users A and B can view the VR image in which the optimum foreground image is superimposed on the background image common to the users. The optimum foreground image is a foreground image that maintains the effect of motion parallax for each user, and users A and B can have more perfect VR experience.</p><p id="p-0097" num="0099">In <figref idref="DRAWINGS">FIGS. <b>17</b> to <b>25</b></figref>, an example in which the original content is separated into two types, a background common to users and a foreground for each user, and the background image and the foreground image are superimposed and displayed using two projectors is shown. As a modification, a method of separating the original content into three types of background, middle scene, and foreground can be considered. In this case, not only the foreground but also the middle scene has different effects due to the motion parallax for each user, and the effect due to the motion parallax differs from the foreground to the middle scene. Thus, a middle scene image for each user is generated similarly to the foreground image. In the real environment, in addition to the first projector for background image projection and the second projector for foreground image projection, a third projector for middle scene image projection is installed. Then, the third projector projects and displays the middle scene image that maintains the effect of motion parallax for each user in a time-division manner in synchronization with the switching of the foreground image. Therefore, each user observes the real environment by wearing shutter glasses that open and close in synchronization with the switching of the foreground image and the middle scene image for himself/herself. In this way, each user can observe the VR image that maintains the effect of motion parallax corresponding to the current position, posture, and line of sight of the user and have more perfect VR experience.</p><p id="p-0098" num="0100">It is also conceivable that the projection image is a moving image and an object in the image moves. For example, when the subject is a moving object such as an automobile, the relative position with respect to an observing user changes from time to time. Thus, it is conceivable that the background switches to the foreground or the foreground switches to the background with the passage of time. The change due to the motion parallax of an automobile traveling in the background is small, but the change due to the motion parallax of the automobile traveling in the foreground is large.</p><p id="p-0099" num="0101"><figref idref="DRAWINGS">FIG. <b>26</b></figref> illustrates an image of an automobile <b>2601</b> traveling from a distance toward the front in an area where the fields of view of users A and B overlap. At this time point, the automobile <b>2601</b> belongs to the background, and the motion parallax of the automobile <b>2601</b> is small for users A and B. Therefore, the image shown in <figref idref="DRAWINGS">FIG. <b>26</b></figref> may be constantly projected and displayed using the first projector for background image projection.</p><p id="p-0100" num="0102"><figref idref="DRAWINGS">FIG. <b>27</b></figref> illustrates an image in which the automobile <b>2601</b> continues to travel toward the front and reaches the foreground area. At this time point, the motion parallax of the automobile <b>2601</b> is large for users A and B. Therefore, the motion parallax of the automobile <b>2601</b> is calculated based on the position, posture, and line of sight of user A for each frame, and the foreground image for user A including the automobile <b>2601</b> is generated. Then, the foreground image including the automobile <b>2601</b> is superimposed and projected on the background image projected by the first projector <b>2801</b> for background image projection using the second projector <b>2802</b> for foreground image projection. In this way, it is possible to display a projection image that maintains the effect of motion parallax that is appropriate for user A as shown in <figref idref="DRAWINGS">FIG. <b>28</b></figref>.</p><p id="p-0101" num="0103">Similarly, the motion parallax of the automobile <b>2601</b> is calculated based on the position, posture, and line of sight of user B for each frame, and a foreground image for user B including the automobile <b>2601</b> is generated. Then, the foreground image including the automobile <b>2601</b> is superimposed and projected on the background image projected by the first projector <b>2801</b> for background image projection using the second projector <b>2802</b> for foreground image projection. In this way, it is possible to display a projection image that maintains the effect of motion parallax that is appropriate for user B as shown in <figref idref="DRAWINGS">FIG. <b>29</b></figref>.</p><p id="p-0102" num="0104">Therefore, the background image common to users A and B is constantly projected and displayed using the first projector <b>2801</b>. Using the second projector <b>2802</b>, the foreground images different for users A and B are projected in a time-division manner and alternately superimposed and displayed on the background image. <figref idref="DRAWINGS">FIG. <b>30</b></figref> shows an operation example of the second projector <b>2802</b> that projects the foreground image. The operation of the first projector <b>2801</b> is not shown, but the first projector <b>2801</b> constantly projects and displays the background image. In <figref idref="DRAWINGS">FIG. <b>30</b></figref>, for convenience, the timing at which the automobile <b>2601</b> reaches the foreground is set to time 0. Then, the second projector <b>2802</b> sequentially switches the image to be superimposed and projected on the background image in the real environment between the segment of 0 to T/2 and the segment of T/2 to T for each frame period. In the segment of 0 to T/2 in the first half of the frame period T, an image of the automobile <b>2601</b> (see <figref idref="DRAWINGS">FIG. <b>28</b></figref>) that maintains the effect of motion parallax corresponding to the position, posture, and line of sight of user A is projected. In the segment of T/2 to T in the latter half of the frame period T, an image of the automobile <b>2601</b> (see <figref idref="DRAWINGS">FIG. <b>29</b></figref>) that maintains the effect of motion parallax corresponding to the position, posture, and line of sight of user B is projected.</p><p id="p-0103" num="0105">Users A and B observe the real environment by wearing shutter glasses that open and close in synchronization with switching to the foreground image for himself/herself. As a result, users A and B can view the VR image in which the foreground image of the automobile <b>2601</b> optimum for each user is superimposed on the background image common to the users. The optimum foreground image is a foreground image that maintains the effect of motion parallax for each user, and users A and B can have more perfect VR experience.</p><p id="p-0104" num="0106">In <figref idref="DRAWINGS">FIGS. <b>28</b> and <b>29</b></figref>, for the sake of simplicity, only one first projector <b>2801</b> and one second projector <b>2802</b> are depicted, but in the case of displaying an image over a wide angle, the first projector <b>2801</b> and the second projector <b>2802</b> may each be composed of a plurality of projectors.</p><p id="p-0105" num="0107">Hereinabove, an example of performing projector stacking to display a foreground image in which the effect of motion parallax is given for each user has been described. As a modification, projector stacking may be used to superimpose an individual foreground image prepared for each user on a background image common to users even if there is no effect of motion parallax.</p><p id="p-0106" num="0108"><figref idref="DRAWINGS">FIG. <b>31</b></figref> shows a display example of projector stacking in which individual foreground images <b>3101</b>, <b>3102</b>, and <b>3103</b> prepared for each user are superimposed on a background image <b>3100</b> common to users A, B, and C in a time-division manner. In the shown example, the background image <b>3100</b> is an image in which a speaker who is on the stage gives a presentation, and the foreground images <b>3101</b>, <b>3102</b>, and <b>3103</b> are the face images of the speaker prepared for each user. <figref idref="DRAWINGS">FIG. <b>32</b></figref> shows a state in which the face image <b>3101</b> for user A is superimposed on the face of the speaker in the background image <b>3100</b>. <figref idref="DRAWINGS">FIG. <b>33</b></figref> shows a state in which the face image <b>3102</b> for user B is superimposed on the face of the speaker in the background image <b>3100</b>, and <figref idref="DRAWINGS">FIG. <b>34</b></figref> shows a state in which the face image <b>3103</b> for user C is superimposed on the face of the speaker in the background image <b>3100</b>. The face image to be prepared for each of users A, B, and C is determined based on a predetermined logic such as the preference and compatibility of each user, but the details thereof will be omitted.</p><p id="p-0107" num="0109">The background image <b>3100</b> is constantly projected and displayed using the first projector <b>3111</b>. Further, using the second projector <b>3112</b>, the individual foreground images <b>3101</b>, <b>3102</b>, and <b>3103</b> prepared for each user are superimposed and projected on the background image <b>3100</b> in a time-division manner. <figref idref="DRAWINGS">FIG. <b>35</b></figref> shows an operation example of the second projector <b>3112</b>. The second projector <b>3112</b> divides the frame period T into three segments, and sequentially switches an image to be projected between the segment of 0 to T/3, the segment of T/3 to 2 T/3, and the segment of 2 T/3 to T for each frame period. In the first segment of 0 to T/3 of the frame period T, the second projector <b>3112</b> superimposes and projects the foreground image <b>3101</b> for user Aon the background image <b>3100</b>. In the subsequent segment of T/3 to 2 T/3, the second projector <b>3112</b> superimposes and projects the foreground image <b>3102</b> for user B on the background image <b>3100</b>. In the subsequent segment of 2 T/3 to T, the second projector <b>3112</b> superimposes and projects the foreground image <b>3103</b> for user C on the background image <b>3100</b>.</p><p id="p-0108" num="0110">Users A to C observe the real environment by wearing shutter glasses that open and close in synchronization with switching to the foreground images <b>3101</b> to <b>3103</b> for himself/herself. As a result, users A to C can view the VR image in which the optimum foreground image is superimposed on the background image common to the users.</p><p id="p-0109" num="0111">In <figref idref="DRAWINGS">FIGS. <b>31</b> to <b>34</b></figref>, for the sake of simplicity, only one first projector <b>3111</b> and one second projector <b>3112</b> are depicted, but in the case of displaying an image over a wide angle, the first projector <b>3111</b> and the second projector <b>3112</b> may each be composed of a plurality of projectors.</p><p id="p-0110" num="0112"><figref idref="DRAWINGS">FIG. <b>36</b></figref> shows a functional configuration example of an image processing device <b>3600</b> that performs processing for realizing projector stacking. The shown image processing device <b>3600</b> includes a content receiving unit <b>3601</b>, a user information acquisition unit <b>3602</b>, a view estimation unit <b>3603</b>, an overlapping area determination unit <b>3604</b>, a transmitting unit <b>3605</b>, and a content processing unit <b>3610</b>. The content processing unit <b>3610</b> includes an object classification unit <b>3611</b>, a common image generation unit <b>3612</b>, an individual image generation unit <b>3613</b>, and an output protocol determination unit <b>3614</b>. The image processing device <b>3600</b> can be configured using an information processing device such as a personal computer (PC).</p><p id="p-0111" num="0113">The content receiving unit <b>3601</b> receives stream data of content from a content source such as a stream distribution server on the cloud or recording media. The content receiving unit <b>3601</b> includes, for example, a communication interface such as Wi-Fi (registered trademark), Ethernet (registered trademark), HDMI (registered trademark) (High-Definition Multimedia Interface).</p><p id="p-0112" num="0114">The user information acquisition unit <b>3602</b> performs a process of acquiring information of each user who observes a projection image to be subject to projection mapping or projector stacking by the image processing device <b>3600</b>. The user information acquisition unit <b>3602</b> acquires, for example, information such as a position, a posture, and a field of view for specifying the view of each user. The user information acquisition unit <b>3602</b> may acquire profile information and sensitive information of each user in order to assign an individual foreground image to each user. The user information acquisition unit <b>3602</b> may be equipped with one or more sensor devices in order to acquire the above-described user information.</p><p id="p-0113" num="0115">The view estimation unit <b>3603</b> estimates the viewing area of each user based on the user information (for example, information such as the position, posture, and field of view of each user) acquired by the user information acquisition unit <b>3602</b>.</p><p id="p-0114" num="0116">The overlapping area determination unit <b>3604</b> determines whether or not an overlapping area in which the fields of view of the users overlap is present based on the viewing area of each user estimated by the view estimation unit <b>3603</b>, and further identifies the range of the overlapping area.</p><p id="p-0115" num="0117">When the overlapping area determination unit <b>3604</b> determines that the fields of view of users overlap, the object classification unit <b>3611</b> classifies objects included in the overlapping area among the objects (subjects) included in the original content received by the content receiving unit <b>3601</b> into a first object group and a second object group. The first object group is a group of objects that are present in the background far away from the user's position and have a small motion parallax between users. On the other hand, the second object group is a group of objects that are present in the foreground close to the user's position and have a large motion parallax between users. The determination whether an object belongs to the background or foreground is performed for each frame.</p><p id="p-0116" num="0118">The common image generation unit <b>3612</b> generates a background image which is common to users and is composed of objects of the first object group present in the background from the original content in the overlapping area where the fields of view of the users overlap. The background image generated by the common image generation unit <b>3612</b> is output to the first projector <b>3620</b> via the transmitting unit <b>3605</b>.</p><p id="p-0117" num="0119">The individual image generation unit <b>3613</b> generates a foreground image which is different for each user and is composed of objects of the second object group present in the foreground from the original content in the overlapping area where the fields of view of the users overlap. The individual image generation unit <b>3613</b> generates an individual image corresponding to the field of view of each user from the original content when the fields of view of the users do not overlap. The individual image for each user generated by the individual image generation unit <b>3613</b> is output to the second projector <b>3630</b> via the transmitting unit <b>3605</b>.</p><p id="p-0118" num="0120">The first projector <b>3620</b> projects the background image common to users generated by the common image generation unit <b>3612</b> on a real environment such as a wall surface of a room. The second projector <b>3630</b> projects the individual image for each user generated by the individual image generation unit <b>3613</b> on a real environment such as a wall surface of a room. When the individual images of each user overlap, the second projector <b>3630</b> switches and outputs the individual images in a time-division manner according to the output protocol determined by the output protocol determination unit <b>3614</b>.</p><p id="p-0119" num="0121">Both the first projector <b>3620</b> and the second projector <b>3630</b> measure the three-dimensional shape of a projection surface in a real environment such as the wall surface of a room, and perform projection mapping of projecting a distortion-free image by correcting the projection image according to the measurement result.</p><p id="p-0120" num="0122">The output protocol determination unit <b>3614</b> determines a protocol for outputting the individual image generated for each user by the individual image generation unit <b>3613</b> to the second projector <b>3630</b>. The output protocol determination unit <b>3614</b> determines an output protocol for projector-stacking the individual images for each user generated by the individual image generation unit <b>3613</b> on the common image common to the users generated by the common image generation unit <b>3612</b>. The output protocol determination unit <b>3614</b> determines a protocol for switching and outputting the individual images in a time-division manner when projecting a plurality of overlapping individual images from the second projector <b>3630</b>. The output protocol determination unit <b>3614</b> outputs the determined output protocol to the second projector <b>3630</b> and the shutter glasses (not shown) of each user via the transmitting unit <b>3605</b>.</p><p id="p-0121" num="0123">The transmitting unit <b>3605</b> is provided with a communication interface such as Wi-Fi (registered trademark), Ethernet (registered trademark), and HDMI (registered trademark).</p><p id="p-0122" num="0124"><figref idref="DRAWINGS">FIG. <b>37</b></figref> shows a processing procedure in which the image processing device <b>3600</b> shown in <figref idref="DRAWINGS">FIG. <b>36</b></figref> performs projection mapping or projector stacking in the form of a flowchart. This processing procedure is activated, for example, when the image processing device <b>3600</b> receives the content to be subject to projection mapping or projector stacking via the content receiving unit <b>3601</b>.</p><p id="p-0123" num="0125">The user information acquisition unit <b>3602</b> performs a process of acquiring information of each user who observes a projection image to be subject to projection mapping or projector stacking by the image processing device <b>3600</b> (step S<b>3701</b>). In this example, it is assumed that the user information acquisition unit <b>3602</b> acquires information such as a position, a posture, and a field of view for specifying the view of each user.</p><p id="p-0124" num="0126">Next, the view estimation unit <b>3603</b> estimates the viewing area of each user based on the information such as the position, posture, and field of view of each user acquired by the user information acquisition unit <b>3602</b> (step S<b>3702</b>).</p><p id="p-0125" num="0127">The overlapping area determination unit <b>3604</b> determines whether or not an overlapping area in which the fields of view of the users overlap is present based on the viewing area of each user estimated by the view estimation unit <b>3603</b> (step S<b>3703</b>). Then, when the overlapping area determination unit <b>3604</b> determines that there is an overlapping area, the overlapping area determination unit <b>3604</b> further specifies the range of the overlapping area.</p><p id="p-0126" num="0128">When the overlapping area determination unit <b>3604</b> determines that the fields of view of the users overlap (Yes in step S<b>3703</b>), the object classification unit <b>3611</b> classifies objects included in the overlapping area among the objects (subjects) included in the original content received by the content receiving unit <b>3601</b> into a first object group and a second object group (step S<b>3705</b>). The determination whether an object belongs to the background or foreground is performed for each frame.</p><p id="p-0127" num="0129">Then, the common image generation unit <b>3612</b> generates a background image which is common to users and is composed of objects of the first object group present in the background from the original content in the overlapping area where the fields of view of the users overlap. The individual image generation unit <b>3613</b> generates a foreground image which is different for each user and is composed of objects of the second object group present in the foreground from the original content in the overlapping area where the fields of view of the users overlap (step S<b>3705</b>).</p><p id="p-0128" num="0130">On the other hand, when the overlapping area determination unit <b>3604</b> determines that the fields of view of the users do not overlap (No in step S<b>3703</b>), the individual image generation unit <b>3613</b> generates an individual image corresponding to the field of view of each user from the original content (step S<b>3708</b>). At this time, the object classification process by the object classification unit <b>3611</b> is not executed. Further, since the overlapping area is not present, the common image generation unit <b>3612</b> does not perform the process of generating a common image in the overlapping area.</p><p id="p-0129" num="0131">Next, the output protocol determination unit <b>3614</b> determines a protocol for outputting the individual image generated for each user by the individual image generation unit <b>3613</b> to the second projector <b>3630</b> (step S<b>3706</b>).</p><p id="p-0130" num="0132">Then, the background image generated by the common image generation unit <b>3612</b> is output to the first projector <b>3620</b> via the transmitting unit <b>3605</b>. The individual image for each user generated by the individual image generation unit <b>3613</b> is output to the second projector <b>3630</b> via the transmitting unit <b>3605</b>. The output protocol determination unit <b>3614</b> outputs the determined output protocol to the second projector <b>3630</b> and the shutter glasses (not shown) of each user via the transmitting unit <b>3605</b> (step S<b>3707</b>).</p><p id="p-0131" num="0133">The first projector <b>3620</b> projects the background image common to users generated by the common image generation unit <b>3612</b> on a real environment such as a wall surface of a room. The second projector <b>3630</b> projects the individual image for each user generated by the individual image generation unit <b>3613</b> on a real environment such as a wall surface of a room. When the individual images of each user overlap, the second projector <b>3630</b> switches and outputs the individual images in a time-division manner according to the output protocol determined by the output protocol determination unit <b>3614</b>.</p><p id="p-0132" num="0134">By performing projection mapping and projector stacking on the images processed by the image processing device <b>3600</b> using the first projector <b>3620</b> and the second projector <b>3630</b> in this way, even when the fields of view of the users overlap, each user can have perfect VR experience without being disturbed by the VR images of other users. At that time, the user does not need to wear a heavy object such as a head-mounted display, and only needs to wear lightweight shutter glasses.</p><heading id="h-0013" level="1">INDUSTRIAL APPLICABILITY</heading><p id="p-0133" num="0135">The technology according to the present disclosure has been described in detail above with reference to a specific embodiment. However, it will be apparent to those skilled in the art that modifications, substitutions, and the like can be made in the embodiments without departing from the gist of the technology according to the present disclosure.</p><p id="p-0134" num="0136">The technology according to the present disclosure can be applied when presenting VR content to a plurality of users in various real environments.</p><p id="p-0135" num="0137">In short, the technology according to the present disclosure has been described in the form of an example, and the contents of the present specification should not be construed in a limited manner. The gist of the technology according to the present disclosure should be determined in consideration of the claims.</p><p id="p-0136" num="0138">The technology disclosed in the present disclosure may also be configured as follows.</p><p id="p-0137" num="0139">(1) An image processing device that executes processing on content to be displayed to a plurality of users, including: a user information acquisition unit that acquires information on each user; a classification unit that specifies an overlapping area in which fields of view of two or more users overlap based on the information on each user and classifies objects included in the overlapping area into a first object group and a second object group; a generation unit that generates a common image common to all users, made up of the first object group and generates individual images different for each user, made up of the second object group; and a determination unit that determines an output protocol for displaying the individual images.</p><p id="p-0138" num="0140">(2) The image processing device according to (1), wherein the classification unit classifies the objects included in the overlapping area into the first object group present in a background and the second object group which is a foreground for at least some users, and the generation unit generates a background image including the first object group as a common image and generates a foreground image for each user including the second object group as the individual image.</p><p id="p-0139" num="0141">(3) The image processing device according to (2), wherein the generation unit generates an individual foreground image in which each object included in the second object group has an effect of motion parallax for each user.</p><p id="p-0140" num="0142">(4) The image processing device according to (2), wherein the generation unit generates an individual image including an object assigned to each user in the second object group.</p><p id="p-0141" num="0143">(5) The image processing device according to any one of (1) to (4), wherein the user information acquisition unit acquires information for specifying a field of view of each user.</p><p id="p-0142" num="0144">(6) The image processing device according to any one of (1) to (5), wherein the user information acquisition unit acquires information on a position and a line of sight of each user, and the classification unit performs the classification based on a distance between each user and each object.</p><p id="p-0143" num="0145">(7) The image processing device according to any one of (1) to (6), wherein the common image is output to a first display device, and the individual image for each user is output to a second display device.</p><p id="p-0144" num="0146">(8) The image processing device according to any one of (1) to (7), further including at least one of a first display device that outputs the common image and a second display device that outputs the individual image for each user.</p><p id="p-0145" num="0147">(9) The image processing device according to (7) or (8), wherein at least one of the first display device and the second display device is a projector.</p><p id="p-0146" num="0148">(10) The image processing device according to any one of (1) to (9), wherein the determination unit determines an output protocol related to a timing for outputting the individual images in a time-division manner.</p><p id="p-0147" num="0149">(11) The image processing device according to any one of (1) to (10), further including a communication unit for notifying a device used by each user of the output protocol.</p><p id="p-0148" num="0150">(12) An image processing method of executing processing on content to be displayed to a plurality of users, including: a user information acquisition step of acquiring information on each user; a classification step of specifying an overlapping area in which fields of view of two or more users overlap based on the information on each user and classifying objects included in the overlapping area into a first object group and a second object group; a generation step of generating a common image common to all users, made up of the first object group and generating individual images different for each user, made up of the second object group; and a determination step of determining an output protocol for displaying the individual images.</p><p id="p-0149" num="0151">(13) An image display system including: an image processing device that specifies an overlapping area in which fields of view of two or more users overlap based on information on each user, classifies objects included in the overlapping area into a first object group and a second object group, generates a common image common to all users, made up of the first object group, generates individual images different for each user, made up of the second object group, and notifies shutter glasses used by each user of an output protocol for displaying the individual images; a first display device that outputs the common image; a second display device that outputs the individual images of each user; and the shutter glasses used by each user.</p><heading id="h-0014" level="1">REFERENCE SIGNS LIST</heading><p id="p-0150" num="0000"><ul id="ul0004" list-style="none">    <li id="ul0004-0001" num="0152"><b>3600</b> Image processing device</li>    <li id="ul0004-0002" num="0153"><b>3601</b> Content receiving unit</li>    <li id="ul0004-0003" num="0154"><b>3602</b> User information acquisition unit</li>    <li id="ul0004-0004" num="0155"><b>3603</b> View estimation unit</li>    <li id="ul0004-0005" num="0156"><b>3604</b> Overlapping area determination unit</li>    <li id="ul0004-0006" num="0157"><b>3605</b> Transmitting unit</li>    <li id="ul0004-0007" num="0158"><b>3610</b> Content processing unit</li>    <li id="ul0004-0008" num="0159"><b>3611</b> Object classification unit</li>    <li id="ul0004-0009" num="0160"><b>3612</b> Common image generation unit</li>    <li id="ul0004-0010" num="0161"><b>3613</b> Individual image generation unit</li>    <li id="ul0004-0011" num="0162"><b>3614</b> Output protocol determination unit</li>    <li id="ul0004-0012" num="0163"><b>3620</b> First projector</li>    <li id="ul0004-0013" num="0164"><b>3630</b> Second projector</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image processing device that executes processing on content to be displayed to a plurality of users, the device comprising:<claim-text>a user information acquisition unit that acquires information on each user;</claim-text><claim-text>a classification unit that specifies an overlapping area in which fields of view of two or more users overlap based on the information on each user and classifies objects included in the overlapping area into a first object group and a second object group;</claim-text><claim-text>a generation unit that generates a common image common to all users, made up of the first object group and generates individual images different for each user, made up of the second object group; and</claim-text><claim-text>a determination unit that determines an output protocol for displaying the individual images.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the classification unit classifies the objects included in the overlapping area into the first object group present in a background and the second object group which is a foreground for at least some users, and</claim-text><claim-text>the generation unit generates a background image including the first object group as a common image and generates a foreground image for each user including the second object group as the individual image.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image processing device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the generation unit generates an individual foreground image in which each object included in the second object group has an effect of motion parallax for each user.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image processing device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the generation unit generates an individual image including an object assigned to each user in the second object group.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the user information acquisition unit acquires information for specifying a field of view of each user.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the user information acquisition unit acquires information on a position and a line of sight of each user, and</claim-text><claim-text>the classification unit performs the classification based on a distance between each user and each object.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the common image is output to a first display device, and the individual image for each user is output to a second display device.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising at least one of a first display device that outputs the common image and a second display device that outputs the individual image for each user.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The image processing device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>at least one of the first display device and the second display device is a projector.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the determination unit determines an output protocol related to a timing for outputting the individual images in a time-division manner.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a communication unit for notifying a device used by each user of the output protocol.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. An image processing method of executing processing on content to be displayed to a plurality of users, the method comprising:<claim-text>a user information acquisition step of acquiring information on each user;</claim-text><claim-text>a classification step of specifying an overlapping area in which fields of view of two or more users overlap based on the information on each user and classifying objects included in the overlapping area into a first object group and a second object group;</claim-text><claim-text>a generation step of generating a common image common to all users, made up of the first object group and generating individual images different for each user, made up of the second object group; and</claim-text><claim-text>a determination step of determining an output protocol for displaying the individual images.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An image display system comprising:<claim-text>an image processing device that specifies an overlapping area in which fields of view of two or more users overlap based on information on each user, classifies objects included in the overlapping area into a first object group and a second object group, generates a common image common to all users, made up of the first object group, generates individual images different for each user, made up of the second object group, and notifies shutter glasses used by each user of an output protocol for displaying the individual images;</claim-text><claim-text>a first display device that outputs the common image;</claim-text><claim-text>a second display device that outputs the individual images of each user; and</claim-text><claim-text>the shutter glasses used by each user.</claim-text></claim-text></claim></claims></us-patent-application>