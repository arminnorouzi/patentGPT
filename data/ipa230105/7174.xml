<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007175A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007175</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941000</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>521</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23245</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>521</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>2213</main-group><subgroup>003</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">CONCURRENT RGBZ SENSOR AND SYSTEM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>14856552</doc-number><date>20150916</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17941000</doc-number></document-id></child-doc></relation></continuation><continuation-in-part><relation><parent-doc><document-id><country>US</country><doc-number>14842822</doc-number><date>20150901</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10145678</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>14856552</doc-number></document-id></child-doc></relation></continuation-in-part><us-provisional-application><document-id><country>US</country><doc-number>62150252</doc-number><date>20150420</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>62182404</doc-number><date>20150619</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>62203386</doc-number><date>20150810</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Samsung Electronics Co., Ltd.</orgname><address><city>Suwon-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>OVSIANNIKOV</last-name><first-name>Ilia</first-name><address><city>Studio City</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Yibing Michelle</first-name><address><city>Temple City</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>WALIGORSKI</last-name><first-name>Gregory</first-name><address><city>Pasadena</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Qiang</first-name><address><city>Pasadena</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Two-dimensional (2D) color information and 3D-depth information are concurrently obtained from a 2D pixel array. The 2D pixel array is arranged in a first group of a plurality of rows. A second group of rows of the array are operable to generate 2D-color information and pixels of a third group of the array are operable to generate 3D-depth information. The first group of rows comprises a first number of rows, the second group of rows comprises a second number of rows that is equal to or less than the first number of rows, and the third group of rows comprises a third number of rows that is equal to or less than the second number of rows. In an alternating manner, 2D-color information is received from a row selected from the second group of rows and 3D-depth information is received from a row selected from the third group of rows.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="152.40mm" wi="158.75mm" file="US20230007175A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="224.11mm" wi="162.98mm" file="US20230007175A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="219.71mm" wi="142.58mm" file="US20230007175A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="223.18mm" wi="161.97mm" orientation="landscape" file="US20230007175A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="221.66mm" wi="162.81mm" orientation="landscape" file="US20230007175A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="222.25mm" wi="161.97mm" file="US20230007175A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="189.31mm" wi="162.64mm" file="US20230007175A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="224.79mm" wi="163.58mm" file="US20230007175A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="163.91mm" wi="162.22mm" file="US20230007175A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="177.63mm" wi="161.80mm" file="US20230007175A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="220.98mm" wi="132.33mm" file="US20230007175A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="246.63mm" wi="132.08mm" file="US20230007175A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="117.18mm" wi="109.14mm" file="US20230007175A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This is a continuation of U.S. patent application Ser. No. 14/856,552, filed Sep. 16, 2015, which is a continuation-in-part patent application of U.S. patent application Ser. No. 14/842,822 filed Sep. 1, 2015, now U.S. Pat. No. 10,145,678, issued Dec. 4, 2018, and claims the priority benefit under 35 U.S.C. &#xa7; 119(e) of U.S. Provisional Patent Application Ser. No. 62/150,252 filed Apr. 20, 2015, U.S. Provisional Patent Application Ser. No. 62/182,404 filed Jun. 19, 2015, and U.S. Provisional Patent Application Ser. No. 62/203,386 filed Aug. 10, 2015, and the disclosures of each are incorporated herein by reference in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure generally relates to image sensors. More specifically, and not by way of limitation, particular embodiments disclosed herein are directed to a triangulation-based system and method of depth measurements on a three-dimensional (3D) object using a laser point scan and a Complementary Metal Oxide Semiconductor (CMOS) image sensor, which is also used for two-dimensional (2D) imaging of the 3D object.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Three-dimensional (3D) imaging systems are increasingly being used in a wide variety of applications, such as industrial production, video games, computer graphics, robotic surgeries, consumer displays, surveillance videos, 3D modeling, real estate sales, and so on.</p><p id="p-0005" num="0004">Existing 3D-imaging technologies may include, for example, the time-of-flight (TOF) based range imaging, stereo vision systems, and structured light (SL) methods.</p><p id="p-0006" num="0005">In the TOF method, distance to a 3D object is resolved based on the known speed of light&#x2014;by measuring the round-trip time it takes for a light signal to travel between a camera and the 3D object for each point of the image. A TOF camera may use a scannerless approach to capture the entire scene with each laser or light pulse. Some exemplary applications of the TOF method may include advanced automotive applications, such as active pedestrian safety or pre-crash detection based on distance images in real time; to track movements of humans such as during interaction with games on video-game consoles; in industrial machine vision to classify objects and help robots find the items, such as items on a conveyor belt, and so on.</p><p id="p-0007" num="0006">In stereoscopic-imaging or stereo-vision systems, two cameras&#x2014;displaced horizontally from one another&#x2014;are used to obtain two differing views on a scene or a 3D object in the scene. By comparing these two images, the relative depth information can be obtained for the 3D object. Stereo vision is highly important in fields, such as robotics, to extract information about the relative position of 3D objects in the vicinity of autonomous systems/robots. Other applications for robotics include object recognition in which stereoscopic depth information allows a robotic system to separate occluding image components, which the robot may otherwise not be able to distinguish as two separate objects&#x2014;such as one object in front of another, partially or fully hiding the other object. 3D stereo displays are also used in entertainment and automated systems.</p><p id="p-0008" num="0007">In the SL approach, the 3D shape of an object may be measured using projected light patterns and a camera for imaging. In the SL method, a known pattern of light&#x2014;often grids or horizontal bars or patterns of parallel stripes&#x2014;is projected onto a scene or a 3D object in the scene. The projected pattern may become deformed or displaced when striking the surface of the 3D object. Such deformation may allow an SL vision system to calculate the depth and surface information of the object. Thus, projecting a narrow band of light onto a 3D surface may produce a line of illumination that may appear distorted from other perspectives than that of the projector, and can be used for geometric reconstruction of the illuminated surface shape. The SL-based 3D imaging maybe used in different applications such as, by a police force to photograph fingerprints in a 3D scene, inline inspection of components during a production process, in health care for live measurements of human body shapes or the micro structures of human skin, and the like.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0009" num="0008">One exemplary embodiment provides a method, comprising: receiving an image of at least one object at an image sensor, the image sensor comprising a two-dimensional (2D) pixel array arranged in a first group of a plurality of rows, pixels of a second group of rows of the array being operable to generate 2D-color information of the at least one object, pixels of a third group of the array being operable to generate 3D-depth information of the at least one object, the first group of rows comprising a first number of rows, the second group of rows comprising a second number of rows that is equal to or less than the first number of rows, and the third group of rows comprising a third number of rows that is equal to or less than the second number of rows; and receiving in an alternatingly manner 2D-color information of the at least one object from a row selected from the second group of rows and receiving 3D-depth information of the at least one object from a row selected from the third group of rows.</p><p id="p-0010" num="0009">One exemplary embodiment provides an image sensor unit, comprising a two-dimensional (2D) pixel array, and a controller. The 2D pixel array is arranged in a first group of a plurality of rows. Pixels of a second group of rows of the array are operable to generate 2D-color information based on an image of at least one object received by the 2D pixel array, and pixels of a third group of the array are operable to generate 3D-depth information of the at least one object. The first group of rows comprise a first number of rows, the second group of rows comprises a second number of rows that is equal to or less than the first number of rows, and the third group of rows comprises a third number of rows that is equal to or less than the second number of rows. The controller is coupled to the 2D pixel array to select in an alternatingly manner a row from the second group of rows to output the generated 2D-color information based on the image of the at least one object and a row from the third group of rows to output the generated 3D-depth information of the at least one object.</p><p id="p-0011" num="0010">One exemplary embodiment provides a system, comprising a two-dimensional (2D) pixel array, a controller and a display. The 2D pixel array is arranged in a first group of a plurality of rows in which pixels of a second group of rows of the array are operable to generate 2D-color information based on an image of at least one object received by the 2D pixel array and in which pixels of a third group of the array are operable to generate 3D-depth information of the at least one object. The first group of rows comprises a first number of rows, the second group of rows comprises a second number of rows that is equal to or less than the first number of rows, and the third group of rows comprises a third number of rows that is equal to or less than the second number of rows. The controller is coupled to the 2D pixel array to select in an alternating manner a row from the second group of rows to output the generated 2D-color information based on the image of the at least one object and a row from the third group of rows to output the generated 3D-depth information of the at least one object. The display is coupled to the 2D pixel array and the controller, and is operative to display a first image of the at least one object based on the generated 2D-color information and to display a second image of the at least one object based on the generated 3D-depth information.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011">In the following section, the aspects of the subject matter disclosed herein will be described with reference to exemplary embodiments illustrated in the figures, in which:</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a highly simplified, partial configuration of a system according to one embodiment disclosed herein;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an exemplary operational configuration of the system in <figref idref="DRAWINGS">FIG. <b>1</b></figref> according to one embodiment disclosed herein;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an exemplary flowchart showing how 3D-depth measurements may be performed according to one embodiment disclosed herein;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an exemplary illustration of how a point scan may be performed for 3D-depth measurements according to one embodiment disclosed herein;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an exemplary timestamping for scanned light spots according to one embodiment disclosed herein;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows exemplary circuit details of the 2D pixel array and a portion of the associated processing circuits in the image-processing unit of the image sensor in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> according to one embodiment disclosed herein;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is an exemplary configuration of an image-sensor unit according to one embodiment disclosed herein;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> shows architectural details of an exemplary CDS+ADC unit for 3D-depth measurement according to one embodiment disclosed herein;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a timing diagram that shows exemplary timing of different signals in the system of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> to generate timestamp-based pixel-specific outputs in a 3D-linear mode of operation according to particular embodiments disclosed herein;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an exemplary Look-Up Table (LUT) to illustrate how an LUT may be used in particular embodiments disclosed herein to determine 3D-depth values;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a timing diagram that shows exemplary timing of different signals in the system of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> to generate a 2D RGB image using a 2D-linear mode of operation according to particular embodiments disclosed herein;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a timing diagram that shows exemplary timing of different signals in the system of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> to generate timestamp-based pixel-specific outputs in a 3D-logarithmic (log) mode of operation according to particular embodiments disclosed herein;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts an overall configuration of the system in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> according to one embodiment disclosed herein;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>13</b></figref> depicts an exemplary flowchart of a process to concurrently generate and obtain 2D-color information and 3D-depth information according to embodiments disclosed herein;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>14</b></figref> depicts an exemplary illustration of how the distance to a semi-transparent object and the distance to an object behind the semi-transparent object may be performed for 3D-depth measurements according to one embodiment disclosed herein;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>15</b></figref> depicts an exemplary illustration of how depth imaging of a semi-transparent medium may be performed for 3D-depth measurements according to one embodiment disclosed herein; and</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>16</b></figref> depicts an exemplary illustration of how depth imaging of an object may be performed for 3D-depth measurements in the presence of multiple return paths according to one embodiment disclosed herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0030" num="0029">In the following detailed description, numerous specific details are set forth in order to provide a thorough understanding of the disclosure. It will be understood, however, by those skilled in the art that the disclosed aspects may be practiced without these specific details. In other instances, well-known methods, procedures, components and circuits have not been described in detail not to obscure the subject matter disclosed herein. Additionally, the described aspects can be implemented to perform low power, 3D-depth measurements in any imaging device or system, including, for example, a smartphone, a User Equipment (UE), a laptop computer, and the like.</p><p id="p-0031" num="0030">Reference throughout this specification to &#x201c;one embodiment&#x201d; or &#x201c;an embodiment&#x201d; means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment disclosed herein. Thus, the appearances of the phrases &#x201c;in one embodiment&#x201d; or &#x201c;in an embodiment&#x201d; or &#x201c;according to one embodiment&#x201d; (or other phrases having similar import) in various places throughout this specification are not necessarily all referring to the same embodiment. As used herein, the word &#x201c;exemplary&#x201d; means &#x201c;serving as an example, instance, or illustration.&#x201d; Any embodiment described herein as &#x201c;exemplary&#x201d; is not to be construed as necessarily preferred or advantageous over other embodiments. Furthermore, the particular features, structures, or characteristics may be combined in any suitable manner in one or more embodiments. Also, depending on the context of discussion herein, a singular term may include the corresponding plural forms and a plural term may include the corresponding singular form. Similarly, a hyphenated term (e.g., &#x201c;two-dimensional,&#x201d; &#x201c;pre-determined,&#x201d; &#x201c;pixel-specific,&#x201d; etc.) may be occasionally interchangeably used with a corresponding non-hyphenated version (e.g., &#x201c;two dimensional,&#x201d; &#x201c;predetermined,&#x201d; &#x201c;pixel specific,&#x201d; etc.), and a capitalized entry (e.g., &#x201c;Counter Clock,&#x201d; &#x201c;Row Select,&#x201d; &#x201c;PIXOUT,&#x201d; etc.) may be interchangeably used with a corresponding non-capitalized version (e.g., &#x201c;counter clock,&#x201d; &#x201c;row select,&#x201d; &#x201c;pixout,&#x201d; etc.). Such occasional interchangeable uses shall not be considered inconsistent with each other.</p><p id="p-0032" num="0031">It is noted at the outset that the terms &#x201c;coupled,&#x201d; &#x201c;operatively coupled,&#x201d; &#x201c;connected,&#x201d; &#x201c;connecting,&#x201d; &#x201c;electrically connected,&#x201d; etc., may be used interchangeably herein to generally refer to the condition of being electrically/electronically connected in an operative manner. Similarly, a first entity is considered to be in &#x201c;communication&#x201d; with a second entity (or entities) if the first entity electrically sends and/or receives (whether through wireline or wireless means) information signals (whether containing address, data, or control information) to/from the second entity regardless of the type (analog or digital) of those signals. It is further noted that various figures (including component diagrams) shown and discussed herein are for illustrative purpose only, and are not drawn to scale. Similarly, various waveforms and timing diagrams are shown for illustrative purpose only.</p><p id="p-0033" num="0032">The terms &#x201c;first,&#x201d; &#x201c;second,&#x201d; etc., as used herein, are used as labels for nouns that they precede, and do not imply any type of ordering (e.g., spatial, temporal, logical, etc.) unless explicitly defined as such. Furthermore, the same reference numerals may be used across two or more figures to refer to parts, components, blocks, circuits, units, or modules having the same or similar functionality. Such usage is, however, for simplicity of illustration and ease of discussion only; it does not imply that the construction or architectural details of such components or units are the same across all embodiments or such commonly-referenced parts/modules are the only way to implement the teachings of particular embodiments disclosed herein.</p><p id="p-0034" num="0033">It is observed here that the earlier-mentioned 3D technologies have many drawbacks. For example, a TOF&#x2014;based 3D-imaging system may require high power to operate optical or electrical shutters. These systems typically operate over a range of few meters to several tens of meters, but the resolution of these systems decreases for measurements over short distances, thereby making 3D imaging within a distance of about one meter almost impractical. Hence, a TOF system may not be desirable for cell phone-based camera applications in which pictures are pre-dominantly taken at close distances. A TOF sensor may also require special pixels with big pixel sizes, usually larger than 7 &#x3bc;m. These pixels also may be vulnerable to ambient light.</p><p id="p-0035" num="0034">The stereoscopic imaging approach generally works only with textured surfaces. It has high-computational complexity because of the need to match features and find correspondences between the stereo pair of images of an object. This requires high system power, which is not a desirable attribute in applications in which power conservation is needed, such as in smartphones. Furthermore, stereo imaging requires two regular, high bit resolution sensors along with two lenses, making the entire assembly unsuitable for applications in portable devices, like cell phones or tablets in which device real estate is at a premium.</p><p id="p-0036" num="0035">The SL approach introduces distance ambiguity, and also requires high system power. For 3D-depth measurements, the SL method may need multiple images with multiple patterns&#x2014;all of these increase computational complexity and power consumption. Furthermore, the SL imaging may also require regular image sensors with high bit resolution. Thus, a structured light-based system may not be suitable for low-cost, low-power, compact image sensors in smartphones.</p><p id="p-0037" num="0036">In contrast to the above-mentioned 3D technologies, particular embodiments disclosed herein provide for implementing a low-power, 3D-imaging system on portable electronic devices, such as smartphones, tablets, UEs, and the like. A 2D-imaging sensor as per particular embodiments disclosed herein can capture both 2D RGB (Red, Green, Blue) images and 3D-depth measurements with visible light laser scanning, while being able to reject ambient light during 3D-depth measurements. It is noted here that although the following discussion may frequently mention a visible light laser as a light source for point-scans and a 2D RGB sensor as an image/light capture device, such mention is for the purpose of illustration and consistency of discussion only. The visible laser and RGB sensor-based examples discussed below may find applications in low-power, consumer-grade mobile electronic devices having cameras such as, smartphones, tablets, or UEs. It is, however, understood that the subject matter disclosed herein is not limited to the visible laser-RGB sensor-based examples mentioned below. Rather, according to particular embodiments disclosed herein, the point-scan based 3D-depth measurements and the ambient-light rejection methodology may be performed using many different combinations of 2D sensors and laser light sources (for point scans) such as, but not limited to: (i) a 2D color (RGB) sensor with a visible light laser source in which the laser source may be a red (R), green (G), or blue (B) light laser, or a laser source producing a combination of these lights; (ii) a visible light laser with a 2D RGB color sensor having an Infrared (IR) cut filter; (iii) a Near Infrared (NIR) laser with a 2D IR sensor; (iv) an NIR laser with a 2D NIR sensor; (v) an NIR laser with a 2D RGB sensor (without an IR cut filter); (vi) an NIR laser with a 2D RGB sensor (without an NIR cut filter); (vii) a 2D RGB-IR sensor with visible or NIR laser; (viii) a 2D RGBW (red, green, blue, white) sensor with either visible or NIR laser; and so on.</p><p id="p-0038" num="0037">During 3D-depth measurements, the entire sensor may operate as a binary sensor in conjunction with the laser scan to reconstruct 3D content. In particular embodiments, the pixel size of the sensor can be as small as 1 &#x3bc;m. Furthermore, due to lower bit resolution, the Analog-to-Digital Converter (ADC) units in the image sensor according to particular embodiments disclosed herein may need significantly lower processing power than that that is needed for high-bit-resolution sensors in traditional 3D-imaging systems. Because of the need for less processing power, the 3D-imaging module according to the subject matter disclosed herein may require lower system power and, hence, may be quite suitable for inclusion in low-power devices like smartphones.</p><p id="p-0039" num="0038">In particular embodiments, the subject matter disclosed herein uses triangulation and point scans with a laser light source for 3D-depth measurements with a group of line sensors. The laser-scanning plane and the imaging plane are oriented using epipolar geometry. An image sensor according to one embodiment disclosed herein may use timestamps to remove ambiguity in the triangulation approach, thereby reducing the amount of depth computations and system power. The same image sensor&#x2014;that is, each pixel in the image sensor&#x2014;may be used in the normal 2D (RGB color or non-RGB) imaging mode as well as in the 3D-laser-scan mode. In the laser-scan mode, however, the resolution of the ADCs in the image sensor is reduced to a binary output (1-bit resolution only), thereby improving the readout speed and reducing power consumption&#x2014;for example, due to switching in the ADC units&#x2014;in the chip incorporating the image sensor and associated processing units. Furthermore, the point-scan approach may allow the system to take all measurements in one pass, thereby reducing the latency for depth measurements and reducing motion blur.</p><p id="p-0040" num="0039">As noted before, in particular embodiments, the entire image sensor may be used for routine 2D RGB color imaging using, for example, ambient light, as well as for 3D-depth imaging using visible laser scan. Such dual use of the same camera unit may save space and cost for mobile devices. Furthermore, in certain applications, a visible laser for 3D applications may be better for user eye safety as compared to a Near Infrared (NIR) laser. The sensor may have higher quantum efficiency at visible spectrum than at the NIR spectrum, leading to lower power consumption of the light source. In one embodiment, the dual-use image sensor may work in a linear mode of operation for 2D imaging as a regular 2D sensor. For 3D imaging, however, the sensor may work in linear mode under moderate lighting condition and in a logarithmic mode under strong ambient light to facilitate continued use of the visible laser source through rejection of the strong ambient light. Furthermore, ambient-light rejection may be needed in case of an NIR laser as well, for example, if the bandwidth of the pass band of an IR-cut filter employed with an RGB sensor is not narrow enough.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a highly simplified, partial configuration of a system <b>15</b> according to one embodiment disclosed herein. As shown, the system <b>15</b> may include an imaging module <b>17</b> coupled to and in communication with a processor or host <b>19</b>. The system <b>15</b> may also include a memory module <b>20</b> coupled to the processor <b>19</b> to store information content such as, image data received from the imaging module <b>17</b>. In particular embodiments, the entire system <b>15</b> may be encapsulated in a single Integrated Circuit (IC) or chip. Alternatively, each of the modules <b>17</b>, <b>19</b> and <b>20</b> may be implemented in a separate chip. Furthermore, the memory module <b>20</b> may include more than one memory chip, and the processor module <b>19</b> may comprise of multiple processing chips as well. In any event, the details about packaging of the modules in <figref idref="DRAWINGS">FIG. <b>1</b></figref> and how the modules are fabricated or implemented&#x2014;in a single chip or using multiple discrete chips&#x2014;are not relevant to the present discussion and, hence, such details are not provided herein.</p><p id="p-0042" num="0041">The system <b>15</b> may be any low-power electronic device configured for 2D- and 3D-camera applications as per the subject matter disclosed herein. The system <b>15</b> may be portable or non-portable. Some examples of the portable version of the system <b>15</b> may include popular consumer electronic gadgets such as, but not limited to, a mobile device, a cellphone, a smartphone, a User Equipment (UE), a tablet, a digital camera, a laptop or desktop computer, an electronic smartwatch, a Machine-to-Machine (M2M) communication unit, a Virtual Reality (VR) equipment or module, a robot, and the like. On the other hand, some examples of the non-portable version of the system <b>15</b> may include a game console in a video arcade, an interactive video terminal, an automobile, a machine vision system, an industrial robot, a VR equipment, a driver-side mounted camera in a vehicle (for example, to monitor whether the driver is awake or not), and so on. The 3D-imaging functionality provided as per the subject matter disclosed herein may be used in many applications such as, but not limited to, virtual reality applications on a virtual reality equipment, online chatting/gaming, 3D texting, searching an online or local (device-based) catalog/database using a 3D image of a product to obtain information related to the product (for example, calorie content of a piece of food item), robotics and machine vision applications, vehicular applications, such as autonomous driving applications, and the like.</p><p id="p-0043" num="0042">In particular embodiments disclosed herein, the imaging module <b>17</b> may include a light source <b>22</b> and an image-sensor unit <b>24</b>. As discussed in more detail with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref> below, in one embodiment, the light source <b>22</b> may be a visible laser. In other embodiments, the light source may be an NIR laser. The image-sensor unit <b>24</b> may include a pixel array and ancillary processing circuits as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> and also discussed below.</p><p id="p-0044" num="0043">In one embodiment, the processor <b>19</b> may be a CPU, which can be a general-purpose microprocessor. In the discussion herein, the terms &#x201c;processor&#x201d; and &#x201c;CPU&#x201d; may be used interchangeably for ease of discussion. It is, however, understood that, instead of or in addition to the CPU, the processor <b>19</b> may contain any other type of processor such as, but not limited to, a microcontroller, a Digital Signal Processor (DSP), a Graphics Processing Unit (GPU), a dedicated Application Specific Integrated Circuit (ASIC) processor, and the like. Furthermore, in one embodiment, the processor/host <b>19</b> may include more than one CPU, which may be operative in a distributed processing environment. The processor <b>19</b> may be configured to execute instructions and to process data according to a particular Instruction Set Architecture (ISA) such as, but not limited to, an x86 instruction set architecture (32-bit or 64-bit versions), a PowerPC&#xae; ISA, or a MIPS (Microprocessor without Interlocked Pipeline Stages) instruction set architecture relying on RISC (Reduced Instruction Set Computer) ISA. In one embodiment, the processor <b>19</b> may be a System on Chip (SoC) having functionalities in addition to a CPU functionality.</p><p id="p-0045" num="0044">In particular embodiments, the memory module <b>20</b> may be a Dynamic Random Access Memory (DRAM) such as, but not limited to, a Synchronous DRAM (SDRAM), or a DRAM-based Three-Dimensional Stack (3DS) memory module such as, but not limited to, a High Bandwidth Memory (HBM) module, or a Hybrid Memory Cube (HMC) memory module.</p><p id="p-0046" num="0045">In other embodiments, the memory module <b>20</b> may be a Solid-State Drive (SSD), a non-3DS DRAM module, or any other semiconductor-based storage system such as, but not limited to, a Static Random Access Memory (SRAM), a Phase-Change Random Access Memory (PRAM or PCRAM), a Resistive Random Access Memory (RRAM or ReRAM), a Conductive-Bridging RAM (CBRAM), a Magnetic RAM (MRAM), a Spin-Transfer Torque MRAM (STT-MRAM), and the like.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an exemplary operational configuration of the system <b>15</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref> according to one embodiment disclosed herein. The system <b>15</b> may be used to obtain depth information (along the Z-axis) for a 3D object, such as the 3D object <b>26</b>, which may be an individual object or an object within a scene (not shown). In one embodiment, the depth information may be calculated by the processor <b>19</b> based on the scan data received from the image-sensor unit <b>24</b>. In another embodiment, the depth information may be calculated by the image-sensor unit <b>24</b> itself such as, in case of the image-sensor unit in the embodiment of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>. In particular embodiments, the depth information may be used by the processor <b>19</b> as part of a 3D user interface to enable the user of the system <b>15</b> to interact with the 3D image of the object or use the 3D image of the object as part of a game or another application running on the system <b>15</b>. The 3D imaging as per the subject matter disclosed herein may be used for other purposes or applications as well, and may be applied to substantially any scene or 3D objects.</p><p id="p-0048" num="0047">In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the X-axis is taken to be the horizontal direction along the front of the device <b>15</b>, the Y-axis is the vertical direction (out of the page in this view), and the Z-axis extends away from the device <b>15</b> in the general direction of the object <b>26</b> being imaged. For the depth measurements, the optical axes of the modules <b>22</b> and <b>24</b> may be parallel to the Z-axis. Other optical arrangements may be used as well to implement the principles described herein, and these alternative arrangements are considered to be within the scope of the subject matter disclosed herein.</p><p id="p-0049" num="0048">The light-source module <b>22</b> may illuminate the 3D object <b>26</b> as shown by exemplary arrows <b>28</b> and <b>29</b> associated with corresponding dotted lines <b>30</b> and <b>31</b> representing an illumination path of a light beam or optical radiation that may be used to point scan the 3D object <b>26</b> within an optical field of view. A line-by-line point scan of the object surface may be performed using an optical-radiation source, which, in one embodiment, may be a laser-light source <b>33</b> operated and controlled by a laser controller <b>34</b>. A light beam from the laser source <b>33</b> may be point scanned, under the control of the laser controller <b>34</b>, in the X-Y direction across the surface of the 3D object <b>26</b> via projection optics <b>35</b>. The point scan may project light spots on the surface of the 3D object along a scan line, as discussed in more detail with reference to <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref> below. The projection optics may be a focusing lens, a glass/plastics surface, or other cylindrical optical element that concentrates laser beam from the laser <b>33</b> as a point or spot on that surface of the object <b>26</b>. In the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a convex structure is shown as a focusing lens <b>35</b>. Any other suitable lens design, however, may be selected for projection optics <b>35</b>. The object <b>26</b> may be placed at a focusing location where illuminating light from the light source <b>33</b> is focused by the projection optics <b>35</b> as a light spot. Thus, in the point scan, a point or narrow area/spot on the surface of the 3D object <b>26</b> may be illuminated sequentially by the focused light beam from the projection optics <b>35</b>.</p><p id="p-0050" num="0049">In particular embodiments, the light source (or illumination source) <b>33</b> may be a diode laser or a Light Emitting Diode (LED) emitting visible light, an NIR laser, a point light source, a monochromatic illumination source (such as, a combination of a white lamp and a monochromator) in the visible light spectrum, or any other type of laser light source. The laser <b>33</b> may be fixed in one position within the housing of the device <b>15</b>, but may be rotatable in X-Y directions. The laser <b>33</b> may be X-Y addressable (for example, by the laser controller <b>34</b>) to perform point scan of the 3D object <b>26</b>. In one embodiment, the visible light may be substantially green light. The visible light illumination from the laser source <b>33</b> may be projected onto the surface of the 3D object <b>26</b> using a mirror (not shown), or the point scan may be completely mirrorless. In particular embodiments, the light-source module <b>22</b> may include more or less components than those shown in the exemplary embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0051" num="0050">In the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the light reflected from the point scan of the object <b>26</b> may travel along a collection path indicated by arrows <b>36</b> and <b>37</b> and dotted lines <b>38</b> and <b>39</b>. The light collection path may carry photons reflected from or scattered by the surface of the object <b>26</b> upon receiving illumination from the laser source <b>33</b>. It is noted here that the depiction of various propagation paths using solid arrows and dotted lines in <figref idref="DRAWINGS">FIG. <b>2</b></figref> (and also in <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref>, as applicable) is for illustrative purpose only. The depiction should not be construed to illustrate any actual optical signal propagation paths. In practice, the illumination and collection signal paths may be different from those shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and may not be as clearly-defined as in the illustration in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0052" num="0051">The light received from the illuminated object <b>26</b> may be focused onto one or more pixels of a 2D pixel array <b>42</b> via collection optics <b>44</b> in the image-sensor unit <b>24</b>. Like the projection optics <b>35</b>, the collection optics <b>44</b> may be a focusing lens, a glass/plastics surface, or other cylindrical optical element that concentrates the reflected light received from the object <b>26</b> onto one or more pixels in the 2D array <b>42</b>. In the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a convex structure is shown as a focusing lens <b>44</b>. Any other suitable lens design may, however, be selected for collection optics <b>44</b>. Furthermore, for ease of illustration, only a 3&#xd7;3 pixel array is shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> (and also in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). It is understood, however, that modern pixel arrays contain thousands or even millions of pixels. The pixel array <b>42</b> may be an RGB pixel array in which different pixels may collect light signals of different colors. As mentioned before, in particular embodiments, the pixel array <b>42</b> may be any 2D sensor, such as a 2D RGB sensor with IR cut filter, a 2D IR sensor, a 2D NIR sensor, a 2D RGBW sensor, a 2D RGB-IR sensor, and the like. As discussed in more detail later, the system <b>15</b> may use the same pixel array <b>42</b> for 2D RGB color imaging of the object <b>26</b> (or a scene containing the object) as well as for 3D imaging (involving depth measurements) of the object <b>26</b>. Additional architectural details of the pixel array <b>42</b> are discussed later with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0053" num="0052">The pixel array <b>42</b> may convert the received photons into corresponding electrical signals, which are then processed by the associated image processing unit <b>46</b> to determine the 3D-depth image of the object <b>26</b>. In one embodiment, the image processing unit <b>46</b> may use triangulation for depth measurements. The triangulation approach is discussed later with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The image processing unit <b>46</b> may also include relevant circuits for controlling the operation of the pixel array <b>42</b>. Exemplary image processing and control circuits are illustrated in <figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref>, which are discussed later below.</p><p id="p-0054" num="0053">The processor <b>19</b> may control the operations of the light-source module <b>22</b> and the image-sensor unit <b>24</b>. For example, the system <b>15</b> may have a mode switch (not shown) controllable by the user to switch from 2D-imaging mode to 3D-imaging mode. If the user selects the 2D-imaging mode using the mode switch, the processor <b>19</b> may activate the image-sensor unit <b>24</b>, but may not activate the light-source module <b>22</b> because 2D imaging may use ambient light. On the other hand, if the user selects 3D imaging using the mode switch, the processor <b>19</b> may activate both of the modules <b>22</b> and <b>24</b>, and may also trigger change in the level of the Reset (RST) signal in the image processing unit <b>46</b> to switch from a linear mode to a logarithmic mode of imaging, for example, if the ambient light is too strong to be rejected by linear mode (as discussed below). The processed image data received from the image processing unit <b>46</b> may be stored by the processor <b>19</b> in the memory <b>20</b>. The processor <b>19</b> may also display the user-selected 2D or 3D image on a display screen (not shown) of the device <b>15</b>. The processor <b>19</b> may be programmed in software or firmware to carry out various processing tasks described herein. Alternatively or additionally, the processor <b>19</b> may comprise programmable hardware logic circuits for carrying out some or all of the functions of processor <b>19</b>. In particular embodiments, the memory <b>20</b> may store program code, look-up tables (like the one shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> and discussed later), and/or interim computational results to enable the processor <b>19</b> to carry out the functions of processor <b>19</b>.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an exemplary flowchart <b>50</b> showing how 3D-depth measurements may be performed according to one embodiment disclosed herein. Various operations illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be performed by a single module or a combination of modules or system components in the system <b>15</b>. In the discussion herein, by way of an example only, specific tasks are described as being performed by specific modules or system components. Other modules or system components may be suitably configured to perform such tasks as well.</p><p id="p-0056" num="0055">In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, at block <b>52</b>, the system <b>15</b> (more specifically, the processor <b>19</b>) may perform a one-dimensional (1D) point scan of a 3D object, such as the object <b>26</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, along a scanning line using a light source, such as the light-source module <b>22</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. As part of the point scan, the light-source module <b>22</b> may be configured, for example, by the processor <b>19</b>, to project a sequence of light spots on a surface of the 3D object <b>26</b> in a line-by-line manner. At block <b>54</b>, the pixel processing unit <b>46</b> in the system <b>15</b> may select a row of pixels in an image sensor, such as the 2D pixel array <b>42</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The image sensor <b>42</b> has a plurality of pixels arranged in a 2D array forming an image plane, the selected row of pixels forms an epipolar line of the scanning line (at block <b>52</b>) on the image plane. A brief discussion of epipolar geometry is provided below with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. At block <b>56</b>, the pixel processing unit <b>46</b> may be operatively configured by the processor <b>19</b> to detect each light spot using a corresponding pixel in the row of pixels. It is observed here that light reflected from an illuminated light spot may be detected by a single pixel or more than one pixel, such as if the light reflected from the illuminated spot becomes focused by the collection optics <b>44</b> onto two or more adjacent pixels. On the other hand, it may be possible that light reflected from two or more light spots may be collected at a single pixel in the 2D array <b>42</b>. A timestamp-based approach discussed below removes depth calculation-related ambiguities resulting from imaging of two different spots by the same pixel or imaging of a single spot by two different pixels. At block <b>58</b>, the image processing unit <b>46</b>&#x2014;as suitably configured by the processor <b>19</b>&#x2014;may generate a pixel-specific output in response to a pixel-specific detection (at block <b>56</b>) of a corresponding light spot in the sequence of light spots (in the point scan at block <b>52</b>). Consequently, at block <b>60</b>, the image processing unit <b>46</b> may determine the 3D distance (or depth) to the corresponding light spot on the surface of the 3D object based at least on the pixel-specific output (at block <b>58</b>) and on a scan angle used by the light source for projecting the corresponding light spot (at block <b>52</b>). The depth measurement is discussed in more detail with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an exemplary illustration of how a point scan may be performed for 3D-depth measurements according to one embodiment disclosed herein. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the X-Y rotational capabilities of the laser source <b>33</b> are illustrated using the arrows <b>62</b> and <b>64</b> depicting the angular motions of a laser in the X-direction (having angle &#x201c;&#x3b2;&#x201d;) and in the Y-direction (having angle &#x201c;&#x3b1;&#x201d;). In one embodiment, the laser controller <b>34</b> may control the X-Y rotation of the laser source <b>33</b> based on scanning instructions/input received from the processor <b>19</b>. For example, if a user selects 3D-imaging mode, the processor <b>19</b> may instruct the laser controller <b>34</b> to initiate 3D-depth measurements of the object surface facing the projection optics <b>35</b>. In response, the laser controller <b>34</b> may initiate a 1D X-Y point scan of the object surface through X-Y movement of the laser light source <b>33</b>. As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the laser <b>33</b> may point scan the surface of the object <b>26</b> by projecting light spots along 1D horizontal scanning lines&#x2014;two of which S<sub>R </sub><b>66</b> and S<sub>R+1 </sub><b>68</b> are identified by dotted lines in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Because of the curvature of the surface of the object <b>26</b>, the light spots <b>70</b>-<b>73</b> may form the scanning line S<sub>R </sub><b>66</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. For ease of illustration and clarity, the light spots constituting the scan line S<sub>R+1 </sub><b>68</b> are not identified using reference numerals. The laser <b>33</b> may scan the object <b>26</b> along rows R, R+1, and so on, for example, one spot at a time in a left-to-right direction. The values of R, R+1, and so on, are with reference to rows of pixels in the 2D pixel array <b>42</b> and, hence, these values are known. For example, in the 2D pixel array <b>42</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the pixel row R is identified using reference numeral &#x201c;75&#x201d; and the row R+1 is identified using reference numeral &#x201c;76.&#x201d; It is understood that rows R and R+1 are selected from the plurality of rows of pixels for illustrative purpose only.</p><p id="p-0058" num="0057">The plane containing the rows of pixels in the 2D pixel array <b>42</b> may be called the image plane, whereas the plane containing the scanning lines, such as the lines S<sub>R </sub>and S<sub>R+1</sub>, may be called the scanning plane. In the embodiment of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the image plane and the scanning plane are oriented using epipolar geometry such that each row of pixels R, R+1, and so on, in the 2D pixel array <b>42</b> forms an epipolar line of the corresponding scanning line S<sub>R</sub>, S<sub>R+1</sub>, and so on. A row of pixels R may be considered epipolar to a corresponding scanning line S<sub>R </sub>if a projection of an illuminated spot (in the scanning line) onto the image plane may form a distinct spot along a line that is the row R itself. For example, in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the arrow <b>78</b> illustrates the illumination of the light spot <b>71</b> by the laser <b>33</b>, whereas the arrow <b>80</b> shows that the light spot <b>71</b> is being imaged or projected along the row R <b>75</b> by the focusing lens <b>44</b>. Although not shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, it is observed that all of the light spots <b>70</b>-<b>73</b> will be imaged by corresponding pixels in the row R. Thus, in one embodiment, the physical arrangement, such as the position and orientation, of the laser <b>33</b> and the pixel array <b>42</b> may be such that illuminated light spots in a scanning line on the surface of the object <b>26</b> may be captured or detected by pixels in a corresponding row in the pixel array <b>42</b>&#x2014;that row of pixels thus forming an epipolar line of the scanning line.</p><p id="p-0059" num="0058">It is understood that the pixels in the 2D pixel array <b>42</b> may be arranged in rows and columns. An illuminated light spot may be referenced by the corresponding row and column in the pixel array <b>42</b>. For example, in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the light spot <b>71</b> in the scanning line S<sub>R </sub>is designated as X<sub>R,i </sub>to indicate that the spot <b>71</b> may be imaged by row R and column i (C<sub>i</sub>) in the pixel array <b>42</b>. The column C<sub>i </sub>is indicated by dotted line <b>82</b>. Other illuminated spots may be similarly identified. As noted before, it may be possible that light reflected from two or more lights spots may be received by a single pixel in a row, or, alternatively, light reflected from a single light spot may be received by more than one pixel in a row of pixels. The timestamp-based approach discussed later may remove the ambiguities in depth calculations arising from such multiple or overlapping projections.</p><p id="p-0060" num="0059">In the illustration of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the arrow having reference numeral &#x201c;84&#x201d; represents the depth or distance Z (along the Z-axis) of the light spot <b>71</b> from the X-axis along the front of the device <b>15</b>, such as the X-axis shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a dotted line having the reference numeral &#x201c;86&#x201d; represents such an axis, which may be visualized as being contained in a vertical plane that also contains the projection optics <b>35</b> and the collection optics <b>44</b>. For ease of explanation of the triangulation method, however, the laser source <b>33</b> is shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> as being on the X-axis <b>86</b> instead of the projection optics <b>35</b>. In a triangulation-based approach, the value of Z may be determined using the following equation:</p><p id="p-0061" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>Z</mi>     <mo>=</mo>     <mrow>      <mfrac>       <mi>hd</mi>       <mrow>        <mi>q</mi>        <mo>-</mo>        <mrow>         <mi>h</mi>         <mo>&#x2062;</mo>         <mi>tan</mi>         <mo>&#x2062;</mo>         <mi>&#x3b8;</mi>        </mrow>       </mrow>      </mfrac>      <mo>.</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0062" num="0000">The parameters in the above Equation (1) are also shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Based on the physical configuration of the device <b>15</b>, the values for the parameters on the right side of Equation (1) may be pre-determined. In Equation (1), the parameter h is the distance (along the Z-axis) between the collection optics <b>44</b> and the image sensor <b>42</b> (which is assumed to be in a vertical plane behind the collection optics <b>44</b>); the parameter d is the offset distance between the light source <b>33</b> and the collection optics <b>44</b> associated with the image sensor <b>24</b>; the parameter q is the offset distance between the collection optics <b>44</b> and a pixel that detects the corresponding light spot (in the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the detecting/imaging pixel i is represented by column C<sub>i </sub>associated with the light spot X<sub>R,i</sub><b>71</b>); and the parameter &#x3b8; is the scan angle or beam angle of the light source for the light spot under consideration (in the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the light spot <b>71</b>). Alternatively, the parameter q may also be considered as the offset of the light spot within the field of view of the pixel array <b>42</b>.</p><p id="p-0063" num="0060">It is seen from Equation (1) that only the parameters &#x3b8; and q are variable for a given point scan; the other parameters h and d are essentially fixed due to the physical geometry of the device <b>15</b>. Because the row R <b>75</b> is an epipolar line of the scanning line S<sub>R</sub>, the depth difference or depth profile of the object <b>26</b> may be reflected by the image shift in the horizontal direction, as represented by the values of the parameter q for different lights spots being imaged. As discussed later below, the timestamp-based approach according to particular embodiments disclosed herein may be used to find the correspondence between the pixel location of a captured light spot and the corresponding scan angle of the laser source <b>33</b>. In other words, a timestamp may represent an association between the values of parameters q and &#x3b8;. Thus, from the known value of the scan angle &#x3b8; and the corresponding location of the imaged light spot (as represented by the parameter q), the distance Z to that light spot may be determined using the triangulation Equation (1).</p><p id="p-0064" num="0061">It is observed here that usage of triangulation for distance measurements is described in the relevant literature including, for example, the U.S. Patent Application Publication No. 2011/0102763 A1 to Brown et al. (Brown). The discussion in the Brown publication related to triangulation-based distance measurement is incorporated herein by reference in its entirety.</p><p id="p-0065" num="0062"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an exemplary timestamping for scanned light spots according to one embodiment disclosed herein. Additional details of generation of individual timestamps are provided later, such as with reference to discussion of <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In contrast to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in the embodiment of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the collection optics <b>44</b> and the laser <b>33</b> are shown in an offset arrangement to reflect the actual physical geometry of these components as shown in the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. By way of an example, the scanning line <b>66</b> is shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> along with corresponding light spots <b>70</b>-<b>73</b>, which, as mentioned before, may be projected based on a left-to-right point scan of the object surface by the sparse laser point source <b>33</b>. Thus, as shown, the first light spot <b>70</b> may be projected at time instant t<sub>1</sub>, the second light spot <b>71</b> may be projected at time instant t<sub>2</sub>, and so on. These light spots may be detected/imaged by respective pixels <b>90</b>-<b>93</b> in the pixel row R <b>75</b>, which is an epipolar line of the scanning line S<sub>R </sub>as discussed earlier. In one embodiment, the charge collected by each pixel if detecting a light spot may be in the form of an analog voltage, which may be output to the image processing unit <b>46</b> for pixel-specific depth determination as discussed below. The analog pixel outputs (pixouts) are collectively indicated by arrow <b>95</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0066" num="0063">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, each detecting pixel <b>90</b>-<b>93</b> in row R may have an associated column number&#x2014;here, columns C<sub>1 </sub>through C<sub>4</sub>. Furthermore, it is seen from <figref idref="DRAWINGS">FIG. <b>4</b></figref> that each pixel column C<sub>i </sub>(i=1, 2, 3, and so on) has an associated value for the parameter q in Equation (1). Thus, if a pixel-specific timestamp t<sub>1</sub>-t<sub>4 </sub>is generated for the detecting pixels <b>90</b>-<b>93</b> (as discussed in more detail later below), the timestamp may provide an indication of the column number of the pixel and, hence, the pixel-specific value of the parameter q. Additionally, in one embodiment, the spot-by-spot detection using pixels in the pixel array <b>42</b> may allow the image processing unit <b>46</b> to &#x201c;link&#x201d; each timestamp with the corresponding illuminated spot and, hence, with the spot-specific scan angle &#x3b8; because the laser <b>33</b> may be suitably controlled to illuminate each spot in the desired sequence with pre-determined values for spot-specific scan angles &#x3b8;. Thus, timestamps provide correspondence between the pixel location of a captured laser spot and a respective scan angle in the form of the values for parameters q and &#x3b8; in Equation (1) for each pixel-specific signal received from the pixel array <b>42</b>. As discussed before, the values of the scan angle and the corresponding location of the detected spot in the pixel array <b>42</b>, as reflected through the value of the parameter q in Equation (1), may allow depth determination for that light spot. In this manner, the 3D-depth map for the surface of the object <b>26</b> in the field of view of the pixel array <b>42</b> may be generated.</p><p id="p-0067" num="0064"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows exemplary circuit details of the 2D pixel array <b>42</b> and a portion of the associated processing circuits in the image processing unit <b>46</b> of the image sensor <b>24</b> in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> according to one embodiment disclosed herein. As noted before, the pixel array <b>42</b> is shown having nine pixels <b>100</b>-<b>108</b> arranged as a 3&#xd7;3 array for ease of illustration only; in practice, a pixel array may contain hundreds of thousands or millions of pixels in multiple rows and columns. In one embodiment, each pixel <b>100</b>-<b>108</b> may have an identical configuration as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In the embodiment of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the 2D pixel array <b>42</b> comprises a Complementary Metal Oxide Semiconductor (CMOS) array in which each pixel is a Four Transistor Pinned Photo-diode (4T PPD) pixel. For ease of illustration, the constituent circuit elements of only pixel <b>108</b> are labeled with reference numerals. The following discussion of the operation of the pixel <b>108</b> equally applies to the other pixels <b>101</b>-<b>107</b> and, hence, the operation of each individual pixel is not described herein.</p><p id="p-0068" num="0065">As shown, the 4T PPD pixel <b>108</b> (and similar other pixels <b>101</b>-<b>107</b>) may comprise a pinned photo-diode (PPD) <b>110</b> and four N-channel Metal Oxide Semiconductor Field Effect Transistors (NMOS) <b>111</b>-<b>114</b> connected as illustrated. The transistor <b>111</b> may operate as a Transfer Gate (TG), Floating Diffusion (FD) transistor. Broadly, the 4T PPD pixel <b>108</b> may operate as follows: First, the PPD <b>110</b> may convert the incident photons into electrons, thereby converting the optical input signal into an electrical signal in the charge domain. Then, the transfer gate <b>111</b> may be &#x201c;closed&#x201d; to transfer all the photon-generated electrons from the PPD <b>110</b> to the floating diffusion. The signal in the charge domain thus is converted to the voltage domain for ease of subsequent processing and measurements. The voltage at the floating diffusion may be later transferred as a pixout signal to an Analog-to-Digital Converter (ADC) using the transistor <b>114</b> and converted into an appropriate digital signal for subsequent processing. More details of the pixel output (PIXOUT) generation and processing are provided below with reference to discussion of <figref idref="DRAWINGS">FIGS. <b>8</b>, <b>10</b> and <b>11</b></figref>.</p><p id="p-0069" num="0066">In the embodiment of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a row decoder/driver <b>116</b> in the image processing unit <b>46</b> is shown to provide three different signals to control the operation of the pixels in the pixel array <b>42</b> to generate the column-specific pixout signals <b>117</b>-<b>119</b>. In the embodiment of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the output <b>95</b> may collectively represent such PIXOUT signals <b>117</b>-<b>119</b>. A Row Select (RSEL) signal may be asserted to select an appropriate row of pixels. In one embodiment, the row to be selected is the epipolar line of the current scanning line (of light spots) being projected by the laser source <b>33</b>. The row decoder/driver <b>116</b> may receive the address or control information for the row to be selected via the row address/control inputs <b>126</b>, for example, from the processor <b>19</b>. In the present discussion, it is assumed that the row decoder/driver <b>116</b> selects the row of pixels containing the pixel <b>108</b>. A transistor, such as the transistor <b>114</b>, in each row of pixels in the pixel array <b>42</b> may be connected to a respective RSEL line <b>122</b>-<b>124</b> as shown. A Reset (RST) signal may be applied to pixels in the selected row to reset those pixels to a pre-determined high voltage level. Each row-specific RST signal <b>128</b>-<b>130</b> is shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> and explained in more detail with reference to the waveforms in <figref idref="DRAWINGS">FIGS. <b>8</b>, <b>10</b> and <b>11</b></figref>. A transistor, such as the transistor <b>112</b>, in each pixel may receive the respective RST signal as shown. A Transfer (TX) signal may be asserted to initiate transfer of the pixel-specific output voltage (PIXOUT) for subsequent processing. Each row-specific TX line <b>132</b>-<b>134</b> is shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. A transfer-gate transistor, such as the transistor <b>111</b>, may receive the respective TX signal as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0070" num="0067">As mentioned before, in particular embodiments disclosed herein, the 2D array <b>42</b> and the rest of the rest of the components in the image-sensor unit <b>24</b> may be used for 2D RGB (or non-RGB) imaging as well as for 3D-depth measurements. Consequently, as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the image-sensor unit <b>24</b> may include a pixel column unit <b>138</b> that includes circuits for Correlated Double Sampling (CDS) as well as column-specific ADCs&#x2014;one ADC per column of pixels&#x2014;to be used during 2D and 3D imaging. The pixel column unit <b>138</b> may receive and process the PIXOUT signals <b>117</b>-<b>119</b> to generate a digital data output (Dout) signal <b>140</b> from which a 2D image may be generated or a 3D-depth measurements can be obtained. The pixel column unit <b>138</b> may also receive a reference input <b>142</b> and a ramp input <b>143</b> during processing of the PIXOUT signals <b>117</b>-<b>119</b>. More details of the operation of the unit <b>138</b> are provided later below. In the embodiment of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a column decoder unit <b>145</b> is shown coupled to the pixel column unit <b>138</b>. The column decoder <b>145</b> may receive a column address/control input <b>147</b> from, for example, the processor <b>19</b>, for the column to be selected in conjunction with a given row select (RSEL) signal. The column selection may be sequential, thereby allowing sequential reception of the pixel output from each pixel in the row selected by the corresponding RSEL signal. The processor <b>19</b> may be aware of the currently projected scanning line of light spots and, hence, may provide appropriate row address inputs to select the row of pixels that forms the epipolar line of the current scanning line and may also provide appropriate column address inputs to enable the pixel column unit <b>138</b> to receive outputs from the individual pixels in the selected row.</p><p id="p-0071" num="0068">Although the discussion herein primarily focuses on the 4T PPD pixel design shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> for 2D and 3D imaging according to the subject matter disclosed herein, different types of pixels may be used in the pixel array <b>42</b> in other embodiments. For example, in one embodiment, each pixel in the pixel array <b>42</b> may be a 3T pixel, which omits the transfer gate transistor, like the transistor <b>111</b> in the 4T PPD design in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In other embodiments, 1T pixels or 2T pixels may be used as well. In yet another embodiment, each pixel in the pixel array <b>42</b> may have a shared-transistor pixel configuration in which transistors and read-out circuitry can be shared among two or more neighboring pixels. In the shared-transistor pixel configuration, each pixel may have at least one photo-diode and one transfer-gate transistor; the rest of the transistors can be shared among two or more pixels. One example of such a shared-transistor pixel is the 2-shared (1&#xd7;2) 2.5T pixel in which five transistors (T) are used for two pixels, resulting in a 2.5T/pixel configuration. Another example of a shared-transistor pixel that may be used in the pixel array <b>42</b> is the 1&#xd7;4 4-shared pixel in which 4 pixels share the readout circuitry, but each pixel has at least one photo-diode and one TX (transfer-gate) transistor. Other pixel configurations than those listed here may be suitably implemented for 2D and 3D imaging as per the subject matter disclosed herein.</p><p id="p-0072" num="0069"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is an exemplary configuration of an image-sensor unit, such as the image-sensor unit <b>24</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, according to one embodiment disclosed herein. For the sake of brevity, only a brief discussion of the architecture in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is provided herein; more relevant operational details are provided later with reference to <figref idref="DRAWINGS">FIGS. <b>8</b>, <b>10</b> and <b>11</b></figref>. As shown, the image-sensor unit <b>24</b> in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> may include a row decoder unit <b>149</b> and a row driver unit <b>150</b>, both of which collectively comprise the row decoder/driver <b>116</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Although not shown in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, the row decoder unit <b>149</b> may receive a row address input (like the input <b>126</b> shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) from, for example, the processor <b>19</b>, and decode the input to enable the row driver unit <b>150</b> to provide appropriate RSEL, RST, and TX signals to the row selected/decoded by the row decoder <b>149</b>. The row driver unit <b>150</b> may also receive control signals (not shown) from, for example, the processor <b>19</b>, to configure the row driver <b>150</b> to apply appropriate voltage levels for the RSEL, RST, and TX signals. In the image-sensor unit <b>24</b> in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, a column ADC unit <b>153</b> may represent the pixel column unit <b>138</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. For ease of illustration, in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, various row-specific driver signals, such as the RSEL, RST, and TX signals, from the row driver <b>150</b> are collectively referenced using a single reference numeral &#x201c;<b>155</b>.&#x201d; Similarly, all column-specific pixel outputs (PIXOUTS), like the PIXOUT signals <b>117</b>-<b>119</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, are collectively referenced using a single reference numeral &#x201c;<b>157</b>.&#x201d; The column ADC unit <b>153</b> may receive the PIXOUT signals <b>157</b> and the reference input <b>142</b> (from a reference signal generator <b>159</b>) and the ramp signal <b>143</b> to generate a pixel-specific output by the corresponding column-specific ADC for the column of the pixel. The 2D imaging is discussed in more detail later with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. In one embodiment, the ADC unit <b>153</b> may include circuitry for CDS, as in case of the pixel column unit <b>138</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, to generate a CDS output (not shown) that is the difference between the reset level of the pixel and the received signal level. In particular embodiments, the 3D-depth values may be combined with the 2D image to generate a 3D image of the object.</p><p id="p-0073" num="0070">The column ADC unit <b>153</b> may include a separate ADC per pixel column in the 2D array <b>42</b>. Each column-specific ADC may receive a respective ramp input <b>143</b> (from a ramp signal generator <b>163</b>) along with the PIXOUT signals <b>157</b>. In one embodiment, the ramp signal generator <b>163</b> may generate the ramp input <b>143</b> based on the reference voltage level received from the reference signal generator <b>159</b>. Each column-specific ADC in the ADC unit <b>153</b> may process the received inputs to generate the corresponding digital data output (Dout) signal <b>140</b>. The ADC unit <b>153</b> may receive information from the column decoder <b>145</b> about which column ADC output to be readout and sent to the Dout bus <b>140</b>, and may also receive information about which column to select for a given row to receive the appropriate pixel output. Although not shown in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, the column decoder unit <b>145</b> may receive a column address input (like the input <b>147</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) from, for example, the processor <b>19</b>, and decode the input to enable the column ADC unit <b>153</b> to select the appropriate pixel column. In the embodiment of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, the decoded column address signals are collectively identified using the reference numeral &#x201c;<b>165</b>.&#x201d;</p><p id="p-0074" num="0071">The digital data outputs <b>140</b> from the ADC units may be processed by a digital processing block <b>167</b>. In one embodiment, for the 2D RGB imaging mode, each ADC-specific data output <b>140</b> may be a multi-bit digital value that substantially corresponds to the actual photon charge collected by the respective pixel. On the other hand, in the 3D-depth measurement mode, each ADC-specific data output <b>140</b> may be a timestamp value representing the time instant when the respective pixel detects the corresponding light spot. This timestamping approach according to the subject matter disclosed herein is discussed later in more detail. The digital processing block <b>167</b> may include circuits to provide timing generation; Image Signal Processing (ISP), such as processing of data outputs <b>140</b> for the 2D-imaging mode; depth calculations for the 3D-imaging mode; and so on. In that regard, the digital processing block <b>167</b> may be coupled to an interface unit <b>168</b> to provide the processed data as an output <b>170</b>, for example, to enable the processor <b>19</b> to render a 2D RGB/non-RGB image or a 3D-depth image of the 3D object <b>26</b> on a display screen (not shown) of the device <b>15</b>. The interface unit <b>168</b> may include a Phase-Locked Loop (PLL) unit for generation of clock signals that support the timing generation functionality in the digital processing block <b>167</b>. Furthermore, the interface unit <b>168</b> may also include a Mobile Industry Processor Interface (MIPI) that provides an industry-standard hardware and software interface to other components or circuit elements in the device <b>15</b> for the data generated by the digital block <b>167</b>. The MIPI specifications support a broad range of mobile products and provide specifications for a camera of a mobile device, display screen, power management, battery interface, and the like. The MIPI-standardized interfaces may yield an improved operability between a peripherals of a mobile device, such as a camera or display screen of a smartphone, and the application processor(s) of the mobile device, which may not be from the same vendor as the vendor (or vendors) providing the peripherals.</p><p id="p-0075" num="0072">In the embodiment of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, a timestamp calibration unit <b>171</b> is shown coupled to the column ADC unit <b>153</b> to provide appropriate calibration signals <b>172</b> to individual column-specific ADCs to enable each column-specific ADC unit to generate an output representing a pixel-specific timestamp value in the 3D-measurement mode. This timestamping approach is discussed in more detail with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0076" num="0073"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> shows architectural details of an exemplary CDS+ADC unit <b>175</b> for 3D-depth measurement according to one embodiment disclosed herein. For ease of discussion, the unit <b>175</b> may be referred below to as &#x201c;ADC unit,&#x201d; however, it is understood that the unit <b>175</b> may also include CDS functionality in addition to the ADC functionality. A simplified version of a CDS unit is represented using the capacitor <b>176</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>. In one embodiment, each column of pixels in the 2D pixel array <b>42</b> may have a column-specific, single-slope ADC unit similar to the ADC unit <b>175</b>. Thus, in the embodiment of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, there may be three ADC units in the pixel column unit <b>138</b>, i.e., one ADC per column. As shown, the ADC <b>175</b> in the embodiment of <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> may include two Operational Transconductance Amplifiers (OTA) <b>177</b> and <b>179</b> connected in series with a binary counter <b>181</b> and a line memory unit <b>183</b>. For ease of illustration, only the inverting (&#x2212;) and non-inverting (+) voltage inputs to the OTAs <b>177</b> and <b>179</b> are shown in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, and the biasing inputs and the power supply connections are not shown. It is understood that an OTA is an amplifier in which a differential input voltage produces an output current. Thus, an OTA may be considered as a voltage-controlled current source. The biasing inputs may be used to provide currents or voltages to control the transconductance of the amplifier. The first OTA <b>177</b> may receive from the CDS unit <b>176</b> a CDS version of the PIXOUT voltage from a pixel, such as the pixel <b>108</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, that is selected in the activated row using the column number received from the column decoder <b>145</b>. The CDS version of a pixout signal may be referred to as a &#x201c;PIX_CDS&#x201d; signal. The OTA <b>177</b> may also receive a Vramp voltage <b>143</b> from the ramp signal generator <b>163</b> (<figref idref="DRAWINGS">FIG. <b>7</b>A</figref>). The OTA <b>177</b> may generate an output current if the pixout voltage <b>157</b> drops below the Vramp voltage <b>143</b>, as discussed below with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The output of the OTA <b>177</b> may be filtered by the second OTA <b>179</b> before being applied to the binary counter <b>181</b>. In one embodiment, the binary counter <b>181</b> may be a 10-bit ripple counter that receives a Clock (Clk) input <b>185</b> and generates a timestamp value <b>186</b> based on the clock cycles counted during a pre-determined time triggered by the generation of the output current by the first OTA <b>177</b>. In the context of the embodiment in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, the Clk input <b>185</b> may be a system-wide clock, an image sensor-specific clock generated by the PLL unit <b>168</b>, or other clock generator (not shown) in the device <b>15</b>. The pixel-specific timestamp value <b>186</b> may be stored in the line memory <b>183</b> against the column number (column #) of the pixel, and subsequently output to the digital processing block <b>167</b> as the Dout signal <b>140</b>. The column number input <b>165</b> may be received from the column decoder unit <b>145</b> shown in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>.</p><p id="p-0077" num="0074">In particular embodiments, the RGB color model may be used for sensing, representation, and display of images on mobile devices, such as the device <b>15</b> in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>. In the RGB color model, the light signals having three primary colors&#x2014;red, green, and blue&#x2014;may be added together in various ways to produce a broad array of colors in the final image. The CDS method may be used in 2D RGB imaging to measure an electrical value, such as a pixel/sensor output voltage, in a manner that allows removal of an undesired offset. For example, a CDS unit, like the CDS unit <b>176</b>, may be employed in each column-specific ADC unit, like the ADC unit <b>175</b>, to perform correlated double sampling. In CDS, the output of the pixel may be measured twice&#x2014;once in a known condition, and once in an unknown condition. The value measured from the known condition may be then subtracted from the value measured from the unknown condition to generate a value with a known relation to the physical quantity being measured, that is, the photoelectron charge representing the pixel-specific portion of the image signal. Using CDS, noise may be reduced by removing the reference voltage of the pixel (such as the voltage of the pixel after it is reset) from the signal voltage of the pixel at the end of each integration period. Thus, in CDS, before the charge of a pixel is transferred as an output, the reset value is sampled. The reference value is &#x201c;deducted&#x201d; from the value after the charge of the pixel is transferred.</p><p id="p-0078" num="0075">It is observed here that, in particular embodiments, the ADC unit <b>175</b> may be used for both 2D imaging as well as 3D-depth measurements. All the inputs for such shared configuration, however, are not shown in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>. In the shared use case, the corresponding Vramp signal may be different as well for 2D imaging.</p><p id="p-0079" num="0076"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a timing diagram <b>190</b> that shows exemplary timing of different signals in the system <b>15</b> of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> to generate timestamp-based pixel-specific outputs in a 3D-linear mode of operation according to particular embodiments disclosed herein. As noted before, in particular embodiments, all pixels in the same image sensor <b>24</b> may be used for 2D as well as 3D imaging. The 3D-depth measurements, however, may be performed using a 3D-linear mode or a 3D-logarithmic mode depending on the level of ambient light. As discussed in more detail below with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the 3D-logarithmic mode may be used for depth measurements if ambient light rejection is needed. The discussion of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, however, relates to the timing waveforms associated with the 3D-linear mode.</p><p id="p-0080" num="0077">Briefly, as discussed earlier with reference to <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref>, the 3D object <b>26</b> may be point-scanned&#x2014;one spot at a time&#x2014;by the laser light source <b>33</b> along a row R <b>75</b> of the pixel array <b>42</b> in which R is known based on the corresponding epipolar relation with the scanning line S<sub>R </sub><b>66</b>. After scanning one row, the scanning operation repeats with another row. If the laser projects the next spot, the earlier-projected light spot may be imaged by the corresponding pixel in the row R. The pixel-specific outputs from all the pixels in the row R may be read out to the depth processing circuit/module in the digital processing block <b>167</b> (<figref idref="DRAWINGS">FIG. <b>7</b>A</figref>).</p><p id="p-0081" num="0078">To generate a pixel-specific output, the corresponding row may have to be initially selected using an RSEL signal. In the context of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, it is assumed that the row decoder/driver <b>116</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> selects the row of pixels containing pixels <b>106</b>-<b>108</b> by asserting the RSEL signal <b>122</b> to a &#x201c;high&#x201d; level as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Thus, all the pixels <b>106</b>-<b>108</b> are selected together. For ease of discussion, the same reference numerals are used in <figref idref="DRAWINGS">FIG. <b>8</b></figref> for the signals, inputs, or outputs that are also shown in <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>. Initially, all the pixels <b>106</b>-<b>108</b> in the selected row may be reset to a high voltage using the RST line <b>128</b>. The &#x201c;reset&#x201d; level of a pixel may represent an absence of the pixel-specific detection of a corresponding light spot. In the 3D-linear mode according to one embodiment disclosed herein, the RST signal <b>128</b> may be released from the high level for a pre-determined time to facilitate integration of photoelectrons received by the pixels <b>106</b>-<b>108</b>, to obtain the corresponding pixel output (PIXOUT) signals <b>117</b>-<b>119</b>, two of which are shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> and discussed later below. The PIXOUT1 signal <b>119</b> represents the output supplied to a corresponding ADC unit by the pixel <b>108</b>, and is shown using a dashed line having the pattern &#x201c;-&#x22c5;&#x22c5;-&#x22c5;&#x22c5;-&#x201d;. The PIXOUT2 signal <b>118</b> represents the output supplied to a corresponding ADC unit by the pixel <b>107</b>, and is shown using a dotted line having the pattern &#x201c;&#x22c5;&#x22c5;&#x22c5;&#x22c5;&#x22c5;&#x22c5;&#x22c5;&#x22c5;&#x22c5;&#x22c5;&#x22c5;&#x22c5;&#x22c5;&#x22c5;&#x201d;. On the other hand, in the 3D-logarithmic mode according to one embodiment disclosed herein, the RST signal may remain high for the selected row during generation of the pixel output as discussed later below. It is noted here that, in one embodiment, other RST lines, that is, like the lines <b>129</b>-<b>130</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, may remain high or &#x201c;on&#x201d; for unselected rows to prevent blooming. It is noted here that, strictly speaking, the PIXOUT signals <b>118</b> and <b>119</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref> (and similar pixout signals in <figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref>) may be slightly modified by a CDS unit such as, the CDS unit <b>176</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, before being applied as PIX_CDS signals to the first OTA, like the OTA <b>177</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, in a respective column-specific ADC unit, such as the ADC unit <b>175</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>. For the simplicity of illustration and ease of discussion, however, the PIXOUT signals in <figref idref="DRAWINGS">FIGS. <b>8</b>, <b>10</b> and <b>11</b></figref> are treated as representatives of respective PIX_CDS signals (not shown) and are considered as having been directly &#x201c;input&#x201d; to the respective OTAs <b>177</b>.</p><p id="p-0082" num="0079">After reset, if a photodiode in a pixel receives incident luminance, such as the photoelectrons in the light reflected from a light spot projected on the surface of the 3D object <b>26</b>, the photodiode may generate corresponding photocurrent. A detection of incident light by a pixel may be called an &#x201c;ON event,&#x201d; whereas a decrease in the intensity of incident light may produce an &#x201c;OFF event.&#x201d; The photocurrent generated in response to an ON event may decrease the pixel output voltage (PIXOUT) from the initial reset level. A pixel thus functions as a transducer to convert received luminance/light signal into a corresponding electrical (analog) voltage, which is generally designated as a PIXOUT signal in <figref idref="DRAWINGS">FIGS. <b>6</b>, <b>8</b>, <b>10</b> and <b>11</b></figref>. Each pixel may be read individually and, in one exemplary embodiment, in the sequence in which the corresponding light spots are projected by the laser source. The analog pixout signal may be converted to a digital value by the corresponding column ADC. In the 2D-imaging mode, the ADC may function as an analog-to-digital converter and generate a multi-bit output. As discussed below, however, in the 3D-depth measurement mode, the ADC may function as a time-to-digital converter and generate a timestamp value representing the time when a light spot is detected by a pixel.</p><p id="p-0083" num="0080">Referring again to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, after the pixel reset is done (with RST <b>128</b> high), the column ADCs associated with pixels <b>106</b>-<b>108</b> may be reset as well before the RST is released. The transfer (TX) signal <b>132</b> may, however, remain high throughout. The ADCs may be reset using either a common ADC reset signal or individual ADC-specific reset signals. In the embodiment of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a common ADC_RST signal <b>192</b> is shown to have been briefly asserted (to a high level) to reset the column-specific ADCs, like the ADC <b>175</b>, in the column ADC unit <b>153</b> (<figref idref="DRAWINGS">FIG. <b>7</b>A</figref>). In one embodiment, the ADCs may be reset to a pre-determined binary value, such as a binary &#x201c;0&#x201d; or other known number, after the pixels are reset. In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, these reset values for ADCs associated with pixels <b>108</b> and <b>107</b> are respectively shown by &#x201c;fields&#x201d; <b>194</b> and <b>195</b> in the signals ADCOUT1 (or ADC output &#x201c;A&#x201d;) and ADCOUT2 (or ADC output &#x201c;B&#x201d;). It is noted here that the term &#x201c;field&#x201d; is used here for the sake of convenience only when discussing the ADC outputs shown in <figref idref="DRAWINGS">FIGS. <b>8</b>, <b>10</b> and <b>11</b></figref>. It is understood that an ADC output may not actually comprise all of such &#x201c;fields&#x201d; at the same time, but may be a specific digital value depending on the current stage of signal processing for the ADC, that is, if the ADC is reset, the output may be a binary &#x201c;0,&#x201d; and if the ADC is triggered to count clock pulses, the output may be a count value as in case of the 3D-depth measurements in <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>11</b></figref>. If the ADC is used for 2D color imaging (as in case of <figref idref="DRAWINGS">FIG. <b>10</b></figref>), then the output may be a multi-bit value representing an image signal. Thus, the ADC output signals in <figref idref="DRAWINGS">FIGS. <b>8</b>, <b>10</b> and <b>11</b></figref> are depicted with such &#x201c;fields&#x201d; merely to illustrate different digital values an ADC may internally generate in progressing toward the final output. In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the reference numeral &#x201c;197&#x201d; is used to refer to the ADCOUT1 signal representing the output of the ADC associated with the pixel <b>108</b>, and the reference numeral &#x201c;198&#x201d; is used to refer to the ADCOUT2 signal representing the output of the ADC associated with the pixel <b>107</b>. Each of the outputs <b>197</b> and <b>198</b> may appear as the Dout signal <b>140</b> (<figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>) when the respective ADC is selected by the column decoder during memory readout. Prior to being reset, the ADC outputs <b>197</b> and <b>198</b> may have unknown values, as indicated by the notation &#x201c;x&#x201d; in the fields <b>199</b> and <b>200</b>.</p><p id="p-0084" num="0081">After ADCs are reset, a pre-determined threshold value may be enabled by de-asserting the ramp input (Vramp) <b>143</b> to a pre-defined voltage level after the pixel reset signal <b>128</b> and ADC reset signal <b>192</b> are released. In the embodiment of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the RAMP input <b>143</b> is common to all column-specific ADCs, thereby providing the same Vramp voltage to each ADC. In other embodiments, however, different Vramp values may be applied to two or more ADCs through separate, ADC-specific ramp inputs. Furthermore, in particular embodiments, the Vramp threshold may be a programmable parameter, allowing it to be variable as desired. After the threshold (RAMP signal) is enabled, the pixel-specific ADCs may wait for the &#x201c;ON event&#x201d; of the corresponding pixel before starting the binary counters, like the counter <b>181</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>.</p><p id="p-0085" num="0082">In the 3D-depth measurement mode, each ADC may generate a single-bit output (representing binary &#x201c;0&#x201d; or &#x201c;1&#x201d;), as opposed to a multi-bit output in case of the 2D-imaging mode (discussed below). Thus, in case of an RGB sensor, any color information received by a pixel in the RGB pixel array <b>42</b> may be effectively ignored. In the absence of any incident light detected by a pixel, the corresponding ADCOUT signal may remain at the binary &#x201c;0&#x201d; value. Thus, columns without any ON events may continue to have digital value &#x201c;0&#x201d; (or other known number) for their respective ADCOUT signals. As noted before, however, if a pixel is hit with incident light, the PIXOUT line of the pixel may start to droop from the reset level, as indicated by the downward slopes of the PIXOUT1 and PIXOUT2 signals in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Assuming that pixel charge is read starting with the pixel that receives the charge first, such a reading may start with the right-most pixel in a row and end with the left-most pixel as shown, for example, in <figref idref="DRAWINGS">FIG. <b>5</b></figref> in which t<sub>1 </sub>is the earliest time instant and t<sub>4 </sub>is the latest time instant. Thus, in the embodiment of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the output of the pixel <b>108</b> (PIXOUT1) may be read before that of the pixel <b>107</b> (PIXOUT2). As soon as the progressively-drooping PIXOUT1 reaches the Vramp threshold <b>143</b>, the single-bit ADCOUT1 may flip from binary &#x201c;0&#x201d; to binary &#x201c;1.&#x201d; Instead of outputting the bit &#x201c;1,&#x201d; however, the corresponding ADC may record the time when the bit flips from &#x201c;0&#x201d; to &#x201c;1&#x201d;. In other words, the ADC associated with the pixel <b>108</b> may function as a time-to-digital converter by starting the binary counter in the ADC, as indicated by the &#x201c;up count&#x201d; field <b>202</b> in ADCOUT1. During the &#x201c;up count&#x201d; period, the counter in the ADC may count the clock pulses in the CLK signal <b>185</b>, which may be applied to each ADC as shown, for example, in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>. The counted clock pulses are shown by the Counter Clock-1 signal <b>204</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, and the counted value (in the &#x201c;up count&#x201d; field) may be provided as a pixel-specific output for the pixel <b>108</b>. A similar counting may occur at the ADC associated with pixel <b>107</b> for the charge collected by the pixel <b>107</b>, as indicated by the Counter Clock-2 signal <b>205</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The pixel-specific counted value (in the &#x201c;up count&#x201d; field <b>207</b>) may be provided by the respective ADC as a pixel-specific output for the pixel <b>107</b>. After scanning all pixels in one row, the pixel-by-pixel charge collection operation may repeat with another row, while the outputs from the earlier-scanned row are read out to the depth calculation unit in the digital block <b>167</b>.</p><p id="p-0086" num="0083">Each ADC output may effectively represent a respective timestamp value providing a temporal indication of a detection by a pixel of a light spot on the object surface illuminated by the laser light source <b>33</b>. A timestamp may be considered to capture the light arrival time for a pixel. In one embodiment, a timestamp value may be generated for a detected light spot by the digital processing block <b>167</b> from the count value (of the counted clock pulses) received from an ADC unit. For example, the digital block <b>167</b> may generate a timestamp by relating the count value to an internal system time or other reference time. The timestamp is generated at the receiving end and, hence, may not necessarily represent the exact time when the corresponding light spot was projected by the light source. The timestamp values may, however, allow the digital block <b>167</b> to establish a temporal correlation among timestamped light spots, thereby allowing the digital block <b>167</b> to determine distances to timestamped light spots in the time-wise order specified by the temporal correlation, that is, the distance to the earliest illuminated light spot being determined first, and so on, until the distance to the last-illuminated light spot is determined. In one embodiment, the timestamping approach may also facilitate resolution of the ambiguity that may arise from multiple light spots being imaged on the same pixel, as discussed later.</p><p id="p-0087" num="0084">All ADC-based counters may stop simultaneously such as when the ramp signal <b>143</b> is asserted again after a pre-determined time period has elapsed. In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the transition of the ramp signal <b>143</b> marking the conclusion of the pre-determined time period for pixel charge integration is indicated by dotted line <b>210</b>. The RSEL <b>122</b> and the RST <b>128</b> signals may also transition their states substantially simultaneously with the change in the level of the ramp signal <b>143</b> (at line <b>210</b>). In one embodiment, all ADC-based counters may be reset at line <b>210</b>. In another embodiment, all ADC-based counters may be reset at any time prior to the selection of the next row of pixels for reading the pixel charge. Despite resetting of ADC counters upon conclusion of scanning of pixels in one row, the timestamp value for each pixel in the pixel array <b>42</b> may remain distinct because of the relational establishment of the timestamp value against an internal system time or other reference source of time, which may remain global and continuously-running.</p><p id="p-0088" num="0085">In the embodiment of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a later-scanned pixel, such as the pixel <b>107</b>, may have a smaller ADC output than the pixel that is scanned earlier, such as the pixel <b>108</b>. Thus, as shown, the ADCOUT2 may have less count value (or less number of clock pulses counted) than the ADCOUT1. Alternatively, in another embodiment, a later-scanned pixel may have a larger ADC output than an earlier-scanned pixel, for example, if each ADC-specific counter starts counting when a pixel is reset and stops counting when an &#x201c;ON event&#x201d; is detected such as, when the pixout signal of the pixel droops below a given threshold (Vramp).</p><p id="p-0089" num="0086">It is noted here that circuits and waveforms shown in <figref idref="DRAWINGS">FIGS. <b>6</b>, <b>8</b>, <b>10</b> and <b>11</b></figref> are based on single-slope ADCs with per column up-counters. It is understood, however, that the timestamping approach may be implemented with up- or down-counters depending on the design choice. Furthermore, single-slope ADCs with global counters may be used as well. For example, in one embodiment, instead of using individual, column-based counters, a global counter (not shown) may be shared by all column ADCs. In that case, the ADCs may be configured such that the column memory, like the line memory <b>183</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, in each ADC may latch the output of the global counter to generate an appropriate ADC-specific output if a column-based comparator unit (not shown) detects an &#x201c;ON event,&#x201d; such as when the column-based comparator unit first senses the respective pixout signal drooping below the ramp threshold <b>143</b>.</p><p id="p-0090" num="0087">Although not shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a dark current offset can be removed by decreasing the Vramp threshold at a rate that is the same as that of the dark current. Dark current may be a relatively small electric current that flows through a photosensitive device, such as a photodiode, even if no photons are entering the device. In image sensors, dark current may cause noise or unwanted artefacts in the collected charge. Dark current may be caused by defects in pixels and may have an effect like the photocurrent. Thus, due to the dark current, the pixel output may still decrease even without the existence of light (or in the absence of the light being received by the pixel). Thus, during charge collection, if the pixels in a row are scanned from right to left, for example, as shown in the context of row <b>75</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref> and discussed with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the pixels on the left side may integrate more dark current than the pixels on the right side. Therefore, in order to prevent registration of any false event due to dark current, the pre-determined ramp threshold (Vramp) may be decreased/adjusted by the rate that the dark current increases along the row of pixels to compensate for the reduced level of pixel output due to the dark current. In one embodiment, this adjusted threshold value may be then used for a pixel to compare the level of the PIXOUT signal of the pixel. Thus, the value of the threshold voltage (Vramp) may be variable and individually-programmable for each ADC. In one embodiment, all pixels associated with a specific ADC may have the same Vramp value. In another embodiment, each pixel may have a pixel-specific Vramp value programmable in the corresponding ADC.</p><p id="p-0091" num="0088">It is observed here that, if a row of light spots is scanned along the surface of the object, two or more different spots from the object scanned may be imaged on the same pixel. The spots may be in the same scanning line or may be on adjacent scanning lines. If multiple spots are scanned across the surface of the object, such overlapping imaging may negatively affect the correlation of the spots and the pixel ON events and, hence, may cause ambiguity in the depth measurements. For example, it is seen from the earlier-mentioned Equation (1) that the depth measurement is related to the scan angle (&#x3b8;) and the pixel location of the imaged light spot, as given by the parameter q in Equation (1). Thus, if the scan angle is not correctly known for a given light spot, the depth calculation may be inaccurate. Similarly, if two or more light spots have the same value of q, the depth calculations may become ambiguous as well. The timestamp-based approach according to particular embodiments disclosed herein may be used to maintain the correct correlation between the pixel location of a captured light spot and the corresponding scan angle of the laser source. In other words, a timestamp may represent an association between the values of parameters q and &#x3b8;. Thus, if two spots land on the same pixel or column from the data output point of view, the time-to-digital conversion in the timestamping approach may allow the imaging system, that is, the digital processing block <b>167</b> (<figref idref="DRAWINGS">FIG. <b>7</b>B</figref>), to establish a temporal correlation between these two spots to identify which light spot was received first in time. Such correlation may not be easily possible in systems that do not use timestamping, such as, the earlier-discussed stereo vision systems or the systems using the structured light approach. As a result, such systems may need to perform a lot of data searching and pixel-matching to solve the correspondence problem.</p><p id="p-0092" num="0089">In one embodiment, if multiple light spots are imaged by the same pixel, timestamps of these light spots may be compared to identify the earliest-received light spot and the distance may be calculated for that light spot only, while ignoring all subsequently-received light spots at the same pixel. Thus, in this embodiment, the timestamp of the earliest-received light spot may be treated as the pixel-specific output for the corresponding pixel. Alternatively, in another embodiment, the distance may be calculated for the last spot that is received the last in time, while ignoring all other light spots imaged by the same pixel. In either case, any light spot received between the first or the last light spot may be ignored for depth calculations. Mathematically, the scan times of light spots projected by a light source may be given as t(0), t(1), . . . , t(n) in which t(i+1)&#x2212;t(i)=d(t) (constant). The pixel/column outputs may be given as a(0), a(1), . . . , a(n), which are timestamps for the ON events and a(i) is always after t(i), but before a(i+1). If a(i) and a(k) (i k) happen to be associated with the same pixel/column, only one of them may be saved as discussed before to remove any ambiguity in depth calculations. Based on the time relationship between the scan time and the output time (represented by timestamps), the processing unit, such as the digital block <b>167</b>, can figure out which output point(s) is missing. Although the processing unit may not be able to recover the missing location, the depth calculations from the available output points may suffice to provide an acceptable 3D-depth profile of the object. It is noted here that, in one embodiment, it also may be possible for two different pixels to image a respective portion of the same light spot. In that embodiment, based on the closeness of the values of the timestamp outputs from these two pixels, the processing unit may infer that a single light spot may have been imaged by two different pixels. To resolve any ambiguity, the processing unit may use the timestamps to find an average of the respective location values q, and use that average value of q in Equation (1) to calculate the 3D depth for such a shared light spot.</p><p id="p-0093" num="0090"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an exemplary Look-Up Table (LUT) <b>215</b> to illustrate how an LUT may be used in particular embodiments disclosed herein to determine 3D-depth values. The LUT-based approach may be used in place of the earlier-discussed triangulation-based depth calculations on-the-fly using the Equation (1). The LUT <b>215</b> lists the parameters &#x3b8;, q, and Z for a scan line S<sub>R</sub>. The relation among these parameters is given by Equation (1). The LUT <b>215</b> may be pre-populated with the values of these parameters for multiple scan lines&#x2014;only one of which, the scan line S<sub>R</sub>, is indicated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The pre-populated LUT <b>215</b> may be stored in the system memory <b>20</b> (<figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>), in the internal memory (not shown) of processor <b>19</b>, or within the digital processing block <b>167</b> (<figref idref="DRAWINGS">FIG. <b>7</b>A</figref>). Initially, to populate the LUT <b>215</b>, a light spot along scan line S<sub>R </sub>may be projected at a reference distance Z<sub>i</sub>, for example, 1 meter, and using a specific scan angle &#x3b8;<sub>i</sub>. These pre-determined values of Z<sub>i </sub>and &#x3b8;<sub>i </sub>may be used in Equation (1) to obtain a corresponding value of q<sub>i</sub>, which would indicate the column/pixel at which the imaged spot should appear for the scan line S<sub>R</sub>. Different values of Z<sub>i </sub>and &#x3b8;<sub>i </sub>may be used to obtain corresponding values of q<sub>i</sub>. If there is a AZ difference between the actual and pre-determined values of Z<sub>i </sub>for a light spot in scanning line S<sub>R</sub>, the corresponding column/pixel should move by &#x394;q. The values in the LUT <b>215</b> may be thus adjusted as necessary. In this manner, for each scanning line S<sub>R</sub>, the LUT <b>215</b> may be pre-populated with depth values Z<sub>i </sub>as a function of &#x3b8;<sub>i </sub>and q<sub>i </sub>using the triangulation Equation (1). As noted before, the pre-populated LUT may be stored in the device <b>15</b>. During operation, the actual values of &#x3b8;<sub>i </sub>and q<sub>i </sub>for each light spot in a scan line of light spots projected on a user-selected 3D object may be used as inputs to an LUT, like the LUT <b>215</b>, to look-up the corresponding value Z<sub>i</sub>. The processor <b>19</b> or the digital block <b>167</b> may be configured to perform such look-ups. Thus, in particular embodiments, the 3D profile of the object may be generated by interpolating into an LUT that has been calibrated using triangulation.</p><p id="p-0094" num="0091">It is observed from the foregoing discussion that the timestamp-based 3D-depth measurement using triangulation according to particular embodiments disclosed herein allows an ADC to be operated as a binary comparator with a low resolution of just a single bit, thereby consuming significantly less switching power in the ADC and, hence, conserving the system power. A high bit resolution ADC in traditional 3D sensors, on the other hand, may require more processing power. Furthermore, timestamp-based ambiguity resolution may also save system power in comparison with traditional imaging approaches that require significant processing power to search and match pixel data to resolve ambiguities. The latency is reduced as well because all depth measurements may be performed in one pass due to imaging/detection of all point-scanned light spots in a single imaging step. In particular embodiments, each pixel in the pixel array may be a single-storage pixel and, hence, can be made as small as 1 micrometer (&#x3bc;m) in size. In a single-storage pixel design, there may be only one photodiode and one junction capacitor per pixel (like the transistor <b>111</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) to integrate and store photoelectrons. On the other hand, a pixel that has one photodiode with multiple capacitors to store photoelectrons coming at different times may not be reduced to such a small size. Thus, the low-power 3D-imaging system with small sensors as per particular embodiments disclosed herein may facilitate easy implementation in mobile applications such as, but not limited to, in cameras in smartphones or tablets.</p><p id="p-0095" num="0092">As mentioned previously, the same image sensor, such as the image-sensor unit <b>24</b> in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, may be used for both 2D-imaging and 3D-depth measurements according to one embodiment disclosed herein. Such dual-mode image sensor may be, for example, part of a camera system on a mobile phone, smartphone, laptop computer, or tablet, or as part of a camera system in an industrial robot or VR equipment. In particular embodiments, there may be a mode switch on the device to allow a user to select between the traditional 2D-camera mode or the 3D-imaging mode using depth measurements as discussed before. In the traditional 2D-camera mode, in particular embodiments, the user may capture color (RGB) images or snapshots of a scene or a particular 3D object within the scene. In the 3D mode, however, the user may be able to generate a 3D image of the object based on the camera system performing the point scan-based depth measurements in the manner discussed earlier. In either mode, the same image sensor may be used in its entirety to carry out the desired imaging. In other words, each pixel in the image sensor may be used for either application-2D or 3D imaging.</p><p id="p-0096" num="0093"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a timing diagram <b>230</b> that shows exemplary timing of different signals in the system <b>15</b> of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> to generate a 2D image using a 2D-linear mode of operation according to particular embodiments disclosed herein. It is noted here that the 2D image may be an RGB image of a scene or a 3D object within the scene under ambient light illumination, which may include occasional use of a camera flash or other similar component (not shown). In contrast to the 3D imaging-related embodiments in <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>11</b></figref>, however, there may not be any illumination by the laser light source <b>33</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) in case of the 2D imaging in the embodiment of <figref idref="DRAWINGS">FIG. <b>10</b></figref>. Many signals shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> are also shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In view of the earlier detailed discussion of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, only the salient aspects of <figref idref="DRAWINGS">FIG. <b>10</b></figref> are discussed herein. It is noted here that the control signals RSEL, RST, TX, RAMP, and ADC_RST shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> are for the row of pixels containing pixels <b>106</b>-<b>108</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> and, hence, for ease of discussion, these signals are identified using the same reference numerals as the reference numerals used in <figref idref="DRAWINGS">FIG. <b>8</b></figref> despite the difference in waveforms and timing of the signals in <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>10</b></figref>. Furthermore, the illustration in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is for a single pixel, that is, the pixel <b>108</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Therefore, the PIXOUT signal <b>119</b>, the Counter Clock signal <b>204</b>, and the ADCOUT signal <b>197</b> in <figref idref="DRAWINGS">FIG. <b>10</b></figref> are shown using the same reference numerals as those for corresponding signals PIXOUT1, Counter Clock-1 and ADCOUT1 in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The pixel output <b>119</b> is generated by linearly integrating the photoelectrons collected by the pixel <b>108</b> over a pre-determined time period. As before, the discussion of <figref idref="DRAWINGS">FIG. <b>10</b></figref> in the context of pixel <b>108</b> remains applicable to corresponding signals associated with other pixels in the pixel array <b>42</b>.</p><p id="p-0097" num="0094">As noted before, in particular embodiments, each column-specific ADC, such as the ADC unit <b>175</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, may be a single-slope ADC. As in case of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, pixels in the same row may be selected and reset together, as shown by the RSEL signal <b>122</b> and the RST signal <b>128</b> in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. The column ADCs may be reset also using the common ADC_RST signal <b>192</b>. In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the reset state of the ADC associated with pixel <b>108</b> is indicated by the field <b>234</b> in the ADCOUT signal <b>197</b>. After the pixel <b>108</b> and the corresponding ADC are reset, a threshold or reference voltage level may be enabled as shown by the voltage level <b>236</b> for the Vramp signal <b>143</b>. The ramp then ramps down from this voltage level <b>236</b> to digitize the comparator offset of the ADC unit, as given by the field <b>238</b> in the ADCOUT signal <b>197</b>. In one embodiment, the clock pulses in the counter clock <b>204</b> may be used to generate a count value as the offset <b>238</b>. The clock pulses may be counted from the time the Vramp signal <b>143</b> reaches the threshold level <b>236</b> until it drops to the reset level of the pixel output&#x2014;here, the PIXOUT signal <b>119</b>. Thereafter, the respective transfer (TX) line <b>132</b> may be pulsed to trigger the transfer of charge accumulated on the photodiode <b>110</b> to the floating diffusion <b>111</b> for readout. While the TX pulse is asserted, the Vramp signal <b>143</b> may rise to the threshold level <b>236</b> and a counter in the pixel-specific ADC, such as the counter <b>181</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, may be initialized with an inverted offset value as indicated by the field <b>240</b>. The inverted offset value <b>240</b> may represent the negative of the offset value <b>238</b>. After the TX pulse <b>132</b> is de-asserted, the ADC unit for the pixel <b>108</b> may start digitizing the received pixel signal (PIXOUT) until the Vramp threshold <b>143</b> drops to the level of the PIXOUT signal <b>119</b>. This operation is illustrated by the &#x201c;up count&#x201d; field <b>242</b> in the ADCOUT signal <b>197</b>. The count value <b>242</b> may be based on the clock pulses of the counter clock <b>204</b> and may represent a combined value including the offset count (at field <b>238</b>) and the pixel-specific portion of the image signal for pixel <b>108</b>, as illustrated using the reference numeral <b>243</b>. A comparator (not shown) in the ADC unit may compare the comparator offset value digitized at field <b>238</b> against the &#x201c;up count&#x201d; value <b>242</b>. Thus, in one embodiment, the RGB image signal <b>244</b> may be obtained by adding the ADC values in the fields <b>240</b> and <b>242</b>, thereby effectively removing the offset value <b>238</b> from the combined value (offset+signal) in the &#x201c;up count&#x201d; field <b>242</b>.</p><p id="p-0098" num="0095">The operation illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> may be performed for each pixel in the pixel array <b>42</b>. Each column ADC may generate a corresponding RGB image signal in the form of a multi-bit output from the ADC-based counter, such as the counter <b>181</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>. The multi-bit output, like the output at reference numeral <b>244</b> in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, may be needed to effectively represent the color content of the image signal. The RGB image signal outputs from the ADCs in the column ADC unit <b>153</b> may be collectively represented by the Dout signal <b>140</b> (<figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref>), which may be processed by the digital block <b>167</b> to present the 2D color image of the scene via the MIPI interface <b>168</b>.</p><p id="p-0099" num="0096">Additional details of the 2D imaging and related waveforms shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> may be obtained from the U.S. Pat. No. 7,990,304 B2 issued on Aug. 2, 2011 to Lim et al. The 2D imaging-related discussion in U.S. Pat. No. 7,990,304 related to the subject matter disclosed herein is incorporated herein by reference in its entirety.</p><p id="p-0100" num="0097"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a timing diagram <b>250</b> that shows exemplary timing of different signals in the system <b>15</b> of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> to generate timestamp-based pixel-specific outputs in a 3D-logarithmic (log) mode of operation according to particular embodiments disclosed herein. As mentioned earlier, the 3D-depth measurements may be performed using a 3D-linear mode or a 3D-logarithmic mode depending on the level of ambient light. Furthermore, during the 3D-depth measurements, a 3D object, such as the 3D object <b>26</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, may be illuminated by the ambient light as well as by the visible light (or other light, such as, NIR light) from the laser scan. Therefore, the 3D-logarithmic mode may be used for depth measurements if ambient light is too strong to be rejected by the 3D-linear mode. In view of the CDS-based imaging to remove the offset or other noise from the final image signal, a logarithmic mode may not be needed for the 2D imaging-related waveforms depicted in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. In case of the 3D-depth measurements according to particular embodiments disclosed herein, however, a strong ambient light may interfere with the light from the laser light source during point scans. In the 3D-linear mode of operation, such interference may overwhelm or suppress the visible/NIR light reflected from a point-scanned light spot and, hence, may result in an inaccurate detection of the light received from the light spot. Hence, in particular embodiments, it may be desirable to reject the pixel charge attributable to the ambient light if the intensity of the ambient light is sensed to be above a pre-determined illuminance level (or intensity threshold), such as <b>10000</b> (<b>10</b>K) lux. Such ambient light rejection may be accomplished using the 3D-log mode of operation shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0101" num="0098">As before, the same reference numerals are used in <figref idref="DRAWINGS">FIGS. <b>8</b>, <b>10</b> and <b>11</b></figref> to refer to the similarly-named signals (or signals having similar functionality) and also for ease of discussion. It is understood, however, that the signals shown in <figref idref="DRAWINGS">FIGS. <b>8</b>, <b>10</b> and <b>11</b></figref> relate to specific modes of imaging. Thus, for example, the timing diagram <b>230</b> shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates a specific relationship among the signals shown therein if a user selects a 2D color imaging mode of operation. The similarly-named signals in <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>11</b></figref>, however, relate to a 3D-imaging mode of operation and, hence, may have different timing relationships. Furthermore, even between <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>11</b></figref>, some signals may differ in waveforms because <figref idref="DRAWINGS">FIG. <b>8</b></figref> relates to a 3D-linear mode of operation, whereas <figref idref="DRAWINGS">FIG. <b>11</b></figref> relates to a 3D-log mode of operation. In view of the earlier detailed discussion of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, only the salient aspects of <figref idref="DRAWINGS">FIG. <b>11</b></figref> are discussed herein. Like <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the timing diagram <b>250</b> in <figref idref="DRAWINGS">FIG. <b>11</b></figref> is also with reference to pixels <b>107</b> and <b>108</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The discussion of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, however, remains applicable to all other pixels in the pixel array <b>42</b>.</p><p id="p-0102" num="0099">In the 3D-linear mode, the pixel-specific output may be generated by linearly integrating the photoelectrons collected by the pixel over a pre-determined time period. Thus, in the linear mode, an output voltage of a pixel is proportional to the total photons collected/integrated over a given time period. In the 3D-log mode, however, the pixel-specific output may be proportional to the natural logarithm of an instantaneous photo-current produced by the pixel during the pre-determined time period upon detecting the laser light reflected from the 3D object. Mathematically, the photo current generated by a photodiode, such as the PPD <b>110</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, may be represented by the following relationship:</p><p id="p-0103" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>I</mi>       <mrow>        <mi>p</mi>        <mo>&#x2062;</mo>        <mi>h</mi>       </mrow>      </msub>      <mo>&#x221d;</mo>      <msup>       <mi>e</mi>       <mfrac>        <msub>         <mi>V</mi>         <mrow>          <mi>p</mi>          <mo>&#x2062;</mo>          <mi>h</mi>         </mrow>        </msub>        <msub>         <mi>V</mi>         <mi>T</mi>        </msub>       </mfrac>      </msup>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0104" num="0000">in which I<sub>ph </sub>is the photocurrent of the diode, V<sub>ph </sub>is the voltage across the diode, and V<sub>T </sub>is the thermal voltage. Thus, V<sub>ph </sub>and, hence, the respective pixel output (PIXOUT) may be made proportional to the natural logarithm of the instantaneous diode current I<sub>ph</sub>, such as if ambient light rejection is desired. As noted before, heavy ambient light may restrict photon collection if linear integration is done. Hence, in such circumstances, the sensing of instantaneous photocurrent using the 3D-log mode may be more desirable.</p><p id="p-0105" num="0100">In particular embodiments, the device <b>15</b> may include an ambient light sensor (not shown). The processor <b>19</b> or the digital block <b>167</b> may be configured to sense the ambient light intensity as soon as the 3D-imaging mode is selected by the user to determine whether to use the 3D-linear mode or the 3D-log mode. In one embodiment, the ambient light level may be sensed substantially simultaneously with the assertion of an RSEL signal, which may indicate the initiation of the imaging of the light reflected from the point-scanned light spots. In another embodiment, the ambient light level may be sensed substantially simultaneously with the initiation of the visible-light point scan by the laser source. Based on the level of the ambient light, the processor <b>19</b> or the digital block <b>167</b> may choose either the 3D-linear mode or the 3D-log mode of depth measurements. In a still further embodiment, the ambient light level may be sensed periodically and continuously during a 3D-depth measurement. In that case, the 3D mode of operation may be switched from linear to logarithmic, and vice versa, at any time prior to or during an ongoing imaging operation. Other approaches for sensing the ambient light level may be suitably devised.</p><p id="p-0106" num="0101">Referring now to the embodiment of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, it is observed that, in the 3D-logarithmic mode, the row-specific RST signal <b>128</b> may be asserted (or turned on &#x201c;high&#x201d;) and may remain high/asserted for the selected row during the entire period of generation of the pixel output. In contrast, in the 3D-linear mode of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the RST signal <b>128</b> may be initially asserted (or turned on &#x201c;high&#x201d;) to reset the pixels in the row to a pre-determined voltage level, but later turned off (or de-asserted) during linear integration of the photoelectrons. The TX signal <b>132</b>, however, may remain high, like in case of the 3D-linear mode of <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Thus, in particular embodiments, the appropriate level of the RST signal may be used to select the linear mode versus the logarithmic mode. In the logarithmic mode, in one embodiment, after the ADCs associated with pixels <b>107</b> and <b>108</b> are reset using the ADC RST signal <b>192</b>, the ADCs may initially sample the ambient level to enable the ADCs to appropriately account for the signal levels of the pixel output (PIXOUT) signals when the signals are received. After ADCs are reset, the RAMP threshold <b>143</b> may be enabled, and the ADC counters may enter a &#x201c;wait state&#x201d; to wait for an &#x201c;ON event&#x201d; to occur at the respective pixel. If a pixel receives incident light (reflected from a projected light spot), the PIXOUT signal of the pixel may start drooping. In contrast to the linear drop in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the PIXOUT signals <b>118</b> and <b>119</b> in <figref idref="DRAWINGS">FIG. <b>11</b></figref> may respectively exhibit short, instantaneous drops <b>252</b> and <b>253</b>, which reflect the instantaneous photo-current produced by the detection of the reflected visible light by the respective pixel. If the PIXOUT signals <b>118</b> and <b>119</b> reach the pre-determined Vramp threshold <b>143</b>, the ADC counters may start counting. All counters may stop simultaneously after a pre-determined time for charge integration is over, as given by the transition of the RAMP signal <b>143</b> to the &#x201c;high&#x201d; state and as indicated by the dotted line <b>255</b>. The counted values are indicated by the data field <b>257</b> of ADCOUT1 and the data field <b>259</b> of the ADCOUT2 signals for pixels <b>108</b> and <b>107</b>, respectively. The count values in the logarithmic mode may be different from those in the linear mode and, hence, different reference numerals are used for the &#x201c;up count&#x201d; fields in the ADCOUT signals in <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>11</b></figref>. As in case of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, pixel scanned later may have a smaller count value for the ADC output than the one that is scanned earlier.</p><p id="p-0107" num="0102">As mentioned earlier with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, instead of per column up-counters, down counters may be used in the ADC units in the embodiments of <figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref>. Similarly, a global counter-based approach may be implemented instead of individual ADC-specific counters.</p><p id="p-0108" num="0103">Thus, as discussed above, the same image sensor (and all of the pixels in the corresponding pixel array) may be used as per the subject matter disclosed herein for routine 2D imaging as well as for 3D-depth measurements. In the 2D mode, the sensor may work in the linear mode as a regular 2D sensor. During the 3D-depth measurements, however, the sensor may operate in a linear mode under moderate ambient light, but may switch to a logarithmic mode of signal detection under strong ambient light to be able to use the visible (or NIR) light source. Thus, the imaging approaches discussed herein may be compatible with existing 2D-sensor designs because the same 4T PPD pixel may be used for both 2D and 3D imaging. This allows for the sensor design to be small in size (with smaller pixels), more versatile, and operable at low power. These attributes, in turn, save space and cost for mobile devices containing such an image sensor. Furthermore, in consumer mobile devices and certain other applications, the usage of visible light laser (in addition to the ambient light) for 3D-depth measurements may be better for eye safety than conventional Near Infrared (NIR) sensors. At the visible spectrum, the sensor may have higher quantum efficiency than at the NIR spectrum, leading to lower power consumption for the light source, which, in turn, conserves power in the mobile devices.</p><p id="p-0109" num="0104">One exemplary embodiment disclosed herein comprises a 2D pixel array in which 2D color image information and 3D-depth information are concurrently obtained to provide a fully synchronized frame rate, phase of color, depth and point of view. In one exemplary embodiment, the color image information and the 3D-depth information are output from the rows of the 2D pixel array in an interleaved or alternating-type of manner. That is, color image information is output from a first selected row followed by depth information output from the same row, then color image information is output from a next selected row followed by depth information output from the same next row, and so on. Alternatively, depth information is output from a first selected row followed by color image information output from the same row, then depth information is output from the next selected row followed by color image information output from the same next row, and so on.</p><p id="p-0110" num="0105">According to the subject matter disclosed herein, the exemplary embodiments depicted in <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>, <b>6</b>, <b>7</b>A and <b>7</b>B</figref> may comprise a 2D pixel array in which 2D color image information and 3D-depth information are concurrently obtained to provide a fully synchronized frame rate, phase of color, depth and point of view. In one exemplary embodiment an image sensor unit <b>24</b> comprises a two-dimensional (2D) pixel array arranged in a plurality of rows in which each pixel of the array is substantially identical to other pixels of the array. In another exemplary embodiment, there may be one or more pixels of the array that are not substantially identical to other pixels of the array. In one exemplary embodiment, the rows of the array are operable to generate both 2D-color information of an object being imaged, as described herein, and are operable to generate 3D-depth information of the object, as described herein. In another exemplary embodiment, one or more of the rows of the array are operable to generate both 2D-color information and 3D-depth information, and other rows of the array are operable to generate either 2D-color information or 3D-depth information, but not both types of information. In another exemplary embodiment, the particular rows that are scanned for 2D-color information and/or 3D-depth information may be fewer than the total number of rows of the 2D pixel array. In one exemplary embodiment, concurrently generating both 2D-color information and 3D-depth information does not require a frame buffer because digital signal processing of the output information is not required, and the information is output shortly after it is acquired.</p><p id="p-0111" num="0106"><figref idref="DRAWINGS">FIG. <b>13</b></figref> depicts an exemplary flowchart of a process <b>300</b> to concurrently generate and obtain 2D-color information and 3D-depth information according to embodiments disclosed herein. The various operations illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref> may be performed by a single module or a combination of modules or system components in, for example, the system <b>15</b>. In the discussion herein, by way of an example only, specific tasks are described as being performed by specific modules or system components. Other modules or system components may be suitably configured to perform such tasks as well.</p><p id="p-0112" num="0107">The process starts at block <b>301</b>. At block <b>302</b>, the system <b>15</b> (or more specifically, the processor <b>19</b>) may perform a 2D-color image capture of an object, such as the object <b>26</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>), along a first row of the 2D pixel array <b>42</b> (<figref idref="DRAWINGS">FIG. <b>42</b></figref>). In one exemplary embodiment, the first row may be the first physical row (corresponding to either the first or the last row as depicted in, for example, <figref idref="DRAWINGS">FIG. <b>6</b></figref>) of the 2D pixel array <b>42</b>. In another exemplary embodiment, the first row may be different from the first physical row (corresponding to either the first or the last row as depicted in, for example, <figref idref="DRAWINGS">FIG. <b>6</b></figref>) of 2D pixel array. In one exemplary embodiment, the color image information is read out from the 2D pixel array <b>42</b>, as described in connection with <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0113" num="0108">At block <b>303</b>, the system <b>15</b> may perform a one-dimensional (1D) point scan of the 3D object, such as the object <b>26</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, along a scanning line using a light source, such as the light-source module <b>22</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In one exemplary embodiment, the selected scanning line corresponds to a second row that may correspond to the same row that was scanned for the color image information in block <b>302</b>. In another exemplary embodiment, the selected scanning line corresponds to a second row that may not correspond to the same row that was scanned for the color image information in block <b>302</b>. The 3D-depth information is read out from the 2D pixel array <b>42</b>, as described in connection with <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In one exemplary embodiment, the order of blocks <b>302</b> and <b>303</b> could be reversed.</p><p id="p-0114" num="0109">At block <b>304</b>, the system <b>15</b> determines whether all of the rows of the 2D pixel array have been scanned for both color image information and for 3D-depth information. If not, the process flows to block <b>305</b> where the indices corresponding to the rows of the color image information scan and the rows of the 3D-depth information scan are incremented (or decremented as the case may be), and the process returns to block <b>302</b>. In an exemplary embodiment in which the 2D-color information and the 3D-depth information are obtained from the same row, the indices could be the same index. In an exemplary embodiment in which the 2D-color information and the 3D-depth information are obtained from different rows, the indices may be different. If, at block <b>304</b>, it is determined that all of the rows of the 2D pixel array have been scanned for both color image information and for 3D-depth information, flow continues to block <b>306</b> where the process ends.</p><p id="p-0115" num="0110">In one exemplary embodiment in which the number of rows outputting 2D-color information is greater than the number of rows outputting 3D-depth information, a selected number of rows of color image information could be output for each row of 3D-depth information in an interleaved or alternating-type of manner.</p><p id="p-0116" num="0111">In one exemplary embodiment, the same row R (or column C) of could be scanned multiple times to adaptively adjust the timing and/or the intensity of the point illuminations output from the laser light source (i.e., adjust the timing of the laser pulses) so better coincide with the response time of each particular pixel in a row and with the mechanical characteristics of the projection optics <b>35</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>). Such a technique could be used to calibrate the imaging module <b>17</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0117" num="0112"><figref idref="DRAWINGS">FIG. <b>14</b></figref> depicts an exemplary illustration of how the distance to a semi-transparent object <b>401</b>, such as glass, and the distance to an object <b>402</b> behind the semi-transparent object <b>401</b> may be performed for 3D-depth measurements according to one embodiment disclosed herein. In <figref idref="DRAWINGS">FIG. <b>14</b></figref>, an X-Y addressable light source <b>403</b>, such as a laser light source, point scans object <b>402</b> through the semi-transparent object <b>401</b>. Reflection <b>404</b> from the semi-transparent object <b>401</b> and reflection <b>405</b> from the object <b>402</b> pass through lens <b>406</b> and are respectively detected by pixels <b>407</b> and <b>408</b> in a row R of the 2D pixel array. Both detected reflections <b>407</b> and <b>408</b> information will have substantially the same timestamps, and the output depth of both reflections can be determined, as disclosed herein.</p><p id="p-0118" num="0113"><figref idref="DRAWINGS">FIG. <b>15</b></figref> depicts an exemplary illustration of how depth imaging of a semi-transparent medium <b>501</b>, such as fog, rain, etc., may be performed for 3D-depth measurements according to one embodiment disclosed herein. As an X-Y addressable light source <b>503</b>, such as a laser light source, point scans the semi-transparent medium <b>501</b>, reflections <b>504</b> will pass through a lens <b>506</b> and a range of pixels <b>507</b> in a row R of the 2D pixel array will be activated that will have substantially the same timestamp. The thickness of medium <b>501</b> can be determined based on the timestamps as disclosed herein.</p><p id="p-0119" num="0114"><figref idref="DRAWINGS">FIG. <b>16</b></figref> depicts an exemplary illustration of how depth imaging of an object <b>601</b> may be performed for 3D-depth measurements in the presence of multiple return paths according to one embodiment disclosed herein. As an X-Y addressable light source <b>602</b>, such as a laser light source, point scans the shiny object <b>601</b> a stray reflection <b>603</b> may be returned from another object <b>604</b>. In this situation, the stray reflection <b>603</b> would likely not be within the epipolar plane of the row R being scanned and, consequently, would not be detected as a reflection <b>605</b> from the object <b>601</b> on which the point scan was directed.</p><p id="p-0120" num="0115"><figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts an overall configuration of the system <b>15</b> in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> according to one embodiment disclosed herein. Hence, for ease of reference and discussion, the same reference numerals are used in <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>12</b> and <b>12</b></figref> for the common system components/units.</p><p id="p-0121" num="0116">As discussed earlier, the imaging module <b>17</b> may include the hardware shown in the exemplary embodiments of <figref idref="DRAWINGS">FIGS. <b>2</b>, <b>6</b>, <b>7</b>A, <b>7</b>B and <b>13</b></figref> to accomplish 2D imaging and 3D-depth measurements as per the subject matter disclosed herein. The processor <b>19</b> may be configured to interface with a number of external devices. In one embodiment, the imaging module <b>17</b> may function as an input device that provides data inputs, that is, in the form of pixel event data such as, the processed data output <b>170</b> in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, to the processor <b>19</b> for further processing. The processor <b>19</b> may also receive inputs from other input devices (not shown) that may be part of the system <b>15</b>. Some examples of such input devices include a computer keyboard, a touchpad, a touch-screen, a joystick, a physical or virtual &#x201c;clickable button,&#x201d; and/or a computer mouse/pointing device. In <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the processor <b>19</b> is shown coupled to the system memory <b>20</b>, a peripheral storage unit <b>265</b>, one or more output devices <b>267</b>, and a network interface unit <b>268</b>. In <figref idref="DRAWINGS">FIG. <b>12</b></figref>, a display unit is shown as an output device <b>267</b>. In some embodiments, the output device <b>267</b> may comprise a touchscreen display. In some embodiments, the system <b>15</b> may include more than one instance of the devices shown. Some examples of the system <b>15</b> include a computer system (desktop or laptop), a tablet computer, a mobile device, a cellular phone, a video gaming unit or console, a machine-to-machine (M2M) communication unit, a robot, an automobile, a virtual reality equipment, a stateless &#x201c;thin&#x201d; client system, a vehicle dash-cam or rearview camera system, or any other type of computing or data processing device. In various embodiments, all of the components shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref> may be housed within a single housing. Thus, the system <b>15</b> may be configured as a standalone system or in any other suitable form factor. In some embodiments, the system <b>15</b> may be configured as a client system rather than a server system.</p><p id="p-0122" num="0117">In particular embodiments, the system <b>15</b> may include more than one processor (e.g., in a distributed processing configuration). If the system <b>15</b> is a multiprocessor system, there may be more than one instance of the processor <b>19</b> or there may be multiple processors coupled to the processor <b>19</b> via their respective interfaces (not shown). The processor <b>19</b> may be a System on Chip (SoC) and/or may include more than one Central Processing Unit (CPU).</p><p id="p-0123" num="0118">As mentioned earlier, the system memory <b>20</b> may be any semiconductor-based storage system such as, but not limited to, DRAM, SRAM, PRAM, RRAM, CBRAM, MRAM, STT-MRAM, and the like. In some embodiments, the memory unit <b>20</b> may include at least one 3DS memory module in conjunction with one or more non-3DS memory modules. The non-3DS memory may include Double Data Rate or Double Data Rate 2, 3 or 4 Synchronous Dynamic Random Access Memory (DDR/DDR2/DDR3/DDR4 SDRAM), or Rambus&#xae; DRAM, flash memory, various types of Read Only Memory (ROM), etc. Also, in some embodiments, the system memory <b>20</b> may include multiple different types of semiconductor memories, as opposed to a single type of memory. In other embodiments, the system memory <b>20</b> may be a non-transitory data storage medium.</p><p id="p-0124" num="0119">The peripheral storage unit <b>265</b>, in various embodiments, may include support for magnetic, optical, magneto-optical, or solid-state storage media, such as hard drives, optical disks (such as Compact Disks (CDs) or Digital Versatile Disks (DVDs)), non-volatile Random Access Memory (RAM) devices, and the like. In some embodiments, the peripheral storage unit <b>265</b> may include more complex storage devices/systems, such as disk arrays (which may be in a suitable RAID (Redundant Array of Independent Disks) configuration) or Storage Area Networks (SANs), and the peripheral storage unit <b>265</b> may be coupled to the processor <b>19</b> via a standard peripheral interface, such as a Small Computer System Interface (SCSI) interface, a Fibre Channel interface, a Firewire&#xae; (IEEE 1394) interface, a Peripheral Component Interface Express (PCI Express&#x2122;) standard based interface, a Universal Serial Bus (USB) protocol based interface, or another suitable interface. Various such storage devices may be non-transitory data storage media.</p><p id="p-0125" num="0120">The display unit <b>267</b> may be an example of an output device. Other examples of an output device include a graphics/display device, a computer screen, an alarm system, a CAD/CAM (Computer Aided Design/Computer Aided Machining) system, a video game station, a smartphone display screen, or any other type of data output device. In some embodiments, the input device(s), such as the imaging module <b>17</b>, and the output device(s), such as the display unit <b>267</b>, may be coupled to the processor <b>19</b> via an I/O or peripheral interface(s).</p><p id="p-0126" num="0121">In one embodiment, the network interface <b>268</b> may communicate with the processor <b>19</b> to enable the system <b>15</b> to couple to a network (not shown). In another embodiment, the network interface <b>268</b> may be absent altogether. The network interface <b>268</b> may include any suitable devices, media and/or protocol content for connecting the system <b>15</b> to a network, whether wired or wireless. In various embodiments, the network may include Local Area Networks (LANs), Wide Area Networks (WANs), wired or wireless Ethernet, telecommunication networks, or other suitable types of networks.</p><p id="p-0127" num="0122">The system <b>15</b> may include an on-board power supply unit <b>270</b> to provide electrical power to various system components illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>. The power supply unit <b>270</b> may receive batteries or may be connectable to an AC electrical power outlet. In one embodiment, the power supply unit <b>270</b> may convert solar energy into electrical power.</p><p id="p-0128" num="0123">In one embodiment, the imaging module <b>17</b> may be integrated with a high-speed interface such as, but not limited to, a Universal Serial Bus 2.0 or 3.0 (USB 2.0 or 3.0) interface or above, that plugs into any Personal Computer (PC) or laptop. A non-transitory, computer-readable data storage medium, such as, but not limited to, the system memory <b>20</b> or a peripheral data storage unit, such as a CD/DVD may store program code or software. The processor <b>19</b> and/or the digital processing block <b>167</b> (<figref idref="DRAWINGS">FIG. <b>7</b>A</figref>) in the imaging module <b>17</b> may be configured to execute the program code, whereby the device <b>15</b> may be operative to perform the 2D imaging and 3D-depth measurements as discussed hereinbefore, such as the operations discussed earlier with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>11</b> and <b>13</b>-<b>16</b></figref>. The program code or software may be proprietary software or open source software which, upon execution by the appropriate processing entity, such as the processor <b>19</b> and/or the digital block <b>167</b>, may enable the processing entity to capture pixel events using their precise timing, process them, render them in a variety of formats, and display them in the 2D and/or 3D formats. As noted earlier, in certain embodiments, the digital processing block <b>167</b> in the imaging module <b>17</b> may perform some of the processing of pixel event signals before the pixel output data are sent to the processor <b>19</b> for further processing and display. In other embodiments, the processor <b>19</b> may also perform the functionality of the digital block <b>167</b>, in which case, the digital block <b>167</b> may not be a part of the imaging module <b>17</b>.</p><p id="p-0129" num="0124">In the preceding description, for purposes of explanation and not limitation, specific details are set forth (such as particular architectures, waveforms, interfaces, techniques, etc.) in order to provide a thorough understanding of the disclosed technology. It will be apparent, however, to those skilled in the art that the disclosed technology may be practiced in other embodiments that depart from these specific details. That is, those skilled in the art will be able to devise various arrangements which, although not explicitly described or shown herein, embody the principles of the disclosed technology. In some instances, detailed descriptions of well-known devices, circuits, and methods are omitted not to obscure the description of the disclosed technology with unnecessary detail. All statements herein reciting principles, aspects, and embodiments of the disclosed technology, as well as specific examples thereof, are intended to encompass both structural and functional equivalents thereof. Additionally, it is intended that such equivalents include both currently known equivalents as well as equivalents developed in the future, such as any elements developed that perform the same function, regardless of structure.</p><p id="p-0130" num="0125">Thus, for example, it will be appreciated by those skilled in the art that block diagrams herein (e.g., in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>) can represent conceptual views of illustrative circuitry or other functional units embodying the principles of the technology. Similarly, it will be appreciated that the flow chart in <figref idref="DRAWINGS">FIG. <b>3</b></figref> represents various processes which may be substantially performed by a processor (e.g., the processor <b>19</b> in <figref idref="DRAWINGS">FIG. <b>12</b></figref> and/or the digital block <b>167</b> in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>). Such processor may include, by way of example, a general-purpose processor, a special-purpose processor, a conventional processor, a digital signal processor (DSP), a plurality of microprocessors, one or more microprocessors in association with a DSP core, a controller, a microcontroller, Application Specific Integrated Circuits (ASICs), Field Programmable Gate Arrays (FPGAs) circuits, any other type of integrated circuit (IC), and/or a state machine. Some or all of the functionalities described above in the context of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>11</b> and <b>13</b>-<b>16</b></figref> also may be provided by such processor, in the hardware and/or software.</p><p id="p-0131" num="0126">If certain aspects of the subject matter disclosed herein require software-based processing, such software or program code may reside in a computer-readable data-storage medium. As noted earlier, such data storage medium may be part of the peripheral storage <b>265</b> or may be part of the system memory <b>20</b> or the internal memory (not shown) of processor <b>19</b>. In one embodiment, the processor <b>19</b> or the digital block <b>167</b> may execute instructions stored on such a medium to carry out the software-based processing. The computer-readable data storage medium may be a non-transitory data-storage medium containing a computer program, software, firmware, or microcode for execution by a general-purpose computer or a processor mentioned above. Examples of computer-readable storage media include a ROM, a RAM, a digital register, a cache memory, semiconductor memory devices, magnetic media, such as internal hard disks, magnetic tapes and removable disks, magneto-optical media, and optical media, such as CD-ROM disks and DVDs.</p><p id="p-0132" num="0127">Alternative embodiments of the imaging module <b>17</b> or the system <b>15</b> comprising such an imaging module according to aspects of the subject matter disclosed herein may include additional components responsible for providing additional functionality, including any of the functionality identified above and/or any functionality necessary to support the solution as per the subject matter disclosed herein. Although features and elements are described above in particular combinations, each feature or element can be used alone without the other features and elements or in various combinations with or without other features. As mentioned before, various 2D- and 3D-imaging functions discussed herein may be provided through the use of hardware (such as circuit hardware) and/or hardware capable of executing software/firmware in the form of coded instructions or microcode stored on a computer-readable data-storage medium (mentioned above). Thus, such functions and illustrated functional blocks are to be understood as being either hardware-implemented and/or computer-implemented, and thus machine-implemented.</p><p id="p-0133" num="0128">The foregoing describes a system and method in which the same image sensor, that is, all of the pixels in the image sensor, may be used to capture both a 2D image of a 3D object and 3D-depth measurements for the object. The image sensor may be part of a camera in a mobile device such as, but not limited to, a smartphone. A laser light laser source may be used to point scan the surface of the object with light spots, which may be then detected by a pixel array in the image sensor to generate the 3D-depth profile of the object using triangulation. In the 3D mode, the laser may project a sequence of light spots on the surface of the object along a scan line. The illuminated light spots may be detected using a row of pixels in the pixel array such that the row forms an epipolar line of the scan line. The detected light spots may be timestamped to remove any ambiguity in triangulation and, hence, to reduce the amount of depth computation and system power. A timestamp may also provide a correspondence between the pixel location of a captured laser spot and the respective scan angle of the laser light source to determine depth using triangulation. The image signals in the 2D mode may be represented by a multi-bit output from an ADC unit in the image sensor, but the ADC unit may produce just a binary output to generate timestamp values for 3D-depth measurements. To reject strong ambient light, the image sensor may be operated in a 3D-logarithmic mode as opposed to a 3D-linear mode.</p><p id="p-0134" num="0129">As will be recognized by those skilled in the art, the innovative concepts described herein can be modified and varied over a wide range of applications. Accordingly, the scope of patented subject matter should not be limited to any of the specific exemplary teachings discussed above, but is instead defined by the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007175A1-20230105-M00001.NB"><img id="EMI-M00001" he="6.01mm" wi="76.20mm" file="US20230007175A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230007175A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.25mm" wi="76.20mm" file="US20230007175A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image sensor unit, comprising:<claim-text>a single two-dimensional (2D) pixel array arranged in a first predetermined number of rows of pixels, at least one first pixel in a first row of pixels being capable of generating 2D-color information of an object based on a first light reflected from the object and the at least one first pixel being capable of generating 3D-depth information of the object based on a second light reflecting from the object, the 3D-depth information being based on time-of-flight information of the object from the 2D pixel array; and</claim-text><claim-text>a controller coupled to the single 2D pixel array, the controller configured to control the 2D pixel array to output the 2D-color information and the 3D-depth information for the object by continuously selecting the at least one first pixel in the first row during a predetermined period of time and controlling the at least one first pixel to output the 2D-color information and to output the 3D-depth information while the at least one first pixel is continuously selected during the predetermined period of time.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image sensor unit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 2D-color information is output from the at least one first pixel before the 3D-depth information is output from the at least one first pixel.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image sensor unit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 3D-depth information is output from the at least one first pixel before the 2D-color information is output from the at least one first pixel.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image sensor unit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising at least one second pixel in a second row of pixels capable of generating 2D-color information of the object based the first light reflected from the object and capable of generating 3D-depth information of the object based on the second light reflecting from the object, the second row being different from the first row, and<claim-text>wherein the controller is further configured to control the 2D pixel array to output the 2D-color information and the 3D-depth information for the object by selecting the at least one second pixel in the second row to output the 2D-color information and to output the 3D-depth information while at least one second pixel is selected.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The image sensor unit of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein a first group of rows of pixels comprise a second predetermined number of rows, a second group of rows of pixels comprise a third pre-determined number of rows, the second predetermined number being equal to or less than the first predetermined number, the third predetermined number being equal to or less than the second predetermined number,<claim-text>wherein the first row of pixels is in the first group of rows of pixels and the second row of pixels is in the second group of rows of pixels, and</claim-text><claim-text>wherein the controller is further configured to sequentially select, in an alternating manner, the first row from the first group of rows to output the 2D-color information and the 3D-depth information generated by the at least one first pixel of the first row and the second row from the second group of rows to output the 2D-color information and the 3D-depth information generated by the at least one second pixel.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The image sensor unit of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the first row selected from the first group of rows is a same row as the second row selected from the second group of rows.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The image sensor unit of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the first row selected from the first group of rows is different from the second row selected from the second group of rows.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The image sensor unit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 3D-depth information comprises triangulation information corresponding to a scanned row of light spots reflected off the object.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The image sensor unit of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the triangulation information comprises timestamp information for the scanned row of light spots.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The image sensor unit of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the 3D-depth information is based on a linear integration of photoelectrons generated by the at least one first pixel.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The image sensor unit of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the 3D-depth information is based on a logarithmic integration of photoelectrons generated by the at least one first pixel.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The image sensor unit of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising a laser light source to illuminate the scanned row of light spots.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The image sensor unit of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the laser light source comprises one of a visible laser light source, a near infrared laser light source, a point light source, a monochromatic illumination source, an X-Y addressable laser light source, or a Micro Electro-Mechanical System (MEMS) based laser scanner.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A system, comprising:<claim-text>a single two-dimensional (2D) pixel array arranged in a first predetermined number of rows of pixels, at least one first pixel in a first row of pixels being capable of generating 2D-color information of an object based on a first light reflected from the object and the at least one first pixel being capable of generating 3D-depth information of the object based on a second light reflecting from the object, the 3D-depth information being based on triangulation information corresponding to a scanned row of light spots reflected off the object;</claim-text><claim-text>a controller coupled to the single 2D pixel array, the controller configured to control the 2D pixel array to output the 2D-color information and the 3D-depth information for the object by continuously selecting the at least one first pixel in the first row during a predetermined period of time and controlling the at least one pixel to output the 2D-color information and to output the 3D-depth information while the at least one first pixel is continuously selected during the predetermined period of time; and</claim-text><claim-text>a display coupled to the single 2D pixel array and the controller, the display to display a first image of the object based on the 2D-color information and a second image of the object based on the 3D-depth information.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the 3D-depth information is further based on time-of-flight information of the object from the 2D pixel array, and<claim-text>wherein the triangulation information comprises timestamp information for the scanned row of light spots.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the 3D-depth information is further based on a linear integration of photoelectrons generated by the at least one first pixel.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the 3D-depth information is based on a logarithmic integration of photoelectrons generated by the at least one first pixel.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising a laser light source to illuminate the scanned row of light spots,<claim-text>wherein the laser light source comprises one of a visible laser light source, a near infrared laser light source, a point light source, a monochromatic illumination source, an X-Y addressable laser light source, or a Micro Electro-Mechanical System (MEMS) based laser scanner.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the system comprises part of a mobile communication device.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising at least one second pixel in a second row of pixels capable of generating 2D-color information of the object based the first light reflected from the object and capable of generating 3D-depth information of the object based on the second light reflecting from the object, the second row being different from the first row,<claim-text>wherein a first group of rows of pixels comprise a second predetermined number of rows, a second group of rows of pixels comprise a third predetermined number of rows, the second predetermined number being equal to or less than the first predetermined number, the third pre-determined number being equal to or less than the second predetermined number,</claim-text><claim-text>wherein the first row of pixels is in the first group of rows of pixels and the second row of pixels is in the second group of rows of pixels, and</claim-text><claim-text>wherein the controller sequentially selects, in an alternating manner, the first row from the first group of rows to output the 2D-color information and the 3D-depth information generated by the at least one first pixel of the first row and the second row from the second group of rows to output the 2D-color information and the 3D-depth information generated by the at least one second pixel.</claim-text></claim-text></claim></claims></us-patent-application>