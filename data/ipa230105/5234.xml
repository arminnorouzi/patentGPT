<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005235A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005235</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17829377</doc-number><date>20220601</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>TW</country><doc-number>111100227</doc-number><date>20220104</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>24</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>262</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>243</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>006</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>2628</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23238</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE PROCESSING SYSTEM AND PROCESSING METHOD OF VIDEO STREAM</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217770</doc-number><date>20210702</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>GENESYS LOGIC, INC.</orgname><address><city>New Taipei City</city><country>TW</country></address></addressbook><residence><country>TW</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Lin</last-name><first-name>Wen-Hsiang</first-name><address><city>New Taipei City</city><country>TW</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Lin</last-name><first-name>Sheng-Yuan</first-name><address><city>New Taipei City</city><country>TW</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Tsai</last-name><first-name>Mi-Lai</first-name><address><city>New Taipei City</city><country>TW</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>GENESYS LOGIC, INC.</orgname><role>03</role><address><city>New Taipei City</city><country>TW</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An image processing system and a processing method of video stream are provided. The first image is obtained according to a parameter. The deformation correction procedure is performed on the first image, and the second image is generated. The identification detection procedure is performed on the second image, and a detected result is generated. Control information is generated according to the detected result. The parameter is adjusted according to the control information, and a third image is generated. The second image and the third image are output. Therefore, the subsequent application could be enhanced through the dual image outputs.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="132.84mm" wi="131.06mm" file="US20230005235A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="179.92mm" wi="101.60mm" file="US20230005235A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="206.59mm" wi="107.78mm" file="US20230005235A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="154.01mm" wi="133.10mm" file="US20230005235A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="112.18mm" wi="92.29mm" file="US20230005235A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="86.28mm" wi="118.03mm" file="US20230005235A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims the priority benefit of U.S. provisional application Ser. No. 63/217,770, filed on Jul. 2, 2021, and Taiwan application serial no. 111100227, filed on Jan. 4, 2022. The entirety of each of the above-mentioned patent applications is hereby incorporated by reference herein and made a part of this specification.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Technical Field</heading><p id="p-0003" num="0002">The disclosure relates to an image processing technology, and more particularly, to an image processing system and a processing method of video stream.</p><heading id="h-0004" level="1">Related Art</heading><p id="p-0004" num="0003">In the prior art, although a camera equipped with a wide-angle lens or a fisheye lens may capture an image with a wider field of view (FoV), the edge of the image may be curved and gives an unnatural appearance. Distortions in wide-angle or fisheye images may make their content difficult to read, and are more likely to cause discomfort to the user's eyes.</p><p id="p-0005" num="0004">A conventional fisheye or wide-angle camera will transmit the panoramic image to a back-end system (such as a host or computer) having stronger computing power to handle de-warping, detection, and display of a de-warped image together. However, the complexity of the back-end device is high and the application situation of the camera is limited.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0006" num="0005">In view of this, embodiments of the disclosure provide an image processing system and a processing method of video stream capable of dual video streaming for detection and display applications respectively.</p><p id="p-0007" num="0006">The processing method of video stream according to the embodiments of the disclosure includes (but is not limited to) the following steps. A first image is obtained according to a parameter. A deformation correction procedure is performed on the first image, and a second image is generated. An identification detection procedure is performed on the second image, and a detected result is generated. A control information is generated according to the detected result. The parameter is adjusted according to the control information, and a third image is generated. The second image and the third image are output.</p><p id="p-0008" num="0007">The image processing system according to the embodiments of the disclosure includes (but is not limited to) a controller. The controller is configured to obtain a first image according to a parameters; perform a deformation correction procedure on the first image and generate a second image; adjust the parameter according to a control information and generate a third image; and output the second image and the third image. The control information is generated according to the detected result generated by performing the identification detection procedure on the second image.</p><p id="p-0009" num="0008">Based on the above, according to the image processing system and the processing method of video stream according to the embodiments of the disclosure, through the deformation correction procedure, two types of images can be respectively generated which are carried by two video streams and used for different back-end applications, thereby allowing for an effective division of labor and improvement in the structural flexibility.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">The accompanying drawings are included to provide a further understanding of the disclosure, and are incorporated in and constitute a part of this specification. The drawings illustrate embodiments of the disclosure and, together with the description, serve to explain the principles of the disclosure.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of components of an image processing system according to an embodiment of the disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of components of an image processing system according to another embodiment of the disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a processing method of video stream according to an embodiment of the disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example illustrating a multi-window preview.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of an application of dual video streaming according to an embodiment of the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0016" num="0015">Reference will now be made in detail to the exemplary embodiments of the disclosure, examples of which are illustrated in the accompanying drawings. Wherever possible, the same reference numbers are used in the drawings and the description to refer to the same or like parts.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of components of an image processing system <b>1</b> according to an embodiment of the disclosure. Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the image processing system <b>1</b> includes (but is not limited to) an image capture device <b>10</b>, a controller <b>30</b>, a back-end device <b>50</b> and a display device <b>60</b>.</p><p id="p-0018" num="0017">The image capture device <b>10</b> may be a camera, a video camera, a monitor, or a device having similar function. The image capture device <b>10</b> may include (but is not limited to) a lens <b>11</b> and an image sensor <b>15</b> (such as a charge coupled device (CCD) or a complementary metal-oxide-semiconductor (CMOS), or the like). In one embodiment, an image may be captured through the lens <b>11</b> and the image sensor <b>15</b>. For example, light is imaged on the image sensor <b>15</b> through the lens <b>11</b>.</p><p id="p-0019" num="0018">In some embodiments, the specifications of the image capture device <b>10</b> (such as imaging aperture, magnification, focal length, imaging viewing angle, size of the image sensor <b>15</b>, or the like) and the number thereof may be adjusted according to actual needs. For example, the lens <b>11</b> is a fish-eye or wide-angle lens, and generates a fish-eye image or a wide-angle image accordingly.</p><p id="p-0020" num="0019">The controller <b>30</b> may be coupled to the image capture device <b>10</b> through a camera interface, I2C, and/or other transmission interfaces. The controller <b>30</b> includes (but is not limited to) a memory <b>31</b>, transmission interfaces <b>32</b>, <b>33</b>, <b>34</b> and an operation unit <b>39</b>.</p><p id="p-0021" num="0020">The memory <b>31</b> may be any type of fixed or removable random access memory (RAM), read only memory (ROM), flash memory, conventional hard disk drive (HDD), solid-state drive (SSD), or similar components. In one embodiment, the memory <b>31</b> is configured to store codes, software modules, configuration, data or files.</p><p id="p-0022" num="0021">The transmission interfaces <b>32</b>, <b>33</b>, and <b>34</b> may be camera interfaces, I2C, or other transmission interfaces. For example, a transmission interface <b>32</b> is an I2C interface, the transmission interface <b>33</b> is a mobile industry processor interface (MIPI), and the transmission interface <b>34</b> is a universal serial bus (USB) interface. For another example, the transmission interface <b>33</b> is a digital parallel bus (DPS) interface, a low-voltage differential signalling (LVDS) interface, a high-speed serial pixel (HiSPi) interface, or a V-by-One interface. In one embodiment, the transmission interfaces <b>32</b>, <b>33</b>, and <b>34</b> are configured to be connected to external devices. For example, the transmission interfaces <b>32</b> and <b>33</b> are connected to the back-end device <b>50</b>, and the transmission interface <b>34</b> is connected to the display device <b>60</b>.</p><p id="p-0023" num="0022">The operation unit <b>39</b> is coupled to the memory <b>31</b> and the transmission interfaces <b>32</b>, <b>33</b> and <b>34</b>. The operation unit <b>39</b> may be an image processor, a graphic processing unit (GPU), other programmable general-purpose or special-purpose microprocessors, digital signal processors (DSP), programmable controllers, field programmable gate arrays (FPGA), application-specific integrated circuits (ASIC), other similar components, or combinations of the above components. In one embodiment, the operation unit <b>39</b> is configured to execute all or part of the operations of the controller <b>30</b>, and may load and execute each code, software module, file and data stored in the memory <b>31</b>.</p><p id="p-0024" num="0023">The back-end device <b>50</b> and the display device <b>60</b> may be a desktop computer, a notebook computer, a server, a smart phone, or a tablet computer. In one embodiment, the back-end device <b>50</b> is configured to calculate and/or detect images. In one embodiment, the display device <b>60</b> is configured to display images. In some embodiments, the functions of back-end device <b>50</b> and/or the display device <b>60</b> may be implemented on a microcontroller, chip, SoC, or system.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of components of an image processing system <b>2</b> according to another embodiment of the disclosure. Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the difference between a controller <b>30</b>&#x2032; of this embodiment and the controller <b>30</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is that the controller <b>30</b>&#x2032; includes a transmission interface <b>35</b>, and the operation unit <b>39</b> is coupled to the transmission interface <b>35</b>. The transmission interface <b>35</b> may be a USB interface and provides endpoints <b>36</b>, <b>37</b>, <b>38</b>. It is worth mentioning that the endpoints <b>36</b>, <b>37</b>, and <b>38</b> in this embodiment are configured to be connected to a back-end device <b>70</b>, wherein the endpoints <b>36</b> and <b>37</b> are configured to be connected to a processor <b>71</b> of the back-end device <b>70</b>, and the endpoint <b>38</b> is configured to be connected to an image unit <b>72</b> of the back-end device <b>70</b>.</p><p id="p-0026" num="0025">The back-end device <b>70</b> may be a desktop computer, a notebook computer, a server, a smart phone or a tablet computer. The processor <b>71</b> may be an image processor, a GPU, other programmable general-purpose or special-purpose microprocessors, digital signal processors, programmable controllers, field programmable logic gate arrays, application-specific integrated circuits, other similar components, or combinations of the above components. The image unit <b>72</b> may be a processing circuit or a control circuit for image output/display/play.</p><p id="p-0027" num="0026">Hereinafter, the method according to the embodiment of the disclosure will be described in conjunction with various devices, components and modules in the image processing system <b>1</b> or the image processing system <b>2</b>. Each process of the method may be adjusted according to the implementation situation, and the disclosure is not limited thereto.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a processing method of video stream according to an embodiment of the disclosure. Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the controllers <b>30</b>, <b>30</b>&#x2032; obtain a first image according to a parameter (step S<b>310</b>). The parameter includes the setting parameter for de-warping, panning, tilting, zooming, rotating, shifting, and/or field of view (FoV) adjusting. In one embodiment, the parameter is a setting parameter for the image capture device <b>10</b> or other external image capture devices to perform image capture operations. For example, the controllers <b>30</b>, <b>30</b>&#x2032; may send a command or a message to the image capture device <b>10</b> or other external image capture devices, and the command or message are related to the setting parameter. For example, changing the image capture range. The image capture device <b>10</b> or other external image capture devices may perform image capture operations according to the setting parameter to obtain images.</p><p id="p-0029" num="0028">In one embodiment, the controllers <b>30</b>, <b>30</b>&#x2032; obtain the first image from the image capture device <b>10</b>. Specifically, the first image is an image captured by the image capture device <b>10</b> or other external image capture devices on one or more target objects. In one embodiment, the target object is a human body. In some embodiments, the first image aims at the upper body of a person (such as waist, shoulders, or above the chest). In other embodiments, the target object may be various types of organisms or non-living organisms. The controllers <b>30</b>, <b>30</b>&#x2032; may obtain the first image captured by the image capture device <b>10</b> through the camera interface and/or I2C.</p><p id="p-0030" num="0029">The controllers <b>30</b>, <b>30</b>&#x2032; perform a deformation correction procedure on the first image, and generate a second image (step S<b>330</b>). The deformation correction procedure is configured to adjust the deformation in the first image. In one embodiment, the deformation correction procedure may be de-warping, panning, tilting, zooming, rotating, shifting, and/or field of view adjusting.</p><p id="p-0031" num="0030">In one embodiment, the image sensor <b>15</b> may output raw data of the first image (such as sensing intensities of a plurality of primary colors) to controllers <b>30</b>, <b>30</b>&#x2032;. The controllers <b>30</b>, <b>30</b>&#x2032; may process the raw data according to an image signal processing (ISP) procedure, and output a visible full-color image to the deformation correction procedure.</p><p id="p-0032" num="0031">In another embodiment, the image sensor <b>15</b> may output YUV or other color-coded data of the first image to the controllers <b>30</b>, <b>30</b>&#x2032;. The controllers <b>30</b>, <b>30</b>&#x2032; may ignore or disable the processing of the color-coded data by the ISP procedure (i.e. skip the ISP procedure for the color-coded data), and directly perform the deformation correction procedure on the color-coded data.</p><p id="p-0033" num="0032">In one embodiment, the first image is a wide-angle image or a fisheye image, and the controllers <b>30</b>, <b>30</b>&#x2032; may generate a de-warped panoramic image (i.e. the second image) through a deformation correction procedure. This panorama may be used for subsequent detection of one or more target objects. For example, panoramas may be configured to detect faces, gestures or body parts. In another embodiment, the controllers <b>30</b>, <b>30</b>&#x2032; may turn the viewing angle of the first image in any axis, zoom in all or part of the region, shift in all or part of the region, and/or adjust the size of the viewing angle to generate the second image.</p><p id="p-0034" num="0033">In one embodiment, the controller <b>30</b> may transmit the second image to the back-end device <b>50</b> through the transmission interface <b>33</b>, and the controller <b>30</b>&#x2032; may transmit the second image to the processor <b>71</b> of the back-end device <b>70</b> through the endpoint <b>37</b>. For example, the controllers <b>30</b>, <b>30</b>&#x2032; may output the second image through a first video stream VS<b>1</b>.</p><p id="p-0035" num="0034">The back-end device <b>50</b> or the processor <b>71</b> performs an identification detection procedure (or called an object detection procedure) on the second image, and generates a detected result (step S<b>330</b>). The identification detection procedure is, for example, to determine, in the second image, one or more regions of interest (RoI) (or bounding boxes, or bounding rectangles) or pinots (which may be located on the target object's outline, center, or anywhere thereon) corresponding to the target object (such as a person, an animal, a non-living body or part thereof), so as to identify the type of the target object (e.g. whether it is a person, male or female, dog or cat, table or chair, or the like).</p><p id="p-0036" num="0035">In one embodiment, the detected result is related to one or more regions of interest (RoI) (or bounding boxes, or bounding rectangles) in the second image. For example, the identification detection procedure may determine a region of interest in the second image corresponding to the target object, and the region of interest may frame all or part of the target object.</p><p id="p-0037" num="0036">The back-end device <b>50</b> or the processor <b>71</b> generates control information CI according to the detected result (step S<b>340</b>). The control information is related to the detected result of the second image. The detected result is, for example, recognizing one or more faces, gestures, body parts, or other target objects in the second image (i.e. object detection), or, for example, determining the position/motion of one or more target objects (i.e. object tracking). In one embodiment, the control information is parameter adjustment for the deformation correction procedure based on the detected result. For example, the position of the face in the image is configured to set the turning angle of the viewing angle in the deformation correction procedure. For another example, the size of the human face in the image is configured to set the area magnification in the deformation correction procedure.</p><p id="p-0038" num="0037">In one embodiment, the back-end device <b>50</b> may transmit the control information CI to the controller <b>30</b> through the transmission interface <b>32</b>, and the processor <b>71</b> may transmit the control information CI to the controller <b>30</b>&#x2032; through the endpoint <b>36</b>.</p><p id="p-0039" num="0038">The controllers <b>30</b>, <b>30</b>&#x2032; adjust the parameter according to the control information, and generate a third image (step S<b>350</b>). For example, the controllers <b>30</b>, <b>30</b>&#x2032; generate a command or a message related to the setting parameter of the image capture device <b>10</b> according to the control information, and accordingly capture the first image again through the image capture device <b>10</b>. In one embodiment, the controllers <b>30</b>, <b>30</b>&#x2032; obtain the first image captured again, adjust the first image through the deformation correction procedure according to the control information CI, and generate the third image.</p><p id="p-0040" num="0039">In one embodiment, the third image includes one or more windows. That is, the third image is divided into one or more windows. The controllers <b>30</b>, <b>30</b>&#x2032; may arrange one or more regions of interest into these windows. For example, the controllers <b>30</b>, <b>30</b>&#x2032; zoom the region of interest in the panorama to fit the size of the designated window, and displace the zoomed region of interest (by, for example, panning, tilting, rotating, panning and/or viewing angle adjusting) to the window.</p><p id="p-0041" num="0040">For example, <figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example illustrating a multi-window preview. Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the controllers <b>30</b>, <b>30</b>&#x2032; divide a third image TIM (shown at the bottom of the diagram) into three windows TW<b>1</b>, TW<b>2</b> and TW<b>3</b>. The window TW<b>3</b> is captured from four people in a first image SIM (as shown at the top of the figure), and the windows TW<b>1</b> and TW<b>2</b> are captured from only one person in the first image SIM, respectively.</p><p id="p-0042" num="0041">In one embodiment, the controllers <b>30</b>, <b>30</b>&#x2032; may output the second image and the third image simultaneously or time-divisionally through the deformation correction procedure. For example, the controllers <b>30</b>, <b>30</b>&#x2032; may generate a first third image according to the detected result of a first second image. Next, the controllers <b>30</b>, <b>30</b>&#x2032; may output the second image and the third image (step S<b>360</b>). For example, the controllers <b>30</b>, <b>30</b>&#x2032; generate the second image and the third image in sequence through time-division multiplexing. More specifically, after the controllers <b>30</b>, <b>30</b>&#x2032; generate the first second image, the third image is then generated according to the detected result of the second image. Furthermore, the controllers <b>30</b>, <b>30</b>&#x2032; of the disclosure may output the second image and the third image simultaneously or in time division; the disclosure does not limit the output timing.</p><p id="p-0043" num="0042">In one embodiment, the controllers <b>30</b>, <b>30</b>&#x2032; may output the second image through the first video stream VS<b>1</b>, and output the third image through a second video stream VS<b>2</b>. Specifically, the second video stream is different from the first video stream. The first video stream VS<b>1</b> carries the second image, and the second video stream VS<b>2</b> carries the third image. That is to say, the controllers <b>30</b>, <b>30</b>&#x2032; may provide the output of dual video streaming. In addition, the two video streams may or may not be output to an external device at the same time, depending on the computing speed.</p><p id="p-0044" num="0043">In one embodiment, taking the structure of <figref idref="DRAWINGS">FIG. <b>1</b></figref> as an example, the operation unit <b>39</b> outputs the first video stream VS<b>1</b> through the transmission interface <b>33</b>, outputs the second video stream VS<b>2</b> through the transmission interface <b>34</b>, and inputs the control information CI through the transmission interface <b>32</b>. For example, the MIPI outputs the first video stream VS<b>1</b>, one endpoint of the USB interface outputs the second video stream VS<b>2</b>, and the I2C interface inputs the control information CI.</p><p id="p-0045" num="0044">In another embodiment, taking the structure of <figref idref="DRAWINGS">FIG. <b>2</b></figref> as an example, the operation unit <b>39</b> outputs the first video stream VS<b>1</b> through the endpoint <b>37</b> of the transmission interface <b>35</b>, outputs the second video stream VS<b>2</b> through the endpoint <b>38</b>, and inputs the control information CI through the endpoint <b>36</b>. For example, the first video stream VS<b>1</b> and the second video stream VS<b>2</b> are output through two endpoints of the USB interface, and the control information CI is input through another endpoint.</p><p id="p-0046" num="0045">The applications of the first video stream VS<b>1</b> and the second video stream VS<b>2</b> may be different. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of an application of dual video streaming according to an embodiment of the disclosure. Referring to both <figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in one embodiment, the first video stream VS<b>1</b> is output to the back-end device <b>50</b>, and the back-end device <b>50</b> performs an identification detection procedure on the first video stream VS<b>1</b> (i.e. calculating and detecting the second image) (step S<b>510</b>) to generate a detected result. The identification detection procedure may be the aforementioned object detection and/or tracking. Moreover, the back-end device <b>50</b> may generate the control information CI according to the detected result. It should be noted that, for the detected result and the control information CI, reference can be made to the descriptions of steps S<b>330</b> and S<b>340</b>, which will not be repeated here. The back-end device <b>50</b> may further feed back the control information CI to the controllers <b>30</b>, <b>30</b>&#x2032; (step <b>530</b>), such that the controllers <b>30</b>, <b>30</b>&#x2032; may generate the third image according to the control information CI. Similarly, in the implementation structure of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the processor <b>71</b> of the back-end device <b>70</b> further feeds back the control information CI to the controller <b>30</b>&#x2032; (step <b>530</b>), such that the controller <b>30</b>&#x2032; may generate the third image according to the control information CI.</p><p id="p-0047" num="0046">In one embodiment, the second video stream VS<b>2</b> is output to the display device <b>60</b>, and the display device <b>60</b> displays the third image (i.e. image preview) carried by the second video stream VS<b>2</b> through a display procedure (step S<b>550</b>). Taking <figref idref="DRAWINGS">FIG. <b>4</b></figref> as an example, the third image TIM is used for multi-window preview and display. Note that the isochronous mode and the bulk mode provided by USB video device class or UVC may allow the display device <b>60</b> to display the third image in real time. However, the embodiments of the disclosure may still adopt other related protocols for real-time video output.</p><p id="p-0048" num="0047">To sum up, the image processing system and the processing method of video stream according to the embodiments of the disclosure provides a dual video streaming. One video stream may allow detection of faces, gestures, or body parts, and control another video stream to provide display of de-warped multiple regions of interest image.</p><p id="p-0049" num="0048">The embodiments of the disclosure provide a flexible architecture and are widely applicable (for example, applicable to different types of video-related products). The system of the embodiments of the disclosure allows for an effective division of labor. For example, the back-end or computer system may be responsible for the detection of faces, gestures, or body parts, and the display device or system may be responsible for display of de-warped multiple regions of interest image. Thereby, manufacturers of ordinary cameras can easily upgrade their products to fisheye or wide-angle cameras, and provide multi-target detection and tracking functions for faces, gestures, or body parts, or the like.</p><p id="p-0050" num="0049">It will be apparent to those skilled in the art that various modifications and variations can be made to the structure of the disclosure without departing from the scope or spirit of the disclosure. In view of the foregoing, it is intended that the disclosure cover modifications and variations of this disclosure provided they fall within the scope of the following claims and their equivalents.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A processing method of video stream, comprising:<claim-text>obtaining a first image according to a parameter;</claim-text><claim-text>performing a deformation correction procedure on the first image and generating a second image;</claim-text><claim-text>performing an identification detection procedure on the second image and generating a detected result;</claim-text><claim-text>generating a control information according to the detected result;</claim-text><claim-text>adjusting the parameter according to the control information and generating a third image; and</claim-text><claim-text>outputting the second image and the third image.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The processing method of video stream according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising:<claim-text>obtaining the first image from an image capture device;</claim-text><claim-text>performing the deformation correction procedure on the first image according to an image signal processing (ISP) procedure to generate the second image, wherein the deformation correction procedure is configured to adjust a deformation in the first image;</claim-text><claim-text>generating the third image through the deformation correction procedure according to the control information; and</claim-text><claim-text>outputting the second image and the third image are simultaneously, wherein the second image is output through a first video stream, and the third image is output through a second video stream, wherein the second video stream is different from the first video stream.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The processing method of video stream according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>generating the second image and the third image simultaneously through the deformation correction procedure.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The processing method of video stream according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>performing the identification detection procedure on the first video stream through a back-end device to generate the detected result, wherein the detected result is related to at least one region of interest (RoI) in the second image.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The processing method of video stream according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:<claim-text>arranging the at least one region of interest into at least one window, wherein the third image comprises the at least one window; and</claim-text><claim-text>displaying the third image through a display device.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The processing method of video stream according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein outputting the second image through the first video stream and outputting the third image through the second video stream comprises:<claim-text>outputting the first video stream through a mobile industry processor interface (MIPI), and outputting the second video stream through a universal serial bus (USB).</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The processing method of video stream according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein outputting the second image through the first video stream and outputting the third image through the second video stream comprises:<claim-text>outputting the first video stream and the second video stream through two endpoints of a USB interface, respectively.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The processing method of video stream according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>inputting the control information through another endpoint of the USB interface.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The processing method of video stream according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the deformation correction procedure comprises de-warping, panning, tilting, zooming, rotating, shifting and field of view (FoV) adjusting, and the parameter comprises a setting parameter of at least one of de-warping, panning, tilting, rotating, zooming, rotating, shifting, and field of view adjusting.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The processing method of video stream according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first image is a wide-angle image or a fisheye image, and the second image is a de-warped panoramic image.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An image processing system, comprising:<claim-text>a controller, configured to:<claim-text>obtain a first image according to a parameter;</claim-text><claim-text>perform a deformation correction procedure on the first image and generate a second image;</claim-text><claim-text>adjust the parameter according to a control information and generate a third image, wherein the control information is generated according to a detected result generated by performing an identification detection procedure on the second image; and</claim-text><claim-text>output the second image and the third image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The image processing system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>an image capture device, coupled to the controller and comprising a lens and an image sensor, and configured to capture the first image through the lens and the image sensor, wherein the controller is further configured to:<claim-text>obtain the first image from an image capture device;</claim-text><claim-text>perform the deformation correction procedure on the first image according to an image signal processing procedure to generate the second image, wherein the deformation correction procedure is configured to adjust a deformation in the first image;</claim-text><claim-text>generate the third image through the deformation correction procedure according to the control information; and</claim-text><claim-text>output the second image and the third image simultaneously, wherein the second image is output through a first video stream, and the third image is output through a second video stream, wherein the second video stream is different from the first video stream.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The image processing system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the controller is further configured to:<claim-text>generate the second image and the third image simultaneously through the deformation correction procedure.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The image processing system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>a back-end device, coupled to the controller and configured to perform the identification detection procedure on the first video stream to generate the detected result, wherein the detected result is related to at least one region of interest in the second image.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The image processing system according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the detected result is related to the at least one region of interest in the second image, and the image processing system further comprises:<claim-text>a display device, coupled to the controller, wherein the controller is further configured to:<claim-text>arrange the at least one region of interest into at least one window, wherein the third image comprises the at least one window; and</claim-text><claim-text>display the third image through the display device.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The image processing system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the controller comprises:<claim-text>a mobile industry processor interface, configured to output the first video stream; and</claim-text><claim-text>a universal serial bus (USB), configured to output the second video stream.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The image processing system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the controller comprises:<claim-text>a USB interface, comprising two endpoints respectively configured to output the first video stream and the second video stream.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The image processing system according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the USB interface further comprises:<claim-text>another endpoint, configured to input the control information.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The image processing system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the deformation correction procedure comprises at least one of de-warping, panning, tilting, zooming, rotating, shifting, and field of view adjusting, and the parameter comprises a setting parameter of at least one of de-warping, panning, tilting, zooming, rotating, shifting, and field of view adjusting.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The image processing system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first image is a wide-angle image or a fisheye image, and the second image is a de-warped panoramic image.</claim-text></claim></claims></us-patent-application>