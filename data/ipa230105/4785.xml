<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004786A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004786</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364086</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>4881</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ARTIFICIAL NEURAL NETWORKS ON A DEEP LEARNING ACCELERATOR</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Micron Technology, Inc.</orgname><address><city>Boise</city><state>ID</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kale</last-name><first-name>Poorna</first-name><address><city>Folsom</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Tiku</last-name><first-name>Saideep</first-name><address><city>Fort Collins</city><state>CO</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Multiple artificial neural networks can be compiled as a single workload. A respective throughput for each of the artificial neural networks can be changed at runtime. The multiple artificial neural networks can be partially compiled individually and then later compiled just-in-time according to changing throughput demands for the artificial neural networks. The multiple artificial neural networks can be deployed on a deep learning accelerator hardware device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="66.89mm" wi="158.75mm" file="US20230004786A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="195.07mm" wi="145.80mm" orientation="landscape" file="US20230004786A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="215.82mm" wi="106.51mm" orientation="landscape" file="US20230004786A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="128.02mm" wi="141.65mm" file="US20230004786A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="152.65mm" wi="144.95mm" file="US20230004786A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="220.47mm" wi="168.66mm" file="US20230004786A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="225.13mm" wi="146.30mm" orientation="landscape" file="US20230004786A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates generally to memory, and more particularly to apparatuses and methods associated with artificial neural networks on a deep learning accelerator.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Memory devices are typically provided as internal, semiconductor, integrated circuits in computers or other electronic devices. There are many different types of memory including volatile and non-volatile memory. Volatile memory can require power to maintain its data and includes random-access memory (RAM), dynamic random access memory (DRAM), and synchronous dynamic random access memory (SDRAM), among others. Non-volatile memory can provide persistent data by retaining stored data when not powered and can include NAND flash memory, NOR flash memory, read only memory (ROM), Electrically Erasable Programmable ROM (EEPROM), Erasable Programmable ROM (EPROM), and resistance variable memory such as phase change random access memory (PCRAM), resistive random access memory (RRAM), and magnetoresistive random access memory (MRAM), among others.</p><p id="p-0004" num="0003">Memory is also utilized as volatile and non-volatile data storage for a wide range of electronic applications. including, but not limited to personal computers, portable memory sticks, digital cameras, cellular telephones, portable music players such as MP3 players, movie players, and other electronic devices. Memory cells can be arranged into arrays, with the arrays being used in memory devices.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an apparatus in the form of a computing system including a memory device in accordance with a number of embodiments of the present disclosure.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of a computing system illustrating switching between different artificial neural networks deployed on a deep learning accelerator in accordance with a number of embodiments of the present disclosure.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of a machine-readable medium including instructions to change the throughput of artificial neural networks at runtime in accordance with a number of embodiments of the present disclosure.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram of a method for operating the plurality of artificial neural networks on a deep learning accelerator in accordance with a number of embodiments of the present disclosure.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example computer system within which a set of instructions, for causing the machine to perform various methodologies discussed herein, can be executed.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of a fog computing network in accordance with a number of embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0011" num="0010">The present disclosure includes apparatuses and methods related to artificial neural networks (ANNs) on a deep learning accelerator (DLA). ANN application workloads deployed on edge devices, such as fog computing nodes, can change dramatically due to usage requirements. Under some situations, the compute units available on an edge device might become oversubscribed. Such situations may be addressed with manual or automated load management switching the deployed ANN model based on workload requirements. However, switching between ANN models at runtime may incur overheads such as recompiling ANN models and creating runtime schedules. Such overhead can be detrimental to the overall robustness and the real-time aspects of the ANN applications.</p><p id="p-0012" num="0011">Some DLAs include a vast systolic array of compute units. A vast systolic array is an homogenous network of tightly coupled data accessing units such as multiply and accumulate units (MAC). However, such approaches can be inefficient if the ANNs do not efficiently utilize the large MAC units. Other DLAs include smaller compute units that work in tandem such that an ANN workload is executed through a fixed schedule. The scalable aspect of smaller compute units allows for the execution of multiple ANNs on the same DLA, such as an application specific integrated circuit (ASIC) or field programmable gate array (FPGA). However, each ANN utilizes a fixed set of computing units and a fixed execution schedule. Such a workload strategy is ignorant of changing workloads and user performance requirements.</p><p id="p-0013" num="0012">Aspects of the present disclosure address the above and other deficiencies. For instance, a respective throughput of each ANN deployed on a DLA can be changed at runtime. For example, the change can be made based on data in the input pipeline and/or user-specified values. The change can be made without going through a full recompilation and redeployment of the ANNs on the DLA. In at least one embodiment, multiple ANNs can be co-compiled as a single workload and deployed on the DLA. The compiler can produce several execution schedules for the ANNs on the DLA. Each execution schedule corresponds to a unique distribution of the ANNs across various compute units of the DLA. The execution schedules can employ techniques such as weight sharing and layer fusion between different ANNs. The execution schedules can be stored in a low-level format that can be accessed at runtime. As the throughput requirements of each ANN change (e.g., as specified by a user or workload-dependent), the execution schedule in use can be changed during runtime. In at least one embodiment, data representing the ANNs can be stored in a partially compiled state individually and then combined execution schedules can be created just-in-time as the throughput demand for the ANNs change. &#x201c;Data representing the ANN&#x201d; refers to any instructions associated with execution of the ANN and/or data such as weights, biases, etc. associated with the ANN.</p><p id="p-0014" num="0013">An ANN can provide learning by forming probability weight associations between an input and an output. The probability weight associations can be provided by a plurality of nodes that comprise the ANN. The nodes together with weights, biases, and activation functions can be used to generate an output of the ANN based on the input to the ANN. As used herein, artificial intelligence refers to the ability to improve a machine through &#x201c;learning&#x201d; such as by storing patterns and/or examples which can be utilized to take actions at a later time. Deep learning refers to a device's ability to learn from data provided as examples. Deep learning can be a subset of artificial intelligence. Artificial neural networks, among other types of networks, can be classified as deep learning.</p><p id="p-0015" num="0014">Fog computing is an architecture that uses edge devices, which are also referred to as nodes, to carry out some or all of the computation and/or storage locally and to communicate at least partially processed data over a network, such as the Internet. Fog computing can be used for surveillance applications. For example, a metropolitan area network can include sensors deployed on infrastructure such as that used for lighting, power transmission, communication, traffic control, etc. The sensors can be used to track automobiles, people, mobile phones, etc. In the fog surveillance application, some of the workload of the overall application can be handled by nodes proximal to the sensors so that that computational workload is distributed throughout the network rather than being bottlenecked at a centralized server.</p><p id="p-0016" num="0015">As used herein, the singular forms &#x201c;a&#x201d;, &#x201c;an&#x201d;, and &#x201c;the&#x201d; include singular and plural referents unless the content clearly dictates otherwise. Furthermore, the word &#x201c;may&#x201d; is used throughout this application in a permissive sense (i.e., having the potential to, being able to), not in a mandatory sense (i.e., must). The term &#x201c;include,&#x201d; and derivations thereof, mean &#x201c;including, but not limited to.&#x201d; The term &#x201c;coupled&#x201d; means directly or indirectly connected.</p><p id="p-0017" num="0016">The figures herein follow a numbering convention in which the first digit or digits correspond to the drawing figure number and the remaining digits identify an element or component in the drawing. Similar elements or components between different figures may be identified by the use of similar digits. For example, <b>126</b> may reference element &#x201c;<b>26</b>&#x201d; in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and a similar element may be referenced as <b>226</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Analogous elements within a Figure may be referenced with a hyphen and extra numeral or letter. See, for example, elements <b>228</b>-<b>1</b>, <b>228</b>-<b>2</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Such analogous elements may be generally referenced without the hyphen and extra numeral or letter. For example, elements <b>228</b>-<b>1</b> and <b>228</b>-<b>2</b> may be collectively referenced as <b>228</b>. As will be appreciated, elements shown in the various embodiments herein can be added, exchanged, and/or eliminated so as to provide a number of additional embodiments of the present disclosure. In addition, as will be appreciated, the proportion and the relative scale of the elements provided in the figures are intended to illustrate certain embodiments of the present invention and should not be taken in a limiting sense.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an apparatus in the form of a computing system <b>100</b> including a memory device <b>104</b> in accordance with a number of embodiments of the present disclosure. The memory device <b>104</b> is coupled to a host <b>102</b> via an interface <b>124</b>. As used herein, a host <b>102</b>, a memory device <b>104</b>, or a memory array <b>110</b>, for example, might also be separately considered to be an &#x201c;apparatus.&#x201d; The interface <b>124</b> can pass control, address, data, and other signals between the memory device <b>104</b> and the host <b>102</b>. The interface <b>124</b> can include a command bus (e.g., coupled to the command/address circuitry <b>106</b>), an address bus (e.g., coupled to the command/address circuitry <b>106</b>), and a data bus (e.g., coupled to the input/output (I/O) circuitry <b>122</b>). Although the command/address circuitry <b>106</b> is illustrated as a single component, embodiments are not so limited, as the command circuitry and address circuitry can be discrete components. In some embodiments, the command bus and the address bus can be comprised of a common command/address bus. In some embodiments, the command bus, the address bus, and the data bus can be part of a common bus. The command bus can pass signals between the host <b>102</b> and the command/address circuitry <b>106</b> such as clock signals for timing, reset signals, chip selects, parity information, alerts, etc. The address bus can pass signals between the host <b>102</b> and the command/address circuitry <b>106</b> such as logical addresses of memory banks in the memory array <b>110</b> for memory operations. The interface <b>124</b> can be a physical interface employing a suitable protocol. Such a protocol may be custom or proprietary, or the interface <b>124</b> may employ a standardized protocol, such as Peripheral Component Interconnect Express (PCIe), Gen-Z interconnect, cache coherent interconnect for accelerators (CCIX), etc. In some cases, the command/address circuitry <b>106</b> is a register clock driver (RCD), such as RCD employed on an RDIMM or LRDIMM.</p><p id="p-0019" num="0018">The memory device <b>104</b> and host <b>102</b> can be a fog computing node, a satellite, a communications tower, a personal laptop computer, a desktop computer, a digital camera, a mobile telephone, a memory card reader, an Internet-of-Things (IoT) enabled device, an automobile, among various other types of systems. For clarity, the system <b>100</b> has been simplified to focus on features with particular relevance to the present disclosure. The host <b>102</b> can include a number of processing resources (e.g., one or more processors, microprocessors, or some other type of controlling circuitry) capable of accessing the memory device <b>104</b>.</p><p id="p-0020" num="0019">The memory device <b>104</b> can provide main memory for the host <b>102</b> or can be used as additional memory or storage for the host <b>102</b>. By way of example, the memory device <b>104</b> can be a dual in-line memory module (DIMM) including memory arrays <b>110</b> operated as double data rate (DDR) DRAM, such as DDR5, a graphics DDR DRAM, such as GDDR6, or another type of memory system. Embodiments are not limited to a particular type of memory device <b>104</b>. Other examples of memory arrays <b>110</b> include RAM, ROM, SDRAM, LPDRAM, PCRAM, RRAM, flash memory, and three-dimensional cross-point, among others. A cross-point array of non-volatile memory can perform bit storage based on a change of bulk resistance, in conjunction with a stackable cross-gridded data access array. Additionally, in contrast to many flash-based memories, cross-point non-volatile memory can perform a write in-place operation, where a non-volatile memory cell can be programmed without the non-volatile memory cell being previously erased.</p><p id="p-0021" num="0020">The command/address circuitry <b>106</b> can decode signals provided by the host <b>102</b>. The command/address circuitry <b>106</b> can also be referred to as a command input and control circuit (or more generally, &#x201c;control circuitry&#x201d;) and can represent the functionality of different discrete ASICs or portions of different ASICs depending on the implementation. The signals can be commands provided by the host <b>102</b>. These signals can include chip enable signals, write enable signals, and address latch signals, among others, that are used to control operations performed on the memory array <b>110</b>. Such operations can include data read operations, data write operations, data erase operations, data move operations, etc. The command/address circuitry <b>106</b> can comprise a state machine, a sequencer, and/or some other type of control circuitry, which may be implemented in the form of hardware, firmware, or software, or any combination of the three. The commands can be decoded by command decode circuitry <b>108</b> and forwarded to the memory array <b>110</b> via column decode circuitry <b>116</b> and/or row decode circuitry <b>118</b>.</p><p id="p-0022" num="0021">Data can be provided to and/or from the memory array <b>110</b> via data lines coupling the memory array <b>110</b> to input/output (I/O) circuitry <b>122</b> via read/write circuitry <b>114</b>. The I/O circuitry <b>122</b> can be used for bi-directional data communication with the host <b>102</b> over an interface. The read/write circuitry <b>114</b> is used to write data to the memory array <b>110</b> or read data from the memory array <b>110</b>. As an example, the read/write circuitry <b>114</b> can comprise various drivers, latch circuitry, etc. In some embodiments, the data path can bypass the command/address circuitry <b>106</b>.</p><p id="p-0023" num="0022">The memory device <b>104</b> includes address decode circuitry <b>120</b> to latch address signals provided over an interface. Address signals are received and decoded by address decode circuitry <b>120</b> and provided therefrom to row decode circuitry <b>118</b> and/or column decode circuitry <b>116</b> to access the memory array <b>110</b>. Data can be read from memory array <b>110</b> by sensing voltage and/or current changes on the sense lines using sensing circuitry <b>112</b>. The sensing circuitry <b>112</b> can be coupled to the memory array <b>110</b>. The sensing circuitry <b>112</b> can comprise, for example, sense amplifiers that can read and latch a page (e.g., row) of data from the memory array <b>110</b>. Sensing (e.g., reading) a bit stored in a memory cell can involve sensing a relatively small voltage difference on a pair of sense lines, which may be referred to as digit lines or data lines.</p><p id="p-0024" num="0023">The memory array <b>110</b> can comprise memory cells arranged in rows coupled by access lines (which may be referred to herein as word lines or select lines) and columns coupled by sense lines (which may be referred to herein as digit lines or data lines). Although the memory array <b>110</b> is shown as a single memory array, the memory array <b>110</b> can represent a plurality of memory arrays arraigned in banks of the memory device <b>104</b>. The memory array <b>110</b> can include a number of memory cells, such as volatile memory cells (e.g., DRAM memory cells, among other types of volatile memory cells) and/or non-volatile memory cells (e.g., RRAM memory cells, among other types of non-volatile memory cells).</p><p id="p-0025" num="0024">The memory device can also include a DLA hardware device <b>126</b>. The DLA hardware device <b>126</b> can be coupled to the command/address circuitry <b>106</b> to receive commands therefrom or to provide instructions thereto. The DLA hardware device <b>126</b> can be coupled to the I/O circuitry <b>122</b> to receive data therefrom or provide data thereto. Such data can be data to be exchanged with the memory array <b>110</b> and/or the host <b>102</b>. Although not specifically illustrated, in some embodiments, the DLA hardware device <b>126</b> can be coupled to the memory array <b>110</b> for direct exchange of data therewith. The DLA hardware device <b>126</b> can be an ASIC, and FPGA, or other hardware component of the memory device <b>104</b>. As illustrated, the DLA hardware device <b>126</b> can have multiple ANNs <b>128</b> co-compiled and deployed thereon.</p><p id="p-0026" num="0025">The ANNs <b>128</b> can be compiled by a compiler <b>103</b> on the host <b>102</b>. The compiler <b>103</b> can be hardware and/or software (executed instructions) that translates instructions (computer code) written in one programming language (a source language) into another language (the target language). In this case, the target language is that of the DLA hardware device <b>126</b>. For example, the compiler <b>103</b> can compile instructions from the host <b>102</b> to cause the DLA hardware device <b>126</b> to execute one or more ANNs <b>128</b> in accordance with the instructions.</p><p id="p-0027" num="0026">The DLA hardware device <b>126</b> can be configured to share compute resources between the ANNs <b>128</b> deployed thereon according to a first execution schedule accessed from the memory array <b>110</b>. The DLA hardware device <b>126</b> can be configured to share compute resources between the ANNs <b>128</b> differently according to a second execution schedule accessed from the memory array <b>110</b>. Execution schedules are described in more detail below. The DLA hardware device <b>126</b> can be configured to share compute resources between the ANNs <b>128</b> differently according to any of a plurality of execution schedules accessed from the memory array <b>110</b>. The DLA hardware device <b>126</b> can be configured to share compute resources between the ANNs <b>128</b> differently without having the ANNs <b>128</b> redeployed on the DLA hardware device <b>126</b>.</p><p id="p-0028" num="0027">The DLA hardware device <b>126</b> can be configured to prefect weights associated with a second ANN <b>128</b> from the memory array <b>110</b> while executing a first ANN <b>128</b>. The DLA hardware device <b>126</b> can be configured to prefect the weights according to an upcoming context switch from the first ANN <b>128</b> to the second ANN <b>128</b> as indicated by the first execution schedule. For example, the first execution schedule may indicate that a portion of time dedicated to execution of the first ANN is about to elapse. The DLA hardware device <b>126</b> can prefetch the weights based on a time remaining in the first execution schedule and a time required to prefect the weights such that the weights are received prior to the context switch. Context switching is described in more detail with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of a computing system illustrating switching between different artificial neural networks deployed on a deep learning accelerator in accordance with a number of embodiments of the present disclosure. The system includes a host (Host CPU) <b>202</b> coupled to memory (Shared Memory) <b>210</b>, which is coupled to a DLA <b>226</b>. The memory <b>210</b> is illustrated as storing two engines (Engine_0 and Engine_1), which represent a first ANN <b>228</b>-<b>1</b> and a second ANN <b>228</b>-<b>2</b>. Each ANN <b>228</b> is associated with respective inputs <b>230</b>-<b>1</b>, <b>230</b>-<b>2</b>, outputs <b>232</b>-<b>1</b>, <b>232</b>-<b>2</b>, weights <b>234</b>-<b>1</b>, <b>234</b>-<b>2</b>, instructions <b>236</b>-<b>1</b>, <b>236</b>-<b>2</b>, and priority information <b>238</b>-<b>1</b>, <b>238</b>-<b>2</b>.</p><p id="p-0030" num="0029">An individual ANN <b>228</b> can receive input data <b>230</b> and can generate an output <b>232</b>, which can be referred to as a predicted output because it is a prediction of the result of the classification, identification, or analysis performed on the input data <b>230</b> by the ANN <b>228</b>. An example of the output <b>232</b> is an identification of an object in an image, where the image is the input data <b>230</b>. The ANN <b>228</b> can include layers of nodes including an initial or input layer and a final or output layer with intermediate layers therebetween. The input data <b>230</b> can be input to the nodes of the input layer. The nodes of the output layer can provide signals that represent the output <b>232</b> of the ANN <b>228</b>.</p><p id="p-0031" num="0030">Each node of the ANN <b>228</b> can be coupled to adjacent nodes. Signals can be provided from the nodes of a previous layer to connected nodes of a subsequent layer. The connection between adjacent nodes can be assigned a weight <b>234</b>. In some embodiments, each connection in the ANN <b>228</b> can have an individual weight <b>234</b> assigned thereto. A topology of the ANN <b>228</b> describes the coupling of the nodes. The topology of the ANN <b>228</b> also describes the quantity of nodes. The topology of the ANN <b>228</b> further describes the layers of the ANN <b>228</b>.</p><p id="p-0032" num="0031">A node can provide (or not provide) an input signal to each of the nodes to which it is coupled. For a given pair of coupled nodes, that signal can be combined with a weight assigned to the connection therebetween. For example, the weight can be multiplied with the signal provided from a first node to the second node. A given node can have a quantity of inputs thereto from a corresponding quantity of nodes coupled thereto. The node can sum the product of the signals input thereto and the corresponding weights assigned to the connections. A bias can be added to the sum. The addition (e.g., sum of the bias and the sum of the product of the signals and the corresponding weights) can be performed by the nodes. The result of the addition can be used in an activation function to determine whether the corresponding node will provide a signal to each of the nodes to which the corresponding node is coupled.</p><p id="p-0033" num="0032">The ANNs <b>228</b> can be represented by instructions <b>236</b>, which, according to the present disclosure, are compiled together to deploy the ANNs <b>228</b> on the DLA <b>226</b> as a single workload. Although the ANNs <b>228</b> are deployed on the DLA <b>226</b>, the backing data for the ANNs <b>228</b> can be stored in the memory <b>210</b>. The priority information <b>238</b> can represent a respective throughput for each of the ANNs <b>228</b>, a relative portion of the available resources of the DLA <b>226</b> that are assigned to or used by each ANN <b>228</b>, and/or a relative portion of execution time of the DLA <b>226</b> that is assigned to each ANN <b>228</b>. The priority information <b>238</b> is a target that can be adjusted. In at least one embodiment, the priority information <b>238</b> can be adjusted by user input. In at least one embodiment, the priority information <b>238</b> can be adjusted automatically based on the respective input data <b>230</b> for each ANN <b>228</b>. For example, the priority information <b>238</b> can be adjusted based on a relative volume of input data <b>230</b> for each ANN <b>228</b> (e.g., where a relatively greater quantity of input data <b>230</b>-<b>1</b> for a first ANN <b>228</b>-<b>1</b> yields a relatively higher priority information <b>238</b>-<b>1</b> for the first ANN <b>228</b>-<b>1</b>).</p><p id="p-0034" num="0033">The right side of <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates the DLA <b>226</b> using memory associated with the first ANN <b>228</b>-<b>1</b> during a first time <b>240</b>-<b>1</b>. As the priority information <b>238</b> indicates that a time for a context switch is approaching, the DLA <b>226</b> can prefetch the weights <b>234</b>-<b>2</b> associated with the second ANN <b>238</b>-<b>2</b> at the second time <b>240</b>-<b>2</b>. After prefetching the weights <b>234</b>-<b>2</b>, the context switch from the first ANN <b>238</b>-<b>1</b> to the second ANN <b>238</b>-<b>2</b> can occur at a third time <b>240</b>-<b>3</b>, after which, the DLA <b>226</b> uses memory associated with the second ANN <b>238</b>-<b>2</b> during a fourth time <b>240</b>-<b>4</b>. Context switching is also referred to herein as changing from one execution schedule to another execution schedule, as described in more detail with respect to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of a machine-readable medium <b>350</b> including instructions <b>352</b> to change the throughput of artificial neural networks at runtime in accordance with a number of embodiments of the present disclosure. The instructions <b>354</b> can be executed to compile a plurality (more than one) ANN as a single workload. As opposed to some previous approaches that compile each ANN as a separate workload (either for a general purpose processor or for a DLA hardware device), at least one embodiment of the present disclosure compiles multiple ANNs as a single workload so that the throughput for each ANN can be changed dynamically without recompiling and/or redeploying the ANNs. In some embodiments, because the multiple ANNs are compiled as a single workload, the DLA can execute both ANNs simultaneously. In some embodiments, although the multiple ANNs are compiled as a single workload, they may not be executed simultaneously by the DLA, or at least not executed simultaneously by the DLA all the time.</p><p id="p-0036" num="0035">The instructions <b>356</b> can be executed to deploy the ANNs on a DLA hardware device. Deploying an ANN on the DLA can include providing instructions for execution of the ANN on the DLA to the DLA, providing weights for operation of the ANN on the DLA, providing inputs to the ANN to the DLA, and/or providing priority information for execution of the ANN (e.g., vis-&#xe0;-vis other ANNs) to the DLA.</p><p id="p-0037" num="0036">The instructions <b>358</b> can be executed to change a throughput for each of the ANNs at runtime. The throughput for each ANN can refer to a relative portion of the available resources of the DLA that are assigned to or used by each ANN and/or a relative portion of execution time of the DLA that is assigned to each ANN. The throughput can be adjusted, for example, by assigning a different portion of the compute resources of the DLA or a different relative portion of the execution time of the DLA to the ANNs. In at least one embodiment, the throughput can be changed based on data in an input pipeline. In at least one embodiment, the throughput can be changed based on a user specification or input. The throughputs for the ANNs can be changed without recompiling the ANNs.</p><p id="p-0038" num="0037">Although not specifically illustrated, the instructions <b>352</b> can be executed to create execution schedules for the ANNs on the DLA hardware device. Each execution schedule can correspond to a respective distribution of the ANNs to the compute units of the DLA hardware device. An execution schedule can share at least one weight between at least two different ANNs. An execution schedule can share at least one layer between at least two different ANNs. The execution schedules can be stored in a low-level format, meaning that the execution schedules can be stored directly in a storage medium (e.g., the memory <b>210</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>), bypassing a file system. In at least one embodiment, in order to effectuate changes in respective throughputs for different ANNs, an active execution schedule can be changed. For example, the DLA hardware device can execute the ANNs according to a first execution schedule and then subsequently execute the ANNs according to a second execution schedule, where the first and second execution schedules correspond to different distributions of the ANNs across the compute units of the DLA hardware device. In some instances, a particular execution schedule can be represented as a directed acyclic graph.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram of a method for operating the plurality of artificial neural networks on a deep learning accelerator in accordance with a number of embodiments of the present disclosure. The method can be performed by processing logic that can include hardware (e.g., a processing device, circuitry, dedicated logic, programmable logic, microcode, hardware of a device, integrated circuit, etc.), software (e.g., instructions run or executed on a processing device), or a combination thereof. In some embodiments, the method is performed by a host (e.g., host <b>102</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>), by a memory device (e.g., the memory device <b>104</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) and/or by a DLA hardware device (e.g., the DLA hardware device <b>126</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>). Although shown in a particular sequence or order, unless otherwise specified, the order of the processes can be modified. Thus, the illustrated embodiments should be understood only as examples, and the illustrated processes can be performed in a different order, and some processes can be performed in parallel. Additionally, one or more processes can be omitted in various embodiments. Thus, not all processes are required in every embodiment. Other process flows are possible.</p><p id="p-0040" num="0039">At block <b>460</b>, the method can include storing data representative of ANNs (e.g., partially compiled ANNs). The ANNs can be stored in a tangible machine-readable medium such as memory. In some embodiments, each of the ANNs can be partially compiled individually. Each ANN can be stored individually. Partially compiled means that the source code of the ANN has been translated into an intermediate form for later just-in-time compiling or later interpretation and execution. Just-in-time compiling is described in more detail below.</p><p id="p-0041" num="0040">At block <b>462</b>, the method can include creating a first execution schedule, in response to a first throughput demand, for operation of the ANNs on a DLA hardware device. At block <b>464</b>, the method can include operating the ANNs by processing the data on the DLA hardware device according to the first execution schedule. The first execution schedule corresponds to a first distribution of the ANNs to compute units of the DLA hardware device. At block <b>466</b>, the method can include creating a second execution schedule, in response to a second throughput demand, for operation of the ANNs on the DLA hardware device. The second execution schedule corresponds to a second distribution (different than the first distribution) of the ANNs to compute units of the DLA hardware device. At block <b>468</b>, the method can include operating the ANNs by processing the data on the DLA hardware device according to the second execution schedule.</p><p id="p-0042" num="0041">Creating the first execution schedule and/or creating the second execution schedule can include just-in-time compiling the ANNs. Just-in-time compiling the ANNs mean that they are compiled at runtime rather than before runtime during compile time. Compiling the ANNs just-in-time can advantageously allow for the ANNs to be tailored more narrowly to the dynamically changing throughput demands for each ANN. Although not specifically illustrated, the method can include determining the first throughput demand and the second throughput demand based on data in an input pipeline. For example, the first ANN can be configured to operate on data input from a first source and the second ANN can be configured to operate on data input from a second source. The first and second sources can provide different and/or variable amounts of data at different times. Alternatively or additionally, the method can include determining the first throughput demand and the second throughput demand based on user input. The user input can be a predefined value or a real-time input from the user. The user input can define a relative proportion of the compute resources and/or compute time of the DLA hardware device that is dedicated to execution of each ANN. Embodiments are not limited to the use of two ANNs as embodiments can include more than two ANNs simultaneously being deployed on a DLA hardware device.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example computer system within which a set of instructions, for causing a machine to perform various methodologies discussed herein, can be executed. In various embodiments, the computer system <b>590</b> can correspond to a system (e.g., the computing system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) that includes, is coupled to, or utilizes a memory sub-system (e.g., the memory device <b>104</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) or can be used to perform the operations of control circuitry. In alternative embodiments, the machine can be connected (e.g., networked) to other machines in a LAN, an intranet, an extranet, and/or the Internet. The machine can operate in the capacity of a server or a client machine in client-server network environment, as a peer machine in a peer-to-peer (or distributed) network environment, or as a server or a client machine in a cloud computing infrastructure or environment.</p><p id="p-0044" num="0043">The machine can be a personal computer (PC), a tablet PC, a set-top box (STB), a Personal Digital Assistant (PDA), a cellular telephone, a web appliance, a server, a network router, a switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine. Further, while a single machine is illustrated, the term &#x201c;machine&#x201d; shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein.</p><p id="p-0045" num="0044">The example computer system <b>590</b> includes a processing device <b>591</b>, a main memory <b>593</b> (e.g., read-only memory (ROM), flash memory, dynamic random access memory (DRAM) such as synchronous DRAM (SDRAM) or Rambus DRAM (RDRAM), etc.), a static memory <b>597</b> (e.g., flash memory, static random access memory (SRAM), etc.), and a data storage system <b>599</b>, which communicate with each other via a bus <b>597</b>.</p><p id="p-0046" num="0045">The processing device <b>591</b> represents one or more general-purpose processing devices such as a microprocessor, a central processing unit, or the like. More particularly, the processing device can be a complex instruction set computing (CISC) microprocessor, reduced instruction set computing (RISC) microprocessor, very long instruction word (VLIW) microprocessor, or a processor implementing other instruction sets, or processors implementing a combination of instruction sets. The processing device <b>591</b> can also be one or more special-purpose processing devices such as an application specific integrated circuit (ASIC), a field programmable gate array (FPGA), a digital signal processor (DSP), network processor, or the like. The processing device <b>591</b> is configured to execute instructions <b>552</b> for performing the operations and steps discussed herein. The computer system <b>590</b> can further include a network interface device <b>595</b> to communicate over the network <b>596</b>.</p><p id="p-0047" num="0046">The data storage system <b>599</b> can include a machine-readable storage medium <b>550</b> (also known as a computer-readable medium) on which is stored one or more sets of instructions <b>552</b> or software embodying any one or more of the methodologies or functions described herein. The instructions <b>552</b> can also reside, completely or at least partially, within the main memory <b>593</b> and/or within the processing device <b>591</b> during execution thereof by the computer system <b>590</b>, the main memory <b>593</b> and the processing device <b>591</b> also constituting machine-readable storage media.</p><p id="p-0048" num="0047">The instructions <b>552</b> can be analogous to the instructions <b>352</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. However, the instructions <b>552</b> can be different instructions to carry out any of the embodiments described herein. In at least one embodiment, the instructions <b>552</b> include instructions to implement functionality corresponding to the host <b>102</b>, the memory device <b>104</b>, the DLA hardware device <b>126</b>, and/or the ANNs <b>128</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0049" num="0048">While the machine-readable storage medium <b>550</b> is shown in an example embodiment to be a single medium, the term &#x201c;machine-readable storage medium&#x201d; should be taken to include a single medium or multiple media that store the one or more sets of instructions. The term &#x201c;machine-readable storage medium&#x201d; shall also be taken to include any medium that is capable of storing or encoding a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present disclosure. The term &#x201c;machine-readable storage medium&#x201d; shall accordingly be taken to include, but not be limited to, solid-state memories, optical media, and magnetic media.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of a fog computing network in accordance with a number of embodiments of the present disclosure. The fog computing network includes a plurality of sensors, such as the sensors <b>670</b>-<b>1</b>, <b>670</b>-<b>2</b>, . . . , <b>670</b>-N coupled to the fog computing node <b>672</b>-<b>1</b>. Additional sensors (although not specifically enumerated) are coupled to different fog computing nodes <b>672</b>-<b>2</b>, . . . , <b>672</b>-M. Non-limiting examples of sensors <b>670</b> include cameras, thermometers, antennas, etc. The sensors <b>670</b> generate data and transmit the data to the fog computing node <b>672</b> coupled thereto. The sensors <b>670</b> can be coupled to the fog computing nodes <b>672</b> in a wired or wireless manner.</p><p id="p-0051" num="0050">The first fog computing node <b>672</b>-<b>1</b> includes a memory device <b>604</b>-<b>1</b> and a DLA hardware device <b>626</b>-<b>1</b>. The second fog computing node <b>672</b>-<b>2</b> includes a memory device <b>604</b>-<b>2</b> and a DLA hardware device <b>626</b>-<b>2</b>. The fog computing node <b>672</b>-M includes a memory device <b>604</b>-M and a DLA hardware device <b>626</b>-M. The DLA hardware devices <b>626</b> can be integrated with the memory devices <b>604</b> in respective single packages as a component thereof. The DLA hardware devices <b>626</b> can be external to the memory devices <b>604</b> (on a separate chip). Although not specifically illustrated, in some embodiments, the fog computing nodes <b>672</b> can include additional processing resources. The DLA hardware devices <b>626</b> can have more than one artificial neural network compiled as a single workload and deployed thereon, as described herein.</p><p id="p-0052" num="0051">A DLA hardware device <b>626</b>-<b>1</b> can be configured to operate a first artificial neural network based on data from a first sensor <b>670</b>-<b>1</b> and to operate a second artificial neural network based on data from a second sensor <b>670</b>-<b>2</b>. The DLA hardware device <b>626</b>-<b>1</b> can be configured to operate the first and the second artificial neural networks according to a first schedule in response to the first sensor <b>670</b>-<b>1</b> providing a greater quantity of data than the second sensor <b>670</b>-<b>2</b> over a period of time. The DLA hardware device <b>626</b>-<b>1</b> can be configured to operate the first and the second artificial neural networks according to a second schedule in response to the second sensor <b>670</b>-<b>2</b> providing a greater quantity of data than the first sensor <b>670</b>-<b>1</b> over the period of time.</p><p id="p-0053" num="0052">The fog computing nodes <b>672</b> are coupled to a fog computing server <b>674</b>, which includes processing resources <b>691</b> and memory resources <b>693</b>. The fog computing server <b>674</b> can be configured to execute a surveillance application using data received from the fog computing nodes <b>672</b>. For example, the surveillance application can function to track the location of mobile devices, automobiles, people, etc., among other surveillance applications.</p><p id="p-0054" num="0053">Although specific embodiments have been illustrated and described herein, those of ordinary skill in the art will appreciate that an arrangement calculated to achieve the same results can be substituted for the specific embodiments shown. This disclosure is intended to cover adaptations or variations of various embodiments of the present disclosure. It is to be understood that the above description has been made in an illustrative fashion, and not a restrictive one. Combinations of the above embodiments, and other embodiments not specifically described herein will be apparent to those of skill in the art upon reviewing the above description. The scope of the various embodiments of the present disclosure includes other applications in which the above structures and methods are used. Therefore, the scope of various embodiments of the present disclosure should be determined with reference to the appended claims, along with the full range of equivalents to which such claims are entitled.</p><p id="p-0055" num="0054">In the foregoing Detailed Description, various features are grouped together in a single embodiment for the purpose of streamlining the disclosure. This method of disclosure is not to be interpreted as reflecting an intention that the disclosed embodiments of the present disclosure have to use more features than are expressly recited in each claim. Rather, as the following claims reflect, inventive subject matter lies in less than all features of a single disclosed embodiment. Thus, the following claims are hereby incorporated into the Detailed Description, with each claim standing on its own as a separate embodiment.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A non-transitory machine-readable medium having computer-readable instructions, which when executed by a machine, cause the machine to:<claim-text>compile a plurality of artificial neural networks as a single workload;</claim-text><claim-text>deploy the plurality of artificial neural networks on a deep learning accelerator (DLA) hardware device; and</claim-text><claim-text>change a respective throughput for each of the plurality of artificial neural networks at runtime.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising instructions to change the respective throughput based on data in an input pipeline.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising instructions to change the respective throughput based on a user specification.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising instructions to change the respective throughput without recompiling the plurality of artificial neural networks.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising instructions to create a plurality of execution schedules for the plurality of artificial neural networks on the DLA hardware device.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The medium of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein each execution schedule corresponds to a respective distribution of the plurality of artificial neural networks to a plurality of compute units of the DLA hardware device.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The medium of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein at least one execution schedule shares at least one weight between at least two of the plurality of artificial neural networks.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The medium of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein at least one execution schedule shares at least one layer between at least two of the plurality of artificial neural networks.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The medium of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising instructions to store the plurality of execution schedules in a low-level format.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The medium of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the instructions to change the respective throughput comprise instructions to operate the DLA hardware device according to a second execution schedule of the plurality of execution schedules after operating the DLA hardware device according to a first execution schedule of the plurality of execution schedules.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A method, comprising:<claim-text>storing data representative of a plurality of artificial neural networks;</claim-text><claim-text>creating a first execution schedule, in response to a first throughput demand, for operation of the plurality of artificial neural networks on a deep learning accelerator (DLA) hardware device;</claim-text><claim-text>operating the plurality of artificial neural networks by processing the data on the DLA hardware device according to the first execution schedule;</claim-text><claim-text>creating a second execution schedule, in response to a second throughput demand, for operation of the plurality of artificial neural networks on the DLA ASIC; and</claim-text><claim-text>operating the plurality of artificial neural networks by processing the data on the DLA hardware device according to the second execution schedule;</claim-text><claim-text>wherein the first execution schedule corresponds to first distribution and the second execution schedule corresponds to a second distribution of the plurality of artificial neural networks to a plurality of compute units of the DLA hardware device.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein storing the data comprises storing the data representative of the plurality of partially compiled artificial neural networks; and<claim-text>wherein creating the first execution schedule and creating the second execution schedule comprise just-in-time compiling the data representative of the plurality of artificial neural networks.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising determining the first throughput demand and the second throughput demand based on data in an input pipeline.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising determining the first throughput demand and the second throughput demand based on user input.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. An apparatus, comprising:<claim-text>a memory array; and</claim-text><claim-text>a deep learning accelerator (DLA) hardware device coupled to the memory array, wherein the DLA hardware device is configured to:<claim-text>share compute resources between a plurality of artificial neural networks deployed thereon according to a first execution schedule accessed from the memory array; and</claim-text><claim-text>share compute resources between the plurality of artificial neural networks differently according to second execution schedule accessed from the memory array.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the DLA hardware device is configured to prefetch weights associated with a second artificial neural network from the memory array while executing a first artificial neural network.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the DLA hardware device is configured to prefetch the weights according to an upcoming context switch from the first artificial neural network to the second artificial neural network as indicated by the first execution schedule.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the DLA hardware device is configured to share compute resources between the plurality of artificial neural networks differently according to any of a plurality of execution schedule accessed from the memory array.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the DLA hardware device is configured to share compute resources differently without having the plurality of artificial neural networks redeployed on the DLA hardware device.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A system, comprising:<claim-text>a first sensor;</claim-text><claim-text>a second sensor; and</claim-text><claim-text>a memory device coupled to the first and the second sensors and to a deep learning accelerator (DLA) hardware device having a first artificial neural network and a second artificial neural network compiled as a single workload deployed thereon;</claim-text><claim-text>wherein the DLA hardware device is configured to:<claim-text>operate the first artificial neural network based on data from the first sensor;</claim-text><claim-text>operate the second artificial neural network based on data from the second sensor;</claim-text><claim-text>operate the first and the second ANN according to a first schedule in response the first sensor producing a greater quantity of data than the second sensor over a period of time; and</claim-text><claim-text>operate the first and the second ANN according to a second schedule in response to the second sensor producing a greater quantity of data than the second sensor over the period of time.</claim-text></claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The system of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the DLA hardware device is a component of the memory device.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The system of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the DLA hardware device comprises an application specific integrated circuit.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The system of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the DLA hardware device comprises a field programmable gate array.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The system of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the first sensor, the second sensor, and the memory device comprise a fog computing node; and<claim-text>wherein the system further comprises:<claim-text>a plurality of fog computing nodes; and</claim-text><claim-text>a server coupled to the plurality of fog computing nodes, wherein the server is configured to execute a surveillance application using data received from the plurality of fog computing nodes.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>