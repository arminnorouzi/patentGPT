<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005197A9-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005197</doc-number><kind>A9</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17204638</doc-number><date>20210317</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><us-publication-filing-type><us-republication-corrected/></us-publication-filing-type><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>60</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0484</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>60</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>001</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04845</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04842</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SKY REPLACEMENT</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63093237</doc-number><date>20201018</date></document-id></us-provisional-application><related-publication><document-id><country>US</country><doc-number>20220301243</doc-number><kind>A1</kind><date>20220922</date></document-id></related-publication></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>ADOBE INC.</orgname><address><city>SAN JOSE</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>JIANMING</first-name><address><city>Campbell</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Erickson</last-name><first-name>Alan</first-name><address><city>Franktown</city><state>CO</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Pao</last-name><first-name>I-Ming</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Feng</last-name><first-name>Guotong</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Sunkavalli</last-name><first-name>Kalyan</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Mandia</last-name><first-name>Frederick</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>Byun</last-name><first-name>Hyunghwan</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="07" designation="us-only"><addressbook><last-name>Leong</last-name><first-name>Betty</first-name><address><city>Los Altos</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="08" designation="us-only"><addressbook><last-name>Payne Stotzner</last-name><first-name>Meredith</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="09" designation="us-only"><addressbook><last-name>Takahashi</last-name><first-name>Yukie</first-name><address><city>Newark</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="10" designation="us-only"><addressbook><last-name>Le</last-name><first-name>Quynn Megan</first-name><address><city>Morgan Hill</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="11" designation="us-only"><addressbook><last-name>Kong</last-name><first-name>Sarah</first-name><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure provides systems and methods for image editing. Embodiments of the present disclosure provide an image editing system for perform image object replacement or image region replacement (e.g., an image editing system for replacing an object or region of an image with an object or region from another image). For example, the image editing system may replace a sky portion of an image with a more desirable sky portion from a different replacement image. The original image and the replacement image (e.g., the image including a desirable object or region) include layers of masks. A sky from the replacement image may replace the sky of the image to produce an aesthetically pleasing composite image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="180.59mm" wi="116.76mm" file="US20230005197A9-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="207.18mm" wi="118.79mm" file="US20230005197A9-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="235.88mm" wi="149.78mm" file="US20230005197A9-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="219.46mm" wi="156.63mm" file="US20230005197A9-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="194.39mm" wi="156.63mm" file="US20230005197A9-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="196.77mm" wi="156.63mm" file="US20230005197A9-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="203.37mm" wi="126.24mm" file="US20230005197A9-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="237.41mm" wi="144.44mm" file="US20230005197A9-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="193.38mm" wi="127.00mm" file="US20230005197A9-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="192.70mm" wi="129.71mm" file="US20230005197A9-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="207.94mm" wi="127.59mm" file="US20230005197A9-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="214.71mm" wi="127.59mm" file="US20230005197A9-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="210.31mm" wi="108.20mm" file="US20230005197A9-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="217.93mm" wi="127.59mm" file="US20230005197A9-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="203.03mm" wi="127.59mm" file="US20230005197A9-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="244.69mm" wi="139.36mm" file="US20230005197A9-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="239.27mm" wi="150.03mm" file="US20230005197A9-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">The following relates generally to image editing, and more specifically to replacing regions or objects of an image with portions of another image.</p><p id="p-0003" num="0002">Image editing is a process used to alter properties of an image, for example, to increase the quality of an image or video. In some cases, an image is altered to have a desired appearance or to improve the visibility or clarity of the image. Replacing portions of an image and inserting portions of one image into another are common image editing tasks.</p><p id="p-0004" num="0003">For example, in some cases users may wish to replace the sky in one image with the sky from another image. Sky replacement and other region replacement tasks can be performed by image editing software applications. However, transferring portions of one image into another image can be difficult and time-consuming. For example, using current techniques, the photographer first manually identifies a sky region and a foreground region with labels or brushes. Once identified, the sky and foreground regions can be segmented from each other.</p><p id="p-0005" num="0004">In some existing cases, the identification and segmentation of the different regions may include manually assigning individual pixels a sky or non-sky label. This manual segmenting process is done for both the image having the desired foreground region and the image having the desired sky region. Segmentation problems may arise due to a number of factors including large variations in appearance, and complicated boundaries with other regions or objects such as trees. That is, small portions of a sky may be located between leaves of a tree, which can make segmentation challenging or inaccurate.</p><p id="p-0006" num="0005">Thus, users who wish to replace regions of an image are often faced with a tedious and time-consuming task. Furthermore, conventional image editing applications may create unwanted visual artefacts and unnatural effects when inserting content from one image into another. Therefore, there is a need in the art for an improved image editing application that can efficiently combine images in a natural looking way.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0007" num="0006">The present disclosure provides systems and methods for image editing. Embodiments of the present disclosure provide an image editing system for perform image object replacement or image region replacement (e.g., an image editing system for replacing an object or region of an image with an object or region from another image). For example, the image editing system may replace a sky portion of an image with a more desirable sky portion from a different sky replacement image (e.g., a sky preset image).</p><p id="p-0008" num="0007">A method, apparatus, non-transitory computer readable medium, and system for image editing are described. Embodiments of the method, apparatus, non-transitory computer readable medium, and system are configured to generate a plurality of region masks for a first image using a mask generation network, wherein the region masks correspond to a same semantically related region of the first image, generate a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image, generate a region-specific layer by combining a second region mask of the plurality of region masks with the second image, and generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0009" num="0008">A method, apparatus, non-transitory computer readable medium, and system for image editing are described. Embodiments of the method, apparatus, non-transitory computer readable medium, and system are configured to generate a first region mask, a second region mask, and a third region mask corresponding to a same semantically related region of a first image, generate a defringing layer by combining the first region mask with grayscale version of the second image, generate a region-specific layer by combining the second region mask and the third region mask to produce a combined region mask, and combining the combined region mask with the second image, and generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0010" num="0009">An apparatus, system, and method for image editing are described. Embodiments of the apparatus, system, and method include a mask generation network configured to generate a plurality of region masks for a first image using a mask generation network, wherein the region masks correspond to a same semantically related region of the first image, a defringing component configured to generate a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image, a region-specific layer component configured to generate a region-specific layer by combining a second region mask of the plurality of region masks with the second image, and a layer composition component configured to generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example of an image editing system according to aspects of the present disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an example of a process for image editing according to aspects of the present disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an example of an image editing apparatus according to aspects of the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example of a preset component according to aspects of the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example of a horizon adjustment process according to aspects of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example of a region layer flowchart according to aspects of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example of a region layer diagram according to aspects of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an example of a lighting layer flowchart according to aspects of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an example of a lighting layer diagram according to aspects of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. <b>10</b> through <b>11</b></figref> show examples of a process for generating a composite image according to aspects of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows an example of a process for color harmonization according to aspects of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. <b>13</b> through <b>14</b></figref> show examples of a process for image editing according to aspects of the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>15</b></figref> shows an example of a preset image diagram according to aspects of the present disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows an example of a preset image representation structure according to aspects of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0025" num="0024">The present disclosure provides systems and methods for image editing. Embodiments of the present disclosure provide an image editing system for performing image object replacement or image region replacement (e.g., an image editing system for replacing an object or region of an image with an object or region from another image). In some embodiments, one or more region masks are generated based on an original image. The region masks are combined with portions of a source image in multiple layers, and the layers are combined to produce a composite image.</p><p id="p-0026" num="0025">In an example scenario, a photographer may perform a photoshoot on an overcast or rainy day. However, the overcast sky may not be desirable for an aesthetically pleasing image. Therefore, the photographer may prefer another sky from another picture taken during a sunny day. However, the process of replacing a sky portion of the image involves segmenting the foreground of the image from the sky, replacing the sky with a sky from another image, and performing color matching to make the composite image natural looking. This process can be difficult and time-consuming.</p><p id="p-0027" num="0026">Using current techniques, a photographer first manually identifies the sky region and foreground region with labels or brushes in a design application. Once identified, the sky and foreground regions can be segmented from each other. In some cases, the identification and segmentation of the different regions may include manually assigning individual pixels a sky or non-sky label. These and other segmentation problems arise due to a number of factors including large variations in image appearance and complicated boundaries with other regions or objects such as trees, mountains, water, and the like. Such techniques can be tedious and time-consuming, and in some cases result in unwanted visual artefacts and unnatural effects in generated composite images.</p><p id="p-0028" num="0027">Embodiments of the present disclosure include an improved image replacement process that automatically segments the foreground and background of an image (i.e., the original image) using one or more image segmentation masks. In some examples, two region masks are generated that have varying levels of sharpness at the boundary (e.g., the boundary between the foreground and the background. These masks are combined to form a composite mask that achieves the appropriate level of boundary sharpness. Adjustment options, such as a brush tool, a fade edge, and a shift edge may be used to combine the masks.</p><p id="p-0029" num="0028">In some embodiments, a defringing layer is generated by combining a region mask of the original image with a grayscale version of the replacement image. The defringing layer is combined with the original image and a region specific layer (e.g., a layer that reveals a replacement background from another image) to produce a composite image. Accordingly, the produced composite image may include the foreground of the original image and the background of the replacement image, but the lighting of the composite image near the region boundary is more realistic.</p><p id="p-0030" num="0029">Embodiments of the present disclosure provide a fully automatic, non-destructive image replacement method using the original image and a replacement image as input. A layer structure enables the non-destructive effect. That is, the replacement effect is provided by adding layers on top of an original image rather than editing the image itself. Embodiments of the present disclosure use an advanced pipeline for mask editing in a preview mode.</p><p id="p-0031" num="0030">Additionally, real-time color harmonization based on the visible sky region of the replacement image may be used to produce more natural colorization. In some examples, horizon-aware sky alignment and placement with advanced padding may also be used. For example, the horizons of the original image and the replacement image may be automatically detected and aligned. Additionally, some embodiments of the present disclosure provide fast preset loading and saving.</p><p id="p-0032" num="0031">Embodiments of the present disclosure may be used in the context of a tool for replacing a sky region of one image with a sky region of another image. A sky replacement example is provided with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> through <b>5</b></figref>. Details regarding generating a region replacement layer (e.g., a sky layer) are provided with reference to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>. Examples of a defringing layer are provided with reference to <figref idref="DRAWINGS">FIGS. <b>8</b> through <b>10</b></figref>. Details regarding color harmonization layer are provided with reference to <figref idref="DRAWINGS">FIGS. <b>11</b> and <b>12</b></figref>. Details regarding preset loading are provided with reference to <figref idref="DRAWINGS">FIGS. <b>13</b> through <b>16</b></figref>.</p><p id="p-0033" num="0032">Sky Replacement</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example of an image editing system according to aspects of the present disclosure. The example shown includes user <b>100</b>, device <b>105</b>, cloud <b>110</b>, image editing apparatus <b>115</b>, and database <b>120</b>. Image editing apparatus <b>115</b> is an example of, or includes aspects of, the corresponding element described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0035" num="0034">The image editing system uses compositing methods based on combining images and segmentation masks to replace a sky portion of an image while suppressing artefacts such as a halo/fringing effect, to automatically create high-quality sky replacement results.</p><p id="p-0036" num="0035">Some compositing techniques such as alpha matting result in unwanted visual artefacts. Even if the segmentation is perfect, differences between the images being composited (e.g., color/brightness differences between the foreground from one image and the sky from another) may lead to objectionable fringing artefacts. This can be mitigated by blurring out the segmentation, but such blurring may lead to halo artefacts.</p><p id="p-0037" num="0036">Therefore, in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the user <b>100</b> interacts with an image editing apparatus <b>115</b> via a device <b>105</b>. In some examples, the image editing apparatus <b>115</b> is located on the cloud <b>110</b>. However, in some examples the image editing apparatus <b>115</b> is located on the device <b>105</b> itself. The user <b>100</b> provide an original image such as a picture or photograph. In the example shown, the original image includes a building in rainy weather. The user <b>100</b> then finds an image with a more appealing background (i.e., from among images stored on the device <b>105</b> or within the database <b>120</b>).</p><p id="p-0038" num="0037">The image editing apparatus <b>115</b> segments the original image to identify a foreground and a background. Then, the image editing apparatus <b>115</b> creates a composite image using the foreground of the original image and a sky region from the selected replacement image. Multiple segmentation masks may be combined with the sky replacement image to create a sky replacement layer, and one of these segmentation masks (or a separate mask) may also be combined with a greyscale version of the replacement sky to create a defringing layer (e.g., which in some case may be referred to as a lighting layer) that reduces the halo effect. In some embodiments, the image editing apparatus <b>115</b> automatically harmonize the colors of the foreground and the new background (i.e., the replacement sky).</p><p id="p-0039" num="0038">Embodiments of the present disclosure utilize compositing methods that generate high-quality composites without fringing and halo artefacts. In some examples, the methods described herein generate multiple masks&#x2014;e.g., a hard mask and a soft mask&#x2014;and use carefully selected blending modes to combine the masks. This method may be performed automatically and works well across many sky replacement examples. In some cases, the input to the system includes a single image and a preset reference image is applied for the replacement sky. In other embodiments, a user <b>100</b> selects two images and a composite is made from the two images. In some cases, the system automatically selects portions of the images for composition (i.e., it can identify and replace a sky region automatically). According to embodiments of the present disclosure, a layer structure is used that enables non-destructive effects.</p><p id="p-0040" num="0039">The device <b>105</b> may be a personal computer, laptop computer, mainframe computer, palmtop computer, personal assistant, mobile device, or any other suitable processing apparatus. The device <b>105</b> may include image editing software. The image editing software can include a variety of editing tools including the sky replacement system described in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0041" num="0040">In some examples, a device <b>105</b> may also include an optical instrument (e.g., an image sensor, camera, etc.) for recording or capturing images, which may be stored locally, transmitted to another location, etc. For example, an image sensor may capture visual information using one or more photosensitive elements that may be tuned for sensitivity to a visible spectrum of electromagnetic radiation. The resolution of such visual information may be measured in pixels, where each pixel may relate an independent piece of captured information. In some cases, each pixel may thus correspond to one component of, for example, a two-dimensional (2D) Fourier transform of an image. Computation methods may use pixel information to reconstruct images captured by the device.</p><p id="p-0042" num="0041">A cloud <b>110</b> is a computer network configured to provide on-demand availability of computer system resources, such as data storage and computing power. In some examples, the cloud <b>110</b> provides resources without active management by the user <b>100</b>. The term cloud is sometimes used to describe data centers available to many users <b>100</b> over the Internet. Some large cloud networks have functions distributed over multiple locations from central servers. A server is designated an edge server if it has a direct or close connection to a user <b>100</b>. In some cases, a cloud <b>110</b> is limited to a single organization. In other examples, the cloud is available to many organizations. In one example, a cloud <b>110</b> includes a multi-layer communications network comprising multiple edge routers and core routers. In another example, a cloud <b>110</b> is based on a local collection of switches in a single physical location.</p><p id="p-0043" num="0042">According to some embodiments, database <b>120</b> stores preset information including the region location information and the low-resolution image data for each of the set of preset images in a same preset information file. In some examples, database <b>120</b> stores high-resolution image data for each of the preset images in separate image files. In some examples, the separate image files includes JPEG or PNG files. According to some embodiments, database <b>120</b> may be configured to store a preset information file including the region location information and the low-resolution image data in a preset information file and to store high-resolution image data for the preset images in separate image files.</p><p id="p-0044" num="0043">The image editing apparatus <b>115</b> includes a computer implemented network that generates a composite image. In some embodiments, the image editing apparatus <b>115</b> includes a mask generation network, a defringing component, a region-specific layer component, a layer composition component, a color harmonization component, an image property component, an image editing application, and a preset component. The preset component will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The image editing apparatus <b>115</b> takes an original image and a replacement image to produce a composite image.</p><p id="p-0045" num="0044">The image editing apparatus <b>115</b> may also include a processor unit, a memory unit, and a user interface. Additionally, image editing apparatus <b>115</b> can communicate with the database <b>120</b> via the cloud <b>110</b>. Further detail regarding the architecture of the image editing apparatus <b>115</b> is provided with reference to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>.</p><p id="p-0046" num="0045">In some cases, the image editing apparatus <b>115</b> is implemented on a server. A server provides one or more functions to users <b>100</b> linked by way of one or more of the various networks. In some cases, the server includes a single microprocessor board, which includes a microprocessor responsible for controlling all aspects of the server. In some cases, a server uses microprocessor and protocols to exchange data with other devices/users on one or more of the networks via hypertext transfer protocol (HTTP), and simple mail transfer protocol (SMTP), although other protocols such as file transfer protocol (FTP), and simple network management protocol (SNMP) may also be used. In some cases, a server is configured to send and receive hypertext markup language (HTML) formatted files (e.g., for displaying web pages). In various embodiments, a server comprises a general purpose computing device, a personal computer, a laptop computer, a mainframe computer, a supercomputer, or any other suitable processing apparatus.</p><p id="p-0047" num="0046">A database <b>120</b> is an organized collection of data. For example, a database <b>120</b> stores data in a specified format known as a schema. A database <b>120</b> may be structured as a single database, a distributed database, multiple distributed databases, or an emergency backup database. In some cases, a database controller may manage data storage and processing in a database <b>120</b>. In some cases, a user <b>100</b> interacts with database controller. In other cases, database controller may operate automatically without user <b>100</b> interaction.</p><p id="p-0048" num="0047">In some instances, the present disclosure uses the terms original image, foreground image, and target image interchangeably. For example, an original image may be the target for replacement with the sky from a replacement image (i.e., the source of the sky). In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the original image, sent by the user <b>100</b> to the image editing apparatus <b>115</b>, may include a foreground region (e.g., the building) and a target replacement region (e.g., the overcast sky, which the user <b>100</b> intends to replace via the image editing apparatus <b>115</b>).</p><p id="p-0049" num="0048">The terms replacement image, background image, preset image, source image, reference image, sky image, and sky preset may also be used interchangeably. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the replacement image, generated by the image editing apparatus <b>115</b> and displayed or returned to user <b>100</b>, may include a background region (e.g., a sunny sky), which in some cases may also be referred to as a source region, a sky region, etc.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an example of a process for image editing according to aspects of the present disclosure. In some examples, these operations are performed by a system including a processor executing a set of codes to control functional elements of an apparatus. Additionally or alternatively, certain processes are performed using special-purpose hardware. Generally, these operations are performed according to the methods and processes described in accordance with aspects of the present disclosure. In some cases, the operations described herein are composed of various substeps, or are performed in conjunction with other operations.</p><p id="p-0051" num="0050">Embodiments of the present disclosure utilize compositing methods that generate high-quality composites without fringing and halo artefacts. In some examples, the methods described herein generate a stack of layers&#x2014;a hard sky mask, a soft sky mask, a lighting mask, and lighting content&#x2014;and use carefully selected blending modes to combine them and suppress the halo/fringing. This method may be performed automatically and works well across many sky replacement examples.</p><p id="p-0052" num="0051">Accordingly, a method for image editing is described. Embodiments of the method generate a first region mask, a second region mask, and a third region mask corresponding to a same semantically related region of a first image. Embodiments of the method are further configured to generate a defringing layer by combining the first region mask with grayscale version of the second image, generate a region-specific layer by combining the second region mask and the third region mask to produce a combined region mask, and combine the combined region mask with the second image. Embodiments of the method are further configured to generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0053" num="0052">At operation <b>200</b>, the user provides an original image to the image editing apparatus. In some cases, the operations of this step refer to, or may be performed by, a user as described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In some cases, the input to the system includes a single image and a preset reference image is applied. In other embodiments, a user selects two images, and a composite is made from the two images. In some cases, the system automatically selects portions of the images for composition (i.e., it can identify and replace a sky region automatically). The original image and the replacement image can be in any of multiple formats (e.g., PNG, JPEG, RAW, etc.).</p><p id="p-0054" num="0053">Furthermore, according to embodiments of the present disclosure, a layer structure is used that provides the non-destructive effect. Some conventional image editing applications only generate a replaced picture (e.g., a new picture with the sky replaced). However, according to embodiments of the present disclosure, an image may be replaced with a composite picture including different layers (i.e., pixel and adjustment layers) with layer masks, which allows users to further adjust the individual component to their preferences (and to undo any unwanted changes).</p><p id="p-0055" num="0054">At operation <b>205</b>, the system displays preset previews. Thumbnail data of the presets are loaded for preview. The full resolution data of a preset may not be loaded into the memory until it is selected by the user for sky replacement. The user may preview the presets before selecting. In some cases, the operations of this step refer to, or may be performed by, an image editing apparatus as described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>3</b></figref>.</p><p id="p-0056" num="0055">At operation <b>210</b>, the user selects a preset image. When a user selects a preset, the regions in the preset image may be detected to find the coordinates for placing a new image into the composition. In some cases, the operations of this step refer to, or may be performed by, a user as described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0057" num="0056">At operation <b>215</b>, the system loads a high resolution version of the sky image. In some examples, the high resolution image of a preset image may not be loaded into the database until it is selected by the user for image replacement. In such cases, when a new image preset is selected for image replacement, the high resolution data of the previously selected image preset may be released from the database. In some cases, the operations of this step refer to, or may be performed by, an image editing apparatus as described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>3</b></figref>.</p><p id="p-0058" num="0057">At operation <b>220</b>, the system generates sky replacement layers. In some cases, the operations of this step refer to, or may be performed by, an image editing apparatus as described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>3</b></figref>. The layer structure may include a layer for the original image, a sky replacement layer (i.e., based on a combination of a foreground mask from the original image and a sky region from the replacement image), a defringing layer (based on a mask from the original image and a grayscale of the replacement sky), and a color harmonization layer. The layer structure is described further with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0059" num="0058">At operation <b>225</b>, the system generates a composite image by combining the layers.</p><p id="p-0060" num="0059">Additionally, an apparatus for performing the method is described. The apparatus includes a processor, memory in electronic communication with the processor, and instructions stored in the memory. The instructions are operable to cause the processor to generate a first region mask, a second region mask, and a third region mask corresponding to a same semantically related region of a first image, generate a defringing layer by combining the first region mask with grayscale version of the second image, generate a region-specific layer by combining the second region mask and the third region mask to produce a combined region mask, and combining the combined region mask with the second image, and generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0061" num="0060">A non-transitory computer readable medium storing code for image editing is described. In some examples, the code comprises instructions executable by a processor to: generate a first region mask, a second region mask, and a third region mask corresponding to a same semantically related region of a first image, generate a defringing layer by combining the first region mask with grayscale version of the second image, generate a region-specific layer by combining the second region mask and the third region mask to produce a combined region mask, and combining the combined region mask with the second image, and generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0062" num="0061">A system for image editing is described. Embodiments of the system are configured for generating a first region mask, a second region mask, and a third region mask corresponding to a same semantically related region of a first image, generating a defringing layer by combining the first region mask with grayscale version of the second image, generating a region-specific layer by combining the second region mask and the third region mask to produce a combined region mask, and combining the combined region mask with the second image, and generating a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0063" num="0062">Some examples of the method, apparatus, non-transitory computer readable medium, and system described above further include applying a mask brush to adjust the combined region mask. Some examples further include applying a fade edge adjustment, a shift edge adjustment, or both the fade edge adjustment and the shift edge adjustment after applying the mask brush.</p><p id="p-0064" num="0063">Some examples of the method, apparatus, non-transitory computer readable medium, and system described above further include applying the mask brush to further adjust the combined region mask. Some examples further include automatically reapplying the fade edge adjustment, the shift edge adjustment, or both the fade edge adjustment and the shift edge adjustment after reapplying the mask brush.</p><p id="p-0065" num="0064">Some examples of the method, apparatus, non-transitory computer readable medium, and system described above further include generating a color harmonization layer based on the second region mask, wherein the composite image includes the color harmonization layer.</p><p id="p-0066" num="0065">Some examples of the method, apparatus, non-transitory computer readable medium, and system described above further include adjusting a position of the second image relative to the first image. Some examples further include automatically regenerating the defringing layer and the region-specific layer based on the position. Some examples further include automatically regenerating the composite image based on the regenerated defringing layer and the regenerated region-specific layer.</p><p id="p-0067" num="0066">In some examples, the first region mask has a more gradual mask boundary than the second region mask, and the second region mask has a more gradual boundary than the third mask. In some examples, the semantically related region of the first image comprises a first sky region, and the composite image comprises a second sky region from the second image.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an example of an image editing apparatus <b>300</b> according to aspects of the present disclosure. In one embodiment, image editing apparatus <b>300</b> includes user interface <b>305</b>, processor unit <b>310</b>, memory unit <b>315</b>, mask generation network <b>320</b>, defringing component <b>325</b>, region-specific layer component <b>330</b>, layer composition component <b>335</b>, color harmonization component <b>340</b>, image property component <b>345</b>, image editing application <b>350</b>, and preset component <b>355</b>. Image editing apparatus <b>300</b> is an example of, or includes aspects of, the corresponding element described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0069" num="0068">An apparatus for image editing is described. Embodiments of the apparatus include a mask generation network <b>320</b> configured to generate a plurality of region masks for a first image using a mask generation network, where the region masks correspond to a same semantically related region of the first image. Embodiments of the apparatus further include a defringing component <b>325</b> configured to generate a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image. Embodiments of the apparatus further include a region-specific layer component <b>330</b> configured to generate a region-specific layer by combining a second region mask of the plurality of region masks with the second image. Embodiments of the apparatus further include a layer composition component <b>335</b> configured to generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0070" num="0069">A user interface <b>305</b> may enable a user to interact with a device. In some embodiments, the user interface <b>305</b> may include an audio device, such as an external speaker system, an external display device such as a display screen, or an input device (e.g., remote control device interfaced with the user interface <b>305</b> directly or through an input/output (IO) controller module). In some cases, a user interface <b>305</b> may be a graphical user interface <b>305</b> (GUI).</p><p id="p-0071" num="0070">A processor unit <b>310</b> is an intelligent hardware device, (e.g., a general-purpose processing component, a digital signal processor (DSP), a central processing unit (CPU), a graphics processing unit (GPU), a microcontroller, an application specific integrated circuit (ASIC), a field programmable gate array (FPGA), a programmable logic device, a discrete gate or transistor logic component, a discrete hardware component, or any combination thereof). In some cases, the processor is configured to operate a memory array using a memory controller. In other cases, a memory controller is integrated into the processor. In some cases, the processor is configured to execute computer-readable instructions stored in a memory to perform various functions. In some embodiments, a processor includes special purpose components for modem processing, baseband processing, digital signal processing, or transmission processing.</p><p id="p-0072" num="0071">Examples of a memory unit <b>315</b> include random access memory (RAM), read-only memory (ROM), or a hard disk. Examples of memory devices include solid state memory and a hard disk drive. In some examples, memory is used to store computer-readable, computer-executable software including instructions that, when executed, cause a processor to perform various functions described herein. In some cases, the memory contains, among other things, a basic input/output system (BIOS) which controls basic hardware or software operation such as the interaction with peripheral components or devices. In some cases, a memory controller operates memory cells. For example, the memory controller can include a row decoder, column decoder, or both. In some cases, memory cells within a memory store information in the form of a logical state.</p><p id="p-0073" num="0072">According to some embodiments, user interface <b>305</b> displays a set of low-resolution previews including the at least one low-resolution preview. In some examples, user interface <b>305</b> receives feedback indicating the preset image, where the preset image is selected based on the feedback. According to some embodiments, user interface <b>305</b> may be configured to display the at least one low-resolution preview and to receive feedback for selecting a preset image from among the plurality of preset images. User interface <b>305</b> is an example of, or includes aspects of, the corresponding element described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0074" num="0073">According to some embodiments, mask generation network <b>320</b> generates a set of region masks for a first image using a mask generation network <b>320</b>, where the region masks correspond to a same semantically related region of the first image. In some examples, mask generation network <b>320</b> combines the second region mask with a third region mask of the set of region masks to create a combined region mask, where the region-specific layer is generated using the combined region mask. In some examples, the first region mask has a more gradual mask boundary than the second region mask. In some examples, the semantically related region of the first image includes a first sky region, and the composite image includes a second sky region from the second image.</p><p id="p-0075" num="0074">According to some embodiments, mask generation network <b>320</b> generates a first region mask, a second region mask, and a third region mask corresponding to a same semantically related region of a first image. In some examples, mask generation network <b>320</b> applies a mask brush to adjust the combined region mask. In some examples, mask generation network <b>320</b> applies a fade edge adjustment, a shift edge adjustment, or both the fade edge adjustment and the shift edge adjustment after applying the mask brush. In some examples, mask generation network <b>320</b> applies the mask brush to further adjust the combined region mask. In some examples, mask generation network <b>320</b> automatically reapplies the fade edge adjustment, the shift edge adjustment, or both the fade edge adjustment and the shift edge adjustment after reapplying the mask brush. In some examples, the first region mask has a more gradual mask boundary than the second region mask, and the second region mask has a more gradual boundary than the third mask. In some examples, the semantically related region of the first image includes a first sky region, and the composite image includes a second sky region from the second image.</p><p id="p-0076" num="0075">According to some embodiments, mask generation network <b>320</b> may be configured to generate a plurality of region masks for a first image using a mask generation network <b>320</b>, wherein the region masks correspond to a same semantically related region of the first image.</p><p id="p-0077" num="0076">In some examples, the mask generation network <b>320</b> includes a convolutional neural network (CNN). A CNN is a class of neural network that is commonly used in computer vision or image classification systems. In some cases, a CNN may enable processing of digital images with minimal pre-processing. A CNN may be characterized by the use of convolutional (or cross-correlational) hidden layers. These layers apply a convolution operation to the input before signaling the result to the next layer. Each convolutional node may process data for a limited field of input (i.e., the receptive field). During a forward pass of the CNN, filters at each layer may be convolved across the input volume, computing the dot product between the filter and the input. During the training process, the filters may be modified so that they activate when they detect a particular feature within the input.</p><p id="p-0078" num="0077">In some embodiments, one or more components of the image editing apparatus <b>300</b> may include (or implement) one or more aspects of an artificial neural network (ANN). An ANN is a hardware or a software component that includes a number of connected nodes (i.e., artificial neurons), which loosely correspond to the neurons in a human brain. Each connection, or edge, transmits a signal from one node to another (like the physical synapses in a brain). When a node receives a signal, it processes the signal and then transmits the processed signal to other connected nodes. In some cases, the signals between nodes comprise real numbers, and the output of each node is computed by a function of the sum of its inputs. Each node and edge is associated with one or more node weights that determine how the signal is processed and transmitted.</p><p id="p-0079" num="0078">During the training process, these weights are adjusted to improve the accuracy of the result (i.e., by minimizing a loss function which corresponds in some way to the difference between the current result and the target result). The weight of an edge increases or decreases the strength of the signal transmitted between nodes. In some cases, nodes have a threshold below which a signal is not transmitted at all. In some examples, the nodes are aggregated into layers. Different layers perform different transformations on their inputs. The initial layer is known as the input layer and the last layer is known as the output layer. In some cases, signals traverse certain layers multiple times.</p><p id="p-0080" num="0079">In some embodiments, multiple neural networks are used for generating segmentation masks. For example, in one embodiment, the mask generation network <b>320</b> may include a model for generating a base mask, one for refining the base mask, and one for detecting difficult regions (e.g., trees or wires). All of the models may be used to generate the following masks: a hard mask (e.g., for the region replacement layer), a soft mask (e.g., for the region replacement layer), and a lighting/defringing layer.</p><p id="p-0081" num="0080">According to some embodiments, mask generation network <b>320</b> generates a foreground region mask for a first image using a mask generation network <b>320</b>. The mask generation network <b>320</b> may be configured to generate a foreground region mask for a first image. In some examples, the mask generation network <b>320</b> includes a convolutional neural network.</p><p id="p-0082" num="0081">According to some embodiments, defringing component <b>325</b> generates a defringing layer by combining a first region mask of the set of region masks with grayscale version of a second image. In some examples, the defringing layer is located between the first image and the region-specific layer. In some examples, defringing component <b>325</b> adjusts a position of the second image relative to the first image. According to some embodiments, defringing component <b>325</b> generates a defringing layer by combining the first region mask with grayscale version of the second image. In some examples, defringing component <b>325</b> adjusts a position of the second image relative to the first image. In some examples, defringing component <b>325</b> automatically regenerates the defringing layer and the region-specific layer based on the position.</p><p id="p-0083" num="0082">According to some embodiments, defringing component <b>325</b> may be configured to generate a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image. According to some embodiments, defringing component <b>325</b> adjusts a position of the second image relative to the first image. In some examples, defringing component <b>325</b> generates a defringing layer based on grayscale data of the second image and the output of the mask generation network <b>320</b>, where the composite image further includes the defringing layer. According to some embodiments, defringing component <b>325</b> may be configured to generate a defringing layer based on an output of the mask generation network <b>320</b> and grayscale data of the second image.</p><p id="p-0084" num="0083">According to some embodiments, region-specific layer component <b>330</b> generates a region-specific layer by combining a second region mask of the set of region masks with the second image. According to some embodiments, region-specific layer component <b>330</b> generates a region-specific layer by combining the second region mask and the third region mask to produce a combined region mask, and combining the combined region mask with the second image. According to some embodiments, region-specific layer component <b>330</b> may be configured to generate a region-specific layer by combining a second region mask of the plurality of region masks with the second image. In some examples, the region-specific layer component <b>330</b> includes a brush tool, a fade edge slider, and a shift edge slider.</p><p id="p-0085" num="0084">According to some embodiments, region-specific layer component <b>330</b> generates a background region layer based on the second image and an output of the mask generation network <b>320</b>, where the composite image includes the first image, the color harmonization layer, and the background region layer. According to some embodiments, region-specific layer component <b>330</b> may be configured to generate a background region layer based on an output of the mask generation network <b>320</b> and the second image.</p><p id="p-0086" num="0085">According to some embodiments, layer composition component <b>335</b> generates a composite image by combining the first image, the defringing layer, and the region-specific layer. In some examples, layer composition component <b>335</b> automatically regenerates the composite image based on the adjusted position. According to some embodiments, layer composition component <b>335</b> generates a composite image by combining the first image, the defringing layer, and the region-specific layer. In some examples, layer composition component <b>335</b> automatically regenerates the composite image based on the regenerated defringing layer and the regenerated region-specific layer. According to some embodiments, layer composition component <b>335</b> may be configured to generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0087" num="0086">According to some embodiments, color harmonization component <b>340</b> generates a color harmonization layer based on at least one of the set of region masks, where the composite image includes the color harmonization layer. According to some embodiments, color harmonization component <b>340</b> generates a color harmonization layer based on the second region mask, where the composite image includes the color harmonization layer. According to some embodiments, color harmonization component <b>340</b> may be configured to generate a color harmonization layer based on at least one of the plurality of region masks.</p><p id="p-0088" num="0087">According to some embodiments, color harmonization component <b>340</b> generates a color harmonization layer based on the foreground property data and the background property data. In some examples, color harmonization component <b>340</b> automatically adjusts the color harmonization layer based on the adjusted position. In some examples, color harmonization component <b>340</b> automatically adjusts the color harmonization layer based on the adjusted colors. In some examples, color harmonization component <b>340</b> computes a set of color harmonization curves, where the color harmonization layer is generated based on the color harmonization curves. In some examples, the color harmonization layer is located between the first image and the background region layer. In some examples, the color harmonization layer applies colors from a background portion of the second image to a foreground portion of the first image.</p><p id="p-0089" num="0088">According to some embodiments, color harmonization component <b>340</b> generates a color harmonization layer based on the foreground property data and the background property data. In some examples, color harmonization component <b>340</b> automatically adjusts the color harmonization layer based on the change. In some examples, the color harmonization layer applies colors from a background portion of the second image to a foreground portion of the first image.</p><p id="p-0090" num="0089">According to some embodiments, color harmonization component <b>340</b> may be configured to generate a color harmonization layer based on the foreground property data and the background property data. In some examples, the color harmonization component <b>340</b> is configured to detect a change in the background property data and automatically adjust the color harmonization layer based on the change.</p><p id="p-0091" num="0090">According to some embodiments, image property component <b>345</b> computes foreground property data based on the foreground region mask and the first image. In some examples, image property component <b>345</b> computes background property data based on a second image. In some examples, the background property data is computed based on an output of the mask generation network <b>320</b>. According to some embodiments, image property component <b>345</b> computes foreground property data based on a foreground region mask and a first image. In some examples, image property component <b>345</b> computes background property data based on a second image. In some examples, image property component <b>345</b> detects a change in the foreground property data or the background property data. In some examples, the change includes a position change of the second image with respect to the first image. In some examples, the change includes a change in color of the second image. In some examples, the change includes a change in scale of the second image.</p><p id="p-0092" num="0091">According to some embodiments, image property component <b>345</b> may be configured to compute foreground property data based on the first image and background property data based on a second image.</p><p id="p-0093" num="0092">According to some embodiments, image editing application <b>350</b> edits the composite image using an image editing application <b>350</b>. According to some embodiments, image editing application <b>350</b> generates a composite image based on the first image, the second image, and the color harmonization layer. In some examples, image editing application <b>350</b> automatically adjusts the composite image based on the adjusted color harmonization layer. In some examples, image editing application <b>350</b> adjusts colors of the second image. In some examples, image editing application <b>350</b> automatically adjusts the composite image based on the adjusted color harmonization layer. In some examples, the composite image replaces a first sky region of the first image with a second sky region from the second image.</p><p id="p-0094" num="0093">According to some embodiments, image editing application <b>350</b> generates a composite image based on the first image, the second image, and the color harmonization layer. According to some embodiments, image editing application <b>350</b> may be configured to generate a composite image based on the first image, the second image, and the color harmonization layer.</p><p id="p-0095" num="0094">Preset component <b>355</b> is an example of, or includes aspects of, the corresponding element described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. According to some embodiments, preset component <b>355</b> receives original image data. In some examples, preset component <b>355</b> retrieves preset information for set of preset images, where the preset information for each of the preset images includes low-resolution image data and region location information. In some examples, the region location information corresponds to a sky region of the preset image. According to some embodiments, preset component <b>355</b> loads the preset information from the preset information file. In some examples, preset component <b>355</b> selects a preset image from among the set of preset images based on the preset information. In some examples, preset component <b>355</b> loads the high-resolution image data for the preset image from one of the separate image files based on the selection. In some examples, the preset information further includes image metadata.</p><p id="p-0096" num="0095">The described systems and methods may be implemented or performed by devices that include a general-purpose processor, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components, or any combination thereof. A general-purpose processor may be a microprocessor, a conventional processor, controller, microcontroller, or state machine. A processor may also be implemented as a combination of computing devices (e.g., a combination of a DSP and a microprocessor, multiple microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration). Thus, the functions described herein may be implemented in hardware or software and may be executed by a processor, firmware, or any combination thereof. If implemented in software executed by a processor, the functions may be stored in the form of instructions or code on a computer-readable medium.</p><p id="p-0097" num="0096">Computer-readable media includes both non-transitory computer storage media and communication media including any medium that facilitates the transfer of code or data. A non-transitory storage medium may be any available medium that can be accessed by a computer. For example, non-transitory computer-readable media can comprise random access memory (RAM), read-only memory (ROM), electrically erasable programmable read-only memory (EEPROM), compact disk (CD) or other optical disk storage, magnetic disk storage, or any other non-transitory medium for carrying or storing data or code.</p><p id="p-0098" num="0097">Also, connecting components may be properly termed computer-readable media. For example, if code or data is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technology such as infrared, radio, or microwave signals, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technology are included in the definition of medium. Combinations of media are also included within the scope of computer-readable media.</p><p id="p-0099" num="0098">A system for image editing is described. The system comprises a mask generation network configured to generate a plurality of region masks for a first image using a mask generation network, wherein the region masks correspond to a same semantically related region of the first image, a defringing component configured to generate a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image, a region-specific layer component configured to generate a region-specific layer by combining a second region mask of the plurality of region masks with the second image, and a layer composition component configured to generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0100" num="0099">A method of manufacturing an apparatus for image editing is described. The method provides a mask generation network configured to generate a plurality of region masks for a first image using a mask generation network, wherein the region masks correspond to a same semantically related region of the first image, a defringing component configured to generate a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image, a region-specific layer component configured to generate a region-specific layer by combining a second region mask of the plurality of region masks with the second image, and a layer composition component configured to generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0101" num="0100">A method of using an apparatus for image editing is described. The method uses a mask generation network configured to generate a plurality of region masks for a first image using a mask generation network, wherein the region masks correspond to a same semantically related region of the first image, a defringing component configured to generate a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image, a region-specific layer component configured to generate a region-specific layer by combining a second region mask of the plurality of region masks with the second image, and a layer composition component configured to generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0102" num="0101">In some examples, the mask generation network comprises a CNN. As described herein, a CNN is a class of neural network that is commonly used in computer vision or image classification systems. In some examples, the region-specific layer component comprises a brush tool, a fade edge slider, and a shift edge slider. Some examples of the apparatus, system, and method described above further include a color harmonization component configured to generate a color harmonization layer based on at least one of the plurality of region masks.</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example of a preset component <b>400</b> according to aspects of the present disclosure. Preset component <b>400</b> is an example of, or includes aspects of, the corresponding element described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In one embodiment, preset component <b>400</b> includes segmentation network <b>405</b>, thumbnail component <b>410</b>, preview component <b>415</b>, user interface <b>420</b>, and color conversion component <b>425</b>.</p><p id="p-0104" num="0103">According to some embodiments, segmentation network <b>405</b> performs a segmentation operation on the high-resolution image data to produce the region location information. In some examples, the region location information includes bounding box information. According to some embodiments, segmentation network <b>405</b> performs a segmentation information on each of a set of preset images to produce region location information. In some examples, the region location information includes bounding box information. According to some embodiments, segmentation network <b>405</b> may be configured to generate region location information for a plurality of preset images. In some examples, the segmentation network <b>405</b> includes a CNN.</p><p id="p-0105" num="0104">According to some embodiments, thumbnail component <b>410</b> generates at least one low-resolution preview based on the original image data and the preset information. According to some embodiments, thumbnail component <b>410</b> generates low-resolution image data for each of the preset images. According to some embodiments, thumbnail component <b>410</b> may be configured to generate low-resolution image data for the preset images.</p><p id="p-0106" num="0105">According to some embodiments, preview component <b>415</b> selects a preset image from among the set of preset images based on the at least one low-resolution preview image. In some examples, the low-resolution preview includes at least one region of the corresponding preset image combined with at least one region of the original image data. According to some embodiments, preview component <b>415</b> may be configured to generate at least one low-resolution preview based on the preset information.</p><p id="p-0107" num="0106">User interface <b>420</b> is an example of, or includes aspects of, the corresponding element described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0108" num="0107">According to some embodiments, color conversion component <b>425</b> loads high-resolution image data for the selected preset image. In some examples, color conversion component <b>425</b> performs color conversion on the high-resolution image data based on the original image data. According to some embodiments, color conversion component <b>425</b> performs color conversion on the preset images to produce the high-resolution image data. According to some embodiments, color conversion component <b>425</b> may be configured to perform color conversion on the preset images to produce the high-resolution image data.</p><p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example of a horizon adjustment process according to aspects of the present disclosure. In one embodiment, layers panel <b>500</b> includes sky layer display <b>505</b>, foreground lighting layer display <b>510</b>, foreground color layer display <b>515</b>, and original image layer <b>520</b>.</p><p id="p-0110" num="0109">Embodiments of the present disclosure provide tools to allow users to adjust a mask both globally (with a fade edge slider) and locally (i.e., with brush tool) to adjust the masks in different modes with precise control on finer details. In conjunction with the layers, an embodiment provides editing controls where users may adjust the composite image. In some examples, an automatic method initializes the control parameters, and the user can fine-tune them. In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a layers panel <b>500</b> may include sky layer display <b>505</b>, foreground lighting layer display <b>510</b>, foreground color layer display <b>515</b>, and original image layer <b>520</b>, which may display corresponding image editing aspects described herein.</p><p id="p-0111" num="0110">Region Layer</p><p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example of a region layer flowchart according to aspects of the present disclosure. In some examples, these operations are performed by a system including a processor executing a set of codes to control functional elements of an apparatus. Additionally or alternatively, certain processes are performed using special-purpose hardware. Generally, these operations are performed according to the methods and processes described in accordance with aspects of the present disclosure. In some cases, the operations described herein are composed of various substeps, or are performed in conjunction with other operations.</p><p id="p-0113" num="0112">At operation <b>600</b>, the system identifies a first region mask and a second region mask for a first image, where the first region mask indicates a semantically related portion of the image, and where the second region mask indicates the semantically related portion of the image with a softer boundary than the first region mask. In some cases, the operations of this step refer to, or may be performed by, a mask generation network as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0114" num="0113">At operation <b>605</b>, the system identifies a reference region from a second image. In some cases, the operations of this step refer to, or may be performed by, a region-specific layer component as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0115" num="0114">At operation <b>610</b>, the system generates a region layer based on the first region mask and the second region mask and the reference region. In some cases, the operations of this step refer to, or may be performed by, a region-specific layer component as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0116" num="0115">At operation <b>615</b>, the system generates a composite image by combining the first image and the region layer. In some cases, the operations of this step refer to, or may be performed by, an image composition component as described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example of a region layer diagram according to aspects of the present disclosure. The example shown includes original image <b>700</b>, hard mask <b>705</b>, soft mask <b>710</b>, brushed hard mask <b>715</b>, brushed soft mask <b>720</b>, and region mask <b>725</b>. Region mask <b>725</b> is an example of, or includes aspects of, the corresponding element described with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0118" num="0117">The hard mask <b>705</b> and the soft mask <b>710</b> may then be manually edited by a user using a mask brush to create a brushed hard mask <b>715</b> and a brushed soft mask <b>720</b>, respectively. Then, a mask blending process is used to combine the brushed hard mask <b>715</b> and the brushed soft mask <b>720</b> into a single mask. For example, the mask blending process may take a weighted average of the two masks. In some cases, user editable parameters may determine how the two masks are blended. In some examples, a fade edge process and a shift edge process may be used to blend the brushed hard mask <b>715</b> and the brushed soft mask <b>720</b>. The blending process can be used to adjust a boundary region (i.e., areas of the resulting mask that are not binary) in either direction (i.e., to reveal more background or more foreground). The fade edge process and the shift edge process may also be user controllable.</p><p id="p-0119" num="0118">The mask that results from combining and editing the brushed hard mask <b>715</b> and a brushed soft mask <b>720</b> may be referred to as a region mask <b>725</b> (or a sky layer in the case of sky replacement). The region mask <b>725</b> also includes a selected region from a reference image (i.e., the image that will be masked prior to combining with the original image). In some cases, the reference image may be moved or repositioned (either automatically or dynamically by the user) so that a different portion is visible through the region mask. In the sky replacement example, the horizon of the reference image may be positioned to align with the horizon of the original image as described herein.</p><p id="p-0120" num="0119">According to an embodiment, both global and local edits to the masks can coexist and be performed in any order without loss of any edit. Local edits (e.g., using the mask brush tool) give fine control to improve the result and fix defects in the generated masks. Local edits are applied to copies of the original hard mask <b>705</b> and soft mask <b>710</b> (e.g., which may result in brushed hard mask <b>715</b> and brushed soft mask <b>720</b>). In some examples, two painted masks are blended together (i.e., according to a fade edge setting) to form a combined region mask. Then, a shift edge is applied to the combined mask. Fade edge and shift edge settings can be adjusted without losing the brush edits (e.g., resulting in region mask <b>725</b>).</p><p id="p-0121" num="0120">Defringing (Lighting) Layer</p><p id="p-0122" num="0121"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an example of a lighting layer flowchart according to aspects of the present disclosure. In some examples, these operations are performed by a system including a processor executing a set of codes to control functional elements of an apparatus. Additionally or alternatively, certain processes are performed using special-purpose hardware. Generally, these operations are performed according to the methods and processes described in accordance with aspects of the present disclosure. In some cases, the operations described herein are composed of various substeps, or are performed in conjunction with other operations.</p><p id="p-0123" num="0122">At operation <b>800</b>, the system identifies a region mask and a lighting mask based on a first image. In some cases, the operations of this step refer to, or may be performed by, a mask generation network as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0124" num="0123">At operation <b>805</b>, the system generates a region layer based on the region mask and a second image. In some cases, the operations of this step refer to, or may be performed by, a region-specific layer component as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0125" num="0124">At operation <b>810</b>, the system generates a lighting layer based on the lighting mask and the second image. In some cases, the operations of this step refer to, or may be performed by, a region-specific layer component as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0126" num="0125">At operation <b>815</b>, the system generates a composite image based on the first image, the region layer, and the lighting layer. In some cases, the operations of this step refer to, or may be performed by, a layer composition component as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0127" num="0126"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an example of a defringing layer diagram according to aspects of the present disclosure. In one embodiment, defringing layer <b>900</b> includes grayscale version <b>905</b> and region mask <b>910</b>. Region mask <b>910</b> is an example of, or includes aspects of, the corresponding element described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0128" num="0127"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an example of a defringing layer diagram according to aspects of the present disclosure. According to certain embodiments, a ML algorithm may create a region mask <b>910</b> (which represents the extent of lighting from the background region). In some cases, the boundary between non-lighting area and defringing region in the defringing layer mask corresponds roughly to the boundary between foreground and background, but the transition is even more blurry or extended than in the soft mask.</p><p id="p-0129" num="0128">The region mask <b>910</b> may be combined with a grayscale version <b>905</b> of the positioned reference image to generate a defringing layer <b>900</b>. In some cases, the grayscale version <b>905</b> may be arranged between a layer including the original image and the replacement region layer (e.g., mask that results from combining and editing the hard mask and the soft mask, or a sky layer in the case of sky replacement) in an image editing application.</p><p id="p-0130" num="0129">A machine learning model may also create a lighting layer mask (which represents the extent of lighting from the background region). In some cases, the boundary between a non-lighting area and lighting region in the lighting layer mask corresponds roughly to the boundary between foreground and background, but the transition is even more blurry or extended than in the soft mask.</p><p id="p-0131" num="0130">A lighting mask may be combined with a grayscale version of the positioned reference image to generate a lighting layer. In some examples the lighting layer may be arranged between a layer including the original image and the replacement region layer (e.g., the mask that results from combining and editing the hard mask and the soft mask, or a sky layer in the case of sky replacement) in an image editing application.</p><p id="p-0132" num="0131">A portion of the sky visible based on the relative position of the reference image and the original image may be used to generate background property data (e.g., sky property data). This data may also depend on user edits (e.g., edits to the tone, saturation, brightness, or color composition of the sky region of the reference image). Foreground property may also be determined based on the hard or soft mask and the original image. The foreground property data and the background property data may be used to generate a harmonization layer. The harmonization layer may adjust the color of the foreground so that it looks more natural with the new background (i.e., so that a landscape will look more natural with a different sky).</p><p id="p-0133" num="0132">In some examples, a harmonization layer may be arranged between the lighting layer and a layer including the original image in an image editing application. In other examples, the harmonization layer may be arranged between the replacement region layer and the layer containing the original image. In some cases, a scaled down version of the reference image and the original image (or the corresponding regions or masks) may be used when determining the harmonization layer to improve computational efficiency.</p><p id="p-0134" num="0133"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example of a process for generating a composite image according to aspects of the present disclosure. In some examples, these operations are performed by a system including a processor executing a set of codes to control functional elements of an apparatus. Additionally or alternatively, certain processes are performed using special-purpose hardware. Generally, these operations are performed according to the methods and processes described in accordance with aspects of the present disclosure. In some cases, the operations described herein are composed of various substeps, or are performed in conjunction with other operations.</p><p id="p-0135" num="0134">A method for image editing is described. Embodiments of the method are configured to generate a plurality of region masks for a first image using a mask generation network, where the region masks correspond to a same semantically related region of the first image. Embodiments of the method are further configured to generate a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image, generate a region-specific layer by combining a second region mask of the plurality of region masks with the second image, and generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0136" num="0135">At operation <b>1000</b>, the system generates a set of region masks for a first image using a mask generation network, where the region masks correspond to a same semantically related region of the first image. In some cases, the operations of this step refer to, or may be performed by, a mask generation network as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0137" num="0136">At operation <b>1005</b>, the system generates a defringing layer by combining a first region mask of the set of region masks with grayscale version of a second image. In some cases, the operations of this step refer to, or may be performed by, a defringing component as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0138" num="0137">At operation <b>1010</b>, the system generates a region-specific layer by combining a second region mask of the set of region masks with the second image. In some cases, the operations of this step refer to, or may be performed by, a region-specific layer component as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0139" num="0138">At operation <b>1015</b>, the system generates a composite image by combining the first image, the defringing layer, and the region-specific layer. In some cases, the operations of this step refer to, or may be performed by, a layer composition component as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0140" num="0139">An apparatus for image editing is also described. The apparatus includes a processor, memory in electronic communication with the processor, and instructions stored in the memory. The instructions are operable to cause the processor to generate a plurality of region masks for a first image using a mask generation network, wherein the region masks correspond to a same semantically related region of the first image, generate a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image, generate a region-specific layer by combining a second region mask of the plurality of region masks with the second image, and generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0141" num="0140">A non-transitory computer readable medium storing code for image editing is also described. In some examples, the code comprises instructions executable by a processor to: generate a plurality of region masks for a first image using a mask generation network, wherein the region masks correspond to a same semantically related region of the first image, generate a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image, generate a region-specific layer by combining a second region mask of the plurality of region masks with the second image, and generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0142" num="0141">A system for image editing is also described. Embodiments of the system are configured for generating a plurality of region masks for a first image using a mask generation network, wherein the region masks correspond to a same semantically related region of the first image, generating a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image, generating a region-specific layer by combining a second region mask of the plurality of region masks with the second image, and generating a composite image by combining the first image, the defringing layer, and the region-specific layer.</p><p id="p-0143" num="0142">Some examples of the method, apparatus, non-transitory computer readable medium, and system described above further include combining the second region mask with a third region mask of the plurality of region masks to create a combined region mask, wherein the region-specific layer is generated using the combined region mask.</p><p id="p-0144" num="0143">Some examples of the method, apparatus, non-transitory computer readable medium, and system described above further include generating a color harmonization layer based on at least one of the plurality of region masks, wherein the composite image includes the color harmonization layer. In some examples, the first region mask has a more gradual mask boundary than the second region mask. In some examples, the defringing layer is located between the first image and the region-specific layer. Some examples of the method, apparatus, non-transitory computer readable medium, and system described above further include adjusting a position of the second image relative to the first image. Some examples further include automatically regenerating the composite image based on the adjusted position.</p><p id="p-0145" num="0144">Some examples of the method, apparatus, non-transitory computer readable medium, and system described above further include editing the composite image using an image editing application. Some examples of the method, apparatus, non-transitory computer readable medium, and system described above further include selecting the second image from among a plurality of candidate images for replacing the semantically related region of the first image. In some examples, the semantically related region of the first image comprises a first sky region, and the composite image comprises a second sky region from the second image.</p><p id="p-0146" num="0145">Color Harmonization Layer</p><p id="p-0147" num="0146"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows an example of a process for generating a composite image according to aspects of the present disclosure. In some examples, these operations are performed by a system including a processor executing a set of codes to control functional elements of an apparatus. Additionally or alternatively, certain processes are performed using special-purpose hardware. Generally, these operations are performed according to the methods and processes described in accordance with aspects of the present disclosure. In some cases, the operations described herein are composed of various substeps, or are performed in conjunction with other operations.</p><p id="p-0148" num="0147">At operation <b>1100</b>, the system generates a foreground region mask for a first image using a mask generation network. In some cases, the operations of this step refer to, or may be performed by, a mask generation network as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0149" num="0148">At operation <b>1105</b>, the system computes foreground property data based on the foreground region mask and the first image. In some cases, the operations of this step refer to, or may be performed by, an image property component as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0150" num="0149">At operation <b>1110</b>, the system computes background property data based on a second image. In some cases, the operations of this step refer to, or may be performed by, an image property component as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0151" num="0150">At operation <b>1115</b>, the system generates a color harmonization layer based on the foreground property data and the background property data. In some cases, the operations of this step refer to, or may be performed by, a color harmonization component as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0152" num="0151">At operation <b>1120</b>, the system generates a composite image based on the first image, the second image, and the color harmonization layer. In some cases, the operations of this step refer to, or may be performed by, an image editing application as described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0153" num="0152"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows an example of a process for color harmonization according to aspects of the present disclosure. The example shown includes background property data <b>1200</b>, adjusted background property data <b>1205</b>, clipped background property mask <b>1210</b>, masked background property <b>1215</b>, masked foreground property <b>1220</b>, foreground harmonization color transfer <b>1225</b>, and foreground color harmonization curves <b>1230</b>.</p><p id="p-0154" num="0153">According to a sky replacement example, the portion of the sky visible based on the relative position of the reference image and the original image may be used to generate background property data <b>1200</b>. The background property may be adjusted to produce adjusted background property data <b>1205</b>. In an example scenario, an adjustment to the position of the background may produce adjusted background property data <b>1205</b>.</p><p id="p-0155" num="0154">A clip mask may be applied to the background property data <b>1200</b> to produce a clipped background property mask <b>1210</b>. The adjusted background property data <b>1205</b> and the clipped background property mask <b>1210</b> may be used to create a masked background property <b>1215</b>.</p><p id="p-0156" num="0155">The background property data <b>1200</b> may depend on user edits (e.g., edits to the tone, saturation, brightness, or color composition of the sky region of the reference image). Masked foreground property <b>1220</b> may also be determined based on the hard or soft mask and the original image. The masked foreground property <b>1220</b> and the masked background property <b>1215</b> may be used to generate a harmonization layer comprising foreground harmonization color transfer <b>1225</b> and foreground color harmonization curves <b>1230</b>. The harmonization layer may adjust the color of the foreground so that it looks more natural with the new background (i.e., so that a landscape will look more natural with a different sky).</p><p id="p-0157" num="0156">In some examples, the harmonization layer may be arranged between the defringing layer and a layer including the original image in an image editing application. In other examples the harmonization layer may be arranged between the replacement region layer and the layer containing the original image. In some cases, a scaled down version of the reference image and the original image (or the corresponding regions or masks) may be used when determining the harmonization layer to improve computational efficiency.</p><p id="p-0158" num="0157">Some embodiments of the present disclosure provide real-time harmonization based on the visible sky region. When the sky is moved, the change is detected, and the foreground harmonization is applied accordingly to show natural composition on canvas.</p><p id="p-0159" num="0158">Preset Loading</p><p id="p-0160" num="0159">Loading high resolution preset images (e.g., images used for sky replacement) can cause bottlenecks for loading and saving. For example, loading and saving <b>25</b> presets can take 10-15 seconds. Furthermore, memory usage can be high when presets are loaded. Accordingly, embodiments of the present disclosure include systems and techniques for preset loading.</p><p id="p-0161" num="0160">In one example, sky replacement presets are color images of high resolutions (e.g., 6000&#xd7;4500). As discussed above, this can pose challenges for loading and saving performance, and memory usage efficiency. Traditional preset representation techniques were designed for patterns, gradients, and styles where the representation data for the preset types are smaller sizes (e.g., 946&#xd7;946 for patterns). Thus, <figref idref="DRAWINGS">FIGS. <b>13</b>-<b>16</b></figref> describe efficient techniques for loading preset images (e.g., sky presets).</p><p id="p-0162" num="0161">The representation of a preset comprises multiple files. For example, the metadata and thumbnails for all of the presets can be stored in a single presets info file, while full resolution image data for the presets can represented in separate JPEG or PNG files. At runtime, the single presets info file including metadata and thumbnails is read initially, and preset thumbnails are shown for preview. When a preset is selected, full resolution image data of the preset is loaded (i.e., lazy loading), and when another preset is selected, the full resolution image data of the previous preset is released from memory. When a preset is deleted or created, the corresponding JPEG or PNG file is deleted or created, and the single presets info file (i.e., the file including the metadata and thumbnails) is updated.</p><p id="p-0163" num="0162"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows an example of a process for image editing according to aspects of the present disclosure. In some examples, these operations are performed by a system including a processor executing a set of codes to control functional elements of an apparatus. Additionally or alternatively, certain processes are performed using special-purpose hardware. Generally, these operations are performed according to the methods and processes described in accordance with aspects of the present disclosure. In some cases, the operations described herein are composed of various substeps, or are performed in conjunction with other operations.</p><p id="p-0164" num="0163">At operation <b>1300</b>, the system receives original image data. In some cases, the operations of this step refer to, or may be performed by, a preset component as described with reference to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>.</p><p id="p-0165" num="0164">At operation <b>1305</b>, the system retrieves preset information for set of preset images, where the preset information for each of the preset images includes low-resolution image data and region location information. In some cases, the operations of this step refer to, or may be performed by, a preset component as described with reference to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>.</p><p id="p-0166" num="0165">At operation <b>1310</b>, the system generates at least one low-resolution preview based on the original image data and the preset information. In some cases, the operations of this step refer to, or may be performed by, a thumbnail component as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0167" num="0166">At operation <b>1315</b>, the system selects a preset image from among the set of preset images based on the at least one low-resolution preview image. In some cases, the operations of this step refer to, or may be performed by, a preview component as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0168" num="0167">At operation <b>1320</b>, the system loads high-resolution image data for the selected preset image. In some cases, the operations of this step refer to, or may be performed by, a color conversion component as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0169" num="0168"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows an example of a process for image editing according to aspects of the present disclosure. In some examples, these operations are performed by a system including a processor executing a set of codes to control functional elements of an apparatus. Additionally or alternatively, certain processes are performed using special-purpose hardware. Generally, these operations are performed according to the methods and processes described in accordance with aspects of the present disclosure. In some cases, the operations described herein are composed of various substeps, or are performed in conjunction with other operations.</p><p id="p-0170" num="0169">At operation <b>1400</b>, the system performs a segmentation information on each of a set of preset images to produce region location information. In some cases, the operations of this step refer to, or may be performed by, a segmentation network as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0171" num="0170">At operation <b>1405</b>, the system generates low-resolution image data for each of the preset images. In some cases, the operations of this step refer to, or may be performed by, a thumbnail component as described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0172" num="0171">At operation <b>1410</b>, the system stores preset information including the region location information and the low-resolution image data for each of the set of preset images in a same presets information file. In some cases, the operations of this step refer to, or may be performed by, a database as described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0173" num="0172">At operation <b>1415</b>, the system stores high-resolution image data for each of the preset images in separate image files. In some cases, the operations of this step refer to, or may be performed by, a database as described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0174" num="0173"><figref idref="DRAWINGS">FIG. <b>15</b></figref> shows an example of a preset image diagram according to aspects of the present disclosure. The example shown includes source image <b>1500</b>, color converted image <b>1535</b>, mask <b>1510</b>, preset region <b>1515</b>, thumbnail <b>1520</b>, presets info file <b>1525</b>, compressed image <b>1530</b>, and image files <b>1535</b>.</p><p id="p-0175" num="0174">The process of creating presets begins with the generation of a new unique ID saved as a preset ID in a presets info file. In some examples, the unique ID may include or be based on a universally unique ID (UUID). The source image <b>1500</b> is processed for color conversion using an RGB color space as the default target color profile. The metadata of the color converted image is saved in the presets info file <b>1525</b>.</p><p id="p-0176" num="0175">Additionally, or alternatively, the color converted image is segmented to create a mask. A region is detected in the mask using an algorithm (for example, an image region with 50-percent bound detection). The detected sky region represented by a bounding box in a source image <b>1500</b> and a thumbnail image created from a color converted image are saved in the presets info file <b>1525</b>.</p><p id="p-0177" num="0176">The full resolution data of the color profile converted image is compressed and saved into an image file <b>1535</b>, such as a JPEG or PNG file. In some examples, the source image <b>1500</b> is copied into an image file <b>1535</b> in the presets folder if the format is JPEG or PNG and color profile is the same as the default target color profile. The saved image file <b>1535</b> is named using the preset ID with a suffix (i.e., .jpg or .png). Other image file <b>1535</b> are full resolution image files of presently used presets.</p><p id="p-0178" num="0177">In some examples, the system receives a subsequent user selection. For example, the user may select a different image preset from a user interface. Then, the system releases the full resolution data of the previously selected source image <b>1500</b>, loads the full resolution data of the selected source file as a new replacement image, converts the new replacement image color profile to the target color profile, detects a sky region from the color converted data, and then calculates sky replacement for a preview using the color converted data of the new replacement image and its detected sky region.</p><p id="p-0179" num="0178">Thus, embodiments of the present disclosure pre-compute the image regions and associate the region information with the preset parameters, which may be loaded with other preset info when an image replacement interface is opened. For the user's new custom preset image, the image region may be computed when the new custom preset image is initially imported, and then the region information may be associated with the new preset. Thus, less computation is performed when the preset is used in the future.</p><p id="p-0180" num="0179">In some cases when loading the presets, only the thumbnail data of the presets are loaded for preview. The full resolution data of a preset may not be loaded into the memory until it is selected by the user for image replacement. When a new preset image is selected for image replacement, the full resolution data of the previously selected preset image will be released from the memory. This preset loading approach provides efficiency of memory usage independent of the number of presets being previewed.</p><p id="p-0181" num="0180"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows an example of a preset image representation structure according to aspects of the present disclosure. The example shown includes presets info file <b>1600</b>, preset node <b>1605</b>, and preset identification <b>1610</b>.</p><p id="p-0182" num="0181">The basic structure of a preset image representation comprises presets info file <b>1600</b> and one or more full resolution image files stored in a default location. The presets info file <b>1600</b> contains preset node <b>1605</b> followed by hierarchy info for the presets and groups. The preset node <b>1605</b> is an instance of a preset containing a preset version ID, preset identification <b>1610</b>, metadata, sky region info, and preset thumbnail data. The preset identification <b>1610</b>, created as a unique ID, is unique and may be used to name the full resolution image file (JPEG or PNG) of the associated unique preset. One or more preset node <b>1605</b> share a preset identification <b>1610</b> and the associated full resolution image file. The metadata includes preset source type (for example, default or custom), image mode, image format, thumbnail image resolution, original image resolution, preset name, and color profile. For example, the sky region info is represented by a bounding box, pre-computed using the sky replacement segmentation pipeline when the preset is created.</p><p id="p-0183" num="0182">In one example, the preset image representation structure applies to preset images used for preloading information that includes a detected sky region of an image and loading the full resolution version of the image based on the user selection.</p><p id="p-0184" num="0183">When a user selects a new preset image, the sky regions in the preset image may be detected to find the coordinates for placing the new sky into the composition. In practice, the sky segmentation for a preset image could take significant computations. Therefore achieving the real-time sky replacement preview can be challenging. Furthermore, the user may preview the preset images before selecting. However, since the preset sky images typically have high resolutions, loading a large number of preset images can be slow and can use a significant amount of memory. Additionally or alternatively, saving preset image changes can also be slow if presets are represented in a single file.</p><p id="p-0185" num="0184">To address these issues, the structure of preset image representation may be configured to contain a single presets info file <b>1600</b> and full resolution image files. The presets info file <b>1600</b> contains a list of preset nodes and a hierarchy of information of the preset nodes and groups. Each preset node in the list contains information such as a version ID, a preset ID, metadata (such as image mode, format, size, color profile, etc.), pre-computed sky region information (e.g., a bounding box of the sky region), and thumbnail data.</p><p id="p-0186" num="0185">The full resolution image files contain the compressed image data of the preset images specified in the presets info file <b>1600</b>. The base name of each image file is the associated preset ID of the preset specified in the presets info file <b>1600</b>, and the suffix of each image file is the associated image format (e.g., .jpg or .png) of the preset specified in the presets info file <b>1600</b>.</p><p id="p-0187" num="0186">Preset images may be loaded by reading the presets info file <b>1600</b> and rendering the preset thumbnails to preview from the preset node <b>1605</b>. Preset identification <b>1610</b> is then selected. For example, a preset thumbnail may be selected by the user (e.g., via the UI), and then the system automatically identifies which preset node is associated with the selected thumbnail.</p><p id="p-0188" num="0187">The full resolution image file name is determined using the preset ID and image format of a currently selected preset image. The full resolution image file is read, and the full resolution image data is converted to the target color profile. The preset image data is established for replacement with the pre-computed sky region info. Sky replacement is then calculated. The full resolution image data of the current preset may be released from the memory when a different select is selected.</p><p id="p-0189" num="0188">The source image file is read and the source image in converted to the default preset color profile to create a new custom preset. The sky region is detected in the source image, and a new unique preset ID is generated. The source image is then converted to the default preset image format (e.g., JPEG or PNG). The converted image file is saved to the presets folder, and is renamed with the new preset ID. A new preset node is created, and is added to the current preset node list.</p><p id="p-0190" num="0189">Presets may then be saved by saving the updated preset nodes and hierarchy info to the presets info file <b>1600</b>. Full resolution images may not be saved at this time since the full resolution images have already been temporarily saved when creating new presets. Thus, the saving process becomes significantly fast by only updating the presets info file <b>1600</b>.</p><p id="p-0191" num="0190">The preset image representation provides increased sky replacement speed by using precomputed sky region info from the selected preset and increased presets loading speed for previewing. Additionally or alternatively, the preset image representation provides increased presets saving speed when there is any update of the presets to persist. Memory is used much more efficiently by loading the full resolution image data of the preset upon selection and releasing the full resolution image data upon deselection.</p><p id="p-0192" num="0191">The workflow of loading presets comprises loading thumbnails and metadata of presets from the presets info file <b>1600</b>. For example, loading presets comprises loading thumbnails, region info, and metadata from the presets info file <b>1600</b>. The thumbnails are rendered in a preset view window. The hierarchy of presets and groups is read from the presets info file and shown in the preset view window. For example, when a preset is selected, the full resolution image of the selected preset is loaded from an associated file (i.e., JPEG/PNG), and the image of the last selected preset is released. The full resolution image data of the last selected preset (if any) is released from memory when a preset is selected for image replacement. The image file name is determined using the preset identification <b>1610</b> and image format of the presently selected preset in the preset node <b>1605</b>. Image data of the selected preset is loaded, and the image is converted to a target color profile determined by the color profile (e.g., of the original image that contains the old sky). In some examples, the preset data is set for image replacement with pre-computed region info and other metadata and the image replacement is calculated for preview.</p><p id="p-0193" num="0192">The description and drawings described herein represent example configurations and do not represent all the implementations within the scope of the claims. For example, the operations and steps may be rearranged, combined or otherwise modified. Also, structures and devices may be represented in the form of block diagrams to represent the relationship between components and avoid obscuring the described concepts. Similar components or features may have the same name but may have different reference numbers corresponding to different figures.</p><p id="p-0194" num="0193">Some modifications to the disclosure may be readily apparent to those skilled in the art, and the principles defined herein may be applied to other variations without departing from the scope of the disclosure. Thus, the disclosure is not limited to the examples and designs described herein, but is to be accorded the broadest scope consistent with the principles and novel features disclosed herein.</p><p id="p-0195" num="0194">In this disclosure and the following claims, the word &#x201c;or&#x201d; indicates an inclusive list such that, for example, the list of X, Y, or Z means X or Y or Z or XY or XZ or YZ or XYZ. Also the phrase &#x201c;based on&#x201d; is not used to represent a closed set of conditions. For example, a step that is described as &#x201c;based on condition A&#x201d; may be based on both condition A and condition B. In other words, the phrase &#x201c;based on&#x201d; shall be construed to mean &#x201c;based at least in part on.&#x201d; Also, the words &#x201c;a&#x201d; or &#x201c;an&#x201d; indicate &#x201c;at least one.&#x201d;</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>generating a plurality of region masks for a first image using a mask generation network, wherein the region masks correspond to a same semantically related region of the first image;</claim-text><claim-text>generating a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image;</claim-text><claim-text>generating a region-specific layer by combining a second region mask of the plurality of region masks with the second image; and</claim-text><claim-text>generating a composite image by combining the first image, the defringing layer, and the region-specific layer.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>combining the second region mask with a third region mask of the plurality of region masks to create a combined region mask, wherein the region-specific layer is generated using the combined region mask.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>generating a color harmonization layer based on at least one of the plurality of region masks, wherein the composite image includes the color harmonization layer.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the first region mask has a more gradual mask boundary than the second region mask.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the defringing layer is located between the first image and the region-specific layer.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>adjusting a position of the second image relative to the first image; and</claim-text><claim-text>automatically regenerating the composite image based on the adjusted position.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>editing the composite image using an image editing application.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>selecting the second image from among a plurality of candidate images for replacing the semantically related region of the first image.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the semantically related region of the first image comprises a first sky region, and the composite image comprises a second sky region from the second image.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A method comprising:<claim-text>generating a first region mask, a second region mask, and a third region mask corresponding to a same semantically related region of a first image;</claim-text><claim-text>generating a defringing layer by combining the first region mask with grayscale version of a second image;</claim-text><claim-text>generating a region-specific layer by combining the second region mask and the third region mask to produce a combined region mask, and combining the combined region mask with the second image; and</claim-text><claim-text>generating a composite image by combining the first image, the defringing layer, and the region-specific layer.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:<claim-text>applying a mask brush to adjust the combined region mask; and</claim-text><claim-text>applying a fade edge adjustment, a shift edge adjustment, or both the fade edge adjustment and the shift edge adjustment after applying the mask brush.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>applying the mask brush to further adjust the combined region mask; and</claim-text><claim-text>automatically reapplying the fade edge adjustment, the shift edge adjustment, or both the fade edge adjustment and the shift edge adjustment after reapplying the mask brush.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:<claim-text>generating a color harmonization layer based on the second region mask, wherein the composite image includes the color harmonization layer.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:<claim-text>adjusting a position of the second image relative to the first image;</claim-text><claim-text>automatically regenerating the defringing layer and the region-specific layer based on the position; and</claim-text><claim-text>automatically regenerating the composite image based on the regenerated defringing layer and the regenerated region-specific layer.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein:<claim-text>the first region mask has a more gradual mask boundary than the second region mask, and the second region mask has a more gradual boundary than the third mask.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein:<claim-text>the semantically related region of the first image comprises a first sky region, and the composite image comprises a second sky region from the second image.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. An apparatus comprising:<claim-text>a mask generation network configured to generate a plurality of region masks for a first image using a mask generation network, wherein the region masks correspond to a same semantically related region of the first image;</claim-text><claim-text>a defringing component configured to generate a defringing layer by combining a first region mask of the plurality of region masks with grayscale version of a second image;</claim-text><claim-text>a region-specific layer component configured to generate a region-specific layer by combining a second region mask of the plurality of region masks with the second image; and</claim-text><claim-text>a layer composition component configured to generate a composite image by combining the first image, the defringing layer, and the region-specific layer.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the mask generation network comprises a convolutional neural network (CNN).</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the region-specific layer component comprises a brush tool, a fade edge slider, and a shift edge slider.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising:<claim-text>a color harmonization component configured to generate a color harmonization layer based on at least one of the plurality of region masks.</claim-text></claim-text></claim></claims></us-patent-application>