<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007237A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007237</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782109</doc-number><date>20191205</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>105</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>159</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>105</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>159</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">FILTER GENERATION METHOD, FILTER GENERATION APPARATUS AND PROGRAM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NIPPON TELEGRAPH AND TELEPHONE CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MIYAZAWA</last-name><first-name>Takehito</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>BANDO</last-name><first-name>Yukihiro</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>KUROZUMI</last-name><first-name>Takayuki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>KIMATA</last-name><first-name>Hideaki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>NIPPON TELEGRAPH AND TELEPHONE CORPORATION</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2019/047655</doc-number><date>20191205</date></document-id><us-371c12-date><date>20220602</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A filter generation method according to one embodiment is a filter generation method for generating a filter for an inter prediction image in moving image coding or video coding, wherein a computer executes a first acquisition procedure for acquiring, for each of subblocks included in a coding object block, a region in a reference image that corresponds to the subblock, a second acquisition procedure for referring to block segmentation information of the reference image and acquiring a coding block that is a block of the reference image which includes the region, and a generation procedure for generating, for the coding object block or each of a plurality of coding object blocks, an image obtained by performing an inverse transformation on one or more coding blocks each acquired in the second acquisition procedure as the filter.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="72.73mm" wi="158.75mm" file="US20230007237A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="184.15mm" wi="94.83mm" orientation="landscape" file="US20230007237A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="178.48mm" wi="135.04mm" orientation="landscape" file="US20230007237A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="77.05mm" wi="99.31mm" file="US20230007237A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="181.53mm" wi="148.25mm" orientation="landscape" file="US20230007237A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="175.43mm" wi="141.73mm" orientation="landscape" file="US20230007237A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="203.37mm" wi="145.71mm" file="US20230007237A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="173.31mm" wi="141.31mm" orientation="landscape" file="US20230007237A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="149.86mm" wi="134.79mm" orientation="landscape" file="US20230007237A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="227.33mm" wi="155.19mm" file="US20230007237A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="92.37mm" wi="123.27mm" file="US20230007237A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to a filter generation method, a filter generation device, and a program.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">As one of moving image coding techniques or video coding techniques, inter coding is known. Inter coding approximates a coding object image by rectangles through block segmentation, searches for a motion parameter between the coding object image and a reference image on a block-by-block basis, and generates a prediction image (e.g., Non-Patent Literature 1). Here, as for the motion parameter, translation represented by two parameters, a movement distance in a longitudinal direction and a movement distance in a lateral direction, has been used.</p><p id="p-0004" num="0003">It is known that, if there is a distortion of a subject (an object) which cannot be fully represented by translation, additional utilization of a higher-order motion, such as affine transformation or projective transformation, increases prediction accuracy and improves coding efficiency. For example, Non-Patent Literature 2 makes a prediction using affine transformation on a distortion of a subject associated with movement of a camera. For example, Non-Patent Literature 3 applies affine transformation, projective transformation, and bilinear transformation on inter-view prediction in a multi-view image.</p><p id="p-0005" num="0004">If a pixel located at coordinates (x, y) is subjected to affine transformation, coordinates (x&#x2032;, y&#x2032;) after the pixel transformation are given by the following expression (1):</p><p id="p-0006" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Math</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>1</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mo>{</mo>     <mtable>      <mtr>       <mtd>        <mrow>         <msup>          <mi>x</mi>          <mo>&#x2032;</mo>         </msup>         <mo>=</mo>         <mrow>          <mi>ax</mi>          <mo>+</mo>          <mrow>           <mi>b</mi>           <mo>&#x2062;</mo>           <mi>y</mi>          </mrow>          <mo>+</mo>          <mi>c</mi>         </mrow>        </mrow>       </mtd>      </mtr>      <mtr>       <mtd>        <mrow>         <msup>          <mi>y</mi>          <mo>&#x2032;</mo>         </msup>         <mo>=</mo>         <mrow>          <mi>dx</mi>          <mo>+</mo>          <mrow>           <mi>e</mi>           <mo>&#x2062;</mo>           <mi>y</mi>          </mrow>          <mo>+</mo>          <mi>f</mi>         </mrow>        </mrow>       </mtd>      </mtr>     </mtable>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0007" num="0005">where a, b, c, d, and e are affine parameters.</p><p id="p-0008" num="0006">As a next-generation standard under review by JVET (Joint Video Experts Team), VVC (Versatile Video Coding) is known (Non-Patent Literature 4). VVC adopts 4/6-parameter affine prediction mode. In 4/6-parameter affine prediction mode, a coding block is segmented into 4&#xd7;4 subblocks, and per-pixel affine transformation is approximated by per-subblock translation. At this time, in 4-parameter affine prediction mode, a motion vector of each subblock is calculated using four parameters (mv<sub>0x</sub>, mv<sub>0y</sub>, mv<sub>1x</sub>, and mv<sub>1y</sub>) composed of two vectors, motion vectors v<sub>0 </sub>(=(mv<sub>0x</sub>, mv<sub>0y</sub>)) and v<sub>1 </sub>(=(mv<sub>1x</sub>, mv<sub>1y</sub>)) of control points located in the upper left and the upper right of the subblock, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, by the following expression (2):</p><p id="p-0009" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Math</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>2</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mo>{</mo>     <mtable>      <mtr>       <mtd>        <mrow>         <msub>          <mi>mv</mi>          <mi>x</mi>         </msub>         <mo>=</mo>         <mrow>          <mrow>           <mfrac>            <mrow>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>1</mn>                <mo>&#x2062;</mo>                <mi>x</mi>               </mrow>              </msub>             </mrow>             <mo>-</mo>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>0</mn>                <mo>&#x2062;</mo>                <mi>x</mi>               </mrow>              </msub>             </mrow>            </mrow>            <mi>W</mi>           </mfrac>           <mo>&#x2062;</mo>           <mi>x</mi>          </mrow>          <mo>+</mo>          <mrow>           <mfrac>            <mrow>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>1</mn>                <mo>&#x2062;</mo>                <mi>y</mi>               </mrow>              </msub>             </mrow>             <mo>-</mo>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>0</mn>                <mo>&#x2062;</mo>                <mi>y</mi>               </mrow>              </msub>             </mrow>            </mrow>            <mi>W</mi>           </mfrac>           <mo>&#x2062;</mo>           <mi>y</mi>          </mrow>          <mo>+</mo>          <mrow>           <mi>m</mi>           <mo>&#x2062;</mo>           <msub>            <mi>v</mi>            <mrow>             <mn>0</mn>             <mo>&#x2062;</mo>             <mi>x</mi>            </mrow>           </msub>          </mrow>         </mrow>        </mrow>       </mtd>      </mtr>      <mtr>       <mtd>        <mrow>         <msub>          <mi>mv</mi>          <mi>y</mi>         </msub>         <mo>=</mo>         <mrow>          <mrow>           <mfrac>            <mrow>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>1</mn>                <mo>&#x2062;</mo>                <mi>y</mi>               </mrow>              </msub>             </mrow>             <mo>-</mo>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>&#x3bd;</mi>               <mrow>                <mn>0</mn>                <mo>&#x2062;</mo>                <mi>y</mi>               </mrow>              </msub>             </mrow>            </mrow>            <mi>W</mi>           </mfrac>           <mo>&#x2062;</mo>           <mi>x</mi>          </mrow>          <mo>+</mo>          <mrow>           <mfrac>            <mrow>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>1</mn>                <mo>&#x2062;</mo>                <mi>x</mi>               </mrow>              </msub>             </mrow>             <mo>-</mo>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>0</mn>                <mo>&#x2062;</mo>                <mi>x</mi>               </mrow>              </msub>             </mrow>            </mrow>            <mi>W</mi>           </mfrac>           <mo>&#x2062;</mo>           <mi>y</mi>          </mrow>          <mo>+</mo>          <mrow>           <mi>m</mi>           <mo>&#x2062;</mo>           <msub>            <mi>v</mi>            <mrow>             <mn>0</mn>             <mo>&#x2062;</mo>             <mi>y</mi>            </mrow>           </msub>          </mrow>         </mrow>        </mrow>       </mtd>      </mtr>     </mtable>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0010" num="0007">where W is a lateral pixel size of the coding block, and H is a longitudinal pixel size of the coding block.</p><p id="p-0011" num="0008">In contrast, in 6-parameter affine prediction mode, the motion vector is calculated using six parameters (mv<sub>0x</sub>, mv<sub>0y</sub>, mv<sub>1x</sub>, mv<sub>1y</sub>, mv<sub>2x</sub>, and mv<sub>2y</sub>) composed of three vectors obtained by adding a motion vector v<sub>2 </sub>(=(mv<sub>2x</sub>, mv<sub>2y</sub>)) of a control point located in the lower left of the subblock, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, by the following expression (3):</p><p id="p-0012" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Math</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>3</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mo>{</mo>     <mtable>      <mtr>       <mtd>        <mrow>         <msub>          <mi>mv</mi>          <mi>x</mi>         </msub>         <mo>=</mo>         <mrow>          <mrow>           <mfrac>            <mrow>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>1</mn>                <mo>&#x2062;</mo>                <mi>x</mi>               </mrow>              </msub>             </mrow>             <mo>-</mo>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>0</mn>                <mo>&#x2062;</mo>                <mi>x</mi>               </mrow>              </msub>             </mrow>            </mrow>            <mi>W</mi>           </mfrac>           <mo>&#x2062;</mo>           <mi>x</mi>          </mrow>          <mo>+</mo>          <mrow>           <mfrac>            <mrow>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>2</mn>                <mo>&#x2062;</mo>                <mi>x</mi>               </mrow>              </msub>             </mrow>             <mo>-</mo>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>0</mn>                <mo>&#x2062;</mo>                <mi>x</mi>               </mrow>              </msub>             </mrow>            </mrow>            <mi>H</mi>           </mfrac>           <mo>&#x2062;</mo>           <mi>y</mi>          </mrow>          <mo>+</mo>          <mrow>           <mi>m</mi>           <mo>&#x2062;</mo>           <msub>            <mi>v</mi>            <mrow>             <mn>0</mn>             <mo>&#x2062;</mo>             <mi>x</mi>            </mrow>           </msub>          </mrow>         </mrow>        </mrow>       </mtd>      </mtr>      <mtr>       <mtd>        <mrow>         <msub>          <mi>mv</mi>          <mi>y</mi>         </msub>         <mo>=</mo>         <mrow>          <mrow>           <mfrac>            <mrow>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>1</mn>                <mo>&#x2062;</mo>                <mi>y</mi>               </mrow>              </msub>             </mrow>             <mo>-</mo>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>&#x3bd;</mi>               <mrow>                <mn>0</mn>                <mo>&#x2062;</mo>                <mi>y</mi>               </mrow>              </msub>             </mrow>            </mrow>            <mi>W</mi>           </mfrac>           <mo>&#x2062;</mo>           <mi>x</mi>          </mrow>          <mo>+</mo>          <mrow>           <mfrac>            <mrow>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>2</mn>                <mo>&#x2062;</mo>                <mi>y</mi>               </mrow>              </msub>             </mrow>             <mo>-</mo>             <mrow>              <mi>m</mi>              <mo>&#x2062;</mo>              <msub>               <mi>v</mi>               <mrow>                <mn>0</mn>                <mo>&#x2062;</mo>                <mi>y</mi>               </mrow>              </msub>             </mrow>            </mrow>            <mi>H</mi>           </mfrac>           <mo>&#x2062;</mo>           <mi>y</mi>          </mrow>          <mo>+</mo>          <mrow>           <mi>m</mi>           <mo>&#x2062;</mo>           <msub>            <mi>v</mi>            <mrow>             <mn>0</mn>             <mo>&#x2062;</mo>             <mi>y</mi>            </mrow>           </msub>          </mrow>         </mrow>        </mrow>       </mtd>      </mtr>     </mtable>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0013" num="0009">As described above, VVC reduces the amount of computation by approximating affine transformation by a combination of translations.</p><p id="p-0014" num="0010">Note that merge mode is adopted in VVC, as in H.265/HEVC. Merge mode is also applied to a coding block to which affine prediction mode is applied. In merge mode, a merge index indicating a position of an adjacent coded block is transmitted instead of transmitting a motion parameter of a coding object block, and decoding is performed using a motion vector of the coded block at the position indicated by the index.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Non-Patent Literature</heading><p id="p-0015" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0011">Non-Patent Literature 1: Recommendation ITU-T H.265: High efficiency video coding, 2013</li>    <li id="ul0001-0002" num="0012">Non-Patent Literature 2: H. Jozawa et al. &#x201c;Two-stage motion compensation using adaptive global MC and local affine MC.&#x201d; IEEE Trans. on CSVT 7.1 (1997): 75-85.</li>    <li id="ul0001-0003" num="0013">Non-Patent Literature 3: R-J-S. Monteiro et al. &#x201c;Light field image coding using high-order intrablock prediction.&#x201d; IEEE Journal of Selected Topics in Signal Processing 11.7 (2017): 1120-1131.</li>    <li id="ul0001-0004" num="0014">Non-Patent Literature 4: JVET-M1002-v1_encoder description VTM4</li></ul></p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0016" num="0015">However, affine transformation, projective transformation, or the like needs more parameters than those in the case of translation. The amount of computation needed for estimation and coding overhead increase, which leads to inefficiency.</p><p id="p-0017" num="0016">Although VVC can reduce the amount of computation, per-subblock translation cannot completely capture deformation of an object. This may cause protrusion of a reference range, a failure to pick up a pixel, or the like, leading to an increase in prediction error. For example, if an object in a reference image undergoes shear deformation, rotational deformation, scaling deformation, or the like, as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, protrusion of a reference range or a failure to pick up a pixel occurs. Especially if, an object in a coding object image has deformed from a rectangle, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, errors accumulate at both the coding object image and a reference image to further increase a prediction error. That is, a scheme which makes a prediction by per-subblock translation cannot fully represent affine transformation, especially if an object in a coding object image is hard to approximate by rectangles.</p><p id="p-0018" num="0017">An object of an embodiment of the present invention, which has been made in view of the above-described points, is to reduce a prediction error while curbing the amount of computation.</p><heading id="h-0007" level="1">Means for Solving the Problem</heading><p id="p-0019" num="0018">In order to attain the above-described object, a filter generation method according to the one embodiment of the present invention is a filter generation method for generating a filter for an inter prediction image in moving image coding or video coding, wherein a computer executes a first acquisition procedure for acquiring, for each of subblocks included in a coding object block, a region in a reference image that corresponds to the subblock, a second acquisition procedure for referring to block segmentation information of the reference image and acquiring a coding block that is a block of the reference image which includes the region, and a generation procedure for generating, for the coding object block or each of a plurality of coding object blocks, an image obtained by performing an inverse transformation on one or more coding blocks each acquired in the second acquisition procedure as the filter.</p><heading id="h-0008" level="1">Effects of the Invention</heading><p id="p-0020" num="0019">It is possible to reduce a prediction error while curbing the amount of computation.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a view showing motion vectors of control points in subblocks.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view (Part I) showing an example of object deformation.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a view (Part II) showing an example of object deformation.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram showing an example of an overall configuration of a coding apparatus according to a first embodiment.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing an example of a functional configuration of a filter generation unit according to the first embodiment.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart showing an example of a filter generation process according to the first embodiment.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram showing an example of an overall configuration of a coding apparatus according to a second embodiment.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram showing an example of a functional configuration of a filter generation unit according to the second embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart showing an example of a filter generation process according to the second embodiment.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram showing an example of a hardware configuration of a coding apparatus according to one embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0031" num="0030">Embodiments of the present invention will be described below. Each embodiment of the present invention will describe a case of creating a prediction image, in which a prediction error due to various types of transformations (e.g., affine transformation, projective transformation, and bilinear transformation) at the time of moving image coding or video coding is reduced, while curbing the amount of computation of the transformations and utilizing the prediction image as a filter. Note that, hereinafter, a prediction error will also be referred to as a &#x201c;prediction residual error.&#x201d;</p><p id="p-0032" num="0031">A first embodiment to be described below will describe a case where a filter in question is applied as an in-loop filter. A second embodiment will describe a case where a filter in question is applied as a post-filter, and a combination with merge mode is made. Note that the embodiments below will be described with affine transformation as an example in mind.</p><heading id="h-0011" level="1">First Embodiment</heading><p id="p-0033" num="0032">Hereinafter, the first embodiment will be described.</p><p id="p-0034" num="0033">(Overall Configuration)</p><p id="p-0035" num="0034">First, an overall configuration of a coding apparatus <b>10</b> according to the first embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram showing an example of the overall configuration of the coding apparatus <b>10</b> according to the first embodiment.</p><p id="p-0036" num="0035">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the coding apparatus <b>10</b> according to the first embodiment has an intra prediction unit <b>101</b>, an inter prediction unit <b>102</b>, a filter generation unit <b>103</b>, a filter unit <b>104</b>, a mode determination unit <b>105</b>, a DCT unit <b>106</b>, a quantization unit <b>107</b>, an inverse quantization unit <b>108</b>, an Inv-DCT unit <b>109</b>, a reference image memory <b>110</b>, and a reference image block segmentation shape memory <b>111</b>.</p><p id="p-0037" num="0036">The intra prediction unit <b>101</b> generates a prediction image (an intra prediction image) of a coding object block by known intra prediction. The inter prediction unit <b>102</b> generates a prediction image (an inter prediction image) of the coding object block by known inter prediction. The filter generation unit <b>103</b> generates a filter for modifying (filtering) the inter prediction image. The filter unit <b>104</b> filters the inter prediction image using the filter generated by the filter generation unit <b>103</b>. Note that the filter unit <b>104</b> may calculate, for example, a per-pixel weighted mean of the inter prediction image and the filter as filtering.</p><p id="p-0038" num="0037">The mode determination unit <b>105</b> determines which one of intra prediction mode and inter prediction mode is selected. The DCT unit <b>106</b> performs a discrete cosine transform (DCT) on a prediction residual error between the coding object block and the inter prediction image or the intra prediction image by a known method, in accordance with a result of the determination by the mode determination unit <b>105</b>. The quantization unit <b>107</b> quantizes the prediction residual error after the discrete cosine transform by a known method. For this reason, the prediction residual error after the discrete cosine transform and the quantization and a prediction parameter used for the intra prediction or the inter prediction are outputted. The prediction residual error and the prediction parameter are a result of coding the coding object block.</p><p id="p-0039" num="0038">The inverse quantization unit <b>108</b> inversely quantizes the prediction residual error outputted from the quantization unit <b>107</b> by a known method. The Inv-DCT unit <b>109</b> performs an inverse discrete cosine transform (Inverse DCT) on the prediction residual error after the inverse quantization by a known method. A decoding image obtained through decoding using the prediction residual error after the inverse discrete cosine transform and the intra prediction image or the inter prediction image (after the filter by the filter unit <b>104</b>) is stored in the reference image memory <b>110</b>. A block segmentation shape (e.g., quadtree block segmentation information) when a reference image is coded is stored in the reference image block segmentation shape memory <b>111</b>.</p><p id="p-0040" num="0039">(Functional Configuration of Filter Generation Unit <b>103</b>)</p><p id="p-0041" num="0040">A detailed functional configuration of the filter generation unit <b>103</b> according to the first embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing an example of the functional configuration of the filter generation unit <b>103</b> according to the first embodiment.</p><p id="p-0042" num="0041">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the filter generation unit <b>103</b> according to the first embodiment includes an affine transformation parameter acquisition unit <b>201</b>, a block segmentation acquisition unit <b>202</b>, an in-reference-image object determination unit <b>203</b>, an inverse affine transformation parameter computation unit <b>204</b>, an affine transformation unit <b>205</b>, a prediction image generation unit <b>206</b>, and a filter region limitation unit <b>207</b>. Here, reference image block segmentation information, coding object image information, and reference image information are inputted to the filter generation unit <b>103</b>. The reference image block segmentation information is information representing block segmentation of a reference image. The coding object image information is information including pixel information of a coding object block, inter prediction mode information (including merge mode information and an affine parameter), and an index indicating the reference image. The reference image information is pixel information of the reference image.</p><p id="p-0043" num="0042">The affine transformation parameter acquisition unit <b>201</b> acquires the affine parameter used for affine transformation. The block segmentation acquisition unit <b>202</b> acquires a reference region (a corresponding rectangular region in the reference image) corresponding to a given subblock of the coding object block, refers to the reference image block segmentation information, and acquires a coding block which fully includes the reference region. Note that the acquisition of the coding block fully including the reference region excludes a portion protruding (even if only partially) from an object region of a coding object. It is thus possible to acquire a more accurate region than that by conventional rectangle approximation.</p><p id="p-0044" num="0043">The in-reference-image object determination unit <b>203</b> adds a coding block to a block set indicating a region of an object in the reference image if the coding block is acquired by the block segmentation acquisition unit <b>202</b>. The inverse affine transformation parameter computation unit <b>204</b> calculates an inverse affine parameter used for inverse affine transformation. The affine transformation unit <b>205</b> uses the inverse affine parameter to perform an inverse affine transformation on the block set created by the in-reference-image object determination unit <b>203</b>. The prediction image generation unit <b>206</b> generates a new prediction image from a result of the inverse affine transformation by the affine transformation unit <b>205</b>. The filter region limitation unit <b>207</b> sets an image limited to a region corresponding to the coding object block of a region of the prediction image generated by the prediction image generation unit <b>206</b> as a filter (i.e., a filter through which the region corresponding to the coding object block of the prediction image is passed).</p><p id="p-0045" num="0044">(Filter Generation Process)</p><p id="p-0046" num="0045">A filter generation process to be executed by the filter generation unit <b>103</b> according to the first embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart showing an example of the filter generation process according to the first embodiment. Note that, hereinafter, a case of generating, at the time of coding respective blocks (coding object blocks) of a given frame image, respective filters for inter prediction images of the coding object blocks will be described.</p><p id="p-0047" num="0046">First, the filter generation unit <b>103</b> acquires a coding object block B, for which a prediction image update process (i.e., steps S<b>102</b> to S<b>110</b> (to be described later)) has not been performed, (step S<b>101</b>). The filter generation unit <b>103</b> then determines, for the coding object block B, whether affine prediction mode is selected, (step S<b>102</b>).</p><p id="p-0048" num="0047">If it is not determined in step S<b>102</b> above that affine prediction mode is selected, the filter generation unit <b>103</b> does not perform processing on the coding object block B and advances to step S<b>110</b>. On the other hand, if it is determined in step S<b>102</b> above that affine prediction mode is selected, the affine transformation parameter acquisition unit <b>201</b> of the filter generation unit <b>103</b> acquires an affine parameter, (step S<b>103</b>).</p><p id="p-0049" num="0048">Subsequently to step S<b>103</b>, the filter generation unit <b>103</b> acquires, of subblocks S's included in the coding block B, a subblock S, for which a process (i.e., steps S<b>105</b> to S<b>106</b> (to be described later)) for identifying a reference region has not been performed, (step S<b>104</b>). The block segmentation acquisition unit <b>202</b> of the filter generation unit <b>103</b> then calculates a motion vector of the subblock S in accordance with known affine prediction mode processing (i.e., performs motion compensation) to acquire a reference region S<sub>p </sub>corresponding to the subblock S, (step S<b>105</b>). The block segmentation acquisition unit <b>202</b> of the filter generation unit <b>103</b> then refers to reference image block segmentation information (an example of a coding parameter) to determine whether any coding block B&#x2032; that fully includes the reference region S<sub>p </sub>is present, (step S<b>106</b>).</p><p id="p-0050" num="0049">If it is not determined in step S<b>106</b> above that any coding block B&#x2032; fully including the reference region S<sub>p </sub>is present, the filter generation unit <b>103</b> regards the subblock S as processed and returns to step S<b>104</b>. On the other hand, if it is determined that any coding block B&#x2032; fully including the reference region S<sub>p </sub>is present, the filter generation unit <b>103</b> acquires the coding block B&#x2032; by means of the block segmentation acquisition unit <b>202</b> and adds the coding block B&#x2032; to a block set R which indicates a region of an object in a reference image by means of the in-reference-image object determination unit <b>203</b>, (step S<b>107</b>). In this case, the filter generation unit <b>103</b> regards the subblock S as processed.</p><p id="p-0051" num="0050">Subsequently, the filter generation unit <b>103</b> determines whether processing is finished for all the subblocks included in the coding block B (i.e., whether the process for identifying a reference region has been performed for all the subblocks), (step S<b>108</b>).</p><p id="p-0052" num="0051">If it is not determined in step S<b>108</b> above that processing is finished for all the subblocks included in the coding block B, the filter generation unit <b>103</b> returns to step S<b>104</b>. For this reason, steps S<b>104</b> to S<b>108</b> (or steps S<b>104</b> to S<b>106</b> if NO in step S<b>106</b>) are repeatedly executed for all the subblocks included in the coding block B.</p><p id="p-0053" num="0052">On the other hand, if it is determined in step S<b>108</b> above that processing is finished for all the subblocks included in the coding block B, the filter generation unit <b>103</b> computes an inverse affine parameter by means of the inverse affine transformation parameter computation unit <b>204</b>, uses the inverse affine parameter to perform an inverse affine transformation on the block set R (i.e., perform an inverse transformation of an affine transformation on the coding object block B) by means of the affine transformation unit <b>205</b>, and sets the block set R after the inverse affine transformation as a new prediction image by means of the prediction image generation unit <b>206</b>, (step S<b>109</b>). With limitation to a region corresponding to the coding object block B of a region of the prediction image (i.e., with limitation of an application region of the prediction image) by the filter region limitation unit <b>207</b>, a filter for the coding object block B is obtained. The limitation of a region to be used as a filter of the prediction image is caused to prevent a coded pixel from being changed and becoming unable to undergo decoding processing if a region after the inverse affine transformation of the block set R includes a position of the coded pixel other than the coding object block B.</p><p id="p-0054" num="0053">Subsequently, the filter generation unit <b>103</b> regards the coding object block B acquired in step S<b>101</b> above as processed, (step S<b>110</b>), and determines whether all the coding object blocks in the frame image are processed (i.e., whether the prediction image update process has been performed for all the coding object blocks), (step S<b>111</b>).</p><p id="p-0055" num="0054">If it is not determined in step S<b>111</b> above that all the coding object blocks are processed, the filter generation unit <b>103</b> returns to step S<b>101</b>. For this reason, steps S<b>101</b> to S<b>111</b> (or steps S<b>101</b> to S<b>102</b> and steps S<b>110</b> to S<b>111</b> if NO in step S<b>102</b>) are repeatedly executed for all the coding blocks included in the frame image.</p><p id="p-0056" num="0055">On the other hand, if it is determined in step S<b>111</b> above that all the coding object blocks are processed, the filter generation unit <b>103</b> ends the filter generation process. In the above-described manner, a filter for each coding object block included in one frame image is generated.</p><heading id="h-0012" level="1">Second Embodiment</heading><p id="p-0057" num="0056">Hereinafter, a second embodiment will be described. Note that the second embodiment will mainly describe differences from the first embodiment and that a description of the same components as the first embodiment will be appropriately omitted.</p><p id="p-0058" num="0057">(Overall Configuration)</p><p id="p-0059" num="0058">First, an overall configuration of a coding apparatus <b>10</b> according to the second embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram showing an example of the overall configuration of the coding apparatus <b>10</b> according to the second embodiment.</p><p id="p-0060" num="0059">As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the coding apparatus <b>10</b> according to the second embodiment has an intra prediction unit <b>101</b>, an inter prediction unit <b>102</b>, a filter generation unit <b>103</b>, a filter unit <b>104</b>, a mode determination unit <b>105</b>, a DCT unit <b>106</b>, a quantization unit <b>107</b>, an inverse quantization unit <b>108</b>, an Inv-DCT unit <b>109</b>, a reference image memory <b>110</b>, and a reference image block segmentation shape memory <b>111</b>.</p><p id="p-0061" num="0060">The second embodiment is different in a position of the filter unit <b>104</b>. In the second embodiment, the filter unit <b>104</b> filters a decoding image (i.e., a decoding image obtained through decoding using an inter prediction image and a prediction residual error after inverse discrete cosine transform by the Inv-DCT unit <b>109</b>).</p><p id="p-0062" num="0061">(Functional Configuration of Filter Generation Unit <b>103</b>)</p><p id="p-0063" num="0062">A detailed functional configuration of the filter generation unit <b>103</b> according to the second embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram showing an example of the functional configuration of the filter generation unit <b>103</b> according to the second embodiment.</p><p id="p-0064" num="0063">As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the filter generation unit <b>103</b> according to the second embodiment includes an affine transformation parameter acquisition unit <b>201</b>, a block segmentation acquisition unit <b>202</b>, an in-reference-image object determination unit <b>203</b>, an inverse affine transformation parameter computation unit <b>204</b>, an affine transformation unit <b>205</b>, a prediction image generation unit <b>206</b>, and a merge mode information acquisition unit <b>208</b>. The second embodiment assumes that coding object image information includes merge mode information. The merge mode information acquisition unit <b>208</b> acquires merge mode information from coding object image information.</p><p id="p-0065" num="0064">(Filter Generation Process)</p><p id="p-0066" num="0065">A filter generation process to be executed by the filter generation unit <b>103</b> according to the second embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart showing an example of the filter generation process according to the second embodiment. Note that, hereinafter, a case of generating, at the time of coding respective blocks (coding object blocks) of a given frame image, respective filters for decoding images of the coding object blocks will be described.</p><p id="p-0067" num="0066">First, the filter generation unit <b>103</b> uses merge mode information acquired by the merge mode information acquisition unit <b>208</b> to acquire an unprocessed merge block group M (i.e., a merge block group M, for which processes in steps S<b>202</b> to S<b>212</b> (to be described later) have not been performed) in the frame image, (step S<b>201</b>). The filter generation unit <b>103</b> then determines, for the merge block group M, whether affine prediction mode is selected, (step S<b>202</b>).</p><p id="p-0068" num="0067">If it is not determined in step S<b>202</b> above that affine prediction mode is selected, the filter generation unit <b>103</b> does not perform processing for the merge block group M and advances to step S<b>212</b>. On the other hand, if it is determined in step S<b>202</b> above that affine prediction mode is selected, the affine transformation parameter acquisition unit <b>201</b> of the filter generation unit <b>103</b> acquires an affine parameter, (step S<b>203</b>).</p><p id="p-0069" num="0068">Subsequently to step S<b>203</b>, the filter generation unit <b>103</b> acquires, of coding blocks B included in the merge block group M, a coding block B, for which a prediction image update process (i.e., a process of steps S<b>202</b> to S<b>211</b> (to be described later)) has not been performed, (step S<b>204</b>). The filter generation unit <b>103</b> then acquires, of subblocks S's included in the coding block B, a subblock S, for which a process (i.e., a process of steps S<b>206</b> to S<b>207</b> (to be described later)) for identifying a reference region has not been performed, (step S<b>205</b>). The block segmentation acquisition unit <b>202</b> of the filter generation unit <b>103</b> then calculates a motion vector of the subblock S in accordance with known affine prediction mode processing (i.e., performs motion compensation) to acquire a reference region S<sub>p </sub>corresponding to the subblock S, (step S<b>206</b>). The block segmentation acquisition unit <b>202</b> of the filter generation unit <b>103</b> then refers to reference image block segmentation information (an example of a coding parameter) to determine whether any coding block B&#x2032; that fully includes the reference region S<sub>p </sub>is present, (step S<b>207</b>).</p><p id="p-0070" num="0069">If it is not determined in step S<b>207</b> above that any coding block B&#x2032; fully including the reference region S<sub>p </sub>is present, the filter generation unit <b>103</b> regards the subblock S as processed and returns to step S<b>205</b>. On the other hand, if it is determined that any coding block B&#x2032; fully including the reference region S<sub>p </sub>is present, the filter generation unit <b>103</b> acquires the coding block B&#x2032; by means of the block segmentation acquisition unit <b>202</b> and adds the coding block B&#x2032; to a block set R which indicates a region of an object in a reference image by means of the in-reference-image object determination unit <b>203</b>, (step S<b>208</b>). In this case, the filter generation unit <b>103</b> regards the subblock S as processed.</p><p id="p-0071" num="0070">Subsequently, the filter generation unit <b>103</b> determines whether processing is finished for all the subblocks included in the coding block B (i.e., whether the process for identifying a reference region has been performed for all the subblocks), (step S<b>209</b>).</p><p id="p-0072" num="0071">If it is not determined in step S<b>209</b> above that processing is finished for all the subblocks included in the coding block B, the filter generation unit <b>103</b> returns to step S<b>205</b>. For this reason, steps S<b>205</b> to S<b>209</b> (or steps S<b>205</b> to S<b>207</b> if NO in step S<b>207</b>) are repeatedly executed for all the subblocks S's included in the coding block B.</p><p id="p-0073" num="0072">On the other hand, if it is determined in step S<b>209</b> above that processing is finished for all the subblocks included in the coding block B, the filter generation unit <b>103</b> regards the coding block B as processed and determines whether processing is finished for all the coding blocks included in the merge block group M (i.e., whether the prediction image update process has been performed for all the coding object blocks), (step S<b>210</b>).</p><p id="p-0074" num="0073">If it is not determined in step S<b>210</b> above that processing is finished for all the coding blocks included in the merge block group M, the filter generation unit <b>103</b> returns to step S<b>204</b>. For this reason, steps S<b>204</b> to S<b>210</b> are repeatedly executed for all the coding blocks B included in the merge block group M.</p><p id="p-0075" num="0074">On the other hand, if it is determined in step S<b>210</b> above that processing is finished for all the coding blocks included in the merge block group M, the filter generation unit <b>103</b> computes an inverse affine parameter by means of the inverse affine transformation parameter computation unit <b>204</b>, uses the inverse affine parameter to perform an inverse affine transformation on the block set R (i.e., perform an inverse transformation of an affine transformation on the coding object block B) by means of the affine transformation unit <b>205</b>, and sets the block set R after the inverse affine transformation as a new prediction image by means of the prediction image generation unit <b>206</b>, (step S<b>211</b>). The prediction image, i.e., a filter for a decoding image is obtained. Since the prediction image is applied not as an in-loop filter but as a post-filter in the second embodiment, it is not necessary to limit an application region of the prediction image to a region corresponding to the merge block group M. However, the effect of preventing image quality degradation in a case where a coding block B&#x2032; in the prediction image covers a wide range including not only an object corresponding to the merge block group M but also a background region is expected from limiting the application region of the prediction image to (pixels of) the region corresponding to the merge block group M, as in the first embodiment.</p><p id="p-0076" num="0075">Subsequently, the filter generation unit <b>103</b> regards the merge block group M acquired in step S<b>201</b> above as processed, (step S<b>212</b>), and determines whether all merge block groups in the frame image are processed (i.e., whether processes in steps S<b>202</b> to S<b>212</b> have been performed for all the merge block groups M's in the frame image), (step S<b>213</b>).</p><p id="p-0077" num="0076">If it is not determined in step S<b>213</b> above that all the merge block groups are processed, the filter generation unit <b>103</b> returns to step S<b>201</b>. For this reason, steps S<b>201</b> to S<b>213</b> (or steps S<b>201</b> to S<b>202</b> and steps S<b>212</b> to S<b>213</b> if NO in step S<b>202</b>) are repeatedly executed for all the merge block groups included in the frame image.</p><p id="p-0078" num="0077">On the other hand, if it is determined in step S<b>213</b> above that all the merge block groups are processed, the filter generation unit <b>103</b> ends the filter generation process. In the above-described manner, a filter corresponding to each merge block group included in one frame image is generated.</p><p id="p-0079" num="0078">[Hardware Configuration]</p><p id="p-0080" num="0079">A hardware configuration of the coding apparatus <b>10</b> according to each of the above-described embodiments will be described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram showing an example of a hardware configuration of a coding apparatus <b>10</b> according to one embodiment.</p><p id="p-0081" num="0080">As shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the coding apparatus <b>10</b> according to the one embodiment has an input device <b>301</b>, a display device <b>302</b>, an external I/F <b>303</b>, a communication I/F <b>304</b>, a processor <b>305</b>, and a memory device <b>306</b>. The pieces of hardware each are connected so as to be capable of communication via a bus <b>307</b>.</p><p id="p-0082" num="0081">The input device <b>301</b> is, for example, a keyboard, a mouse, a touch panel, or the like. The display device <b>302</b> is, for example, a display or the like. Note that the coding apparatus <b>10</b> need not have at least one of the input device <b>301</b> and the display device <b>302</b>.</p><p id="p-0083" num="0082">The external I/F <b>303</b> is an interface with an external apparatus. As the external apparatus, for example, a recording medium <b>303</b><i>a</i>, such as a CD (Compact Disc), a DVD (Digital Versatile Disk), an SD memory card (Secure Digital memory card), or a USB (Universal Serial Bus) memory card, is available.</p><p id="p-0084" num="0083">The communication I/F <b>304</b> is an interface for connecting the coding apparatus <b>10</b> to a communication network. The processor <b>305</b> is, for example, one of various types of arithmetic devices, such as a CPU (Central Processing Unit) or a GPU (Graphics Processing Unit). The memory device <b>306</b> is, for example, one of various types of storage devices, such as an HDD (Hard Disk Drive), an SSD (Solid State Drive), a RAM (Random Access Memory), a ROM (Read Only Memory), or a flash memory.</p><p id="p-0085" num="0084">The coding apparatus <b>10</b> according to each of the embodiments has the hardware configuration shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, thereby being capable of implementing the filter generation process and the like described above. Note that the hardware configuration shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is an example and that the coding apparatus <b>10</b> may have another hardware configuration. For example, the coding apparatus <b>10</b> may have a plurality of processors <b>305</b> or a plurality of memory devices <b>306</b>.</p><p id="p-0086" num="0085">[Conclusion]</p><p id="p-0087" num="0086">As described above, the coding apparatuses <b>10</b> according to the first and second embodiments create, as a filter for an inter prediction image, a prediction image, in which a prediction residual error (a prediction error) due to various types of transformations (affine transformation is named as an example above) at the time of moving image coding or video coding is reduced, while curbing the amount of computation of the transformations. This allows the coding apparatuses <b>10</b> according to the first and second embodiments to reduce a prediction residual error while curbing the amount of computation and improve image quality of a decoding image. Note that, for example, the effects can be expected from the coding apparatuses <b>10</b> according to the first and second embodiments, especially in a case where affine prediction is often selected, as in inter-view prediction in a stereo image, a multi-view image, or a LightField image.</p><p id="p-0088" num="0087">Note that although the first and second embodiments have described the coding apparatus <b>10</b> having the filter generation unit <b>103</b> as an example, the present invention is not limited to this. For example, a filter generation apparatus different from the coding apparatus <b>10</b> may have the filter generation unit <b>103</b>.</p><p id="p-0089" num="0088">The present invention is not limited to the above-described embodiments that are specifically disclosed, and various modifications, changes, combinations with known techniques, and the like can be made without departing from the description of the claims.</p><heading id="h-0013" level="1">REFERENCE SIGNS LIST</heading><p id="p-0090" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0000">    <ul id="ul0003" list-style="none">        <li id="ul0003-0001" num="0089"><b>10</b> Coding apparatus</li>        <li id="ul0003-0002" num="0090"><b>101</b> Intra prediction unit</li>        <li id="ul0003-0003" num="0091"><b>102</b> Inter prediction unit</li>        <li id="ul0003-0004" num="0092"><b>103</b> Filter generation unit</li>        <li id="ul0003-0005" num="0093"><b>104</b> Filter unit</li>        <li id="ul0003-0006" num="0094"><b>105</b> Mode determination unit</li>        <li id="ul0003-0007" num="0095"><b>106</b> DCT unit</li>        <li id="ul0003-0008" num="0096"><b>107</b> Quantization unit</li>        <li id="ul0003-0009" num="0097"><b>108</b> Inverse quantization unit</li>        <li id="ul0003-0010" num="0098"><b>109</b> Inv-DCT unit</li>        <li id="ul0003-0011" num="0099"><b>110</b> Reference image memory</li>        <li id="ul0003-0012" num="0100"><b>111</b> Reference image block segmentation shape memory</li>        <li id="ul0003-0013" num="0101"><b>201</b> affine transformation parameter acquisition unit</li>        <li id="ul0003-0014" num="0102"><b>202</b> Block segmentation acquisition unit</li>        <li id="ul0003-0015" num="0103"><b>203</b> In-reference-image object determination unit</li>        <li id="ul0003-0016" num="0104"><b>204</b> Inverse affine transformation parameter computation unit</li>        <li id="ul0003-0017" num="0105"><b>205</b> affine transformation unit</li>        <li id="ul0003-0018" num="0106"><b>206</b> Prediction image generation unit</li>        <li id="ul0003-0019" num="0107"><b>207</b> Filter region limitation unit</li>        <li id="ul0003-0020" num="0108"><b>208</b> Merge mode information acquisition unit</li>    </ul>    </li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007237A1-20230105-M00001.NB"><img id="EMI-M00001" he="8.81mm" wi="76.20mm" file="US20230007237A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230007237A1-20230105-M00002.NB"><img id="EMI-M00002" he="13.38mm" wi="76.20mm" file="US20230007237A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230007237A1-20230105-M00003.NB"><img id="EMI-M00003" he="13.38mm" wi="76.20mm" file="US20230007237A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer implemented method for generating a filter for an inter prediction image in moving image coding or video coding, the method comprising:<claim-text>acquiring, for each of subblocks included in a coding object block, a region in a reference image that corresponds to a subblock;</claim-text><claim-text>referring to block segmentation information of the reference image;</claim-text><claim-text>acquiring a coding block, the coding block including a block of the reference image which includes the region; and</claim-text><claim-text>generating, for each of a plurality of coding object blocks, an image as the filter by performing an inverse transformation on the acquired coding block.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the generating comprises:<claim-text>generating the image as the filter, through which a region corresponding to a region represented by the coding object block or a region represented by the plurality of decoding object blocks is passed.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the inverse transformation includes an inverse transformation of a transformation on the coding object block, and</claim-text><claim-text>the transformation includes one of affine transformation, projective transformation, or bilinear transformation.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. A filter generation device for generating a filter for an inter prediction image in moving image coding or video coding, comprising a processor configured to execute a method comprising:<claim-text>acquiring, for each of subblocks included in a coding object block, a region in a reference image that corresponds to a subblock;</claim-text><claim-text>referring to block segmentation information of the reference image;</claim-text><claim-text>acquiring a coding block, the coding block representing a block of the reference image which includes the region; and</claim-text><claim-text>generating, for the coding object block, an image as the filter by performing an inverse transformation on the acquired coding block.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. A computer-readable non-transitory recording medium storing computer-executable program instructions that when executed by a processor cause a computer to execute a method comprising:<claim-text>acquiring, for each of subblocks included in a coding object block, a region in a reference image that corresponds to a subblock;</claim-text><claim-text>referring to block segmentation information of the reference image;</claim-text><claim-text>acquiring a coding block, the coding block representing a block of the reference image which includes the region; and</claim-text><claim-text>generating, for the coding object block or the acquired coding object block, an image as the filter by performing an inverse transformation on the acquired coding block.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the filter is associated with the inter prediction image of the coding object block.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer implemented method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the inverse transformation includes an inverse transformation of a transformation on the coding object block, and</claim-text><claim-text>the transformation includes one of affine transformation, projective transformation, or bilinear transformation.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The filter generation device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the generating comprises:<claim-text>generating the image as the filter, through which a region corresponding to a region represented by the coding object block or a region represented by the plurality of decoding object blocks is passed.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The filter generation device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the filter is associated with the inter prediction image of the coding object block.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The filter generation device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein<claim-text>the inverse transformation includes an inverse transformation of a transformation on the coding object block, and</claim-text><claim-text>the transformation includes one of affine transformation, projective transformation, or bilinear transformation.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The filter generation device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein<claim-text>the inverse transformation includes an inverse transformation of a transformation on the coding object block, and</claim-text><claim-text>the transformation includes one of affine transformation, projective transformation, or bilinear transformation.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the generating comprises:<claim-text>generating the image as the filter, through which a region corresponding to a region represented by the coding object block or a region represented by the plurality of decoding object blocks is passed.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the filter is associated with an inter prediction image of the coding object block.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the inverse transformation includes an inverse transformation of a transformation on the coding object block, and</claim-text><claim-text>the transformation includes one of affine transformation, projective transformation, or bilinear transformation.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein<claim-text>the inverse transformation includes an inverse transformation of a transformation on the coding object block, and</claim-text><claim-text>the transformation includes one of affine transformation, projective transformation, or bilinear transformation.</claim-text></claim-text></claim></claims></us-patent-application>