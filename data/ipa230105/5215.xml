<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005216A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005216</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943479</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-058979</doc-number><date>20200327</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>98</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>761</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>98</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">THREE-DIMENSIONAL MODEL GENERATION METHOD AND THREE-DIMENSIONAL MODEL GENERATION DEVICE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2021/012093</doc-number><date>20210323</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17943479</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Panasonic Intellectual Property Management Co., Ltd.</orgname><address><city>Osaka</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>TERANISHI</last-name><first-name>Kensho</first-name><address><city>Osaka</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>YOSHIKAWA</last-name><first-name>Satoshi</first-name><address><city>Hyogo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>MATSUNOBU</last-name><first-name>Toru</first-name><address><city>Osaka</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>FUKUDA</last-name><first-name>Masaki</first-name><address><city>Osaka</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A three-dimensional model generation method executed by an information processing device includes: obtaining images generated by shooting a subject from respective viewpoints; searching for a similar point that is similar to a first point in a first image among the images, from second points in a search area in a second image different from the first image, the search area being provided based on the first point; calculating an accuracy of a search result of the searching, using degrees of similarity between the first point and the respective second points; and generating a three-dimensional model using the search result and the accuracy.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="146.56mm" wi="158.75mm" file="US20230005216A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="104.48mm" wi="116.92mm" file="US20230005216A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="203.20mm" wi="132.93mm" file="US20230005216A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="168.74mm" wi="162.73mm" file="US20230005216A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="130.89mm" wi="152.57mm" file="US20230005216A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="246.13mm" wi="161.71mm" file="US20230005216A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="246.13mm" wi="161.71mm" file="US20230005216A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="181.86mm" wi="162.64mm" file="US20230005216A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="181.86mm" wi="162.64mm" file="US20230005216A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="166.12mm" wi="104.06mm" file="US20230005216A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="210.23mm" wi="94.57mm" file="US20230005216A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="175.94mm" wi="162.90mm" file="US20230005216A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This is a continuation application of PCT International Application No. PCT/JP2021/012093 filed on Mar. 23, 2021, designating the United States of America, which is based on and claims priority of Japanese Patent Application No. 2020-058979 filed on Mar. 27, 2020. The entire disclosures of the above-identified applications, including the specifications, drawings and claims are incorporated herein by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to a three-dimensional model generation method and a three-dimensional model generation device.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Patent Literature (PTL) 1 discloses a technique of generating a three-dimensional model of a subject, using images obtained by shooting the subject from a plurality of viewpoints.</p><heading id="h-0004" level="1">CITATION LIST</heading><heading id="h-0005" level="1">Patent Literature</heading><p id="p-0005" num="0004">PTL 1: Japanese Unexamined Patent Application Publication No. 2017-130146</p><heading id="h-0006" level="1">SUMMARY</heading><heading id="h-0007" level="1">Technical Problem</heading><p id="p-0006" num="0005">In the generating of three-dimensional models, there is a demand for improving three-dimensional model generation accuracy.</p><p id="p-0007" num="0006">The present disclosure provides a three-dimensional model generation method, and so on, capable of improving three-dimensional model generation accuracy.</p><heading id="h-0008" level="1">Solution to Problem</heading><p id="p-0008" num="0007">A three-dimensional model generation method according to an aspect of the present disclosure is a three-dimensional model generation method executed by an information processing device, and includes: obtaining images generated by shooting a subject from respective viewpoints; searching for a similar point that is similar to a first point in a first image among the images, from second points in a search area in a second image different from the first image, the search area being provided based on the first point; calculating an accuracy of a search result of the searching, using degrees of similarity between the first point and the respective second points; and generating a three-dimensional model of the subject using the search result and the accuracy.</p><p id="p-0009" num="0008">A three-dimensional model generation device according to an aspect of the present disclosure includes: memory; and a processor, wherein using the memory, the processor: obtains images generated by shooting a subject from respective viewpoints; searches for a similar point that is similar to a first point in a first image among the images, in a search area in a second image different from the first image, the search area being provided based on the first point; calculates an accuracy of a three-dimensional point generated based on the first point, using degrees of similarity between the first point and respective second points in the search area; and generates a three-dimensional model of the subject using a result of the search and the accuracy.</p><p id="p-0010" num="0009">It should be noted that the present disclosure may be implemented as a program that causes a computer to execute the steps included in the three-dimensional model generation method described above. Furthermore, the present disclosure may be implemented as a non-transitory computer-readable recording medium, such as a CD-ROM, having the above program recorded thereon. Furthermore, the present disclosure may be implemented as information, data, or signal representing the above program. In addition, the program, information, data, and signal may be distributed via a communication network such as the Internet.</p><p id="p-0011" num="0010">Advantageous Effects</p><p id="p-0012" num="0011">A three-dimensional model generation method, and so on according to the present disclosure can improve generation accuracy of three-dimensional models.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0013" num="0012">These and other advantages and features will become apparent from the following description thereof taken in conjunction with the accompanying Drawings, by way of non-limiting examples of embodiments disclosed herein.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram for describing an outline of a three-dimensional model generation method according to an embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating the characteristic configuration of a three-dimensional model generation device according to the embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for describing search processing by a searcher.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating a relationship between a subject and frames.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating an example of a case where a first accuracy is lower than a predetermined accuracy.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating an edge for which a first accuracy tends to be lower than a predetermined accuracy in a first frame.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an example of a case where a first accuracy is higher than a predetermined accuracy.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating an edge for which a first accuracy tends to be higher than a predetermined accuracy in a first frame.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram for describing a first example of three-dimensional model generation processing.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram for describing a second example of three-dimensional model generation processing.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating an example of the operation of a three-dimensional model generation device.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart illustrating an example of details of the generation processing in step S<b>104</b> by a generator.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram for describing search processing by a searcher in a variation.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><heading id="h-0011" level="2">(Circumstances Leading to the Present Disclosure)</heading><p id="p-0027" num="0026">In the technique disclosed in PTL 1, a three-dimensional model is generated by searching for a similar point between images. Typically, in the searching of a similar point, when a similar point of a single pixel in a single image is searched for from another image, an epipolar line on the other image is calculated from a geometric constraint of a camera, and searching for a plurality of pixels on the epipolar line is performed. However, there is the problem that search accuracy deteriorates when a plurality of pixels that are similar to a single pixel are present on the epipolar line, such as in the case where the same textures are lined up on the epipolar line.</p><p id="p-0028" num="0027">In view of this, the present disclosure provides a three-dimensional model generation method, and so on, that can improve generation accuracy of three-dimensional models.</p><p id="p-0029" num="0028">A three-dimensional model generation method according to an aspect of the present disclosure is a three-dimensional model generation method executed by an information processing device, and includes: obtaining images generated by shooting a subject from a plurality of viewpoints; searching for a similar point that is similar to a first point in a first image among the images, from second points in a search area in a second image different from the first image, the search area being based on the first point; calculating an accuracy of a search result, using degrees of similarity each between the first point and a different one of the second points; and generating a three-dimensional model of the subject using the search result and the accuracy.</p><p id="p-0030" num="0029">According to this method, since the three-dimensional model is generated using search results and accuracies of the search results, the accuracy of generation of the three-dimensional model can be improved by avoiding using search results with lower accuracies or by preferentially using search results with higher accuracies for generating the three-dimensional model, for example. Furthermore, in the searching, the similar point may be searched for in each of second images, each of the second images being the second image. In the calculating of the accuracy, a first accuracy may be calculated for each of first search results corresponding to a different one of the second images. In the generating of the three-dimensional model, the three-dimensional model may be generated using the first search results and the first accuracies.</p><p id="p-0031" num="0030">According to this method, since the three-dimensional model is generated using a plurality of first search results and a plurality of first accuracies, three-dimensional points with higher accuracies can be generated by avoiding using first search results with lower accuracies or by preferentially using first search results with higher accuracies for generating the three-dimensional model, for example. Therefore, the accuracy of generation of the three-dimensional model can be improved.</p><p id="p-0032" num="0031">Furthermore, in the generating of the three-dimensional model, the three-dimensional model may be generated without using the first search result for a second image for which the first accuracy calculated is lower than a predetermined accuracy.</p><p id="p-0033" num="0032">According to this method, since a first search result with an accuracy lower than the predetermined threshold is not used for generating the three-dimensional model, three-dimensional points with higher accuracies can be generated.</p><p id="p-0034" num="0033">Furthermore, the generating of the three-dimensional model may include: selecting, from among the second images, N second images in a descending order of the first accuracy calculated (where N is an integer greater than or equal to 1); and generating the three-dimensional model using N first search results corresponding to the N second images selected.</p><p id="p-0035" num="0034">According to this method, since N first search results with higher accuracies are preferentially used for generating the three-dimensional model, three-dimensional points with higher accuracies can be generated.</p><p id="p-0036" num="0035">Furthermore, the generating of the three-dimensional model may include: generating, based on the first search results, three-dimensional points each of which corresponds to a different one of the first search results; generating an integrated three-dimensional point by weighted averaging of the three-dimensional points in which a greater weight is given to a three-dimensional point having a corresponding first search result with a higher first accuracy; and generating the three-dimensional model including the integrated three-dimensional point generated.</p><p id="p-0037" num="0036">According to this method, since an integrated three-dimensional point is generated by weighted averaging of three-dimensional points generated using the first search results in which a greater weight is given to a three-dimensional point with a higher accuracy, three-dimensional points with higher accuracies can be generated.</p><p id="p-0038" num="0037">Furthermore, the searching may be performed for each of first points, each of the first points being the first point. In the calculating of the accuracy, the first accuracies may be calculated for each of the first points. In the generating of the three-dimensional model, a three-dimensional model including three-dimensional points obtained by generating, for each of the first points, a three-dimensional point using the first search results and the first accuracies may be generated as the three-dimensional model.</p><p id="p-0039" num="0038">Therefore, a three-dimensional point with a higher accuracy can be generated for each of the first points. Therefore, the accuracy of generation of the three-dimensional model can be improved.</p><p id="p-0040" num="0039">Furthermore, the calculating of the accuracy may include calculating, for each of the first points, a sum of values indicating the first accuracies calculated for the first point, as a second accuracy of a search result based on the first point. In the generating of the three-dimensional model, a three-dimensional model including the three-dimensional points and the second accuracies corresponding to the three-dimensional points may be generated as the three-dimensional model.</p><p id="p-0041" num="0040">Therefore, each of the three-dimensional points included in the three-dimensional model can be associated with a second accuracy that is based on the first accuracies of the search results used when generating the three-dimensional point.</p><p id="p-0042" num="0041">Furthermore, the generating of the three-dimensional model may include correcting at least one low-accuracy three-dimensional point located between two high-accuracy three-dimensional points using the two high-accuracy three-dimensional points as reference, the at least one low-accuracy three-dimensional point being associated with a second accuracy lower than a predetermined accuracy, the two high-accuracy three-dimensional points each being associated with a second accuracy higher than the predetermined accuracy.</p><p id="p-0043" num="0042">According to this method, low-accuracy three-dimensional points can be corrected with respect to high-accuracy three-dimensional points by permitting a greater displacement for correction for a three-dimensional point with a lower accuracy, for example, and therefore, the accuracy of generation of the three-dimensional model can be improved.</p><p id="p-0044" num="0043">Furthermore, the search area may be an area defined by pixels on an epipolar line corresponding to the first point. Therefore, a candidate for a pixel similar to the first pixel can be efficiently selected from among the second pixels.</p><p id="p-0045" num="0044">A three-dimensional model generation device according to an aspect of the present disclosure includes memory and a processor. Using the memory, the processor: obtains images generated by shooting a subject from a plurality of viewpoints; searches for a similar point that is similar to a first point in a first image among the images, in a search area in a second image different from the first image, the search area being based on the first point; calculates an accuracy of a three-dimensional point generated based on the first point, using degrees of similarity each between the first point and a different one of second points in the search area; and generates a three-dimensional model of the subject using a result of the search and the accuracy.</p><p id="p-0046" num="0045">According to this method, since the three-dimensional model is generated using search results and accuracies of the search results, the accuracy of generation of the three-dimensional model can be improved by avoiding using search results with lower accuracies or by preferentially using search results with higher accuracies for generating the three-dimensional model, for example.</p><p id="p-0047" num="0046">Hereinafter, respective embodiments of a three-dimensional model generation method, etc., according to the present disclosure will be described in detail with reference to the drawings. It should be noted that each of the subsequently described embodiments shows a specific example of the present disclosure. Accordingly, numerical values, shapes, structural components, the arrangement and connection of the structural components, steps, and the processing order of the steps, etc., shown in each of the following embodiments are merely examples, and are therefore not intended to limit the scope of the present disclosure.</p><p id="p-0048" num="0047">Furthermore, the respective figures are not necessarily precise illustrations. In the figures, structural components that are substantially the same are assigned the same reference signs, and overlapping description thereof may be omitted or simplified.</p><heading id="h-0012" level="1">Embodiment</heading><heading id="h-0013" level="2">[Outline]</heading><p id="p-0049" num="0048">First, referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the outline of a three-dimensional model generation method according to an embodiment will be described.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram for describing the outline of the three-dimensional model generation method according to an embodiment. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating the characteristic configuration of three-dimensional model generation device <b>100</b> according to the embodiment.</p><p id="p-0051" num="0050">In the three-dimensional model generation method, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the three-dimensional model of a predetermined region is generated from a plurality of images shot from a plurality of different viewpoints using a plurality of imaging devices <b>301</b>. Here, the predetermined region is a region including a stationary static object or a moving body such as a person, or the both. In other words, the predetermined region is a region including, for example, at least one of a stationary static object or a moving body as a subject.</p><p id="p-0052" num="0051">As an example of the predetermined region including a static object and an animal body, there is a venue where a sport game such as a basketball game is being held, a space on a road where a person or a vehicle exists, or the like. Note that the predetermined region may include not only a specific object to be used as a subject, but also scenery, etc. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a case where subject <b>500</b> is a building. Additionally, hereinafter, the predetermined region including not only a specific object to be used as a subject, but also scenery, etc. is also simply called the subject.</p><p id="p-0053" num="0052">As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, three-dimensional model generation system <b>400</b> includes imaging device group <b>300</b> including a plurality of imaging devices <b>301</b>, estimation device <b>200</b>, and three-dimensional model generation device <b>100</b>.</p><p id="p-0054" num="0000">(Imaging devices)</p><p id="p-0055" num="0053">Imaging devices <b>301</b> are imaging devices that shoot a predetermined area. Each of imaging devices <b>301</b> shoots a subject, and outputs each of a plurality of shot frames to estimation device <b>200</b>. In Embodiment 1, two or more imaging devices <b>301</b> are included in imaging device group <b>300</b>. Additionally, imaging devices <b>301</b> shoot the same subject from a plurality of mutually different viewpoints. A frame is, in other words, an image.</p><p id="p-0056" num="0054">Note that, although it has been assumed that three-dimensional model generation system <b>400</b> includes imaging device group <b>300</b>, three-dimensional model generation system <b>400</b> is not limited to this, and may include one imaging device <b>301</b>. For example, in three-dimensional model generation system <b>400</b>, while moving one imaging device <b>301</b>, the one imaging device <b>301</b> may be caused to generate, for a subject that exists in a real space, a multi-viewpoint image formed from a plurality of frames having mutually different viewpoints. The plurality of frames are frames shot (generated) with imaging devices <b>301</b> that differ from each other in at least one of the positions or orientations of imaging devices <b>301</b>, respectively.</p><p id="p-0057" num="0055">Additionally, each imaging device <b>301</b> may be an imaging device that generates a two-dimensional image, or an imaging device with a three-dimensional measuring sensor that generates a three-dimensional model. In the present embodiment, each of imaging devices <b>301</b> is an imaging device that generates a two-dimensional image.</p><p id="p-0058" num="0056">Each of imaging devices <b>301</b> may be directly connected to estimation device <b>200</b> by wired communication or wireless communication, so as to be able to output the shot frames to estimation device <b>200</b>, or may be indirectly connected to estimation device <b>200</b> via a hub, such as a communication equipment or a server, which is not illustrated.</p><p id="p-0059" num="0057">Note that the frames shot with imaging devices <b>301</b>, respectively, may be output to estimation device <b>200</b> in real time. Additionally, after the frames are once recorded in external storage devices, such as a memory or a cloud server, the frames may be output from these external storage devices to estimation device <b>200</b>.</p><p id="p-0060" num="0058">Additionally, imaging devices <b>301</b> may be fixed cameras such as surveillance cameras, respectively, may be mobile cameras such as video cameras, smart phones, or wearable cameras, or may be moving cameras such as drones with a shooting function.</p><heading id="h-0014" level="2">(Estimation Device)</heading><p id="p-0061" num="0059">Estimation device <b>200</b> performs camera calibration by causing one or more imaging devices <b>301</b> to shoot a subject from a plurality of viewpoints. Estimation device <b>200</b> performs the camera calibration that estimates the positions and orientations of imaging devices <b>301</b> based on, for example, a plurality of frames shot with imaging devices <b>301</b>. Here, the orientation of imaging device <b>301</b> indicates at least one of the shooting direction of imaging device <b>301</b>, or the inclination of imaging device <b>301</b>. The shooting direction of imaging device <b>301</b> is the direction of the optical axis of imaging device <b>301</b>. The inclination of imaging device <b>301</b> is the rotation angle around the optical axis of imaging device <b>301</b> from a reference orientation.</p><p id="p-0062" num="0060">Specifically, estimation device <b>200</b> estimates camera parameters of imaging devices <b>301</b>, based on a plurality of frames obtained from imaging devices <b>301</b>. Here, the camera parameters are parameters that indicates the characteristics of imaging devices <b>301</b>, and are a plurality of parameters that includes internal parameter including a focal point distance, an image center, etc. of imaging device <b>301</b>, and external parameters indicating the position (more specifically, three-dimensional position) and orientation of imaging device <b>301</b>. That is, the position and orientation of each of imaging devices <b>301</b> are obtained by estimating respective camera parameters.</p><p id="p-0063" num="0061">Note that the estimation method of estimating, by estimation device <b>200</b>, the positions and orientations of imaging devices <b>301</b> is not particularly limited. Estimation device <b>200</b> may estimate the positions and orientations of imaging devices <b>301</b> by using, for example, the Visual-SLAM (Simultaneous Localization and Mapping) technology. Alternatively, estimation device <b>200</b> may estimate the positions and orientations of imaging devices <b>301</b> by using, for example, the Structure-From-Motion technology.</p><p id="p-0064" num="0062">Here, a method of estimating positions and orientations of imaging devices <b>301</b> by estimation device <b>200</b> will be described using <figref idref="DRAWINGS">FIG. <b>3</b></figref></p><p id="p-0065" num="0063">Estimation device <b>200</b> performs searching for feature points by extracting characteristic points from frames <b>531</b> to <b>533</b> shot with imaging devices <b>301</b> as feature points, respectively, by using the Visual-SLAM technology or the Structure-From-Motion technology, and extracts a set of similar points that are similar between a plurality of frames among the extracted feature points. Since estimation device <b>200</b> can identify a point on subject <b>510</b> reflected in common in frames <b>531</b> to <b>533</b> by performing the searching for feature points, the three-dimensional coordinates of the point on subject <b>510</b> can be obtained with the principle of triangulation by using the set of the extracted similar points.</p><p id="p-0066" num="0064">In this manner, estimation device <b>200</b> can estimate the position and orientation of each imaging device <b>301</b> by extracting a plurality of sets of similar points, and using the sets of similar points. In the process of estimating the position and orientation of each imaging device <b>301</b>, estimation device <b>200</b> calculates three-dimensional coordinates for each set of similar points, and generates three-dimensional model <b>520</b> including a plurality of three-dimensional points indicated by the plurality of calculated three-dimensional coordinates. Each of the plurality of three-dimensional points indicates the position on subject <b>510</b> in the three-dimensional space. Estimation device <b>200</b> obtains, as estimation results, the position and orientation of each imaging device <b>301</b>, and the map information. Since the obtained three-dimensional model <b>520</b> is processed for optimization with the camera parameters, the obtained three-dimensional model <b>520</b> is information with higher accuracy than predetermined accuracy. Additionally, three-dimensional model <b>520</b> includes the three-dimensional position of each of the plurality of three-dimensional points. Note that three-dimensional model <b>520</b> may include not only the plurality of three-dimensional positions, but also the color of each three-dimensional point, the surface shape around each three-dimensional point, information indicating the frame from which each three-dimensional point is generated, etc.</p><p id="p-0067" num="0065">Additionally, in order to increase the speed of the estimation processing, estimation device <b>200</b> may generate three-dimensional model <b>520</b> including a sparse three-dimensional point cloud by limiting the number of sets of similar points to a predetermined number. Because estimation device <b>200</b> can estimate the position and orientation of each imaging device <b>301</b> with sufficient accuracy, even with the predetermined number of sets of similar points. Note that the predetermined number may be determined to be a number with which the position and orientation of each imaging device <b>301</b> can be estimated with sufficient accuracy. Additionally, estimation device <b>200</b> may estimate the position and orientation of each imaging device <b>301</b> by using sets that are similar at or above a predetermined degree of similarity among the sets of similar points. As a result, estimation device <b>200</b> can limit the number of sets of similar points used for the estimation processing to the number of sets that are similar at or above the predetermined similarity.</p><p id="p-0068" num="0066">Additionally, based on, for example, the position and orientation of imaging device <b>301</b> estimated by using the above-described technology, estimation device <b>200</b> may calculate the distance between imaging device <b>301</b> and subject <b>510</b> as a camera parameter. Note that three-dimensional model generation system <b>400</b> may include a distance measurement sensor, and the distance between imaging device <b>301</b> and subject <b>510</b> may be measured by using the distance measurement sensor.</p><p id="p-0069" num="0067">Estimation device <b>200</b> may be directly connected to three-dimensional model generation device <b>100</b> by wired communication or wireless communication, or may be indirectly connected to estimation device <b>200</b> via a hub, such as a communication equipment or a server, which is not illustrated. Accordingly, estimation device <b>200</b> outputs a plurality of frames received from imaging devices <b>301</b>, and a plurality of estimated camera parameters of imaging devices <b>301</b> to three-dimensional model generation device <b>100</b>.</p><p id="p-0070" num="0068">Note that the camera parameters estimated by estimation device <b>200</b> may be output to three-dimensional model generation device <b>100</b> in real time. Additionally, after the camera parameters are once recorded in external storage devices, such as a memory or a cloud server, the camera parameters may be output from these external storage devices to three-dimensional model generation device <b>100</b>.</p><p id="p-0071" num="0069">Estimation device <b>200</b> includes at least a computer system that includes, for example, a control program, a processing circuit such as a processor or a logical circuit that executes the control program, and a recording device such as an internal memory or an accessible external memory storing the control program. (Three-dimensional model generation device)</p><p id="p-0072" num="0070">Three-dimensional model generation device <b>100</b> generates the three-dimensional model of a predetermined area, based on a plurality of frames shot with imaging devices <b>301</b>, and camera parameters estimated by estimation device <b>200</b>. Specifically, three-dimensional model generation device <b>100</b> is a device that performs three-dimensional model generation processing that generates the three-dimensional model of a subject in a virtual three-dimensional space, based on the respective camera parameters of imaging devices <b>301</b>, and the plurality of frames.</p><p id="p-0073" num="0071">Note that the three-dimensional model of the subject is data including the three-dimensional shape of the subject and the color of the subject that are restored in the virtual three-dimensional space from the frames obtained by shooting the actual subject. The three-dimensional model of the subject is a set of points indicating the respective three-dimensional positions of a plurality of points on the subject reflected to each of a plurality of two-dimensional images shot with a plurality of imaging devices <b>301</b> from multiple viewpoints, that is, a plurality of different viewpoints.</p><p id="p-0074" num="0072">A three-dimensional position is represented by, for example, three-value information formed from a X component, a Y component, and a Z component that indicate the positions on an X-axis, a Y-axis, and a Z-axis that are orthogonal to each other, respectively. Note that the information included in a plurality of points indicating three-dimensional positions may include not only the three-dimensional positions (that is, information indicating coordinates), but also information indicating the color of each point, information representing the surface shapes of each point and its surrounding, etc. In this manner, the information on three-dimensional positions includes information other than information on an imaging viewpoint of a frame and the distance from the subject.</p><p id="p-0075" num="0073">Three-dimensional model generation device <b>100</b> includes at least a computer system that includes, for example, a control program, a processing circuit such as a processor or a logical circuit that executes the control program, and a recording device such as an internal memory or an accessible external memory storing the control program. Three-dimensional model generation device <b>100</b> is an information processing device. The function of each processing unit of three-dimensional model generation device <b>100</b> may be realized by software, or may be realized by hardware.</p><p id="p-0076" num="0074">Additionally, three-dimensional model generation device <b>100</b> may store the camera parameters in advance. In this case, three-dimensional model generation system <b>400</b> need not include estimation device <b>200</b>. Additionally, imaging devices <b>301</b> may be communicatively connected to three-dimensional model generation device <b>100</b> wirelessly or with wires.</p><p id="p-0077" num="0075">Additionally, frames shot with imaging devices <b>301</b> may be directly output to three-dimensional model generation device <b>100</b>. In this case, imaging devices <b>301</b> may be directly connected to three-dimensional model generation device <b>100</b> by wired communication or wireless communication, or may be indirectly connected to three-dimensional model generation device <b>100</b> via a hub, such as a communication equipment or a server, which is not illustrated.</p><heading id="h-0015" level="2">[Configuration of Three-Dimensional Model Generation Device]</heading><p id="p-0078" num="0076">Subsequently, referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the details of the configuration of three-dimensional model generation device <b>100</b> will be described.</p><p id="p-0079" num="0077">Three-dimensional model generation device <b>100</b> is a device that generates a three-dimensional model from a plurality of frames. Three-dimensional model generation device <b>100</b> includes receiver <b>110</b>, storage <b>120</b>, obtainer <b>130</b>, generator <b>140</b>, and outputter <b>150</b>.</p><p id="p-0080" num="0078">Receiver <b>110</b> receives, from estimation device <b>200</b>, frames shot with imaging devices <b>301</b>, and the camera parameters estimated by estimation device <b>200</b>. Accordingly, receiver <b>110</b> obtains a first frame (first image) of a subject shot from a first viewpoint, and a second frame (second image) of the subject shot from a second viewpoint. That is, the frames received by receiver <b>110</b> include the first frame and the second frame. Receiver <b>110</b> ma obtain three-dimensional model <b>520</b> from estimation device <b>200</b>. Receiver <b>110</b> outputs the received frames and camera parameters to storage <b>120</b>. Receiver <b>110</b> is, for example, a communication interface for communicating with estimation device <b>200</b>. When three-dimensional model generation device <b>100</b> and estimation device <b>200</b> perform wireless communication, receiver <b>110</b> includes, for example, an antenna and a wireless communication circuit. Alternatively, when three-dimensional model generation device <b>100</b> and estimation device <b>200</b> perform wired communication, receiver <b>110</b> includes, for example, a connector connected to a communication line, and a wired communication circuit. Note that receiver <b>110</b> may receive frames from imaging devices <b>301</b> without going through estimation device <b>200</b>.</p><p id="p-0081" num="0079">Storage <b>120</b> stores the plurality of frames and the camera parameters that are received by receiver <b>110</b>. Storage <b>120</b> may store three-dimensional model <b>520</b> received by receiver <b>110</b>. Note that storage <b>120</b> may store processing results of respective processing units included in three-dimensional model generation device <b>100</b>. Storage <b>120</b> stores, for example, a control program executed by each processing unit included in three-dimensional model generation device <b>100</b>. Storage <b>120</b> is realized by, for example, an HDD (Hard Disk Drive), flash memory, etc.</p><p id="p-0082" num="0080">Obtainer <b>130</b> obtains, from storage <b>120</b>, a plurality of frames, and the camera parameters of each imaging device <b>301</b>, which are stored in storage <b>120</b>, and outputs them to generator <b>140</b>.</p><p id="p-0083" num="0081">Note that three-dimensional model generation device <b>100</b> need not include storage <b>120</b> and obtainer <b>130</b>. In this case, receiver <b>110</b> may output, to generator <b>140</b>, the frames received from imaging devices <b>301</b>, and the camera parameters of each imaging device <b>301</b> received from estimation device <b>200</b>.</p><p id="p-0084" num="0082">Generator <b>140</b> generates a three-dimensional model using a plurality of frames and a camera parameter. Generator <b>140</b> has searcher <b>141</b>, calculator <b>142</b>, and model generator <b>143</b>.</p><p id="p-0085" num="0083">Searcher <b>141</b> searches for a similar point that is similar to a first point in a first frame among the frames from a plurality of second points in a search area, which is based on the first point, in a second frame. Specifically, the first point is one first pixel among a plurality of first pixels forming the first frame. The search area is an area in second frames that is defined by an epipolar line corresponding to the first point in the first frame, and is an area formed by a plurality of second points on the epipolar line, for example. Specifically, the second points are second pixels included in the search area. Note that each of the first point and the second point may be or need not be a feature point.</p><p id="p-0086" num="0084">Searcher <b>141</b> may search for a similar point (similar pixel) that is similar to each of the first pixels forming the first frame from the second pixels in the search area in the second frame. Searcher <b>141</b> may search for the similar point for each of a plurality of first pixels or for one first pixel. Searcher <b>141</b> may search for the similar point for each of a plurality of second frames. Searcher <b>141</b> may search for the similar point for each of a plurality of second frames or for one second frame.</p><p id="p-0087" num="0085"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for describing search processing by searcher <b>141</b>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example in which imaging device <b>301</b> at first viewpoint V<b>1</b> shoots first frame <b>531</b> containing subject <b>510</b>, imaging device <b>301</b> at second viewpoint V<b>2</b> shoots second frame <b>532</b> containing subject <b>510</b>, and imaging device <b>301</b> at third viewpoint V<b>3</b> shoots second frame <b>533</b> containing subject <b>510</b>.</p><p id="p-0088" num="0086">For each first pixel, searcher <b>141</b> calculates an epipolar line that is a straight line connecting the position of imaging device <b>301</b> having shot the first frame and the two-dimensional coordinates of the first pixel in the first frame projected onto the second frame to be processed. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, searcher <b>141</b> calculates epipolar line <b>552</b> that is straight line L<b>1</b> connecting first viewpoint V<b>1</b> and first pixel <b>541</b> projected onto second frame <b>532</b>. Searcher <b>141</b> also calculates epipolar line <b>553</b> that is straight line L<b>1</b> projected onto second frame <b>533</b>. Searcher <b>141</b> then searches for a similar point that is similar to first pixel <b>541</b> to be processed in first frame <b>531</b> from epipolar lines <b>552</b> and <b>553</b> in second frames <b>532</b> and <b>533</b>.</p><p id="p-0089" num="0087">Calculator <b>142</b> calculates an accuracy of a search result using a degree of similarity between the first pixel and each of the second pixels in the search area. The accuracy of a search result corresponds to a certainty of a similar point (a second pixel similar to a first pixel) found in the search area, for example. The higher the accuracy of the search result, the higher the certainty of the similar point is. The certainty of the similar point is different from the degree of similarity of the similar point. For example, when there is a plurality of second pixels having high degrees of similarity in the search area, the certainties of the similar points are low even though the degrees of similarity are high.</p><p id="p-0090" num="0088">As the degree of similarity, calculator <b>142</b> calculates N(I, J), which indicates a normalized cross correlation (NCC) between small areas of the first frame and the second frame to be processed, according to Equation 1. N(I, J) is expressed by a numerical value from &#x2212;1 to 1, and indicates a higher degree of similarity as N(I, J) comes closer to 1.</p><p id="p-0091" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Math</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>1</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>N</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>I</mi>       <mo>,</mo>       <mi>J</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mfrac>      <mrow>       <mo>&#x2211;</mo>       <mrow>        <mrow>         <mo>(</mo>         <mrow>          <msub>           <mi>I</mi>           <mi>xy</mi>          </msub>          <mo>-</mo>          <mover>           <msub>            <mi>I</mi>            <mrow>             <mi>x</mi>             <mo>,</mo>             <mi>y</mi>            </mrow>           </msub>           <mo>_</mo>          </mover>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#xb7;</mo>        <mrow>         <mo>(</mo>         <mrow>          <msub>           <mi>J</mi>           <mi>st</mi>          </msub>          <mo>-</mo>          <mover>           <msub>            <mi>J</mi>            <mi>st</mi>           </msub>           <mo>_</mo>          </mover>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mrow>      <msqrt>       <mrow>        <msup>         <mrow>          <mo>(</mo>          <mrow>           <msub>            <mi>I</mi>            <mi>xy</mi>           </msub>           <mo>-</mo>           <mover>            <msub>             <mi>I</mi>             <mi>xy</mi>            </msub>            <mo>_</mo>           </mover>          </mrow>          <mo>)</mo>         </mrow>         <mn>2</mn>        </msup>        <mo>&#x2062;</mo>        <msup>         <mrow>          <mo>(</mo>          <mrow>           <msub>            <mi>J</mi>            <mi>st</mi>           </msub>           <mo>-</mo>           <mover>            <msub>             <mi>J</mi>             <mi>st</mi>            </msub>            <mo>_</mo>           </mover>          </mrow>          <mo>)</mo>         </mrow>         <mn>2</mn>        </msup>       </mrow>      </msqrt>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mn>1</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0092" num="0089">I<sub>xy </sub>and J<sub>xy </sub>indicate a pixel value in a small area in each frame. <o ostyle="single">I<sub>xy</sub></o> and <o ostyle="single">J<sub>xy</sub></o> indicate a pixel average in a small area in each frame.</p><p id="p-0093" num="0090">Calculator <b>142</b> calculates a plurality of degrees of similarity, in order to calculate the degree of similarity between the first pixel and each of the second pixels in the search area. Calculator <b>142</b> calculates a first sum, which is a sum of degrees of similarity calculated for one search area according to Equation 2. The first sum corresponds to a first accuracy, which is an example of the accuracy of the search result. The smaller the first sum, the higher the first accuracy is. Calculator <b>142</b> searches for a similar point that is similar to the first pixel, and calculates the first accuracy for each of the second frames to be searched. The first accuracy indicates the accuracy of a search of a plurality of second pixels in a search area in a second frame to be processed for a similar point that is similar to the first pixel.</p><p id="p-0094" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Math</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>2</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>Matching</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mi>Score</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mrow>       <mo>(</mo>       <mrow>        <mi>I</mi>        <mo>,</mo>        <mi>J</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>     <mo>=</mo>     <mrow>      <mover>       <munder>        <mo>&#x2211;</mo>        <mrow>         <mi>I</mi>         <mo>=</mo>         <mi>X</mi>        </mrow>       </munder>       <mi>S</mi>      </mover>      <mrow>       <mover>        <munder>         <mo>&#x2211;</mo>         <mrow>          <mtext> </mtext>          <mrow>           <mi>J</mi>           <mo>=</mo>           <mi>Y</mi>          </mrow>         </mrow>        </munder>        <mi>T</mi>       </mover>       <mrow>        <msub>         <mi>N</mi>         <mi>i</mi>        </msub>        <mo>(</mo>        <mrow>         <mi>I</mi>         <mo>,</mo>         <mi>J</mi>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mn>2</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0095" num="0091">Note that, in Equation 2, (I, J) indicates coordinates of a first pixel in a first frame. (X, Y) indicates coordinates of a starting point of an epipolar line in a second frame to be processed, and (S, T) indicates coordinates of an end point of the epipolar line in the second frame to be processed. i denotes a frame number for identifying a second frame to be referred to.</p><p id="p-0096" num="0092">Note that the first sum in Equation 2 may include only N(I, J) that is greater than first threshold Th<b>1</b>.</p><p id="p-0097" num="0093">Next, a specific example of calculation processing by calculator <b>142</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref> to <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0098" num="0094"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating a relationship between subject <b>510</b> and frames <b>561</b> to <b>563</b>. <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example where optical axes of imaging devices <b>301</b> having shot the subject are parallel to each other. Note that the example where the optical axes of imaging devices <b>301</b> are parallel to each other is illustrated for the convenience of explanation, and the present disclosure is not limited to the example where the optical axes of imaging devices <b>301</b> are parallel to each other.</p><p id="p-0099" num="0095"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating an example of a case where the first accuracy for first frame <b>561</b> is lower than a predetermined accuracy. That is, <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example of a case where the first sum calculated according to Equation 2 is greater than second threshold Th<b>2</b> (not shown). When the first sum is greater than second threshold Th<b>2</b>, calculator <b>142</b> may determine that the accuracy of the search for a second pixel as a similar point to first pixel <b>571</b> is lower than the predetermined accuracy. This will be specifically described below.</p><p id="p-0100" num="0096">Parts (a) to (c) of <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrate first frame <b>561</b>, second frame <b>562</b>, and second frame <b>563</b>, respectively. Part (d) of <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a graph illustrating a relationship between a plurality of second pixels on epipolar line <b>572</b> in second frame <b>562</b> to be searched for a pixel similar to first pixel <b>571</b> in first frame <b>561</b> and the degrees of similarity (matching scores) between first pixel <b>571</b> and the second pixels. The horizontal axis of the graph illustrated in part (d) of <figref idref="DRAWINGS">FIG. <b>5</b></figref> indicates the position of the second pixel on the epipolar line, and the vertical axis indicates the score (degree of similarity) of the second pixel. Epipolar line <b>572</b> is a straight line connecting the position of imaging device <b>301</b> having shot first frame <b>561</b> and first pixel <b>571</b> (or a point in the subject in the three-dimensional space corresponding to first pixel <b>571</b>) projected onto second frame <b>562</b>. Part (e) of <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a graph illustrating a relationship between a plurality of second pixels on epipolar line <b>573</b> in second frame <b>563</b> to be searched for a pixel similar to first pixel <b>571</b> in first frame <b>561</b> and the degrees of similarity (matching scores) between first pixel <b>571</b> and the second pixels. Epipolar line <b>573</b> is a straight line connecting the position of imaging device <b>301</b> having shot first frame <b>561</b> and first pixel <b>571</b> (or a point in the subject in the three-dimensional space corresponding to first pixel <b>571</b>) projected onto second frame <b>563</b>. Note that, in parts (d) and (e) of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the degree of similarity (matching score) is shown simply as score.</p><p id="p-0101" num="0097">As illustrated in parts (b) and (c) of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, there are many pixels (a predetermined number of pixels or more, for example) similar to first pixel <b>571</b> on epipolar lines <b>572</b> and <b>573</b> associated with first pixel <b>571</b>, and therefore, many second pixels (a predetermined number of second pixels or more, for example) having a matching score greater than first threshold Th<b>1</b> are included. For example, when epipolar lines <b>572</b> and <b>573</b> extend along an edge in second frames <b>562</b> and <b>563</b>, many second pixels having a matching score greater than first threshold Th<b>1</b> tend to be included in epipolar lines <b>572</b> and <b>573</b>. For example, when an edge indicated by a dashed line in <figref idref="DRAWINGS">FIG. <b>6</b></figref> extends along an epipolar line, many second pixels (a predetermined number of second pixels or more, for example) having a matching score greater than first threshold Th<b>1</b> tend to be included.</p><p id="p-0102" num="0098">Calculator <b>142</b> may determine a second pixel having a matching score greater than first threshold Th<b>1</b> to be a similar point that is similar to first pixel <b>571</b>.</p><p id="p-0103" num="0099">Although the horizontal axis of the graph illustrated in part (d) of <figref idref="DRAWINGS">FIG. <b>5</b></figref> has been described as indicating the position of the second pixel for the sake of explanation, the position of the second pixel is not essential for calculation of the first sum.</p><p id="p-0104" num="0100"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an example of a case where the first accuracy for first frame <b>561</b> is equal to or higher than the predetermined accuracy. That is, <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example of a case where the first sum calculated according to Equation 2 is equal to or smaller than second threshold Th<b>2</b>. When the first sum is equal to or smaller than second threshold Th<b>2</b>, calculator <b>142</b> may determine that the accuracy of the search for a second pixel as a similar point to first pixel <b>571</b> is equal to or higher than the predetermined accuracy. This will be specifically described below.</p><p id="p-0105" num="0101">Parts (a) to (c) of <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrate first frame <b>561</b>, second frame <b>562</b>, and second frame <b>563</b>, respectively. Part (d) of <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a graph illustrating a relationship between a plurality of second pixels on epipolar line <b>575</b> in second frame <b>562</b> to be searched for a pixel similar to first pixel <b>574</b> in first frame <b>561</b> and the degrees of similarity (matching scores) between first pixel <b>574</b> and the second pixels. Epipolar line <b>575</b> is a straight line connecting the position of imaging device <b>301</b> having shot first frame <b>561</b> and first pixel <b>574</b> (or a point in the subject in the three-dimensional space corresponding to first pixel <b>574</b>) projected onto second frame <b>562</b>. Part (e) of <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a graph illustrating a relationship between a plurality of second pixels on epipolar line <b>576</b> in second frame <b>563</b> to be searched for a pixel similar to first pixel <b>574</b> in first frame <b>561</b> and the degrees of similarity (matching scores) between first pixel <b>574</b> and the second pixels. Epipolar line <b>576</b> is a straight line connecting the position of imaging device <b>301</b> having shot first frame <b>561</b> and first pixel <b>574</b> (or a point in the subject in the three-dimensional space corresponding to first pixel <b>574</b>) projected onto second frame <b>563</b>. Note that, in parts (d) and (e) of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the degree of similarity (matching score) is shown simply as score. As illustrated in parts (b) and (c) of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, there are a little pixels (less than a predetermined number of pixels, for example) similar to first pixel <b>574</b> on epipolar lines <b>575</b> and <b>576</b> associated with first pixel <b>574</b>, and therefore, there are a little second pixels (less than a predetermined number of second pixels, for example) having a matching score greater than first threshold Th<b>1</b>. For example, when epipolar lines <b>575</b> and <b>576</b> intersect with an edge in second frames <b>562</b> and <b>563</b>, the number of second pixels having a matching score greater than first threshold Th<b>1</b> tends to be small. For example, when an edge indicated by a dashed line in <figref idref="DRAWINGS">FIG. <b>8</b></figref> intersects with an epipolar line, the number of second pixels having a matching score greater than first threshold Th<b>1</b> tend to be small (less than a predetermined number of second pixels, for example).</p><p id="p-0106" num="0102">As described above with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> and <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the greater the first sum, the greater the number of second pixels similar to the first pixel is, and therefore, the higher the possibility that a wrong second pixel is matched with the first pixel is. To the contrary, the smaller the first sum, the smaller the number of second pixels similar to the first pixel is, and therefore, the higher the possibility that an appropriate second pixel is matched with the first pixel is. Therefore, the first sum can be used as an index of accuracy: the greater the value of the first sum, the lower the accuracy of the search is.</p><p id="p-0107" num="0103">The first accuracy may be an inverse of the first sum or a predetermined fixed value minus the first sum. Even when the first sum is equal to or smaller than second threshold Th<b>2</b>, calculator <b>142</b> may determine the accuracy of the search is lower than the predetermined accuracy if there is no second pixel having a matching score greater than first threshold Th<b>1</b>. This is because there is no similar point that is similar to the first pixel on the epipolar line.</p><p id="p-0108" num="0104">Referring back to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, calculator <b>142</b> calculates the first sum for each of the second frames. Therefore, a plurality of first sums is obtained for one first pixel. Calculator <b>142</b> calculates a second sum, which is a sum of the first sums obtained for one first pixel, according to Equation <b>3</b>. The second sum corresponds to a second accuracy, which is an example of the accuracy of the search result. The second accuracy indicates the accuracy of a search of a plurality of second frames for similar points that are similar to a plurality of first pixels to be processed.</p><p id="p-0109" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Math</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>3</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>Total</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mi>Matching</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mrow>       <mi>Score</mi>       <mo>&#x2062;</mo>       <mrow>        <mtext> </mtext>        <mtext>  </mtext>       </mrow>       <mo>(</mo>       <mrow>        <mi>I</mi>        <mo>,</mo>        <mi>J</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>     <mo>=</mo>     <mrow>      <mover>       <munder>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>0</mn>        </mrow>       </munder>       <mi>n</mi>      </mover>      <mrow>       <mover>        <munder>         <mo>&#x2211;</mo>         <mrow>          <mi>I</mi>          <mo>=</mo>          <mi>X</mi>         </mrow>        </munder>        <mi>S</mi>       </mover>       <mrow>        <mover>         <munder>          <mo>&#x2211;</mo>          <mrow>           <mtext> </mtext>           <mrow>            <mi>J</mi>            <mo>=</mo>            <mi>Y</mi>            <mtext> </mtext>           </mrow>          </mrow>         </munder>         <mi>T</mi>        </mover>        <mrow>         <msub>          <mi>N</mi>          <mrow>           <mi>i</mi>           <mtext> </mtext>          </mrow>         </msub>         <mo>(</mo>         <mrow>          <mi>I</mi>          <mo>,</mo>          <mi>J</mi>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mn>3</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0110" num="0105">n denotes the number of a plurality of second frames to be referred to.</p><p id="p-0111" num="0106">Calculator <b>142</b> calculates a plurality of first sums for each of the first pixels in the first frame. Calculator <b>142</b> then calculates a second sum from the first sums. The second sum can be used as an index of the second accuracy: the greater the value of the second sum, the lower the second accuracy is.</p><p id="p-0112" num="0107">The second accuracy may be an inverse of the second sum or a predetermined fixed value minus the second sum.</p><p id="p-0113" num="0108">When the first sum obtained according to Equation 2 is 0, the first sum need not be included in the calculation of the second sum according to Equation 3.</p><p id="p-0114" num="0109">Model generator <b>143</b> generates a three-dimensional model using the search result from searcher <b>141</b> and the accuracy calculated by calculator <b>142</b>. Specifically, model generator <b>143</b> generates a three-dimensional model using a plurality of first search results, which are results of searches performed for a plurality of second frames, and a plurality of first accuracies. For each of a plurality of first pixels, model generator <b>143</b> generates a three-dimensional model including a plurality of three-dimensional points that is obtained by generating three-dimensional points using a plurality of first search results and a plurality of first accuracies.</p><p id="p-0115" num="0110"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram for describing a first example of three-dimensional model generation processing.</p><p id="p-0116" num="0111">For example, model generator <b>143</b> may generate a three-dimensional model without using a first search result for a second frame for which the first sum indicating the first accuracy calculated is greater than second threshold Th<b>2</b>, that is, the first accuracy is lower than the predetermined accuracy. As illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, model generator <b>143</b> generates three-dimensional point <b>522</b> using a first search result for second frame <b>533</b> having accuracy A<b>2</b> (a first sum calculated for second frame <b>533</b>) equal to or smaller than second threshold Th<b>2</b>, without using a first search result for second frame <b>532</b> having accuracy A<b>1</b> (a first sum calculated for second frame <b>532</b>) greater than second threshold Th<b>2</b>. In this way, model generator <b>143</b> calculates, based on the principle of triangulation, three-dimensional point <b>522</b> indicated by three-dimensional coordinates on subject <b>510</b> using sets of the first pixel and a different one of a plurality of second pixels selected as similar points based on first search results excluding any first search result for which the accuracy is determined to be low. For example, as illustrated in FIG. <b>9</b>, model generator <b>143</b> generates, as three-dimensional point <b>522</b>, an intersection point between straight line L<b>1</b> connecting first viewpoint V<b>1</b> and first pixel <b>541</b> and straight line L<b>3</b> connecting third viewpoint V<b>3</b> and second pixel <b>543</b>.</p><p id="p-0117" num="0112">Alternatively, for example, model generator <b>143</b> may selects N second frames from the second frames in descending order of the calculated first accuracy (N denotes an integer equal to or greater than 1), and generate a three-dimensional model using N first search results corresponding to the selected N second frames. Model generator <b>143</b> calculates, based on the principle of triangulation, a three-dimensional point indicated by three-dimensional coordinates on subject <b>510</b> using sets of the first pixel and a different one of a plurality of second pixels selected as similar points using N first search results selected in descending order of accuracy.</p><p id="p-0118" num="0113">When a plurality of three-dimensional points is calculated for one first pixel, model generator <b>143</b> may generate one three-dimensional point corresponding to the one first pixel by calculating an average of the three-dimensional points. Second threshold Th<b>2</b> is an example of the predetermined accuracy. In this way, since any first search result having an accuracy greater than second threshold Th<b>2</b> is not used for the three-dimensional model generation processing, a three-dimensional point can be generated with higher accuracy.</p><p id="p-0119" num="0114">In these two examples in the first example, N first search results having higher accuracies are preferentially used for the three-dimensional model generation processing, and therefore, a three-dimensional point can be generated with higher accuracy.</p><p id="p-0120" num="0115"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram for describing a second example of the three-dimensional model generation processing.</p><p id="p-0121" num="0116">For example, based on a plurality of first search results, model generator <b>143</b> generates a plurality of three-dimensional points each corresponding to a different one of the first search results. Specifically, model generator <b>143</b> determines a set of a first pixel and a second pixel similar to the first pixel for each of a plurality of second frames. Model generator <b>143</b> generates a plurality of three-dimensional points by generating one three-dimensional point from one set. Model generator <b>143</b> may generate an integrated three-dimensional point by weighted averaging of the three-dimensional points in which a greater weight is given to a three-dimensional point having a first search result with a higher first accuracy, and generate a three-dimensional model including the integrated three-dimensional point generated. While first search results with higher accuracies are preferentially used in the first example described above, in the second example, as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, three-dimensional points <b>521</b> and <b>522</b> indicated by three-dimensional coordinates on subject <b>510</b> are calculated based on the principle of triangulation using sets of first pixel <b>541</b> and a different one of second pixels <b>542</b> and <b>543</b> selected as similar points based on all the first search results. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, model generator <b>143</b> generates, as three-dimensional point <b>521</b>, an intersection point between straight line L<b>1</b> connecting first viewpoint V<b>1</b> and first pixel <b>541</b> and straight line L<b>2</b> connecting second viewpoint V<b>2</b> and second pixel <b>542</b>, and generates, as three-dimensional point <b>522</b>, an intersection point between straight line L<b>1</b> and straight line L<b>3</b> connecting third viewpoint V<b>3</b> and second pixel <b>543</b>. In the generation of the three-dimensional model, an integrated three-dimensional point is generated by weighted averaging of three-dimensional points <b>521</b> and <b>522</b> each generated using a plurality of first search results in which a greater weight is given to a three-dimensional point having a higher first accuracy. Therefore, a plurality of three-dimensional points can be integrated in such a manner that many three-dimensional points having high accuracies are included as constituents, and three-dimensional points can be generated with higher accuracy.</p><p id="p-0122" num="0117">Model generator <b>143</b> may generate a three-dimensional model including a plurality of three-dimensional points and a plurality of second accuracies each corresponding to a different one of the three-dimensional points. Therefore, each of the three-dimensional points included in the three-dimensional model can be associated with a second accuracy that is based on a plurality of first accuracies of a plurality of search results used when generating the three-dimensional point.</p><p id="p-0123" num="0118">Therefore, model generator <b>143</b> can filters (smooth) the three-dimensional model using the second accuracy as a weight, for example. Specifically, model generator <b>143</b> may correct at least one low-accuracy three-dimensional point located between two high-accuracy three-dimensional points with respect to the two high-accuracy three-dimensional points, the at least one low-accuracy three-dimensional point being associated with a second accuracy equal to or lower than a predetermined accuracy, and the two high-accuracy three-dimensional points each being associated with a second accuracy higher than the predetermined accuracy. In that case, low-accuracy three-dimensional points can be corrected with respect to high-accuracy three-dimensional points by permitting a greater displacement for correction for a three-dimensional point with a lower accuracy, for example, and therefore, the accuracy of generation of the three-dimensional model can be improved.</p><heading id="h-0016" level="2">[Operation of Three-Dimensional Model Generation Device]</heading><p id="p-0124" num="0119">Next, an operation of three-dimensional model generation device <b>100</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref>. <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating an example of the operation of three-dimensional model generation device <b>100</b>.</p><p id="p-0125" num="0120">In three-dimensional model generation device <b>100</b>, first, receiver <b>110</b> receives, from estimation device <b>200</b>, a plurality of frames shot by a plurality of imaging devices <b>301</b> and a camera parameter of each imaging device <b>301</b> (S<b>101</b>). Step S<b>101</b> is an example of a step of obtaining a plurality of images. Receiver <b>110</b> need not receive the frames and the camera parameters at one timing, and may receive the frames and the camera parameters at different timings. That is, the obtaining of the frames and the obtaining of the camera parameters may be performed at the same timing or at different timings.</p><p id="p-0126" num="0121">Storage <b>120</b> then stores the frames shot by imaging devices <b>301</b> and the camera parameter of each imaging device <b>301</b> received by receiver <b>110</b> (S<b>102</b>).</p><p id="p-0127" num="0122">Obtainer <b>130</b> then obtains the frames and the camera parameters stored in storage <b>120</b>, and outputs the frames and camera parameters obtained to generator <b>140</b> (S<b>103</b>).</p><p id="p-0128" num="0123">Generator <b>140</b> then generates a three-dimensional model using the frames and the camera parameters (S<b>104</b>). Details of step S<b>104</b> of generating a three-dimensional model will be described later with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0129" num="0124">Outputter <b>150</b> then outputs the three-dimensional model generated by generator <b>140</b> (S<b>105</b>).</p><p id="p-0130" num="0125"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart illustrating an example of details of the generation processing in step S<b>104</b> by generator <b>140</b>.</p><p id="p-0131" num="0126">Generator <b>140</b> performs loop <b>1</b> for each set of frames of a multi-view image shot at a corresponding timing (S<b>111</b>). In loop <b>1</b>, loop <b>2</b> is performed for each set of frames.</p><p id="p-0132" num="0127">Generator <b>140</b> performs loop <b>2</b> for each first pixel in a first frame of a set of frames to be processed (S<b>112</b>). In loop <b>2</b>, a process from step S<b>113</b> to step S<b>115</b> is performed for each first pixel.</p><p id="p-0133" num="0128">For a first pixel to be processed, searcher <b>141</b> searches for a similar point that is similar to the first pixel from a plurality of second pixels on an epipolar line associated with the first pixel in a plurality of second frames of the set of frames to be processed (S<b>113</b>). Details of step S<b>113</b> has already been described in the description of searcher <b>141</b>, and therefore descriptions thereof will be omitted.</p><p id="p-0134" num="0129">Calculator <b>142</b> calculates a first accuracy of the search for a similar point that is similar to the first pixel to be processed, for each of the second frames (S<b>114</b>). Details of step S<b>114</b> has already been described in the description of calculator <b>142</b>, and therefore descriptions thereof will be omitted.</p><p id="p-0135" num="0130">Model generator <b>143</b> generates a three-dimensional model using the first search results obtained in step S<b>113</b> and the first accuracies obtained in step S<b>114</b> (S<b>115</b>). Details of step S<b>115</b> has already been described in the description of model generator <b>143</b>, and therefore descriptions thereof will be omitted.</p><p id="p-0136" num="0131">Loop <b>2</b> ends when the process from step S<b>113</b> to step S<b>115</b> ends for all the first pixels included in the first frame of the set of frames to be processed.</p><p id="p-0137" num="0132">Loop <b>1</b> ends when loop <b>2</b> ends for all the sets of frames.</p><heading id="h-0017" level="2">(Advantageous Effects, etc.)</heading><p id="p-0138" num="0133">A three-dimensional model generation method according to the present embodiment is a three-dimensional model generation method executed by an information processing device, and includes: obtaining frames generated by shooting a subject from a plurality of viewpoints (S<b>101</b>); searching for a similar point that is similar to a first pixel in a first frame among the frames, from second pixels in a search area in a second frame different from the first frame, the search area being based on the first pixel (S<b>113</b>); calculating an accuracy of a search result, using degrees of similarity each between the first pixel and a different one of the second pixels (S<b>114</b>); and generating a three-dimensional model using the search result and the accuracy (S<b>115</b>).</p><p id="p-0139" num="0134">According to this method, since the three-dimensional model is generated using search results and accuracies of the search results, the accuracy of generation of the three-dimensional model can be improved by avoiding using search results with lower accuracies or by preferentially using search results with higher accuracies for generating the three-dimensional model, for example.</p><p id="p-0140" num="0135">Furthermore, in the three-dimensional model generation method according to the present embodiment, in the searching (S<b>113</b>), the similar point is searched for in each of second frames each of which is the second frame. In the calculating of the accuracy (S<b>114</b>), a first accuracy is calculated for each of first search results corresponding to a different one of the second frames. In the generating of the three-dimensional model of the subject (S<b>115</b>), the three-dimensional model is generated using the first search results and the first accuracies.</p><p id="p-0141" num="0136">According to this method, since the three-dimensional model is generated using a plurality of first search results and a plurality of first accuracies, three-dimensional points with higher accuracies can be generated by avoiding using first search results with lower accuracies or by preferentially using first search results with higher accuracies for generating the three-dimensional model, for example. Therefore, the accuracy of generation of the three-dimensional model can be improved.</p><p id="p-0142" num="0137">Furthermore, in the three-dimensional model generation method according to the present embodiment, in the generating of the three-dimensional model (S<b>115</b>), the three-dimensional model may be generated without using the first search result for a second frame for which the accuracy calculated is lower than a predetermined accuracy. According to this method, since a first search result with an accuracy lower than the predetermined threshold is not used for generating the three-dimensional model, three-dimensional points with higher accuracies can be generated.</p><p id="p-0143" num="0138">Furthermore, in the three-dimensional model generation method according to the present embodiment, the generating of the three-dimensional model (S<b>115</b>) may include: selecting, from among the second frames, N second frames in a descending order of the first accuracy calculated (where N is an integer greater than or equal to 1); and generating the three-dimensional model using N first search results corresponding to the N second frames selected. According to this method, since N first search results with higher accuracies are preferentially used for generating the three-dimensional model, three-dimensional points with higher accuracies can be generated.</p><p id="p-0144" num="0139">Furthermore, in the three-dimensional model generation method according to the present embodiment, the generating of the three-dimensional model (S<b>115</b>) includes: generating, based on the first search results, three-dimensional points each of which corresponds to a different one of the first search results; generating an integrated three-dimensional point by weighted averaging of the three-dimensional points in which a greater weight is given to a three-dimensional point having a corresponding first search result with a higher first accuracy; and generating the three-dimensional model including the integrated three-dimensional point generated. According to this method, since an integrated three-dimensional point is generated by weighted averaging of three-dimensional points generated using the first search results in which a greater weight is given to a three-dimensional point with a higher accuracy, three-dimensional points with higher accuracies can be generated.</p><p id="p-0145" num="0140">Furthermore, in the three-dimensional model generation method according to the present embodiment, the searching (S<b>113</b>) is performed for each of first pixels each of which is the first pixel, in the calculating of the accuracy (S<b>114</b>), the first accuracies are calculated for each of the first pixels, and in the generating of the three-dimensional model (S<b>115</b>), a three-dimensional model including three-dimensional points obtained by generating, for each of the first pixels, a three-dimensional point using the first search results and the first accuracies is generated as the three-dimensional model. Therefore, a three-dimensional point with a higher accuracy can be generated for each of the first pixels. Therefore, the accuracy of generation of the three-dimensional model can be improved.</p><p id="p-0146" num="0141">Furthermore, in the three-dimensional model generation method according to the present embodiment, the calculating of the accuracy (S<b>114</b>) includes calculating, for each of the first pixels, a sum of values (i.e., matching scores) indicating the first accuracies calculated for the first pixel, as a second accuracy of a search result based on the first pixel. In the generating of the three-dimensional model (S<b>115</b>), a three-dimensional model including the three-dimensional points and the second accuracies is generated as the three-dimensional model. Therefore, each of the three-dimensional points included in the three-dimensional model can be associated with a second accuracy that is based on the first accuracies of the search results used when generating the three-dimensional point.</p><p id="p-0147" num="0142">Furthermore, in the three-dimensional model generation method according to the present embodiment, the generating of the three-dimensional model (S<b>115</b>) includes: correcting at least one low-accuracy three-dimensional point located between two high-accuracy three-dimensional points using the two high-accuracy three-dimensional points as reference, the at least one low-accuracy three-dimensional point being associated with a second accuracy lower than a predetermined accuracy, the two high-accuracy three-dimensional points each being associated with a second accuracy higher than the predetermined accuracy. According to this method, low-accuracy three-dimensional points can be corrected with respect to high-accuracy three-dimensional points by permitting a greater displacement for correction for a three-dimensional point with a lower accuracy, for example, and therefore, the accuracy of generation of the three-dimensional model can be improved.</p><p id="p-0148" num="0143">Furthermore, in the three-dimensional model generation method according to the present embodiment, the search area is an area defined by second pixels on an epipolar line corresponding to the first pixel. Therefore, a candidate for a pixel similar to the first pixel can be efficiently selected from among the second pixels.</p><heading id="h-0018" level="2">(Variation 1)</heading><p id="p-0149" num="0144">In the three-dimensional model generation method according to the embodiment described above, a three-dimensional model of a subject is generated by calculating an accuracy of a search result, the calculation of an accuracy of a search result may be omitted.</p><p id="p-0150" num="0145">For example, a three-dimensional model generation method according to Variation 1 is performed by an information processing device. According to the three-dimensional model generation method according to Variation 1, a first image obtained by shooting a subject from a first viewpoint and a second image obtained by shooting the subject from a second viewpoint are obtained, a search area in the second image is determined based on a first point in the first image in order to search for a similar point that is similar to the first point, the similar point is searched for in the search area by calculating a degree of similarity indicating the degree of similarity between the first point and each of a plurality of second points in the search area, and a three-dimensional model of the subject is generated based on a result of the search and a variation of the degrees of similarity calculated.</p><p id="p-0151" num="0146">For example, the degrees of similarity in the graph illustrated in part (d) of <figref idref="DRAWINGS">FIG. <b>7</b></figref> is more locally distributed than the degrees of similarity in the graph illustrated in part (d) of <figref idref="DRAWINGS">FIG. <b>5</b></figref>. In other words, the degrees of similarity in the graph illustrated in part (d) of <figref idref="DRAWINGS">FIG. <b>7</b></figref> has a greater variation and a greater standard deviation than the degrees of similarity in the graph illustrated in part (d) of <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Calculator <b>142</b> calculates a standard deviation of the degrees of similarity, for example. When the calculated standard deviation is equal to or greater than a predetermined threshold, and the highest degree of similarity is equal to or higher than a predetermined threshold, model generator <b>143</b> uses the second pixel (similar point) having the highest degree of similarity for generating the three-dimensional model. When the calculated standard deviation is smaller than the predetermined threshold, model generator <b>143</b> does not use the second pixel having the highest degree of similarity for generating the three-dimensional model. Note that the index of a variation of degrees of similarity is not limited to the standard deviation.</p><p id="p-0152" num="0000">(Other variations)</p><p id="p-0153" num="0147">Although NCC is calculated as the degree of similarity (matching score) in the three-dimensional model generation methods according to the embodiment described above and Variation 1, the present disclosure is not limited to this. For example, as the degree of similarity, SSD(I, J) indicating a sum of squared differences (SSD) between pixel values of small areas of a first frame and a second frame to be processed may be calculated according to Equation 4. The smaller the value of SSD(I, J), the higher degree of similarity SSD(I, J) indicates.</p><p id="p-0154" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Math. 4]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0155" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>SSD(I,J)=&#x3a3;(I<sub>xy</sub>&#x2212;J<sub>st</sub>)<sup>2 </sup>&#x2003;&#x2003;(Equation 4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0156" num="0148">I_<sub>xy </sub>and J_<sub>st </sub>indicate a pixel value in a small area in each frame.</p><p id="p-0157" num="0149">Although the search area is an area in a plurality of second frames that is defined by an epipolar line corresponding to a first point (fist pixel) in a first frame in the three-dimensional model generation methods according to the embodiment described above and Variation 1, the present disclosure is not limited to this. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, searcher <b>141</b> determines third pixels <b>582</b> and <b>583</b> in second frames <b>532</b> and <b>533</b> corresponding to first pixel <b>581</b> in first frame <b>531</b> based on a relationship between the position and posture of imaging device <b>301</b> having shot each of second frames <b>532</b> and <b>533</b> and known three-dimensional model <b>513</b> (a three-dimensional point cloud) obtained in advance. Searcher <b>141</b> then may determine, as search areas <b>592</b> and <b>593</b>, areas including third pixels <b>582</b> and <b>583</b> with respect to third pixels <b>582</b> and <b>583</b>. For example, searcher <b>141</b> may determine rectangular areas centered on third pixels <b>582</b> and <b>583</b>, respectively, as search areas <b>592</b> and <b>593</b>. The search area is not limited to a rectangular area and may be an area having a square, circular, or other particular shape. Note that known three-dimensional model <b>513</b> obtained in advance may be three-dimensional model <b>520</b> generated by estimation device <b>200</b> described above with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0158" num="0150">Although the three-dimensional model generation method, etc., according to the present disclosure has been described based on the embodiments described above, the present disclosure is not limited to the foregoing embodiments.</p><p id="p-0159" num="0151">For example, in the foregoing embodiments and variations, each of the processing units included in the three-dimensional model generation device is described as being implemented by a CPU and a control program. For example, each of the structural components of these processing units may be configured of one or more electronic circuits. Each of the one or more electronic circuits may be a general-purpose circuit or a dedicated circuit. The one or more electronic circuits may include, for example, a semiconductor device, an integrated circuit (IC), or a large-scale integration (LSI), etc. The IC or LSI may be integrated in a single chip or several chips. Although referred to here as IC or LSI, the name may change depending on the scale of integration, and may be referred to as a system LSI, very large scale integration (VLSI), or ultra large scale integration (ULSI). Furthermore, a field programmable gate array (FPGA) that can be programmed after manufacturing of the LSI may be used for the same purpose.</p><p id="p-0160" num="0152">Furthermore, general or specific aspects of the present disclosure may be implemented as a system, an apparatus, a method, an integrated circuit, or a computer program. Alternatively, the general or specific aspects of the present disclosure may be implemented as a non-transitory computer-readable recording medium, such as an optical disc, a hard disk drive (HDD), or a semiconductor memory, on which the computer program is recorded.</p><p id="p-0161" num="0153">Furthermore, the general or specific aspects of the present disclosure may be implemented as any combination of a system, an apparatus, a method, an integrated circuit, a computer program, and a recording medium.</p><p id="p-0162" num="0154">The present disclosure also includes forms obtained by making various modifications to the above embodiments that can be conceived by those skilled in the art, as well as forms realized by combining structural components and functions in the embodiments, without departing from the essence of the present disclosure.</p><heading id="h-0019" level="1">INDUSTRIAL APPLICABILITY</heading><p id="p-0163" num="0155">The present disclosure can be applied to a three-dimensional model generation device or a three-dimensional model generation system, and can be applied to, for example, figure creation, topography or building structure recognition, human activity recognition, free-viewpoint video generation, or the like.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005216A1-20230105-M00001.NB"><img id="EMI-M00001" he="12.02mm" wi="76.20mm" file="US20230005216A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005216A1-20230105-M00002.NB"><img id="EMI-M00002" he="10.92mm" wi="76.20mm" file="US20230005216A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005216A1-20230105-M00003.NB"><img id="EMI-M00003" he="10.92mm" wi="76.20mm" file="US20230005216A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A three-dimensional model generation method executed by an information processing device, the three-dimensional model generation method comprising:<claim-text>obtaining images generated by shooting a subject from respective viewpoints;</claim-text><claim-text>searching for a similar point that is similar to a first point in a first image among the images, from second points in a search area in a second image different from the first image, the search area being provided based on the first point;</claim-text><claim-text>calculating an accuracy of a search result of the searching, using degrees of similarity between the first point and the respective second points; and</claim-text><claim-text>generating a three-dimensional model of the subject using the search result and the accuracy.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The three-dimensional model generation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>in the calculating of the accuracy, the accuracy of the search result is calculated based on a sum of degrees of similarity obtained by calculating the degrees of similarity between the first point and the respective second points.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The three-dimensional model generation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>in the searching, the similar point is searched for in each of second images, each of the second images being the second image,</claim-text><claim-text>in the calculating of the accuracy, a first accuracy is calculated for each of first search results corresponding to a different one of the second images, and</claim-text><claim-text>in the generating of the three-dimensional model, the three-dimensional model is generated using the first search results and the first accuracies.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The three-dimensional model generation method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>in the generating of the three-dimensional model, the three-dimensional model is generated without using the first search result for a second image for which the first accuracy calculated is lower than a predetermined accuracy.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The three-dimensional model generation method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the generating of the three-dimensional model includes:<claim-text>selecting, from among the second images, N second images in a descending order of the first accuracy calculated (where N is an integer greater than or equal to <b>1</b>); and</claim-text><claim-text>generating the three-dimensional model using N first search results corresponding to the N second images selected.</claim-text></claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The three-dimensional model generation method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the generating of the three-dimensional model includes:<claim-text>generating, based on the first search results, three-dimensional points each of which corresponds to a different one of the first search results;</claim-text><claim-text>generating an integrated three-dimensional point by weighted averaging of the three-dimensional points in which a greater weight is given to a three-dimensional point having a corresponding first search result with a higher first accuracy; and</claim-text><claim-text>generating the three-dimensional model including the integrated three-dimensional point generated.</claim-text></claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The three-dimensional model generation method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the searching is performed for each of first points, each of the first points being the first point,</claim-text><claim-text>in the calculating of the accuracy, the first accuracies are calculated for each of the first points, and</claim-text><claim-text>in the generating of the three-dimensional model, a three-dimensional model including three-dimensional points obtained by generating, for each of the first points, a three-dimensional point using the first search results and the first accuracies is generated as the three-dimensional model.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The three-dimensional model generation method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the calculating of the accuracy includes calculating, for each of the first points, a sum of values indicating the first accuracies calculated for the first point, as a second accuracy of a search result based on the first point, and</claim-text><claim-text>in the generating of the three-dimensional model, a three-dimensional model including the three-dimensional points and the second accuracies corresponding to the three-dimensional points is generated as the three-dimensional model.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The three-dimensional model generation method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein<claim-text>the generating of the three-dimensional model includes:<claim-text>correcting at least one low-accuracy three-dimensional point located between two high-accuracy three-dimensional points using the two high-accuracy three-dimensional points as reference, the at least one low-accuracy three-dimensional point being associated with a second accuracy lower than a predetermined accuracy, the two high-accuracy three-dimensional points each being associated with a second accuracy higher than the predetermined accuracy.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The three-dimensional model generation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the search area is an area defined by pixels on an epipolar line corresponding to the first point.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A three-dimensional model generation device comprising:<claim-text>memory; and</claim-text><claim-text>a processor, wherein</claim-text><claim-text>using the memory, the processor:<claim-text>obtains images generated by shooting a subject from respective viewpoints;</claim-text><claim-text>searches for a similar point that is similar to a first point in a first image among the images, in a search area in a second image different from the first image, the search area being provided based on the first point;</claim-text><claim-text>calculates an accuracy of a three-dimensional point generated based on the first point, using degrees of similarity between the first point and respective second points in the search area; and</claim-text><claim-text>generates a three-dimensional model of the subject using a result of the search and the accuracy.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A three-dimensional model generation method executed by an information processing device, the three-dimensional model generation method comprising:<claim-text>obtaining a first image and a second image, the first image being generated by shooting a subject from a first viewpoint, the second image being generated by shooting the subject from a second viewpoint;</claim-text><claim-text>specifying a search area in the second image for searching for a similar point that is similar to a first point in the first image, based on the first point;</claim-text><claim-text>searching for the similar point in the search area by calculating degrees of similarity each indicating a degree of similarity between the respective second points and the first point; and</claim-text><claim-text>generating a three-dimensional model of the subject based on a result of the searching and a variation in the degrees of similarity calculated.</claim-text></claim-text></claim></claims></us-patent-application>