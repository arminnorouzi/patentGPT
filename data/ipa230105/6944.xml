<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230006945A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230006945</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17867646</doc-number><date>20220718</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>49</main-group><subgroup>506</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>49</main-group><subgroup>90</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>49</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>69</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>49</main-group><subgroup>103</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>45</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>49</main-group><subgroup>101</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>49</main-group><subgroup>506</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>49</main-group><subgroup>9021</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>49</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>69</main-group><subgroup>161</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>49</main-group><subgroup>103</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>45</main-group><subgroup>66</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>49</main-group><subgroup>101</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Network Interface Device</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16870814</doc-number><date>20200508</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11394664</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17867646</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16226453</doc-number><date>20181219</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10686731</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16870814</doc-number></document-id></child-doc></relation></continuation><continuation-in-part><relation><parent-doc><document-id><country>US</country><doc-number>15847778</doc-number><date>20171219</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10686872</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16226453</doc-number></document-id></child-doc></relation></continuation-in-part></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Xilinx, Inc.</orgname><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Pope</last-name><first-name>Steven L.</first-name><address><city>Cambridge</city><country>GB</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Roberts</last-name><first-name>Derek</first-name><address><city>Cambridge</city><country>GB</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Riddoch</last-name><first-name>David J.</first-name><address><city>Huntingdon</city><country>GB</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Kitariev</last-name><first-name>Dmitri</first-name><address><city>Newport Beach</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Xilinx, Inc.</orgname><role>02</role><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Roughly described: a network interface device has an interface. The interface is coupled to first network interface device circuitry, host interface circuitry and host offload circuitry. The host interface circuitry is configured to interface to a host device and has a scheduler configured to schedule providing and/or receiving of data to/from the host device. The interface is configured to allow at least one of: data to be provided to said host interface circuitry from at least one of said first network device interface circuitry and said host offload circuitry; and data to be provided from said host interface circuitry to at least one of said first network interface device circuitry and said host offload circuitry.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="100.16mm" wi="158.75mm" file="US20230006945A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="207.18mm" wi="142.92mm" orientation="landscape" file="US20230006945A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="240.79mm" wi="146.64mm" orientation="landscape" file="US20230006945A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="240.79mm" wi="146.64mm" orientation="landscape" file="US20230006945A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="242.99mm" wi="146.64mm" orientation="landscape" file="US20230006945A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="242.99mm" wi="146.64mm" orientation="landscape" file="US20230006945A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="251.46mm" wi="143.09mm" orientation="landscape" file="US20230006945A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="240.79mm" wi="146.64mm" orientation="landscape" file="US20230006945A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="215.31mm" wi="114.05mm" orientation="landscape" file="US20230006945A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="191.26mm" wi="119.89mm" orientation="landscape" file="US20230006945A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="225.81mm" wi="129.88mm" orientation="landscape" file="US20230006945A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="198.29mm" wi="128.61mm" orientation="landscape" file="US20230006945A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="225.81mm" wi="129.88mm" orientation="landscape" file="US20230006945A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO OTHER APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. application Ser. No. 16/870,814, entitled &#x201c;NETWORK INTERFACE DEVICE&#x201d; by Steven L. Pope, Derek Roberts, David J. Riddoch and Dmitri Kitariev, filed 8 May 2020, (Atty. Docket No. LVLS 2054-3), which is a continuation of U.S. application Ser. No. 16/226,453, entitled &#x201c;NETWORK INTERFACE DEVICE&#x201d; by Steven L. Pope, Derek Roberts, David J. Riddoch and Dmitri Kitariev, filed 19 Dec. 2018, (Atty. Docket No. LVLS 2054-2), which is a continuation-in-part of U.S. application Ser. No. 15/847,778, entitled &#x201c;NETWORK INTERFACE DEVICE&#x201d; by Steven L. Pope, Derek Roberts, and David J. Riddoch, filed 19 Dec. 2017, (Atty. Docket No. LVLS 2054-1), which applications are incorporated herein by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Field</heading><p id="p-0003" num="0002">This application relates to network interface devices having a field programmable gate array application.</p><heading id="h-0004" level="1">Background</heading><p id="p-0004" num="0003">Network interface devices are known and are typically used to provide an interface between a computing device and a network. The network interface device can be configured to process data which is received from the network and/or process data which is to be put on the network.</p><p id="p-0005" num="0004">For some network interface devices, there is a drive to provide increased specializations of designs towards specific applications.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0006" num="0005">According to a first aspect, there is provided a network interface device arranged to couple a host computing device to a network, said network interface device comprising: a first interface configured to receive data from the host computing device; a first field programmable gate array application; and a transport engine for processing data, wherein the first interface is configured to: direct at least some of the data received from the host to the first field programmable gate array application; and direct at least some of the data received from the host to the transport engine, wherein the first field programmable gate array application is configured to perform processing with respect to the at least some of the received data.</p><p id="p-0007" num="0006">In one embodiment, the first field programmable gate array application is configured to pass the processed at least some of the data to the transport engine, wherein the transport engine is configured to perform processing of the at least some of the data received from the first field programmable gate array application and cause the processed data to be transmitted over the network.</p><p id="p-0008" num="0007">In one embodiment, the network interface device comprises a second interface between the transport engine and the first field programmable gate array application, wherein the second interface is configured to receive at least one of: protocol processed data packets from the transport engine and data packets from the network, wherein the second interface is configured to pass the received data packets to the first field programmable gate array application for processing.</p><p id="p-0009" num="0008">In one embodiment, in response to processing the received data packets, the first field programmable gate array application is configured to provide processed data packets to at least one of: the transport engine for protocol processing; and the second interface for transmission over the network.</p><p id="p-0010" num="0009">In one embodiment, the network interface device comprises a second field programmable gate array application configured to at least one of: process data received from at least one of: the first interface and the first programmable gate array application; and provide data processed by the second field programmable gate array application to at least one of the: the first interface and the first programmable gate array application.</p><p id="p-0011" num="0010">In one embodiment, the processing of the data received at the second field programmable gate array application comprises writing or reading from a data structure maintained by the second field programmable gate array application.</p><p id="p-0012" num="0011">In one embodiment, the providing data processed by the second field programmable gate array application comprises providing data read from a data structure maintained by the second field programmable gate array application to at least one of: the first interface and the first programmable gate array application.</p><p id="p-0013" num="0012">In one embodiment, in response to receiving a read request from the first interface or the first programmable gate array application, updating the data structure.</p><p id="p-0014" num="0013">In one embodiment, the first interface is configured to direct data packets of a first data flow to the field programmable gate array application and to direct data packets of a second data flow to the transport engine.</p><p id="p-0015" num="0014">In one embodiment, the transport engine is provided by application specific integrated circuit gates within a field programmable gate array also providing said first field programmable gate array application.</p><p id="p-0016" num="0015">In one embodiment, said FPGA is configured to perform one or more of the following: reduce data in one or more flows; aggregate data in one or more flows; perform hybrid processing; detect a DDOS attack; machine learning; perform a software defined networking function; provide an Open Computing Language kernel offload; provide an offload function; and take remedial action against a distributed denial of service attack.</p><p id="p-0017" num="0016">According to a second aspect, there is provided a network interface device arranged to couple a host computing device to a network, said network interface device comprising: a transport engine for performing protocol processing of data packets received from the network, a first field programmable gate array application; a first interface configured to receive processed data packets from the transport engine and to: provide at least some of the processed data packets to the field programmable gate array application for processing.</p><p id="p-0018" num="0017">In one embodiment, the first interface is configured to: provide at least some of the processed data packets to the host computing device without being processed by the application.</p><p id="p-0019" num="0018">In one embodiment, the first field programmable gate array application is configured to process the at least some of the data packets to provide filtering.</p><p id="p-0020" num="0019">In one embodiment, the network interface device comprises: a second field programmable gate array application; a second interface between the transport engine and the second field programmable gate array application, wherein the second interface is configured to provide the data packets received from the network to the second field programmable gate array application, wherein the second field programmable gate array application is configured to process the data packets and provide them to the transport engine.</p><p id="p-0021" num="0020">In one embodiment, the processing by the second field programmable gate array application comprises distributed denial of service (DDOS) mitigation, the first field programmable gate array application maintains a data store, the processing by the first field programmable gate array application comprising updating the data store.</p><p id="p-0022" num="0021">According to a third aspect, there is provided a network interface device arranged to couple a host computing device to a network comprising: a first interface configured to receive data packets from the host computing device, the first interface comprising a first memory and a second memory, wherein the first memory is memory mapped to a shared memory location in the host computing device, wherein the second memory is configured to receive data packets transferred from the host, wherein the network interface device comprises: a field programmable gate array application; and a transport engine for performing protocol processing of data packets, wherein the first interface is configured to provide the data packets in the first memory to the FPGA application for processing by the FPGA application, wherein the first interface is configured to provide the data packets in the second memory to the transport engine for protocol processing.</p><p id="p-0023" num="0022">According to a fourth aspect, there is provided a data processing system comprising: a network interface device according to the third aspect; and a host computing device.</p><p id="p-0024" num="0023">In one embodiment, the FPGA application is configured to provide data to the host computing device, wherein the host computing device is configured to provide flow control over the data provided by the FPGA application to the host computing device.</p><p id="p-0025" num="0024">In one embodiment, the flow control comprises credits based flow control.</p><p id="p-0026" num="0025">In one embodiment, the flow control comprises Xon/Xoff flow control.</p><p id="p-0027" num="0026">According to a fifth aspect, there is provided a data processing system comprising a host computing device and a network interface device arranged to couple the host computing device to a network, the network interface device comprising a field programmable gate array application and a transport engine, the host computing device comprising: one or more processors; a first memory configured to receive data packets for transmission over a network; a second memory configured to receive data packets for delivery to the field programmable gate array application, wherein the second memory is mapped to a memory region on the network interface device associated with the field programmable gate array application; a transmit queue comprising a pointer to a location in the first memory, wherein the one or more processors are configured to transfer data from the first memory at the location pointed to by the pointer to a memory of the network interface device associated with the transport engine.</p><p id="p-0028" num="0027">In one embodiment, the second memory is mapped to the memory region associated with the field programmable gate array application using a wide memory aperture.</p><p id="p-0029" num="0028">According to a sixth aspect, there is provided a network interface device arranged to couple a host computing device to a network comprising: a transport engine for performing protocol processing of data packets; a field programmable gate array application configured to process data packets to be sent over the network, wherein the transport engine is configured to perform protocol processing of data packets to be sent over the network, the network interface device comprising: a second interface configured to: receive data packets protocol processed by the transport engine; and cause the data packets to be transmitted over the network, wherein the second interface is configured to back-pressure the transport engine.</p><p id="p-0030" num="0029">According to a seventh aspect, there is provided a network interface device comprising: an interface comprising a plurality of input ports and a plurality of output ports; first network interface device circuitry having at least one input port and at least one port configured to be coupled to respective ports of the interface; host interface circuitry configured to interface to a host device, said host interface circuitry having at least one input port and at least one output port configured to be coupled to respective ports of the interface, said host interface circuitry comprising a scheduler configured to schedule at least one of the providing of data to the host device and the receiving of data from the host; and host offload circuitry configured to perform an offload operation for the host device, said host offload circuitry having at least one input port and at least one output port configured to be coupled to respective ports of the interface, wherein said interface is configured to allow at least one of: data to be provided to said host interface circuitry from at least one of said first network device interface circuitry and said host offload circuitry; and data to be provided from said host interface circuitry to at least one of said first network interface device circuitry and said host offload circuitry.</p><p id="p-0031" num="0030">The at least one input port and said at least one output port of the host offload circuitry may be connected via the interface only to respective ports of said host interface circuitry.</p><p id="p-0032" num="0031">The network interface device may comprise second network interface device circuitry having at least one input port and at least one output port configured to be coupled to respective ports of the interface, said second network interface device circuitry configured to perform network interface device operations.</p><p id="p-0033" num="0032">The second network interface device circuitry may comprise at least one kernel.</p><p id="p-0034" num="0033">The second network interface device circuitry may be configured to perform at least one of: an accelerator function and a flow steering function.</p><p id="p-0035" num="0034">The at least one input port and said at least one output port of the second network interface device circuitry may be connected via the interface only to respective ports of said first network interface device circuitry.</p><p id="p-0036" num="0035">The host offload circuitry may comprise at least one kernel.</p><p id="p-0037" num="0036">The host offload circuitry may be configured to support a first component of an application, said application component being configured to at least one of provide data to and receive data from a second component of the application,</p><p id="p-0038" num="0037">The second component of the application may be provided by said host device.</p><p id="p-0039" num="0038">The first network interface device circuitry may be configured to stream data directly to at least one of a kernel in said network interface device and a kernel in said host device.</p><p id="p-0040" num="0039">The first network interface device circuitry may be configured to stream data to a destination comprising at least one of said second network interface device circuitry and said host device.</p><p id="p-0041" num="0040">The first network interface device circuitry may be configured to receive data from a destination comprising at least one of said second network interface device circuitry and said host device.</p><p id="p-0042" num="0041">The interface may comprise a cross bar switch.</p><p id="p-0043" num="0042">The host interface circuitry may be configured to manage a plurality of queues of data.</p><p id="p-0044" num="0043">According to an eighth aspect, there is provided a system comprising a host device and a network interface device, said network interface comprising: an interface comprising a plurality of input ports and a plurality of output ports; first network interface device circuitry having at least one input port and at least one port configured to be coupled to respective ports of the interface; host interface circuitry configured to interface to the host device, said host interface circuitry having at least one input port and at least one output port configured to be coupled to respective ports of the interface, said host interface circuitry comprising a scheduler configured to schedule at least one of the providing of data to the host device and the receiving of data from the host; and host offload circuitry configured to perform an offload operation for the host device, said host offload circuitry having at least one input port and at least one output port configured to be coupled to respective ports of the interface, wherein said interface is configured to allow at least one of: data to be provided to said host interface circuitry from at least one of said first network device interface circuitry and said host offload circuitry; and data to be provided from said host interface circuitry to at least one of said first network interface device circuitry and said host offload circuitry.</p><p id="p-0045" num="0044">The host may comprise at least one of: at least one application; and a protocol processing function, a plurality of which are configured to offload at least a part of respective functionality to said network interface device.</p><p id="p-0046" num="0045">The host offload circuitry may be configured to provide at least one of said offloaded respective functionality.</p><p id="p-0047" num="0046">In the above, many different embodiments have been described. It should be appreciated that further embodiments may be provided by the combination of any two or more of the embodiments described above.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0048" num="0047">Some embodiments will now be described by way of example only with reference to the accompanying Figures in which:</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a schematic view of a data processing system coupled to a network;</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a schematic view of a network interface device according to embodiments of the application;</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a schematic view of a network interface device according to embodiments of the application;</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a schematic view of a network interface device according to embodiments of the application;</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a schematic view of a network interface device according to embodiments of the application;</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a data processing system according to embodiments of the application;</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a schematic view of a network interface device according to embodiments of the application;</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a schematic view of part of a network interface device according to some embodiments;</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows part of the arrangement of <figref idref="DRAWINGS">FIG. <b>8</b></figref> with some parts shown in more detail; and</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows part of the arrangement of <figref idref="DRAWINGS">FIG. <b>8</b></figref> and a part of a host device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0059" num="0058">The following description is presented to enable any person skilled in the art to make and use the invention, and is provided in the context of a particular application. Various modifications to the disclosed embodiments will be readily apparent to those skilled in the art.</p><p id="p-0060" num="0059">The general principles defined herein may be applied to other embodiments and applications without departing from the spirit and scope of the present invention. Thus, the present invention is not intended to be limited to the embodiments shown, but is to be accorded the widest scope consistent with the principles and features disclosed herein.</p><p id="p-0061" num="0060">When data is to be transferred between two data processing systems over a data channel, such as a network, each of the data processing systems has a suitable network interface to allow it to communicate across the channel. Often the network is based on Ethernet technology. Data processing systems that are to communicate over a network are equipped with network interfaces that are capable of supporting physical and logical requirements of the network protocol. The physical hardware component of network interfaces is referred to as network interface devices or network interface cards (NICs).</p><p id="p-0062" num="0061">Most computer systems include an operating system (OS) through which user level applications communicate with the network. A portion of the operating system, known as the kernel, includes protocol stacks for translating commands and data between the applications and a device driver specific to the network interface devices, and the device drivers for directly controlling the network interface devices. By providing these functions in the operating system kernel, the complexities of and differences among network interface devices can be hidden from the user level application. In addition, the network hardware and other system resources (such as memory) can be safely shared by many applications and the system can be secured against faulty or malicious applications.</p><p id="p-0063" num="0062">A typical data processing system <b>100</b> for carrying out transmission across a network is shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The data processing system <b>100</b> comprises a host computing device <b>101</b> coupled to a network interface device <b>102</b> that is arranged to interface the host to network <b>103</b>. The host computing device <b>101</b> includes an operating system <b>104</b> supporting one or more user level applications <b>105</b>. The host computing device <b>101</b> may also include a network protocol stack (not shown). For example, the protocol stack may be a component of the application, a library with which the application is linked, or be provided by the operating system. In some embodiments, more than one protocol stack may be provided.</p><p id="p-0064" num="0063">The network protocol stack may be a Transmission Control Protocol (TCP) stack. The application <b>105</b> can send and receive TCP/IP messages by opening a socket and reading and writing data to and from the socket, and the operating system <b>104</b> causes the messages to be transported across the network. For example, the application can invoke a system call (syscall) for transmission of data through the socket and then via the operating system <b>104</b> to the network <b>103</b>. This interface for transmitting messages may be known as the message passing interface.</p><p id="p-0065" num="0064">Instead of implementing the stack in the host <b>101</b>, some systems offload the protocol stack to the network interface device <b>102</b>. For example, in the case that the stack is a TCP stack, the network interface device <b>102</b> may comprise a TCP Offload Engine (TOE) for performing the TCP protocol processing. By performing the protocol processing in the network interface device <b>102</b> instead of in the host computing device <b>101</b>, the demand on the host system's <b>101</b> processor/s may be reduced. Data to be transmitted over the network may be sent by an application <b>105</b> via a TOE-enabled virtual interface driver, by-passing the kernel TCP/IP stack entirely. Data sent along this fast path therefore need only be formatted to meet the requirements of the TOE driver.</p><p id="p-0066" num="0065">The host computing device <b>101</b> may comprise one or more processors and one or more memories. In some embodiments, the host computing device <b>101</b> and the network interface device <b>102</b> may communicate via a bus, for example a peripheral component interconnect express (PCIe bus).</p><p id="p-0067" num="0066">During operation of the data processing system, data to be transmitted onto the network may be transferred from the host computing device <b>101</b> to the network interface device <b>102</b> for transmission. In one example, data packets may be transferred from the host to the network interface device directly by the host processor. The host may provide data to one or more buffers <b>106</b> located on the network interface device <b>102</b>. The network interface device <b>102</b> may then prepare the data packets and transmit them over the network <b>103</b>.</p><p id="p-0068" num="0067">Alternatively, the data may be written to a buffer <b>107</b> in the host system <b>101</b>. The data may then be retrieved from the buffer <b>107</b> by the network interface device and transmitted over the network <b>103</b>.</p><p id="p-0069" num="0068">In both of these cases, data is temporarily stored in one or more buffers prior to transmission over the network.</p><p id="p-0070" num="0069">Some embodiments relate to the transmission of data over a network from a data processing system. A data processing system could be any kind of computing device, such as a server, personal computer or handheld device. Some embodiments may be suitable for use in networks that operate TCP/IP over Ethernet. In other embodiments one or more different protocols may be used. Embodiments may be used with any suitable networks, wired or wireless.</p><p id="p-0071" num="0070">Buffer <b>106</b> may be any suitable memory, a FIFO, or set of registers. The buffer may be such that any operations which require modifications to the frame (for example checksum insertion into headers) are completed before that portion of the frame is transferred to the MAC (media access control).</p><p id="p-0072" num="0071">As mentioned, the protocol processing may be offloaded to the network interface device to be performed in the network interface. In the case that such offloading is performed at the network interface device, it may desirable to implement functionality on the network interface device that can make use of the protocol processing capabilities of the network interface device. By implementing such functionality in the network interface device instead of the host, potential improvements in latency may be achieved by reducing the data transfer between the host and the network interface device.</p><p id="p-0073" num="0072">According to some embodiments of the application, a Field Programmable Gate Array (FPGA) application is provided on the network interface device. In some embodiments, the NIC functionality may be embedded in an FPGA. The FPGA application may be understood to be an application that is implemented using an integrated circuit designed to be configured by a customer or designer after its manufacture. FPGAs may offer the ability to deploy a circuit which provides an application specific solution. An FPGA can allow for customization of a network interface device. Such applications may be niche or small volume applications. The FPGA is an array of programmable logic blocks. An FPGA may be characterised as being reprogrammable at the logic element granularity. An FPGA may be regarded as being an array of gates.</p><p id="p-0074" num="0073">A first interface (for example a Fabric interface) may be provided in the network interface device between the FPGA application and the host computing device, the first interface being configured to receive data from the host and pass at least some of the data to the FPGA application. The data received from the host over the first interface for delivery to the FPGA application may be any type of data transaction. For example, the data may be a data packet having a payload and header according to a suitable network protocol. The data packet may be an egress data packet for transmission over the network. On the other hand, the data may be a &#x201c;read&#x201d; request for access to a data structure maintained by the FPGA application. In response to such a request (which may take the form of a local instructions issued to an address memory mapped to a memory accessible to the FPGA), the FPGA application may return the requested data to the host via the first interface. The data received could, in other words, be a lookup request received from the host. In some cases, the read operation from the host may cause side effects such as an update to data maintained by the FPGA application. For example, the FPGA could comprise a store/database (of, for example, a key value). The FPGA application may receive from the host an update to the store/database from the FPGA in the form of memory &#x201c;write&#x201d; instructions, which may take the form of a store instruction to a mapped address. The write could update a data structure or cause any other side effect as implemented by the FPGA.</p><p id="p-0075" num="0074">If the first interface receives data for transmission over the network, the first interface may be configured to pass some of this data to the FPGA application for processing. The first interface may pass the remaining data to a transport engine for protocol processing and transmission over the network without it being processed by the FPGA application. The FPGA application may be configured to perform the processing of data packets it receives. In some cases (e.g. the data is for transmission over a network), the FPGA application is configured to then pass the processed data to the transport engine for protocol processing to form protocol processed data packets. Hence, the processing by the FPGA application is optional from the perspective of the first interface. The first interface has the capability to apportion work to the FPGA application. Once the protocol processing has been performed, the data packets are then transmitted over the network.</p><p id="p-0076" num="0075">A second interface may be provided in the network interface device, for interfacing the device with the network. The second interface may be a Layer2 Streaming Interface. The second interface is configured to receive data packets from the FPGA application or from the transport engine and cause them to be transmitted over the network.</p><p id="p-0077" num="0076">The network interface device is also configured to receive ingress data packets from the network and pass them to the host or the FPGA depending on a classification of the packets, e.g. MAC or virtual local area network (VLAN).</p><p id="p-0078" num="0077">The transport engine is configured to receive some ingress data packets from the network and perform receive protocol processing of the data packet prior to passing at least some of the data packets to the first interface. The first interface is configured to receive these protocol processed data packets from the transport engine and pass some of them to the FPGA application. The first interface may be configured to cause the remaining data packets to be transmitted to the host without being processed by the FPGA application. The FPGA application may then be configured to perform processing of the data packets passed to it prior to the data packets being transmitted to the host via the first interface. The first interface may be configured to receive data packets from the FPGA application that have been processed by the FPGA application, and to pass these processed packets to the host computing device. Alternatively, instead of transmitting the data packets to the host, the FPGA application may be configured to process the data packets by terminating them. The FPGA application may be configured to process the data packets by consuming them. The FPGA application may process the data packets by filtering them and passing only some of them to the host.</p><p id="p-0079" num="0078">In some embodiments, the second interface may be configured to receive data packets from the network and provide them to the FPGA application prior to protocol processing at the transport engine. The FPGA application may perform the processing of the data packets and may then pass the processed data packets to the transport engine. The transport engine is configured to receive the data packets from the FPGA application, protocol process them, and then pass them to the first interface to be transmitted to the host. The transport engine may backpressure the second interface. A third interface, which may be a MAC layer, may also be provided between the second interface and the network. The third interface may receive data packets from the network and provide them to the second interface. The third interface may be back-pressured by the transport engine or by the second interface.</p><p id="p-0080" num="0079">A third interface, which may be MAC layer, may also be provided between the second interface and the network. The third interface may receive data packets from the network and provide them to the second interface.</p><p id="p-0081" num="0080">In some embodiments, the transport engine may receive egress data packets from the host for protocol processing prior to them being passed to the FPGA application. The FPGA application may then process the protocol processed data packets prior to passing them to the second interface for transmission over the network. The second interface may be configured to pass the data packets to the third interface, which is configured to cause them to be transmitted over the network. The second interface may be back-pressured by the third interface. The transport engine may be back pressured by the second interface or the third interface.</p><p id="p-0082" num="0081">The FPGA application may need to compete for host memory and PCI bandwidth used to communicate with the host. The FPGA application may also need to compete for network bandwidth. The FPGA application may need to compete for these resources with, for example, data flows being sent and received between the host and the network.</p><p id="p-0083" num="0082">The back pressuring of the transport engine or the interfaces could, for example, be used to free up additional resources for the FPGA application. For example, the transport engine may be back-pressured by the second interface. This may free up resources for the FPGA application communicate over the network, by reducing the proportion of network resources in use for transmitting data packets from the transport engine.</p><p id="p-0084" num="0083">Different techniques may be used by the network interface device for scheduling so as to allocate the resources appropriately. In one example, credit-base flow control may be implemented. For example, the FPGA application may have data to be written to the host. The FPGA application may make the transfer of data to the host in response to determining that sufficient credits are available for the making of the transfer of the data to the host. For example, the FPGA application may receive the credits from the host and, in response, to send the data to the host. The credits may be bus credits, such as PCIe tags. The credits may be sent by the first interface to both the FPGA application and the transport engine. The credits may be shared amongst the FPGA application and the transport engine. The credits may be shared amongst the slices of the transport engine. By using this flow control method, the host may exert control over the resources used by the host.</p><p id="p-0085" num="0084">In another example XOFF/XON flow control may be implemented. For example, the host may transmit to the network interface device XOFF/XON codes that indicate to the FPGA application whether or not data should be transmitted from the FPGA application to the host. The FPGA application may transmit data to the host in response to receiving an indication to transmit. The FPGA application may continue to transmit data to the host until an indication not to transmit is received. The FPGA application may then resume transmission until an indication to transmit is again received.</p><p id="p-0086" num="0085">Some embodiments may have the advantage that the network interface device can be programmed to provide functions for the data packets in the receive path or on the transmit path as a customer or designer would see fit. These functions could be performed for only some or for all of the data packets on the receive path or transmit path.</p><p id="p-0087" num="0086">As the FPGA application is provided with the first interface where the data paths also interface to the host, the FPGA application may make use of data path operations. For example the data path operation may be a checksum offload operation. The first interface may allow the network interface device to properly schedule work between host applications and the FPGA application. The FPGA application with this architecture is able to communicate with the other network interface device applications using a relative high bandwidth and/or relative low latency interconnect. Some embodiments may have the advantage that the FPGA application is integrated within the network interface functionality.</p><p id="p-0088" num="0087">In some embodiments, a plurality of FPGA applications may be provided in the network interface device. The plurality of FPGA applications may be configured to perform different types of processing.</p><p id="p-0089" num="0088">Some embodiments may support a significant amount of per-frame packet processing.</p><p id="p-0090" num="0089">Reference is made to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, which shows a network interface device <b>200</b> according some embodiments of the application.</p><p id="p-0091" num="0090">The network interface device functionality may be embedded in an FPGA <b>205</b> in some embodiments. That FPGA <b>205</b> may have network interface functionality and FPGA functionality. The FPGA functionality may provide an FPGA application <b>240</b>, which may programed into the FPGA <b>205</b> according to the needs of the network interface device user. The FPGA application <b>240</b> may, for example, provide filtering of the messages on the receive path from the network <b>230</b> to the host. The FPGA application <b>240</b> may provide a firewall.</p><p id="p-0092" num="0091">The FPGA application <b>240</b> may be used for any suitable purpose. For example, the FPGA application <b>240</b> may reduce data in one or more flows. For example, the FPGA application <b>240</b> may remove duplicate messages in an arbitration application. The FPGA application <b>240</b> may alternatively or additionally be used to aggregate data. For example the FPGA may buffer and consolidate network captures for more efficient host delivery. Alternatively or additionally, the FPGA may be used for hybrid processing. For example the FPGA may provide a toolkit to enable hybrid applications to implement fast-path processing (low-latency) on the FPGA while using a slice to handle slower (and more complex TCP) protocol activity. This is also known as Delegated-Send on the transmit side. The FPGA may be used to detect frames which are part of DDOS attack. Those frames may be dropped or filtered. The FPGA may alternatively or additionally be used for machine learning with a neural network or the like.</p><p id="p-0093" num="0092">The FPGA may be programmable to provide the FPGA application <b>240</b> using a high level programming language, such as C-based languages. The high level programming of the FPGA may be achieved using high level synthesis. The FPGA application <b>240</b> may implement a neural network and perform feature extraction or classification based on incoming frames.</p><p id="p-0094" num="0093">In some embodiments, the network interface device functionality may be implemented as &#x201c;hard&#x201d; logic within the FPGA. For example, the hard logic may be Application Specific Integrated Circuit (ASIC) gates. The FPGA application <b>240</b> may be implemented as &#x201c;soft&#x201d; logic. The soft logic may be provided by programming the FPGA LUTs (look up tables). The hard logic may be capable of being clocked at a higher rate as compared to the soft logic.</p><p id="p-0095" num="0094">The network interface device <b>200</b> includes a first interface <b>210</b> configured to receive data from the host <b>230</b>. The first interface <b>210</b> may be a fabric interface. The first interface <b>210</b> interfaces the transport engine <b>215</b> with the host computing device <b>235</b>. The first interface <b>210</b> also interfaces the FPGA application <b>240</b> with the host <b>235</b>. The first interface may provide an application programming interface, allowing the host <b>235</b> to perform memory read and writes to memory associated with the FPGA application <b>240</b>.</p><p id="p-0096" num="0095">Some drivers may be associated with the memory interfaces. This allows host applications to directly access the FPGA application <b>240</b> from the host and make uses of the functions of the FPGA application <b>240</b> at the same time that the FPGA is used as a network interface device. The memory based interface may be mapped through a different PCI function than is used by the message passing interfaces. The message passing interfaces typically have one function per Ethernet port. The host software may attach a driver to each function and may attach the network stack to the interface to the network.</p><p id="p-0097" num="0096">The network interface device <b>200</b> further comprises a transport engine <b>215</b>, configured to process the data packets in accordance with a transport protocol, such as TCP/IP. The transport engine <b>215</b> may comprise a protocol stack. The transport engine <b>215</b> may comprise a plurality of slices or data pipeline, some of the slices being RX slices configured to perform receive processing of the ingress data packets received from the network <b>230</b>, and some of the slices being TX slices configured to perform transmit processing of the egress data packets to be transmitted onto the network <b>230</b>. In some embodiments, a slice may be able to handle both data to be transmitted and received data.</p><p id="p-0098" num="0097">In the example shown, four slices are provided. However, it should be appreciated that in other embodiments, a different number of slices are used. In one embodiment, a slice may be arranged to process received data or to process data to be transmitted. In other embodiments, a slice may be arranged such that it is able to process received data and data to be transmitted. In some embodiments, the number of slices may be the same as the number of ports. In some embodiments, there may be a transmit slice and a receive slice for each port. In some embodiments, there may not be a direct correlation between the number of ports and the number of slices. In some embodiments, a slice can be switched dynamically from processing received data to processing transmitted data and vice versa.</p><p id="p-0099" num="0098">Each slice may be regarded as a processing engine. Each slice may thus execute micro code to implement functions such as parsing, matching offload and delivery semantics for the data path. The slice may act on any bit of a frame.</p><p id="p-0100" num="0099">The slices may perform a parsing action on the data which the slice is processing. There may be a matching action which matches the data against for example a filter and action function which performs an action or not in dependence on the result of the matching.</p><p id="p-0101" num="0100">The network interface device <b>200</b> also comprises a second interface <b>220</b> configured to interface the transport engine <b>215</b> with the network <b>230</b> and configured to interface the FPGA application <b>240</b> with the network <b>230</b>. The second interface maybe a layer 2 streaming interface. The second interface may provide an interface to the network <b>230</b> via a third interface <b>225</b>. The third interface <b>225</b> may receive data packets from the second interface and cause them to be transmitted over the network <b>230</b>. The third interface <b>225</b> may similarly receive data packets from the network <b>230</b> and pass them to the second interface <b>220</b>. The third interface <b>225</b> may comprise a MAC interface. The third interface <b>225</b> may comprise a plurality of MAC interfaces. The third interface <b>225</b> may comprise one or more Physical Coding Sublayer (PCS) interfaces. The third interface <b>225</b> may comprise one or more Physical Medium Attachment (PMA) interfaces.</p><p id="p-0102" num="0101">The fabric interface is configured to allow the co-resident FPGA application <b>240</b> to receive frames from the host. The FPGA application <b>240</b> may be able to transmit frames to the work with data path processing at the same bandwidth as the host PCI.</p><p id="p-0103" num="0102">The layer 2 streaming interface is configure to allow the FPGA application <b>240</b> to receive frames form any active MAC layer interface and to transmit frames to a data path associated with that MAC.</p><p id="p-0104" num="0103">The network interface device may provide functionality such as flow steering and low latency operation, hardware timestamping and clock synchronization.</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a path that may be taken by ingress data packets upon their reception from the network <b>230</b>. The third interface <b>225</b> is configured to receive the data packets from the network and pass them to the second interface <b>220</b>. The second interface <b>220</b> is configured to pass the data packets to the transport engine <b>215</b>, which performs processing of the data packets, prior to passing the processed data packets to the fabric interface <b>210</b>. The fabric interface <b>210</b> is configured to pass some of the data packets to the host <b>235</b>, and some of the data packets to the FPGA application <b>240</b>. The fabric interface may determine to pass data packets of a first flow to the host <b>235</b> and data packets of a second flow to the FPGA <b>205</b>. The data packets received at the FPGA application <b>240</b> are processed by the FPGA application <b>240</b>, before being passed to the host <b>235</b>. Hence, the FPGA <b>205</b> can provide accelerator functions for some data packets prior to them being received at the host <b>235</b>.</p><p id="p-0106" num="0105">As explained, the FPGA <b>205</b> may provide a plurality of FPGA applications. The FPGA <b>205</b> could provide a first application and a second application. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, it is shown that data may be directed from a first interface <b>210</b> to the first field programmable gate array application <b>240</b> for processing. Additionally, however, the first and second interface may be configured to direct data to a second field programmable gate array for processing.</p><p id="p-0107" num="0106">Reference is made to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, which shows an example of a network interface device <b>700</b> according to embodiments of the application. In this case, the FPGA <b>705</b> provides a first FPGA application <b>715</b> and a second FPGA application <b>710</b>. The second interface <b>220</b> is configured to receive the data packets from the network and provide the data packets to the second FPGA application <b>710</b> for processing. The second FPGA application <b>710</b> may process the data by performing DDOS mitigation, such as searching the data packets for a DDOS attack and taking remedial action against a DDOS attack. The data packets may comprise updates to a data store, maintained by the first FPGA application <b>715</b>. The second FPGA application <b>710</b> is configured to provide security by searching the data packets to detect any threats posed by the data packets and taking any action as required.</p><p id="p-0108" num="0107">After processing by the second FPGA application <b>710</b>, the second FPGA application <b>710</b> is configured to provide the data packets to the transport engine <b>215</b> for protocol processing. Once the data packets have been processed, they are passed to a first interface, which is configured to provide the data packets to the first FPGA application <b>715</b>. The first FPGA application <b>715</b> is configured to process the data packets. The processing by the first FPGA application <b>715</b> may comprise updating the data store maintained by the first FPGA application <b>715</b> with the data contained in the data packets.</p><p id="p-0109" num="0108">The host may also provide queries of the data in the data store maintained by the first FPGA application <b>715</b>. In response to a query, the first FPGA application <b>715</b> may be configured to provide the requested data from the data store to the host <b>235</b>.</p><p id="p-0110" num="0109">Additionally, although not shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the first FPGA application <b>715</b> may communicate with the second interface <b>220</b> to send and receive data with the transport engine <b>215</b> and the network <b>230</b>. The first FPGA application <b>715</b> may send data to the transport engine <b>215</b> on the receive path or the transmit path. The first FPGA application <b>715</b> may receive data from the transport engine <b>215</b> on the receive path or the transmit path. Additionally, although not shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the second FPGA application <b>710</b> may communicate with the first interface <b>210</b> to send and receive data with the transport engine <b>215</b> and the host <b>235</b>. The second FPGA application <b>710</b> may send data to the transport engine <b>215</b> on the receive path or the transmit path. The second FPGA application <b>710</b> may receive data from the transport engine <b>215</b> on the receive path or the transmit path.</p><p id="p-0111" num="0110">The first FPGA application <b>715</b> and the second FPGA application <b>710</b> may both have access to shared state, which they may use for communicating with each other.</p><p id="p-0112" num="0111">Reference is made to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, which shows the same network interface device <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, but, in this case, indicates a path that may be taken by data received from the host <b>235</b>. The first interface <b>210</b> is configured to receive data from the host <b>235</b> and to direct some of the data to the transport engine <b>215</b> for processing without it first being processed by the FPGA application <b>240</b>. The first interface <b>210</b> is also configured to transfer some data to the FPGA application <b>240</b> for processing by the FPGA application <b>240</b>. Once the data have been processed by the FPGA application <b>240</b>, the FPGA application <b>240</b> may be configured to transfer those data packets to the first interface, which is configured to pass them to the transport engine <b>215</b> for protocol processing. The transport engine <b>215</b> is configured to process the data it receives to produce protocol processed data packets prior to them being transmitted onto the network. The protocol processed data packets may be passed to the second interface <b>220</b> and subsequently the third interface <b>225</b> prior to their transmission onto the network <b>230</b>.</p><p id="p-0113" num="0112">As will be explained with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref> below, a host application may communicate directly with the FPGA application <b>240</b> via memory mapping. The host application may write to a shared memory on the host <b>235</b>, which is memory mapped to a memory accessible to the FPGA application <b>240</b>. Hence, the FPGA application <b>240</b> may receive data written to the shared memory by the host <b>235</b>. Likewise, the FPGA application <b>240</b> may write to the memory that is accessible to it, with that data being automatically copied from that memory to the memory of the host <b>235</b> so that it received by the host application <b>235</b>.</p><p id="p-0114" num="0113">In some cases, some data may be transferred to the transport engine <b>215</b> from the host <b>235</b> and some data may be transferred to the FPGA application <b>240</b> from the host <b>235</b> in parallel. For example, a first host application may transfer data to the transport engine <b>215</b> via the first interface, whilst a second host application is transferring data to the FPGA application <b>240</b> at the same time.</p><p id="p-0115" num="0114">Reference is made to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, which shows a network interface device <b>400</b> according to some embodiments of the application. The network interface device <b>400</b> is similar to the network interface device <b>200</b> shown in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>, with like elements being indicated with like reference numerals. However, <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows the network interface device <b>400</b> having two separate MAC interfaces <b>405</b>, <b>410</b>. In this case, the two MAC interfaces <b>405</b>, <b>410</b> may be understood to constitute the third interface. Each MAC interface may be associated with different slices of the transport engine. For example, the first MAC interface <b>405</b> may be associated with a first receive slice <b>415</b><i>a</i>, such that data packets, which are received at the first MAC interface <b>405</b> are delivered by the second interface <b>220</b> to the first receive slice <b>415</b><i>a </i>for receive processing. The first MAC interface <b>405</b> may also be associated with a first transmit slice <b>415</b><i>c</i>, such that data packets for which transmit processing is carried out at the transmit slice <b>415</b><i>c </i>are delivered by the second interface <b>220</b> to the MAC interface <b>405</b> for transmission over the network. The second MAC interface <b>410</b> may be associated with a second receive slice <b>415</b><i>b</i>, such that data packets, which are received at the second MAC interface <b>410</b> are delivered by the second interface <b>220</b> to the second receive slice <b>415</b><i>c </i>for receive processing. The second MAC interface <b>410</b> may also be associated with a second transmit slice <b>415</b><i>d</i>, such that data packets for which transmit processing is carried out at the transmit slice <b>415</b><i>d </i>are delivered by the second interface <b>220</b> to the second MAC interface <b>410</b> for transmission over the network.</p><p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a path that may be taken by data packets upon reception from the network <b>230</b>. The data packets may be received at the third interface (e.g. the second MAC interface <b>410</b>) and be passed to the second interface <b>220</b>. The second interface <b>200</b> may be configured to transfer some data packets to the transport engine <b>215</b> and other data packets to the FPGA application <b>240</b>. The FPGA application <b>240</b> is configured to process the data packets and pass them (e.g. via the second interface) to the transport engine <b>215</b>. The transport engine <b>415</b> is configured to process the data packets prior to passing them to the host via the first interface <b>210</b>.</p><p id="p-0117" num="0116">The transport engine <b>215</b> may be configured to back-pressure the second interface. The transport engine <b>215</b> comprises one or more buffers for storing data on which receive protocol processing is to be performed. If a large amount of traffic is received from the network <b>230</b>, there is a risk of buffer overflow for the one or more buffers stored by the transport engine <b>215</b>. Therefore, in this case, the transport engine <b>215</b> may provide an indication to the second interface <b>220</b> to reduce the rate of data transfer to the transport engine <b>215</b>. The transport engine <b>215</b> may be configured to monitor buffer utilization of the one or more buffers, and if the buffer utilization gets too high to provide an indication of such to the second interface <b>220</b>, which reduces the rate at which is provides data packets to the transport engine <b>215</b> for processing. Similarly, the second interface <b>220</b> may back-pressure the third interface, by providing the indication that the buffer utilization is too high to the third interface. The third interface being then configured to reduce the rate at which it transfers data packets to the second interface <b>220</b>. The back pressuring of the third interface may be specific to the MAC interface associated with the particular slice, which is configured to perform receive processing for that MAC interface. For example, if the second receive slice <b>415</b><i>b </i>determines that the buffer utilization of a buffer associated with this slice gets too high, an indication of such may be provided to the second MAC interface <b>410</b>, which reduces the rate of transfer to the second receive slice <b>415</b><i>b. </i></p><p id="p-0118" num="0117">Reference is made to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, which shows the same network interface device <b>400</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, but, in this case, indicates a path that may be taken by egress data packets received from the host <b>235</b> for transmission onto the network <b>230</b>. The data packets are received from the host <b>235</b> and transferred via the first interface <b>210</b> to the transport engine <b>215</b>, which performs processing of the data packets and passes the data packets to the second interface <b>220</b>. The second interface <b>220</b> is configured to pass some of the data packets to the third interface for transmission onto the network <b>230</b>, without being processed by the FPGA application <b>240</b>. The second interface <b>220</b> is configured to pass some of the data packets to the FPGA application <b>240</b>. The FPGA application <b>240</b> processes the data packets and then passes them to the third interface <b>410</b> for transmission over the network.</p><p id="p-0119" num="0118">In this case, the second interface <b>220</b> may back-pressure the third interface. The second interface <b>220</b> comprises one or more buffers for storing data on which receive protocol processing is to be performed. If a large amount of traffic is received at the second interface <b>220</b> for delivery to the network <b>230</b>, there is a risk of buffer overflow for the one or more buffers stored by the second interface <b>220</b>. Therefore, in this case, the second interface <b>220</b> may provide an indication to the transport engine to reduce the rate of data transfer to the second interface <b>220</b>. The second interface <b>220</b> may be configured to monitor buffer utilization of the one or more buffers, and if the buffer utilization gets too high to provide an indication of such to the second interface <b>220</b>, which reduces the rate at which is provides data packets to the third interface. Similarly, the third interface may back-pressure the second interface <b>220</b>, by providing an indication that the buffer utilization of the third interface is too high, the second interface <b>220</b> being then configured to reduce the rate at which it transfers data packets to the third interface.</p><p id="p-0120" num="0119">The concepts explained with regard to <figref idref="DRAWINGS">FIGS. <b>2</b>, <b>3</b>, <b>4</b>, <b>5</b>, and <b>7</b></figref> are not limited to those specific embodiments and may be combined.</p><p id="p-0121" num="0120">Reference is made to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, which illustrates how data may be transferred from the host <b>605</b> to the network interface device <b>610</b>. As explained previously, the first interface <b>620</b> is configured to receive data from the host <b>605</b>. The host <b>605</b> may make the determination as to which of these is to apply to particular data. The host <b>605</b> may transfer data using different methods to memory associated with the first interface <b>620</b>. For example, the host <b>605</b> may memory map data to a first memory <b>615</b> of the first interface <b>620</b>. The first interface <b>620</b> is configured to provide the data in the first memory <b>615</b> to the FPGA application <b>630</b>. Alternatively, the host <b>605</b> may transfer data to a second memory <b>625</b> of the first interface <b>620</b>. The first interface <b>620</b> then being configured to provide the data in the second memory <b>625</b> to the transport engine <b>635</b> for processing. The first memory <b>615</b> and the second memory <b>625</b> may each comprise one or more buffers.</p><p id="p-0122" num="0121">The host comprise a memory <b>640</b>, which is memory mapped to the memory <b>615</b> on the network interface device <b>610</b>. The memory mapping may be such that a user level process/host application <b>670</b>, which is configured to write to the memory <b>640</b>, and the network interface device <b>610</b> both have access to shared memory to which data is written. When a user level process <b>670</b> running on the host transfers data to the memory <b>640</b>, the data may be automatically transferred from the memory <b>640</b> to the memory <b>615</b>.</p><p id="p-0123" num="0122">The communication between the FPGA application <b>630</b> and the memory <b>640</b> of the host is bidirectional, i.e. the FPGA application can also transfer data to the host <b>605</b> as well as receive data from the host <b>605</b>. To transfer data to the host <b>605</b>, the FPGA application <b>630</b> is configured to write data to memory <b>615</b>, the data being automatically transferred to memory <b>640</b>, so that a user level process <b>670</b> running on the host may access the data from the memory <b>640</b>.</p><p id="p-0124" num="0123">The host <b>605</b> comprises a memory <b>645</b> at which data may be stored for transfer to the memory <b>625</b> of the network interface device <b>610</b>. The data may be provided to the memory <b>625</b> from a second application <b>675</b> running on the host <b>605</b>. Hence, the second application <b>675</b> may transfer data using this socket interface to the network interface device <b>610</b>. The data in memory <b>625</b> is provided to the transport engine. The host <b>605</b> also comprises a transmit queue <b>650</b> comprising a series of pointers pointing to locations in the memory <b>645</b> from which data is to be transferred to the network interface device <b>610</b>. The host may comprise a queue <b>655</b> identifying the next available location in the memory at which data is to be written by the application <b>675</b>. The queues <b>650</b> and <b>655</b> may comprise FIFO buffers. The data may be stored in the buffer <b>645</b> at the next available location&#x2014;as identified by the next pointer in the queue <b>655</b>&#x2014;by a user level process <b>675</b>. A processor of the host device is configured to read the next pointer from the transmit queue <b>650</b> and read the data from the buffer <b>645</b> at the location identified by the pointer and transfer the data to the network interface device <b>610</b>. The network interface device <b>610</b> is configured to store the data in memory <b>625</b> at a location identified by the next pointer in the receive queue <b>660</b>. The data in memory <b>625</b> may then be provided to the transport engine <b>635</b> for protocol processing.</p><p id="p-0125" num="0124">The communication between the transport engine <b>635</b> and the host <b>605</b> is also bidirectional. A similar memory transfer mechanism may be implemented for transfer of the data from the fabric interface to the host <b>605</b>.</p><p id="p-0126" num="0125"><figref idref="DRAWINGS">FIG. <b>6</b></figref> also shows the second interface <b>660</b> and the third interface <b>665</b>, for the sending and receiving of data from the network. Data may be transferred from the FPGA application <b>630</b> or the transport engine <b>635</b> to the second interface <b>660</b>, which is configured to pass the data to the third interface <b>665</b>. The third interface <b>665</b> is configured to cause the data to be transmitted over the network. On the receive path, the data may be received from the network at the third interface <b>665</b>, and passed to the second interface <b>660</b>. The second interface may transfer the data to the FPGA application <b>630</b> or to the transport engine <b>635</b>.</p><p id="p-0127" num="0126">The two data transfer methods shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> may occur in parallel. For example, the interface used by the application <b>675</b> to transfer data to the transport engine <b>635</b> may be active at the same time as the interface used by the application <b>670</b> to transfer data to the FPGA application <b>630</b>. Furthermore, these interfaces may be both in use by multiple applications in parallel. A plurality of host applications may be configured to write to memory <b>645</b> to transfer data to the transport engine <b>635</b> and a plurality of host applications may be configured to write to memory <b>640</b> to transfer data to the FPGA application <b>630</b>. In some examples, both of the interfaces may have associated with it a driver and a software stack.</p><p id="p-0128" num="0127">In some cases an application may be provided in the host that may be used to transfer state between the host <b>605</b> and the FPGA application. The application may be an Open Computing Language application. A shared memory (e.g. a memory aperture) may be mapped onto the application. The shared memory may be used to transfer state between parts of the application running on the host and parts on the FPGA. Hence, transfer of state between the host and the parts on the FPGA may be achieved.</p><p id="p-0129" num="0128">Reference is made to <figref idref="DRAWINGS">FIGS. <b>8</b>, <b>9</b> and <b>10</b></figref> which shows an interface arrangement. This embodiment may be used in conjunction with or separately from the embodiments previously described. The arrangement of <figref idref="DRAWINGS">FIG. <b>8</b></figref> is provided by an FPGA or similar device. In some embodiments, a part of the arrangement of the <figref idref="DRAWINGS">FIG. <b>8</b></figref> may be implemented by an ASIC or the like. In this scenario, the other parts may be provided by an FPGA. This is will be described in more detail with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0130" num="0129">Reference is first made to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In this arrangement, there is an interface <b>800</b>. This interface <b>800</b> may be a fabric interface such as previously described. In some embodiments, the interface may be an interface which operates in accordance with the AXI (advanced eXtensible interface) protocol. In some embodiments, the interface may be one which operates in accordance with the AXI-ST (AXI-streaming) protocol. However, it should be appreciated that the Interface can operate in accordance with any other suitable protocol in other embodiments.</p><p id="p-0131" num="0130">The interface <b>800</b> may be regarded as providing a cross bar switch function.</p><p id="p-0132" num="0131">A streaming subsystem comprising the network interface device streaming core <b>802</b> is provided with a plurality of input ports and a corresponding plurality of output ports. These ports are referenced <b>816</b>. In other embodiments, there may be more input ports than output ports or vice versa.</p><p id="p-0133" num="0132">In the example shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the network interface device streaming core <b>802</b> has four input ports and four output ports. In the example shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, two input ports and two output ports are each associated with a single respective buffer <b>814</b>. Thus there is a one to one relationship between a port and a buffer. Two of the input ports and two of the output ports are each associated with buffers. Thus there is a one to two relationship between a port and two buffers. It should be appreciated that this is by way of example only.</p><p id="p-0134" num="0133">In some embodiments, all of the ports may be associated with the same number of buffers. In other embodiments, such as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, there may be a different number of buffers associated with a port.</p><p id="p-0135" num="0134">In some embodiments, more than two buffers may be associated with a given port.</p><p id="p-0136" num="0135">The ports of the network interface device streaming core <b>802</b> are configured to communicate with respective ports <b>818</b> of the interface <b>800</b>.</p><p id="p-0137" num="0136">A MDMA (multi-queue direct memory access) streaming subsystem <b>810</b> is provided. This is configured to provide an interface to the host. In this example, the MDMDA streaming subsystem <b>810</b> is provided with an input port and an output port <b>828</b>. In other embodiments the MDMA streaming subsystem may be provided with more two ports. These ports of the MDMA streaming subsystem connect to respective ports <b>830</b> of the interface <b>800</b>. In this example, each port is associated with three buffers <b>826</b>. This is by way of example only and in other embodiments, a port of the stream may be associated with more or less than three buffers.</p><p id="p-0138" num="0137">The streaming subsystem <b>810</b> includes its own local scheduler to manage DMA queues. This however will be described in more detail later.</p><p id="p-0139" num="0138">One or more streaming subsystems with computer kernels <b>806</b> and <b>808</b> are also provided. In the example shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, there are two such streaming subsystems with compute kernels <b>806</b> and <b>88</b>. It should be appreciated that there may be more or less than two such streaming subsystems. In some embodiments, the number of such streaming subsystems may change during operation. This may be dependent on requirements of the host system. A streaming subsystem with computer kernels may be added to and/or removed from a data path.</p><p id="p-0140" num="0139">In this example, each of the streaming subsystems with compute kernels <b>806</b> and <b>808</b> is provided with an input port and output port which are referenced <b>834</b>. These respective ports are configured to communicate with respective ports <b>836</b> provided on the interface. Each of these ports is shown with a single buffer <b>832</b>. However, this is by way of example only and in other embodiments more than one buffer may be associated with a given port.</p><p id="p-0141" num="0140">In some embodiments, there may be a single streaming subsystem with compute kernels which is capable of supporting a plurality of functions. In such an embodiment, the streaming subsystem with compute kernels may be provided with more than two ports.</p><p id="p-0142" num="0141">It should be appreciated that the streaming subsystem with compute kernels <b>806</b> and <b>808</b> are configured only to communicate with the MDMA streaming subsystem <b>810</b> in some embodiments. By configuring the communication path in the way, the host is able to configure functions which are to be performed on the network interface device without having to compete for access to resource which is used for communications between the network interface streaming core and the stream subsystem with network interface device extensions kernels.</p><p id="p-0143" num="0142">A streaming subsystem with network interface device extension kernels <b>804</b> is provided in some embodiments. In this example, the streaming subsystem with network interface device extension kernels <b>804</b> is provided with two input ports and two output ports. The ports are referenced <b>822</b>. These ports <b>822</b> are connected to respective ports <b>824</b> of the interface <b>800</b>. In other embodiments, there may be more than two or less than two input ports and/or output ports.</p><p id="p-0144" num="0143">In this example, each port is associated with three buffers <b>820</b>. However, this is by way of example only and in some embodiments, each port may be associated with less than three buffers or more than three buffers.</p><p id="p-0145" num="0144">The number of ports of the network interface device streaming core <b>802</b> which are active may be dependent on the number of functions being performed by this core and may dynamically change. In other embodiments, there may be more than one network interface device streaming core which may be added to and/or removed from a data path. Where there is the possibility of more than one network interface device streaming core, each network interface device streaming core may be associated with a fewer number of ports. For example, a pair of ports may be provided with each network interface device streaming core. In other embodiments, where there is the possibility of more than one network interface device streaming core, different instances may be provided with differing numbers of ports.</p><p id="p-0146" num="0145">In some embodiments, data from one or more of the ports of the streaming subsystem containing the network interface device core may be received via the interface <b>800</b> at the one or more ports of the streaming subsystem with network interface device extension cores. In some embodiments, data is received at one or more of the ports of the streaming subsystem containing the network interface device core via the interface <b>800</b> from one or more port of the streaming subsystem with network interface device extension cores.</p><p id="p-0147" num="0146">It should be appreciated that the streaming subsystem with network interface device extension kernels is, in some embodiments, configured to only communicate with the with respective ports of the streaming subsystem containing the network interface device streaming core.</p><p id="p-0148" num="0147">In some embodiments, the number of such streaming subsystems with network interface device extension kernel functions may change during operation. This may be dependent on requirements of the host system. A streaming subsystem with network interface device extension kernel functions may be added to and/or removed from a data path.</p><p id="p-0149" num="0148">In some embodiments, data from one or more of the ports of the streaming subsystem containing the network interface device core may be received via the interface <b>800</b> at the one or more ports of the MDMA streaming subsystem <b>810</b>. In some embodiments, data is received at one or more of the ports of the streaming subsystem containing the network interface device core via the interface <b>800</b> from one or more port of the MDMA streaming subsystem.</p><p id="p-0150" num="0149">One or more the buffers previously discussed may be provided by a FIFO (first in first out) buffer.</p><p id="p-0151" num="0150">Separate buffers may be provided for the receive data and for the transmit data. For each data flow there may be one or more pairs of transmit and receive buffers.</p><p id="p-0152" num="0151">The size of the buffers may be sized in dependence on the size of the maximum transmit unit MTU.</p><p id="p-0153" num="0152">One or more of the links between two ports may be provided a link operating in accordance with the AXI-ST protocol or any other suitable protocol. In some embodiments, all of the links to the external ports of the interface <b>800</b> operate using the same protocol.</p><p id="p-0154" num="0153">The links to the external ports of the interface may support data rates of 200 Gb in some embodiments. In other embodiments, data rates of more of less than this value may be supported.</p><p id="p-0155" num="0154">In the embodiment shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the interface <b>800</b> may be a 9 input port by 9 output port device. In other embodiments, the interface may have any other suitable size. In some embodiments, the fabric or interface may be made up from two or more independent sub-fabrics. In the case of a 9&#xd7;9 port fabric, this may be made up of a 3&#xd7;3 and 5&#xd7;5 fabric</p><p id="p-0156" num="0155">In some embodiments the interface <b>800</b> may have a port to the MDMA streaming subsystem which has a simple single cycle round robin arbiter.</p><p id="p-0157" num="0156">One or more of the streaming subsystem with network interface device with extension kernel and the streaming subsystem may be implemented by a transmit and/or receive slice such as previously described.</p><p id="p-0158" num="0157">One or more of the streaming subsystem with network interface device with extension kernel and the streaming subsystem may be implemented by an FPGA application such as previously described.</p><p id="p-0159" num="0158">Reference is made to <figref idref="DRAWINGS">FIG. <b>9</b></figref> which shows the arrangement of <figref idref="DRAWINGS">FIG. <b>8</b></figref> but with the components of the MDMA streaming subsystem shown in more detail. The MDMA streaming subsystem comprises MDMA function <b>906</b> which is configured to receive data from and transmit data to the interface <b>800</b>. The QDMA function may provide functions such as receive side processing and/or the like and/or may provide a queue function. Data which is to be output to the interface <b>100</b> may be received from the MDMA function <b>906</b> which may be provided with data received from a host via a PCIe or the like interface <b>904</b>. Data which is received from the interface <b>800</b> is provided to the MDMA function which may provide the data to the host via the PCI or the like interface.</p><p id="p-0160" num="0159">A MDMA source scheduler <b>902</b> is provided to schedule or arbitrate the data which is to be output to the interface. A MDMA destination scheduler <b>908</b> is provided to schedule or arbitrate the data which is to be received from the interface. The MDMA function may comprise a PCI or the like core.</p><p id="p-0161" num="0160">In some embodiments, the functions of the network interface device streaming core and/or the MDMA streaming function may be provided by an ASIC (application specific integrated circuit) or similar arrangement. AXI ST fabric features may be used in some embodiments to support arbitration/scheduling. In some embodiments, a channel select field may be provided via dedicated lines. This may be used on the link between a cycle-arbiter and destination.</p><p id="p-0162" num="0161">Some embodiments may use a credit packet passing from a destination to a source to control scheduling. Some embodiments may use a credit packet passing from the source to the destination. The credit packets may run on the same busses as data packets in some embodiments</p><p id="p-0163" num="0162">Some embodiments may use a packet header which may comprise one or more of: a packet type: routing information; a scheduler flow identifier which is used by the scheduler; and length information.</p><p id="p-0164" num="0163">The packet type may indicate if the packet is a data packet, a credit packet, a configuration capsule, a barrier packet and/or any other suitable packet type.</p><p id="p-0165" num="0164">In some embodiments the routing information may indicate how a packet should get from the source to the destination through the fabric or interface.</p><p id="p-0166" num="0165">The network interface device streaming core may provide the basic network interface device in accordance with for example IP protocols. The streaming subsystem with network interface device with extension kernel allows the network interface device to provide functions depending on the context in which the network interface device is being used. For example the streaming subsystem with network interface device with extension kernel can provide support for specific programming languages not supported by the core. By way of example only, this may be P4 language and/or the like. The streaming subsystem with network interface device with extension kernel may allow for flow steering and/or acceleration functions to be performed.</p><p id="p-0167" num="0166">The streaming subsystem with compute kernels may provide functions for the host. By way of example, this function may be an acceleration function or the providing of a key data base.</p><p id="p-0168" num="0167">Reference is made to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. <figref idref="DRAWINGS">FIG. <b>10</b></figref> shows how the MDMA function <b>810</b> provides an interface to the host <b>930</b>. The PCIe interface <b>904</b> of the MDMA function <b>810</b> is configured to interface with a PCIe interface <b>922</b> of the host.</p><p id="p-0169" num="0168">As shown schematically in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, there is an application <b>934</b> with a compute offload requirement. This application <b>934</b> thus offloads to the network interface device a function. This offloaded function may be provided by one or more streaming subsystems with computer kernels <b>806</b> and <b>808</b> such as previously described in relation to <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref>.</p><p id="p-0170" num="0169">As shown schematically in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, there is a protocol processing stack <b>932</b> with at least a partial offload to the network interface device. This offloaded function may be provided by one or more streaming subsystems with computer kernels <b>806</b> and <b>808</b> such as previously described in relation to <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref>. In other embodiments, the offloaded protocol function may at least partially be implemented by the streaming subsystem with network interface device extension kernels <b>804</b> such as previously described in relation to <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref>.</p><p id="p-0171" num="0170">There may of course be more than one application on the host device which is configured to offload at least a part of its functionality to the network interface device.</p><p id="p-0172" num="0171">In some embodiments, the protocol processing stack may be such that there is no offload of functionality to the network interface device.</p><p id="p-0173" num="0172">In some embodiments, each of the applications in the host and/or the protocol processing stack may be provided with a respective driver stack (not shown), each of which is attached to a different PCI function within the interface.</p><p id="p-0174" num="0173">It should be appreciated that the arrangements shown in <figref idref="DRAWINGS">FIGS. <b>8</b>, <b>9</b> and <b>10</b></figref> may be used at least partially in conjunction with at least a part of any of the previous embodiments.</p><p id="p-0175" num="0174">It should be appreciated that one or more of the slices discussed previously may be used in conjunction with the arrangement of <figref idref="DRAWINGS">FIGS. <b>8</b>, <b>9</b> and <b>10</b></figref>.</p><p id="p-0176" num="0175">One or more slices may be implemented by the FPGA and/or one or more slices may be implemented in ASIC.</p><p id="p-0177" num="0176">The applicant hereby discloses in isolation each individual feature described herein and any combination of two or more such features, to the extent that such features or combinations are capable of being carried out based on the present specification as a whole in the light of the common general knowledge of a person skilled in the art, irrespective of whether such features or combinations of features solve any problems disclosed herein, and without limitation to the scope of the claims. The applicant indicates that aspects of the present invention may consist of any such individual feature or combination of features. In view of the foregoing description it will be evident to a person skilled in the art that various modifications may be made within the scope of the invention.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A network interface device configured to interface between a network and a host device, the network interface device comprising:<claim-text>first network interface device circuitry;</claim-text><claim-text>host interface circuitry;</claim-text><claim-text>host offload circuitry; and</claim-text><claim-text>interface circuitry coupled to: i) the first network interface device circuitry, ii) the host interface circuitry, and iii) the host offload circuitry, the interface circuitry comprising a plurality of hardware ports;</claim-text><claim-text>wherein the first network interface device circuitry has at least one port configured to be coupled to at least one respective port of the interface circuitry;</claim-text><claim-text>wherein the host interface circuitry is configured to interface to the host device, said host interface circuitry having at least one port configured to be coupled to respective port of the interface circuitry; and</claim-text><claim-text>wherein the host offload circuitry is configured to perform an offload operation for the host device, said host offload circuitry having at least one port configured to be coupled to respective port of the interface circuitry.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host offload circuitry is configured to support a first component of an application, said first component being configured to at least one of provide data to and receive data from a second component of the application.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. A network interface device as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein said second component of said application is provided by said host device.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said host offload circuitry comprises at least one kernel.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host offload circuitry comprises a plurality of streaming subsystems, each of which is configured to communicate with a respective port of the interface circuitry.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host offload circuitry comprises a streaming subsystem comprising a compute kernel.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host offload circuitry is configured to provide a key data base.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host offload circuitry is configured to provide flow steering.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host offload circuitry is configured to implement at least part of a protocol processing stack offloaded from the host.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host offload circuitry comprises a field programmable gate array providing a field programmable gate array application.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host offload circuitry comprises at least one of:<claim-text>one or more transmit slices for performing transmit protocol processing of data received from the host device to provide data packets for transmission over the network; and</claim-text><claim-text>one or more receive slices for performing receive protocol processing of data packets received from the network for delivery to the host device.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host offload circuitry comprises a subsystem configured to implement a plurality of offload functions, wherein the subsystem provides a plurality of ports.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein first network interface device circuitry is configured to stream data directly to at least one of a kernel in said network interface device and a kernel in said host device.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interface circuitry comprises a cross bar switch circuit.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A network interface device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interface circuitry is configured to operate in accordance with Advanced eXtensible Interface (AXI) protocol.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A system comprising a host device and a network interface device, said network interface configured to interface between a network and the host device, the network interface device comprising:<claim-text>first network interface device circuitry;</claim-text><claim-text>host interface circuitry;</claim-text><claim-text>host offload circuitry; and</claim-text><claim-text>interface circuitry coupled to i) the first network interface device circuitry, ii) the host interface circuitry, and iii) the host offload circuitry, the interface circuitry comprising a plurality of hardware ports,<claim-text>wherein the first network interface device circuitry has at least one port configured to be coupled to at least one respective port of the interface circuitry,</claim-text><claim-text>wherein the host interface circuitry is configured to interface to the host device, said host interface circuitry having at least one port configured to be coupled to a respective port of the interface circuitry,</claim-text><claim-text>wherein the host offload circuitry is configured to perform an offload operation for the host device, said host offload circuitry having at least one port configured to be coupled to a respective port of the interface circuitry.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A system as claimed in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein said host comprises at least one of: at least one application; and a protocol processing function, a plurality of which are configured to offload at least a part of respective functionality to said network interface device, wherein said host offload circuitry is configured to provide at least one of said offloaded respective functionality.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A system as claimed in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the host offload circuitry is configured to support a first component of an application, said first component being configured to at least one of provide data to and receive data from a second component of the application, wherein the host device is configured to support the second component of the application.</claim-text></claim></claims></us-patent-application>