<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007208A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007208</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782828</doc-number><date>20201204</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20110101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>3745</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20110101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>353</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20110101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>369</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>3745</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>353</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23241</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>379</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23219</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHODS AND SYSTEMS OF LOW POWER FACIAL RECOGNITION</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16859943</doc-number><date>20200427</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17782828</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62947893</doc-number><date>20191213</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY SEMICONDUCTOR SOLUTIONS CORPORATION</orgname><address><city>Kanagawa</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WONG</last-name><first-name>Ping Wah</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>CHAN</last-name><first-name>Kevin</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>XIAO</last-name><first-name>Sa</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>SONY SEMICONDUCTOR SOLUTIONS CORPORATION</orgname><role>03</role><address><city>Kanagawa</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/045297</doc-number><date>20201204</date></document-id><us-371c12-date><date>20220606</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An image sensor comprises a plurality of pixels. Pixels are capable of detecting a change in an amount of light intensity and pixels are capable of detecting an amount of light intensity. In a first mode the sensor outputs data from the first one or more of the pixels. In a second mode the sensor outputs data from the second one or more of the pixels. The first mode may be a lower power operation mode and the second mode may be a higher power operation mode. At least one of the first mode and the second mode is selected by a processor based on at least one of a result of processing data output in the first mode and a result of processing data output in the second mode.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="222.08mm" wi="146.22mm" file="US20230007208A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="246.97mm" wi="156.97mm" file="US20230007208A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="237.41mm" wi="163.75mm" file="US20230007208A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="179.92mm" wi="151.98mm" file="US20230007208A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="235.29mm" wi="134.70mm" file="US20230007208A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="133.01mm" wi="131.66mm" file="US20230007208A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="225.30mm" wi="162.73mm" orientation="landscape" file="US20230007208A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="200.74mm" wi="154.69mm" file="US20230007208A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="158.58mm" wi="160.36mm" file="US20230007208A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="170.77mm" wi="165.61mm" file="US20230007208A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="213.28mm" wi="163.49mm" file="US20230007208A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="148.93mm" wi="136.40mm" file="US20230007208A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="191.01mm" wi="156.63mm" orientation="landscape" file="US20230007208A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="251.63mm" wi="150.45mm" file="US20230007208A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="252.98mm" wi="167.98mm" file="US20230007208A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="249.51mm" wi="141.65mm" orientation="landscape" file="US20230007208A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to cameras with event based sensor and image sensor capabilities, and to dynamically switch between event based sensors and RGB sensors for low power facial recognition.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">In the related art, RGB sensors are used to generate image data to capture details of a scene. RGB sensors provide imagery which may be used in a number of applications from security to sporting events. RGB sensors, however, may require high rates of power consumption which render the use of RGB sensors in many applications infeasible or undesirable. For example, in some applications a camera may be desired to record video and/or still image data relating to an event. If the event occurs infrequently or if the timing of the event is unknown or not easily predicted, the RGB camera may need to be kept constantly operating at a high frame rate which may make the use of an RGB camera or sensor impractical due to the high rate of power consumption. As a result, to record image data of such an event would, using conventional methods, require a great deal of power.</p><p id="p-0004" num="0003">RGB sensor or cameras are highly valuable assets in the application of facial recognition systems. RGB sensors or cameras with RGB sensors may be used to record images or videos of a scene. RGB data from the sensors or cameras may be analyzed and used to identify one or more faces from the image data. RGB sensors or cameras, however, require a great deal of power as compared to other types of sensors. As such, using a camera system to constantly record and analyze images to recognize facial data requires a high amount of power.</p><p id="p-0005" num="0004">What is needed is an image-capturing system capable of capturing image data relating to an event without requiring an excessive amount of power consumption.</p><heading id="h-0003" level="1">SUMMARY</heading><heading id="h-0004" level="1">Technical Problem</heading><p id="p-0006" num="0005">A camera with a combination of event based sensor (&#x201c;EBS&#x201d;) and RGB sensing capabilities in which the operation of the imaging, or RGB, functions is triggered in response to the detection of an event can overcome some of the limitations of using a regular imaging type device, or RGB sensor, alone to reliably detect events while providing efficient operation. By intelligently using EBS data collected in an EBS operating mode to detect the occurrence of an event or to detect a scenario in which imaging data is desired, the high-power consumption RGB mode may be activated or deactivated only as needed.</p><p id="p-0007" num="0006">As compared to RGB sensors, EBS sensors provide a benefit of lower power operation. EBS sensors are capable of providing high speed object detection while operating in low power. RGB sensors provide for high accuracy color image and/or video while operating at relatively high power compared to EBS sensors. As disclosed herein, when a triggering event is registered based on information received from an EBS sensor, an RGB sensor may be activated. A triggering event may be a detection of an object or a particular type of object in EBS data. RGB mode may be switched to when a triggering event occurs while in an EBS mode. For example, a sensor or camera system in EBS mode may be used to detect a face in one or more EBS frames. The sensor or camera system may be switched to RGB mode to implement a process of facial recognition using image data from the RGB mode.</p><p id="p-0008" num="0007">In some embodiments, a triggering event may be based on data from the EBS sensor being analyzed by a neural network. In some embodiments, a triggering event may be based on EBS event tracking by a recurrent neural network. For example, the EBS data may be fed to a neural network and may trigger an RGB mode when a set of desired object categories, such as a face, is detected.</p><p id="p-0009" num="0008">Therefore, the present disclosure provides cameras, sensor systems, devices, and methods that are capable of providing both imaging and object detection and recognition functions with improved image sensor efficiency and effectiveness as compared to other configurations.</p><heading id="h-0005" level="1">Solution to Problem</heading><p id="p-0010" num="0009">In accordance with embodiments and aspects of the present disclosure, there is provided a camera or a sensor system having EBS and image sensor (e.g. red, green, blue (&#x201c;RGB&#x201d;) image sensor) capabilities or functions. The EBS and image sensor capabilities may be provided by separate EBS and imaging sensor devices. The EBS and image sensing capabilities may also be implemented by a sensor device having combined EBS and imaging sensing capabilities. A sensor device having combined EBS and imaging sensing capabilities can include a sensor device with an array of pixels that includes both EBS and image sensing pixels. Moreover, a combined EBS and image sensor can include photoelectric conversion regions that are provided as part of pixels that perform both EBS and image sensing functions. For ease of description, the discussion herein will refer to EBS and image sensor functions as being provided by separate EBS and image sensors, however, unless specifically stated otherwise, it should be understood that the EBS and image sensors can be integrated into a single sensor device that provides both the EBS and image sensor functions.</p><p id="p-0011" num="0010">As disclosed herein, an event detected by a EBS sensor or by EBS capable pixels may trigger activation of an image sensor or pixels capable of image sensing such as RGB pixels. Moreover, in accordance with at least some embodiments of the present disclosure, activation of an image sensor includes activating the image sensor at a particular framerate. In accordance with at least some embodiments of the present disclosure, an event may comprise detection of an object.</p><p id="p-0012" num="0011">In accordance with at least some embodiments and aspects of the present disclosure, object detection may be performed by a processor in communication with an EBS sensor or EBS capable sensor. The processor may execute a neural network or another type of analysis algorithm. For example, a processor may be capable of analyzing EBS sensor data and detecting an object that is determined to be moving relative to the EBS sensor and in response may be capable of activating or deactivating an RGB sensor or RGB capabilities of certain pixels. In accordance with further embodiments and aspects of the present disclosure, detection of an object that is travelling relatively quickly may result in the activation of an image sensor at a relatively high frame rate. The frame rate of the activated image sensor can also by varied based on characteristics of the object detected by the processor analyzing EBS sensor data. For instance, the image sensor can be operated at a relatively low frame rate where a detected object is moving slowly. The image sensor can be operated at a relatively high frame rate where a detected object is moving quickly.</p><p id="p-0013" num="0012">In accordance with at least some embodiments of the present disclosure, the characteristics of an object detected by the EBS sensor can be analyzed in connection with determining the operating parameters of the image sensor. For instance, a neural network or other decision making facility can determine whether a detected event has been triggered by an object within a desired object category. If a desired object category has been detected, the frame rate of the image sensor may be adjusted based on characteristics of the object. The amount of time for which the image sensor will be activated may also depend on detected characteristics of a detected object. For instance, data from the EBS sensor can be analyzed, for example by a neural network or other decision making facility, to detect a type of object, a speed of the detected object, or other information which may be used to adjust settings for activation of the RGB mode of the sensor.</p><p id="p-0014" num="0013">In general, it is desirable to discontinue operation of the image sensor after a desired condition has occurred and return to EBS sensor operation in order to conserve power. Embodiments and aspects of the present disclosure can discontinue operation of the image sensor and return the system to an EBS mode when certain conditions are satisfied. These can include after a determination is made that nothing of interest is occurring. For instance, imaging of an object can be discontinued, and the image sensor can be returned to sleep mode after an object that was previously moving has stopped. Image sensor operation can also be discontinued after an object has been identified, and it is determined that continued imaging of the identified object is not required or desired. As another example, image sensor operation can be discontinued after an object has moved out of the imaged scene. As still another example, image sensor operation can be discontinued after a predetermined period of time has elapsed. In accordance with embodiments of the present disclosure, EBS sensor operation may remain active continuously, whether or not image sensor operation is active.</p><p id="p-0015" num="0014">The present disclosure can provide cameras, systems, or devices with event based sensing and imaging capabilities that are capable of improved power consumption, data transmission, and data processing efficiencies.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a schematic configuration example of an image sensor in accordance with embodiments of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view illustrating a lamination structure example of an image sensor according to in accordance with embodiments of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating a functional configuration example of an image sensor in accordance with embodiments of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is illustrates an array example of unit pixels in accordance with embodiments of the present disclosure in a case of employing a Bayer array in a color filter array.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> illustrates aspects of an image sensor in accordance with embodiments of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates aspects of an image sensor in accordance with other embodiments of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> illustrates aspects of an image sensor in accordance with other embodiments of the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>5</b>D</figref> illustrates aspects of an image sensor in accordance with other embodiments of the present disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>5</b>E</figref> illustrates aspects of an image sensor in accordance with other embodiments of the present disclosure.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>5</b>F</figref> illustrates aspects of an image sensor in accordance with other embodiments of the present disclosure.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a circuit diagram illustrating a schematic configuration example of a unit pixel with combined event detection and image sensor functions in accordance with embodiments of the present disclosure.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a circuit diagram illustrating a schematic configuration example of a group of image sensing pixels in accordance with embodiments of the present disclosure.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>6</b>C</figref> is a circuit diagram illustrating a schematic configuration example of an event detection pixel in accordance with embodiments of the present disclosure.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram illustrating a schematic configuration example of an address event detection unit in accordance with embodiments of the present disclosure.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a circuit diagram illustrating a schematic configuration example of a subtractor and a quantizer in accordance with embodiments of the present disclosure.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating a schematic configuration example of a column ADC in accordance with embodiments of the present disclosure.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> is a timing chart illustrating an example of an operation of an image sensor in accordance with embodiments of the present disclosure.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> is a timing chart illustrating an example of an operation of an image sensor in accordance with other embodiments of the present disclosure.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating an example of the operation of an imaging device in accordance with embodiments of the present disclosure.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>12</b>A</figref> is a block diagram of an image sensor system with an EBS sensor and an RGB sensor in accordance with embodiments of the present disclosure.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> is a block diagram of an image sensor system with a sensor with both EBS and RGB pixels in accordance with embodiments of the present disclosure.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>12</b>C</figref> is a block diagram of an image sensor system with a sensor with pixels capable of sensing both EBS and RGB data in accordance with embodiments of the present disclosure.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>13</b></figref> depicts block diagram of an image processing system in accordance with embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0039" num="0038">Hereinafter, embodiments of the present disclosure will be described in detail on the basis of the accompanying drawings. Furthermore, in the following embodiments, the same reference numeral will be given to the same or equivalent portion or element, and redundant description thereof will be omitted.</p><p id="p-0040" num="0039">A typical event based sensor (EBS) employs a so-called event-driven type driving method in which the existence or nonexistence of address event ignition is detected for every unit pixel, and a pixel signal and ignition time information are read out from a unit pixel in which the address event ignition is detected.</p><p id="p-0041" num="0040">Furthermore, the unit pixel in this description represents a minimum unit of a pixel or unit pixel including one photoelectric conversion element (also referred to as &#x201c;light-receiving element&#x201d;) and can correspond to each dot in image data that is read out from an image sensor as an example. In addition, the address event represents an event that occurs for every address that is allocable to each of a plurality of the unit pixels which are arranged in a two-dimensional lattice shape. An event detection sensor responds to a change in intensity without being confined to the boundary of the integration time within frames of a traditional image sensor. Intensity change is correlated with a change in photocurrent, and if this change exceeds a constant threshold value it could be detected as an event.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a schematic configuration example of an imaging device according to at least some embodiments of the present disclosure. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, for example, an imaging device <b>100</b> includes an imaging lens <b>110</b>, a solid-state imaging device or image sensor <b>200</b>, a recording unit <b>120</b>, a communication interface <b>124</b>, and a processor system or control system <b>130</b>. The various components of the imaging device <b>100</b> may be interconnected to one another by a communications bus <b>128</b> or signal lines. As examples, the imaging device <b>100</b> can be provided as or as part of a camera that is mounted in an industrial robot, an in-vehicle camera, or as part of or in connection with other devices or instruments.</p><p id="p-0043" num="0042">The imaging lens <b>110</b> can include an optical system that collects light from within a field of view <b>114</b>. The collected or incident light is directed (e.g. condensed) onto a light-receiving surface of the image sensor <b>200</b>. In particular, the imaging lens <b>110</b> can collect light from within a selected area of a scene by directing the field of view <b>114</b> to encompass that portion of the scene. The light-receiving surface is a surface of a substrate on which photoelectric conversion elements of pixels <b>310</b> included in the image sensor <b>200</b> are arranged. The image sensor <b>200</b> photoelectrically converts the incident light to generate image data. As discussed herein, the image sensor <b>200</b> can include different sets of photoelectric conversion elements disposed on the same or different substrates. Moreover, the image sensor <b>200</b> can include photoelectric conversion elements that perform single or multiple functions. These functions can include event detection and imaging functions. In addition, the image sensor <b>200</b> can execute predetermined signal processing such as noise removal and white balance adjustment with respect to the generated image data. A result obtained by the signal processing and a detection signal indicating the existence or nonexistence of an address event ignition and ignition time information can be output by the image sensor <b>200</b> to the processor system <b>130</b>. A method of generating the detection signal indicating the existence or nonexistence of the address event ignition will be described later.</p><p id="p-0044" num="0043">The recording system <b>120</b> is, for example, constituted by a flash memory, a dynamic random access memory (DRAM), a static random access memory (SRAM), or the like, and records data provided from the image sensor <b>200</b>.</p><p id="p-0045" num="0044">The processor system <b>130</b> is, for example, constituted by a central processing unit (CPU) and the like. For example, the processor system <b>130</b> can include one or more general purpose processors, controllers, field programmable gate arrays (FPGAs), graphical processing units (GPUs), application specific integrated circuits (ASIC), or combinations thereof. Moreover, the processor system <b>130</b> can execute application programming or routines, stored as software or firmware in memory or data storage included in or interconnected to the processor system <b>130</b> to perform various functions and methods as described herein. For example, the processor system <b>130</b> can process data output from the image sensor <b>200</b>. For example, as described herein, the processor system <b>130</b> can process event detection signals output by the EBS sensor function or portion of the image sensor <b>200</b> and can control the imaging sensor function or portion of the solid-state imaging device, at least in part in response to the event detection signals. The processor system <b>130</b> can also control components of the imaging device <b>100</b> in addition to the image sensor <b>200</b>, such as the operation of the recording unit <b>120</b>, the communication interface <b>124</b>, focusing and shutter operations that might be supported by the imaging lens <b>110</b>, and the like. In accordance with further embodiments of the present disclosure, the processor system <b>130</b> can implement advanced processing capabilities, including but not limited to neural network and artificial intelligence capabilities and functions, as described herein.</p><p id="p-0046" num="0045">Next, a configuration example of the image sensor <b>200</b> will be described in detail with reference to the accompanying drawings.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view illustrating a lamination structure example of an image sensor <b>200</b> in accordance with at least some embodiments of the present disclosure. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the image sensor <b>200</b> can have a structure in which a light-receiving chip <b>201</b> and a logic chip <b>202</b> are vertically laminated. A side of the light receiving chip <b>201</b> opposite the logic chip <b>202</b> is a light receiving surface <b>204</b>. In joining of the light-receiving chip <b>201</b> and the logic chip <b>202</b>, for example, so-called direct joining in which joining surfaces of the chips are planarized, and the chips are laminated with an inter-electron force can be used. However, there is no limitation thereto, and for example, so-called Cu&#x2014;Cu joining in which copper (Cu) electrode pads formed on joining surfaces are bonded, bump joining, and the like can also be used.</p><p id="p-0048" num="0047">In addition, the light-receiving chip <b>201</b> and the logic chip <b>202</b> are electrically connected to each other, for example, through a connection portion such as a through-silicon via (TSV) that penetrates through a semiconductor substrate. In the connection using the TSV, for example, a so-called twin TSV method in which two TSVs including a TSV that is formed in the light-receiving chip <b>201</b> and a TSV that is formed from the light-receiving chip <b>201</b> to the logic chip <b>202</b> are connected to each other on chip external surfaces, a so-called shared TSV method in which the light-receiving chip <b>201</b> and the logic chip <b>202</b> are connected with a TSV that penetrates through both the chips, and the like can be employed.</p><p id="p-0049" num="0048">However, in the case of using the Cu&#x2014;Cu joining or the bump joining in the joining of the light-receiving chip <b>201</b> and the logic chip <b>202</b>, both the light-receiving chip <b>201</b> and the logic chip <b>202</b> are electrically connected to each other through a Cu&#x2014;Cu joint or a bump joint.</p><p id="p-0050" num="0049">As can be appreciated by one of skill in the art after consideration of the present disclosure, an imaging device <b>200</b> implemented as connected light receiving <b>201</b> and logic <b>202</b> chips can include image sensor <b>200</b> components disposed as part of the light receiving chip <b>201</b>, with some or all of the processor system <b>130</b> components disposed as part of the logic chip <b>202</b>. Other components, such as the recording unit <b>120</b> and communication interface components can be distributed amongst one or both of the chips <b>201</b> and <b>202</b>. In accordance with still other embodiments, a data storage or other chip can be laminated and electrically connected to the light receiving <b>201</b> and logic <b>202</b> chips. Moreover, the light receiving chip can include multiple substrates joined to respective logic chips <b>202</b> or to a common logic chip <b>202</b>, for example where the image sensor <b>200</b> includes multiple sensor devices.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating a functional configuration example of the image sensor <b>200</b> according to at least some embodiments of the present disclosure. As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the image sensor <b>200</b> can include a drive circuit <b>211</b>, a signal processor <b>212</b>, an arbiter <b>213</b>, a column ADC <b>220</b>, and a pixel array <b>300</b>. Some or all of the components can be entirely or partially integrated into, or implemented by, the processor system <b>130</b>.</p><p id="p-0052" num="0051">A plurality of unit cells or pixels <b>310</b>, also referred to herein simply as pixels <b>310</b>, are arranged in the pixel array <b>300</b>. Details of the unit pixels <b>310</b> will be described later. For example, each of the unit pixels <b>310</b> includes a photoelectric conversion element such as a photodiode, and a circuit that generates a pixel signal of a voltage value corresponding to the amount of charge generated in the photoelectric conversion element, hereinafter, referred to as a pixel circuit. Moreover, as discussed in greater detail elsewhere herein, the pixel circuit can include either or both of a first or imaging signal generation circuit and a second or address event detection readout circuit. Each photoelectric conversion element can be associated with a respective pixel circuit, or multiple photoelectric conversion elements can be associated with a common pixel circuit.</p><p id="p-0053" num="0052">In this example, the plurality of unit pixels <b>310</b> are arranged in the pixel array <b>300</b> in a two-dimensional lattice shape. The plurality of unit pixels <b>310</b> may be grouped into a plurality of pixel blocks or groups, each including a predetermined number of unit pixels. Hereinafter, an assembly of unit pixels which are arranged in a horizontal direction is referred to as a &#x201c;row,&#x201d; and an assembly of unit pixels which are arranged in a direction orthogonal to the row is referred to as a &#x201c;column.&#x201d;</p><p id="p-0054" num="0053">Each of the unit pixels <b>310</b> generates charges corresponding to an amount of light received at the respective photoelectric conversion element. In addition, at least some of the unit pixels <b>310</b> can be operated to detect the existence or nonexistence of address event ignition on the basis of whether or not a value of a current (hereinafter referred to as a photocurrent) produced by charges generated in the photoelectric conversion element or a variation amount thereof exceeds a predetermined threshold value. When the address event is ignited, a signal is output to the arbiter <b>213</b>.</p><p id="p-0055" num="0054">The arbiter <b>213</b> arbitrates requests received from the unit pixels <b>310</b> performing the event detection function and transmits a predetermined response to the unit pixel <b>310</b> which issues the request on the basis of the arbitration result. The unit pixel <b>310</b> which receives the response supplies a detection signal indicating the existence or nonexistence of the address event ignition (hereinafter, simply referred to as &#x201c;address event detection signal&#x201d;) to the drive circuit <b>211</b> and the signal processor <b>212</b>.</p><p id="p-0056" num="0055">The drive circuit <b>211</b> drives each of the unit pixels <b>310</b> and allows each of the unit pixels <b>310</b> to output a pixel signal to the column ADC <b>220</b>.</p><p id="p-0057" num="0056">For every unit pixel <b>310</b> column, the column ADC <b>220</b> converts an analog pixel signal from the column into a digital signal. In addition, the column ADC <b>220</b> supplies a digital signal generated through the conversion to the signal processor <b>212</b>.</p><p id="p-0058" num="0057">The signal processor <b>212</b> executes predetermined signal processing such as correlated double sampling (CDS) processing (noise removal) and white balance adjustment with respect to the digital signal transmitted from the column ADC <b>220</b>. In addition, the signal processor <b>212</b> supplies a signal processing result and an address event detection signal to the recording unit <b>120</b> through the signal line <b>209</b>.</p><p id="p-0059" num="0058">The unit pixels <b>310</b> within the pixel array unit <b>300</b> may be disposed in pixel groups <b>314</b>. In the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, for example, the pixel array unit <b>300</b> is constituted by pixel groups <b>314</b> that include an assembly of unit pixels <b>310</b> that receive wavelength components necessary to reconstruct color information from a scene. For example, in the case of reconstructing a color on the basis of three primary colors of RGB, in the pixel array unit <b>300</b>, optical color filter materials can be deposited onto the pixels according to a predetermined color filter array to control light of desired wavelengths to reach the pixel surface. Specifically, a unit pixel <b>310</b> that receives light of a red (R) color, a unit pixel <b>310</b> that receives light of a green (G) color, and a unit pixel <b>310</b> that receives light of a blue (B) color are arranged in groups <b>314</b><i>a </i>according to the predetermined color filter array.</p><p id="p-0060" num="0059">Examples of the color filter array configurations include various arrays or pixel groups such as a Bayer array of 2&#xd7;2 pixels, a color filter array of 3&#xd7;3 pixels which is employed in an X-Trans (registered trademark) CMOS sensor (hereinafter, also referred to as &#x201c;X-Trans (registered trademark) type array&#x201d;), a Quad Bayer array of 4&#xd7;4 pixels (also referred to as &#x201c;Quadra array&#x201d;), and a color filter of 4&#xd7;4 pixels in which a white RGB color filter is combined to the Bayer array (hereinafter, also referred to as &#x201c;white RGB array&#x201d;). In addition, and as discussed in greater detail elsewhere herein, event detection pixels can be interspersed or included within the pixel array <b>300</b>. As also discussed in greater detail elsewhere herein, the event detection pixels may be provided as dedicated event detection pixels, which only perform an event detection function, or as combined event detection and image sensing pixels, which perform both event detection and image sensor functions.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic view illustrating an array example of unit pixels <b>310</b> in the case of employing pixel groups <b>314</b> with an arrangement of unit pixels <b>310</b> and associated color filters in the color filter array configured to form a plurality of Bayer arrays <b>310</b>A. As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in the case of employing the Bayer array as the color filter array configuration, in the pixel array <b>300</b>, a basic pattern <b>310</b>A including a total of four unit pixels <b>310</b> of 2&#xd7;2 pixels is repetitively arranged in a column direction and a row direction. For example, the basic pattern <b>310</b>A is constituted by a unit pixel <b>310</b>R including a color filter <b>401</b> of a red (R) color, a unit pixel <b>310</b>Gr including a color filter <b>401</b> of a green (Gr) color, a unit pixel <b>310</b>Gb including a color filter <b>401</b> of a green (Gb) color, and a unit pixel <b>310</b>B including a color filter <b>401</b> of a blue (B) color.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>D</figref> depict various configuration examples of an imaging device <b>100</b>, and in particular of arrangements of a solid-state imaging device or image sensor <b>200</b> pixels, in accordance with embodiments of the present disclosure. More particularly, <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> depicts an image sensor <b>200</b> having a first or EBS sensor <b>530</b>, which includes an array <b>300</b> of pixels <b>310</b> in the form of address event detection pixels <b>503</b> disposed on a first light receiving chip or substrate <b>201</b><i>a</i>, and a second or imaging sensor <b>540</b>, which includes an array <b>300</b> of pixels <b>310</b> in the form of image sensing pixels <b>502</b> disposed on a second light receiving chip or substrate <b>201</b><i>b</i>. As can be appreciated by one of skill in the art after consideration of the present disclosure, an imaging device <b>100</b> including separate EBS <b>530</b> and imaging <b>540</b> sensors can be configured with separate lens assemblies <b>110</b> that collect light from within the same or similar fields of view, or can be configured with a shared lens assembly <b>110</b> that directs light to the sensors <b>530</b> and <b>540</b> via a beam splitter. In accordance with embodiments of the present disclosure, the number of address event detection pixels <b>503</b> included in the EBS sensor <b>530</b> can be equal to the number of image sensing pixels <b>502</b> included in the imaging sensor <b>540</b>. Moreover, the area of each address event detection pixel <b>503</b> can be the same as the area of each image sensing pixel <b>502</b>. Alternatively, the EBS sensor <b>530</b> and the imaging sensor <b>540</b> can have different numbers of pixels <b>310</b>. For example, the image sensor <b>200</b> can include a EBS sensor <b>530</b> having a relatively low number of event detection pixels <b>503</b>, thereby providing a relatively low resolution, and an imaging sensor <b>540</b> having a relatively high number of image sensing pixels <b>502</b>, thereby providing a relatively high resolution. In accordance with at least some embodiments of the present disclosure, event detection and image sensing operations can be performed simultaneously.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> depicts image sensor <b>200</b> with pixels <b>310</b> configured as combined or shared event detection and image sensing pixels <b>501</b> disposed on a single light receiving chip or substrate <b>201</b>. As can be appreciated by one of skill in the art after consideration of the present disclosure, the shared event detection and image sensing pixels <b>501</b> can be selectively operated in event detection or image sensing modes. Moreover, in accordance with at least some embodiments of the present disclosure, event detection and image sensing operations can be performed simultaneously with some pixels operating in event detection mode and some pixel operating in image sensing mode.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> depicts image sensor <b>200</b> having an array of unit pixels <b>310</b> that includes a plurality of event detection pixels <b>503</b> and a plurality of image sensing pixels <b>502</b> formed on the same light receiving chip or substrate <b>201</b>. In the illustrate example, the majority of the unit pixels are in the form of image sensing pixels <b>502</b>, with a smaller number of event detection pixels <b>503</b> disposed amongst the image sensing pixels <b>502</b>. However, an image sensor <b>200</b> having both event detection <b>503</b> and image sensing <b>502</b> pixels disposed on the same light receiving chip or substrate <b>201</b> can include the same number of pixels <b>502</b> and <b>503</b> or can have more event detection pixels <b>503</b> than image sensing pixels <b>502</b>. In accordance with at least some embodiments of the present disclosure, event detection and image sensing operations can be performed simultaneously.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>5</b>D</figref> depicts an image sensor <b>200</b> having an array of unit pixels <b>310</b> that includes groups of shared event detection and image sensing pixels <b>501</b>, and groups of image sensing pixels <b>502</b>, formed on the same light receiving chip or substrate <b>201</b>. The individual groups can be configured as Bayer arrays that alternate between Bayer array groups of shared event detection and image sensing pixels <b>501</b>, and Bayer array groups of image sensing pixels <b>502</b>. Accordingly, <figref idref="DRAWINGS">FIG. <b>5</b>D</figref> is an example of an image sensor <b>200</b> in which different shared event detection and image sensing pixels <b>501</b> can respond to light within different wavelength ranges. For example, the shared event detection and image sensing pixels <b>501</b> can be associated with color filters. Alternatively, the shared pixels <b>501</b> can all receive light within the same wavelength range. Although an equal number of groups containing equal numbers of respective pixels <b>310</b> are depicted in the figure, other configurations are possible. As can be appreciated by one of skill in the art after consideration of the present disclosure, the shared event detection and image sensing pixels <b>501</b> can be selectively operated in event detection or image sensing modes. Moreover, in accordance with at least some embodiments of the present disclosure, event detection and image sensing operations can be performed simultaneously.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>5</b>E</figref> depicts an image sensor <b>200</b> having an array of unit pixels <b>310</b> that includes groups of shared event detection and image sensing pixels <b>501</b>, and groups of event detection pixels <b>503</b>, formed on the same light receiving chip or substrate <b>201</b>. The individual groups of shared event detection and image sensing pixels can be configured as Bayer arrays that alternate with groups of event detection pixels <b>503</b>. Although an equal number of groups containing equal numbers of respective pixels <b>310</b> are depicted in the figure, other configurations are possible. As can be appreciated by one of skill in the art after consideration of the present disclosure, the shared event detection and image sensing pixels <b>501</b> can be selectively operated in event detection or image sensing modes. Moreover, in accordance with at least some embodiments of the present disclosure, event detection and image sensing operations can be performed simultaneously.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>5</b>F</figref> depicts an image sensor <b>200</b> having an array of unit pixels <b>310</b> that includes groups of shared event detection and image sensing pixels <b>501</b>, groups of image sensing pixels <b>502</b>, and groups of event detection pixels <b>503</b>, all formed on the same light receiving chip or substrate <b>201</b>. Some or all of the individual groups of pixels can be configured as Bayer arrays. For instance, in at least one example configuration, groups of shared event detection and image sensing pixels <b>501</b> and groups of image sensing pixels can be configured as Bayer arrays, while each of the event detection pixels <b>503</b> can be configured to receive light from within the same wavelength range. For example, the shared event detection and image sensing pixels <b>501</b> and the image sensing pixels can be associated with color filters, and the event detection pixels <b>503</b> can be without color filters. Although an arrangement in which &#xbd; of the pixels <b>310</b> are shared event detection and image sensing pixels <b>501</b>, &#xbc; of the pixels <b>310</b> are image sensing pixels <b>502</b>, and &#xbc; of the pixels <b>310</b> are event detection pixels <b>503</b>, other configurations are possible. As can be appreciated by one of skill in the art after consideration of the present disclosure, the shared event detection and image sensing pixels <b>501</b> can be selectively operated in event detection or image sensing modes. Moreover, in accordance with at least some embodiments of the present disclosure, event detection and image sensing operations can be performed simultaneously.</p><p id="p-0068" num="0067">Next, a configuration example of a unit pixel <b>310</b> will be described. <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a circuit diagram illustrating a schematic configuration example of the unit pixel <b>310</b> according to at least some embodiments of the present disclosure, and in particular in accordance with embodiments that include pixels <b>310</b> configured as combined or shared event detection (EBS) and image sensor (IS) pixels <b>501</b> that perform both event detection and image sensor functions. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, the unit pixel <b>310</b> includes, for example, a pixel imaging signal generation unit (or readout circuit) <b>320</b>, a light-receiving unit <b>330</b>, and an address event detection unit (or readout circuit) <b>400</b>. According to at least one example embodiment, the event detection readout circuit <b>400</b> can trigger operation of the image signal generation readout circuit <b>320</b> based on charge generated by a photoelectric conversion element (or photoelectric conversion region) <b>333</b> and based on operation of the logic circuit <b>210</b>. The logic circuit <b>210</b> in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a logic circuit including, for example, the drive circuit <b>211</b>, the signal processor <b>212</b>, and the arbiter <b>213</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In accordance with at least some embodiments of the present disclosure, the logic circuit can be implemented in the processor system <b>130</b>. As described in greater detail elsewhere herein, the logic circuit <b>210</b> can make determinations as to whether to trigger operation of the image signal generation readout circuit <b>320</b> or the operation of image signal generation circuits <b>320</b> associated with other unit pixels <b>310</b> based on the output of the event detection readout circuit <b>400</b> or the output of other event detection readout circuits <b>400</b>.</p><p id="p-0069" num="0068">For example, the light-receiving unit <b>330</b> includes a first or imaging transmission transistor or gate (first transistor) <b>331</b>, a second or address event detection transmission transistor or gate (second transistor) <b>332</b>, and a photoelectric conversion element <b>333</b>. A first transmission or control signal TG1 transmitted from the drive circuit <b>211</b> is selectively supplied to a gate of the first transmission transistor <b>331</b> of the light-receiving unit <b>330</b>, and a second transmission or control signal TG2 transmitted from the drive circuit <b>211</b> is selectively supplied to a gate of the second transmission transistor <b>332</b>. An output through the first transmission transistor <b>331</b> of the light-receiving unit <b>330</b> is connected to the pixel imaging signal generation unit <b>320</b>, and an output through the second transmission transistor <b>332</b> is connected to the address event detection unit <b>400</b>.</p><p id="p-0070" num="0069">The pixel imaging signal generation unit <b>320</b> can include a reset transistor (third transistor) <b>321</b>, an amplification transistor (fourth transistor) <b>322</b>, a selection transistor (fifth transistor) <b>323</b>, and a floating diffusion layer (FD) <b>324</b>.</p><p id="p-0071" num="0070">In accordance with at least some embodiments of the present disclosure, the first transmission transistor <b>331</b> and the second transmission transistor <b>332</b> of the light-receiving unit <b>330</b> are constituted, for example, by using an N-type metal-oxide-semiconductor (MOS) transistor (hereinafter, simply referred to as &#x201c;NMOS transistor&#x201d;). Similarly, the reset transistor <b>321</b>, the amplification transistor <b>322</b>, and the selection transistor <b>323</b> of the pixel imaging signal generation unit <b>320</b> are each constituted, for example, by using the NMOS transistor.</p><p id="p-0072" num="0071">The address event detection unit <b>400</b> can include a current-voltage conversion unit <b>410</b> and a subtractor <b>430</b>. The address event detection unit <b>400</b> can further be provided with a buffer, a quantizer, and a transmission unit. Details of the address event detection unit <b>400</b> will be described in the following description in connection with <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0073" num="0072">In the illustrated configuration, the photoelectric conversion element <b>333</b> of the light-receiving unit <b>330</b> photoelectrically converts incident light to generate a charge. The first transmission transistor <b>331</b> transmits a charge generated in the photoelectric conversion element <b>333</b> to the floating diffusion layer <b>324</b> of the image signal generation readout circuit <b>320</b> in accordance with the first control signal TG1. The second transmission transistor <b>332</b> supplies an electric signal (photocurrent) based on the charge generated in the photoelectric conversion element <b>333</b> to the address event detection unit <b>400</b> in accordance with the second control signal TG2.</p><p id="p-0074" num="0073">When an instruction for image sensing is given by the processor system <b>130</b>, the drive circuit <b>211</b> in the logic circuit <b>210</b> outputs the control signal TG1 for setting the first transmission transistor <b>331</b> of the light-receiving unit <b>330</b> of selected unit pixels <b>310</b> in the pixel array <b>300</b> to an ON-state. With this arrangement, a photocurrent generated in the photoelectric conversion element <b>333</b> of the light-receiving unit <b>330</b> is supplied to the pixel imaging signal generation readout circuit <b>320</b> through the first transmission transistor <b>331</b>. More particularly, the floating diffusion layer <b>324</b> accumulates charges transmitted from the photoelectric conversion element <b>333</b> through the first transmission transistor <b>331</b>. The reset transistor <b>321</b> discharges (initializes) the charges accumulated in the floating diffusion layer <b>324</b> in accordance with a reset signal transmitted from the drive circuit <b>211</b>. The amplification transistor <b>322</b> allows a pixel signal of a voltage value corresponding to an amount of charge accumulated in the floating diffusion layer <b>324</b> to appear in a vertical signal line VSL. The selection transistor <b>323</b> switches a connection between the amplification transistor <b>322</b> and the vertical signal line VSL in accordance with a selection signal SEL transmitted from the drive circuit <b>211</b>. Furthermore, the analog pixel signal that appears in the vertical signal line VSL is read out by the column ADC <b>220</b> and is converted into a digital pixel signal.</p><p id="p-0075" num="0074">When an instruction for address event detection initiation is given by the processor system <b>130</b>, the drive circuit <b>211</b> in the logic circuit <b>210</b> outputs the control signal for setting the second transmission transistor <b>332</b> of the light-receiving unit <b>330</b> in the pixel array unit <b>300</b> to an ON-state. With this arrangement, a photocurrent generated in the photoelectric conversion element <b>333</b> of the light-receiving unit <b>330</b> is supplied to the address event detection unit <b>400</b> of each unit pixel <b>310</b> through the second transmission transistor <b>332</b>.</p><p id="p-0076" num="0075">When detecting address event ignition on the basis of the photocurrent from the light-receiving unit <b>330</b>, the address event detection unit <b>400</b> of each unit pixel <b>310</b> outputs a request to the arbiter <b>213</b>. With respect to this, the arbiter <b>213</b> arbitrates the request transmitted from each of the unit pixels <b>310</b> and transmits a predetermined response to the unit pixel <b>310</b> that issues the request on the basis of the arbitration result. The unit pixel <b>310</b> that receives the response supplies a detection signal indicating the existence or nonexistence of the address event ignition (hereinafter, referred to as &#x201c;address event detection signal&#x201d;) to the drive circuit <b>211</b> and the signal processor <b>212</b> in the logic circuit <b>210</b>.</p><p id="p-0077" num="0076">The drive circuit <b>211</b> can also set the second transmission transistor <b>332</b> in the unit pixel <b>310</b> that is a supply source of the address event detection signal to an OFF-state. With this arrangement, a supply of the photocurrent from the light-receiving unit <b>330</b> to the address event detection unit <b>400</b> in the unit pixel <b>310</b> is stopped.</p><p id="p-0078" num="0077">Next, the drive circuit <b>211</b> sets the first transmission transistor <b>331</b> in the light-receiving unit <b>330</b> of the unit pixel <b>310</b> to an ON-state by the transmission signal TG1. With this arrangement, a charge generated in the photoelectric conversion element <b>333</b> of the light-receiving unit <b>330</b> is transmitted to the floating diffusion layer <b>324</b> through the first transmission transistor <b>331</b>. In addition, a pixel signal of a voltage value corresponding to a charge amount of charges accumulated in the floating diffusion layer <b>324</b> appears in the vertical signal line VSL that is connected to the selection transistor <b>323</b> of the pixel imaging signal generation unit <b>320</b>.</p><p id="p-0079" num="0078">As described above, in the image sensor<b>200</b>, a pixel signal SIG is output from the unit pixel <b>310</b> in which the address event ignition is detected to the column ADC <b>220</b>. In accordance with further embodiments of the present disclosure, a pixel signal is output from the unit pixels <b>310</b> within a group or sub array of unit pixels <b>310</b> associated with the address of the unit pixel <b>310</b> from which an address event detection signal has been provided.</p><p id="p-0080" num="0079">Furthermore, for example, the light-receiving unit <b>330</b>, the pixel imaging signal generation unit <b>320</b>, and two log (LG) transistors (sixth and seventh transistors) <b>411</b> and <b>414</b> and two amplification transistors (eighth and ninth transistors) <b>412</b> and <b>413</b> in the current-voltage conversion unit <b>410</b> of the address event detection unit <b>400</b> are disposed, for example, in the light-receiving chip <b>201</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and other components can be disposed, for example, in the logic chip <b>202</b> that is joined to the light-receiving chip <b>201</b> through the Cu&#x2014;Cu joining. Therefore, in the following description, in the unit pixel <b>310</b>, configurations which are disposed in the light-receiving chip <b>201</b> are referred to as &#x201c;upper layer circuit&#x201d;.</p><p id="p-0081" num="0080">A configuration example of a group of unit pixels <b>310</b> configured as image sensing pixels <b>502</b> with a shared pixel imaging signal generation readout circuitry <b>320</b> in accordance with at least some embodiments of the present disclosure is depicted in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>. In this example, each photoelectric conversion element <b>333</b> is selectively connected to the floating diffusion <b>324</b> via a respective transfer gate <b>331</b>. In addition, the components of the pixel imaging signal readout circuit <b>320</b> are shared by the photoelectric conversion units <b>333</b>. In this example, four photoelectric conversion units <b>333</b><i>a</i>-<b>333</b><i>d</i>, and four corresponding transfer gates <b>331</b><i>a</i>-<b>331</b><i>d</i>, are shown. However, any number of photoelectric conversion units <b>333</b> and respective transfer gates <b>331</b> can be included in connection with a shared pixel imaging signal readout circuit <b>320</b>.</p><p id="p-0082" num="0081">A configuration example of a unit pixel <b>310</b> configured as a single function address event detection pixel <b>503</b> and associated address event detection readout circuit <b>400</b> elements is depicted in <figref idref="DRAWINGS">FIG. <b>6</b>C</figref>. As shown, this example includes a single photoelectric conversion element <b>333</b> selectively connected by a transfer gate <b>332</b> to components of an address event detection readout circuit <b>400</b>. An event scan control block <b>415</b> controls operation of the address event detection readout circuit <b>400</b>.</p><p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram illustrating a schematic configuration example of the address event detection unit <b>400</b> according to at least some embodiments of the present disclosure. As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the address event detection unit <b>400</b> includes a current-voltage conversion unit <b>410</b>, a buffer <b>420</b>, a subtractor <b>430</b>, a quantizer <b>440</b>, and a transmission unit <b>450</b>. The current-voltage conversion unit <b>410</b> converts the photocurrent from the light-receiving unit <b>330</b> into a voltage signal and supplies the voltage signal generated through the conversion to the buffer <b>420</b>. The buffer <b>420</b> corrects the voltage signal transmitted from the current-voltage conversion unit <b>410</b>, and outputs a voltage signal after correction to the subtractor <b>430</b>. The subtractor <b>430</b> lowers a voltage level of the voltage signal transmitted from the buffer <b>420</b> in accordance with a row drive signal transmitted from the drive circuit <b>211</b> and, supplies the lowered voltage signal to the quantizer <b>440</b>. The quantizer <b>440</b> quantizes the voltage signal transmitted from the subtractor <b>430</b> into a digital signal, and outputs the digital signal generated through the quantization to the transmission unit <b>450</b> as a detection signal. The transmission unit <b>450</b> transmits the detection signal transmitted from the quantizer <b>440</b> to the signal processor <b>212</b> and the like. For example, when address event ignition is detected, the transmission unit <b>450</b> supplies a request for transmission of an address event detection signal from the transmission unit <b>450</b> to the drive circuit <b>211</b> and the signal processor <b>212</b> to the arbiter <b>213</b>. In addition, when receiving a response with respect to the request from the arbiter <b>213</b>, the transmission unit <b>450</b> supplies the detection signal to the drive circuit <b>211</b> and the signal processor <b>212</b>.</p><p id="p-0084" num="0083">The current-voltage conversion unit <b>410</b> in the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref> can include the two LG transistors <b>411</b> and <b>414</b>, the two amplification transistors <b>412</b> and <b>413</b>, and a constant-current circuit <b>415</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>. For example, a source of the LG transistor <b>411</b> and a gate of the amplification transistor <b>413</b> are connected to a drain of the second transmission transistor <b>332</b> of the light-receiving unit <b>330</b>. In addition, for example, a drain of the LG transistor <b>411</b> is connected to a source of the LG transistor <b>414</b> and a gate of the amplification transistor <b>412</b>. For example, a drain of the LG transistor <b>414</b> is connected to a power supply terminal VDD. In addition, for example, a source of the amplification transistor <b>413</b> is grounded, and a drain thereof is connected to a gate of the LG transistor <b>411</b> and a source of the amplification transistor <b>412</b>. For example, a drain of the amplification transistor <b>412</b> is connected to a power supply terminal VDD through the constant-current circuit <b>415</b>. For example, the constant-current circuit <b>415</b> is constituted by a load MOS transistor such as a p-type MOS transistor. In this connection relationship, a loop-shaped source follower circuit is constructed. With this arrangement, a photocurrent from the light-receiving unit <b>330</b> is converted into a voltage signal in a logarithmic value corresponding to a charge amount thereof. Furthermore, the LG transistors <b>411</b> and <b>414</b>, and the amplification transistors <b>412</b> and <b>413</b> may be each constituted, for example, by an NMOS transistor.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a circuit diagram illustrating a schematic configuration example of the subtractor <b>430</b> and the quantizer <b>440</b> according to at least some embodiments of the present disclosure. As illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the subtractor <b>430</b> includes capacitors <b>431</b> and <b>433</b>, an inverter <b>432</b>, and a switch <b>434</b>. In addition, the quantizer <b>440</b> includes a comparator <b>441</b>. One end of the capacitor <b>431</b> is connected to an output terminal of the buffer <b>420</b>, and the other end is connected to an input terminal of the inverter <b>432</b>. The capacitor <b>433</b> is connected to the inverter <b>432</b> in parallel. The switch <b>434</b> opens or closes a route connecting both ends of the capacitor <b>433</b> in accordance with a row drive signal. The inverter <b>432</b> inverts a voltage signal that is input through the capacitor <b>431</b>. The inverter <b>432</b> outputs an inverted signal to a non-inverting input terminal (+) of the comparator <b>441</b>. When the switch <b>434</b> is turned on, a voltage signal Vinit is input to a buffer <b>420</b> side of the capacitor <b>431</b>. In addition, the opposite side becomes a virtual ground terminal. A potential of the virtual ground terminal is set to zero for convenience. At this time, when a capacity of the capacitor <b>431</b> is set as C1, a potential Qinit that is accumulated in the capacitor <b>431</b> is expressed by the following Expression (1). On the other hand, both ends of the capacitor <b>433</b> are short-circuited, and thus an accumulated charge thereof becomes zero.</p><p id="p-0086" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Qinit=<i>C</i>1&#xd7;Vinit&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0087" num="0085">Next, when considering a case where the switch <b>434</b> is turned off, and a voltage of the capacitor <b>431</b> on the buffer <b>420</b> side varies and reaches Vafter, a charge Qafter accumulated in the capacitor <b>431</b> is expressed by the following Expression (2).</p><p id="p-0088" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Qafter=<i>C</i>1&#xd7;Vafter&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0089" num="0086">On the other hand, when an output voltage is set as Vout, a charge Q2 accumulated in the capacitor <b>433</b> is expressed by the following Expression (3).</p><p id="p-0090" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Q</i>2=&#x2212;<i>C</i>2&#xd7;<i>V</i>out&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0091" num="0087">At this time, a total charge amount of the capacitors <b>431</b> and <b>433</b> does not vary, and thus the following Expression (4) is established.</p><p id="p-0092" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Qinit=Qafter+<i>Q</i>2&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0093" num="0088">When Expression (1) to Expression (3) are substituted for Expression (4), the following Expression (5) is obtained.</p><p id="p-0094" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>V</i>out=&#x2212;(<i>C</i>1/<i>C</i>2)&#xd7;(Vafter&#x2212;Vinit)&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0095" num="0089">Expression (5) represents a subtraction operation of a voltage signal, and a gain of the subtraction result becomes C1/C2. Typically, it is desired to maximize (or alternatively, improve) the gain, and thus it is preferable to make a design so that C1 becomes large and C2 becomes small. On the other hand, when C2 is excessively small, kTC noise increases, and thus there is a concern that noise characteristics deteriorate. Accordingly, a reduction in the capacity of C2 is limited to a range capable of permitting noise. In addition, since the address event detection unit <b>400</b> including the subtractor <b>430</b> is mounted for every unit pixel <b>310</b>, a restriction on an area is present in capacities C1 and C2. Values of the capacities C1 and C2 are determined in consideration of the restriction.</p><p id="p-0096" num="0090">The comparator <b>441</b> compares a voltage signal transmitted from the subtractor <b>430</b> and a predetermined threshold voltage Vth that is applied to an inverting input terminal (&#x2212;). The comparator <b>441</b> outputs a signal indicating the comparison result to the transmission unit <b>450</b> as a detection signal. In addition, when a conversion gain by the current-voltage conversion unit <b>410</b> is set as CG<sub>log</sub>, and a gain of the buffer <b>420</b> is set to &#x201c;1&#x201d;, a gain A of the entirety of the address event detection unit <b>400</b> is expressed by the following Expression (6).</p><p id="p-0097" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Math</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>1</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mi>A</mi>     <mo>=</mo>     <mrow>      <mfrac>       <mrow>        <mi>C</mi>        <mo>&#x2062;</mo>        <mrow>         <msub>          <mi>G</mi>          <mi>log</mi>         </msub>         <mo>&#xb7;</mo>         <mi>C</mi>        </mrow>        <mo>&#x2062;</mo>        <mn>1</mn>       </mrow>       <mrow>        <mi>C</mi>        <mo>&#x2062;</mo>        <mn>2</mn>       </mrow>      </mfrac>      <mo>&#x2062;</mo>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>n</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>N</mi>       </munderover>       <mrow>        <msub>         <mi>i</mi>         <mi>photo</mi>        </msub>        <mo>&#x2062;</mo>        <mi>_n</mi>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0098" num="0091">In Expression (6), i<sub>photo_</sub>n represents a photocurrent of an nth unit pixel <b>310</b>, and a unit thereof is, for example, an ampere (A). N represents the number of the unit pixels <b>310</b> in a pixel block and is &#x201c;1&#x201d; in this embodiment.</p><p id="p-0099" num="0092"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating a schematic configuration example of the column ADC according to at least some embodiments of the present disclosure. The column ADC <b>220</b> includes a plurality of ADCs <b>230</b> which are provided for every column of the unit pixels <b>310</b>. Each of the ADCs <b>230</b> converts an analog pixel signal that appears in the vertical signal line VSL into a digital signal. For example, the pixel signal is converted into a digital signal in which a bit length is greater than that of a detection signal. For example, when the detection signal is set to two bits, the pixel signal is converted into a digital signal of three or greater bits (16 bits and the like). The ADC <b>230</b> supplies a generated digital signal to the signal processor <b>212</b>.</p><p id="p-0100" num="0093">Next, an operation of the image sensor <b>200</b> according to at least embodiments of the present disclosure will be described in detail with reference to the accompanying drawings.</p><p id="p-0101" num="0094">First, an example of the operation of the image sensor <b>200</b> will be described by using a timing chart. <figref idref="DRAWINGS">FIG. <b>10</b>A</figref> is a timing chart illustrating an example of the operation of the image sensor according to an embodiment of the present disclosure.</p><p id="p-0102" num="0095">As illustrated in <figref idref="DRAWINGS">FIG. <b>10</b>A</figref>, at a timing TO, when an instruction for address event detection initiation is given by the processor system <b>130</b>, the drive circuit <b>211</b> raises the control signal TG2 applied to the gate of the second transmission transistor <b>332</b> of all of the light-receiving units <b>330</b> in the pixel array unit <b>300</b> to a high level. With this arrangement, the second transmission transistors <b>332</b> of all of the light-receiving units <b>330</b> enter an ON-state, and a photocurrent based on a charge generated in the photoelectric conversion element <b>333</b> of each of the light-receiving units <b>330</b> is supplied from each the light-receiving units <b>330</b> to each of a plurality of the address event detection units <b>400</b>.</p><p id="p-0103" num="0096">In addition, in a period in which the control signal TG2 is at a high level, all of the transmission signals TG1 applied to the gate of the first transmission transistor <b>331</b> in each of the light-receiving units <b>330</b> are maintained at a low level. Accordingly, in this period, a plurality of the transmission transistors <b>331</b> in all of the light-receiving units <b>330</b> are in an OFF-state.</p><p id="p-0104" num="0097">Next, a case where the address event detection unit <b>400</b> of an arbitrary unit pixel <b>310</b> configured to perform event detection detects address event ignition in a period in which the control signal TG2 is in a high level will be assumed. In this case, the address event detection unit <b>400</b> that detects the address event ignition transmits a request to the arbiter <b>213</b>. With respect to this, the arbiter <b>213</b> arbitrates the request, and returns a response for the request to the address event detection unit <b>400</b> that issues the request.</p><p id="p-0105" num="0098">The address event detection unit <b>400</b> that receives the response raises a detection signal that is input to the drive circuit <b>211</b> and the signal processor <b>212</b> to a high level, for example, in a period of a timing T1 to a timing T2. Furthermore, in this description, it is assumed that the detection signal is a one-bit signal.</p><p id="p-0106" num="0099">The drive circuit <b>211</b> to which a high-level detection signal is input from the address event detection unit <b>400</b> at the timing T1 lowers all control signals TG2 to a low level at a subsequent timing T2. With this arrangement, supply of a photocurrent from all of the light-receiving units <b>330</b> of the pixel array unit <b>300</b> to the address event detection unit <b>400</b> is stopped.</p><p id="p-0107" num="0100">In accordance with embodiments of the present disclosure, where a determination by the processor system <b>130</b> is made that pixel imaging signal generation circuit <b>320</b> should be enabled, at the timing T2, the drive circuit <b>211</b> raises a selection signal SEL that is applied to a gate of the selection transistor <b>323</b> in the pixel imaging signal generation unit <b>320</b> of the unit pixel <b>310</b> in which the address event ignition is detected (hereinafter, referred to as &#x201c;reading-out target unit pixel&#x201d;) to a high level, and raises a reset signal RST that is applied to a gate of the reset transistor <b>321</b> of the same pixel imaging signal generation unit <b>320</b> to a high level for a constant pulse period, thereby discharging (initializing) charges accumulated in the floating diffusion layer <b>324</b> of the pixel imaging signal generation unit <b>320</b>. In this manner, a voltage, which appears in the vertical signal line VSL in a state in which the floating diffusion layer <b>324</b> is initialized, is read out by the ADC <b>230</b> connected to the vertical signal line VSL in the column ADC <b>220</b> as a reset-level pixel signal (hereinafter, simply referred to as &#x201c;reset level&#x201d;), and is converted into a digital signal.</p><p id="p-0108" num="0101">Next, at a timing T3 after reading out the reset level, the drive circuit <b>211</b> applies a transmission signal TRG of a constant pulse period to the gate of the first transmission transistor <b>331</b> of the light-receiving unit <b>330</b> in the reading-out target unit pixel <b>310</b>. With this arrangement, a charge generated in the photoelectric conversion element <b>333</b> of the light-receiving unit <b>330</b> is transmitted to the floating diffusion layer <b>324</b> in the pixel imaging signal generation unit <b>320</b>, and a voltage corresponding to charges accumulated in the floating diffusion layer <b>324</b> appears in the vertical signal line VSL. In this manner, the voltage that appears in the vertical signal line VSL is read out by the ADC <b>230</b> connected to the vertical signal line VSL in the column ADC <b>220</b> as a signal-level pixel signal of the light-receiving unit <b>330</b> (hereinafter, simply referred to as &#x201c;signal level&#x201d;) and is converted into a digital value.</p><p id="p-0109" num="0102">The signal processor <b>212</b> executes CDS processing in which a difference between the reset level and the signal level which are read out as described above is obtained as a net pixel signal corresponding to a light-reception amount of the photoelectric conversion element <b>333</b>.</p><p id="p-0110" num="0103">Next, at a timing T4, the drive circuit <b>211</b> lowers the selection signal SEL that is applied to the gate of the selection transistor <b>323</b> in the pixel imaging signal generation readout circuit <b>320</b> of the reading-out target unit pixel <b>310</b> to a low level, and raises the control signal TG2 that is applied to the gate of the second transmission transistor <b>332</b> of all of the light-receiving units <b>330</b> in the pixel array unit <b>300</b> to a high level. With this arrangement, address event ignition detection in all of the light-receiving units <b>330</b> in the pixel array unit <b>300</b> is restarted.</p><p id="p-0111" num="0104"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> is a timing chart illustrating an example of an operation of an image sensor in accordance with other embodiments of the present disclosure. At a timing TO, when an instruction for address event detection initiation is given by the processor system <b>130</b>, the drive circuit <b>211</b> raises the control signal TG2 applied to the gate of the transmission transistor <b>332</b> associated with photoelectric conversion elements <b>333</b> of selectively activated address event detection units <b>400</b>. More particularly, some or all of the address event detection units <b>400</b> may be activated.</p><p id="p-0112" num="0105">In addition, the transmission signal TG1 applied to the gates of the first transmission transistors <b>331</b> are maintained in a low level. Accordingly, the associated transmission transistors <b>331</b> are in an OFF-state.</p><p id="p-0113" num="0106">In this example, an arbitrary address event detection unit <b>400</b> detects address event ignition at a time T1 during which the control signal TG2 is at a high level, and the associated transmission transistor <b>332</b> is in an ON-state. In response to the event trigger, image frame capture begins. The image frame capture can be a full frame image capture that involves all of the image sensing pixels <b>502</b> included in the pixel array <b>300</b>. Alternatively, an event detection by a particular event detection unit <b>400</b> can operate as a trigger for image capture of by a set of image sensing pixels <b>502</b> in a vicinity of the event detection unit <b>400</b>, or otherwise associated with the event detection unit <b>400</b>. Readout of signals obtained by the image sensing pixels can then be performed. Moreover, as discussed elsewhere herein, the processor system <b>130</b> can operate to control the frame rate of enabled image sensing pixels <b>502</b> or circuits <b>320</b>.</p><p id="p-0114" num="0107"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates aspects of the operation of an imaging device <b>100</b> in accordance with embodiments of the present disclosure according to at least some embodiments of the present disclosure. Initially, the imaging device <b>100</b> may be monitoring a scene (step <b>1100</b>) in a EBS mode. In at least some operating scenarios, monitoring a scene in EBS mode includes one or more pixels outputting EBS data to a processor in communication with the imaging device.</p><p id="p-0115" num="0108">As the imaging device <b>100</b> monitors the scene, the EBS data output by the pixels may be analyzed by a processor (step <b>1104</b>). The processor may be configured to be capable of analyzing EBS data to detect changes in light intensity within the scene. As can be appreciated by one of skill in the art after consideration of the present disclosure, the shared event detection and image sensing <b>501</b> or address event detection <b>503</b> pixels can be operated such that events, in the form of changes in light intensity within the scene are detected. Moreover, in accordance with at least some embodiments of the present disclosure, the imaging device <b>100</b> may be operated to detect events continuously.</p><p id="p-0116" num="0109">The detection operation can be performed by the drive circuit <b>211</b>, and/or through execution of application programming by the processor system <b>130</b>. As can be appreciated by one of skill in the art after consideration of the present disclosure, events are generally indicated by signals output from one or more event detection pixels <b>501</b>, <b>503</b> within the pixel array <b>300</b>.</p><p id="p-0117" num="0110">In analyzing the EBS data, the processor may be capable of detecting a triggering event. A triggering event may be detected by the processor by identifying one or more of a plurality of possible patterns or otherwise event associated information in EBS data. For example, a triggering event may be detected by monitoring event density in EBS data and determining that an object exists. In some embodiments, EBS data may be used as an input to a neural network which may output a decision as to whether a triggering event has occurred. For example, a neural network may be trained to detect a face or a set of desired object categories in input EBS data or to otherwise detect a set of meaningful events in input EBS data.</p><p id="p-0118" num="0111">If a triggering event is detected at step <b>1108</b>, the processor may generate a signal to switch the sensor into RGB mode in step <b>1112</b>. If a triggering event is not detected at step <b>1108</b>, the method may return to step <b>1104</b> in which EBS data is analyzed.</p><p id="p-0119" num="0112">After a triggering event is detected at step <b>1108</b>, the RGB mode may be activated in step <b>1112</b>. In some embodiments, after the triggering event has been detected, a determination can be made relating to parameters that should be applied in collecting image data in the RGB mode. For example, the imaging system <b>100</b> can be operated to activate the entire frame to collect data in the RGB mode, or to activate a region of the frame to collect data in the RGB mode. In another example, the imaging system <b>100</b> can be operated to collect image data at a particular frame rate.</p><p id="p-0120" num="0113">A determination can then be made as to whether to discontinue image sensing operations (step <b>1116</b>). In accordance with embodiments of the present disclosure, the acquisition of image information can continue for a predetermined period of time or until a predetermined number of frames of image data have been acquired. Accordingly, the acquisition of image information can be discontinued after an initial image or set of images has been acquired. In accordance with still other embodiments of the present disclosure, image information can continue to be acquired for as long as a detected object remains within the field of view <b>114</b> of the imaging system <b>100</b>. The acquisition of image information related to an object can be discontinued after the object is determined to have left the field of view of the imaging device <b>100</b>. As yet another alternative, the acquisition of image information related to an object can be continued until sufficient image information has been acquired to allow application programming executed by the processor system <b>130</b> of the imaging system <b>104</b> of an associated system, to perform object recognition and to determine that image acquisition operations associated with that object can be discontinued.</p><p id="p-0121" num="0114">After a determination that image sensing operations can be discontinued, a determination can next be made as to whether operation of the image sensor system <b>100</b> should be discontinued (step <b>1120</b>). If operation is to continue, the process can involve switching from the RGB mode back to the EBS mode in step <b>1124</b> before returning to step <b>1104</b>. Otherwise, the operation can end at step <b>1128</b>.</p><p id="p-0122" num="0115"><figref idref="DRAWINGS">FIGS. <b>12</b>A, <b>12</b>B, and <b>12</b>C</figref> are block diagrams illustrating a variety of systems for switching between EBS pixel signals and RGB pixel signals. As discussed above in relation to <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>D</figref>, various configurations of an imaging device <b>100</b> may be implemented in various embodiments. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, an image sensor <b>200</b> may have a first or EBS sensor <b>530</b> and a second or imaging sensor <b>540</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, an image sensor <b>200</b> may have pixels <b>310</b> configured as combined or shared event detection and image sensing pixels <b>501</b> which may be selectively operated in event detection or image sensing modes. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, an image sensor <b>200</b> may have an array of unit pixels <b>310</b> including a plurality of event detection pixels <b>503</b> and a plurality of image sensing pixels <b>502</b>. No matter the type of image sensor <b>200</b> being used, the switching between event detection or EBS mode and the image sensing or RGB mode may be implemented with a switching system as illustrated in <figref idref="DRAWINGS">FIGS. <b>12</b>A, <b>12</b>B, and <b>12</b>C</figref>.</p><p id="p-0123" num="0116">As can be appreciated in <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>, EBS pixel data may be output by an EBS sensor <b>1200</b> and RGB pixel data may be output by an image sensor <b>1204</b> as described above in relation to <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>F</figref>. The EBS pixel data and RGB pixel data may be output simultaneously or separately depending on implementation. EBS pixel data may be input into an EBS event analysis system such as a processor or CPU <b>1220</b> in communication with the image sensor <b>200</b> as well as a computer system executing a neural network <b>1212</b>. In some embodiments, the CPU <b>1220</b> may be capable of executing the neural network itself and thus a separate neural network <b>1212</b> may not be necessary.</p><p id="p-0124" num="0117">The neural network <b>1212</b> of <figref idref="DRAWINGS">FIG. <b>12</b>A</figref> may implement a convolutional neural network or some other type of analysis algorithm. The neural network <b>1212</b> may be capable of controlling a switch <b>1208</b>. In some embodiments, the switch <b>1208</b> may be controlled by the CPU <b>1220</b>. The switch <b>1208</b> may be, for example, a transistor. The switch <b>1208</b> may control the flow of data from the EBS pixels and the RGB pixels to an output circuit <b>1216</b>. In this way, the neural network <b>1212</b> may be capable of analyzing data from the EBS sensor <b>1200</b> and, based on analysis of the EBS pixel data, control whether EBS pixel data or RGB pixel data is output from the imaging device <b>100</b>. The neural network <b>1212</b> and/or CPU <b>1220</b> may be capable of controlling a frame rate or other data capture quality variable of the EBS sensor <b>1200</b> and/or RGB or image sensor <b>1204</b> via a feedback system <b>1224</b>. The entire frame of RGB data may be sent to the output circuit, or a region of the RGB frame may be sent to the output circuit. The frame rate or other data capture quality variable may be altered based on qualities of any detected object. For example, a faster object may warrant increasing a frame rate.</p><p id="p-0125" num="0118">Switching logic may be used to switch a sensor from a EBS data mode to an RGB data mode and vice versa. In some embodiments, EBS data may be analyzed by a computer system capable of controlling a switch to switch the EBS/RGB switchable sensor between EBS and RGB mode. Analysis may be performed through a neural network or another method of data analysis. Depending on decision logic, an output circuit may output either EBS or RGB data from the sensor.</p><p id="p-0126" num="0119">For example, a processor may be configured to process an output from a sensor operating in a EBS mode and/or a sensor operating in an RGB mode. The processor may be configured to output an event signal based on EBS data and/or output an image signal based on RGB data. The processor may further be configured to select between the EBS mode and RGB mode based on processing of EBS and/or RGB data.</p><p id="p-0127" num="0120">As illustrated in <figref idref="DRAWINGS">FIGS. <b>12</b>B and <b>12</b>C</figref>, a single sensor with capabilities for both EBS data and image data may be used. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>12</b>B</figref>, EBS pixel data and RGB pixel data may be output by a sensor <b>1228</b> with both EBS pixels and RGB pixels as described above in relation to <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>F</figref>. The EBS pixel data and RGB pixel data may be output simultaneously or separately depending on implementation. EBS pixel data may be input into a EBS event analysis system such as a processor or CPU <b>1220</b> in communication with the image sensor <b>200</b> as well as a computer system executing a neural network <b>1212</b>. In some embodiments, the CPU <b>1220</b> may be capable of executing the neural network itself and thus a separate neural network <b>1212</b> may not be necessary.</p><p id="p-0128" num="0121">The neural network <b>1212</b> of <figref idref="DRAWINGS">FIG. <b>12</b>B</figref> may implement a convolutional neural network or some other type of analysis algorithm. The neural network <b>1212</b> may be capable of controlling a switch <b>1208</b>. In some embodiments, the switch <b>1208</b> may be controlled by the CPU <b>1220</b>. The switch <b>1208</b> may be, for example, a transistor. The switch <b>1208</b> may control the flow of data from the EBS pixels and the RGB pixels to an output circuit <b>1216</b>. In this way, the neural network <b>1212</b> may be capable of analyzing data from the sensor <b>1228</b> and, based on analysis of the EBS pixel data, control whether EBS pixel data or RGB pixel data is output from the imaging device <b>100</b>. The neural network <b>1212</b> and/or CPU <b>1220</b> may be capable of controlling a frame rate or other data capture quality variable of the sensor <b>1200</b> via a feedback system <b>1224</b>. The frame rate or other data capture quality variable may be altered based on qualities of any detected object. For example, a faster object may warrant increasing a frame rate.</p><p id="p-0129" num="0122">In an alternative embodiments, as illustrated in <figref idref="DRAWINGS">FIG. <b>12</b>C</figref>, EBS pixel data and RGB pixel data may be output by a sensor <b>1232</b> with pixels capable of generating both EBS pixel data and RGB pixel data as described above in relation to <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>F</figref>. The EBS pixel data and RGB pixel data may be output simultaneously or separately depending on implementation. EBS pixel data may be input into a EBS event analysis system such as a processor or CPU <b>1220</b> in communication with the image sensor <b>200</b> as well as a computer system executing a neural network <b>1212</b>. In some embodiments, the CPU <b>1220</b> may be capable of executing the neural network itself and thus a separate neural network <b>1212</b> may not be necessary.</p><p id="p-0130" num="0123">The neural network <b>1212</b> of <figref idref="DRAWINGS">FIG. <b>12</b>C</figref> may implement a convolutional neural network or some other type of analysis algorithm. The neural network <b>1212</b> may be capable of controlling the sensor <b>1232</b> via the feedback system <b>1224</b>. In some embodiments, the feedback system <b>1224</b> may be controlled by the CPU <b>1220</b>. The feedback system <b>1224</b> may control the flow of data from the EBS pixels and the RGB pixels to an output circuit <b>1216</b>. In this way, the neural network <b>1212</b> may be capable of analyzing data from the sensor <b>1228</b> and, based on analysis of the EBS pixel data, control whether EBS pixel data or RGB pixel data is output from the imaging device <b>100</b>.</p><p id="p-0131" num="0124">Whether an event warrants switching from EBS to RGB depends on the application. Many methods of switching which support a low power design may be used and certain embodiments may be as described herein.</p><p id="p-0132" num="0125">For example, depending on application, one or more of the following methods may be used to determine when and whether to switch from EBS to RGB mode: a detection of a high EBS event density, detection of a low EBS event density, analysis of EBS data by a neural network, analysis of EBS data by a recurrent neural network, detection of EBS motion in a particular direction. It should be noted that such methods should not be considered as the only possible methods of determining when and whether to switch from EBS mode to RGB mode.</p><p id="p-0133" num="0126">Data collected via the EBS mode may also be used to determine to send the entire frame of RGB data to the output or send a region of the RGB frame to the output. The EBS data may also be used to determine speed of an object and may be used to switch to a higher frame rate.</p><p id="p-0134" num="0127">In one embodiment, a sensor may be switched from EBS mode to RGB mode when EBS event density exceeds a threshold amount in the entire scene or a predefined region of the scene. Such an embodiment may be useful for capturing motion. For example, a sensor set to switch from EBS mode to RGB mode based on EBS event density exceeding a threshold amount may be used to recognize a vehicle entering into a scene or to recognize a person entering a room, etc.</p><p id="p-0135" num="0128">In some embodiments, the processor system <b>130</b> may be capable of using event detection data to determine a frame rate to apply to the RGB mode. The determined frame rate for the RGB mode can be based on the identity of the object as determined from the event detection data, the relative velocity of the object, or a degree of interest in an identified object. For example, a relatively high frame rate could be applied to an automobile, a moderate frame rate can be applied to a cyclist, and a relatively low frame rate can be applied to a pedestrian. A higher frame rate can be applied to an object moving at a faster apparent velocity than an object that is stationary or moving at a lower apparent velocity.</p><p id="p-0136" num="0129">The various operations performed by the processing system <b>130</b> on the event detection data and/or the image data can include applying one or more neural networks to analyze the collected information.</p><p id="p-0137" num="0130">Embodiments of the present disclosure can continue to operate event detection pixels <b>502</b>, <b>503</b> while image sensing pixels <b>501</b>, <b>502</b> are in operation. As noted elsewhere herein, event detection pixels <b>502</b>, <b>503</b> generally operate asynchronously. By continuing to operate the event detection pixels <b>502</b>, <b>503</b>, event detection functions can be performed continuously, without loss or diminution of temporal event detection performance of the imaging device <b>100</b>.</p><p id="p-0138" num="0131">Accordingly, embodiments of the present disclosure provide imaging devices <b>100</b> with one or more pixel arrays <b>300</b> that are capable of performing both event detection and imaging operations. Moreover, the event detection pixels can be operated continuously, and the image sensing pixels can be operated selectively. Moreover, a frame rate applied for operation of the image sensing pixels can be selected based on characteristics of or an identification of the detected event or events. After a selected time period, after an event being imaged is no longer present, or after some other criterion has been met, operation of the image sensing pixels can be discontinued, while operation of the event detection pixels continues. Accordingly, continuous monitoring for events is provided in combination with selected imaging operations, thereby providing relevant image data while conserving power, data transmission, and data processing resources.</p><p id="p-0139" num="0132">EBS sensors or sensors comprising EBS pixels may be capable of generating frames of data indicating changes in light intensity. For example, a positive change in light intensity may be reflected in a frame by a pixel of a value such as +1 or a particular color such as red. A negative change in light intensity may similarly be reflected in a frame by pixel of a particular value such as &#x2212;1 or of another color such as blue. If a EBS pixel does not detect a change in light intensity, a zero value or a color such as white may be used.</p><p id="p-0140" num="0133">In some embodiments, a EBS sensor or a sensor comprising EBS pixels may be capable of indicating an amount of change in light intensity. For example, a relatively high change in light intensity may be reflected by a pixel of a value of +1.00 while a relatively low, but positive, change in light intensity may be reflected by a value of +0.01 for example. The values +1.00 and +0.01 may be represented by an 8-bit digital value of 255 and 1, respectively. Similarly, a range of colors may be used to indicate amounts of change.</p><p id="p-0141" num="0134">However, EBS cameras provide change information and time information only. For example, data from EBS sensors corresponding to an event for a pixel may correspond to three states: &#x2212;1 indicates a negative change, +1 indicates a positive change, and 0 indicates no change. Information on the time of change may also be provided. EBS cameras alone do not provide color information or shades of gray. For this reason, EBS cameras are not general purpose cameras for capturing image or video information. The above references to EBS pixels being associated with pixels of colors should not be interpreted as EBS pixels being associated with colors from a scene but instead the use of colors only as a visualization of changes in light intensity.</p><p id="p-0142" num="0135">When operating in EBS mode, a EBS/RGB switchable sensor may operate in a relatively lower power consumption state. When operating in RGB mode, the EBS/RGB switchable sensor may operate in a relatively higher power consumption state. For this reason, the EBS mode may be used for lower power and the RGB mode may be activated, or switched to, only as needed.</p><p id="p-0143" num="0136">Switching logic may be used to switch a sensor from a EBS data mode to an RGB data mode and vice versa. In some embodiments, switching logic may be used to only switch the RGB data on and off. In some embodiments, EBS data may be analyzed by a computer system capable of controlling a switch to switch the EBS/RGB switchable sensor between EBS and RGB mode. Analysis may be performed through a neural network or another method of data analysis. Depending on decision logic, an output circuit may output either EBS or RGB data from the sensor.</p><p id="p-0144" num="0137">For example, a processor may be configured to process an output from a sensor operating in a EBS mode and/or a sensor operating in an RGB mode. The processor may be configured to output an event signal based on EBS data and/or output an image signal based on RGB data. The processor may further be configured to select between the EBS mode and RGB mode based on processing of EBS and/or RGB data.</p><p id="p-0145" num="0138">Whether an event warrants switching from EBS to RGB depends on the application. Many methods of switching which support a low power design may be used and certain embodiments may be as described herein.</p><p id="p-0146" num="0139">In some embodiments, a switch between EBS and RGB mode may be triggered based on processing of EBS frames with a convolutional neural network (&#x201c;CNN&#x201d;). In such an embodiment, EBS frames may be fed to a classification CNN such as a LeNet, VGG16, ResNet, etc., to a detection CNN such as a RCNN, YOLO, SSD, etc., or other types of neural network. If a specific object, such as a person, a face or vehicle, is recognized or otherwise detected with a high probability, RGB mode may be triggered to capture a color image of the object for further analysis.</p><p id="p-0147" num="0140">If the neural network decides the probability of a certain category of object, such as a person, a face or a car, exceeds a pre-defined threshold, the RGB mode may be triggered.</p><p id="p-0148" num="0141">For example, one or more EBS frames may be used as an input to a CNN which may output a triggering decision. In some embodiments, a single EBS frame may be used as an input to generate a triggering decision. A single EBS frame may be a collection of EBS signals collected over a particular time frame such as 1 millisecond. In some embodiments, a number of EBS frames may be used as an input. For example, a series of EBS frames taken over a given time period, for example 1 second, may be used.</p><p id="p-0149" num="0142">A CNN may comprise a number of layers and may be trained to detect one or more types of EBS-related events. For example, a CNN may comprise a number of convolutional layers (e.g., conv1, conv2, conv3, conv4, conv5, etc.) and one or more max pooling layers. A CNN may be trained through a process of inputting EBS frames showing known events. In some embodiments, a CNN may be trained to output a triggering decision in the event of detecting EBS data showing the occurrence of a particular event. A triggering decision may be as simple as a +1 for yes and a 0 for no. In some embodiments, a triggering decision may be more complex, for example, an identification of an event type for a detected event. For example, the CNN may detect an input with EBS data showing a high number of events which exceeds a pre-defined spatio-temporal density, or the CNN may detect an input with EBS data which is recognized by the CNN as being indicative of an existence of a particular object such as a person, a face or vehicle. The triggering decision may include information about the object as detected and/or recognized by the CNN.</p><p id="p-0150" num="0143">A block diagram of certain embodiments of the present disclosure is illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>. As can be appreciated, an image system capable of receiving EBS data <b>1300</b> and RGB data <b>1304</b> as inputs may be capable of generating an output. The EBS data <b>1300</b> may be output from a EBS sensor or other type of sensor capable of generating EBS data <b>1300</b>. The EBS data <b>1300</b> may be input into an output circuit <b>1308</b> capable of processing EBS data <b>1300</b> for further analysis by one or more of a face detection neural network <b>1316</b> and/or a CPU <b>1320</b>. The face detection neural network <b>1316</b> may be a neural network such as a CNN trained to detect faces in EBS data. While the present embodiment is a system for detecting and recognizing faces, it should be appreciated that the same principles may be applied to detecting and recognizing other types of objects.</p><p id="p-0151" num="0144">RGB data <b>1304</b> may be input into an output circuit <b>1312</b> capable of processing RGB data <b>1304</b> for further analysis by a facial recognition neural network <b>1328</b>. The facial recognition neural network <b>1328</b> may be a neural network such as a CNN trained to recognize faces in RGB data. Specifically, it is trained to determine the identity of the face. While the present embodiment is a system for detecting and recognizing faces, it should be appreciated that the same principles may be applied to detecting and recognizing other types of objects.</p><p id="p-0152" num="0145">Prior to the RGB data <b>1304</b> being input into the facial recognition neural network, the RGB data <b>1304</b> may be input into an on/off logic system <b>1324</b> which may be controlled by one or both of the face detection neural network and/or the CPU <b>1320</b>. For example, the face detection neural network <b>1316</b> may be trained to switch the on/off logic system <b>1324</b> upon detecting one or more faces in the input EBS data.</p><p id="p-0153" num="0146">When the on/off logic system <b>1324</b> is switched to allow RGB data <b>1304</b> to be input into the facial recognition neural network <b>1328</b>, the facial recognition neural network <b>1328</b> may begin processing the RGB data <b>1304</b> to recognize faces in the RGB data <b>1304</b>. The facial recognition neural network <b>1328</b> may then output information relating to any recognized faces into an output circuit <b>1332</b>. For example, the neural network may be in communication with one or more online databases which may be used to gather names or other identifying information relating to the recognized faces. Such identifying information may be output into the output circuit <b>1332</b>.</p><p id="p-0154" num="0147">Using a system as illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref> in accordance with the systems and methods described herein, an object recognition system may be implemented. Such an object recognition system may comprise a first sensor, such as a EBS sensor configured to detect a change of an amount of light intensity, a second sensor, such as an RGB sensor configured to detect an amount of light intensity, and a processor.</p><p id="p-0155" num="0148">The processor may be configured to process an output from the first sensor and output an event signal. For example, the processor may be capable of processing EBS data to detect the presence of one or more faces or other objects in the EBS data. If a face or other object is detected, the processor may be configured to process an object detection based on the event signal. For example, the processor may be capable of detecting an object in EBS data and then recognize what type of object or the identity of the object that has been detected. In some embodiments, the processor may be trained to specifically detect facial data. The processor may be further configured to process an object recognition based on an output from the second sensor according to the object detection. For example, upon detecting a face in the EBS data, the processor may next analyze RGB data of the same or a similar scene and make an attempt to recognize any face in the RGB data.</p><p id="p-0156" num="0149">In some embodiments, the RGB sensor may not be activated unless and until a face (or other type of object) has been detected using EBS data. In response to detecting a face (or other type of object) the processor may be configured to activate the second sensor. Furthermore, in response to processing the object detection based on the event signal, the processor may be configured to deactivate the first sensor.</p><p id="p-0157" num="0150">In some embodiments, after processing the object recognition based on the output from the second sensor according to the object detection, the processor may reactivate the first sensor and deactivate the second sensor.</p><p id="p-0158" num="0151">In the above descriptions, it should be appreciated that any of the various embodiments including separate EBS and RGB sensor or single sensors capable of both EBS and RGB data may be used to implement the various systems and methods described herein.</p><p id="p-0159" num="0152">In some embodiments, switching from EBS to RGB may be triggered based on a detected direction of motion in EBS data. For example, a predefined object recognized by a convolutional neural network or other method of detecting objects may be monitored to detect a direction of motion of the object. Depending on the detected direction of motion of the object, the sensor may be switched to RGB mode or to a high-frames-per-second (&#x201c;FPS&#x201d;) RGB mode.</p><p id="p-0160" num="0153">The switch of a sensor from EBS mode to RGB mode may be for a pre-determined amount of time. For example, after switching to RGB mode, the sensor may be switched back to EBS mode after a certain number of seconds or after a certain number of image frames. In some embodiments, the RGB mode may be analyzed to determine when an event has ended, at which time the sensor may be switched back to EBS mode.</p><p id="p-0161" num="0154">Hereinbefore, embodiments of the present disclosure have been described, but the technical range of the present disclosure is not limited to the above-described embodiments, and various modifications can be made in a range not departing from the gist of the present disclosure. In addition, constituent elements in other embodiments and modification examples may be appropriately combined.</p><p id="p-0162" num="0155">Disclosed herein is a combination EBS and RGB camera capable of utilizing advantages of both EBS and RGB modes. A sensor as described herein normally operates in a EBS mode and switches to RGB mode when an event warrants the switch. As used herein, RGB may refer to data relating to an amount of light intensity. An RGB sensor or a sensor operating in an RGB mode may be capable of or configured to detect an amount of light intensity.</p><p id="p-0163" num="0156">As described herein, a EBS/RGB switchable sensor may be in a variety of forms. For example, in some embodiments, separate EBS and RGB sensors may be used. In such an embodiment, the separate EBS and RGB sensors may each comprise a plurality of pixels. The separate EBS and RGB sensors may be physically connected and may share a single lens.</p><p id="p-0164" num="0157">In some embodiments, a single sensor with a mosaic of RGB and EBS pixels may be used. For example, a single sensor may comprise a grid of pixels. The grid of pixels may be a variety of non-switchable RGB pixels and EBS pixels. The pattern of pixels may be laid out in a random fashion or may be a particular pattern. In some embodiments, the EBS pixels may be in a small section of the grid of pixels or may be spread out evenly throughout the grid.</p><p id="p-0165" num="0158">In some embodiments, a single sensor with switchable RGB and EBS pixels may be used. For example, a sensor may comprise a grid of pixels. Each pixel may be capable of detecting both EBS and the intensity of a color. For example, a first pixel may be switchable between collecting EBS data and red color data, while a second pixel may be switchable between collecting EBS data and green color data, and a third pixel may be switchable between collecting EBS data and blue color data.</p><p id="p-0166" num="0159">Additional embodiments may include other combinations of switchable and non-switchable pixels and/or other color mosaic patterns.</p><p id="p-0167" num="0160">As described herein, a EBS/RGB switchable sensor may be used in one or both of a EBS mode and an RGB mode. EBS sensors are advantageous in that EBS sensors are capable of capturing event data at high rates. EBS sensors also consume relatively lower power than RGB sensors.</p><p id="p-0168" num="0161">In addition, the effects in the embodiments described in this specification are illustrative only, and other effect may exist without a limitation.</p><p id="p-0169" num="0162">Furthermore, the present technology can include the following configurations:</p><p id="p-0170" num="0163">(1)</p><p id="p-0171" num="0164">An object recognition system comprising:</p><p id="p-0172" num="0165">a first sensor configured to detect a change of an amount of light intensity;</p><p id="p-0173" num="0166">a second sensor configured to detect an amount of light intensity; and</p><p id="p-0174" num="0167">a processor configured to:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0168">process an output from the first sensor and output an event signal,</li>        <li id="ul0002-0002" num="0169">process an object detection based on the event signal, and</li>        <li id="ul0002-0003" num="0170">process an object recognition based on an output from the second sensor according to the object detection.</li>    </ul>    </li></ul></p><p id="p-0175" num="0171">(2)</p><p id="p-0176" num="0172">The object recognition system of (1), wherein in response to processing the object detection based on the event signal, the processor activates the second sensor.</p><p id="p-0177" num="0173">(3)</p><p id="p-0178" num="0174">The object recognition system of (2), wherein in response to processing the object detection based on the event signal, the processor deactivates the first sensor.</p><p id="p-0179" num="0175">(4)</p><p id="p-0180" num="0176">The object recognition system of (3), wherein after processing the object recognition based on the output from the second sensor according to the object detection, the processor reactivates the first sensor and deactivates the second sensor.</p><p id="p-0181" num="0177">(5)</p><p id="p-0182" num="0178">The object recognition system of (1), wherein processing the object detection based on the event signal comprises detecting an object in the output.</p><p id="p-0183" num="0179">(6)</p><p id="p-0184" num="0180">The object recognition system of (5), wherein processing the object recognition based on the output from the second sensor according to the object detection comprises recognizing the object.</p><p id="p-0185" num="0181">(7)</p><p id="p-0186" num="0182">The object recognition system of (1), wherein processing the object recognition based on the output from the second sensor according to the object detection comprising recognizing a face.</p><p id="p-0187" num="0183">(8)</p><p id="p-0188" num="0184">An imaging system, comprising:</p><p id="p-0189" num="0185">a first sensor configured to detect a change of an amount of light intensity; and</p><p id="p-0190" num="0186">a second sensor configured to detect an amount of light intensity, wherein an output from the first sensor is processed by a processor to output an event signal, a first object is detected by the processor based on the event signal, and an object recognition is processed by the processor based on an output from the second sensor according to the object detection.</p><p id="p-0191" num="0187">(9)</p><p id="p-0192" num="0188">The imaging system of (8), wherein in response to processing the object detection based on the event signal, the processor activates the second sensor.</p><p id="p-0193" num="0189">(10)</p><p id="p-0194" num="0190">The imaging system of (9), wherein in response to processing the object detection based on the event signal, the processor deactivates the first sensor.</p><p id="p-0195" num="0191">(11)</p><p id="p-0196" num="0192">The imaging system of (10), wherein after processing the object recognition based on the output from the second sensor according to the object detection, the processor reactivates the first sensor and deactivates the second sensor.</p><p id="p-0197" num="0193">(12)</p><p id="p-0198" num="0194">The imaging system of (8), wherein processing the object detection based on the event signal comprises detecting an object in the output.</p><p id="p-0199" num="0195">(13)</p><p id="p-0200" num="0196">The imaging system of (12), wherein processing the object recognition based on the output from the second sensor according to the object detection comprises recognizing the object.</p><p id="p-0201" num="0197">(14)</p><p id="p-0202" num="0198">The imaging system of (8), wherein processing the object recognition based on the output from the second sensor according to the object detection comprising recognizing a face.</p><p id="p-0203" num="0199">(15)</p><p id="p-0204" num="0200">A method of implementing object recognition, the method comprising performing functions as follows with a processor:</p><p id="p-0205" num="0201">processing an output from a first sensor configured to detect a change of an amount of light intensity;</p><p id="p-0206" num="0202">outputting an event signal based on the output from the first sensor,</p><p id="p-0207" num="0203">processing an object detection based on the event signal, and</p><p id="p-0208" num="0204">processing an object recognition according to the object detection based on an output from a second sensor configured to detect an amount of light intensity.</p><p id="p-0209" num="0205">(16)</p><p id="p-0210" num="0206">The method of (15), wherein in response to processing the object detection based on the event signal, the processor activates the second sensor.</p><p id="p-0211" num="0207">(17)</p><p id="p-0212" num="0208">The method of (16), wherein in response to processing the object detection based on the event signal, the processor deactivates the first sensor.</p><p id="p-0213" num="0209">(18)</p><p id="p-0214" num="0210">The method of (17), wherein after processing the object recognition based on the output from the second sensor according to the object detection, the processor reactivates the first sensor and deactivates the second sensor.</p><p id="p-0215" num="0211">(19)</p><p id="p-0216" num="0212">The method of (15), wherein processing the object detection based on the event signal comprises detecting an object in the output.</p><p id="p-0217" num="0213">(20)</p><p id="p-0218" num="0214">The method of (19), wherein processing the object recognition based on the output from the second sensor according to the object detection comprises recognizing the object.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007208A1-20230105-M00001.NB"><img id="EMI-M00001" he="10.92mm" wi="76.20mm" file="US20230007208A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An object recognition system, comprising:<claim-text>a first sensor configured to detect a change of an amount of light intensity;</claim-text><claim-text>a second sensor configured to detect an amount of light intensity; and</claim-text><claim-text>a processor configured to:<claim-text>process an output from the first sensor and output an event signal,</claim-text><claim-text>process an object detection based on the event signal, and</claim-text><claim-text>process an object recognition based on an output from the second sensor according to the object detection.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The object recognition system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in response to processing the object detection based on the event signal, the processor activates the second sensor.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The object recognition system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein in response to processing the object detection based on the event signal, the processor deactivates the first sensor.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The object recognition system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein after processing the object recognition based on the output from the second sensor according to the object detection, the processor reactivates the first sensor and deactivates the second sensor.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The object recognition system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the object detection based on the event signal comprises detecting an object in the output.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The object recognition system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein processing the object recognition based on the output from the second sensor according to the object detection comprises recognizing the object.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The object recognition system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the object recognition based on the output from the second sensor according to the object detection comprising recognizing a face.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. An imaging system, comprising:<claim-text>a first sensor configured to detect a change of an amount of light intensity; and</claim-text><claim-text>a second sensor configured to detect an amount of light intensity, wherein an output from the first sensor is processed by a processor to output an event signal, an object is detected by the processor based on the event signal, and an object recognition is processed by the processor based on an output from the second sensor according to the object detection.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The imaging system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein in response to processing the object detection based on the event signal, the processor activates the second sensor.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The imaging system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein in response to processing the object detection based on the event signal, the processor deactivates the first sensor.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The imaging system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein after processing the object recognition based on the output from the second sensor according to the object detection, the processor reactivates the first sensor and deactivates the second sensor.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The imaging system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein processing the object detection based on the event signal comprises detecting an object in the output.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The imaging system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein processing the object recognition based on the output from the second sensor according to the object detection comprises recognizing the object.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The imaging system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein processing the object recognition based on the output from the second sensor according to the object detection comprising recognizing a face.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A method of implementing object recognition, the method comprising performing functions as follows with a processor:<claim-text>processing an output from a first sensor configured to detect a change of an amount of light intensity;<claim-text>outputting an event signal based on the output from the first sensor,</claim-text><claim-text>processing an object detection based on the event signal, and</claim-text><claim-text>processing an object recognition according to the object recognition based on an output from a second sensor configured to detect an amount of light intensity.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein in response to processing the object detection based on the event signal, the processor activates the second sensor.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein in response to processing the object detection based on the event signal, the processor deactivates the first sensor.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein after processing the object recognition based on the output from the second sensor according to the object detection, the processor reactivates the first sensor and deactivates the second sensor.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein processing the object detection based on the event signal comprises detecting an object in the output.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein processing the object recognition based on the output from the second sensor according to the object detection comprises recognizing the object.</claim-text></claim></claims></us-patent-application>