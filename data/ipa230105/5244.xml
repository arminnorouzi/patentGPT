<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005245A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005245</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17939136</doc-number><date>20220907</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>772</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0012</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>772</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">TRAINING DATA GENERATING SYSTEM, METHOD FOR GENERATING TRAINING DATA, AND RECORDING MEDIUM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2020/009909</doc-number><date>20200309</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17939136</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>OLYMPUS CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>HAYAMI</last-name><first-name>Takehito</first-name><address><city>Yokohama-shi</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>KITAMURA</last-name><first-name>Makoto</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>KANDA</last-name><first-name>Yamato</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>OLYMPUS CORPORATION</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A training data generating system includes a processor. The processor acquires a plurality of medical images. The processor associates medical images with each other which are included in the plurality of medical images based on similarities of an imaging target to generate an associated image group including medical images associated with each other. The processor outputs, to a display, an application target image to be an image as an application target of representative training information based on the associated image group. The processor accepts input of representative contour information indicative of a contour of a specific region in the application target image as the representative training information. The processor applies contour information, as training information, to each medical image included in the associated image group based on the representative training information input to the application target image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="151.72mm" wi="104.82mm" file="US20230005245A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="233.17mm" wi="161.46mm" orientation="landscape" file="US20230005245A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="174.92mm" wi="106.85mm" file="US20230005245A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="237.83mm" wi="148.84mm" orientation="landscape" file="US20230005245A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="77.13mm" wi="144.70mm" file="US20230005245A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="235.29mm" wi="156.72mm" orientation="landscape" file="US20230005245A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="173.40mm" wi="108.71mm" file="US20230005245A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="148.76mm" wi="146.90mm" orientation="landscape" file="US20230005245A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="229.11mm" wi="155.79mm" orientation="landscape" file="US20230005245A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="217.42mm" wi="111.84mm" file="US20230005245A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="205.15mm" wi="149.44mm" orientation="landscape" file="US20230005245A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="186.61mm" wi="65.96mm" file="US20230005245A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="185.50mm" wi="144.36mm" file="US20230005245A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="221.57mm" wi="108.63mm" file="US20230005245A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="215.98mm" wi="156.97mm" orientation="landscape" file="US20230005245A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="201.25mm" wi="156.89mm" orientation="landscape" file="US20230005245A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="97.79mm" wi="86.11mm" file="US20230005245A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="198.54mm" wi="115.40mm" file="US20230005245A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="202.10mm" wi="158.24mm" orientation="landscape" file="US20230005245A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation of International Patent Application No. PCT/JP2020/009909, having an international filing date of Mar. 9, 2020, which designated the United States, the entirety of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Machine learning, such as deep learning, has been widely used as an image recognition technique. The machine learning requires the input of a large number of training images suitable for learning. One of the methods to prepare such a large number of training images is data augmentation. The data augmentation is a method to increase the number of training images by processing the original images that are actually captured to generate new images. From the perspective of improving the recognition accuracy of machine learning, however, it is desirable to prepare a large number of original images to apply a teacher label to each of those original images. Japanese Unexamined Patent Application Publication No. 2019-118694 discloses a medical image generating device that generates images for machine learning of ultrasonic diagnostic equipment. Also in Japanese Unexamined Patent Application Publication No. 2019-118694, the medical image generating device sets, as an image of interest, a medical image displayed when the user performed an image save or freeze operation, and mechanically extracts a plurality of images similar to such image of interest from time series images. The user applies label information to the image of interest. The medical image generating device applies the label information to the plurality of images similar to the image of interest based on the label information attached to the image of interest.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">In accordance with one of some aspect, there is provided a training data generating system comprising a processor, the processor being configured to implement:</p><p id="p-0005" num="0004">acquiring a plurality of medical images:</p><p id="p-0006" num="0005">associating medical images with each other which are included in the plurality of medical images based on similarities of an imaging target to generate an associated image group comprising medical images associated with each other;</p><p id="p-0007" num="0006">outputting, to a display, an application target image to be an image as an application target of representative training information based on the associated image group;</p><p id="p-0008" num="0007">accepting input of representative contour information indicative of a contour of a specific region in the application target image as the representative training information; and</p><p id="p-0009" num="0008">applying contour information, as training information, to each medical image included in the associated image group based on the representative training information input to the application target image.</p><p id="p-0010" num="0009">In accordance with one of some aspect, there is provided a method of generating training data, the method comprising:</p><p id="p-0011" num="0010">acquiring a plurality of medical images;</p><p id="p-0012" num="0011">associating medical images with each other which are included in the plurality of medical images based on similarities of an imaging target to generate an associated image group comprising medical images associated with each other:</p><p id="p-0013" num="0012">outputting, to a display, an application target image to be an image as an application target of representative training information based on the associated image group;</p><p id="p-0014" num="0013">accepting input of representative contour information indicative of a contour of a specific region in the application target image as the representative training information; and applying contour information, as training information, to each medical image included in the associated image group based on the representative training information input to the application target image.</p><p id="p-0015" num="0014">In accordance with one of some aspect, there is provided a non-transitory computer readable recording medium storing thereon a computer program that causes a computer to perform a method comprising:</p><p id="p-0016" num="0015">acquiring a plurality of medical images:</p><p id="p-0017" num="0016">associating medical images with each other which are included in the plurality of medical images based on similarities of an imaging target to generate an associated image group comprising medical images associated with each other;</p><p id="p-0018" num="0017">outputting, to a display, an application target image to be an image as an application target of representative training information based on the associated image group;</p><p id="p-0019" num="0018">accepting input of representative contour information indicative of a contour of a specific region in the application target image as the representative training information; and</p><p id="p-0020" num="0019">applying contour information, as training information, to each medical image included in the associated image group based on the representative training information input to the application target image.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a first configuration example of a training data generating system.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating a flow of process in the first configuration example.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an operation of an image associating section.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a method of determining an associated image group.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates operations of an image output section, an input accepting section, and a training information applying section.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating a flow of process in a first modification of the first configuration example.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example of an image displayed on a display section in a second modification of the first configuration example.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a configuration example of the training data generating system in a third modification of the first configuration example.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating a flow of process in the third modification of the first configuration example.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates a configuration example of the training data generating system in a fourth modification of the first configuration example.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates a first operation example of an input adjuster.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a second operation example of the input adjuster.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart illustrating a flow of process in a second configuration example.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates an operation of an image associating section.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates operations of an image output section, an input accepting section, and a training information applying section.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates a second modification of the second configuration example.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart illustrating a flow of process in a third modification of the second configuration example.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates the third modification of the second configuration example.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DESCRIPTION OF EXEMPLARY EMBODIMENTS</heading><p id="p-0039" num="0038">The following disclosure provides many different embodiments, or examples, for implementing different features of the provided subject matter. These are, of course, merely examples and are not intended to be limiting. In addition, the disclosure may repeat reference numerals and/or letters in the various examples. This repetition is for the purpose of simplicity and clarity and does not in itself dictate a relationship between the various embodiments and/or configurations discussed. Further, when a first element is described as being &#x201c;connected&#x201d; or &#x201c;coupled&#x201d; to a second element, such description includes embodiments in which the first and second elements are directly connected or coupled to each other, and also includes embodiments in which the first and second elements are indirectly connected or coupled to each other with one or more other intervening elements in between.</p><heading id="h-0006" level="1">1. First Configuration Example</heading><p id="p-0040" num="0039">In Japanese Unexamined Patent Application Publication No. 2019-118694 described above, the image obtained when the user performs an image save or freeze operation becomes an image of interest. This results in that an image with low similarity to time series images or with a small number of images with high similarity among the time series images may be the image of interest. In this case, the problem exists that, with label information applied to the image of interest, a medical image generating device fails to apply the label information to the time series images or applies the label information to only a small number of images. For example, in a case where an image captured when the angle of camera in such as an endoscope is temporarily changed significantly becomes the image of interest, the label information may not be applied, based on the similarity, to the images captured at angles other than that angle.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a first configuration example of a training data generating system <b>10</b>. The training data generating system <b>10</b> includes a processing section <b>100</b>, a storage section <b>200</b>, a display section <b>300</b>, and an operation section <b>400</b>. Note that when the training data generating system <b>10</b> generates training data, an endoscope <b>600</b> and a training device <b>500</b>, which are illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, do not need to be connected to the training data generating system <b>10</b>.</p><p id="p-0042" num="0041">The processing section <b>100</b> includes an image acquisition section <b>110</b>, an image associating section <b>120</b>, an image output section <b>130</b>, an input accepting section <b>140</b>, and the training information applying section <b>150</b>. The image acquisition section <b>110</b> acquires a plurality of medical images. The image associating section <b>120</b> associates medical images with each other which are included in the plurality of medical images based on similarities of an imaging target to generate an associated image group. The associated image group represents a group including medical images associated with each other at the above-described association. The image output section <b>130</b> outputs, to the display section <b>300</b>, an application target image to be an image as an application target of representative training information based on the associated image group. The input accepting section <b>140</b> accepts input of the representative training information from the operation section <b>400</b>. The training information applying section <b>150</b> applies training information to the medical images included in the associated image group based on the representative training information input for the application target image.</p><p id="p-0043" num="0042">With such a configuration, the application target image, which is displayed on the display section <b>300</b> in order for the user to apply the training information, is displayed based on the associated image group to which the association has been made on the basis of the similarity of the imaging target. That is, since the similarity is determined prior to presentation to the user, the application target image is an image that has similarity to each medical image of the associated image group. This allows the representative training information to be attached to the medical images associated with a large number of images with high similarity, so that the training information applying section <b>150</b> can automatically apply the training information to a larger number of medical images based on a single piece of representative training information. Consequently, this can reduce the number of application target images that require the user to input the representative training information, allowing the user to generate a training image with fewer work steps.</p><p id="p-0044" num="0043">Details of the training data generating system <b>10</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> will be described below.</p><p id="p-0045" num="0044">The training data generating system <b>10</b> is an information processing device such as a personal computer (PC). Alternatively, the training data generating system <b>10</b> may be a system in which a terminal device and the information processing device are connected through a network. For example, the terminal device includes the display section <b>300</b>, the operation section <b>400</b>, and the storage section <b>200</b>, and the information processing device includes the processing section <b>100</b>. Still alternatively, the training data generating system <b>10</b> may be a cloud system in which a plurality of information processing devices is connected to each other through a network.</p><p id="p-0046" num="0045">The storage section <b>200</b> stores a plurality of medical images captured with the endoscope <b>600</b>. The medical images are in vivo images captured with a medical endoscope. The medical endoscope is a videoscope including a gastrointestinal endoscope, or a surgical rigid scope, for example. The medical images are the time series images. For example, the endoscope <b>600</b> captures a video of the interior of the body, and the image of each frame in the video corresponds to each of the medical images. The storage section <b>200</b> is a storage device such as a semiconductor memory and a hard disk drive. The semiconductor memory is, for example, a volatile memory such as a random-access memory (RAM) or a non-volatile memory such as an electrically erasable programmable read only memory (EEPROM).</p><p id="p-0047" num="0046">The processing section <b>100</b> is a processor. The processor may be an integrated circuit device such as a central processing unit (CPU), a microcomputer, a digital signal processor (DSP), an application-specific integrated circuit (ASIC) or a field-programmable gate array (FPGA). The processing section <b>100</b> may include one or more processors. The processing section <b>100</b> as the processor may be, for example, a processing circuit or a processing device each including one or more circuit elements, or a circuit device in which one or more circuit elements are mounted on the board.</p><p id="p-0048" num="0047">An operation of the processing section <b>100</b> may be implemented by software processing. That is, the storage section <b>200</b> stores a program in which operations of all or a part of the image acquisition section <b>110</b>, the image associating section <b>120</b>, the image output section <b>130</b>, the input accepting section <b>140</b>, and the training information applying section <b>150</b> each included in the processing section <b>100</b> are described. The processor executes the program stored in the storage section <b>200</b>, so that the operations of the processing section <b>100</b> are implemented. Note that the program may be one in which a detecting section <b>160</b> mentioned below in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is further described. The above-mentioned program may be stored in an information storage medium, which is a computer readable storage medium. The information storage medium can be implemented by, for example, an optical disk, a memory card, a hard disk drive (HDD), or a semiconductor memory. The computer is a device provided with an input device, a processing section, a storage section, and an output section.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating a flow of process in the first configuration example. In step S<b>1</b>, the image acquisition section <b>110</b> acquires a plurality of medical images. Specifically, the image acquisition section <b>110</b>, which is an access control section that reads data from the storage section <b>200</b>, acquires the plurality of medical images from the storage section <b>200</b> to output the plurality of medical images to the image associating section <b>120</b>.</p><p id="p-0050" num="0049">In step S<b>2</b>, the image associating section <b>120</b> extracts feature points of the plurality of medical images to perform matching between the images for each feature point. In step S<b>3</b>, the image associating section <b>120</b> selects the medical image with a large number of matched feature points. The image output section <b>130</b> outputs, to the display section <b>300</b>, the selected medical image as an application target image. The display section <b>300</b> displays the application target image. The display section <b>300</b> is a display such as a liquid crystal display device or an electro luminescence (EL) display device.</p><p id="p-0051" num="0050">In step S<b>4</b>, the input accepting section <b>140</b> accepts input of training information for the displayed application target image. That is, the user uses the operation section <b>400</b> to apply the training information to the application target image displayed on the display section <b>300</b>. The applied training information is input to the input accepting section <b>140</b>. The input accepting section <b>140</b> is a communication interface between, for example, the operation section <b>400</b> and the processing section <b>100</b>. The operation section <b>400</b> is a device for the user to perform operation input or information input to the training data generating system <b>10</b>, such as a pointing device, a keyboard, or a touch panel.</p><p id="p-0052" num="0051">In step S<b>5</b>, the training information applying section <b>150</b> applies, based on the applied training information to the application target image, the training information to each medical image associated by the image associating section <b>120</b> to store the training information in the storage section <b>200</b>. The medical image to which training information is applied is hereinafter referred to as a training image.</p><p id="p-0053" num="0052">The training image stored in the storage section <b>200</b> is input to the training device <b>500</b>. The training device <b>500</b> includes a storage section <b>520</b> and a processor <b>510</b>. The training image input from the storage section <b>200</b> is stored as training data <b>521</b> in the storage section <b>520</b>. The storage section <b>520</b> stores a training model <b>522</b> of machine learning. The processor <b>510</b> uses the training data <b>521</b> to perform machine learning on the training model <b>522</b>. The machine-learned training model <b>522</b> is transferred to an endoscope as a trained model to be used for image recognition in the endoscope.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an operation of the image associating section <b>120</b>. In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, IA<b>1</b> to IAm denote a plurality of medical images obtained by the image acquisition section <b>110</b> where m is an integer equal to or greater than three. Here, it is defined as m=5, more specifically, IAm=IA<b>5</b>.</p><p id="p-0055" num="0054">In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, no feature point is detected in IA<b>1</b>, the common feature points FPa are detected in IA<b>2</b> to IA<b>5</b>, the common feature points FPb are detected in IA<b>2</b> to IA<b>4</b>, and the common feature points FPc are detected in IA<b>4</b> and IA<b>5</b>. The feature point indicates a feature of an image and is a point characterized by a feature amount detected in the image. Various feature amounts can be assumed, which include, for example, edges or gradients in an image or their statistics. The image associating section <b>120</b> extracts the feature point with methods such as the Scale Invariant Feature Transform (SIFT), the Speeded-Up Robust Features (SURF), and the Histograms of Oriented Gradients (HOG).</p><p id="p-0056" num="0055">The image associating section <b>120</b> determines a group of images with high similarity among imaging targets as an associated image group GIM based on the feature points FPa to FPc. The &#x201c;similarities of an imaging target&#x201d; refers to the degree to which target objects seen in the medical images are similar where the higher the similarity, the more likely the target objects are identical. It can be determined that when employing the feature points, the greater the number of feature points in common, the higher the similarity of imaging targets to each other. The image associating section <b>120</b>, for example, associates medical images in which the number of the feature points in common is equal to or more than a threshold. In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the image associating section <b>120</b> associates the medical images that each have two or more feature points in common. The image associating section <b>120</b> determines IA<b>2</b> to IA<b>4</b> and IAm as the associated image group GIM since IA<b>4</b> has two or more feature points in common with each of IA<b>2</b>, IA<b>3</b>, and IAm.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a method of determining the associated image group. Here, IX<b>1</b> to IX<b>5</b> are assumed to be obtained as a plurality of medical images. The image associating section <b>120</b> determines the similarity between each of the medical images IX<b>1</b> to IX<b>5</b> and the other medical image. A circle mark indicates that a high similarity is determined while a cross mark indicates that a low similarity is determined. IX<b>1</b>, IX<b>2</b>, IX<b>3</b>, IX<b>4</b>, and IX<b>5</b> are determined to be highly similar to two, three, three, two, and two of medical images, respectively.</p><p id="p-0058" num="0057">The image associating section <b>120</b> selects, as an application target image, the medical images that each have a large number of highly similar medical images and determines, as the associated image group, the medical images with high similarity to the application target image. In the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, IX<b>2</b> is selected as the application target image and IX<b>1</b> to IX<b>3</b> and IX<b>5</b> are set as the associated image group. Alternatively, IX<b>3</b> is selected as the application target image and IX<b>1</b> to IX<b>4</b> are set as the associated image group. The image associating section <b>120</b> may select, for example, the image with the smaller number between IX<b>2</b> and IX<b>3</b> or equivalently the preceding image in the time series as the application target image. Still alternatively, the image associating section <b>120</b> may select those as one associated image group in consideration of another associated image group, such as determining IX<b>1</b> to IX<b>3</b> and IX<b>5</b> as the associated image group when IX<b>4</b> is included in the other associated image group.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates operations of the image output section <b>130</b>, the input accepting section <b>140</b>, and the training information applying section <b>150</b>. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, reference designators IB<b>1</b> to IBn correspond to IA<b>2</b> to IAm in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. With the method described in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the medical image IB<b>3</b> among those IB<b>1</b> to IBn, which belong to the associated image group GIM, is selected as an application target image RIM. The n is an integer equal to or greater than two.</p><p id="p-0060" num="0059">The image output section <b>130</b> displays the application target image RIM selected by the image associating section <b>120</b> on the display section <b>300</b>. The user applies a representative training information ATIN to the application target image RIM through the operation section <b>400</b>.</p><p id="p-0061" num="0060">The representative training information ATIN refers to training information applied to the application target image RIM. <figref idref="DRAWINGS">FIG. <b>5</b></figref> diagrammatically illustrates an example where the training information is contour information surrounding a detection target for machine learning. The training information may be a text, a bounding box, a contour, or a region. The text is the information used as artificial intelligence (AI) training data that performs a classification process. The text is the information that indicates the content or conditions of an imaging target, such as the type of lesion or the grade of the lesion. The bounding box is the information used as the AI training data that performs a detection process. The bounding box is a rectangle circumscribed on an imaging target such as a lesion and indicates the presence and position of the lesions. The contour or the region is the information used as the AI training data that performs a segmentation process. The contour is the information that indicates the boundary between the imaging target such as the lesion and the region other than the imaging target such as a normal mucosa. The region is the information that indicates the region occupied by the imaging target such as the lesion and contains information of the contour.</p><p id="p-0062" num="0061">The training information applying section <b>150</b> applies training information AT<b>1</b> to ATn to the respective medical images IB<b>1</b> to IBn included in the associated image group GIM based on the representative training information ATIN accepted by the input accepting section <b>140</b>. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the representative training information ATIN as the training information AT<b>3</b> is applied as it is to IB<b>3</b> of the application target image RIM. The training information applying section <b>150</b> geometrically transforms the representative training information ATIN based on feature points to apply the training information to the medical images other than IB<b>3</b>. That is, in between two images that each have two or more feature points in common, parallel displacements, an amount of rotation, and a scaling factor between the images can be obtained on the basis of the positions and correspondence of and between such feature points. The training information applying section <b>150</b> converts the representative training information ATIN into the training information AT<b>1</b>, AT<b>2</b>, and AT<b>4</b> to ATn by geometric transformation with the parallel displacements, the amount of rotation, and the scaling factor.</p><p id="p-0063" num="0062">In accordance with the present embodiment, the image output section <b>130</b> outputs, to the display section <b>300</b>, the medical images selected from the associated image group GIM as the application target image RIM.</p><p id="p-0064" num="0063">With this processing, any of the medical images included in the associated image group GIM are displayed as the application target image RIM on the display section <b>300</b>. Since the application target image RIM is associated with the other medical image in the associated image group GIM, such correlation can be used to apply the training information for each medical image based on the representative training information ATIN.</p><p id="p-0065" num="0064">Additionally, in the present embodiment, the number of medical images associated with each medical image in the association based on similarities of an imaging target is considered as the number of associated images. For example, in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the number of associated images for IX<b>1</b> to IX<b>5</b> are two, three, three, two, and two, respectively. The image output section <b>130</b> outputs, to the display section <b>300</b>, the medical images selected based on the number of associated images as the application target image RIM.</p><p id="p-0066" num="0065">Specifically, the image associating section <b>120</b> selects the application target image RIM from the plurality of medical images based on the number of associated images to determine the medical images associated with the application target image RIM among the plurality of medical images as the associated image group GIM. More Specifically, the image associating section <b>120</b> selects the medical image with the highest number of associated images as the application target image RIM among the plurality of medical images in which the number of associated images is calculated.</p><p id="p-0067" num="0066">This increases the number of images associated with a single application target image RIM, thus reducing the number of application target images RIM that require the user to attach the representative training information ATIN. This reduces the user's workload in generation of training images. Note that the image associating section <b>120</b> selects the application target image RIM in the above-described configuration example or alternatively, the image output section <b>130</b> may select the application target image RIM. For example, when there is a plurality of candidates with the same number of images with high similarity as in IX<b>2</b> and IX<b>3</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the image output section <b>130</b> may select any of the plurality of candidates as the application target image. The image output section <b>130</b> may select such as those with a larger number of feature points included in each candidate image as the application target image.</p><p id="p-0068" num="0067">Additionally, in the present embodiment, the input accepting section <b>140</b> accepts representative contour information indicative of a contour of a specific region in the application target image RIM as the representative training information ATIN. The training information applying section <b>150</b> applies the contour information to the medical images included in the associated image group GIM as the training information AT<b>1</b> to ATn.</p><p id="p-0069" num="0068">The specific region refers to a region that is targeted for detection by the AI being machine-learned based on the training images generated by the training data generating system <b>10</b>.</p><p id="p-0070" num="0069">In accordance with the present embodiment, since the contour information indicative of the contour of the specific region is applied to the medical images as the respective training information AT<b>1</b> to ATn, performing machine learning based on those training information enables detection of the specific region by segmentation.</p><p id="p-0071" num="0070">Additionally, in the present embodiment, the image associating section <b>120</b> extracts the feature points FPa to FPc, each indicating a feature of an image, from each medical image in the plurality of medical images IA<b>1</b> to IAm to perform the association of the medical images IB<b>1</b> to IBn each including the feature point in common with each other out of the plurality of medical images IA<b>1</b> to IAm, and thereby generates the associated image group GIM. The training information applying section <b>150</b> uses the common feature points FPa to FPc to apply the contour information indicative of the contour of the specific region to the medical images IB<b>1</b> to IBn included in the associated image group GIM as the training information AT<b>1</b> to ATn.</p><p id="p-0072" num="0071">This enables application of the contour information to each medical image using information of the feature points that are matched when generating the associated image group GIM. As described above, in between the images that each have feature points in common, the representative contour information can be converted into the contour information of each medical image by geometric transformation.</p><heading id="h-0007" level="1">2. Modifications of the First Configuration Example</heading><p id="p-0073" num="0072">In a first modification, similarity of an imaging target is determined based on image similarity. A hardware configuration of the training data generating system <b>10</b> is the same as those in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating a flow of process in the first modification. A description of steps S<b>1</b> and S<b>15</b>, which are similar to steps S<b>1</b> and S<b>5</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, respectively, is omitted here.</p><p id="p-0074" num="0073">In step S<b>12</b>, the image associating section <b>120</b> calculates the image similarity between medical images included in a plurality of medical images. The image similarity is an index of how similar two images are, or equivalently, the index indicating a degree of the similarity of the images themselves. As long as images are similar to each other, the imaging targets seen in those images can be determined to be highly similar. The image similarity adopts, for example, the Sum of Squared Difference (SSD), the Sum of Absolute Difference (SAD), the Normalized Cross Correlation (NCC), and the Zero-mean Normalized Cross Correlation (ZNCC).</p><p id="p-0075" num="0074">In step S<b>13</b>, the image associating section <b>120</b> selects images with a large number of images with high image similarity out of the plurality of medical images as application target images. The image associating section <b>120</b>, for example, compares the image similarity and the threshold to determine the degree of the similarity. The methods of selecting the application target image and setting an associated image group are the same as those described in such as <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The image output section <b>130</b> outputs the selected application target image to the display section <b>300</b>.</p><p id="p-0076" num="0075">In step S<b>14</b>, the input accepting section <b>140</b> accepts input of training information for the displayed application target image. Examples of the training information when employing the image similarity are considered to include, but are not limited to, a text.</p><p id="p-0077" num="0076">In a second modification, display of an association state is performed. <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example of an image displayed on the display section <b>300</b> in the second modification.</p><p id="p-0078" num="0077">The image output section <b>130</b> outputs, to the display section <b>300</b>, association information <b>303</b> between the application target image RIM and each of medical images <b>302</b> other than the application target image RIM out of the plurality of medical images. Reference numeral <b>301</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref> denotes an arrow indicating the passage of time. That is, the medical images <b>302</b> are displayed in time series along the arrow <b>301</b>. The association information <b>303</b> is represented by a line connecting the application target image RIM and the medical image <b>302</b>. The images connected with each line are those that have been determined to be imaging targets with a high degree of similarity and be associated. The images not connected with the line are those that have been determined to be the imaging targets with a low degree of similarity and not be associated.</p><p id="p-0079" num="0078">This enables the user to determine the state of association between the application target image RIM and each medical image. For example, the user can determine whether an appropriate medical image is selected as the application target image RIM.</p><p id="p-0080" num="0079">The input accepting section <b>140</b> accepts input to specify, as new application target images, any of the medical images <b>302</b> other than the application target image RIM. The input accepting section <b>140</b> changes the application target image based on the input accepted. That is, the user views the association information <b>303</b> displayed on the display section <b>300</b> and selects the relevant medical images when he or she determines that there exists an application target image that is more appropriate than the current application target image RIM. The input accepting section <b>140</b> determines the medical image selected by the user as the new application target image.</p><p id="p-0081" num="0080">With this processing, the user can newly select, as an application target image, another medical image that he or she has determined to be appropriate as a medical image to which representative training information is to be applied, to apply the representative training information to the application target image. This enables the generation of training images that can achieve higher accuracy in machine learning.</p><p id="p-0082" num="0081">Note that, after the user selected a new application target image, the image associating section <b>120</b> may reconfigure an associated image group, and then the image output section <b>130</b> may display the relevant association information on the display section <b>300</b>. More specifically, the image associating section <b>120</b> may set, to a new associated image group, the medical images that are highly similar to the application target image newly selected by the user to output the relevant association information to the image output section <b>130</b>.</p><p id="p-0083" num="0082">In a third modification, adjustment for display of specific regions is performed. <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a configuration example of the training data generating system <b>10</b> in the third modification. The processing section <b>100</b> further includes the detecting section <b>160</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The image output section <b>130</b> includes a display adjuster <b>131</b>. Note that the description of components already discussed will be omitted as appropriate.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating a flow of process in the third modification. A description of steps S<b>21</b>, S<b>26</b>, and S<b>27</b>, which are similar to steps S<b>1</b>, S<b>4</b>, and S<b>5</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, respectively, is omitted here.</p><p id="p-0085" num="0084">In step S<b>22</b>, the detecting section <b>160</b> detects a specific region in each medical image out of the plurality of medical images. The specific region is an area to which training information is to be applied. In step S<b>23</b>, the image associating section <b>120</b> extracts feature points from each medical image out of the plurality of medical images to perform matching between the images for the feature points. At this time, the image associating section <b>120</b> may extract feature points based on the specific region detected by the detecting section <b>160</b>. The image associating section <b>120</b>, for example, may extract feature points that characterize a specific region.</p><p id="p-0086" num="0085">In step S<b>24</b>, the display adjuster <b>131</b> scales an application target image depending on conditions of the associated image to display it on the display section <b>300</b>. In the associated image group, for example, the display adjuster <b>131</b> scales the application target image such that a specific region has the appropriate size when the size of the specific region seen in the image is large or small. In step S<b>25</b>, the display adjuster <b>131</b> adjusts such that the detected specific region is easily viewable to display the application target image on the display section <b>300</b>. The display adjuster <b>131</b> performs such as parallel translation of the specific region such that it is located in the center of a screen and a rotation process and projective transformation such that the specific region is easily viewable. The projective transformation represents a transformation that changes a tilt of the plane having a specific region, or equivalently, that changes an angle formed by the plane having the specific region and a camera line-of-sight. Note that either or both of steps S<b>25</b> and S<b>24</b> may be implemented.</p><p id="p-0087" num="0086">In accordance with the present embodiment, the display adjuster <b>131</b> applies geometric transformation to the application target image before outputting it to the display section <b>300</b>. The geometric transformation includes such as parallel translation, a rotation, scaling, a projective transformation, or combinations thereof.</p><p id="p-0088" num="0087">With this processing, the geometric transformation can be performed such that the specific region to which the representative training information is applied by the user is easily viewable. This enables the specific region to be presented to the user in an easily viewable manner, and reduction of the user's workload, or the user to accurately apply the representative training information.</p><p id="p-0089" num="0088">In accordance with the present embodiment, the detecting section <b>160</b> detects information for a specific region from the plurality of medical images through image processing. The display adjuster <b>131</b> performs the geometric transformation based on the information for the detected specific region to adjust display of the specific region. The image processing is an image recognition process that recognizes the position, the area, or the like of the specific region from the image. Various techniques can be used for the image recognition process. One example thereof includes a machine learning-based image recognition technique or a feature amount extraction-based image recognition technique. The detected information for the specific region contains a position, size, contour, or area of the specific region, or combinations thereof. The use of the information as above enables such as the parallel translation, rotation, or scaling to be performed such that the specific region is properly displayed.</p><p id="p-0090" num="0089">With this processing, the detecting section <b>160</b> detects in advance the specific region to which the training information is to be applied and consequently, the display adjuster <b>131</b> can geometrically transform the application target image such that the above specific region is properly displayed.</p><p id="p-0091" num="0090">In a fourth modification, adjustment for the input representative training information is performed. <figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates a configuration example of the training data generating system <b>10</b> in the fourth modification. The input accepting section <b>140</b> includes an input adjuster <b>141</b> in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. Note that the description of components already discussed will be omitted as appropriate.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates a first operation example of the input adjuster <b>141</b>. The input adjuster <b>141</b> detects estimated contour information ATES for a specific region in the application target image through the image processing. The input accepting section <b>140</b> accepts representative contour information ATINb indicative of a contour of a specific region in the application target image as the representative training information. The input adjuster <b>141</b> corrects the representative contour information ATINb based on the estimated contour information ATES when the estimated contour information ATES has equal to or more than a predetermined degree of reliability.</p><p id="p-0093" num="0092">The input adjuster <b>141</b> performs the image recognition process by AI processing to output the estimated contour information ATES and the degree of reliability of the estimation. For example, the degree of reliability for each position along a contour is output. The input adjuster <b>141</b> corrects the representative contour information ATINb based on the estimated contour information ATES for areas where the reliability is equal to or more than the predetermined degree. The input adjuster <b>141</b>, for example, replaces the representative contour information ATINb with the estimated contour information ATES for areas where the reliability is equal to or more than the predetermined degree. The input adjuster <b>141</b> outputs, to the training information applying section <b>150</b>, the corrected representative contour information as the final representative training information ATF.</p><p id="p-0094" num="0093">In accordance with the present embodiment, when estimated contour information made based on the image processing is reliable, the representative contour information input by the user is corrected based on the estimated contour information. For example, the cases are considered in which contour information input by the user is inaccurate, however, even in the presence of such a case, the contour information can be corrected to be accurate. Then, the correction is performed only when the estimated contour information is reliable, thus enabling the highly accurate correction.</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a second operation example of the input adjuster <b>141</b>. The input adjuster <b>141</b> detects the estimated contour information ATES for a specific region in the application target image through the image processing. The input accepting section <b>140</b> accepts the representative contour information ATINb indicative of a contour of a specific region in the application target image as the representative training information. The input adjuster <b>141</b> outputs the representative training information based on the estimated contour information ATES when the estimated contour information ATES has equal to or more than a predetermined degree of reliability, and outputs the representative training information based on the representative contour information ATINb when the estimated contour information ATES has less than the predetermined degree of reliability.</p><p id="p-0096" num="0095">The input adjuster <b>141</b> performs the image recognition process by the AI processing to output the estimated contour information ATES and the degree of reliability of the estimation. For example, the degree of reliability for the entire contour is output. The input adjuster <b>141</b> outputs, to the training information applying section <b>150</b>, the estimated contour information ATES as the final representative training information ATF in the presence of equal to or more than the predetermined degree of reliability. Additionally, the input adjuster <b>141</b> outputs, to the training information applying section <b>150</b>, the representative contour information ATINb as the final representative training information ATF in the presence of less than the predetermined degree of reliability. <figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates the case where the estimated contour information ATES is selected.</p><p id="p-0097" num="0096">In a fifth modification, the feature point is extracted again based on the input representative training information. A hardware configuration of the training data generating system <b>10</b> is the same as those in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0098" num="0097">The image associating section <b>120</b> extracts a feature point indicative of a feature of an image from each medical image in the plurality of medical images to perform the association of medical images each including the feature point in common with each other out of the plurality of medical images, and thereby generates an associated image group. The input accepting section <b>140</b> accepts representative contour information indicative of a contour of a specific region in an application target image as representative training information. The image associating section <b>120</b> extracts the feature point to perform the association again based on a position of the contour of the representative contour information when the contour of the representative contour information is away from the feature point by equal to or more than a predetermined distance.</p><p id="p-0099" num="0098">Specifically, the image associating section <b>120</b> calculates distance between contour of the representative contour information and a feature point of the application target image. The distance represents, for example, the distance between the closest point of the contour to a feature point and the feature point. When the application target image has a plurality of feature points, the distance is determined for each feature point, of which such as the shortest or average distance is determined as the final distance. The image associating section <b>120</b> extracts the feature points in each medical image again when the obtained distance is equal to or more than a predetermined distance. Then, the image associating section <b>120</b> extracts the feature point present near the contour of the representative contour information input by the user to reconfigure the associated image group with the feature point.</p><p id="p-0100" num="0099">In accordance with the present embodiment, the feature point is extracted again based on the contour of the representative contour information input by the user, thus allowing the feature point relevant to the specific region recognized by the user to be extracted again. The reconfiguration of the associated image group with the feature point yields the associated image group which is associated properly by the feature point of the specific region. This enables the training information applying section <b>150</b> to accurately apply the contour information to each medical image based on the representative contour information.</p><heading id="h-0008" level="1">3. Second Configuration Example</heading><p id="p-0101" num="0100">A second configuration example of the training data generating system <b>10</b> is now described. A hardware configuration of the training data generating system <b>10</b> is the same as those in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart illustrating a flow of process in the second configuration example. A description of steps S<b>31</b> and S<b>32</b>, which are similar to steps S<b>1</b> and S<b>2</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, respectively, is omitted here.</p><p id="p-0102" num="0101">In step S<b>33</b>, the image associating section <b>120</b> synthesizes a plurality of medical images based on the results of association in step S<b>32</b> to build a three-dimensional model. In step S<b>34</b>, the image output section <b>130</b> adjusts a position of the three-dimensional model such that more corresponding points in the associated medical images are captured, to output a two-dimensional image of the position as an application target image to the display section <b>300</b>. In step S<b>35</b>, the input accepting section <b>140</b> accepts input of representative training information for the application target image. Examples of the training information are considered to include, but are not limited to, a bounding box, a contour, and a region.</p><p id="p-0103" num="0102">In step S<b>36</b>, the input accepting section <b>140</b> automatically adjusts the input representative training information so as to match the adjacent contour in the three-dimensional model, or automatically adjusts the input representative training information depending on the depth of the three-dimensional model. In step S<b>37</b>, the training information applying section <b>150</b> applies the training information to the medical image or region associated with the three-dimensional model based on the applied representative training information to the three-dimensional model.</p><p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates an operation of the image associating section <b>120</b>. The feature points are extracted from the medical images IB<b>1</b> to IBn as in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, which is not illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, and the medical images with high similarities of the imaging target are associated with each other as the associated image group GIM.</p><p id="p-0105" num="0104">The image associating section <b>120</b> uses association of the medical images IB<b>1</b> to IBn included in the associated image group GIM to synthesize a three-dimensional model MDL based on the medical images IB<b>1</b> to IBn. Specifically, the image associating section <b>120</b> estimates at what camera position and line-of-sight direction each medical image has been captured based on the feature points in each medical image and association of the feature points between the medical images with each other to estimate the three-dimensional model MDL based on the estimation result. Methods for synthesizing the three-dimensional model MDL can employ the Structure from Motion (SfM), for example.</p><p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates operations of the image output section <b>130</b>, the input accepting section <b>140</b>, and the training information applying section <b>150</b>. The image output section <b>130</b> generates application target images RIM<b>1</b> and RIM<b>2</b> based on the three-dimensional model MDL. The application target images RIM<b>1</b> and RIM<b>2</b> each represent the two-dimensional image when the three-dimensional model MDL is viewed from visual lines VL<b>1</b> and VL<b>2</b>, respectively. For example, the visual line VL<b>2</b> is viewed in the opposite direction to the visual line VL<b>1</b>. That is, given that the appearance of the three-dimensional model MDL in the application target image RIM<b>1</b> is the front, the appearance of the three-dimensional model MDL in the application target image RIM<b>2</b> is therefore the back.</p><p id="p-0107" num="0106">The user applies the representative training information ATIN<b>1</b> and ATIN<b>2</b> to the application target images RIM<b>1</b> and RIM<b>2</b>, respectively. <figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates a case where training information represents contour information. The input accepting section <b>140</b> applies the representative training information to the three-dimensional model MDL based on the representative training information ATIN<b>1</b> and ATIN<b>2</b>. That is, the input accepting section <b>140</b> generates a contour of a specific region in the three-dimensional model MDL based on the contour of the representative training information ATIN<b>1</b> and ATIN<b>2</b>. For example, when the specific region is a convex portion, the boundary defined by the convex portion and the surface other than that of the convex portion is the contour of the specific region in the three-dimensional model MDL.</p><p id="p-0108" num="0107">The training information applying section <b>150</b> applies the training information AT<b>1</b> to ATn to the medical images IB<b>1</b> to IBn in the associated image group GIM based on the applied representative training information to the three-dimensional model MDL. Upon generation of the three-dimensional model MDL, a camera position and a line-of-sight direction responsive to each medical image are estimated. In addition, as long as a three-dimensional model is generated, the distance between each point of the model and the camera position is also estimated. The training information applying section <b>150</b> uses the camera position and the line-of-sight direction to convert the representative training information for the three-dimensional model MDL into the training information AT<b>1</b> to ATn for the medical images IB<b>1</b> to IBn.</p><p id="p-0109" num="0108">In accordance with the present embodiment, the image output section <b>130</b> outputs, to the display section <b>300</b>, an image generated with the associated image group GIM as an application target image.</p><p id="p-0110" num="0109">Since the associated image group GIM is associated based on similarities of an imaging target, the image generated with such associated image group GIM becomes the image relevant to the imaging target. Displaying such image as the application target image to the display section <b>300</b> properly presents the imaging target to the user.</p><p id="p-0111" num="0110">Additionally, in the present embodiment, the image output section <b>130</b> generates the three-dimensional model MDL of the imaging target based on the associated image group GIM to output, to the display section <b>300</b>, the images RIM<b>1</b> and RIM<b>2</b> made based on the three-dimensional model MDL as the application target image.</p><p id="p-0112" num="0111">This enables the three-dimensional model MDL of the imaging target to be presented to the user. The use of the three-dimensional model MDL allows for proper application of training information to lesions and other area where a three-dimensional structure is considered important. Furthermore, it is possible, irrespective of how an imaging target appears in each medical image, to present the user with a three-dimensional model MDL viewed from any camera line-of-sight and display an application target image suitable for applying representative training information.</p><p id="p-0113" num="0112">Specifically, the image output section <b>130</b> displays an application target image when viewing the three-dimensional model MDL at camera line-of-sight where more corresponding points are captured. The corresponding points represent feature points in common with between medical images that are used to build the three-dimensional model MDL. Since association of the feature points is used to build the three-dimensional model MDL, positions of the feature points in the three-dimensional model MDL are known. It is thus possible to know at what camera line-of-sight when viewing the three-dimensional model MDL more corresponding points are captured, so that the application target image viewed at such camera line-of-sight is displayed. <figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates two of the application target images RIM<b>1</b> and RIM<b>2</b>. For example, the image viewed at the camera line-of-sight with the largest number of corresponding points is selected as any one of the images RIM<b>1</b> and RIM<b>2</b>.</p><p id="p-0114" num="0113">The fact that an image with more corresponding points being captured is displayed indicates that an area with highly accurate association is displayed when building the three-dimensional model MDL based on the associated image group GIM. Such application target image is considered to have a highly accurate association with each medical image in the associated image group GIM. This ensures that highly accurate training information can be generated when the training information is applied to each medical image based on the representative training information applied to the application target image.</p><p id="p-0115" num="0114">Additionally, in the present embodiment, the image output section <b>130</b> outputs, to the display section <b>300</b>, the plurality of two-dimensional images RIM<b>1</b> and RIM<b>2</b> with different line-of-sight directions with respect to the three-dimensional model MDL as the plurality of application target images. The input accepting section <b>140</b> accepts the representative contour information indicative of a contour of a specific region in the application target image as representative training information ATIN<b>1</b> and ATIN<b>2</b> for each of the plurality of the two-dimensional images. The training information applying section <b>150</b> applies contour information to the medical images IB<b>1</b> to IBn included in the associated image group GIM as the training information AT<b>1</b> to ATn.</p><p id="p-0116" num="0115">When the three-dimensional model MDL is viewed at a single line-of-sight direction, areas may be present that are invisible behind such as a convex portion. In accordance with the present embodiment, displaying the plurality of two-dimensional images with the different line-of-sight directions displays the two-dimensional images in which the three-dimensional model MDL is viewed from various directions and applies the representative contour information to each of the two-dimensional images. As a result, contour information is applied to the three-dimensional model MDL without missing. Note that the number of two-dimensional images displayed is not limited to two, and may be multiple while two of the two-dimensional images are displayed as the application target image in <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0117" num="0116">In the above, the description has been given using an example of a case where the &#x201c;image generated with an associated image group&#x201d; is an image based on the three-dimensional model MDL, but not limited thereto, a developed view, illustrating such as digestive tract, generated based on the associated image group GIM may be applicable as in the modifications described below.</p><heading id="h-0009" level="1">4. Modifications of the Second Configuration Example</heading><p id="p-0118" num="0117">In a first modification, adjustment of input of representative contour information is performed. A hardware configuration of the training data generating system <b>10</b> is the same as those in <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0119" num="0118">The input accepting section <b>140</b> accepts the representative contour information indicative of a contour of a specific region in the application target images RIM<b>1</b> and RIM<b>2</b> as the representative training information ATIN<b>1</b> and ATIN<b>2</b>. The input adjuster <b>141</b> adjusts the representative contour information input based on the contour and depth of the three-dimensional model MDL.</p><p id="p-0120" num="0119">The depth represents the depth of the three-dimensional model MDL when viewed as the two-dimensional image displayed as the application target image and corresponds to camera line-of-sight when generating the two-dimensional image. For example, in a case where a convex portion of the three-dimensional model MDL is contoured, when the contour applied by the user is viewed in a depth direction, a plurality of surfaces of the three-dimensional model MDL may be present in such depth direction. In such cases, the input adjuster <b>141</b> adjusts the representative contour information such that the closest surface is contoured. The input adjuster <b>141</b> also adjusts the representative contour information so as to match the contour of the three-dimensional model MDL adjacent to the representative contour information when the representative contour information applied by the user is apart from a contour of the three-dimensional model MDL in the application target image.</p><p id="p-0121" num="0120">In accordance with the present embodiment, the representative contour information in the three-dimensional model MDL can be applied properly based on the representative contour information applied to the application target image, which is a two-dimensional image. That is, the depth of the three-dimensional model MDL is considered, and thereby the representative contour information is applied to the contour intended by the user. Furthermore, consideration of the contour of the three-dimensional model MDL allows correction to accurate contour information even in the case where, for example, the contour information input by the user is inaccurate.</p><p id="p-0122" num="0121">In a second modification, detection of an input error of representative contour information is performed. A hardware configuration of the training data generating system <b>10</b> is the same as those in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates the second modification. <figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates one application target image, but a plurality of application target images may be displayed as described above.</p><p id="p-0123" num="0122">The image output section <b>130</b> generates a two-dimensional image based on the three-dimensional model MDL to output the two-dimensional image as the application target image RIM. The input accepting section <b>140</b> accepts representative contour information ATINc indicative of a contour of a specific region in the application target image RIM as representative training information. In a case where the three-dimensional model MDL is absent on the depth of the contour of the representative contour information ATINc in a line-of-sight direction when the two-dimensional image is generated based on the three-dimensional model MDL, the input accepting section <b>140</b> outputs error information instructing a change of the line-of-sight direction.</p><p id="p-0124" num="0123"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows that the three-dimensional model MDL displayed in the application target image RIM and the representative contour information ATINc do not overlap. That is, the three-dimensional model MDL is absent on the depth of the contour of the representative contour information ATINc in the line-of-sight direction. For this case, the input accepting section <b>140</b> outputs the error information. For example, the image output section <b>130</b> receives input of error information and causes the display section <b>300</b> to display indication prompting the user to change the line-of-sight direction based on the error information. Alternatively, the image associating section <b>120</b> receives input of error information and performs again association of medical images to generate again an associated image group and a three-dimensional model based on the error information. And then, the image output section <b>130</b> displays the application target image again.</p><p id="p-0125" num="0124">In accordance with the present embodiment, upon reception of input of the representative contour information ATINc inappropriate for the three-dimensional model MDL, the error information is output to prevent such inappropriate representative contour information from being used. For example, the cases are considered in which an appropriate three-dimensional model is not generated, and camera line-of-sight is inappropriate when presenting the three-dimensional model to the user. In accordance with the present embodiment, even in the presence of such cases, the inappropriate representative contour information can be prevented from being used.</p><p id="p-0126" num="0125">In a third modification, generation of a developed view based on an associated image group and display of an application target image based on the developed view are performed. In other words, in the third modification, the &#x201c;image generated with an associated image group&#x201d; is a developed view. <figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart illustrating a flow of process in the third modification. A description of steps S<b>41</b> and S<b>42</b>, which are similar to steps S<b>1</b> and S<b>2</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, respectively, is omitted here. <figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates the third modification.</p><p id="p-0127" num="0126">In step S<b>43</b>, the image associating section <b>120</b> synthesizes the medical images IB<b>1</b> to IBn based on association of the medical images IB<b>1</b> to IBn in the associated image group GIM to generate a developed view TKZ. Specifically, the image associating section <b>120</b> stacks the medical images IB<b>1</b> to IBn based on correspondence of feature points between medical images with each other such that positions of the corresponding feature points are consistent with each other, and thereby generates the developed view TKZ. <figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates the state of stacking the images IB<b>1</b> to IB<b>3</b> among IB<b>1</b> to IBn. The images IB<b>4</b> to IBn are also to be stacked.</p><p id="p-0128" num="0127">The image output section <b>130</b> outputs, to the display section <b>300</b>, an application target image based on the developed view TKZ. The image output section <b>130</b>, for example, may display the entire developed view TKZ on the display section <b>300</b>, or alternatively, may display a part of the developed view TKZ in which a specific region is captured, on the display section <b>300</b>.</p><p id="p-0129" num="0128">In step S<b>44</b>, the input accepting section <b>140</b> accepts the representative training information ATIN input by the user. <figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates a case where training information represents contour information. The input accepting section <b>140</b> automatically adjusts the input representative training information so as to match the adjacent contour in the developed view.</p><p id="p-0130" num="0129">In step S<b>46</b>, the training information applying section <b>150</b> applies the training information to the medical image or region associated with the developed view based on the representative training information applied to the developed view. In step S<b>43</b>, since the medical images IB<b>1</b> to IBn are stacked to generate the developed view TKZ, positions of each medical image in the developed view TKZ are known. The training information applying section <b>150</b> uses the positions of each medical image in the developed view TKZ to convert the representative training information ATIN into the training information for each medical image.</p><p id="p-0131" num="0130">Although the embodiments to which the present disclosure is applied and the modifications thereof have been described in detail above, the present disclosure is not limited to the embodiments and the modifications thereof, and various modifications and variations in components may be made in implementation without departing from the spirit and scope of the present disclosure. The plurality of elements disclosed in the embodiments and the modifications described above may be combined as appropriate to implement the present disclosure in various ways. For example, some of all the elements described in the embodiments and the modifications may be deleted. Furthermore, elements in different embodiments and modifications may be combined as appropriate. Thus, various modifications and applications can be made without departing from the spirit and scope of the present disclosure. Any term cited with a different term having a broader meaning or the same meaning at least once in the specification and the drawings can be replaced by the different term in any place in the specification and the drawings.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A training data generating system comprising a processor, the processor being configured to implement:<claim-text>acquiring a plurality of medical images;</claim-text><claim-text>associating medical images with each other which are included in the plurality of medical images based on similarities of an imaging target to generate an associated image group comprising medical images associated with each other;</claim-text><claim-text>outputting, to a display, an application target image to be an image as an application target of representative training information based on the associated image group;</claim-text><claim-text>accepting input of representative contour information indicative of a contour of a specific region in the application target image as the representative training information; and</claim-text><claim-text>applying contour information, as training information, to each medical image included in the associated image group based on the representative training information input to the application target image.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The training data generating system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the processor outputs, to the display, a medical image selected from the associated image group as the application target image.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The training data generating system as defined in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>in a case where the number of medical images associated with each medical image in the association based on the similarity is considered as the number of associated images,</claim-text><claim-text>the processor outputs, to the display, a medical image selected based on the number of associated images as the application target image.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The training data generating system as defined in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the processor selects the application target image from the plurality of medical images in the association based on the number of medical images associated with each medical image to determine a medical image associated with the application target image out of the plurality of medical images as the associated image group.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The training data generating system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the processor extracts a feature point indicative of a feature of an image from each medical image in the plurality of medical images to perform the association of medical images each including the feature point in common with each other out of the plurality of medical images, and thereby generates the associated image group, and</claim-text><claim-text>uses the common feature point to apply the contour information indicative of the contour of the specific region to the medical images included in the associated image group as the training information.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The training data generating system as defined in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the processor applies geometric transformation to the application target image before outputting the application target image to the display.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The training data generating system as defined in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the processor detects information of the specific region from the plurality of medical images through image processing and</claim-text><claim-text>performs the geometric transformation based on the information of the specific region to adjust display of the specific region.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The training data generating system as defined in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the processor detects estimated contour information of the specific region from the application target image through image processing and when the estimated contour information has equal to or more than a predetermined degree of reliability, corrects the representative contour information based on the estimated contour information.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The training data generating system as defined in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the processor detects estimated contour information of the specific region from the application target image through image processing, outputs the representative training information based on the estimated contour information when the estimated contour information has equal to or more than a predetermined degree of reliability, and outputs the representative training information based on the representative contour information when the estimated contour information has less than the predetermined degree of reliability.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The training data generating system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the processor extracts a feature point indicative of a feature of an image from each medical image in the plurality of medical images to perform the association of medical images each including the feature point in common with each other out of the plurality of medical images, and thereby generates the associated image group, and</claim-text><claim-text>extracts the feature point to perform the association again based on a position of the contour of the representative contour information when the contour of the representative contour information is away from the feature point by equal to or more than a predetermined distance.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The training data generating system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the processor outputs, to the display, association information between the application target image and the medical images other than the application target image out of the plurality of medical images.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The training data generating system as defined in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>the processor accepts input to specify any of the medical images other than the application target image out of the plurality of medical images as the new application target image to change the application target image based on the accepted input.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The training data generating system as defined in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the processor outputs, to the display, the image generated by using the associated image group as the application target image.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The training data generating system as defined in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein<claim-text>the processor generates a three-dimensional model of the imaging target based on the associated image group to output, to the display, the image generated based on the three-dimensional model as the application target image.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The training data generating system as defined in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein<claim-text>the processor outputs, to the display, a plurality of two-dimensional images with different line-of-sight directions with respect to the three-dimensional model as the plurality of application target images,</claim-text><claim-text>accepts input of the representative contour information for each of the plurality of two-dimensional images as the representative training information, and</claim-text><claim-text>applies the contour information to each medical image included in the associated image group as the training information.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The training data generating system as defined in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein<claim-text>the processor adjusts the representative contour information input based on the contour and depth of the three-dimensional model.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The training data generating system as defined in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein<claim-text>the processor generates a two-dimensional image based on the three-dimensional model to output the two-dimensional image as the application target image, and</claim-text><claim-text>in a case where the three-dimensional model is absent on the depth of the contour of the representative contour information in a line-of-sight direction when the two-dimensional image is generated based on the three-dimensional model, outputs error information instructing a change of the line-of-sight direction.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A method of generating training data, the method comprising:<claim-text>acquiring a plurality of medical images;</claim-text><claim-text>associating medical images with each other which are included in the plurality of medical images based on similarities of an imaging target to generate an associated image group comprising medical images associated with each other;</claim-text><claim-text>outputting, to a display, an application target image to be an image as an application target of representative training information based on the associated image group;</claim-text><claim-text>accepting input of representative contour information indicative of a contour of a specific region in the application target image as the representative training information; and</claim-text><claim-text>applying contour information, as training information, to each medical image included in the associated image group based on the representative training information input to the application target image.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory computer readable recording medium storing thereon a computer program that causes a computer to perform a method comprising:<claim-text>acquiring a plurality of medical images;</claim-text><claim-text>associating medical images with each other which are included in the plurality of medical images based on similarities of an imaging target to generate an associated image group comprising medical images associated with each other;</claim-text><claim-text>outputting, to a display, an application target image to be an image as an application target of representative training information based on the associated image group;</claim-text><claim-text>accepting input of representative contour information indicative of a contour of a specific region in the application target image as the representative training information; and</claim-text><claim-text>applying contour information, as training information, to each medical image included in the associated image group based on the representative training information input to the application target image.</claim-text></claim-text></claim></claims></us-patent-application>