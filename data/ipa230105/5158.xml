<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005159A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005159</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363368</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>194</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>174</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>194</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>174</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>002</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20224</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">IMAGE BACKGROUND ALTERATIONS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>HEWLETT-PACKARD DEVELOPMENT COMPANY, L.P.</orgname><address><city>Spring</city><state>TX</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>DAL ZOTTO</last-name><first-name>Rafael</first-name><address><city>Porto Alegre</city><country>BR</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Executable code causes a processor to segment a first frame to determine a background portion of the first frame, and segment a second frame to determine a background portion of the second frame. The executable code causes the processor to compare the background portion of the first frame to the background portion of the second frame to determine a difference between the first frame and the second frame. The executable code also causes the processor to alter the background portion of the second frame responsive to the difference. The executable code causes the processor to display the altered second frame on a display.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="95.33mm" wi="139.11mm" file="US20230005159A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="141.14mm" wi="105.24mm" orientation="landscape" file="US20230005159A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="147.07mm" wi="100.16mm" orientation="landscape" file="US20230005159A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="189.57mm" wi="144.02mm" file="US20230005159A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="169.42mm" wi="110.15mm" orientation="landscape" file="US20230005159A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="193.72mm" wi="110.15mm" orientation="landscape" file="US20230005159A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="169.42mm" wi="110.15mm" orientation="landscape" file="US20230005159A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">The use of background effects in real-time video is a feature offered by many video conference providers. These background effects allow a user to blur, obscure, or replace the user's background, which may be distracting to other viewers, particularly when there is movement in the background.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0002" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0003" num="0002">Various examples are described below referring to the following figures:</p><p id="p-0004" num="0003"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a block diagram of a system for image background alteration in accordance with various examples.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a diagram depicting human segmentation of an image in accordance with various examples.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow diagram of a process for background image alteration in accordance with various examples.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of a system including a non-transitory computer-readable medium storing executable code for background image alteration in accordance with various examples.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram of an electronic device for background image alteration in accordance with various examples.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram of a system including a non-transitory computer-readable medium storing executable code for background image alteration in accordance with various examples.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0003" level="1">DETAILED DESCRIPTION</heading><p id="p-0010" num="0009">As described above, the use of background effects in real-time video is a feature offered by many video conference providers. These background effects allow a user to blur or obscure the user's background, which may be distracting to other viewers, particularly when there is movement in the background. However, the user has to enable this feature. If the user forgets to enable the feature or does not timely enable the feature, certain information or images may be unintentionally shared, some of which may be distracting, embarrassing, personal, business confidential, or otherwise unsuitable for sharing.</p><p id="p-0011" num="0010">In examples of this disclosure, machine learning techniques and image transformation techniques are used to automatically perform a mitigation action on the background responsive to detecting certain conditions. The examples herein intelligently apply mitigation actions to the user's background (e.g., obscure the background, such as by blurring) responsive to the detection of certain conditions in the background to avoid distractions. The users may determine how to obscure the background in some examples (such as inserting blur, images, video, etc. into the background). Also, the examples herein may automatically perform mitigation actions on the background without the user having to enable the feature.</p><p id="p-0012" num="0011">In examples, an input video (e.g., from a web camera) is decomposed into a series of frames that are processed in sequence. Human segmentation is used to distinguish the human and non-human (e.g., background) areas of each frame, and motion detection is applied to the non-human areas to determine whether blur or another mitigation action should be performed on the video. First, the person or persons in the foreground of the video are segmented from the person's background. Human segmentation is based on face and body part recognition, with the goal of separating the person or persons in the frame from the background. Other components in the foreground may also be segmented from the background in some examples. A variety of different solutions and techniques may be used to perform the segmentation. In one example, pixels of an image are classified as person or non-person. Then, the non-person portion of the image is analyzed using motion detection to determine whether there is movement in the background.</p><p id="p-0013" num="0012">To identify movement in the background, a previous frame is compared to a current frame to detect a difference in the pixels in the background. A user can set a sensitivity level of the system, which may allow some amount of background movement to occur before a mitigation action is taken. If there is a difference in the pixels greater than the sensitivity level, movement of a background object is detected. After movement is detected, a variety of mitigation actions may be taken. One mitigation action is to apply a blur effect to the background of the image. The blur effect may be applied to the entire background or just to a portion of the background. The blur may be applied instantly or gradually over a number of frames. The user may select how quickly the blur is applied. In another mitigation action, instead of blurring the background, a white or dark color may be applied to the background. In another mitigation action, an image may be applied to the background, where the image applied is pre-selected by the user.</p><p id="p-0014" num="0013">In one example, a non-transitory computer-readable medium is provided. The non-transitory computer-readable medium includes all electronic mediums or media of storage, except signals. The non-transitory computer-readable medium stores executable code. When executed by a processor of an electronic device, the executable code causes the processor to segment a first frame to determine a background portion of the first frame, and segment a second frame to determine a background portion of the second frame. The executable code causes the processor to compare the background portion of the first frame to the background portion of the second frame to determine a difference between the first frame and the second frame. The executable code also causes the processor to alter the background portion of the second frame responsive to the difference. The executable code causes the processor to display the altered second frame on a display.</p><p id="p-0015" num="0014">In another example, an electronic device is provided. The electronic device includes a processor. The processor receives a video including a first frame and a second frame. The processor also segments the first frame to distinguish a person in the first frame from a background of the first frame, and segments the second frame to distinguish a person in the second frame from a background of the second frame. The processor compares the background of the second frame to the background of the first frame. Responsive to the background of the second frame matching the background of the first frame, the processor displays the second frame. Responsive to the background of the second frame being different than the background of the first frame, the processor blurs a portion of the second frame and displays the second frame with the blurred portion.</p><p id="p-0016" num="0015">In another example, a non-transitory computer-readable medium is provided. The non-transitory computer-readable medium includes all electronic mediums or media of storage, except signals. The non-transitory computer-readable medium stores executable code. When executed by a processor of an electronic device, the executable code causes the processor to store background pixels of a first image frame. The executable code causes the processor to compare the background pixels of the first image frame to background pixels of a second image frame. Responsive to determining a difference between the background pixels of the first image frame and the background pixels of the second image frame, the executable code causes the processor to alter a portion of the background pixels of the second image frame. The executable code also causes the processor to display the second image frame with the altered background pixels.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a block diagram of a system <b>100</b> for image background alteration in accordance with various examples. System <b>100</b> is an electronic device that may be a desktop computer, notebook, laptop, tablet, smartphone, or other electronic device. System <b>100</b> includes a processor <b>102</b>, memory <b>104</b>, display <b>106</b>, and video source <b>108</b>. Memory <b>104</b> includes executable code <b>110</b> and <b>112</b>. Processor <b>102</b> may be a central processing unit, a graphics processing unit, a microprocessor, a microcontroller, or any other component for performing the operations described herein. Memory <b>104</b> may be a hard drive, a solid-state drive (SSD), flash memory, random access memory (RAM), or other suitable memory device. Memory <b>104</b> may also include multiple memory devices or chips in some examples. Display <b>106</b> is any suitable type of display for displaying moving or static images. Video source <b>108</b> may be a camera that captures video and provides the video to system <b>100</b> for display in an example. Video source <b>108</b> may be a stored video file or files in another example.</p><p id="p-0018" num="0017">In operation of an example described herein, video is provided by video source <b>108</b> to system <b>100</b>. In one example, video source <b>108</b> is a camera such as a web camera on a personal computer. A user participates in a video call using the web camera in one example. The user may use any commercial or proprietary video conference application to engage in the video call. In this example, the user is in front of the camera and the user's image is being transmitted to other viewers of the video call. The background of the environment that the user is situated in is also captured by the camera and transmitted to the other viewers. The user may be situated in an office, a home, an airport, a restaurant, a train station, or another public or private place. Some commercial video conference applications allow the user to replace the user's actual background with a video or image. However, if the user wishes to use the actual background of the environment the user is situated in, motion of people or other objects in the background may be distracting to the other viewers.</p><p id="p-0019" num="0018">In an example, system <b>100</b> performs segmentation and motion detection to determine if motion is occurring in the background of the video. Segmentation may be performed by processor <b>102</b> executing executable code <b>110</b> stored in memory <b>104</b>. Segmentation is performed to distinguish the foreground of the video (such as the user or users on the video call) from the background of the video. Segmentation may be performed on each frame of the video as those frames are received from video source <b>108</b>. After segmentation is performed, motion detection is performed by analyzing the background of the video frames. Processor <b>102</b> performs motion detection by executing executable code <b>112</b> stored in memory <b>104</b>. In one example, motion detection is performed by comparing pixels of a first frame to pixels of a second frame. The second frame may be the immediate frame after the first frame in an example. In another example, the second frame may be separated from the first frame by any number of other frames. Motion is detected by comparing the background portion of the first frame to the background portion of the second frame. If a change is detected in the background portion from the first frame to the second frame, motion is detected. Additional aspects of motion detection are described with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref> below. If no motion is detected in the background, or if a small acceptable amount of motion is detected in the background, the background is displayed normally, without alteration, to the viewers on the video call. If an amount of background motion is detected that is above an acceptable threshold, a mitigation action may be taken to alter the background. The mitigation action is performed to obscure, partially obscure, or otherwise block the background motion in the video, so certain images in the background (e.g., distracting movement) are not unintentionally shared.</p><p id="p-0020" num="0019">A change or difference in the background pixels between the first frame and the second frame may be detected using any of a variety of methods. As one example, a pixel-by-pixel comparison may be performed between the two frames. The color and/or the light intensity of the pixels may be compared. Image histograms may be used to compare the frames in another example. A histogram may be a graph of the pixel intensity for a frame. One of both of the frames may be aligned, resized, or cropped before the comparison is performed in some examples. One or both of the frames may be normalized before the comparison, which may help if light levels change from one frame to the other. Noise reduction may be performed on the frames in some examples. The frames may be compressed before the comparison is performed in another example. The frames may be converted to grayscale before the comparison in some examples. For any of the comparison methods and techniques, a threshold may be set, and changes below the threshold may be ignored in some examples. If the amount of change between the two frames exceeds the threshold, a mitigation action may be performed.</p><p id="p-0021" num="0020">One example of a mitigation action is blurring all or a portion of the background. As one example, if a user is engaged in a video call in a coffee shop, background motion may be detected if another customer walks through the field of view of the camera behind the user. The background may be blurred to obscure the other customer walking past the camera. In some examples, the entire background is blurred. In other examples, portions of the background may be blurred while other portions of the background remain un-blurred. A user setting may allow the user to determine whether all or just a portion of the background should be blurred. The user setting may also allow the user to select how much of the background should be blurred when motion is detected. For example, the pixels that change from the first frame to the second frame may be blurred while other pixels that do not change from the first frame to the second frame remain un-blurred. In another example, the pixels that change from frame to frame may be blurred along with pixels near or adjacent to the pixels that changed. Other pixels could remain un-blurred. The user may adjust the settings to select how much background blur is applied when motion is detected. In another example, the user may also adjust a setting to select whether the blur is a heavy blur or a light blur. A heavy blur may obscure the pixels more than a light blur.</p><p id="p-0022" num="0021">The user may also select how quickly or gradually a blur is applied to the video if motion is detected. In one example, each frame of the video is compared to the next sequential frame to detect motion. If motion occurs over a large number of sequential frames, the background may be blurred gradually, with the blur increasing as the motion continues. A light blur could be applied to the early frames, with the later frames receiving heavier blur. Stated another way, blur may be applied with a fade-in effect. If the motion in the background stops, the blur may be removed gradually over a number of subsequent frames by using a fade-out effect in some examples.</p><p id="p-0023" num="0022">Applying blur or another mitigation action may be stopped when the motion stops. If two sequential frames match, or if the difference between the two frames is below a threshold for motion detection, system <b>100</b> may stop applying the mitigation action to the frames. The two frames would be displayed normally, without any mitigation action performed on the background. Normal display of the background would continue, without any mitigation action, until motion is detected again.</p><p id="p-0024" num="0023">Another example of a mitigation action is to apply a solid color (black, white, blue, etc.) over the background instead of blurring the background if motion is detected. The user could select the color, along with how gradually to apply the color to the background (e.g., instantly or over a specific number of frames). The solid color could obscure the background so that motion in the background is not distracting to other viewers. In addition, the solid color could be applied to either the whole background or a portion of the background, as determined by the user.</p><p id="p-0025" num="0024">Another example of a mitigation action is to apply an image or video over the background if motion is detected. The user could select an image or video to apply to the background if motion is detected. The image or video could be any suitable image or video, such as a photograph, a painting, a patterned image, a video with an acceptable amount of motion, etc. The image or video could be applied instantly when motion is detected or gradually over a number of frames. The image or video could also be applied to a portion of the background or the entire background. The user may select any suitable image or vide that the user believes is sufficient to obscure motion in the background of the video.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a diagram depicting human segmentation of an image <b>150</b> in accordance with various examples. Human segmentation is a process based on face and body part recognition, with the goal of separating the person in the frame from the background. A number of suitable techniques may be used for performing human segmentation, including the use of commercial segmentation products. In one example, a machine learning model with a neural network distinguishes faces and body parts from the background of a video. The neural network groups pixels from an image or frame into semantic areas of objects. Pixels are classified into two classes: person and non-person.</p><p id="p-0027" num="0026">Image <b>150</b> shows an outline of a person positioned in front of a camera. One frame of the video is shown as image <b>150</b>. In this example, a segmentation program, such as executable code <b>110</b> executed by processor <b>102</b>, has classified portion <b>152</b> as a person. Executable code <b>110</b> identified a number of points <b>154</b>A to <b>154</b>J as body parts of a person. Points <b>154</b>A, <b>154</b>B, <b>154</b>C, and <b>154</b>D are identified as arms and shoulders of a person in image <b>150</b>. Points <b>154</b>E, <b>154</b>F, <b>154</b>G, <b>154</b>H, and <b>154</b>J are identified as body parts of a person's face in image <b>150</b>. Therefore, the portion <b>152</b> is identified as a person. The other portion of image <b>150</b>, which is portion <b>156</b>, is classified as non-person. In examples herein, the non-person portion <b>156</b> is considered the background, and the non-person portion <b>156</b> is where motion detection is applied via executable code <b>112</b>. Therefore, portion <b>156</b> represents the background pixels of image <b>150</b>.</p><p id="p-0028" num="0027">Segmentation is performed on each frame of a video source as the frames are received by system <b>100</b>. In an example, a first frame is received from video source <b>108</b>. Processor <b>102</b> performs segmentation (using executable code <b>110</b>) on the first frame to classify each pixel of the first frame as person or non-person. The first frame and the classification of each pixel are stored in memory <b>104</b> in one example. Next, a second frame is received from video source <b>108</b>. Processor <b>102</b> performs segmentation on the second frame to classify each pixel of the second frame as person or non-person. The second frame and the classification of each pixel may also be stored in memory <b>104</b>.</p><p id="p-0029" num="0028">Processor <b>102</b> then performs motion detection of the background images using executable code <b>112</b>. Motion detection may be performed using any suitable technique. In one example, motion detection is performed by comparing the non-person pixels of the first frame to the non-person pixels of the second frame. The pixels that were determined to be part of a person are ignored for this analysis. The non-person pixels are compared, and if a pixel changes from the first frame to the second frame, motion is detected for that pixel. If motion is detected, a mitigation action may be taken. The mitigation action is applied to the second frame. The first frame may be sent to the display <b>106</b> for display, and the mitigation action is performed on the second frame before the second frame is sent to display <b>106</b> for display. The mitigation action may include blurring part or all of the background of the second frame, or applying an image or video to all or part of the background of the second frame. After the mitigation action is applied to the second frame, the second frame is sent to display <b>106</b> for display.</p><p id="p-0030" num="0029">Processor <b>102</b> also performs segmentation on the next frame (e.g., a third frame). After segmentation is performed on the third frame, the non-person pixels of the third frame are compared to the non-person pixels of the original (non-altered) second frame. The pixels in the second frame used for the comparison are the pixels as they appeared before the mitigation action was applied to the second frame. If a difference is found between non-person pixels in the third frame and non-person pixels in the second frame, a mitigation action may be applied to the third frame. This process continues for each frame of the video, with each subsequent frame compared to the previous frame.</p><p id="p-0031" num="0030">In some examples, the user may select a sensitivity level for the motion detection process. Stated another way, the user may adjust a setting to allow a certain amount of motion before a mitigation action is applied. For example, if a user is sitting in an office in front of a window, and a tree is visible through the window, the user may allow a small amount of movement of the leaves and branches of the tree without applying a mitigation action. A slight movement of the tree caused by the wind would not result in blur or another mitigation action applied to the background. However, large movements in the background could still result in mitigation actions. For example, if another person walked into the office and was visible through the user's camera, the user settings may classify this amount of movement as enough movement to trigger the motion detection and apply a mitigation action. The user settings may create a predetermined threshold for the number of pixels that change from a first frame to a second frame before motion is detected. The predetermined threshold may be triggered by a number of non-person pixels exceeding the threshold changing from one frame to the next (for example, 100 or more pixels), or the threshold may be triggered by a percentage of the non-person pixels changing from one frame to the next (for example, 5% or more of the non-person pixels). Other predetermined thresholds may be used in other examples for determining that motion has been detected in the background. A user may adjust the predetermined threshold at any time, even while a video is being displayed.</p><p id="p-0032" num="0031">In another example, the user may select how quickly or gradually to apply the mitigation actions. A user may adjust a setting that allows motion in the background for a predetermined number of frames. If the motion continues beyond that predetermined number of frames, a mitigation action is performed on subsequent frames until the motion ceases or drops below an acceptable threshold. In this example, more than two frames may be analyzed for motion detection using executable code <b>112</b>. The mitigation action may be applied if enough pixels change over a predetermined number of frames. If the motion stops after a number of frames that is lower than the predetermined number of frames, the mitigation action may not be applied.</p><p id="p-0033" num="0032">As described herein, motion detection is performed on the background of the image or frame. Therefore, motion of the person in the video, as determined by human segmentation, is ignored. In this example, the pixels that form portion <b>152</b> are ignored for motion detection. In another example, if two or more persons are in the foreground of the frame, the human segmentation process identifies each person, and the pixels that form each person are ignored for motion detection.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow diagram of a process <b>200</b> for background image alteration in accordance with various examples herein. The actions performed in process <b>200</b> may be performed in any suitable order. The hardware components described above with respect to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> may perform process <b>200</b> in some examples.</p><p id="p-0035" num="0034">Process <b>200</b> begins at block <b>210</b>, where a video is received from a video source. The video source may be a camera in one example. In another example, the video source may be a memory that stores the video.</p><p id="p-0036" num="0035">Process <b>200</b> continues at block <b>220</b>, where the video is decomposed into video frames. A processor such as processor <b>102</b> may decompose the video into video frames.</p><p id="p-0037" num="0036">Process <b>200</b> continues at block <b>230</b>, where segmentation is performed to segment a person or persons from the background for each video frame. Segmentation may be performed by processor <b>102</b> executing executable code <b>110</b> in one example. In some examples, machine learning inference hardware may be used, such as chips utilizing field programmable gate arrays (FPGAs). Segmentation may be performed using any suitable techniques, such as commercially available or open-source programs.</p><p id="p-0038" num="0037">Process <b>200</b> continues at block <b>240</b>, where background frame pixels are saved for each video frame. The background frame pixels may be saved in memory <b>104</b> in one example. The background frame pixels for multiple frames may be saved in some examples.</p><p id="p-0039" num="0038">Process <b>200</b> continues at block <b>250</b>, where the previous background frame pixels are compared to current background frame pixels. Processor <b>102</b> may perform the comparison using executable code <b>112</b> in one example. The comparison is performed to determine any difference between the background pixels from frame to frame, which indicates that motion has occurred. As described above, a minimum threshold for motion may be set by a user, so that some motion is allowed without triggering the mitigation actions.</p><p id="p-0040" num="0039">Process <b>200</b> continues at block <b>260</b>, where processor <b>102</b> determines if motion is detected. Motion may be detected by finding a difference between the previous background frame pixels and the current background frame pixels. If the difference between the background pixels in the frames is above a threshold, motion is detected. In some example, the previous background frame and the current background frame may be non-sequential frames in the video source. Some frames may be skipped in some examples. In other examples, more than two frames may be compared to detect motion that may continue across multiple frames.</p><p id="p-0041" num="0040">If motion is detected at block <b>260</b>, process <b>200</b> proceeds to block <b>270</b>. At block <b>270</b>, a mitigation action is performed. The mitigation action may be any action taken to obscure the motion that is detected at block <b>260</b>. For example, the mitigation action may be blurring the background of the current frame or a portion of the background of the current frame. In another example, the mitigation action may be replacing the background or a portion of the background with an image or video. The background or a portion of the background may be replaced with a solid color in some examples. The mitigation action may be applied gradually, over a number of frames, in some examples. After the mitigation action is taken, process <b>200</b> proceeds to block <b>280</b>.</p><p id="p-0042" num="0041">If no motion is detected at block <b>260</b>, process <b>200</b> proceeds to block <b>280</b>. At block <b>280</b>, the frame is released for display. The frame may be provided to any suitable display, such as display <b>106</b>. The frame may also be transmitted over a network to a display. If a mitigation action was performed in block <b>270</b>, the frame shows the results of the mitigation action, such as a blurred background. If no mitigation was taken, the frame is displayed as it was received from the video source.</p><p id="p-0043" num="0042">Process <b>200</b> is repeated for each frame of the video source in one example. Segmentation and motion detection may be performed continuously on the frames of a video as those frames are received from the video source.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of a system <b>300</b> including a non-transitory computer-readable medium storing executable code for background image alteration in accordance with various examples described herein. System <b>300</b> includes a processor <b>302</b>, a storage device <b>304</b>, and a display <b>306</b>. Storage device <b>304</b> includes executable code <b>308</b>, <b>310</b>, <b>312</b>, and <b>314</b>. Processor <b>302</b> may be processor <b>102</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in one example. Storage device <b>304</b> may include memory <b>104</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in one example. Display <b>306</b> may be display <b>106</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in one example.</p><p id="p-0045" num="0044">System <b>300</b> provides background image alteration as described herein. A video source, such as video source <b>108</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, provides frames of a video to system <b>300</b>. Processor <b>302</b> executes executable code <b>308</b>, <b>310</b>, <b>312</b>, and <b>314</b> to provide background image alteration to the video. Processor <b>302</b> executes executable code <b>308</b> to segment a first frame to determine a background portion of the first frame, and to segment a second frame to determine a background portion of the second frame. The pixels that form the background portion of the first frame and the background portion of the second frame may be stored in a memory or storage device, such as memory <b>104</b> or storage device <b>304</b>. Pixels of more than two frames may be stored in some examples. Also, more than two frames may be segmented using executable code <b>308</b> in some examples.</p><p id="p-0046" num="0045">Processor <b>302</b> executes executable code <b>310</b> to compare the background portion of the first frame to the background portion of the second frame, to determine a difference between the first frame and the second frame. The background portions of the frames are compared to determine if motion has occurred in the background of the video. A difference in the frames indicates that motion has occurred. In some examples, more than two frames may be compared, to determine if the motion is continuous across a larger number of frames. Motion that is not continuous across a larger number of frames may be ignored in some examples. In some examples, the difference in the pixels may have to reach a predetermined threshold (such as a user-defined threshold) for motion to be detected.</p><p id="p-0047" num="0046">Processor <b>302</b> executes executable code <b>312</b> to alter the background portion of the second frame responsive to a difference between the background portion of the first frame and the background portion of the second frame. If no difference exists between the background portion of the first frame and the background portion of the second frame, no mitigation action or alteration is performed on the second frame. However, if a difference is detected, and that difference is above a predetermined threshold, the background portion of the second frame is altered. The background portion may be altered by blurring all or part of the background portion in one example. The background portion may be altered by replacing all or part of the background portion with an image, a video, or a solid color.</p><p id="p-0048" num="0047">Processor <b>302</b> executes executable code <b>314</b> to display the altered second frame on a display. The altered second frame may be displayed on display <b>306</b> in one example. Processor <b>302</b> may repeat the execution of executable code <b>308</b>, <b>310</b>, <b>312</b>, and <b>314</b> for each subsequent frame received from the video source.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram of an electronic device <b>400</b> for background image alteration in accordance with various examples described herein. Electronic device <b>400</b> includes a processor <b>402</b>, a storage device <b>404</b>, and a display <b>406</b>. In some examples, storage device <b>404</b> and/or display <b>406</b> may be separate from electronic device <b>400</b>. Storage device <b>404</b> includes executable code <b>408</b>, <b>410</b>, <b>412</b>, <b>414</b>, and <b>416</b>. Processor <b>402</b> may be processor <b>102</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in one example. Storage device <b>404</b> may include memory <b>104</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in one example. Display <b>406</b> may be display <b>106</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in one example.</p><p id="p-0050" num="0049">Electronic device <b>400</b> provides background image alteration as described herein. Processor <b>402</b> executes executable code <b>408</b>, <b>410</b>, <b>412</b>, <b>414</b>, and <b>416</b> to provide background image alteration to the video. Processor <b>402</b> executes executable code <b>408</b> to receive a video including a first frame and a second frame. The first frame and second frame may be stored in a suitable storage device by processor <b>402</b> for processing in accordance with various examples herein. In one example, the video frames may be stored in storage device <b>404</b>.</p><p id="p-0051" num="0050">Processor <b>402</b> executes executable code <b>410</b> to segment the first frame to distinguish a person in the first frame from a background of the first frame, and to segment the second frame to distinguish a person in the second frame from a background of the second frame. The pixels that form the background portion of the first frame and the background portion of the second frame may be stored in a memory or storage device, such as memory <b>104</b> or storage device <b>404</b>. Pixels of more than two frames may be stored in some examples. Also, more than two frames may be segmented using executable code <b>410</b> in some examples.</p><p id="p-0052" num="0051">Processor <b>402</b> executes executable code <b>412</b> to compare the background of the second frame to the background of the first frame. The background portions of the frames are compared to determine if motion has occurred in the background of the video. A difference in the frames indicates that motion has occurred. In some examples, more than two frames may be compared, to determine if the motion is continuous across a larger number of frames. Motion that is not continuous across a larger number of frames may be ignored in some examples. In some examples, the difference in the pixels may have to reach a predetermined threshold (such as a user-defined threshold) for motion to be detected.</p><p id="p-0053" num="0052">Processor <b>402</b> executes executable code <b>414</b> to display the second frame responsive to the background of the second frame matching the background of the first frame. If the background of the second frame matches the background of the first frame (e.g., the difference between the frames is below an acceptable threshold), no motion is detected in the background between the two frames. If no motion is detected, the second frame may be displayed without alteration.</p><p id="p-0054" num="0053">Processor <b>402</b> executes executable code <b>416</b> to blur a portion of the second frame and display the second frame with the blurred portion responsive to the background of the second frame being different than the background of the first frame. The difference between the first frame and the second frame may have to reach a predetermined threshold before the second frame is blurred in some examples. Stated another way, in some examples a small difference between the backgrounds of the first frame and the second frame is ignored and the frames are displayed without background image alteration. In other examples, the second frame may be altered by a mitigation action other than blurring. For example, the second frame may be altered by replacing all or part of the background with an image, a video, or a solid color.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram of a system <b>500</b> including a non-transitory computer-readable medium storing executable code for background image alteration in accordance with various examples described herein. System <b>500</b> includes a processor <b>502</b>, a storage device <b>504</b>, and a display <b>506</b>. Storage device <b>504</b> includes executable code <b>508</b>, <b>510</b>, <b>512</b>, and <b>514</b>. Processor <b>502</b> may be processor <b>102</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in one example. Storage device <b>504</b> may include memory <b>104</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in one example. Display <b>506</b> may be display <b>106</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in one example.</p><p id="p-0056" num="0055">System <b>500</b> provides background image alteration as described herein. Processor <b>502</b> executes executable code <b>508</b>, <b>510</b>, <b>512</b>, and <b>514</b> to provide background image alteration to the video. Processor <b>502</b> executes executable code <b>508</b> to store background pixels of a first image frame. The background pixels of the first image frame may be stored in storage device <b>504</b> in one example. The first image frame may be provided to processor <b>502</b> via a video source, such as video source <b>108</b>. Video source <b>108</b> may be a camera in one example.</p><p id="p-0057" num="0056">Processor <b>502</b> executes executable code <b>510</b> to compare the background pixels of the first image frame to background pixels of a second image frame. Segmentation has been performed on the image frames to separate the background pixels from a person or persons in the image frames. The comparison is performed on the background pixels to detect if motion has occurred in the background between the first image frame and the second image frame. A difference in the background pixels indicate that motion has occurred. In some examples, if the difference in the background pixels is below a predetermined threshold, the difference is ignored and motion is not detected.</p><p id="p-0058" num="0057">Processor <b>502</b> executes executable code <b>512</b> to alter a portion of the background pixels of the second image frame responsive to determining a difference between the background pixels of the first image frame and the background pixels of the second image frame. The difference between the background pixels of the first image frame and the background pixels of the second image frame indicates that motion has occurred in the video. If motion has occurred, the second frame is altered to obscure the motion. The second frame may be altered by blurring all or a portion of the background of the second frame. In another example, the second frame may be altered by replacing all or a portion of the background with an image, a video, or a solid color.</p><p id="p-0059" num="0058">Processor <b>502</b> executes executable code <b>514</b> to display the second image frame with the altered background pixels. In one example, the second image frame may be displayed on display <b>506</b>. In another example, the second image frame may be transmitted over a network and displayed on another display or on multiple displays.</p><p id="p-0060" num="0059">As described herein, machine learning techniques and image transformations are used to automatically perform a mitigation action on a background of a video responsive to detecting certain conditions. Users may therefore use the background of the environment the users are in, and the examples herein intelligently blur or otherwise obscure the background responsive to the detection of certain conditions to avoid distractions for a viewer.</p><p id="p-0061" num="0060">The above discussion is meant to be illustrative of the principles and various examples of the present disclosure. Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><p id="p-0062" num="0061">In the figures, certain features and components disclosed herein may be shown in exaggerated scale or in somewhat schematic form, and some details of certain elements may not be shown in the interest of clarity and conciseness. In some of the figures, in order to improve clarity and conciseness, a component or an aspect of a component may be omitted.</p><p id="p-0063" num="0062">In the above description and in the claims, the term &#x201c;comprising&#x201d; is used in an open-ended fashion, and thus should be interpreted to mean &#x201c;including, but not limited to . . . .&#x201d; Additionally, the word &#x201c;or&#x201d; is used in an inclusive manner. For example, &#x201c;A or B&#x201d; means any of the following: &#x201c;A&#x201d; alone, &#x201c;B&#x201d; alone, or both &#x201c;A&#x201d; and &#x201c;B&#x201d;.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A non-transitory computer-readable medium storing executable code, which, when executed by a processor of an electronic device, causes the processor to:<claim-text>segment a first frame to determine a background portion of the first frame, and segment a second frame to determine a background portion of the second frame;</claim-text><claim-text>compare the background portion of the first frame to the background portion of the second frame to determine a difference between the first frame and the second frame;</claim-text><claim-text>responsive to the difference, alter the background portion of the second frame; and</claim-text><claim-text>display the altered second frame on a display.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the executable code, when executed by the processor, causes the processor to alter the background portion of the second frame by applying blur to the background portion of the second frame.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the executable code, when executed by the processor, causes the processor to alter the background portion of the second frame by applying an image to the background portion of the second frame.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the executable code, when executed by the processor, causes the processor to determine a difference between the first frame and the second frame by detecting a difference between pixels of the first frame and pixels of the second frame.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the executable code, when executed by the processor, causes the processor to alter the background portion of the second frame responsive to the difference between pixels of the first frame and pixels of the second frame exceeding a predetermined threshold.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. An electronic device, comprising:<claim-text>a processor to:<claim-text>receive a video including a first frame and a second frame;</claim-text><claim-text>segment the first frame to distinguish a person in the first frame from a background of the first frame, and segment the second frame to distinguish a person in the second frame from a background of the second frame;</claim-text><claim-text>compare the background of the second frame to the background of the first frame;</claim-text><claim-text>responsive to the background of the second frame matching the background of the first frame, display the second frame; and</claim-text><claim-text>responsive to the background of the second frame being different than the background of the first frame, blur a portion of the second frame and display the second frame with the blurred portion.</claim-text></claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The electronic device of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein blurring a portion of the second frame includes blurring all pixels of the background of the second frame.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The electronic device of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the background of the second frame being different than the background of the first frame comprises a change in a predetermined number of pixels between the background of the second frame and the background of the first frame.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The electronic device of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the processor is to:<claim-text>receive a third frame;</claim-text><claim-text>segment the third frame to distinguish a person in the third frame from a background of the third frame;</claim-text><claim-text>compare the background of the third frame to the background of the second frame;</claim-text><claim-text>responsive to the background of the third frame matching the background of the second frame, display the third frame; and</claim-text><claim-text>responsive to the background of the third frame being different than the background of the second frame, blur a portion of the third frame and display the third frame with the blurred portion.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The electronic device of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the first frame and the second frame are non-sequential frames of the video.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A non-transitory computer-readable medium storing executable code, which, when executed by a processor of an electronic device, causes the processor to:<claim-text>store background pixels of a first image frame;</claim-text><claim-text>compare the background pixels of the first image frame to background pixels of a second image frame;</claim-text><claim-text>responsive to determining a difference between the background pixels of the first image frame and the background pixels of the second image frame, alter a portion of the background pixels of the second image frame; and</claim-text><claim-text>display the second image frame with the altered background pixels.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the executable code, when executed by the processor, causes the processor to alter a portion of the background pixels of the second image frame by blurring the portion of the background pixels.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the executable code, when executed by the processor, causes the processor to alter a portion of the background pixels of the second image frame by replacing the portion of the background pixels with a static image.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein determining the difference between the background pixels of the first image frame and the background pixels of the second image frame includes determining a difference between a predetermined number of background pixels.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the executable code, when executed by the processor, causes the processor to alter a portion of the background pixels of the second image frame by replacing the portion of the background pixels with a video.</claim-text></claim></claims></us-patent-application>