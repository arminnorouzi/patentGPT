<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005160A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005160</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17725292</doc-number><date>20220420</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202110748585.5</doc-number><date>20210702</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>194</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>194</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">MULTI-TASK DEEP LEARNING-BASED REAL-TIME MATTING METHOD FOR NON-GREEN-SCREEN PORTRAITS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>COMMUNICATION UNIVERSITY OF ZHEJIANG</orgname><address><city>HANGZHOU CITY</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>YU</last-name><first-name>Dingguo</first-name><address><city>Hangzhou City</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>LIN</last-name><first-name>Qiang</first-name><address><city>Hangzhou</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>MA</last-name><first-name>Xiaoyu</first-name><address><city>Hangzhou City</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A multi-task deep learning-based real-time matting method for non-green-screen portraits is provided. The method includes: performing binary classification adjustment on an original dataset, inputting an image or video containing portrait information, and performing preprocessing; constructing a deep learning network for person detection, extracting image features by using a deep residual neural network, and obtaining a region of interest (ROI) of portrait foreground and a portrait trimap in the ROI through logistic regression; and constructing a portrait alpha mask matting deep learning network. An encoder sharing mechanism effectively accelerates a computing process of the network. An alpha mask prediction result of the portrait foreground is output in an end-to-end manner to implement portrait matting. In this method, green screens are not required during portrait matting. In addition, during the matting, only original images or videos need to be provided, without a need to provide manually annotated portrait trimaps.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="76.54mm" wi="158.75mm" file="US20230005160A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="222.84mm" wi="118.19mm" orientation="landscape" file="US20230005160A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="198.46mm" wi="88.48mm" orientation="landscape" file="US20230005160A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="207.69mm" wi="117.77mm" file="US20230005160A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="149.78mm" wi="109.05mm" file="US20230005160A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="206.50mm" wi="92.20mm" file="US20230005160A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="124.29mm" wi="105.66mm" file="US20230005160A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This patent application claims the benefit of and priority to Chinese Patent Application No. 202110748585.5, filed on Jul. 2, 2021, the disclosure of which is incorporated herein by reference in its entirety.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to the technical fields of deep learning, object detection, automatic trimap generation, and alpha mask matting of portrait foreground, and specifically, to a multi-task deep learning-based real-time matting method for non-green-screen portraits.</p><heading id="h-0003" level="1">BACKGROUND ART</heading><p id="p-0004" num="0003">In recent years, due to the rapid development of the Internet information age, human daily life is ubiquitously flooded with a large amount of digital content. Among the large amount of digital content, digital image information, including images and videos, has gradually become an important carrier of information dissemination by virtue of its intuitiveness, understandability, and rich and diverse content forms. The progress of the times has spawned many Internet content production organizations and even individual creators. However, editing and processing of the digital image information are complex and difficult. There are specific requirements for related industries, and practitioners need to spend a lot of effort and time on content creation. Therefore, people's demand for efficient and easy-to-access content production methods becomes increasingly urgent. A digital image matting technology is one of key researches on digital image information editing and processing technologies.</p><p id="p-0005" num="0004">The digital image matting technology mainly aims to separate foreground and background of an image or video, to implement high-accuracy foreground extraction and virtual background replacement. As a main application field of digital image matting, portrait matting came into being with production needs of the film industry as early as the middle of the twentieth century. A portrait matting technology is used to implement early film special effects by extracting a foreground actor and combining the foreground actor with virtual scene background. After decades of industrial technology development, film and television special effects integrated with digital image matting can reduce content production costs and ensure the safety of actors, and also bring exciting viewing experience to audiences. The portrait matting technology has become an irreplaceable part of film and television program production.</p><p id="p-0006" num="0005">In early researches, the digital portrait matting technology requires users to provide prior background knowledge. In traditional film and television production, a solid green or blue screen with a large difference in color from person skin and clothing is usually used as background at a shooting site. Pixels of a subject and the background are compared to implement portrait matting. However, professional green screen background requires a high level of erection, and lighting conditions of the site are strictly limited. It is difficult for general users to use a green screen technology at low costs. As the digital age rapidly develops, public's demand for the digital portrait matting technology has been more widely extended to scenes such as image editing and network meetings to meet their needs for entertainment, privacy protection, and the like. After decades of development, researches of the digital portrait matting technology have also made remarkable achievements. However, existing algorithms mainly have three shortcomings. First, in some researches, manually annotated portrait trimaps need to be provided, and construction of the trimaps requires a lot of labor and time. Secondly, most of research algorithms take a long time, and a small quantity of frames per second cannot implement real-time portrait matting. Finally, for an existing portrait matting algorithm featuring fast computing, a scene image that contains a subject and a scene image that does not contain the subject under the same background usually need to be provided. This limits usage scenes of the algorithm.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">In view of the shortcomings in the prior art and the technical problems of digital image matting, the present disclosure provides a multi-task deep learning-based real-time matting method for non-green-screen portraits.</p><p id="p-0008" num="0007">The present disclosure provides a multi-task deep learning-based real-time matting method for non-green-screen portraits. Focusing on key technologies such as person detection, trimap generation, and portrait alpha mask matting during portrait matting in complex natural environments, the present disclosure implements requirement-free, real-time, and automatic portrait matting when professional green screen devices are lacked. The present disclosure can be applied to application programs such as network meetings and photography editing to provide convenient digital portrait matting services for general users.</p><p id="p-0009" num="0008">The objectives of the present disclosure are achieved by the following technical solutions:</p><p id="p-0010" num="0009">A multi-task deep learning-based real-time matting method for non-green-screen portraits includes the following steps:</p><p id="p-0011" num="0010">step 1: performing binary classification adjustment on an original multi-class multi-object detection dataset, inputting an image or video file in an adjusted dataset (that is, inputting an image or video containing portrait information), and performing corresponding data preprocessing on the image or video to obtain preprocessed data of the original input file;</p><p id="p-0012" num="0011">step 2: using an encoder-logistic regression structure to construct a deep learning network for person detection, inputting the preprocessed data obtained in step 1, constructing a loss function, training and optimizing the deep learning network for person detection to obtain a person detection model;</p><p id="p-0013" num="0012">step 3: extracting feature maps from an encoder of the person detection model in step 2, and performing feature stitching and fusing multi-scale image features to form an encoder of a portrait alpha mask matting network, to implement an encoder shared by the person detection and portrait alpha mask matting networks;</p><p id="p-0014" num="0013">step 4: constructing a decoder of the portrait alpha mask matting network, forming an end-to-end encoder-decoder portrait alpha mask matting network structure together with the shared encoder in step 3, inputting an image containing person information and a trimap, constructing a loss function, and training and optimizing the portrait alpha mask matting network;</p><p id="p-0015" num="0014">step 5: inputting the preprocessed data obtained in step 1 to a trained network in step 4, and outputting a region of interest (ROI) of portrait foreground and a portrait trimap in the ROI through logistic regression of the person detection model in step 2; and</p><p id="p-0016" num="0015">step 6: inputting the ROI of the portrait foreground and the portrait trimap in step 5 into the portrait alpha mask matting network constructed in step 4 to obtain a portrait alpha mask prediction result.</p><p id="p-0017" num="0016">In step 1, the binary classification adjustment is performed to modify the original 80-class common objects in context (COCO) dataset to two classes: person and others, and the dataset is supplemented according to this criterion. A task of detecting other classes of objects is abandoned to improve accuracy of subsequent person detection by the network model.</p><p id="p-0018" num="0017">In step 1, the data preprocessing may include video frame processing and input image resizing.</p><p id="p-0019" num="0018">The video frame processing may include:</p><p id="p-0020" num="0019">Convert the video into video frames by using FFmpeg such that a processed video file can be processed by using a same method as that used to process an image file in subsequent work. Specifically, convert the video into video frames by using FFmpeg, use an original video number as a folder name in a project directory, and store the frames as image files in the folder.</p><p id="p-0021" num="0020">The input image resizing may include:</p><p id="p-0022" num="0021">Resize the input images by unifying sizes of different input images through cropping and padding and keep sizes of network feature maps the same as those of the original images. Specifically, unify sizes of different input images. Calculate a zoom factor with a longest side of an original image as a reference side, compress the longest side in equal proportions to an input criterion specified by the subsequent network, and fill vacant content on a short side of the image with gray background.</p><p id="p-0023" num="0022">In step 2, the preprocessed data obtained in step 1 is input, an error of the ROI, a confidence level error of the ROI, and a person binary classification cross-entropy error are used to construct the loss function, and the person detection network (namely, the deep learning network for person detection) is trained and optimized.</p><p id="p-0024" num="0023">The deep learning network for person detection is implemented through model prediction of a deep residual neural network.</p><p id="p-0025" num="0024">The deep residual neural network includes an encoder and logistic regression.</p><p id="p-0026" num="0025">The encoder is a fully convolutional residual neural network. In the network, skip connections are used to construct residual blocks res_block of different depths and a feature sequence X is obtained by extracting features of the image containing the portrait information. For a processed frame {V<sub>t</sub>}<sub>t=1</sub><sup>T </sup>in step 1, a feature sequence {x<sub>t</sub>}<sub>t=1</sub><sup>T&#x2032;</sup> of a length T is extracted. V<sub>t </sub>represents a t<sup>th </sup>frame, and x<sub>t </sub>represents a feature sequence of the t<sup>th </sup>frame.</p><p id="p-0027" num="0026">The feature extraction may include:</p><p id="p-0028" num="0027">Use a deep learning technology to perform a cognitive process of the original image or the frame obtained after the video is preprocessed, and convert the image into a feature sequence that can be recognized by a computer.</p><p id="p-0029" num="0028">The logistic regression is an output structure for multi-scale detection of a central position (x<sub>i</sub>, y<sub>i</sub>) of a ROI, a length and width (w<sub>i</sub>, h<sub>i</sub>) of the ROI, a confidence level C<sub>i </sub>of the ROI, a class p<sub>i</sub>(c), c&#x2208;classes of an object in the ROI, and the person foreground f(pixel<sub>i</sub>) and background b(pixel<sub>i</sub>) binary classification results. classes represents all classes in the training sample, and pixel<sub>i </sub>represents an i<sup>th </sup>pixel in the ROI.</p><p id="p-0030" num="0029">In step 3, large, medium, and small feature maps may be extracted from the encoder of the person detection model in step 2, and the feature stitching is performed to fuse the multi-scale image features to form the encoder of the portrait alpha mask matting network, to implement the encoder shared by the person detection and portrait alpha mask matting networks.</p><p id="p-0031" num="0030">In step 3, forward access to the deep residual neural network constructed in step 2 may be performed to obtain outputs of the residual blocks res_block with downsampling multiples of 8 times, 16 times, and 32 times. The outputs separately pass through a 3&#xd7;3 convolution kernel and a 1&#xd7;1 convolution kernel. The outputs are stitched to form large, medium, and small fused image feature structures as the encoder of the portrait alpha mask matting network, to implement the encoder shared by the person detection and portrait alpha mask matting networks.</p><p id="p-0032" num="0031">The encoder shared by the person detection and portrait alpha mask matting networks in step 3 may specifically include:</p><p id="p-0033" num="0032">(3.1) Perform forward access to the fully convolutional deep residual neural network to obtain the outputs of the residual blocks res_block with the downsampling multiples of 8 times, 16 times, and 32 times. Convolution kernels with a stride of 2 are used to implement the downsampling. core<sub>8</sub>, core<sub>16</sub>, and core<sub>32 </sub>are set as the convolution kernels during the downsampling, and a size of the convolution kernel is x,y. A size of an input input is m,n, and a size of an output output is m/2,n/2. Convolution corresponding to the output is expressed by formula (1). fun(&#x22c5;) represents an activation function, and &#x3b2; represents a bias.</p><p id="p-0034" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>output<sub>m/2,n/2</sub>=fun(&#x3a3;&#x3a3;input<sub>mn</sub>*core<sub>xy</sub>+&#x3b2;)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0035" num="0033">(3.2) Fuse and stitch the output to form large, medium, and small fused image feature structures as the encoder of the portrait alpha mask matting network, to implement the encoder shared by the person detection and portrait alpha mask matting networks.</p><p id="p-0036" num="0034">In step 4, a main structure of the decoder includes upsampling, convolution, an exponential linear unit (ELU) activation function, and a fully connected layer for outputting. The image containing the person information and the trimap are input, the network loss function with an alpha mask prediction error and an image compositing error as a core is constructed, and the portrait alpha mask matting network is trained and optimized.</p><p id="p-0037" num="0035">The upsampling is used to restore an image feature size after downsampling in the encoder. A scaled ELU (SELU) activation function is used. Hyperparameters &#x3bb; and &#x3b1; in the SELU activation function are fixed constants. The activation function is expressed by formula (2):</p><p id="p-0038" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>SeLU</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>x</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mi>&#x3bb;</mi>      <mo>&#x2062;</mo>      <mrow>       <mo>{</mo>       <mtable>        <mtr>         <mtd>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mrow>            <mi>x</mi>            <mo>&#x3e;</mo>            <mn>0</mn>            <mtext>                 </mtext>           </mrow>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mrow>            <mi>&#x3b1;</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mrow>             <msup>              <mi>e</mi>              <mi>x</mi>             </msup>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>)</mo>           </mrow>           <mo>,</mo>           <mrow>            <mi>x</mi>            <mo>&#x2264;</mo>            <mn>0</mn>           </mrow>          </mrow>         </mtd>        </mtr>       </mtable>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0039" num="0036">In step 4, that the loss function of the portrait alpha mask matting network is constructed may specifically include:</p><p id="p-0040" num="0037">(4.1) Compute an alpha mask prediction error, as expressed by formula (3):</p><p id="p-0041" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>&#x3b1;lp</sub>=&#x221a;{square root over ((&#x3b1;<sub>pre</sub>&#x2212;&#x3b1;<sub>gro</sub>)<sup>2</sup>)}+&#x3b5;,&#x3b1;<sub>pre</sub>,&#x3b1;<sub>gro</sub>&#x2208;[0,1]&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0042" num="0038">where &#x3b1;<sub>pre </sub>and &#x3b1;<sub>gro </sub>respectively represent predicted and ground-truth alpha mask values, and E represents a very small constant.</p><p id="p-0043" num="0039">(4.2) Compute an image compositing error, as expressed by formula (4):</p><p id="p-0044" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>com</sub>=&#x221a;{square root over ((<i>c</i><sub>pre</sub><i>&#x2212;c</i><sub>gro</sub>)<sup>2</sup>)}+&#x3b5;&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0045" num="0040">where c<sub>pre </sub>and c<sub>gro </sub>respectively represent predicted and ground-truth alpha composite images, and E represents a very small constant.</p><p id="p-0046" num="0041">(4.3) Construct an overall loss function based on the alpha mask prediction error and the image compositing error, as expressed by formula (5):</p><p id="p-0047" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>overall</sub>=&#x3c9;<sub>1</sub>Loss<sub>&#x3b1;lp</sub>&#x3c9;<sub>2</sub>Loss<sub>com</sub>,&#x3c9;<sub>1</sub>+&#x3c9;<sub>2</sub>=1&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0048" num="0042">In step 5, the preprocessed image data obtained in step 1 is input to the trained person detection network model, and the ROI of the portrait foreground and the portrait trimap in the ROI are predicted through the logistic regression.</p><p id="p-0049" num="0043">The ROI of the portrait foreground is obtained by performing edge dilation on a general ROI for object detection. This prevents a fine edge of a person being placed outside the ROI during object detection. The portrait trimap in the ROI is obtained through erosion and dilation of the person binary classification cross-entropy error in the loss function in step 2.</p><p id="p-0050" num="0044">In step 5, the outputting a ROI of portrait foreground and a portrait trimap in the ROI may specifically include:</p><p id="p-0051" num="0045">(5.1) Use a relative intersection over union (RIOU) obtained by improving an original criterion for determining the ROI of the portrait foreground. This enables the ROI to have a stronger enclosing capability and prevents the fine edge of the person being placed outside the ROI during the object detection. The RIOU is expressed by formula (7):</p><p id="p-0052" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>RIOU</mi>     <mo>=</mo>     <mrow>      <mfrac>       <mrow>        <mo>[</mo>        <mrow>         <msub>          <mi>ROI</mi>          <mi>p</mi>         </msub>         <mo>&#x22c2;</mo>         <msub>          <mi>ROI</mi>          <mi>g</mi>         </msub>        </mrow>        <mo>]</mo>       </mrow>       <mrow>        <mo>[</mo>        <mrow>         <mrow>          <msub>           <mi>ROI</mi>           <mrow>            <mi>p</mi>            <mo>&#x22c3;</mo>           </mrow>          </msub>          <mo>&#x2062;</mo>          <msub>           <mi>ROI</mi>           <mi>g</mi>          </msub>         </mrow>         <mo>+</mo>         <mrow>          <mi>&#x3b1;</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <msub>             <mi>ROI</mi>             <mi>g</mi>            </msub>            <mo>-</mo>            <msub>             <mi>ROI</mi>             <mi>p</mi>            </msub>           </mrow>           <mo>&#x22c2;</mo>           <msub>            <mi>ROI</mi>            <mi>g</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>]</mo>       </mrow>      </mfrac>      <mo>-</mo>      <mfrac>       <mrow>        <mo>[</mo>        <mrow>         <mrow>          <msub>           <mi>ROI</mi>           <mi>edge</mi>          </msub>          <mo>-</mo>          <msub>           <mi>ROI</mi>           <mi>p</mi>          </msub>         </mrow>         <mo>&#x22c2;</mo>         <msub>          <mi>ROI</mi>          <mi>g</mi>         </msub>        </mrow>        <mo>]</mo>       </mrow>       <mrow>        <mo>[</mo>        <msub>         <mi>ROI</mi>         <mi>edge</mi>        </msub>        <mo>]</mo>       </mrow>      </mfrac>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0053" num="0046">where ROI<sub>edge </sub>represents a minimum bounding rectangle ROI that can enclose ROI<sub>p </sub>and ROI<sub>g</sub>, [&#x22c5;] represents an area of the ROI, ROI<sub>p </sub>represents a predicted value of the ROI of the portrait foreground, and ROI<sub>g </sub>represents a ground-truth value of the ROI of the portrait foreground.</p><p id="p-0054" num="0047">(5.2) For person foreground and background binary classification results, use an erosion algorithm to remove noise, and then use a dilation algorithm to generate a clear edge contour. The finally obtained portrait trimap is expressed by formula (8):</p><p id="p-0055" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>trimap</mi>      <mi>i</mi>     </msub>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mtable>       <mtr>        <mtd>         <mrow>          <mrow>           <mn>1</mn>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>          </mrow>          <mo>&#x2208;</mo>          <mrow>           <mi>f</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>           <mo>)</mo>          </mrow>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mn>0.5</mn>          <mtext>     </mtext>          <mi>otherwise</mi>          <mtext>               </mtext>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mrow>           <mn>0</mn>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>          </mrow>          <mo>&#x2208;</mo>          <mrow>           <mi>b</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>           <mo>)</mo>          </mrow>         </mrow>        </mtd>       </mtr>      </mtable>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0056" num="0048">where f(pixel<sub>i</sub>) indicates that the i<sup>th </sup>pixel pixel<sub>i </sub>belongs to the foreground, b(pixel<sub>i</sub>) indicates that the i<sup>th </sup>pixel pixel<sub>i </sub>belongs to the background, trimap<sub>i </sub>represents an alpha mask channel value of the i<sup>th </sup>pixel pixel<sub>i</sub>, and otherwise indicates that it cannot be determined whether the pixel belongs to the foreground or background.</p><p id="p-0057" num="0049">In step 6, feature mapping is performed on the ROI of the original portrait foreground in step 5, and the portrait trimap in the ROI is input into the portrait alpha mask matting network model to reduce a convolution computing scale and accelerate network computing. After an original resolution of the image is restored through the upsampling of the decoder, a portrait alpha mask prediction result is obtained from the output of the fully connected layer. The portrait matting is completed.</p><p id="p-0058" num="0050">In the present disclosure, the binary classification adjustment is performed on the original dataset, the image or video containing the portrait information is input, and preprocessed network input data is obtained through video frame processing and input image resizing; the deep learning network for person detection is constructed, the image features are extracted by using the deep residual neural network, and the ROI of the portrait foreground and the portrait trimap in the ROI are obtained through the logistic regression; and the portrait alpha mask matting deep learning network is constructed. An encoder sharing mechanism effectively accelerates a computing process of the network. The alpha mask prediction result of the portrait foreground is output in an end-to-end manner to implement the portrait matting. In this method, green screens are not required during the portrait matting. In addition, during the matting, only original images or videos need to be provided, without a need to provide manually annotated portrait trimaps. This provides great convenience for users. Finally, the encoder sharing mechanism proposed in the present disclosure accelerates task computing, implement real-time portrait matting while providing high-definition image quality, and meets user requirements in a plurality of scenarios.</p><p id="p-0059" num="0051">Compared with the prior art, the present disclosure has the following advantages:</p><p id="p-0060" num="0052">The present disclosure provides a multi-task deep learning-based real-time matting method for non-green-screen portraits. Focusing on the key technologies such as person detection, trimap generation, and portrait alpha mask matting during portrait matting in complex natural environments, the present disclosure implements requirement-free, real-time, and automatic portrait matting when professional green screen devices are lacked. The method in the present disclosure resolves limitations of traditional digital image matting technologies on devices and sites, and is applied to application programs such as network meetings and photography editing to provide real-time and convenient digital portrait matting services for general users. The innovation of the present disclosure is embodied in the following aspects:</p><p id="p-0061" num="0053">(1) The present disclosure innovatively proposes the modification and supplement to the traditional COCO 80-class multi-object detection dataset, to form the unique person-others two-class dataset in the present disclosure. This significantly reduces difficulty in training sample construction and improves the accuracy of the subsequent person detection by the network model.</p><p id="p-0062" num="0054">(2) The present disclosure innovatively proposes a RIOU for determining the ROI during the object detection. This enables the ROI to have a stronger enclosing capability and prevents a fine edge of a person being placed outside the ROI during the object detection.</p><p id="p-0063" num="0055">(3) The present disclosure innovatively proposes the encoder sharing mechanism for the person detection network and portrait alpha mask matting network. This greatly reduces the time consumed by the algorithm during the image feature recognition, and implements high-definition real-time portrait matting.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0064" num="0056"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a network structure of a multi-task deep learning-based real-time matting method for non-green-screen portraits according to the present disclosure;</p><p id="p-0065" num="0057"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of a binary classification process of an original multi-class dataset according to the present disclosure;</p><p id="p-0066" num="0058"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of person detection in an method according to the present disclosure;</p><p id="p-0067" num="0059"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of portrait alpha mask matting in an method according to the present disclosure;</p><p id="p-0068" num="0060"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of an inference process of an method according to the present disclosure; and</p><p id="p-0069" num="0061"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an overall flowchart of a method according to the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0070" num="0062">The following further describes a multi-task deep learning-based real-time matting method for non-green-screen portraits with reference to the accompanying drawings.</p><p id="p-0071" num="0063">As shown in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>6</b></figref>, a multi-task deep learning-based real-time matting method <b>600</b> for non-green-screen portraits includes the following steps:</p><p id="p-0072" num="0064">Step S<b>601</b>: Improve an original dataset, input an image or video in an improved dataset, and perform corresponding data preprocessing on the image or video to obtain preprocessed data of an original input file.</p><p id="p-0073" num="0065">That the original dataset is improved and the data preprocessing is performed in step 1 may specifically include:</p><p id="p-0074" num="0066">(1.1) Perform binary classification adjustment and supplement on a multi-class multi-object detection dataset. The binary classification adjustment is performed to modify an original 80-class COCO dataset to two classes: person and others, and supplement the dataset according to this criterion.</p><p id="p-0075" num="0067">(1.2) Perform video frame processing by using FFmpeg to convert the video into video frames such that a processed video file can be processed by using a same method as that used to process an image file in subsequent work.</p><p id="p-0076" num="0068">(1.3) Resize the input images by unifying sizes of different input images through cropping and padding and keep sizes of network feature maps the same as those of the original images.</p><p id="p-0077" num="0069">Step S<b>602</b>: Use an encoder-logistic regression structure to construct a deep learning network <b>103</b> for person detection. Input the preprocessed data obtained in step 1, construct a loss function, and train and optimize the deep learning network for person detection to obtain a person detection model.</p><p id="p-0078" num="0070">The deep learning network <b>103</b> for person detection may specifically include:</p><p id="p-0079" num="0071">(2.1) The encoder <b>104</b> is a fully convolutional residual neural network. In the network, skip connections are used to construct residual blocks res_block of different depths, and a feature sequence is obtained by extracting features of the image containing the portrait information.</p><p id="p-0080" num="0072">(2.2) The loss function is constructed by adding a cross-entropy error of the person-others binary classification as an additional load to a general object detection task.</p><p id="p-0081" num="0073">(2.3) The logistic regression <b>105</b> is an output structure for multi-scale detection of a central position (x<sub>i</sub>, y<sub>i</sub>) of a ROI, a length and width (w<sub>i</sub>, h<sub>i</sub>) of the ROI, a confidence level C<sub>i </sub>of the ROI, and a class p<sub>i</sub>(c), c E classes of an object in the ROI. classes represents all classes in the training sample, namely, [class0:person, class1:others], and pixel<sub>i </sub>represents an i<sup>th </sup>pixel in the ROI.</p><p id="p-0082" num="0074">Step S<b>603</b>: Fuse multi-scale image features to form an encoder of a portrait alpha mask matting network, to implement an encoder shared by the person detection and portrait alpha mask matting networks.</p><p id="p-0083" num="0075">The multi-scale encoder shared by the person detection and portrait alpha mask matting networks may specifically include:</p><p id="p-0084" num="0076">(3.1) Perform forward access to the fully convolutional deep residual neural network <b>104</b> to obtain outputs of the residual blocks res_block with downsampling multiples of 8 times, 16 times, and 32 times. Convolution kernels with a stride of 2 are used to implement the downsampling. core<sub>8</sub>, core<sub>16</sub>, and core<sub>32 </sub>are set as the convolution kernels during the downsampling, and a size of the convolution kernel is x,y. A size of an input input is m,n, and a size of an output output is m/2,n/2. Convolution corresponding to the output is expressed by formula (1). fun(&#x22c5;) represents an activation function, and &#x3b2; represents a bias.</p><p id="p-0085" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>output<sub>m/2,n/2</sub>=fun(&#x3a3;&#x3a3;input<sub>mn</sub>*core<sub>xy</sub>+&#x3b2;)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0086" num="0077">(3.2) Fuse and stitch the output to form large, medium, and small fused image feature structures as the encoder of the portrait alpha mask matting network, to implement the encoder shared by the person detection and portrait alpha mask matting networks.</p><p id="p-0087" num="0078">Step S<b>604</b>: Construct a decoder <b>106</b> of the portrait alpha mask matting network, and form an end-to-end encoder-decoder portrait alpha mask matting network structure together with the shared encoder in step 3. Input an image containing person information and a trimap <b>107</b>, construct a loss function, and train and optimize the portrait alpha mask matting network.</p><p id="p-0088" num="0079">A main structure of the decoder <b>106</b> of the portrait alpha mask matting network may include upsampling, convolution, an ELU activation function, and a fully connected layer for outputting.</p><p id="p-0089" num="0080">(4.1) The upsampling is implemented by an upsampling operation to restore an image feature size after downsampling in the encoder.</p><p id="p-0090" num="0081">(4.2) A SELU activation function is used to set outputs of some neurons in the deep learning network to 0 to form a sparse network structure. Hyperparameters &#x3bb; and &#x3b1; in the SELU activation function are fixed constants, and the activation function is expressed by formula (2):</p><p id="p-0091" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>SeLU</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>x</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mi>&#x3bb;</mi>      <mo>&#x2062;</mo>      <mrow>       <mo>{</mo>       <mtable>        <mtr>         <mtd>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mrow>            <mi>x</mi>            <mo>&#x3e;</mo>            <mn>0</mn>            <mtext>                 </mtext>           </mrow>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mrow>            <mi>&#x3b1;</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mrow>             <msup>              <mi>e</mi>              <mi>x</mi>             </msup>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>)</mo>           </mrow>           <mo>,</mo>           <mrow>            <mi>x</mi>            <mo>&#x2264;</mo>            <mn>0</mn>           </mrow>          </mrow>         </mtd>        </mtr>       </mtable>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0092" num="0082">That the loss function of the portrait alpha mask matting network is constructed may specifically include:</p><p id="p-0093" num="0083">(4.3) An alpha mask prediction error is expressed by formula (3):</p><p id="p-0094" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>&#x3b1;lp</sub>=&#x221a;{square root over ((&#x3b1;<sub>pre</sub>&#x2212;&#x3b1;<sub>gro</sub>)<sup>2</sup>)}+&#x3b5;,&#x3b1;<sub>pre</sub>,&#x3b1;<sub>gro</sub>&#x2208;[0,1]&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0095" num="0084">where &#x3b1;<sub>pre </sub>and &#x3b1;<sub>gro </sub>respectively represent predicted and ground-truth alpha mask values, and E represents a very small constant.</p><p id="p-0096" num="0085">(4.4) An image compositing error is expressed by formula (4):</p><p id="p-0097" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>com</sub>=&#x221a;{square root over ((<i>c</i><sub>pre</sub><i>&#x2212;c</i><sub>gro</sub>)<sup>2</sup>)}+&#x3b5;&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0098" num="0086">where c<sub>pre </sub>and c<sub>gro </sub>respectively represent predicted and ground-truth alpha composite images.</p><p id="p-0099" num="0087">(4.5) An overall loss function is constructed based on the alpha mask prediction error and the image compositing error, as expressed by formula (5):</p><p id="p-0100" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>overall</sub>=&#x3c9;<sub>1</sub>Loss<sub>&#x3b1;lp</sub>&#x3c9;<sub>2</sub>Loss<sub>com</sub>,&#x3c9;<sub>1</sub>+&#x3c9;<sub>2</sub>=1&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0101" num="0088">Step S<b>605</b>: Input the preprocessed image data obtained in step 1 to a trained network, and output a ROI <b>108</b> of portrait foreground and a portrait trimap <b>107</b> in the ROI <b>108</b> through the logistic regression of the person detection network in step 2.</p><p id="p-0102" num="0089">That the ROI <b>108</b> of the portrait foreground and the portrait trimap <b>107</b> in the ROI <b>108</b> are output may specifically include:</p><p id="p-0103" num="0090">(5.1) Use a RIOU obtained by improving an original criterion for determining the ROI of the portrait foreground. This enables the ROI to have a stronger enclosing capability and prevents a fine edge of a person being placed outside the ROI during object detection. The RIOU is expressed by formula (7):</p><p id="p-0104" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>RIOU</mi>     <mo>=</mo>     <mrow>      <mfrac>       <mrow>        <mo>[</mo>        <mrow>         <msub>          <mi>ROI</mi>          <mi>p</mi>         </msub>         <mo>&#x22c2;</mo>         <msub>          <mi>ROI</mi>          <mi>g</mi>         </msub>        </mrow>        <mo>]</mo>       </mrow>       <mrow>        <mo>[</mo>        <mrow>         <mrow>          <msub>           <mi>ROI</mi>           <mrow>            <mi>p</mi>            <mo>&#x22c3;</mo>           </mrow>          </msub>          <mo>&#x2062;</mo>          <msub>           <mi>ROI</mi>           <mi>g</mi>          </msub>         </mrow>         <mo>+</mo>         <mrow>          <mi>&#x3b1;</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <msub>             <mi>ROI</mi>             <mi>g</mi>            </msub>            <mo>-</mo>            <msub>             <mi>ROI</mi>             <mi>p</mi>            </msub>           </mrow>           <mo>&#x22c2;</mo>           <msub>            <mi>ROI</mi>            <mi>g</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>]</mo>       </mrow>      </mfrac>      <mo>-</mo>      <mfrac>       <mrow>        <mo>[</mo>        <mrow>         <mrow>          <msub>           <mi>ROI</mi>           <mi>edge</mi>          </msub>          <mo>-</mo>          <msub>           <mi>ROI</mi>           <mi>p</mi>          </msub>         </mrow>         <mo>&#x22c2;</mo>         <msub>          <mi>ROI</mi>          <mi>g</mi>         </msub>        </mrow>        <mo>]</mo>       </mrow>       <mrow>        <mo>[</mo>        <msub>         <mi>ROI</mi>         <mi>edge</mi>        </msub>        <mo>]</mo>       </mrow>      </mfrac>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0105" num="0091">where ROI<sub>edge </sub>represents a minimum bounding rectangle ROI that can enclose ROI<sub>p </sub>and ROI<sub>g</sub>, and [&#x22c5;] represents an area of the ROI.</p><p id="p-0106" num="0092">(5.2) For person foreground and background binary classification results, use an erosion algorithm to remove noise, and then use a dilation algorithm to generate a clear edge contour. The finally obtained portrait trimap <b>107</b> is expressed by formula (8):</p><p id="p-0107" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>trimap</mi>      <mi>i</mi>     </msub>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mtable>       <mtr>        <mtd>         <mrow>          <mrow>           <mn>1</mn>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>          </mrow>          <mo>&#x2208;</mo>          <mrow>           <mi>f</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>           <mo>)</mo>          </mrow>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mn>0.5</mn>          <mtext>   </mtext>          <mi>otherwise</mi>          <mtext>            </mtext>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mrow>           <mn>0</mn>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>          </mrow>          <mo>&#x2208;</mo>          <mrow>           <mi>b</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>           <mo>)</mo>          </mrow>         </mrow>        </mtd>       </mtr>      </mtable>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0108" num="0093">where f(pixel<sub>i</sub>) indicates that an i<sup>th </sup>pixel pixel<sub>i </sub>belongs to the foreground, b(pixel<sub>i</sub>) indicates that the i<sup>th </sup>pixel pixel<sub>i </sub>belongs to the background, and trimap<sub>i </sub>represents an alpha mask channel value of the i<sup>th </sup>pixel pixel<sub>i</sub>.</p><p id="p-0109" num="0094">Step S<b>606</b>: Input the ROI <b>108</b> of the portrait foreground and the portrait trimap <b>107</b> in step 5 into the portrait alpha mask matting network constructed in step 4 to obtain a portrait alpha mask prediction result.</p><p id="p-0110" num="0095">More specifically, the multi-task deep learning-based real-time matting method for non-green-screen portraits divides portrait matting into two parts of algorithm tasks: the person detection task <b>101</b> in step 1 and the portrait foreground alpha mask matting task <b>102</b> in step 2, specifically including the following steps:</p><p id="p-0111" num="0096">In step 1, the data preprocessing includes video frame processing and input image resizing.</p><p id="p-0112" num="0097">The video frame processing may include:</p><p id="p-0113" num="0098">Convert the video into frames by using FFmpeg, use an original video number as a folder name in a project directory, and store the frames as image files in the folder. In this way, a processed video file can be processed by using a same method as that used to process an image file in subsequent work.</p><p id="p-0114" num="0099">The input image resizing may include:</p><p id="p-0115" num="0100">Unify sizes of different input images. Calculate a zoom factor with a longest side of an original image as a reference side, compress the longest side in equal proportions to an input criterion specified by the subsequent network, and fill vacant content on a short side with gray background through padding. Keep a size of a network feature map the same as that of the original image. This prevents abnormal network output values caused by an invalid size of the input image.</p><p id="p-0116" num="0101">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the binary classification adjustment is performed to modify the original 80-class COCO dataset to two classes: person and others, and the dataset is supplemented according to this criterion. A task of detecting other classes of objects is abandoned to improve accuracy of subsequent person detection by the network model.</p><p id="p-0117" num="0102">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the deep learning network <b>103</b> for person detection in the first part of the entire network is implemented through model prediction of the deep residual neural network. The deep residual neural network <b>103</b> includes an encoder <b>104</b> and logistic regression <b>105</b>.</p><p id="p-0118" num="0103">Step S<b>301</b>: The encoder <b>104</b> is a fully convolutional residual neural network. In the network <b>104</b>, skip connections are used to construct residual blocks res_block of different depths, and a feature sequence x is obtained by extracting features of the image containing the portrait information. For a processed frame {V<sub>t</sub>}<sub>t=1</sub><sup>T</sup>, a feature sequence {x<sub>t</sub>}<sub>t=1</sub><sup>T </sup>of a length T is extracted. V<sub>t </sub>represents a t<sup>th </sup>frame, and x<sub>t </sub>represents a feature sequence of the t<sup>th </sup>frame.</p><p id="p-0119" num="0104">The feature extraction may include:</p><p id="p-0120" num="0105">Use a deep learning technology to perform a cognitive process of the original image or the frame obtained after the video is preprocessed, and convert the image into a feature sequence that can be recognized by a computer.</p><p id="p-0121" num="0106">Step S<b>302</b>: The logistic regression <b>105</b> is an output structure for multi-scale detection of a central position (x<sub>i</sub>, y<sub>i</sub>) of a ROI, a length and width (w<sub>i</sub>, h<sub>i</sub>) of the ROI, a confidence level C<sub>i </sub>of the ROI, a class p<sub>i</sub>(c), c&#x2208;classes of an object in the ROI, and the person foreground f(pixel<sub>i</sub>) and background b(pixel<sub>i</sub>) binary classification results. classes represents all classes in the training sample, namely, [class0:person, class1:others], and pixel<sub>i </sub>represents the i<sup>th </sup>pixel in the ROI.</p><p id="p-0122" num="0107">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the portrait alpha mask matting network in the second part <b>102</b> of the entire network includes the shared encoder <b>106</b> and the portrait alpha mask matting decoder, specifically including the following implementation:</p><p id="p-0123" num="0108">Step S<b>401</b>: Perform forward access to the deep residual neural network to obtain the outputs of the residual blocks res_block with downsampling multiples of 8 times, 16 times, and 32 times. To reduce a negative effect of a gradient caused by pooling, the downsampling adopts convolution kernels with a stride of 2. core<sub>8</sub>, core<sub>16</sub>, and core<sub>32 </sub>are set as the convolution kernels during the downsampling. Quantities of channels channel_n are equal to corresponding inputs input<sub>8</sub>, input<sub>16</sub>, and input<sub>32</sub>, and a size of the convolution kernel is x,y. A size of an input input is m,n, and a size of an output output is m/2,n/2. Convolution corresponding to the output is expressed by formula (1). fun(&#x22c5;) represents an activation function, and &#x3b2; represents a bias.</p><p id="p-0124" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>output<sub>m/2,n/2</sub>=fun(&#x3a3;&#x3a3;input<sub>mn</sub>*core<sub>xy</sub>+&#x3b2;)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0125" num="0109">Further, corresponding outputs pass through a 3&#xd7;3 convolution kernel conv3 to expand a receptive field of the feature map and increase local context information of the image feature. Then, the outputs pass through a 1&#xd7;1 convolution kernel conv1 to reduce a feature channel dimension. The outputs are fused and stitched to form large, medium, and small fused image feature structures as the encoder of the portrait alpha mask matting network, to implement the encoder shared by the person detection and portrait alpha mask matting networks.</p><p id="p-0126" num="0110">Step S<b>402</b>: A main structure of the decoder includes upsampling, convolution, an ELU activation function, and a fully connected layer for outputting. Input the image containing the person information and the trimap, construct a network loss function with an alpha mask prediction error and an image compositing error as a core, and train and optimize the portrait alpha mask matting network.</p><p id="p-0127" num="0111">The upsampling is implemented by an upsampling operation. A specific value in the input image feature is mapped and filled to a corresponding area of the output upsampled image feature, and a blank area after upsampling is filled with the same value to restore the size of the image feature after downsampling in the encoder.</p><p id="p-0128" num="0112">A SELU activation function is used to set outputs of some neurons in the deep learning network to 0 to form a sparse network structure. This effectively reduces overfitting of the matting network, and avoids gradient disappearance of a traditional sigmoid activation function during back propagation. Hyperparameters &#x3bb; and &#x3b1; in the SELU activation function are fixed constants, and the activation function is expressed by formula (2):</p><p id="p-0129" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>SeLU</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>x</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mi>&#x3bb;</mi>      <mo>&#x2062;</mo>      <mrow>       <mo>{</mo>       <mtable>        <mtr>         <mtd>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mrow>            <mi>x</mi>            <mo>&#x3e;</mo>            <mn>0</mn>            <mtext>                 </mtext>           </mrow>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mrow>            <mi>&#x3b1;</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mrow>             <msup>              <mi>e</mi>              <mi>x</mi>             </msup>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>)</mo>           </mrow>           <mo>,</mo>           <mrow>            <mi>x</mi>            <mo>&#x2264;</mo>            <mn>0</mn>           </mrow>          </mrow>         </mtd>        </mtr>       </mtable>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0130" num="0113">The alpha mask prediction error is expressed by formula (3):</p><p id="p-0131" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>&#x3b1;lp</sub>=&#x221a;{square root over ((&#x3b1;<sub>pre</sub>&#x2212;&#x3b1;<sub>gro</sub>)<sup>2</sup>)}+&#x3b5;,&#x3b1;<sub>pre</sub>,&#x3b1;<sub>gro</sub>&#x2208;[0,1]&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0132" num="0114">where &#x3b1;<sub>pre </sub>and &#x3b1;<sub>gro </sub>respectively represent predicted and ground-truth alpha mask values, and &#x3b5; represents a very small constant.</p><p id="p-0133" num="0115">The image compositing error is expressed by formula (4):</p><p id="p-0134" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>com</sub>=&#x221a;{square root over ((<i>c</i><sub>pre</sub><i>&#x2212;c</i><sub>gro</sub>)<sup>2</sup>)}&#x3b5;&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0135" num="0116">where c<sub>pre </sub>and c<sub>gro </sub>respectively represent predicted and ground-truth alpha composite images, and &#x3b5; represents a very small constant.</p><p id="p-0136" num="0117">An overall loss function is constructed based on the alpha mask prediction error and the image compositing error, as expressed by formula (5):</p><p id="p-0137" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>overall</sub>=&#x3c9;<sub>1</sub>Loss<sub>&#x3b1;lp</sub>&#x3c9;<sub>2</sub>Loss<sub>com</sub>,&#x3c9;<sub>1</sub>+&#x3c9;<sub>2</sub>=1&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0138" num="0118">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, after the algorithm provided in the present disclosure is trained, a real-time portrait matting inference process <b>500</b> can be carried out.</p><p id="p-0139" num="0119">Step S<b>501</b>: The improvement and the data preprocessing is performed on dataset to be processed.</p><p id="p-0140" num="0120">Step S<b>502</b>: Input preprocessed image data to the trained person detection network model, and predict a ROI of portrait foreground and a portrait trimap <b>107</b> in the ROI through the logistic regression.</p><p id="p-0141" num="0121">Generally, a ROI is determined based on an IOU during object detection, as expressed by formula (6). ROI<sub>p </sub>and ROI<sub>9 </sub>respectively represent predicted and ground-truth ROIs.</p><p id="p-0142" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>IOU</mi>     <mo>=</mo>     <mfrac>      <mrow>       <mo>[</mo>       <mrow>        <msub>         <mi>ROI</mi>         <mi>p</mi>        </msub>        <mo>&#x22c2;</mo>        <msub>         <mi>ROI</mi>         <mi>g</mi>        </msub>       </mrow>       <mo>]</mo>      </mrow>      <mrow>       <mo>[</mo>       <mrow>        <msub>         <mi>ROI</mi>         <mrow>          <mi>p</mi>          <mo>&#x22c3;</mo>         </mrow>        </msub>        <mo>&#x2062;</mo>        <msub>         <mi>ROI</mi>         <mi>g</mi>        </msub>       </mrow>       <mo>]</mo>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0143" num="0122">The present disclosure proposes the RIOU for determining the ROI of the portrait foreground. This enables the ROI to have a stronger enclosing capability and prevents a fine edge of a person being placed outside the ROI during object detection. The RIOU is expressed by formula (7):</p><p id="p-0144" num="0000"><maths id="MATH-US-00009" num="00009"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>RIOU</mi>     <mo>=</mo>     <mrow>      <mfrac>       <mrow>        <mo>[</mo>        <mrow>         <msub>          <mi>ROI</mi>          <mi>p</mi>         </msub>         <mo>&#x22c2;</mo>         <msub>          <mi>ROI</mi>          <mi>g</mi>         </msub>        </mrow>        <mo>]</mo>       </mrow>       <mrow>        <mo>[</mo>        <mrow>         <mrow>          <msub>           <mi>ROI</mi>           <mrow>            <mi>p</mi>            <mo>&#x22c3;</mo>           </mrow>          </msub>          <mo>&#x2062;</mo>          <msub>           <mi>ROI</mi>           <mi>g</mi>          </msub>         </mrow>         <mo>+</mo>         <mrow>          <mi>&#x3b1;</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <msub>             <mi>ROI</mi>             <mi>g</mi>            </msub>            <mo>-</mo>            <msub>             <mi>ROI</mi>             <mi>p</mi>            </msub>           </mrow>           <mo>&#x22c2;</mo>           <msub>            <mi>ROI</mi>            <mi>g</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>]</mo>       </mrow>      </mfrac>      <mo>-</mo>      <mfrac>       <mrow>        <mo>[</mo>        <mrow>         <mrow>          <msub>           <mi>ROI</mi>           <mi>edge</mi>          </msub>          <mo>-</mo>          <msub>           <mi>ROI</mi>           <mi>p</mi>          </msub>         </mrow>         <mo>&#x22c2;</mo>         <msub>          <mi>ROI</mi>          <mi>g</mi>         </msub>        </mrow>        <mo>]</mo>       </mrow>       <mrow>        <mo>[</mo>        <msub>         <mi>ROI</mi>         <mi>edge</mi>        </msub>        <mo>]</mo>       </mrow>      </mfrac>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0145" num="0123">where ROI<sub>edge </sub>represents a minimum bounding rectangle ROI that can enclose ROI<sub>p </sub>and ROI<sub>g</sub>, and [&#x22c5;] represents an area of the ROI.</p><p id="p-0146" num="0124">Further, for person foreground and background binary classification results, use an erosion algorithm to remove noise, and then use a dilation algorithm to generate a clear edge contour. The finally obtained portrait trimap <b>107</b> is expressed by formula (8):</p><p id="p-0147" num="0000"><maths id="MATH-US-00010" num="00010"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>trimap</mi>      <mi>i</mi>     </msub>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mtable>       <mtr>        <mtd>         <mrow>          <mrow>           <mn>1</mn>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>          </mrow>          <mo>&#x2208;</mo>          <mrow>           <mi>f</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>           <mo>)</mo>          </mrow>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mn>0.5</mn>          <mtext>   </mtext>          <mi>otherwise</mi>          <mtext>             </mtext>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mrow>           <mn>0</mn>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>          </mrow>          <mo>&#x2208;</mo>          <mrow>           <mi>b</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>           <mo>)</mo>          </mrow>         </mrow>        </mtd>       </mtr>      </mtable>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0148" num="0125">where f(pixel<sub>i</sub>) indicates that an i<sup>th </sup>pixel pixel<sub>i </sub>belongs to the foreground, b(pixel<sub>i</sub>) indicates that the i<sup>th </sup>pixel pixel<sub>i </sub>belongs to the background, and trimap<sub>i </sub>represents an alpha mask channel value of the i<sup>th </sup>pixel pixel<sub>i</sub>.</p><p id="p-0149" num="0126">Step S<b>503</b>: Perform feature mapping on the ROI <b>108</b> of the original portrait foreground in step 2, and input the portrait trimap <b>107</b> in the ROI <b>108</b> into the portrait alpha mask matting network model to reduce a convolution computing scale and accelerate network computing. After an original resolution of the image is restored through the upsampling of the decoder, a portrait alpha mask prediction result a is obtained from the output of the fully connected layer.</p><p id="p-0150" num="0127">Step S<b>504</b>; In combination with the original input image, the portrait matting task is completed through foreground extraction, as expressed by formula (9). I represents the input image, F represents the portrait foreground, and B represents the background image.</p><p id="p-0151" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>I=&#x3b1;F+(1&#x2212;&#x3b1;)B&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0152" num="0128">The foregoing is merely a description of the embodiments of the present disclosure, and is not a limitation to the present disclosure. Those of ordinary skill in the art should realize that any changes and modifications made to the present disclosure fall within the protection scope of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005160A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.67mm" wi="76.20mm" file="US20230005160A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005160A1-20230105-M00002.NB"><img id="EMI-M00002" he="12.02mm" wi="76.20mm" file="US20230005160A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005160A1-20230105-M00003.NB"><img id="EMI-M00003" he="9.14mm" wi="76.20mm" file="US20230005160A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230005160A1-20230105-M00004.NB"><img id="EMI-M00004" he="5.67mm" wi="76.20mm" file="US20230005160A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230005160A1-20230105-M00005.NB"><img id="EMI-M00005" he="12.02mm" wi="76.20mm" file="US20230005160A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230005160A1-20230105-M00006.NB"><img id="EMI-M00006" he="9.14mm" wi="76.20mm" file="US20230005160A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230005160A1-20230105-M00007.NB"><img id="EMI-M00007" he="5.67mm" wi="76.20mm" file="US20230005160A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230005160A1-20230105-M00008.NB"><img id="EMI-M00008" he="6.35mm" wi="76.20mm" file="US20230005160A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00009" nb-file="US20230005160A1-20230105-M00009.NB"><img id="EMI-M00009" he="12.02mm" wi="76.20mm" file="US20230005160A1-20230105-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00010" nb-file="US20230005160A1-20230105-M00010.NB"><img id="EMI-M00010" he="9.14mm" wi="76.20mm" file="US20230005160A1-20230105-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00011" nb-file="US20230005160A1-20230105-M00011.NB"><img id="EMI-M00011" he="5.67mm" wi="76.20mm" file="US20230005160A1-20230105-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00012" nb-file="US20230005160A1-20230105-M00012.NB"><img id="EMI-M00012" he="12.02mm" wi="76.20mm" file="US20230005160A1-20230105-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00013" nb-file="US20230005160A1-20230105-M00013.NB"><img id="EMI-M00013" he="9.14mm" wi="76.20mm" file="US20230005160A1-20230105-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A multi-task deep learning-based real-time matting method for non-green-screen portraits, comprising:<claim-text>step 1: performing binary classification adjustment on an original multi-class multi-object detection dataset, inputting an image or video containing portrait information, and performing data preprocessing on the image or video to obtain preprocessed data of an original input file;</claim-text><claim-text>step 2: using an encoder-logistic regression structure to construct a deep learning network for person detection, inputting the preprocessed data obtained in step 1, constructing a loss function, training and optimizing the deep learning network for person detection to obtain a person detection model;</claim-text><claim-text>step 3: extracting feature maps from an encoder of the person detection model in step 2, and performing feature stitching and fusing multi-scale image features to form an encoder of a portrait alpha mask matting network, to implement an encoder shared by the person detection and portrait alpha mask matting networks;</claim-text><claim-text>step 4: constructing a decoder of the portrait alpha mask matting network, forming an end-to-end encoder-decoder portrait alpha mask matting network structure together with the shared encoder in step 3, inputting an image containing person information and a trimap, constructing a loss function, and training and optimizing the portrait alpha mask matting network;</claim-text><claim-text>step 5: inputting the preprocessed data obtained in step 1 to a trained network in step 4, and outputting a region of interest (ROI) of portrait foreground and a portrait trimap in the ROI through logistic regression of the person detection model in step 2; and</claim-text><claim-text>step 6: inputting the ROI of the portrait foreground and the portrait trimap in step 5 into the portrait alpha mask matting network constructed in step 4 to obtain a portrait alpha mask prediction result.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The multi-task deep learning-based real-time matting method for non-green-screen portraits according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the data preprocessing in step 1 comprises video frame processing and input image resizing.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The multi-task deep learning-based real-time matting method for non-green-screen portraits according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the deep learning network for person detection in step 2 is implemented through model prediction of a deep residual neural network.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The multi-task deep learning-based real-time matting method for non-green-screen portraits according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a main structure of the decoder in step 4 comprises upsampling, convolution, an exponential linear unit (ELU) activation function, and a fully connected layer for outputting.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The multi-task deep learning-based real-time matting method for non-green-screen portraits according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the upsampling is used to restore an image feature size after downsampling in the encoder, a scaled ELU (SELU) activation function is used, hyperparameters &#x3bb; and &#x3b1; are fixed constants, and the activation function is expressed by formula (2):</claim-text><claim-text><maths id="MATH-US-00011" num="00011"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>SeLU</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>x</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mi>&#x3bb;</mi>      <mo>&#x2062;</mo>      <mrow>       <mo>{</mo>       <mrow>        <mtable>         <mtr>          <mtd>           <mrow>            <mi>x</mi>            <mo>,</mo>            <mrow>             <mi>x</mi>             <mo>&#x3e;</mo>             <mn>0</mn>             <mtext>                 </mtext>            </mrow>           </mrow>          </mtd>         </mtr>         <mtr>          <mtd>           <mrow>            <mrow>             <mi>&#x3b1;</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mrow>              <msup>               <mi>e</mi>               <mi>x</mi>              </msup>              <mo>-</mo>              <mn>1</mn>             </mrow>             <mo>)</mo>            </mrow>            <mo>,</mo>            <mrow>             <mi>x</mi>             <mo>&#x2264;</mo>             <mn>0</mn>            </mrow>           </mrow>          </mtd>         </mtr>        </mtable>        <mo>.</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The multi-task deep learning-based real-time matting method for non-green-screen portraits according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the constructing a loss function, and training and optimizing the portrait alpha mask matting network in step 4 specifically comprise:<claim-text>(4.1) computing an alpha mask prediction error, as expressed by formula (3):<claim-text><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>&#x3b1;lp</sub>=&#x221a;{square root over ((&#x3b1;<sub>pre</sub>&#x2212;&#x3b1;<sub>gro</sub>)<sup>2</sup>)}+&#x3b5;,&#x3b1;<sub>pre</sub>,&#x3b1;<sub>gro</sub>&#x2208;[0,1]&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></claim-text></claim-text><claim-text>wherein Loss<sub>&#x3b1;lp </sub>represents the alpha mask prediction error, &#x3b1;<sub>pre </sub>and &#x3b1;<sub>gro </sub>respectively represent predicted and ground-truth alpha mask values, and &#x3b5; represents a very small constant;</claim-text><claim-text>(4.2) computing an image compositing error, as expressed by formula (4):<claim-text><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>com</sub>=&#x221a;{square root over ((<i>c</i><sub>pre</sub><i>&#x2212;c</i><sub>gro</sub>)<sup>2</sup>)}+&#x3b5;&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></claim-text></claim-text><claim-text>wherein Loss<sub>com </sub>represents the image compositing error, c<sub>pre </sub>and c<sub>gro </sub>respectively represent predicted and ground-truth alpha composite images, and &#x3b5; represents a very small constant; and</claim-text><claim-text>(4.3) constructing an overall loss function based on the alpha mask prediction error and the image compositing error, as expressed by formula (5):<claim-text><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Loss<sub>overall</sub>=&#x3c9;<sub>1</sub>Loss<sub>&#x3b1;lp</sub>+&#x3c9;<sub>2</sub>Loss<sub>com</sub>,&#x3c9;<sub>1</sub>+&#x3c9;<sub>2</sub>=1&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></claim-text></claim-text><claim-text>wherein Loss<sub>overall </sub>represents the overall loss function, &#x3c9;<sub>1 </sub>and &#x3c9;<sub>2 </sub>respectively represent weights of the alpha mask prediction error Loss<sub>&#x3b1;lp </sub>and the image compositing error Loss<sub>&#x3b1;lp</sub>.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The multi-task deep learning-based real-time matting method for non-green-screen portraits according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the outputting a ROI of portrait foreground and a portrait trimap in the ROI in step 5 specifically comprise:<claim-text>(5.1) using a relative intersection over union (RIOU) obtained by improving an original criteria for determining the ROI of the portrait foreground, wherein the RIOU is expressed by formula (7):</claim-text></claim-text><claim-text><maths id="MATH-US-00012" num="00012"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>RIOU</mi>     <mo>=</mo>     <mrow>      <mfrac>       <mrow>        <mo>[</mo>        <mrow>         <msub>          <mi>ROI</mi>          <mi>p</mi>         </msub>         <mo>&#x22c2;</mo>         <msub>          <mi>ROI</mi>          <mi>g</mi>         </msub>        </mrow>        <mo>]</mo>       </mrow>       <mrow>        <mo>[</mo>        <mrow>         <mrow>          <msub>           <mi>ROI</mi>           <mrow>            <mi>p</mi>            <mo>&#x22c3;</mo>           </mrow>          </msub>          <mo>&#x2062;</mo>          <msub>           <mi>ROI</mi>           <mi>g</mi>          </msub>         </mrow>         <mo>+</mo>         <mrow>          <mi>&#x3b1;</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <msub>             <mi>ROI</mi>             <mi>g</mi>            </msub>            <mo>-</mo>            <msub>             <mi>ROI</mi>             <mi>p</mi>            </msub>           </mrow>           <mo>&#x22c2;</mo>           <msub>            <mi>ROI</mi>            <mi>g</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>]</mo>       </mrow>      </mfrac>      <mo>-</mo>      <mfrac>       <mrow>        <mo>[</mo>        <mrow>         <mrow>          <msub>           <mi>ROI</mi>           <mi>edge</mi>          </msub>          <mo>-</mo>          <msub>           <mi>ROI</mi>           <mi>p</mi>          </msub>         </mrow>         <mo>&#x22c2;</mo>         <msub>          <mi>ROI</mi>          <mi>g</mi>         </msub>        </mrow>        <mo>]</mo>       </mrow>       <mrow>        <mo>[</mo>        <msub>         <mi>ROI</mi>         <mi>edge</mi>        </msub>        <mo>]</mo>       </mrow>      </mfrac>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><claim-text>wherein ROI<sub>edge </sub>represents a minimum bounding rectangle ROI that can enclose ROI<sub>p </sub>and ROI<sub>g</sub>, [&#x22c5;] represents an area of the ROI, ROI<sub>p </sub>represents a predicted value of the ROI of the portrait foreground, and ROI<sub>g </sub>represents a ground-truth value of the ROI of the portrait foreground; and</claim-text><claim-text>(5.2) for person foreground and background binary classification results, using an erosion method to remove noise, and then using a dilation method to generate a clear edge contour, to obtain the portrait trimap, as expressed by formula (8):</claim-text></claim-text><claim-text><maths id="MATH-US-00013" num="00013"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>trimap</mi>      <mi>i</mi>     </msub>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mtable>       <mtr>        <mtd>         <mrow>          <mrow>           <mn>1</mn>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>          </mrow>          <mo>&#x2208;</mo>          <mrow>           <mi>f</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>           <mo>)</mo>          </mrow>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mn>0.5</mn>          <mtext>   </mtext>          <mi>otherwise</mi>          <mtext>              </mtext>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mrow>           <mn>0</mn>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>          </mrow>          <mo>&#x2208;</mo>          <mrow>           <mi>b</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <msub>            <mi>pixel</mi>            <mi>i</mi>           </msub>           <mo>)</mo>          </mrow>         </mrow>        </mtd>       </mtr>      </mtable>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><claim-text>wherein f(pixel<sub>i</sub>) indicates that an i<sup>th </sup>pixel pixel<sub>i </sub>belongs to the foreground, b(pixel<sub>i</sub>) indicates that the i<sup>th </sup>pixel pixel<sub>i </sub>belongs to the background, otherwise indicates that it cannot be determined whether the pixel belongs to the foreground or background, and trimap<sub>i </sub>represents an alpha mask channel value of the i<sup>th </sup>pixel pixel<sub>i</sub>.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A multi-task deep learning-based real-time matting system for non-green-screen portraits, comprising an input unit, a processor and a memory storing program codes, wherein the processor performs the stored program codes for:<claim-text>step 1: performing binary classification adjustment on an original multi-class multi-object detection dataset, and performing data preprocessing on an image or video containing portrait information and inputted from the input unit, to obtain preprocessed data of an original input file;</claim-text><claim-text>step 2: using an encoder-logistic regression structure to construct a deep learning network for person detection, inputting the preprocessed data obtained in step 1, constructing a loss function, training and optimizing the deep learning network for person detection to obtain a person detection model;</claim-text><claim-text>step 3: extracting feature maps from an encoder of the person detection model in step 2, and performing feature stitching and fusing multi-scale image features to form an encoder of a portrait alpha mask matting network, to implement an encoder shared by the person detection and portrait alpha mask matting networks;</claim-text><claim-text>step 4: constructing a decoder of the portrait alpha mask matting network, forming an end-to-end encoder-decoder portrait alpha mask matting network structure together with the shared encoder in step 3, inputting an image containing person information and a trimap, constructing a loss function, and training and optimizing the portrait alpha mask matting network;</claim-text><claim-text>step 5: inputting the preprocessed data obtained in step 1 to a trained network in step 4, and outputting a region of interest (ROI) of portrait foreground and a portrait trimap in the ROI through logistic regression of the person detection model in step 2; and</claim-text><claim-text>step 6: inputting the ROI of the portrait foreground and the portrait trimap in step 5 into the portrait alpha mask matting network constructed in step 4 to obtain a portrait alpha mask prediction result.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A computer program product comprising a non-volatile computer readable medium having computer executable codes stored thereon, the codes comprising instructions for:<claim-text>step 1: performing binary classification adjustment on an original multi-class multi-object detection dataset, and performing data preprocessing on an image or video containing portrait information and inputted from the input unit, to obtain preprocessed data of an original input file;</claim-text><claim-text>step 2: using an encoder-logistic regression structure to construct a deep learning network for person detection, inputting the preprocessed data obtained in step 1, constructing a loss function, training and optimizing the deep learning network for person detection to obtain a person detection model;</claim-text><claim-text>step 3: extracting feature maps from an encoder of the person detection model in step 2, and performing feature stitching and fusing multi-scale image features to form an encoder of a portrait alpha mask matting network, to implement an encoder shared by the person detection and portrait alpha mask matting networks;</claim-text><claim-text>step 4: constructing a decoder of the portrait alpha mask matting network, forming an end-to-end encoder-decoder portrait alpha mask matting network structure together with the shared encoder in step 3, inputting an image containing person information and a trimap, constructing a loss function, and training and optimizing the portrait alpha mask matting network;</claim-text><claim-text>step 5: inputting the preprocessed data obtained in step 1 to a trained network in step 4, and outputting a region of interest (ROI) of portrait foreground and a portrait trimap in the ROI through logistic regression of the person detection model in step 2; and</claim-text><claim-text>step 6: inputting the ROI of the portrait foreground and the portrait trimap in step 5 into the portrait alpha mask matting network constructed in step 4 to obtain a portrait alpha mask prediction result.</claim-text></claim-text></claim></claims></us-patent-application>