<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004797A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004797</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17781827</doc-number><date>20210211</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">PHYSICS-GUIDED DEEP MULTIMODAL EMBEDDINGS FOR TASK-SPECIFIC DATA EXPLOITATION</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62987697</doc-number><date>20200310</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SRI INTERNATIONAL</orgname><address><city>Menlo Park</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>CHIU</last-name><first-name>Han-Pang</first-name><address><city>West Windsor</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SEYMOUR</last-name><first-name>Zachary</first-name><address><city>Pennington</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>MITHUN</last-name><first-name>Niluthpol C.</first-name><address><city>Lawrenceville</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>SAMARASEKERA</last-name><first-name>Supun</first-name><address><city>Skillman</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>KUMAR</last-name><first-name>Rakesh</first-name><address><city>West Windsor</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>YAO</last-name><first-name>Yi</first-name><address><city>Princeton</city><state>NJ</state><country>US</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/US2021/017731</doc-number><date>20210211</date></document-id><us-371c12-date><date>20220602</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method, apparatus and system for object detection in sensor data having at least two modalities using a common embedding space includes creating first modality vector representations of features of sensor data having a first modality and second modality vector representations of features of sensor data having a second modality, projecting the first and second modality vector representations into the common embedding space such that related embedded modality vectors are closer together in the common embedding space than unrelated modality vectors, combining the projected first and second modality vector representations, and determining a similarity between the combined modality vector representations and respective embedded vector representations of features of objects in the common embedding space to identify at least one object depicted by the captured sensor data. In some instances, data manipulation of the method, apparatus and system can be guided by physics properties of a sensor and/or sensor data.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="64.94mm" wi="158.75mm" file="US20230004797A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="178.73mm" wi="169.50mm" file="US20230004797A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="156.29mm" wi="175.77mm" file="US20230004797A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="207.26mm" wi="176.78mm" file="US20230004797A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="209.97mm" wi="177.88mm" file="US20230004797A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="119.21mm" wi="158.75mm" file="US20230004797A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="137.41mm" wi="182.03mm" file="US20230004797A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="214.71mm" wi="136.40mm" file="US20230004797A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="217.76mm" wi="158.83mm" file="US20230004797A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="187.88mm" wi="158.24mm" file="US20230004797A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="190.67mm" wi="126.83mm" file="US20230004797A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">Embodiments of the present principles generally relate to the evaluation and generation of sensor data, and more particularly, to the evaluation and generation of sensor data using multimodal embeddings.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Sensor fusion for tasks such as target detection and recognition in challenging environments is an important problem to solve in many fields. However, sensor data in these environments is typically very noisy in nature. In addition, data captured using different sensors can be dramatically different due to diverse physical characteristics. Current sensor fusion methods for these tasks are limited to early stage fusion (raw data level) for same (or similar) types of sensors. They cannot fuse complementary information from different types of sensors to achieve more robust and accurate task performance. In addition, past approaches to tasks such as target detection and recognition focus on one single data source and rely on extracting hand-crafted features or deep-learned features.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">Embodiments of methods, apparatuses and systems for object detection in sensor data having at least two modalities using a common embedding space are disclosed herein.</p><p id="p-0005" num="0004">In some embodiments in accordance with the present principles, a method for training a common embedding space for combining sensor data captured of a common scene having at least two modalities includes for each of a plurality of the captured sensor data having a first modality of the at least two modalities, creating respective first modality sensor-data vector representations of the features of the sensor data having the first modality using a sensor data-specific neural network (e.g., convolutional neural network, recurrent neural network, transformer, etc.) for each of a plurality of the captured sensor data having a second modality of the at least two modalities, creating respective second modality sensor-data vector representations of the features of the sensor data having the second modality using a sensor data-specific neural network, embedding the first modality sensor-data vector representations and the second modality sensor-data vector representations in a common embedding space such that embedded modality vectors that are related, across modalities, are closer together in the common embedding space than unrelated modality vectors, and respectively combining the embedded first modality sensor-data vector representations and the second modality vector representations.</p><p id="p-0006" num="0005">In some embodiments of the present principles, the method can further include constraining at least one of the creating of the first and second modality sensor-data vector representations and the embedding of the first and the second modality sensor-data vector representations by physics properties of at least one of a respective sensor having captured the first modality sensor data and the second modality sensor data, and physics properties of the first modality sensor data and the second modality sensor data.</p><p id="p-0007" num="0006">In some embodiments in accordance with the present principles, a method for object detection in sensor data having at least two modalities using a common embedding space includes creating respective first modality sensor-data vector representations of features of sensor data having a first modality of the at least two modalities, creating respective second modality sensor-data vector representations of features of sensor data having a second modality of the at least two modalities, projecting the first modality sensor-data vector representations and the second modality sensor-data vector representations into the common embedding space such that embedded modality vectors that are related, across modalities, are closer together in the common embedding space than unrelated modality vectors, combining the projected first modality sensor-data vector representations and the second modality sensor-data vector representations, and determining a similarity between the combined modality sensor-data vector representations and respective embedded vector representations of features of objects in the common embedding space using a distance function to identify at least one object depicted by the sensor data having the at least two modalities.</p><p id="p-0008" num="0007">In some embodiments of the present principles, the method can further include constraining at least one of the creating of the first and second modality sensor-data vector representations and the embedding of the first and the second modality sensor-data vector representations by physics properties of at least one of a respective sensor having captured the first modality sensor data and the second modality sensor data, and physics properties of the first modality sensor data and the second modality sensor data.</p><p id="p-0009" num="0008">In some embodiments in accordance with the present principles, an apparatus for object detection in sensor data having at least two modalities using a common embedding space includes at least one feature extraction module configured to create respective first modality sensor-data vector representations of features of sensor data having a first modality of the at least two modalities and respective second modality sensor-data vector representations of features of sensor data having a second modality of the at least two modalities, at least one embedding module configured to project the first modality sensor-data vector representations and the second modality sensor-data vector representations into the common embedding space such that embedded modality vectors that are related, across modalities, are closer together in the common embedding space than unrelated modality vectors, a fusion module configured to combine the projected first modality sensor-data vector representations and the second modality sensor-data vector representations, and an inference module configured to determine a similarity between the combined modality sensor-data vector representations and respective embedded vector representations of features of objects in the common embedding space using a distance function to identify at least one object depicted by the sensor data having the at least two modalities.</p><p id="p-0010" num="0009">In some embodiments of the present principles, the apparatus can further be configured to constrain/guide at least one of the creating of the first and second modality sensor-data vector representations and the embedding of the first and the second modality sensor-data vector representations by physics properties of at least one of a respective sensor having captured the first modality sensor data and the second modality sensor data, and physics properties of the first modality sensor data and the second modality sensor data.</p><p id="p-0011" num="0010">Other and further embodiments in accordance with the present principles are described below.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011">So that the manner in which the above recited features of the present principles can be understood in detail, a more particular description of the principles, briefly summarized above, may be had by reference to embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments in accordance with the present principles and are therefore not to be considered limiting of its scope, for the principles may admit to other equally effective embodiments.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a high-level block diagram of a sensor data fusion system in accordance with an embodiment of the present principles.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a high-level functional diagram of the functionality of a sensor data fusion system of the present principles, such as the sensor data fusion system of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in accordance with an embodiment of the present principles.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> depicts a high-level functional diagram of the operation of the optional generator module and the optional discriminator module of a sensor data fusion system in accordance with an embodiment of the present principles.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> depicts a high-level functional diagram of a simulation of the data capture of a learned sensor, such as Sensor <b>2</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, in a sensor data fusion system in accordance with an embodiment of the present principles.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> depicts a high-level functional diagram of the operation of the optional generator module and the optional discriminator module of a sensor data fusion system in accordance with an alternate embodiment of the present principles.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> depicts a high-level functional diagram of a simulation of the characteristics of a data modality, such as Data Model <b>2</b> of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, in a sensor data fusion system in accordance with an embodiment of the present principles.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a high-level functional diagram of a sensor data fusion system of the present principles that can be implemented for 3D target/object detection in accordance with an embodiment of the present principles.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a high-level functional block diagram of a sensor data fusion system of the present principles that can be implemented for underwater target/object sensing applications in accordance with an embodiment of the present principles.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a flow diagram of a method for training a common embedding space for combining sensor data captured of a common scene having at least two modalities in accordance with an embodiment of the present principles.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts a flow diagram of a method for object detection using a common embedding space trained in accordance with an embodiment of the present principles.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts a high-level block diagram of a computing device suitable for use with embodiments of a sensor data fusion system, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in accordance with an embodiment of the present principles.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts a high-level block diagram of a network in which embodiments of a sensor data fusion system in accordance with the present principles can be applied.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0025" num="0024">To facilitate understanding, identical reference numerals have been used, where possible, to designate identical elements that are common to the figures. The figures are not drawn to scale and may be simplified for clarity. It is contemplated that elements and features of one embodiment may be beneficially incorporated in other embodiments without further recitation.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0026" num="0025">Embodiments of the present principles generally relate to methods, apparatuses and systems for sensor data fusion for combining sensor data from multiple and different types of sensors having multiple modalities using multimodal embeddings. While the concepts of the present principles are susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and are described in detail below. It should be understood that there is no intent to limit the concepts of the present principles to the particular forms disclosed. On the contrary, the intent is to cover all modifications, equivalents, and alternatives consistent with the present principles and the appended claims. For example, although embodiments of the present principles will be described primarily with respect to specific sensors, such teachings should not be considered limiting. Embodiments in accordance with the present principles can function with substantially any sensor and/or detector.</p><p id="p-0027" num="0026">Some embodiments of the present principles provide an end-to-end pipeline system that fuses multi-sensor (i.e., multi-modal) data in a common embedding space by directly optimizing target metrics and learning the contribution of each sensor (mode) to the results. In some embodiments, physics properties of the respective sensors and/or the sensor data, such as physics equations and principles, are incorporated into learning feature extraction, that ensures physical properties of each sensor are recoverable from their low-dimensional representations, to facilitate learning with fewer examples while maintaining generalization performance.</p><p id="p-0028" num="0027">Some embodiments of the present principles include attention-based mode fusion, which can include learning an importance of each sensor (mode), in some embodiments through self-attention mechanism, that provides the information for selecting modes based on available payload and computational resources.</p><p id="p-0029" num="0028">Some embodiments of the present principles include physics-guided embedding. In such embodiments, physics properties of the respective sensors and/or the sensor data (i.e., imaged objects) provide additional structure and constraints into the embedding space through encoded domain knowledge. The constraints in the embedding space provide additional expandability and verification of the results, by comparing results to known physics properties including, but not limited to, surface reflection of objects, temperature, and humidity. For example, from the perspective of physically plausible compositionality, the embedding of a mixture of two materials (e.g., grass as foreground and soil as background) will ideally lie in between the embeddings of these two materials. However, from the perspective of physically plausible properties, the embeddings of a material with different environmental temperature or humidity not only cluster in the close neighborhood but also form a trajectory that consistently evolves as temperature or humidity changes.</p><p id="p-0030" num="0029">Some embodiments of the present principles include late-stage fusion. Compared to traditional early-stage raw data fusion, fusing feature data in a common embedding space in accordance with the present principles increases the robustness to spoof data from single modalities.</p><p id="p-0031" num="0030">Embodiments of the present principles enable the use of a single model during inference, after training. The trained multi-modal common embedding space enables the use of only one sensor model. That is, in some embodiments, in a scenario where additional sensor information may not be available (e.g., for a new place), embodiments of the present principles can implement a Generative Adversarial Network (GAN) and/or variational autoencoders to learn differences across sensors and later augment available sensor data.</p><p id="p-0032" num="0031">In other embodiments, a model simulator can be utilized to learn differences across data modalities and augment a trained network when there is limited data for a new data model.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a high-level block diagram of a sensor data fusion system <b>100</b> for combining sensor data from multiple different types of sensors using multimodal embedding in accordance with an embodiment of the present principles. The sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustratively comprises a feature extraction module <b>110</b>, an embedding module <b>120</b>, a fusion module <b>130</b>, and an inference module <b>135</b>. In some embodiments and as specifically depicted in the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a sensor data fusion system of the present principles can further comprise an optional generator module <b>140</b> and optional discriminator module <b>150</b>, the function of which will be described further below. Although the embodiment of the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustratively comprises a single feature extraction module <b>110</b>, in some embodiments, a sensor data fusion system of the present principles can include more than one feature extraction module and in some embodiments can include at least one feature extraction module for each different sensor type (modality) and/or each different type of sensor data (modality) to be combined.</p><p id="p-0034" num="0033">As depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, embodiments of a sensor data fusion system in accordance with the present principles, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, can be implemented in a computing device <b>900</b> (described in greater detail with respect to <figref idref="DRAWINGS">FIG. <b>9</b></figref>) in accordance with the present principles. That is, in some embodiments, sensor data can be communicated to the sensor data fusion system <b>100</b> using the computing device <b>900</b> via, for example, any input/output means associated with the computing device <b>900</b>. In addition, images and data that can be used to train an embedding space in accordance with the present principles and any queries to be processed can be accessed by the sensor data fusion system <b>100</b> from the computing device <b>900</b> using any input/output means associated with the computing device <b>900</b>. Results of a sensor data fusion system in accordance with the present principles can be presented to a user using an output device of the computing device <b>900</b>, such as a display, a printer or any other form of output device (described in greater detail with respect to <figref idref="DRAWINGS">FIG. <b>9</b></figref>).</p><p id="p-0035" num="0034">In the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, images of a scene captured by at least two different types (e.g., different modalities) of sensors are communicated to a respective one of the at least one feature extraction module <b>110</b>. In some embodiments, at the at least one feature extraction module <b>110</b>, neural networks can be applied to the respective captured images of the at least two different types of sensors to extract the visual features of the images of the at least two different types of sensors. Such neural networks can include, but are not limited to, convolutional neural networks (CNNs), recurrent neural networks (RNNs), transformers, and substantially any neural network that can extract vector representations for features of the sensor data. In some embodiments, model (sensor) specific neural networks can be used to determine respective image features of the images from the at least two different types of sensors. The extracted image features from the at least one feature extraction module <b>110</b> are communicated to respective ones of the at least one embedding module <b>120</b>. In some embodiments, at each of the at least one embedding module <b>120</b>, the extracted image features of the at least two different types of sensors are embedded/projected into a common embedding space.</p><p id="p-0036" num="0035">In some embodiments, a common embedding space in accordance with the present principles can be created using Word2vec. Word2vec is a machine learning process/model that produces word embedding vectors, where words are associated with a number to produce a numerical essence of the word. In some embodiments, respective word vectors for the words representative of text interacted with by a user(s) can be created. Word2vec produces word embeddings (arrays of numbers) where the words (i.e., representative of text) with similar meanings or context are physically close to each other in the embedded space. The numbers are typically arranged in arrays that allow mathematical processes to be performed on the numbers. Quantifying words as a series of numbers allows machine learning to find anew word similar to the other two words based on numbers and data properties of each word based on a model. The words can then be graphed and compared to words based on mathematical properties. The distance between graphed words can be described as vectors or a distance with a direction. Moving from one graphed word to another graphed word in space allows one to represent/graph the idea of word relationships which are hard coded &#x201c;word vectors.&#x201d; In such embodiments, a convolutional neural network (CNN) can be used to create an embedding space in accordance with the present principles.</p><p id="p-0037" num="0036">In some embodiments, a common embedding space in accordance with the present principles can be pre-trained using an embedding model referred to as DeViSE (see, A. Frome, G. Corrado, and J. Shlens, &#x201c;DeViSE: A deep visual-semantic embedding model,&#x201d; Adv. Neural . . . , pp. 1-11, 2013). Specifically, an image embedding is trained that maps every image to a word embedding space. This can be achieved using a convolutional neural network attaching a fully connected layer to transform. To avoid a sparse sampling of the embedding space during pre-training, additional data from external sources can be implemented to train the semantic embedding space to produce a dense object class sampling in the semantic embedding space. In some embodiments, the word embeddings are not learned but are initialized using GloVE (see, J. Pennington, R. Socher, and C. D. Manning, &#x201c;GloVe: Global Vectors for Word Representation.&#x201d;).</p><p id="p-0038" num="0037">In some embodiments, an embedding space in accordance with the present principles can be further trained using data captured by the different types (modalities) of sensors. That is, features are extracted from images captured by the at least two different types (modalities) of sensors by at least one respective feature extraction module and are embedded into the common embedding space by at least one embedding module. Because the features of the images from the different types of sensors are embedded in a common embedding space, a relationship between data captured by the different types of sensors can be determined (as will be described in greater detail below).</p><p id="p-0039" num="0038">That is, in some embodiments, to more specifically train a common embedding space in accordance with the present principles, sensor data have at least a first modality and a second modality can be received by a feature extraction module of a sensor data fusion system of the present principles, such as the feature extraction module <b>110</b> of the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For each of a plurality of the captured sensor data having a first modality of the at least two modalities, respective first modality sensor-data vector representations of the features of the sensor data having the first modality are created. Similarly, for each of a plurality of the captured sensor data having the second modality of the at least two modalities, respective second modality sensor-data vector representations of the features of the sensor data having the second modality are created. The first modality sensor-data vector representations and the second modality sensor-data vector representations can then be embedded in a common embedding space such that embedded modality vectors that are related, across modalities, are closer together in the common embedding space than unrelated modality vectors. The embedded first modality sensor-data vector representations and the second modality vector representations can then be combined into a resultant vector to train the common embedding space.</p><p id="p-0040" num="0039">Once a common embedding space is trained in accordance with embodiments of the present principles and as described above, the trained, common embedding space can be used to identify at least one object depicted by captured sensor data having at least two modalities. For example, in some embodiments, for each of a plurality of sensor data having a first modality, respective first modality sensor-data vector representations of the features of the sensor data having the first modality are created. Similarly, for each of a plurality of the sensor data having a second modality of the at least two modalities, respective second modality sensor-data vector representations of the features of the sensor data having the second modality are created. The first modality sensor-data vector representations and the second modality sensor-data vector representations can then be projected in the trained common embedding space such that embedded modality vectors that are related, across modalities, are closer together in the common embedding space than unrelated modality vectors. The projected first modality sensor-data vector representations and the second modality vector representations can then be combined into a resultant vector. In accordance with the present principles, a similarity can be determined between the combined, resultant vectors and respective embedded vector representations of features of objects in the trained, common embedding space using a distance function to identify at least one object depicted by the sensor data having the at least two modalities.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a high-level functional diagram of the functionality of a sensor data fusion system of the present principles, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in accordance with an embodiment of the present principles. Illustratively in the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, four (4) different type of sensors <b>202</b><sub>1</sub>, <b>202</b><sub>2</sub>, <b>2023</b>, <b>2024</b> (collectively sensors <b>202</b>) having different modalities (mod) capture images of a same scene. In some embodiments, such as the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the four (4) sensors can include but are not limited to sensors such as Radar sensors, RGB sensors, electro-optic (EO) sensors, satellite image sensors, InfraRed sensors, etc. In the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the data captured by each of the four (4) different type of sensors <b>202</b><sub>1</sub>, <b>202</b><sub>2</sub>, <b>2023</b>, <b>2024</b> is communicated to a respective one of four feature extraction modules <b>1101</b>, <b>1102</b>, <b>1103</b>, <b>1104</b> (collectively feature extraction modules <b>110</b>). As described above, at the feature extraction modules <b>110</b>, image features of the images captured by the sensors <b>202</b> are extracted, for example in some embodiments, using neural networks, such as model-specific CNNs.</p><p id="p-0042" num="0041">For example, in some embodiments a first sensor type can include a satellite sensor collecting satellite imagery of a scene. In such embodiments, image features of the data captured by the satellite sensor can be extracted by a respective feature extraction module configured to extract image features of satellite imagery using CNNs trained to extract image features of satellite imagery. In addition, a second type sensor can include an electro-optical (EO) sensor collecting images of, for example, the same scene. In such embodiments, image features of the data captured by the EO sensor can be extracted by a respective feature extraction module configured to extract image features of EO data using CNNs trained to extract image features from EO data. Even further, a third type sensor can include an infrared (IR) sensor collecting IR images of, for example, the same scene. In such embodiments, image features of the data captured by the IR sensor can be extracted by a respective feature extraction module configured to extract image features of IR data using CNNs trained to extract image features from IR data. The extracted satellite, EO, and IR image features can then be projected/embedded into a common embedding space, by, for example, at least one embedding module of the present principles, as described above.</p><p id="p-0043" num="0042">That is, and as depicted in the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the extracted image features of each of the feature extraction modules <b>110</b> are communicated to a respective one of four embedding modules <b>1201</b>, <b>1202</b>, <b>1203</b>, <b>1204</b> (collectively embedding modules <b>120</b>). At the embedding modules <b>120</b>, the extracted features of the data from the different types of sensors <b>202</b> are projected/embedded into the common embedding space <b>210</b>. In some embodiments, the image embedding modules <b>120</b> determine respective feature vector representations of the image data of the different types of sensors for projecting/embedding the image data into the common embedding space <b>210</b>. That is, image features received by the embedding modules <b>120</b> can be projected/embedded in the common embedding space <b>210</b> by, in some embodiments, projecting/embedding word vectors representative of the features of the image data captured by the different types of sensors <b>202</b> into the embedding space <b>210</b> via, in some embodiments, a linear projection. In some embodiments in accordance with the present principles, the embedding modules <b>120</b> can include a deep image encoder (not shown) implementing a neural network, in some embodiments a convolutional neural network (CNN) with fully connected (FC) layers, to process the image data for projecting/embedding the image data into the common embedding space. In some other embodiments, alternatively or in addition, the neural network can include a transformer.</p><p id="p-0044" num="0043">Referring back to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, in some embodiments the embedded feature vectors of the data from the different types of sensors can be fused in the embedding space by the fusion module <b>130</b>. For example, in some embodiments and as depicted in the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the fusion module <b>130</b> combines respective embedded feature vectors of the extracted features of the of data captured by the different types of sensors <b>202</b> for each of the modalities into a combined feature vector representation of respective features of a common scene captured by the different types of sensors <b>202</b>. In some embodiments of the present principles, the fusion module <b>130</b> incorporates a late fusion technique, which includes first, a determination of feature vectors of image data of the different types of sensors, projecting/embedding the determined feature vectors in a common embedding space, and then a combination of the projected/embedded feature vectors of image data of the different types of sensors for each of multiple modalities. Such a late fusion/combination in accordance with the present principles provides more discriminative results than early fusion techniques.</p><p id="p-0045" num="0044">The combined feature vectors of the image data of the different types of sensors for each of multiple modalities can be used for detection of targets/objects in sensor data in accordance with some embodiments of the present principles. For example, and referring back to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in some embodiments, the inference module <b>135</b> is configured to determine a similarity between a combination of projected feature vector representations of image data of the different types of sensors for each of multiple modalities, combined as described above, and respective embedded feature vector representations of image data in the previously trained common embedding space. In some embodiments, an inference module of the present principles, such as the inference module <b>135</b> of the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, can implement a distance function to determine a similarity between the combination of the projected feature vector representations and the respective embedded feature vector representations in the previously trained common embedding space to identify at least one object depicted by the sensor data having multiple modalities. In some embodiments, the distance function can include at least one of a cosine function, a Euclidean function, and/or a Lagrangian point 1, L1, function, and the like.</p><p id="p-0046" num="0045">In some embodiments of a sensor data fusion system of the present principles, the feature extraction of the present principle, performed by, for example, the feature extraction modules <b>110</b> can include physics guided feature extraction. For example, in some embodiments, features extracted from sensor data captured by sensors of a specific type can be constrained/limited/guided according to physics properties of the sensor and/or sensor data modality. That is, in some embodiments feature extraction can be constrained/guided by incorporating physics equations/properties and principles into learning feature extraction using machine learning, that ensures physical properties of each mode are recoverable from their low-dimensional representations to facilitate learning with fewer examples while maintaining generalization performance. For example, in some embodiments neural networks, such as CNNs, trained to perform feature extraction according to physics equations/physics properties of a specific type of sensor and/or sensor data modality can be implemented by a feature extraction module of the present principles to extract features of data captured by sensors of that specific type. As such and in accordance with the present principles, outlier data captured by sensors can be ignored. For example, in some embodiments, for each data model, an encoder and decoder can be implemented that deconstructs features and reconstructs features using incorporated physics properties/equations. As such, if the reconstructed features have similar features to the original data, the features can be considered to preserve the physics properties of the original data.</p><p id="p-0047" num="0046">Alternatively or in addition, in some embodiments of a sensor data fusion system of the present principles, the fusion of the present principle, performed by, for example, the fusion module <b>130</b> of the present principles can include attention-based mode fusion. For example, in some embodiments during training of a common embedding space as described above, individual contributions of the different sensors <b>202</b> to resulting sensor data combinations (i.e., multimodal embeddings) are noted by, in some embodiments, the fusion module <b>130</b>. As such, during application or use of a sensor data fusion system of the present principles, the fusion module <b>130</b> can weigh a respective contribution of each of the different sensors <b>202</b> to achieve a desired combined signal. In some embodiments, attention can be based on sensor modalities instead of individual sensors. For example, in some embodiments, attention to the contribution of sensor modalities to a combined signal instead of a contribution of individual sensors themselves can be taken into account when weighing data captured by different types of sensor for achieving a desired combined signal.</p><p id="p-0048" num="0047">In some embodiments, the attention-based fusion of the present principles can include the fusion of multimodal information using attention between modalities in a neural network. Such embodiments are able to dynamically adjust the relative importance of each modality to generate better data combinations. In some embodiments, benefits of attention-based multimodal fusion of the present principles can include: (1) the modalities that contribute more to a data combination can dynamically receive a stronger weight, and (2) the network can detect interference (e.g., noise) and other sources of uncertainty in each modality and dynamically down-weight the modalities that are less certain.</p><p id="p-0049" num="0048">In some embodiments of a sensor data fusion system of the present principles, the projection/embedding of the sensor data into a common embedding space in accordance with the present principles, by for example the embedding module <b>120</b>, can be constrained/guided by physics properties of the sensors and/or associated data. For example, in some embodiments, a common embedding space can be trained using physics equations and principles of the sensors and associated data that provide additional structure and constraints into the embedding space. Subsequently, sensor data can be projected/embedded into the common embedding space in accordance with the respective physics properties of the sensors and associated data in the common embedding space. Adding physics properties into the embedding space during training in accordance with embodiments of the present principles improves the capabilities of the embedding space, for example, (1) reducing training data, (2) providing better explainability and prediction (detection/classification) of a new object during inference based on its physics properties.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> depicts a high-level functional diagram of the functionality of the optional generator module <b>140</b> and the optional discriminator module <b>150</b> of a sensor data fusion system, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in accordance with an embodiment of the present principles. In the embodiment of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, during the training of the common embedding space <b>210</b> using, illustratively, the data from two different sensors (sensor modalities), Sensor <b>1</b>, <b>202</b><sub>1</sub>, and Sensor <b>2</b>, <b>202</b><sub>2</sub>, capturing images of a common scene, a generative adversarial (GAN) network <b>360</b> comprising the generator module <b>140</b> and the discriminator module <b>150</b>, can be implemented to learn the sensor data characteristics and sensor response of at least one of the sensors when capturing data, illustratively in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, Sensor <b>2</b>. For example, in some embodiments the GAN network <b>360</b> can learn a difference between the data of the Sensor <b>1</b> when capturing a scene and the data of the Sensor <b>2</b> when capturing the same scene.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> depicts a high-level functional diagram of a simulation of the data capture of a learned sensor, such as Sensor <b>2</b>, <b>202</b><sub>2</sub>, of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, in a sensor data fusion system, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in accordance with an embodiment of the present principles. As depicted in the embodiment of <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, during an application/implementation in which a learned sensor, for example Sensor <b>2</b>, <b>202</b><sub>2</sub>, of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, is not available to capture data of a scene, the data capture of the learned sensor, such as Sensor <b>2</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, can be emulated by a GAN network of the present principles, such as the GAN network <b>360</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>. That is, as described above with respect to <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, during training, the GAN network <b>360</b> determines a difference between the data of a scene captured by the Sensor <b>1</b>, <b>202</b><sub>1</sub>, and the data of the scene captured by the Sensor <b>2</b>, <b>202</b><sub>2</sub>. As such, in the embodiment of <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, the data of the scene captured by Sensor <b>1</b>, <b>202</b><sub>1</sub>, can be used by the GAN network <b>360</b> to emulate data of the scene as would have been captured by the Sensor <b>2</b>, <b>202</b><sub>2</sub>. That is, in the embodiment of <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, the GAN network <b>360</b> uses the determined difference between the data captured of a scene by Sensor <b>1</b>, <b>202</b><sub>1</sub>, and Sensor <b>2</b>, <b>202</b><sub>2</sub>, during training to emulate data as would have been captured by Sensor <b>2</b>, <b>202</b><sub>2</sub>, of a same scene as captured by Sensor <b>1</b>, <b>2021</b>. As depicted in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, the data captured by the Sensor <b>1</b>, <b>202</b><sub>1</sub>, and the emulated data of Sensor <b>2</b>, <b>202</b><sub>2</sub>, can be combined in the common embedding space <b>210</b> as previously described above.</p><p id="p-0052" num="0051">As described above, embodiments of the present principles provide an end-to-end pipeline system that, during training, fuses multi-sensor (i.e., multi-modal) data in a common embedding space by directly optimizing target metrics and learning the contribution of each sensor (mode) to the results. As such, even during applications/implementations in which a learned sensor is not available to capture data, the data capture of the learned sensor can be emulated as described above. That is, embodiments of the present principles enable the use of a single sensor model to emulate data captured by a plurality of different sensors.</p><p id="p-0053" num="0052">In some embodiments of a sensor data fusion system of the present principles, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the generator <b>140</b> and the discriminator <b>150</b> of the GAN network <b>360</b> comprise an adversarial relationship. More specifically, in some embodiments emulated data of learned sensors created by the generator <b>140</b> are reviewed by the discriminator <b>150</b>, which determines if the generator <b>140</b> has created data that adequately reflects data from the learned sensor. The back and forth between the creation of the data by the generator <b>140</b> and the review of the data by the discriminator <b>150</b> continues until the created data is acceptable to the discriminator <b>150</b> as data that would be produced by a learned sensor. Although embodiments of the present principles are described as implementing a GAN network to learn a model of a sensor for future use when the learned sensor is not available, alternatively or in addition, in some embodiments of the present principles, an encoder/decoder pair, such as a variational autoencoder (not shown), can be implemented to learn a model of a sensor for future use when the learned sensor is not available as described above with respect to the GAN network <b>360</b>. That is, in some embodiments, a Variational Autoencoder can merge high-level representations of several heterogeneous sensors (data sources) into a single latent representation by learning to reconstruct the input data from the common homogeneous representation and, as such learn respective models of the contributing sensors. As such, in later applications when a sensor may not be available, the sensor model can be used to emulate a contribution of the unavailable sensor to data captured by available sensors.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> depicts a high-level functional diagram of the operation of the optional generator module <b>140</b> and the optional discriminator module <b>150</b> of a sensor data fusion system, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in which different data modalities are combined in accordance with an alternate embodiment of the present principles. In the embodiment of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, during the training of the common embedding space <b>210</b> using, illustratively, data models of two different modalities, Data Model <b>1</b>, <b>402</b><sub>1</sub>, and Data Model <b>2</b>, <b>402</b><sub>2</sub>, the generator module <b>140</b> and the discriminator module <b>150</b>, can be implemented to learn and simulate the characteristics of at least one data model, illustratively in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, Data Model <b>2</b>, <b>402</b><sub>2</sub>. For example, in some embodiments such as the embodiment of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the generator module <b>140</b> and the discriminator <b>150</b> can determine differences between the different data modalities, Data Model <b>1</b>, <b>402</b><sub>1</sub>, and Data Model <b>2</b>, <b>402</b><sub>2</sub>.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> depicts a functional diagram of a simulation of the characteristics of a data modality, such as Data Model <b>2</b>, <b>402</b><sub>2</sub>, of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, in a sensor data fusion system of the present principles, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in accordance with an embodiment of the present principles. As depicted in the embodiment of <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, during an application/implementation in which a learned data modality, for example Data Model <b>2</b>, <b>402</b><sub>2</sub>, of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, is not available, the characteristics of the learned data modality, such as Data Model <b>2</b>, <b>402</b><sub>2</sub>, of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, can be simulated by the generator <b>140</b> and the discriminator <b>150</b>. That is, as described above with respect to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, during training, the generator <b>140</b> and the discriminator <b>150</b> can determine differences between the data modalities, Data Model <b>1</b>, <b>402</b><sub>1</sub>, and Data Model <b>2</b>, <b>402</b><sub>2</sub>. As such, in the embodiment of <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the available data modality, Data Model <b>1</b>, <b>402</b><sub>1</sub>, can be used by the generator <b>140</b> and the discriminator <b>150</b> to simulate the second data modality, Data Model <b>2</b>, <b>402</b><sub>2</sub>. That is, in the embodiment of <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the generator <b>140</b> and the discriminator <b>150</b> use the determined difference between the data modalities, Data Model <b>1</b>, <b>402</b><sub>1</sub>, and Data Model <b>2</b>, <b>402</b><sub>2</sub>, determined during training, to simulate the second data modality, Data Model <b>2</b>, <b>402</b><sub>2</sub>. As depicted in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the Data Model <b>1</b>, <b>402</b><sub>1</sub>, and the simulated Data Model <b>2</b>, <b>402</b><sub>2</sub>, can be projected/embedded into the common embedding space <b>210</b> as previously described above.</p><p id="p-0056" num="0055">As described above, in some embodiments of a sensor data fusion system of the present principles, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the generator <b>140</b> and the discriminator <b>150</b> can comprise an adversarial relationship. More specifically, in some embodiments simulated Data Models/modalities created by the generator <b>140</b> are reviewed by the discriminator <b>150</b>, which determines if the generator <b>140</b> has created data that adequately simulates the Data Model. The back and forth between the creation of the data by the generator <b>140</b> and the review of the data by the discriminator <b>150</b> continues until the created data is acceptable to the discriminator <b>150</b> as data that simulates a missing Data Model.</p><p id="p-0057" num="0056">In one application/implementation, a sensor data fusion system of the present principles, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, can be implemented for 3D target/object detection. For example, <figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a high-level functional diagram of a sensor data fusion system of the present principles, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, that can be implemented for 3D target/object detection in accordance with an embodiment of the present principles. In the embodiment of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, point cloud data of a scene captured, illustratively, by a Lidar sensor <b>502</b>, can be projected/embedded into a common embedding space <b>610</b>. That is, in some embodiments, image features of the point cloud data can be extracted using a model-specific CNN using a respective feature extraction module <b>510</b><sub>1</sub>. The extracted image features of the point cloud data are communicated to an embedding module <b>520</b><sub>1</sub>. As described above, at the embedding module <b>520</b><sub>1</sub>, the extracted image features of the point cloud data are projected/embedded into the common embedding space <b>610</b>.</p><p id="p-0058" num="0057">In the embodiment of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, image data of the scene captured, illustratively, by an RGB sensor <b>504</b>, can be projected/embedded into the common embedding space <b>610</b>. That is, in some embodiments, image features of the image data <b>504</b> can be extracted using a model-specific CNN using a respective feature extraction module <b>510</b><sub>2</sub>. The extracted image features of the image data are communicated to the embedding module <b>520</b><sub>2</sub>. As described above, at the embedding module <b>520</b><sub>2</sub>, the extracted image features of the image data are projected/embedded into the common embedding space <b>610</b>.</p><p id="p-0059" num="0058">As described above and depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, respective vector representations of the projected/embedded point cloud data of the Lidar sensor and the vector representations of the projected/embedded image data of the RGB sensor are combined (e.g., fused) into a joint representation illustratively by the fusion module <b>530</b>. From the joint representations, 3D targets/objects can be determined. For example in the embodiment of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a 3D box predictor <b>550</b> can used to determine 3D targets/objects in a captured scene. In some embodiments, the 3D box predictor <b>550</b> predicts a size of the object, a center position and an orientation of the object and specifically recovers boxes with 3D center position (x, y, z), and 3D size (w, h, l) also, recovering rotation around axis (yaw, pitch, roll). In some embodiments of the present principles, however, such as in autonomous driving datasets, roll and pitch can be assumed to be zero for simplicity.</p><p id="p-0060" num="0059">Alternatively or in addition, in some embodiments the 3D box predictor <b>550</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> can determine 2D bounding boxes from the 2D image data captured by, for example an RGB sensor and using the point cloud data captured by, for example a Lidar sensor, can determine 3D bounding boxes for targets/objects of captured scenes. In such embodiments, powerful neural network based 2D image-based object detectors can be used to provide 2D bounding boxes, which can help reduce search space for 3D bounding box detection. In such embodiments, the common embedding space <b>610</b> can be trained using both the image data from, for example an RGB sensor, and the point cloud data from, for example a Lidar sensor, such that if image data is not available during application/implementation time, the image data can be emulated as described above with respect to at least <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> and <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a high-level functional block diagram of a sensor data fusion system of the present principles, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, that can be implemented for underwater target/object sensing applications in accordance with an embodiment of the present principles. For example, in the sensor data fusion system of <figref idref="DRAWINGS">FIG. <b>6</b></figref> data from different sensor modalities, sources, and different levels of fidelities: acoustic sensors (multi-static sonars), radar sensors, cameras, LiDAR sensors, above water sensors, active sonars, and any future sensors can be used to generate a combined signal for underwater target/object sensing applications. Specifically, in the illustrated embodiment of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, sensor data <b>602</b> from a Radar sensor is received and features of the Radar sensor data <b>602</b> are extracted using, for example, a respective feature extraction module <b>6101</b> of the present principles which can include a sensor-specific CNN pretrained to extract features from Radar sensor data. The extracted features of the Radar sensor data are projected/embedded into a common embedding space <b>710</b> by, for example, a respective embedding module <b>6201</b> of the present principles. As described above, the extracted features of the Radar sensor data projected/embedded into the common embedding space <b>710</b> can be used to respectively assist in identifying an object in sensor data of a common scene and/or to train the common embedding space <b>710</b>.</p><p id="p-0062" num="0061">Similarly and as depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, sensor data <b>604</b> from an Acoustic sensor is received and features of the Acoustic sensor data <b>604</b> are extracted using, for example, a feature extraction module <b>6102</b> of the present principles which can include a sensor-specific CNN pretrained to extract features from Acoustic sensor data. The extracted features of the Acoustic sensor data are projected/embedded into the common embedding space <b>710</b> by, for example an embedding module <b>6202</b> of the present principles. As described above, the extracted features of the Acoustic sensor data projected/embedded into the common embedding space <b>710</b> can be used to respectively assist in identifying an object in sensor data of a common scene and/or to train the common embedding space <b>710</b>.</p><p id="p-0063" num="0062">As further depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, sensor data <b>606</b> from a Sonar sensor is received and features of the Sonar sensor data <b>606</b> are extracted using, for example, a feature extraction module of the present principles <b>6103</b> which can include a sensor-specific CNN pretrained to extract features from Sonar sensor data. The extracted features of the Sonar sensor data are projected/embedded into the common embedding space <b>710</b> by, for example an embedding module <b>6203</b> of the present principles. As described above, the extracted features of the Sonar sensor data projected/embedded into the common embedding space <b>710</b> can be used to respectively assist in identifying an object in sensor data of a common scene and/or to train the common embedding space <b>710</b>.</p><p id="p-0064" num="0063">In the underwater target/object sensing sensor data fusion system of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, sensor data <b>608</b> from a Lidar sensor is received and features of the Lidar sensor data <b>608</b> are extracted using, for example, a feature extraction module <b>6104</b> of the present principles which can include a sensor-specific CNN pretrained to extract features from Lidar sensor data. The extracted features of the Lidar sensor data are projected/embedded into the common embedding space <b>710</b> by, for example an embedding module <b>6204</b> of the present principles. As described above, the extracted features of the Lidar sensor data projected/embedded into the common embedding space <b>710</b> can be used to respectively assist in identifying an object in sensor data of a common scene and/or to train the common embedding space <b>710</b>.</p><p id="p-0065" num="0064">The projected/embedded features of each of the different modality sensors (e.g., the Radar sensor, the Acoustic sensor, the Sonar sensor, and the Lidar sensor) can be combined in the common embedding space <b>710</b> by, for example a fusion module <b>630</b> of the present principles.</p><p id="p-0066" num="0065">In the embodiments of the present principles, such as the embodiment of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the combined projected sensor data can be used for detection of targets/objects in an underwater application in accordance with the present principles. For example, and as described above, in some embodiments, an inference module <b>635</b> of the present principles, such as the inference module <b>135</b> of the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, is configured to determine a similarity between the combined, projected vector representations of the image data of the sensors having different modalities and respective embedded vector representations of features of image data embedded in the common embedding space using a distance function to identify at least one object depicted by the image data of the sensors having the different modalities.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a flow diagram of a method for training a common embedding space for combining sensor data captured of a common scene having at least two modalities in accordance with an embodiment of the present principles. The method <b>700</b> begins at <b>702</b> during which, for each of a plurality of the captured sensor data having a first modality of the at least two modalities, respective first modality sensor-data vector representations are created of the features of the sensor data having the first modality using a sensor data-specific neural network. The method <b>700</b> can proceed to <b>704</b>.</p><p id="p-0068" num="0067">At <b>704</b>, for each of a plurality of the captured sensor data having a second modality of the at least two modalities, respective second modality sensor-data vector representations are created of the features of the sensor data having the second modality using a sensor data-specific neural network. The method <b>700</b> can proceed to <b>706</b>.</p><p id="p-0069" num="0068">At <b>706</b>, the first modality vector representations and the second modality vector representations are embedded in a common embedding space such that embedded modality vectors that are related, across modalities, are closer together in the common embedding space than unrelated modality vectors. The method <b>700</b> can proceed to <b>708</b>.</p><p id="p-0070" num="0069">At <b>708</b>, the embedded first modality vector representations and the second modality vector representations are respectively combined. In some embodiments of the present principles, the first modality vector representations and the second modality vector representations are respectively combined into respective, combined vector representations. The method <b>700</b> can be exited.</p><p id="p-0071" num="0070">In some embodiments of a method in accordance with the present principles, such as the method <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, at least one of the creating of the first and second modality vector representations and the embedding of the first and second modality vector representations are constrained/guided by physics properties of at least one of a respective sensor having captured the first modality sensor data and the second modality sensor data, and physics properties of the first modality sensor data and the second modality sensor data.</p><p id="p-0072" num="0071">In some embodiments of a method in accordance with the present principles, such as the method <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a difference between sensor data having a first modality of the at least two modalities and at least sensor data having a second modality of the at least two modalities is determined such that, in later applications, if sensor data of the first modality or the second modality is not available, the missing sensor data can be emulated from the sensor data of the first modality or the second modality that is available.</p><p id="p-0073" num="0072">As described above, once the common embedding space is created as described above, the created embedding space can be implemented for different purposes, including but not limited to, target/object detection, target/object recognition, structure classification and the like. For example, <figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts a flow diagram of a method for object detection in sensor data captured using sensors having at least two modalities using a common embedding space trained in accordance with an embodiment of the present principles. The method <b>800</b> begins at <b>802</b> during which, sensor-data vector representations are created of features of sensor data having a first modality using a sensor data-specific neural network. The method <b>800</b> can proceed to <b>804</b>.</p><p id="p-0074" num="0073">At <b>804</b>, sensor-data vector representations are created of features of sensor data having a second modality using a sensor data-specific neural network. The method <b>800</b> can proceed to <b>806</b>.</p><p id="p-0075" num="0074">At <b>806</b>, the sensor-data vector representations of the sensor data having the first modality and the second modality are projected into a common embedding space such that embedded modality vectors that are related, across modalities, are closer together in the common embedding space than unrelated modality vectors. The method <b>800</b> can proceed to <b>808</b>.</p><p id="p-0076" num="0075">At <b>808</b>, the projected first modality vector representations and the second modality vector representations are respectively combined. The method <b>800</b> can proceed to <b>810</b>.</p><p id="p-0077" num="0076">At <b>810</b>, a similarity is determined between the combined modality vector representations and respective embedded vector representations of features of objects in the common embedding space using a distance function to identify at least one object depicted by the sensor data having the first modality and the second modality. The method <b>800</b> can be exited.</p><p id="p-0078" num="0077">In some embodiments of a method in accordance with the present principles, such as the method <b>800</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, at least one of the creating of the first and second modality vector representations and the embedding of the first and second modality vector representations are constrained/guided by physics properties of at least one of a respective sensor having captured the first modality sensor data and the second modality sensor data, and physics properties of the first modality sensor data and the second modality sensor data.</p><p id="p-0079" num="0078">In some embodiments of a method in accordance with the present principles, such as the method <b>800</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, in instances in which one of the sensor data of the first modality or the second modality is not available, a previously learned difference between the sensor data of the first modality and the second modality (as previously described) can be used along with the available sensor modality data to emulate the missing data before projecting the sensor data having the first modality and the second modality into the common embedding space.</p><p id="p-0080" num="0079">Embodiments of the present principles use a common embedding space, trained as described above, to capture and learn as much information, such as 3D information, of a scene to, for example, enable a navigation of an individual or vehicle through an area of the scene. For example, embodiments of the present principles can be used to enable navigation of an autonomous vehicle, such as an autonomous car, an aerial vehicle, and/or an underwater vehicle, through an area. Embodiments of a trained embedding space of the present principles can be implemented as an object detector, object classifier, and/or target detector as described above.</p><p id="p-0081" num="0080">As depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, embodiments of a sensor data fusion system in accordance with the present principles, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, can be implemented in a computing device <b>900</b>. <figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts a high-level block diagram of a computing device <b>900</b> suitable for use with embodiments of a sensor data fusion system in accordance with the present principles such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In some embodiments, the computing device <b>900</b> can be configured to implement methods of the present as processor-executable executable program instructions <b>922</b> (e.g., program instructions executable by processor(s) <b>910</b>) in various embodiments.</p><p id="p-0082" num="0081">In the embodiment of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the computing device <b>900</b> includes one or more processors <b>910</b><i>a</i>-<b>910</b><i>n </i>coupled to a system memory <b>920</b> via an input/output (I/O) interface <b>930</b>. The computing device <b>900</b> further includes a network interface <b>940</b> coupled to I/O interface <b>930</b>, and one or more input/output devices <b>950</b>, such as cursor control device <b>960</b>, keyboard <b>970</b>, and display(s) <b>980</b>. In various embodiments, a user interface can be generated and displayed on display <b>980</b>. In some cases, it is contemplated that embodiments can be implemented using a single instance of computing device <b>900</b>, while in other embodiments multiple such systems, or multiple nodes making up the computing device <b>900</b>, can be configured to host different portions or instances of various embodiments. For example, in one embodiment some elements can be implemented via one or more nodes of the computing device <b>900</b> that are distinct from those nodes implementing other elements. In another example, multiple nodes may implement the computing device <b>900</b> in a distributed manner.</p><p id="p-0083" num="0082">In different embodiments, the computing device <b>900</b> can be any of various types of devices, including, but not limited to, a personal computer system, desktop computer, laptop, notebook, tablet or netbook computer, mainframe computer system, handheld computer, workstation, network computer, a camera, a set top box, a mobile device, a consumer device, video game console, handheld video game device, application server, storage device, a peripheral device such as a switch, modem, router, or in general any type of computing or electronic device.</p><p id="p-0084" num="0083">In various embodiments, the computing device <b>900</b> can be a uniprocessor system including one processor <b>910</b>, or a multiprocessor system including several processors <b>910</b> (e.g., two, four, eight, or another suitable number). Processors <b>910</b> can be any suitable processor capable of executing instructions. For example, in various embodiments processors <b>910</b> may be general-purpose or embedded processors implementing any of a variety of instruction set architectures (ISAs). In multiprocessor systems, each of processors <b>910</b> may commonly, but not necessarily, implement the same ISA.</p><p id="p-0085" num="0084">System memory <b>920</b> can be configured to store program instructions <b>922</b> and/or data <b>932</b> accessible by processor <b>910</b>. In various embodiments, system memory <b>920</b> can be implemented using any suitable memory technology, such as static random-access memory (SRAM), synchronous dynamic RAM (SDRAM), nonvolatile/Flash-type memory, or any other type of memory. In the illustrated embodiment, program instructions and data implementing any of the elements of the embodiments described above can be stored within system memory <b>920</b>. In other embodiments, program instructions and/or data can be received, sent or stored upon different types of computer-accessible media or on similar media separate from system memory <b>920</b> or computing device <b>900</b>.</p><p id="p-0086" num="0085">In one embodiment, I/O interface <b>930</b> can be configured to coordinate I/O traffic between processor <b>910</b>, system memory <b>920</b>, and any peripheral devices in the device, including network interface <b>940</b> or other peripheral interfaces, such as input/output devices <b>950</b>. In some embodiments, I/O interface <b>930</b> can perform any necessary protocol, timing or other data transformations to convert data signals from one component (e.g., system memory <b>920</b>) into a format suitable for use by another component (e.g., processor <b>910</b>). In some embodiments, I/O interface <b>930</b> can include support for devices attached through various types of peripheral buses, such as a variant of the Peripheral Component Interconnect (PCI) bus standard or the Universal Serial Bus (USB) standard, for example. In some embodiments, the function of I/O interface <b>930</b> can be split into two or more separate components, such as a north bridge and a south bridge, for example. Also, in some embodiments some or all of the functionality of I/O interface <b>930</b>, such as an interface to system memory <b>920</b>, can be incorporated directly into processor <b>910</b>.</p><p id="p-0087" num="0086">Network interface <b>940</b> can be configured to allow data to be exchanged between the computing device <b>900</b> and other devices attached to a network (e.g., network <b>990</b>), such as one or more external systems or between nodes of the computing device <b>900</b>. In various embodiments, network <b>990</b> can include one or more networks including but not limited to Local Area Networks (LANs) (e.g., an Ethernet or corporate network), Wide Area Networks (WANs) (e.g., the Internet), wireless data networks, some other electronic data network, or some combination thereof. In various embodiments, network interface <b>940</b> can support communication via wired or wireless general data networks, such as any suitable type of Ethernet network, for example; via digital fiber communications networks; via storage area networks such as Fiber Channel SANs, or via any other suitable type of network and/or protocol.</p><p id="p-0088" num="0087">Input/output devices <b>950</b> can, in some embodiments, include one or more display terminals, keyboards, keypads, touchpads, scanning devices, voice or optical recognition devices, or any other devices suitable for entering or accessing data by one or more computer systems. Multiple input/output devices <b>950</b> can be present in computer system or can be distributed on various nodes of the computing device <b>900</b>. In some embodiments, similar input/output devices can be separate from the computing device <b>900</b> and can interact with one or more nodes of the computing device <b>900</b> through a wired or wireless connection, such as over network interface <b>940</b>.</p><p id="p-0089" num="0088">Those skilled in the art will appreciate that the computing device <b>900</b> is merely illustrative and is not intended to limit the scope of embodiments. In particular, the computer system and devices can include any combination of hardware or software that can perform the indicated functions of various embodiments, including computers, network devices, Internet appliances, PDAs, wireless phones, pagers, and the like. The computing device <b>900</b> can also be connected to other devices that are not illustrated, or instead can operate as a stand-alone system. In addition, the functionality provided by the illustrated components can in some embodiments be combined in fewer components or distributed in additional components. Similarly, in some embodiments, the functionality of some of the illustrated components may not be provided and/or other additional functionality can be available.</p><p id="p-0090" num="0089">The computing device <b>900</b> can communicate with other computing devices based on various computer communication protocols such a Wi-Fi, Bluetooth.RTM. (and/or other standards for exchanging data over short distances includes protocols using short-wavelength radio transmissions), USB, Ethernet, cellular, an ultrasonic local area communication protocol, etc. The computing device <b>900</b> can further include a web browser.</p><p id="p-0091" num="0090">Although the computing device <b>900</b> is depicted as a general purpose computer, the computing device <b>900</b> is programmed to perform various specialized control functions and is configured to act as a specialized, specific computer in accordance with the present principles, and embodiments can be implemented in hardware, for example, as an application specified integrated circuit (ASIC). As such, the process steps described herein are intended to be broadly interpreted as being equivalently performed by software, hardware, or a combination thereof</p><p id="p-0092" num="0091">Those skilled in the art will also appreciate that, while various items are illustrated as being stored in memory or on storage while being used, these items or portions of them can be transferred between memory and other storage devices for purposes of memory management and data integrity. Alternatively, in other embodiments some or all of the software components can execute in memory on another device and communicate with the illustrated computer system via inter-computer communication. Some or all of the system components or data structures can also be stored (e.g., as instructions or structured data) on a computer-accessible medium or a portable article to be read by an appropriate drive, various examples of which are described above. In some embodiments, instructions stored on a computer-accessible medium separate from the computing device <b>900</b> can be transmitted to the computing device <b>900</b> via transmission media or signals such as electrical, electromagnetic, or digital signals, conveyed via a communication medium such as a network and/or a wireless link. Various embodiments can further include receiving, sending or storing instructions and/or data implemented in accordance with the foregoing description upon a computer-accessible medium or via a communication medium. In general, a computer-accessible medium can include a storage medium or memory medium such as magnetic or optical media, e.g., disk or DVD/CD-ROM, volatile or non-volatile media such as RAM (e.g., SDRAM, DDR, RDRAM, SRAM, and the like), ROM, and the like.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts a high-level block diagram of a network in which embodiments of a sensor data fusion system in accordance with the present principles, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, can be applied. The network environment <b>1000</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref> illustratively comprises a user domain <b>1002</b> including a user domain server/computing device <b>1004</b>. The network environment <b>1000</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref> further comprises computer networks <b>1006</b>, and a cloud environment <b>1010</b> including a cloud server/computing device <b>1012</b>.</p><p id="p-0094" num="0093">In the network environment <b>1000</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, a sensor data fusion system in accordance with the present principles, such as the sensor data fusion system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, can be included in at least one of the user domain server/computing device <b>1004</b>, the computer networks <b>1006</b>, and the cloud server/computing device <b>1012</b>. For example, in some embodiments, a user can use a local server/computing device (e.g., the user domain server/computing device <b>1004</b>) to provide sensor data fusion in accordance with the present principles. In some other embodiments, a user can implement a sensor data fusion system in the computer networks <b>1006</b> to provide sensor data fusion in accordance with the present principles. Alternatively or in addition, in some embodiments, a user can implement a sensor data fusion system in the cloud server/computing device <b>1012</b> of the cloud environment <b>1010</b> to provide sensor data fusion in accordance with the present principles. For example, in some embodiments it can be advantageous to perform processing functions of the present principles in the cloud environment <b>1010</b> to take advantage of the processing capabilities and storage capabilities of the cloud environment <b>1010</b>.</p><p id="p-0095" num="0094">In some embodiments in accordance with the present principles, a sensor data fusion system can be located in a single and/or multiple locations/servers/computers to perform all or portions of the herein described functionalities of a system in accordance with the present principles. For example, in some embodiments of the present principles, some of the components/modules of a sensor data fusion system, such as the feature extraction module <b>110</b>, the embedding module <b>120</b>, the fusion module <b>130</b>, the inference module <b>135</b>, the optional generator module <b>140</b> and the optional discriminator module <b>150</b>, can be located in one or more than one of the a user domain <b>1002</b>, the computer network environment <b>1006</b>, and the cloud environment <b>1010</b> and other ones of the components/modules of the sensor data fusion system can be located in at least other ones of the user domain <b>1002</b>, the computer network environment <b>1006</b>, and the cloud environment <b>1010</b> for providing the functions described above either locally or remotely.</p><p id="p-0096" num="0095">The methods and processes described herein may be implemented in software, hardware, or a combination thereof, in different embodiments. In addition, the order of methods can be changed, and various elements can be added, reordered, combined, omitted or otherwise modified. All examples described herein are presented in a non-limiting manner. Various modifications and changes can be made as would be obvious to a person skilled in the art having benefit of this disclosure. Realizations in accordance with embodiments have been described in the context of particular embodiments. These embodiments are meant to be illustrative and not limiting. Many variations, modifications, additions, and improvements are possible. Accordingly, plural instances can be provided for components described herein as a single instance. Boundaries between various components, operations and data stores are somewhat arbitrary, and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and can fall within the scope of claims that follow. Structures and functionality presented as discrete components in the example configurations can be implemented as a combined structure or component. These and other variations, modifications, additions, and improvements can fall within the scope of embodiments as defined in the claims that follow.</p><p id="p-0097" num="0096">In the foregoing description, numerous specific details, examples, and scenarios are set forth in order to provide a more thorough understanding of the present disclosure. It will be appreciated, however, that embodiments of the disclosure can be practiced without such specific details. Further, such examples and scenarios are provided for illustration, and are not intended to limit the disclosure in any way. Those of ordinary skill in the art, with the included descriptions, should be able to implement appropriate functionality without undue experimentation.</p><p id="p-0098" num="0097">References in the specification to &#x201c;an embodiment,&#x201d; etc., indicate that the embodiment described can include a particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it is believed to be within the knowledge of one skilled in the art to affect such feature, structure, or characteristic in connection with other embodiments whether or not explicitly indicated.</p><p id="p-0099" num="0098">Embodiments in accordance with the disclosure can be implemented in hardware, firmware, software, or any combination thereof. Embodiments can also be implemented as instructions stored using one or more machine-readable media, which may be read and executed by one or more processors. A machine-readable medium can include any mechanism for storing or transmitting information in a form readable by a machine (e.g., a computing device or a &#x201c;virtual machine&#x201d; running on one or more computing devices). For example, a machine-readable medium can include any suitable form of volatile or non-volatile memory.</p><p id="p-0100" num="0099">Modules, data structures, and the like defined herein are defined as such for ease of discussion and are not intended to imply that any specific implementation details are required. For example, any of the described modules and/or data structures can be combined or divided into sub-modules, sub-processes or other units of computer code or data as can be required by a particular design or implementation.</p><p id="p-0101" num="0100">In the drawings, specific arrangements or orderings of schematic elements can be shown for ease of description. However, the specific ordering or arrangement of such elements is not meant to imply that a particular order or sequence of processing, or separation of processes, is required in all embodiments. In general, schematic elements used to represent instruction blocks or modules can be implemented using any suitable form of machine-readable instruction, and each such instruction can be implemented using any suitable programming language, library, application-programming interface (API), and/or other software development tools or frameworks. Similarly, schematic elements used to represent data or information can be implemented using any suitable electronic arrangement or data structure. Further, some connections, relationships or associations between elements can be simplified or not shown in the drawings so as not to obscure the disclosure.</p><p id="p-0102" num="0101">This disclosure is to be considered as exemplary and not restrictive in character, and all changes and modifications that come within the guidelines of the disclosure are desired to be protected.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for training a common embedding space for combining sensor data captured from a common scene having at least two modalities, the method comprising:<claim-text>for each of a plurality of the captured sensor data having a first modality of the at least two modalities, creating respective first modality sensor-data vector representations of features of the sensor data having the first modality using a sensor data-specific neural network;</claim-text><claim-text>for each of a plurality of the captured sensor data having a second modality of the at least two modalities, creating respective second modality sensor-data vector representations of the features of the sensor data having the second modality using a sensor data-specific neural network;</claim-text><claim-text>embedding the first modality sensor-data vector representations and the second modality sensor-data vector representations in a common embedding space such that embedded modality vectors that are related, across modalities, are closer together in the common embedding space than unrelated modality vectors; and</claim-text><claim-text>respectively combining the embedded first modality sensor-data vector representations and the second modality vector representations;</claim-text><claim-text>wherein at least one of the creating of the first and second modality sensor-data vector representations and the embedding of the first and the second modality sensor-data vector representations are guided by physics properties of at least one of a respective sensor having captured the first modality sensor data and the second modality sensor data, and physics properties of the first modality sensor data and the second modality sensor data.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a sensor data-specific neural network is pretrained to recognize features of sensor data having a modality to which the sensor data-specific neural network is to be applied.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first modality sensor-data vector representations and the second modality sensor-data vector representations are combined using late fusion.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining a difference between the plurality of the captured sensor data having the first modality and the captured sensor data having the second modality of the at least two modalities.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the determined difference between the captured sensor data having the first modality and the second modality is used to determine missing data of one of the first modality or the second modality from captured data of the other one of the second modality or the first modality.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the difference is determined using a generative adversarial network.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising:<claim-text>determining a contribution of each of the embedded first modality sensor-data vector representations and the second modality vector representations to the combination.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the physics properties comprise at least one of surface reflection, temperature, or humidity.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method for at least one of object detection, object classification, or object segmentation in sensor data having at least two modalities using a common embedding space, comprising:<claim-text>creating respective first modality sensor-data vector representations of features of sensor data having a first modality of the at least two modalities;</claim-text><claim-text>creating respective second modality sensor-data vector representations of features of sensor data having a second modality of the at least two modalities;</claim-text><claim-text>projecting the first modality sensor-data vector representations and the second modality sensor-data vector representations into the common embedding space such that embedded modality vectors that are related, across modalities, are closer together in the common embedding space than unrelated modality vectors;</claim-text><claim-text>combining the projected first modality sensor-data vector representations and the second modality sensor-data vector representations;</claim-text><claim-text>determining a similarity between the combined modality sensor-data vector representations and respective embedded vector representations of features of objects in the common embedding space using a distance function, to identify at least one object depicted by the sensor data having the at least two modalities;</claim-text><claim-text>wherein at least one of the creating of the first and second modality sensor-data vector representations and the projecting of the first and the second modality sensor-data vector representations are guided by physics properties of at least one of a respective sensor having captured the first modality sensor data and the second modality sensor data, and physics properties of the first modality sensor data and the second modality sensor data.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, comprising:<claim-text>determining a difference between the plurality of the sensor data having the first modality and the sensor data having the second modality of the at least two modalities.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein at least one of the first modality sensor-data vector representations and the second modality sensor-data vector representations are created using the determined difference between the plurality of the sensor data having the first modality and the sensor data having the second modality.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein at least one of the first modality sensor-data vector representations and the second modality sensor-data vector representations are created using a sensor data-specific neural network.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein a contribution of each of the embedded first modality sensor-data vector representations and the second modality vector representations to the combination is predetermined.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method <b>13</b>, wherein the first modality sensor-data vector representations and the second modality sensor-data vector representations are combined using attention-based mode fusion.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. An apparatus for object detection in sensor data having at least two modalities using a common embedding space, comprising:<claim-text>at least one feature extraction module configured to create respective first modality sensor-data vector representations of features of sensor data having a first modality of the at least two modalities and respective second modality sensor-data vector representations of features of sensor data having a second modality of the at least two modalities;</claim-text><claim-text>at least one embedding module configured to project the first modality sensor-data vector representations and the second modality sensor-data vector representations into the common embedding space such that embedded modality vectors that are related, across modalities, are closer together in the common embedding space than unrelated modality vectors;</claim-text><claim-text>a fusion module configured to combine the projected first modality sensor-data vector representations and the second modality sensor-data vector representations;</claim-text><claim-text>an inference module configured to determine a similarity between the combined modality sensor-data vector representations and respective embedded vector representations of features of objects in the common embedding space using a distance function to identify at least one object depicted by the sensor data having the at least two modalities;</claim-text><claim-text>wherein at least one of the creating of the first and second modality sensor-data vector representations and the projecting of the first and the second modality sensor-data vector representations are guided by physics properties of at least one of a respective sensor having captured the first modality sensor data and the second modality sensor data, and physics properties of the first modality sensor data and the second modality sensor data.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising:<claim-text>a generative adversarial network configured to determine a difference between the plurality of the sensor data having the first modality and the sensor data having the second modality of the at least two modalities.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the generative adversarial network uses the determined difference between the sensor data having the first modality and the second modality to determine missing data of one of the first modality or the second modality from data of the other one of the second modality or the first modality.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the fusion module is configured to determine a contribution of each of the projected first modality sensor-data vector representations and the second modality sensor-data vector representations of the at least two modalities to the combination.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the fusion module is configured to apply attention-based mode fusion to combine the first modality sensor-data vector representations and the second modality sensor-data vector representations.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the physics properties comprise at least one of surface reflection, temperature, or humidity.</claim-text></claim></claims></us-patent-application>