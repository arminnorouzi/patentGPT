<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004744A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004744</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364078</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>86</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>50</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00798</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>4808</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>86</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>50</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2420</main-group><subgroup>52</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2420</main-group><subgroup>42</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">TECHNIQUES FOR IDENTIFYING CURBS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Zoox, Inc.</orgname><address><city>Foster City</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Pfeiffer</last-name><first-name>David</first-name><address><city>Foster City</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Zeng</first-name><address><city>Menlo Park</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Zhai</last-name><first-name>Qiang</first-name><address><city>Foster City</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><orgname>Zoox, Inc.</orgname><role>02</role></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques for identifying curbs are discussed herein. For instance, a vehicle may generate sensor data using one or more sensors, where the sensor data represents points associated with a driving surface and a sidewalk. The vehicle may then quantize the points into distance bins that are located laterally along the driving direction of the vehicle in order to generate spatial lines. Next, the vehicle may determine separation points for the spatial lines, where the separation points are configured to separate the points associated with the driving surface from the points associated with the sidewalk. The vehicle may then generate, using the separation points, a curve that represents the curb between the driving surface and the sidewalk. This way, the vehicle may use the curve while navigating, such as to avoid the curb and/or stop at a location that is proximate to the curb.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="234.95mm" wi="149.86mm" file="US20230004744A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="264.08mm" wi="170.86mm" file="US20230004744A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="242.40mm" wi="151.13mm" orientation="landscape" file="US20230004744A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="262.64mm" wi="170.94mm" file="US20230004744A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="257.56mm" wi="168.15mm" orientation="landscape" file="US20230004744A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="252.14mm" wi="166.12mm" orientation="landscape" file="US20230004744A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="257.47mm" wi="167.98mm" file="US20230004744A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="257.47mm" wi="167.39mm" file="US20230004744A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="257.47mm" wi="156.63mm" file="US20230004744A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">An autonomous vehicle may be configured to navigate from a first location to a second location along a path. For example, when providing a user with a ride, the autonomous vehicle may be configured to pick the user up at a pick-up location and navigate, along a path, to a destination location for dropping the user off. While navigating, it may be important for the autonomous vehicle to determine the locations of curbs. For instance, the autonomous vehicle may need to determine the locations of the curbs so that that autonomous vehicle avoids the curbs while navigating. Additionally, and for safety reasons, the autonomous vehicle may need to determine the locations of the curbs so that the autonomous vehicle is able to pick the user up and/or drop the user off at a location that is proximate to a curb.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0002" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0003" num="0002">The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identify the figure in which the reference number first appears. The same reference numbers in different figures indicate similar or identical items.</p><p id="p-0004" num="0003"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a pictorial flow diagram of an example process for identifying curbs within an environment.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example of generating spatial lines using points represented by sensor data.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of determining a separation point associated with a spatial line.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of generating, using separation points, a curve that represents a curb.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a block diagram of an example system for implementing the techniques described herein.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a flow diagram of an example process for generating a curve that represents a curb.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a flow diagram of an example process for identifying at least a reliable point for use in identifying a location of a curb.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a flow diagram of an example process for determining a separation point associated with a spatial line.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0003" level="1">DETAILED DESCRIPTION</heading><p id="p-0012" num="0011">As discussed above, an autonomous vehicle may be configured to navigate from a first location to a second location along a path. For instance, when providing a user with a ride, the autonomous vehicle may be configured to pick the user up at a pick-up location and navigate, along a path, to a destination location for dropping the user off. While navigating, it may be important for the autonomous vehicle to determine the locations of curbs. In examples, the autonomous vehicle may need to determine the locations of the cubs so that the autonomous vehicle avoids the curbs while navigating. The autonomous vehicle may need to determine the locations of the curbs so that the autonomous vehicle is able to pick a user up and/or drop a user off at a location that is proximate to a curb.</p><p id="p-0013" num="0012">As such, the present application relates to techniques for identifying curbs. For instance, a vehicle may generate sensor data using one or more sensors, such as one or more lidar, image, radar, or other sensors. In some examples, the sensor data may represent locations of points within an environment (e.g., x-coordinates, y-coordinates, and z-coordinates associated with points) and probabilities associated with the points. The vehicle may then analyze the sensor data in order to identify points that are associated with driving surface(s) for the vehicle and points that are associated with sidewalk(s). Based on identifying the points, the vehicle may use the points to generate spatial lines that represent the environment. The vehicle may then analyze the spatial lines in order to identify, for each spatial line, a respective separation point that separates the points associated with the driving surface(s) from the points associated with the sidewalk(s). Using the separation points for the spatial lines, the vehicle may then generate a curve that represents a curb between the driving surface(s) and the sidewalk(s). The vehicle may then use the curve to navigate in order avoid the curb and/or in order to determine a location for stopping that is proximate to the curb.</p><p id="p-0014" num="0013">For more detail, the vehicle may include one or more sensors that the vehicle can use to generate sensor data. The one or more sensor may include, but are not limited to, light detection and ranging (lidar) sensor(s), radar sensor(s), ultrasound, time-of-flight sensor(s), imaging device(s) (e.g., camera(s)), and/or any other type of sensor. In some examples, the sensor data may represent at least locations of points (which may be referred to, in examples, as &#x201c;first points) within an environment for which the vehicle is located and probabilities associated with accuracies of the locations of the first points. In some examples, the sensor data may be input to a model to determine classifications for objects associated with the first points. For example, and for a first point, the vehicle may (1) generate the sensor data that represents at least the coordinate location (e.g., the x-coordinate, the y-coordinate, and the z-coordinate) of the of the first point with respect to the vehicle and a probability or confidence value that the location of the first point is accurate and (2) a classification of an object associated with the first point. As described herein, the classification may include, but is not limited to, a driving surface, a sidewalk, a person, another vehicle, a building, a street sign, and/or any other type of object that may be located within the environment for which the vehicle is navigating.</p><p id="p-0015" num="0014">In some examples, the vehicle may generate the sensor data representing the first points over a period of time. The period of time may include, but is not limited to, one millisecond, fifty milliseconds, one second, five seconds, and/or any other period of time. By generating the sensor data over the period of time, the vehicle may capture a greater number of the first points, which may better represent the environment surrounding the vehicle.</p><p id="p-0016" num="0015">The vehicle may then analyze the sensor data in order to identify points (referred to, in examples, as &#x201c;second points&#x201d;) for identifying curbs within the environment. In some examples, the vehicle may analyze the sensor data in order to identify second points that are associated with specific classifications. For example, the vehicle may analyze the sensor data in order to identify second points that are associated with classifications such as driving surfaces and sidewalks, but discard the first points that are associated with other classifications. This is because, as described in more detail below, the vehicle can use points associated with driving surfaces and sidewalks to identify curbs. Additionally, or alternatively, in some examples, the vehicle may analyze the sensor data in order to identify second points that are associated with probabilities that satisfy (e.g., that are equal to or greater than) a threshold probability. For example, if the threshold probability is 99%, then the vehicle may analyze the sensor data in order to identify second points that are associated with probabilities that are equal to or greater than 99%, but discard the first points that are associated with probabilities that are less than 99%. This is because the vehicle may only use points that for which the vehicle has a high confidence of accuracy in order to identify curbs.</p><p id="p-0017" num="0016">Additionally, or alternatively, in some examples, the vehicle may analyze the sensor data in order to identify second points that are located within a threshold distance to the vehicle. The threshold distance may include, but is not limited to, one meter, five meters, ten meters, and/or any other distance. For example, such as when the threshold distance includes ten meters, the vehicle may analyze the sensor data in order to identify second points that are associated with locations that are within ten meters from the vehicle, but discard the first points that are associated with locations that are farther than ten meters from the vehicle. This is because the vehicle may use locations of curbs that are proximate to the vehicle while navigating.</p><p id="p-0018" num="0017">The vehicle may then use the second points in order to generate spatial lines. In some examples, to generate the spatial lines, the vehicle may quantize the second points into distance bins that are located laterally along the driving direction of the vehicle (e.g., located along the x-direction with respect to the vehicle). For example, each of the distance bins may include a specific lateral distance along the driving direction. As described herein, the lateral distance may include, but is not limited to, ten centimeters, twenty centimeters, fifty centimeters, one hundred centimeters, and/or any other distance. The vehicle may then quantize the second points into the distance bins such that all of the second points that fall within a respective distance bin are included in that distance bin. These distance bins may then represent the spatial lines that the vehicle uses for generating the curve that represents a curb.</p><p id="p-0019" num="0018">For instance, and for each spatial line, the vehicle may identify a separation point that separates the second points that are associated with the driving surface(s) from the second points that are associated with the sidewalk(s). This is because, if the environment does include a curb, it will likely be located between the driving surface(s) and the sidewalk(s). In some examples, to identify the separation point for a spatial line, the vehicle may analyze the classifications in order to identify a point along the spatial line (e.g., in the y-direction) that is between the second points associated with the driving surface(s) and the second points associated with the sidewalk(s). The vehicle may then determine that the point that is between the second points associated with the driving surface(s) and the second points associated with the sidewalk(s) includes the separation point for the spatial line.</p><p id="p-0020" num="0019">Additionally, or alternatively, in some examples, to identify the separation point for a spatial line, the vehicle may analyze the sensor data, which again represents the locations (e.g., z-coordinates) associated with the second points, to identify a point along the spatial line (e.g., in the y-direction) at which the locations of the second points rise from a first height (e.g., in the z-direction) to a second height (e.g., in the z-direction). In such examples, the vehicle may use a threshold distance when identifying the rise, such as five centimeters, ten centimeters, twenty centimeters, and/or any other distance. The vehicle may then determine that the point at which the rise occurs includes the separation point for the spatial line.</p><p id="p-0021" num="0020">Additionally, or alternatively, in some examples, to identify the separation point for a spatial line, the vehicle may analyze various points along the spatial line, such as laterally in the y-direction. For each point along the spatial line, the vehicle may assert that all of the second points on one side of the point lay on a first surface line (e.g., a surface line associated with the driving surface(s)) and all points on the other side of the point lay on a second surface line (e.g., a surface line associated with the sidewalk(s)). The vehicle may then determine an energy associated with the point, where the energy is based on the differences between the locations of the second points (e.g., in the z-direction) and the locations of the surface lines. Using the energies of the points, the vehicle may then select the point that includes the maximum energy. This may be because the closer the second points are to the surface lines associated with a point, the higher the energy. However, the farther the second points are from the surface lines associated with a point, the lower the energy. As such, the point that is associated with the maximum energy may be represent the separation point between the driving surface(s) and the sidewalk(s) for the spatial line.</p><p id="p-0022" num="0021">The vehicle may then use the separation points associated with the spatial lines in order to generate the curve that represents the curb. For example, the vehicle may initially determine a curve using the separation points, where the curve includes lines (e.g., straight lines) connecting each of the separation points along the lateral direction of the vehicle. In some examples, the vehicle may then perform additional processing in order to better fit the curve to the curb. For example, the vehicle may then analyze the curve in order to identify at least one separation point that includes an &#x201c;outlier point&#x201d; from the rest of the separation points. The vehicle may then analyze the energies associated with additional separation points that are proximate to the outlier point. In some examples, the additional separation points may be proximate to the outlier point based on the additional separation points being within a threshold distance (e.g., 0.5 meters, 1 meter, 2 meters, etc.) to the outlier point. The vehicle may them use the energies to update the position of the outlier point, which is described in more detail below. Additionally, the vehicle may continue to perform these processes in order to more accurately generate the curve that represents curb.</p><p id="p-0023" num="0022">In some examples, the curve may represent a straight and/or substantially straight line that represents the curb. However, in other examples, the curve may not represent a straight and/or substantially straight line. For example, such as at an intersection, the curb may turn at a specific angle. For example, at a four-way stop, the curve may include a 90-degree turn. As such, these processes may still be performed in order to generate curves that represent curbs that are not straight and/or substantially straight. For example, such as when the curb includes a turn, the vehicle may still be able to identify &#x201c;outlier&#x201d; point(s) from the rest of the separation points, where the &#x201c;outlier&#x201d; points do not fit the turn of the curb. The vehicle may then perform similar processes, as those described herein, to update the positions of the &#x201c;outlier&#x201d; point(s) in order to create the curve that represents the turn.</p><p id="p-0024" num="0023">Additionally, in some examples, the vehicle may use one or more profiles in order to generate the curves that represent the curbs. For example, a first profile may be associated with a first type of environment, such as cities, a second profile may be associated with a second type of environment, such as rural areas, a third profile may be associated with a third type of environment, such as freeways, and/or so forth. Each profile may then indicate characteristic(s) of curbs that are likely to be located in the respective environment. For a first example, and using the example above, the first profile may indicate that curbs are likely to include sharp turns at intersections, such as 90-degrees. As such, when navigating through a city, the vehicle may then use the first profile to determine that a curve that includes a sharp turn is probable. For a second example, and again using the example above, the second profile may indicate that curbs are likely to include more circular turns. As such, when navigating through a rural area, the vehicle may use the second profile to determine that a curve that includes a circular turn is more probable than a curve that represents a sharp turn.</p><p id="p-0025" num="0024">Additionally, the vehicle may use the profiles to determine other characteristics associated with the curbs. For example, and again using the example above, the second profile may indicate that curbs are likely to include various heights depending on whether the curbs are associated with driveways, areas located between driveways, pedestrian walkways, and/or the like. As such, when navigating through a rural area, the vehicle may change the threshold distance that the vehicle uses to determine whether a curb exists.</p><p id="p-0026" num="0025">The vehicle may then perform one or more actions based on the curve that represents the curb. For a first example, such as when the vehicle is still navigating to a destination location, the vehicle may use the curve in order to avoid navigating over the curb and/or in order to avoid navigating within a threshold distance to the curb. For a second example, such as when the vehicle is approaching a destination location, the vehicle may use the curve in order to determine a location to stop that is proximate to the curb. While these are just a couple examples of how the vehicle may use the curve to navigate, in other examples, the vehicle may use the curve to perform additional and/or alternative actions.</p><p id="p-0027" num="0026">While the examples above describe generating a curve that represents a curb located on one side of the vehicle, in some examples, the vehicle may perform similar processes in order to generate more than one curve representing more than one curb. For example, the vehicle may perform the processes described above in order to generate a first curve that represents a first curb located to a first side of the vehicle (e.g., a right side of the vehicle) and a second curve that represents a second curb located to a second side of vehicle (e.g., a left side of the vehicle). Additionally, while the examples herein describe generating a curve that represents a curb located between a driving surface and a sidewalk, in other examples, the vehicle may perform similar processes in order to generate a curve that represents a curve between a driving surface and any type of non-driving surface. As described herein, a non-driving surface may include, but is not limited to, a sidewalk, a lawn, a bike lane, and/or any other surface for which the vehicle should not navigate.</p><p id="p-0028" num="0027">By performing the processes described herein, the vehicle may better determine the actual locations of curbs in real time. For example, determining locations of curbs may include using maps or other information indicating historic locations of curbs. However, by using maps, the locations of the curbs may change over a period of time, such as when construction is being performed in an area which may not be accounted for. As such, the maps that the vehicle uses may not be accurate and, as such, the vehicle may be unable to correctly determine the locations of the curbs. As such, by performing the processes described herein, the vehicle is able to accurately determine the locations of curbs without determining the locations of the vehicle and/or without using maps that may be inaccurate for one or more reasons.</p><p id="p-0029" num="0028">Additionally, and as described in more detail below, the vehicle may determine the locations of the curbs using two-dimensional processing techniques, such as when identifying the separation points. This may also provide improvements over techniques that use three-dimensional processing to determine the locations of curbs. For example, it may be more difficult for vehicles to identify curbs using the three-dimensional processing techniques since points are more spread out in the environment and/or the classifications of points may be uncertain. As such, by performing the two-dimensional processing techniques described herein, the vehicle is able to group the points into distance bins and then use the distance bins to identify the separation points for the curbs. As such, the vehicle may use a greater number of separation points to identify the curbs at the different distance bins, which may increase the accuracy when identifying the curb.</p><p id="p-0030" num="0029">The techniques described herein may be implemented in a number of ways. Example implementations are provided below with reference to the following figures. Although discussed in the context of an autonomous vehicle, the methods, apparatuses, and systems described herein may be applied to a variety of systems (e.g., a sensor system or a robotic platform), and are not limited to autonomous vehicles. In another example, the techniques may be utilized in an aviation or nautical context, or in any system evaluating distances between reference points in an environment (e.g., in a system using route-relative planning). Additionally, the techniques described herein may be used with real data (e.g., captured using sensor(s)), simulated data (e.g., generated by a simulator), or any combination of the two.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a pictorial flow diagram of an example process <b>100</b> for identifying curbs within an environment. At operation <b>102</b>, the process <b>100</b> may include generating sensor data using one or more sensor of a vehicle. For instance, an example <b>104</b> illustrates a vehicle <b>106</b> using the sensor(s) to generate the sensor data, which is represented by <b>108</b>. The one or more sensor may include, but are not limited to, lidar sensor(s), radar sensor(s), imaging device(s) (e.g., camera(s)), and/or any other type of sensor. In some examples, the sensor data may represent at least locations of points within the environment for which the vehicle <b>106</b> is located and probabilities associated with accuracies of the points. In some examples, the vehicle <b>106</b> may then determine classifications associated with the points, such as by inputting the sensor data into a model that outputs the classifications. Techniques for determining classifications may be found, for example, in U.S. patent application Ser. No. 15/409,841 titled &#x201c;Spatial and Temporal Information for Semantic Segmentation&#x201d; and filed Jan. 19, 2017 and U.S. patent application Ser. No. 17/246,258, titled &#x201c;Object Detecting and Tracking, Including Parallelized Lidar Semantic Segmentation&#x201d; and filed Apr. 30, 2021, which are incorporated by reference herein in their entirety. In the examples of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the classifications may include at least a driving surface <b>110</b> (where the sensor data is represented by the solid lines), a sidewalk <b>112</b> (where the sensor data is represented by the dashed lines), and a fire hydrant <b>114</b> (where the sensor data is represented by the dotted lines). As shown by the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a curb <b>116</b> is located between the driving surface <b>110</b> and the sidewalk <b>112</b>.</p><p id="p-0032" num="0031">In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the driving surface <b>110</b> may include a street that the vehicle <b>106</b> is navigating. However, in other examples, the driving surface <b>110</b> may include any other surface for which the vehicle <b>106</b> is able to navigate, such as a parking lot, a driveway, and/or the like. Additionally, in some examples, the vehicle <b>106</b> may generate the sensor data over a period of time. For example, the vehicle <b>106</b> may accumulate the sensor data over multiple lidar spins and/or frames, where the sensor data is adjusted based on the motion of the vehicle <b>106</b> over the period of time. As described herein, the period of time may include, but is not limited to, one millisecond, fifty milliseconds, one second, five seconds, and/or any other period of time. By generating the sensor data over the period of time, the vehicle <b>106</b> may capture a greater number of the points, which may better represent the environment surrounding the vehicle <b>106</b>. For example, the vehicle <b>106</b> may generate a greater number of the points that represent at least the driving surface <b>110</b> and the sidewalk <b>112</b>.</p><p id="p-0033" num="0032">At operation <b>118</b>, the process <b>100</b> may include generating spatial lines using points represented by the senor data. For instance, an example <b>120</b> illustrates the vehicle <b>106</b> generating spatial lines <b>122</b> using the points represented by the sensor data. In some examples, before generating the spatial lines <b>122</b>, the vehicle <b>106</b> may analyze the sensor data and/or the classifications in order to identify specific points that the vehicle <b>106</b> may use to identify the curb <b>116</b> within the environment. In examples, the vehicle <b>106</b> may analyze the classifications in order to identify points that are associated with specific classifications. For instance, the vehicle <b>106</b> may select points that are associated with classifications such as the driving surface <b>110</b> and the sidewalk <b>112</b>, but discard points that are associated with other classifications, such as the fire hydrant <b>114</b>. Additionally, or alternatively, the vehicle <b>106</b> may analyze the sensor data in order to identify points that are associated with probabilities that satisfy (e.g., that are equal to or greater than) a threshold probability. For instance, if the threshold probability is 99%, then the vehicle <b>106</b> may select points that are associated with probabilities that are equal to or greater than 99%, but discard points that are associated with probabilities that are less than 99%.</p><p id="p-0034" num="0033">Additionally, or alternatively, the vehicle <b>106</b> may analyze the sensor data in order to identify points that are located within a threshold distance to the vehicle <b>106</b>. As described herein, the threshold distance may include, but is not limited to, one meter, five meters, ten meters, and/or any other distance. For example, such as when the threshold distance includes ten meters, the vehicle <b>106</b> may analyze the sensor data in order to identify points that are associated with locations that are within ten meters to the vehicle <b>106</b>, but discard the first points that are associated with locations that are farther than ten meters from the vehicle <b>106</b>.</p><p id="p-0035" num="0034">As will be described in more detail with regard to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the vehicle <b>106</b> may then use the points in order to generate spatial lines <b>122</b>. In some examples, to generate the spatial lines <b>122</b>, the vehicle <b>106</b> may quantize the points into distance bins that are located laterally along the driving direction of the vehicle <b>106</b>, where the location of the vehicle <b>106</b> is represented by <b>124</b>. For example, each of the bins may include a specific lateral distance along the driving direction. As described herein, the lateral distance may include, but is not limited to, ten centimeters, twenty centimeters, fifty centimeters, one hundred centimeters, and/or any other distance. The vehicle <b>106</b> may then quantize the points into the distance bins such that all of the points that fall within a respective distance bin are included in that distance bin. These distance bins may then represent the spatial lines <b>122</b> that the vehicle <b>106</b> uses for generating the curve that represents the curb <b>116</b>.</p><p id="p-0036" num="0035">At operation <b>126</b>, the process <b>100</b> may include determining separation points associated with the spatial lines. For instance, an example <b>128</b> illustrates the vehicle <b>106</b> determining separation points <b>130</b> associated with the spatial lines <b>122</b>. For example, and as described in more detail with regard to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, for each spatial line <b>122</b>, the vehicle <b>106</b> may identify the separation point <b>130</b> that separates the points that are associated with the driving surface <b>110</b> from the points that are associated with the sidewalk <b>112</b>. This is because, if the environment does include the curb <b>116</b>, the curb <b>116</b> may be located between the driving surface <b>110</b> and the sidewalk <b>112</b> (which is represented by the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). In some examples, to identify the separation point <b>130</b>, the vehicle <b>106</b> may analyze various points along the spatial line <b>122</b>, such as laterally in the y-direction. For each point along the spatial line <b>122</b>, the vehicle <b>106</b> may assert that all of the points on one side of the point lay on a first surface line (e.g., a surface line associated with the driving surface <b>110</b>) and all of the points on the other side of the point lay on a second surface line (e.g., a surface line associated with the sidewalk <b>112</b>).</p><p id="p-0037" num="0036">As will be described in more detail with regard to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the vehicle <b>106</b> may then determine an energy associated with the point, where the energy is based on the difference between the locations of the points (e.g., in the z-direction) and the locations of the surface lines. Using the energies of the points, the vehicle <b>106</b> may select the point that includes the maximum energy. This may be because, the closer the points are to the surface lines associated with a point, the higher the energy. However, the farther the points are from the surface lines associated with a point, the lower the energy. As such, the point that is associated with the maximum energy may be represent the separation point <b>130</b> between the driving surface <b>110</b> and the sidewalk <b>112</b>. The vehicle <b>106</b> may then perform similar processes to determine the respective separation point <b>130</b> for each of the other spatial lines <b>122</b>.</p><p id="p-0038" num="0037">At operation <b>132</b>, the process <b>100</b> may include generating a curve that represents a curb using the separation points. For instance, an example <b>134</b> illustrates the vehicle <b>106</b> generating a curve <b>136</b> that represents the curb <b>116</b>. As will be descried in more detail with regard to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the vehicle <b>106</b> may initially generate the curve <b>136</b> to include straight lines connecting each of the spatial lines <b>122</b> along the lateral direction of the vehicle <b>106</b>. The vehicle <b>106</b> may then perform additional processing in order to better fit the curve <b>136</b> to the curb <b>116</b>. For example, the vehicle <b>106</b> may then analyze the curve <b>136</b> in order to identify at least one separation point <b>130</b> that includes an &#x201c;outlier point&#x201d; from the rest of the separation points <b>130</b>. The vehicle <b>106</b> may then analyze the energies associated with separation points <b>130</b> that are proximate to the identified separation point <b>130</b>. In some examples, the separation points <b>130</b> may be proximate to the identified separation point <b>130</b> based on the separation points <b>130</b> being within a threshold distance (e.g., 0.5 meters, 1 meter, 2 meters, etc.) to the identified separation point <b>130</b>. The vehicle <b>106</b> may the use the energies to update the position of at least the identified separation point <b>130</b>. The vehicle <b>106</b> may then continue to perform these processes in order to generate the curve <b>136</b>.</p><p id="p-0039" num="0038">As discussed above, the vehicle <b>106</b> may generate spatial lines <b>122</b> using points represented by sensor data. As such, <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example of generating the spatial lines <b>122</b> using points <b>202</b>(<b>1</b>)-(<b>5</b>) (also referred to as &#x201c;points <b>202</b>&#x201d;). In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the vehicle <b>106</b> may generate the spatial lines <b>122</b> by quantizing the points <b>202</b> into distance bins <b>204</b>(<b>1</b>)-(<b>5</b>) (also referred to as &#x201c;distance bins <b>204</b>&#x201d;) that are located laterally along the driving direction of the vehicle <b>106</b> (e.g., the x-direction). For example, each of the distance bins <b>204</b> may include a specific lateral distance along the driving direction. As described herein, the lateral distance may include, but is not limited to, ten centimeters, twenty centimeters, fifty centimeters, one hundred centimeters, and/or any other distance. The vehicle <b>106</b> may then quantize the points <b>202</b> into the distance bins <b>204</b> such that all of the points <b>202</b> that fall within a respective distance bin <b>204</b> are included in that distance bin <b>204</b>.</p><p id="p-0040" num="0039">For example, and as shown, all of the points <b>202</b>(<b>1</b>) fall with the distance bin <b>204</b>(<b>1</b>) and, as such, the vehicle <b>106</b> may include all of the points <b>202</b>(<b>1</b>) within the distance bin <b>204</b>(<b>1</b>). Additionally, all of the points <b>202</b>(<b>2</b>) fall with the distance bin <b>204</b>(<b>2</b>) and, as such, the vehicle <b>106</b> may include all of the points <b>202</b>(<b>2</b>) within the distance bin <b>204</b>(<b>2</b>). Furthermore, all of the points <b>202</b>(<b>3</b>) fall with the distance bin <b>204</b>(<b>3</b>) and, as such, the vehicle <b>106</b> may include all of the points <b>202</b>(<b>3</b>) within the distance bin <b>204</b>(<b>3</b>). Moreover, all of the points <b>202</b>(<b>4</b>) fall with the distance bin <b>204</b>(<b>4</b>) and, as such, the vehicle <b>106</b> may include all of the points <b>202</b>(<b>4</b>) within the distance bin <b>204</b>(<b>4</b>). Finally, all of the points <b>202</b>(<b>5</b>) fall with the distance bin <b>204</b>(<b>5</b>) and, as such, the vehicle <b>106</b> may include all of the points <b>202</b>(<b>5</b>) within the distance bin <b>204</b>(<b>5</b>).</p><p id="p-0041" num="0040">In some examples, the vehicle <b>106</b> determines that the points <b>202</b> fall within the distance bins <b>204</b> using the locations (e.g., the coordinates) associated with the points <b>202</b>. For example, the vehicle <b>106</b> may determine the lateral locations of the points <b>202</b> with respect to the vehicle <b>106</b>, such as by using the x-coordinates associated with the points <b>202</b>. Each distance bin <b>204</b> may then be associated with a respective lateral distance range. The vehicle <b>106</b> may thus determine that the points <b>202</b> fall within the distance bins <b>204</b> using the lateral locations of the points and the distance ranges. For example, if the distance bin <b>204</b>(<b>1</b>) is associated with a lateral distance range that includes between 100 centimeters and 200 centimeters in front of the vehicle <b>106</b>, then the vehicle <b>106</b> may determine that the points <b>202</b>(<b>1</b>) that include lateral locations between 100 centimeters and 200 centimeters in front of the vehicle <b>106</b> are included within the distance bin <b>204</b>(<b>1</b>). The vehicle may perform similar processes for each of the other distance bins <b>204</b>(<b>2</b>)-(<b>5</b>).</p><p id="p-0042" num="0041">The vehicle <b>106</b> may then generate a first spatial line <b>122</b> that includes the points <b>202</b>(<b>1</b>) within the distance bin <b>204</b>(<b>1</b>), a second spatial line <b>122</b> that includes the points <b>202</b>(<b>2</b>) within the distance bin <b>204</b>(<b>2</b>), a third spatial line <b>122</b> that includes the points <b>202</b>(<b>3</b>) within the distance bin <b>204</b>(<b>3</b>), a fourth spatial line <b>122</b> that includes the points <b>202</b>(<b>4</b>) within the distance bin <b>204</b>(<b>4</b>), and a fifth spatial line <b>122</b> that includes the points <b>202</b>(<b>5</b>) within the distance bin <b>204</b>(<b>5</b>).</p><p id="p-0043" num="0042">After generating the spatial lines <b>122</b>, and as also described above, the vehicle <b>106</b> may then determine the separation points <b>130</b> associated with the spatial lines <b>122</b>. As such, <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of determining a separation point <b>130</b> associated with a spatial line <b>122</b>. As shown by the top illustration, the vehicle <b>106</b> may determine that points <b>302</b>(<b>1</b>)-(<b>10</b>) (also referred to as &#x201c;points <b>302</b>&#x201d;) are associated with the spatial line <b>122</b>. In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the points <b>302</b>(<b>1</b>)-(<b>5</b>) may be classified as the driving surface <b>110</b> and the points <b>302</b>(<b>6</b>)-(<b>10</b>) may be classified as the sidewalk <b>112</b>. Additionally, each of the points <b>302</b> may be associated with a respective location, such as x-coordinates, y-coordinates, and z-coordinates.</p><p id="p-0044" num="0043">Next, and as illustrated by the middle illustration, the vehicle <b>106</b> may determine a potential separation point <b>304</b>. In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the potential separation point <b>304</b> is laterally alone the y-direction with respect to the vehicle <b>106</b>. The vehicle <b>106</b> may assert that all of the points <b>302</b>(<b>1</b>)-(<b>5</b>) on one side of the potential separation point <b>304</b> lay on a first surface line <b>306</b>(<b>1</b>) (e.g., a surface line associated with the driving surface <b>110</b>) and all of the points <b>302</b>(<b>6</b>)-(<b>10</b>) on the other side of the potential separation point <b>304</b> lay on a second surface line <b>306</b>(<b>2</b>) (e.g., a surface line associated with the sidewalk <b>112</b>). The vehicle <b>106</b> may then determine an energy associated with the potential separation point <b>304</b>, where the energy is based on the differences between the locations of the points <b>302</b> (e.g., in the z-direction) and the locations of the surface lines <b>306</b>(<b>1</b>)-(<b>2</b>).</p><p id="p-0045" num="0044">For example, and for the point <b>302</b>(<b>10</b>), the vehicle <b>106</b> may determine a difference <b>308</b> between the second surface line <b>306</b>(<b>2</b>) and the point <b>302</b>(<b>10</b>). In some examples, the vehicle <b>106</b> determines the difference <b>308</b> as the distance between the point <b>302</b>(<b>10</b>) and the second surface line <b>306</b>(<b>2</b>), where the distance is determined using the location (e.g., the z-coordinate) associated with the point <b>302</b>(<b>10</b>). In some examples, the vehicle <b>106</b> may also determine an additional difference (not illustrated for clarity reasons) between the first surface line <b>306</b>(<b>1</b>) and the point <b>302</b>(<b>10</b>). In such examples, the vehicle <b>106</b> determines the additional difference as the distance between the point <b>302</b>(<b>10</b>) and the first surface line <b>306</b>(<b>1</b>), where the additional distance is determined using the location (e.g., the z-coordinate) associated with the point <b>302</b>(<b>10</b>). The vehicle <b>106</b> may then use the difference(s) to determine at least a first probability that the point <b>302</b>(<b>10</b>) is associated with the second surface line <b>306</b>(<b>2</b>) and/or a second probability that the point <b>302</b>(<b>10</b>) is associated with the first surface line <b>306</b>(<b>1</b>). In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, since the point <b>302</b>(<b>10</b>) is closer to the second surface line <b>306</b>(<b>2</b>) than the point <b>302</b>(<b>10</b>) is to the first surface line <b>306</b>(<b>1</b>), the vehicle <b>106</b> may determine that the first probability that the point <b>302</b>(<b>10</b>) is associated with the second surface line <b>306</b>(<b>2</b>) is greater than the second probability that the point <b>302</b>(<b>10</b>) is associated with the first surface line <b>306</b>(<b>1</b>).</p><p id="p-0046" num="0045">The vehicle <b>106</b> may then perform similar processes to determine differences between the points <b>302</b>(<b>1</b>)-(<b>9</b>) and the surface line(s) <b>306</b>(<b>1</b>)-(<b>2</b>) and use the differences to determine probabilities associated with the points <b>302</b>(<b>1</b>)-(<b>9</b>). Using the probabilities for the points <b>302</b>(<b>1</b>)-(<b>10</b>), the vehicle <b>106</b> may determine an energy <b>310</b> associated with the potential separation point <b>304</b>. In some examples, the lesser the differences between the points <b>302</b> and the surface lines <b>306</b>(<b>1</b>)-(<b>2</b>), the greater the energy <b>310</b> associated with the potential separation point <b>304</b>. Additionally, the greater the differences between the points <b>302</b> and the surface lines <b>306</b>(<b>1</b>)-(<b>2</b>), the lesser the energy <b>310</b> associated with the potential separation point <b>304</b>.</p><p id="p-0047" num="0046">In some examples, the vehicle <b>106</b> may perform similar processes in order to determine energies <b>310</b> associated with other potential separation points <b>304</b> for the spatial line <b>122</b>. The vehicle <b>106</b> may then use the energies <b>310</b> to determine the final separation point for the spatial line <b>122</b>. For example, and as illustrated by the bottom illustration, the vehicle <b>106</b> may determine that the separation point <b>130</b> corresponds to the potential separation point <b>304</b> that is associated with the maximum energy <b>310</b>. The vehicle <b>106</b> may make this determination since the potential separation point <b>304</b> that includes the maximum energy <b>310</b> may best represent the location of the curb <b>116</b>. This may be because, and as described above, the potential separation point <b>304</b> that includes the maximum energy <b>310</b> will include the smallest differences between the points <b>302</b> and the surface lines <b>306</b>(<b>1</b>)-(<b>2</b>). As such, the potential separation point <b>304</b> that includes the maximum energy <b>310</b> may best represent the separation between the driving surface <b>110</b> and the sidewalk <b>112</b>.</p><p id="p-0048" num="0047">It should be noted that, although the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the surface lines <b>306</b>(<b>1</b>)-(<b>2</b>) as including flat lines, in other examples, the surface lines <b>306</b>(<b>1</b>)-(<b>2</b>) may include any other type of line. For examples, one or more of the surface lines <b>306</b>(<b>1</b>)-(<b>2</b>) may be represented by a line that includes an angle (e.g., similar to an angle of a driving surface).</p><p id="p-0049" num="0048">After determining the separation points <b>130</b>, and as also described above, the vehicle <b>106</b> may use the separation points <b>130</b> to generate the curve <b>136</b> that represents the curb <b>116</b>. As such, <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of generating, using separation points <b>402</b>(<b>1</b>)-(<b>6</b>) (also referred to as &#x201c;separation points <b>402</b>&#x201d;) (which may represent, and/or include, the separation points <b>140</b>), the curve <b>146</b> that represents the curb <b>116</b>. For instance, and as illustrated by the left illustration, the vehicle <b>106</b> may initially generate a curve <b>404</b> by connecting each of the separation points <b>402</b> to one another. In some examples, the vehicle <b>106</b> initially generates the curve <b>404</b> by connecting the separation points <b>402</b> using straight lines. The vehicle <b>106</b> may then analyze the curve <b>404</b> in order to identify one or more of the points <b>402</b> that include &#x201c;outlier points&#x201d; for the curve <b>404</b>.</p><p id="p-0050" num="0049">For instance, and as shown by the left illustration, the vehicle <b>106</b> may determine that the separation point <b>402</b>(<b>10</b>) includes an outlier point. Based on the determination, the vehicle <b>106</b> may adjust the position of at least the separation point <b>402</b>(<b>10</b>). For example, the vehicle <b>106</b> may identify additional separation points <b>402</b> that are located proximate to the separation point <b>402</b>(<b>10</b>). In some examples, the vehicle <b>106</b> identifies the additional separation points <b>402</b> as being within a threshold number of points (e.g., one point, two points, five points, etc.) to the separation point <b>402</b>(<b>10</b>). For example, the vehicle <b>106</b> may identify additional separation points <b>402</b>(<b>8</b>)-(<b>9</b>) that are within two points before the separation point <b>402</b>(<b>10</b>) and additional separation points <b>402</b>(<b>11</b>)-(<b>12</b>) that are within two points after the separation point <b>402</b>(<b>10</b>). Additionally, or alternatively, in some examples, the vehicle <b>106</b> identifies the additional separation points <b>402</b> as being within a threshold distance to the separation point <b>402</b>(<b>10</b>). For example, the vehicle <b>106</b> may identify additional separation points <b>402</b>(<b>8</b>)-(<b>9</b>) that are within fifty centimeters before the separation point <b>402</b>(<b>10</b>) and additional separation points <b>402</b>(<b>11</b>)-(<b>12</b>) that are within fifty centimeters after the separation point <b>402</b>(<b>10</b>).</p><p id="p-0051" num="0050">Next, and as shown by the middle illustration, the vehicle <b>106</b> may then determine energies <b>406</b> associated with the separation points <b>402</b>(<b>8</b>)-(<b>12</b>) and/or the lines that connect the separation points <b>402</b>(<b>8</b>)-(<b>12</b>). In some examples, the vehicle <b>106</b> then computes the gradient on the energy direction to determine a new position for at least the separation point <b>402</b>(<b>10</b>). For example, and as shown by the right illustration, the vehicle <b>106</b> moves the separation point <b>402</b>(<b>10</b>) towards the direction of the energy <b>406</b>. Additionally, in the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the vehicle <b>106</b> also moved the additional separation points that are proximate to the separation point <b>402</b>(<b>10</b>). In some examples, the vehicle <b>106</b> may continue to perform these processes in order to generate the curve <b>146</b> that represents the curb <b>116</b>.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a block diagram of an example system <b>500</b> for implementing the techniques described herein, in accordance with embodiments of the disclosure. In at least one example, the system <b>500</b> can include the vehicle <b>106</b>. The vehicle <b>106</b> can include a vehicle computing device <b>502</b>, one or more sensor systems <b>504</b>, one or more emitters <b>506</b>, one or more communication connections <b>508</b>, at least one direct connection <b>510</b>, and one or more drive systems <b>512</b>.</p><p id="p-0053" num="0052">The vehicle computing device <b>502</b> can include one or more processors <b>514</b> and a memory <b>516</b> communicatively coupled with the processor(s) <b>514</b>. In the illustrated example, the vehicle <b>106</b> is an autonomous vehicle. However, the vehicle <b>106</b> may be any other type of vehicle (e.g., a manually driven vehicle, a semi-autonomous vehicle, etc.), or any other system having at least an image capture device. In the illustrated example, the memory <b>516</b> of the vehicle computing device <b>502</b> stores a localization component <b>518</b>, a perception component <b>520</b>, a planning component <b>522</b>, a curb-determination component <b>524</b>, one or more system controllers <b>526</b>, and one or more maps <b>528</b>. Though depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref> as residing in the memory <b>516</b> for illustrative purposes, it is contemplated that the localization component <b>518</b>, the perception component <b>520</b>, the planning component <b>522</b>, the curb-determination component <b>524</b>, the system controller(s) <b>526</b>, and/or the map(s) <b>528</b> can additionally, or alternatively, be accessible to the vehicle <b>106</b> (e.g., stored on, or otherwise accessible by, memory remote from the vehicle <b>106</b>).</p><p id="p-0054" num="0053">In at least one example, the localization component <b>518</b> can include functionality to receive sensor data <b>530</b> from the sensor system(s) <b>504</b> and to determine a position and/or orientation of the vehicle <b>106</b> (e.g., one or more of an x-, y-, z-position, roll, pitch, or yaw). For example, the localization component <b>518</b> can include and/or request/receive a map of an environment and can continuously determine a location and/or orientation of the vehicle <b>106</b> within the map. In some instances, the localization component <b>518</b> can utilize SLAM (simultaneous localization and mapping), CLAMS (calibration, localization and mapping, simultaneously), relative SLAM, bundle adjustment, non-linear least squares optimization, or the like to receive image data, lidar data, radar data, IMU data, GPS data, wheel encoder data, and the like to accurately determine a location of the vehicle <b>106</b>. In some instances, the localization component <b>518</b> can provide data to various components of the vehicle <b>106</b> to determine an initial position of the vehicle <b>106</b> for generating a candidate trajectory, as discussed herein.</p><p id="p-0055" num="0054">In some instances, the perception component <b>520</b> can include functionality to perform object detection, segmentation, and/or classification. In some instances, the perception component <b>520</b> can provide processed sensor data <b>530</b> that indicates a presence of an object that is proximate to the vehicle <b>106</b> and/or a classification of the object as an object type (e.g., car, pedestrian, cyclist, animal, building, tree, road surface, curb, sidewalk, unknown, etc.). In additional and/or alternative examples, the perception component <b>520</b> can provide processed sensor data <b>530</b> that indicates one or more characteristics associated with a detected object and/or the environment in which the object is positioned. In some instances, characteristics associated with an object can include, but are not limited to, an x-position (global position), a y-position (global position), a z-position (global position), an orientation (e.g., a roll, pitch, yaw), an object type (e.g., a classification), a velocity of the object, an acceleration of the object, an extent of the object (size), etc. Characteristics associated with the environment can include, but are not limited to, a presence of another object in the environment, a state of another object in the environment, a time of day, a day of a week, a season, a weather condition, an indication of darkness/light, etc.</p><p id="p-0056" num="0055">In general, the planning component <b>522</b> can determine a path for the vehicle <b>106</b> to follow to traverse through an environment. For example, the planning component <b>522</b> can determine various routes and trajectories and various levels of detail. For example, the planning component <b>522</b> can determine a route to travel from a first location (e.g., a current location) to a second location (e.g., a target location). For the purpose of this discussion, a route can be a sequence of waypoints for travelling between two locations. As non-limiting examples, waypoints include streets, intersections, global positioning system (GPS) coordinates, etc. Further, the planning component <b>522</b> can generate an instruction for guiding the vehicle <b>106</b> along at least a portion of the route from the first location to the second location. In at least one example, the planning component <b>522</b> can determine how to guide the vehicle <b>106</b> from a first waypoint in the sequence of waypoints to a second waypoint in the sequence of waypoints. In some instances, the instruction can be a trajectory, or a portion of a trajectory. In some instances, multiple trajectories can be substantially simultaneously generated (e.g., within technical tolerances) in accordance with a receding horizon technique, wherein one of the multiple trajectories is selected for the vehicle <b>106</b> to navigate.</p><p id="p-0057" num="0056">In at least one example, the planning component <b>522</b> can determine a pickup location associated with a location. As used herein, a pickup location can be a specific location (e.g., a parking space, a loading zone, a portion of a ground surface, etc.) within a threshold distance of a location (e.g., an address or location associated with a dispatch request) where the vehicle <b>106</b> can stop to pick up a passenger. In at least one example, the planning component <b>522</b> can determine a pickup location based at least in part on determining a user identity (e.g., determined via image recognition or received as an indication from a user device, as discussed herein). Arrival at a pickup location, arrival at a destination location, entry of the vehicle by a passenger, and receipt of a &#x201c;start ride&#x201d; command are additional examples of events that may be used for event-based data logging.</p><p id="p-0058" num="0057">The curb-detection component <b>524</b> may be configured to perform the processes described herein in order to determine the locations of curbs within an environment. For example, the curb-detection component <b>524</b> may be configured to analyze the sensor data <b>530</b> in order to identify points, represented by the sensor data, for identifying a curb. As described herein, the points may be classified as belonging to at least driving surface(s) and sidewalk(s). The curb-detection component <b>524</b> may then be configured to generate the spatial lines using the points. After generating the spatial lines, the curb-detection component <b>524</b> may be configured to analyze the spatial lines in order to determine the separation points associated with the spatial lines.</p><p id="p-0059" num="0058">The curb-detection component <b>524</b> may then be configured to generate, using the spatial lines, the curve that represents the curb. In some examples, the curb-detection component <b>524</b> may be configured to continue these processes in order to continue determining locations of curbs. Additionally, in some examples, the curb-detection component <b>524</b> may be configured to perform the processes described herein to identify curbs that are located on more than one side of the vehicle <b>106</b>. For example, the curb-detection component <b>524</b> may be configured to determine the locations of curbs that are located on both a first side (e.g., a driver side) of the vehicle <b>106</b> and curbs that are located on a second side (e.g., a passenger side) of the vehicle <b>106</b>.</p><p id="p-0060" num="0059">In at least one example, the vehicle computing device <b>502</b> can include the system controller(s) <b>526</b>, which can be configured to control steering, propulsion, braking, safety, emitters, communication, and other systems of the vehicle <b>106</b>. These system controller(s) <b>526</b> can communicate with and/or control corresponding systems of the drive system(s) <b>512</b> and/or other components of the vehicle <b>106</b>.</p><p id="p-0061" num="0060">The memory <b>516</b> can further include the map(s) <b>528</b> that can be used by the vehicle <b>106</b> to navigate within the environment. For the purpose of this discussion, a map can be any number of data structures modeled in two dimensions, three dimensions, or N-dimensions that are capable of providing information about an environment, such as, but not limited to, topologies (such as intersections), streets, mountain ranges, roads, terrain, and the environment in general. In some instances, a map can include, but is not limited to: texture information (e.g., color information (e.g., RGB color information, Lab color information, HSV/HSL color information), and the like), intensity information (e.g., lidar information, radar information, and the like); spatial information (e.g., image data projected onto a mesh, individual &#x201c;surfels&#x201d; (e.g., polygons associated with individual color and/or intensity)), reflectivity information (e.g., specularity information, retroreflectivity information, BRDF information, BSSRDF information, and the like). In one example, a map can include a three-dimensional mesh of the environment. In some instances, the map can be stored in a tiled format, such that individual tiles of the map represent a discrete portion of an environment and can be loaded into working memory as needed. In at least one example, the map(s) <b>528</b> can include at least one map (e.g., images and/or a mesh). In some example, the vehicle <b>106</b> can be controlled based at least in part on the map(s) <b>528</b>. That is, the map(s) <b>528</b> can be used in connection with the localization component <b>518</b>, the perception component <b>520</b>, and/or the planning component <b>522</b> to determine a location of the vehicle <b>106</b>, identify entities in an environment, and/or generate routes and/or trajectories to navigate within an environment.</p><p id="p-0062" num="0061">In some instances, aspects of some or all of the components discussed herein can include any models, algorithms, and/or machine learning algorithms. For example, in some instances, the components in the memory <b>516</b> can be implemented as a neural network. As described herein, an exemplary neural network is a biologically inspired algorithm which passes input data through a series of connected layers to produce an output. Each layer in a neural network can also comprise another neural network, or can comprise any number of layers (whether convolutional or not). As can be understood in the context of this disclosure, a neural network can utilize machine learning, which can refer to a broad class of such algorithms in which an output is generated based at least in part on learned parameters.</p><p id="p-0063" num="0062">Although discussed in the context of neural networks, any type of machine learning can be used consistent with this disclosure. For example, machine learning algorithms can include, but are not limited to, regression algorithms (e.g., ordinary least squares regression (OLSR), linear regression, logistic regression, stepwise regression, multivariate adaptive regression splines (MARS), locally estimated scatterplot smoothing (LOESS)), instance-based algorithms (e.g., ridge regression, least absolute shrinkage and selection operator (LASSO), elastic net, least-angle regression (LARS)), decisions tree algorithms (e.g., classification and regression tree (CART), iterative dichotomiser 3 (ID3), Chi-squared automatic interaction detection (CHAID), decision stump, conditional decision trees), Bayesian algorithms (e.g., na&#xef;ve Bayes, Gaussian na&#xef;ve Bayes, multinomial na&#xef;ve Bayes, average one-dependence estimators (AODE), Bayesian belief network (BNN), Bayesian networks), clustering algorithms (e.g., k-means, k-medians, expectation maximization (EM), hierarchical clustering), association rule learning algorithms (e.g., perceptron, back-propagation, hopfield network, Radial Basis Function Network (RBFN)), deep learning algorithms (e.g., Deep Boltzmann Machine (DBM), Deep Belief Networks (DBN), Convolutional Neural Network (CNN), Stacked Auto-Encoders), Dimensionality Reduction Algorithms (e.g., Principal Component Analysis (PCA), Principal Component Regression (PCR), Partial Least Squares Regression (PLSR), Sammon Mapping, Multidimensional Scaling (MDS), Projection Pursuit, Linear Discriminant Analysis (LDA), Mixture Discriminant Analysis (MDA), Quadratic Discriminant Analysis (QDA), Flexible Discriminant Analysis (FDA)), Ensemble Algorithms (e.g., Boosting, Bootstrapped Aggregation (Bagging), AdaBoost, Stacked Generalization (blending), Gradient Boosting Machines (GBM), Gradient Boosted Regression Trees (GBRT), Random Forest), SVM (support vector machine), supervised learning, unsupervised learning, semi-supervised learning, etc. Additional examples of architectures include neural networks such as ResNet50, ResNet101, VGG, DenseNet, PointNet, and the like.</p><p id="p-0064" num="0063">As discussed above, in at least one example, the sensor system(s) <b>504</b> can include lidar sensors, radar sensors, sonar sensors, location sensors (e.g., GPS, compass, etc.), inertial sensors (e.g., inertial measurement units (IMUs), accelerometers, magnetometers, gyroscopes, etc.), cameras (e.g., RGB, IR, intensity, depth, time of flight, etc.), microphones, wheel encoders, environment sensors (e.g., temperature sensors, humidity sensors, light sensors, pressure sensors, etc.), etc. The sensor system(s) <b>504</b> can include multiple instances of each of these or other types of sensors. For instance, the lidar sensors can include individual lidar sensors located at the corners, front, back, sides, and/or top of the vehicle <b>106</b>. As another example, the camera sensors can include multiple cameras disposed at various locations about the exterior and/or interior of the vehicle <b>106</b>. The sensor system(s) <b>504</b> can provide input to the vehicle computing device <b>502</b>. Additionally or alternatively, the sensor system(s) <b>504</b> can send the sensor data <b>530</b>, via the one or more network(s) <b>532</b>, to a computing device(s) <b>534</b> at a particular frequency, after a lapse of a predetermined period of time, upon occurrence of one or more conditions, in near real-time, etc.</p><p id="p-0065" num="0064">The vehicle <b>106</b> can also include the emitter(s) <b>506</b> for emitting light and/or sound, as described above. The emitter(s) <b>506</b> in this example include interior audio and visual emitters to communicate with passengers of the vehicle <b>106</b>. By way of example and not limitation, interior emitters can include speakers, lights, signs, display screens, touch screens, haptic emitters (e.g., vibration and/or force feedback), mechanical actuators (e.g., seatbelt tensioners, seat positioners, headrest positioners, etc.), and the like. The emitter(s) <b>506</b> in this example also include exterior emitters. By way of example and not limitation, the exterior emitters in this example include lights to signal a direction of travel or other indicator of vehicle action (e.g., indicator lights, signs, light arrays, etc.), and one or more audio emitters (e.g., speakers, speaker arrays, horns, etc.) to audibly communicate with pedestrians or other nearby vehicles, one or more of which comprising acoustic beam steering technology.</p><p id="p-0066" num="0065">The vehicle <b>106</b> can also include the communication connections(s) <b>508</b> that enable communication between the vehicle <b>106</b> and one or more other local or remote computing device(s). For instance, the communication connection(s) <b>508</b> can facilitate communication with other local computing device(s) on the vehicle <b>106</b> and/or the drive system(s) <b>512</b>. Also, the communication connection(s) <b>508</b> can allow the vehicle <b>106</b> to communicate with other nearby computing device(s) (e.g., other nearby vehicles, traffic signals, etc.). The communications connection(s) <b>508</b> also enable the vehicle <b>106</b> to communicate with the remote teleoperations computing devices or other remote services.</p><p id="p-0067" num="0066">The communications connection(s) <b>508</b> can include physical and/or logical interfaces for connecting the vehicle computing device <b>502</b> to another computing device or a network, such as network(s) <b>532</b>. For example, the communications connection(s) <b>508</b> can enable Wi-Fi-based communication such as via frequencies defined by the IEEE 802.11 standards, short range wireless frequencies such as Bluetooth&#xae;, cellular communication (e.g., 2G, 2G, 4G, 4G LTE, 5G, etc.) or any suitable wired or wireless communications protocol that enables the respective computing device to interface with the other computing device(s).</p><p id="p-0068" num="0067">In at least one example, the vehicle <b>106</b> can include one or more drive systems <b>512</b>. In some instances, the vehicle <b>106</b> can have a single drive system <b>512</b>. In at least one example, if the vehicle <b>106</b> has multiple drive systems <b>512</b>, individual drive systems <b>512</b> can be positioned on opposite ends of the vehicle <b>106</b> (e.g., the front and the rear, etc.). In at least one example, the drive system(s) <b>512</b> can include one or more sensor systems to detect conditions of the drive system(s) <b>512</b> and/or the surroundings of the vehicle <b>106</b>. By way of example and not limitation, the sensor system(s) <b>504</b> can include one or more wheel encoders (e.g., rotary encoders) to sense rotation of the wheels of the drive systems, inertial sensors (e.g., inertial measurement units, accelerometers, gyroscopes, magnetometers, etc.) to measure orientation and acceleration of the drive system(s), cameras or other image sensors, ultrasonic sensors to acoustically detect entities in the surroundings of the drive system(s), lidar sensors, radar sensors, etc. Some sensors, such as the wheel encoders can be unique to the drive system(s) <b>512</b>. In some cases, the sensor system(s) <b>504</b> on the drive system(s) <b>512</b> can overlap or supplement corresponding systems of the vehicle <b>106</b> (e.g., sensor system(s) <b>504</b>).</p><p id="p-0069" num="0068">The drive system(s) <b>512</b> can include many of the vehicle systems, including a high voltage battery, a motor to propel the vehicle <b>106</b>, an inverter to convert direct current from the battery into alternating current for use by other vehicle systems, a steering system including a steering motor and steering rack (which can be electric), a braking system including hydraulic or electric actuators, a suspension system including hydraulic and/or pneumatic components, a stability control system for distributing brake forces to mitigate loss of traction and maintain control, an HVAC system, lighting (e.g., lighting such as head/tail lights to illuminate an exterior surrounding of the vehicle), and one or more other systems (e.g., cooling system, safety systems, onboard charging system, other electrical components such as a DC/DC converter, a high voltage junction, a high voltage cable, charging system, charge port, etc.). Additionally, the drive system(s) <b>512</b> can include a drive system controller which can receive and preprocess the sensor data <b>530</b> from the sensor system(s) <b>504</b> and to control operation of the various vehicle systems. In some instances, the drive system controller can include one or more processors and memory communicatively coupled with the one or more processors. The memory can store instructions to perform various functionalities of the drive system(s) <b>512</b>. Furthermore, the drive system(s) <b>512</b> also include one or more communication connection(s) that enable communication by the respective drive system with one or more other local or remote computing device(s).</p><p id="p-0070" num="0069">In at least one example, the direct connection <b>510</b> can provide a physical interface to couple the one or more drive system(s) <b>512</b> with the body of the vehicle <b>106</b>. For example, the direct connection <b>510</b> can allow the transfer of energy, fluids, air, data, etc. between the drive system(s) <b>512</b> and the vehicle <b>106</b>. In some instances, the direct connection <b>510</b> can further releasably secure the drive system(s) <b>512</b> to the body of the vehicle <b>106</b>.</p><p id="p-0071" num="0070">As further illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the computing device(s) <b>534</b> can include processor(s) <b>536</b>, communication connection(s) <b>538</b>, and memory <b>540</b>. The processor(s) <b>514</b> of the vehicle <b>106</b> and/or the processor(s) <b>536</b> of the computing device(s) <b>534</b> (and/or other processor(s) described herein) can be any suitable processor capable of executing instructions to process data and perform operations as described herein. By way of example and not limitation, the processor(s) <b>514</b> and the processor(s) <b>536</b> can comprise one or more Central Processing Units (CPUs), Graphics Processing Units (GPUs), or any other device or portion of a device that processes electronic data to transform that electronic data into other electronic data that can be stored in registers and/or memory. In some instances, integrated circuits (e.g., ASICs, etc.), gate arrays (e.g., FPGAs, etc.), and other hardware devices can also be considered processors in so far as they are configured to implement encoded instructions.</p><p id="p-0072" num="0071">The memory <b>516</b> and the memory <b>540</b> (and/or other memory described herein) are examples of non-transitory computer-readable media. The memory <b>516</b> and the memory <b>540</b> can store an operating system and one or more software applications, instructions, programs, and/or data to implement the methods described herein and the functions attributed to the various systems. In various implementations, the memory can be implemented using any suitable memory technology, such as static random access memory (SRAM), synchronous dynamic RAM (SDRAM), nonvolatile/Flash-type memory, or any other type of memory capable of storing information. The architectures, systems, and individual elements described herein can include many other logical, programmatic, and physical components, of which those shown in the accompanying figures are merely examples that are related to the discussion herein.</p><p id="p-0073" num="0072">It should be noted that while <figref idref="DRAWINGS">FIG. <b>5</b></figref> is illustrated as a distributed system, in alternative examples, components of the computing device(s) <b>534</b> can be associated with the vehicle <b>106</b>. That is, the vehicle <b>106</b> can perform one or more of the functions associated with the computing device(s) <b>534</b> and/or the computing device(s) <b>534</b> can perform one or more of the functions associated with the vehicle <b>106</b>. For example, the computing device(s) <b>534</b> may store the curb-detection component <b>524</b>, receive the sensor data <b>530</b> from the vehicle <b>106</b>, and then use the curb-detection component <b>524</b> to generate the curve that represents the curb, using similar processes as those described above with regard to the vehicle <b>106</b>.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIGS. <b>6</b>-<b>8</b></figref> illustrate example processes in accordance with the disclosure. These processes are illustrated as logical flow graphs, each operation of which represents a sequence of operations that may be implemented in hardware, software, or a combination thereof. In the context of software, the operations represent computer-executable instructions stored on one or more computer-readable storage media that, when executed by one or more processors, perform the recited operations. Generally, computer-executable instructions include routines, programs, objects, components, data structures, and the like that perform particular functions or implement particular abstract data types. The order in which the operations are described is not intended to be construed as a limitation, and any number of the described operations may be omitted or combined in any order and/or in parallel to implement the processes.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a flow diagram of an example process <b>600</b> for generating a curve that represents a curb. At operation <b>602</b>, the process <b>600</b> may include receiving sensor data from one or more sensors, the sensor data representing a driving surface and/or a sidewalk. For instance, the vehicle <b>106</b> may receive the sensor data from the one or more sensors, such as one or more lidar sensors. As described herein, the sensor data may represent locations of points within an environment and/or probabilities associated with accuracies associated with the points. In some examples, the vehicle <b>106</b> may then analyze the sensor data, such as by using one or more models, in order to determine classifications associated with the points. In some examples, the vehicle <b>106</b> may then filter the points using one or more processes. For example, the vehicle <b>106</b> may filter the points to identify points that are associated with the driving surface and/or the sidewalk, identify points that are located within a threshold distance to the vehicle <b>106</b>, and/or identify points that are associated with probabilities that satisfy a threshold probability.</p><p id="p-0076" num="0075">At operation <b>604</b>, the process <b>600</b> may include generating spatial lines using the sensor data. For instance, the vehicle <b>106</b> may use the points (e.g., the filtered points) in order to generate the spatial lines. As described herein, to generate the spatial lines, the vehicle <b>106</b> may quantize the points into distance bins that are located laterally along the driving direction of the vehicle <b>106</b>. For example, each of the distance bins may include a specific lateral distance along the driving direction. As described herein, the lateral distance may include, but is not limited to, ten centimeters, twenty centimeters, fifty centimeters, one hundred centimeters, and/or any other distance. The vehicle <b>106</b> may then quantize the points into the distance bins such that all of the points that fall within a respective distance bin are included in that distance bin. These distance bins may then represent the spatial lines that the vehicle <b>106</b> uses for identifying the locations of the curb.</p><p id="p-0077" num="0076">At operation <b>606</b>, the process <b>600</b> may include determining separation points associated with the spatial lines, the separation points representing areas between the driving surface and the sidewalk. For instance, the vehicle <b>106</b> may determine the separation points associated with the spatial lines. As described herein, for each spatial line, the vehicle <b>106</b> may identify the separation point that separates the points that are associated with the driving surface from the points that are associated with the sidewalk. In some examples, to identify the separation point, the vehicle <b>106</b> may analyze various points along the spatial line, such as laterally in the y-direction. For each point along the spatial line, the vehicle <b>106</b> may perform the processes described herein to determine a respective energy. The vehicle <b>106</b> may then determine that the separation point for the spatial line includes the point along the spatial line with the maximum energy. Additionally, the vehicle <b>106</b> may perform similar processes to determine the separation points for each of the other spatial lines.</p><p id="p-0078" num="0077">At operation <b>608</b>, the process <b>600</b> may include determining whether the heights associated with the areas satisfies a threshold height. For instance, the vehicle <b>106</b> may determine the heights associated with the areas. In some examples, the vehicle <b>106</b> may determine the heights by taking differences between the points that are associated with the driving surface and the points that are associated with the sidewalk. For example, and to determine a height, the vehicle <b>106</b> may determine the difference between a first coordinate (e.g., a z-coordinate) for a first point that is associated with the driving surface and a second coordinate (e.g., a z-coordinate) for a second point that is associated with the sidewalk. The vehicle <b>106</b> may then determine whether the heights satisfy (e.g., area equal to or greater than) the threshold height.</p><p id="p-0079" num="0078">If, at operation <b>608</b>, it is determined that the heights do not satisfy the threshold height, then at operation <b>610</b>, the process <b>600</b> may include determining that there is not a curb between the driving surface and the sidewalk. For instance, if the vehicle <b>106</b> determines that the heights do not satisfy the threshold height, then the vehicle <b>106</b> may determine that there is no curb.</p><p id="p-0080" num="0079">However, if, at operation <b>608</b>, it is determined that the heights satisfy the threshold height, then at operation <b>612</b>, the process <b>600</b> may include generating a curve that represents the curb using the separation points. For instance, if the vehicle <b>106</b> determines that the heights satisfy the threshold height, then the vehicle <b>106</b> may generate the curve that represents the curb. Additionally, the vehicle <b>106</b> may use the curve to perform one or more actions. For example, the vehicle <b>106</b> may use the curve to avoid the curb and/or to stop at a location that is proximate to the curb.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a flow diagram of an example process <b>700</b> for identifying at least a reliable point for use in identifying a location of a curb. At operation <b>702</b>, the process <b>700</b> may include receiving sensor data representing a point. For instance, the vehicle <b>106</b> may receive the sensor data from the one or more sensors, such as one or more lidar sensors. As described herein, the sensor data may represent locations of points within an environment and/or probabilities associated with accuracies associated with the points. In some examples, the vehicle <b>106</b> may then analyze the sensor data, such as by using one or more models, in order to determine classifications associated with the points.</p><p id="p-0082" num="0081">At operation <b>704</b>, the process <b>700</b> may include determining whether the point is associated with classification(s). For instance, the vehicle <b>106</b> may determine whether the point is associated with specific classification(s). As described herein, in some examples, the specific classification(s) may include driving surface(s) and/or sidewalks(s). If, at operation <b>704</b>, it is determined that the point is not associated with the classification(s), then at operation <b>706</b>, the process <b>700</b> may include determining not to use the point to determine a location of a curb. For instance, if the vehicle <b>106</b> determines that the point is not associated with the classification(s), then the vehicle <b>106</b> may determine not to use the point to determine the location of the curb.</p><p id="p-0083" num="0082">However, if, at operation <b>704</b>, it is determined that the point is associated with the classification(s), then at operation <b>708</b>, the process <b>700</b> may include determining whether the point is within a threshold distance. For instance, if the vehicle <b>106</b> determines that the point is associated with the classification(s), then the vehicle <b>106</b> may determine whether the point is within the threshold distance to the vehicle <b>106</b>. In some examples, the vehicle <b>106</b> may make the determination using the location of the point with respect to the vehicle <b>106</b>. If, at operation <b>708</b>, it is determined that the point is within the threshold distance, then again at operation <b>706</b>, the process <b>700</b> may include determining not to use the point to determine the location of the curb. For instance, if the vehicle <b>106</b> determines that the point is not within the threshold distance, then the vehicle <b>106</b> may determine not to use the point to determine the location of the curb.</p><p id="p-0084" num="0083">However, if, at operation <b>708</b>, it is determined that the point is within the threshold distance, then at operation <b>710</b>, the process <b>700</b> may include determining whether the point is associated with a probability that satisfies a threshold probability. For instance, if the vehicle <b>106</b> determines that the point is within the threshold distance, then the vehicle <b>106</b> may determine whether the point is associated with the probability that satisfies the threshold probability. If, at operation <b>710</b>, it is determined that the probability does not satisfy the threshold probability, then again at operation <b>706</b>, the process <b>700</b> may include determining not to use the point to determine the location of the curb. For instance, if the vehicle <b>106</b> determines that the probability does not satisfy the threshold probability, then the vehicle <b>106</b> may determine not to use the point to determine the location of the curb.</p><p id="p-0085" num="0084">However, if, at operation <b>710</b>, it is determined that the point is associated with the probability that satisfies the threshold probability, then at operation <b>712</b>, the process <b>700</b> may include determining to use the point to determine the location of the curb. For instance, if the vehicle <b>106</b> determines that the probability satisfies the threshold probability, then the vehicle <b>106</b> may determine to use the point to determine the location of the curb. Additionally, the vehicle <b>106</b> may continue to perform the example process <b>700</b> in order to identify additional points that the vehicle <b>106</b> may use determine the location of the curb.</p><p id="p-0086" num="0085">While the process <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an order for operations <b>704</b>, <b>708</b>, and <b>710</b>, in other examples, the process <b>700</b> may include any other order for operations <b>704</b>, <b>708</b>, and <b>710</b>. Additionally, in some examples, the process <b>700</b> may not include one more of the operations <b>704</b>, <b>708</b>, or <b>710</b>.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a flow diagram of an example process <b>800</b> for determining a separation point associated with a spatial line. At operation <b>802</b>, the process <b>800</b> may include generating a spatial line using points represented by sensor data. For instance, the vehicle <b>106</b> may generate the spatial line using the points. As described herein, the spatial line may be associated with a distance bin, where the locations of the points are within the distance bin. In some examples, the vehicle <b>106</b> initially processes the points before generating the spatial line, such as by performing the process <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>. For example, the vehicle <b>106</b> may determine that the points are associated with specific classification(s), determine that the points are within a threshold distance to the vehicle <b>106</b>, and/or determine that probabilities associated with the points satisfy a threshold probability.</p><p id="p-0088" num="0087">At operation <b>804</b>, the process <b>800</b> may include determining a potential separation point associated with the spatial line. For instance, the vehicle <b>106</b> may determine the potential separation point associated with the spatial line. In some examples, the vehicle <b>106</b> determines the potential separation point by moving across the spatial line along a specific direction, such as the y-direction.</p><p id="p-0089" num="0088">At operation <b>806</b>, the process <b>800</b> may include determining an energy associated with the potential separation point. For instance, the vehicle <b>106</b> may analyze the potential separation point in order to determine the energy. In some examples, to determine the energy, the vehicle <b>106</b> initially asserts that all of the points on one side of the potential separation point lay on a first surface line (e.g., a surface line associated with the driving surface(s)) and all points on the other side of the potential separation point lay on a second surface line (e.g., a surface line associated with the sidewalk(s)). The vehicle <b>106</b> may then determine differences between the locations of the points (e.g., in the z-direction) and the locations of the surface lines. Using the differences, the vehicle <b>106</b> may determine the energy.</p><p id="p-0090" num="0089">At operation <b>808</b>, the process <b>800</b> may include determining whether to identify an additional potential separation point. For instance, the vehicle <b>106</b> may determine whether to identify the additional potential separation point for the spatial line. In some examples, the vehicle <b>106</b> may determine a threshold number of potential separation points for the spatial line. In such examples, the threshold number of potential separation points may be based on the length of the spatial line. In some examples, the vehicle <b>106</b> may determine to identify the additional potential separation point based on the energy of the potential separation point. For instance, the vehicle <b>106</b> may determine to identify the additional potential separation point when the energy is equal to or greater than a threshold energy.</p><p id="p-0091" num="0090">If, at operation <b>808</b>, it is determined to identify the additional potential separation point, then the process <b>800</b> may repeat back at operation <b>804</b>. For instance, if the vehicle <b>106</b> determines to identify the additional potential separation point, then the vehicle <b>106</b> may determine the additional potential separation point for the spatial line. For instance, the vehicle <b>106</b> may determine the additional potential separation point by moving in the y-direction along the spatial line.</p><p id="p-0092" num="0091">However, if at operation <b>808</b>, it is determined not to identify the additional potential separation point, then at operation <b>810</b>, the process <b>800</b> may include determining a separation point associated with the spatial line based at least in part on the energy. For instance, if the vehicle <b>106</b> determines not to identify the additional potential separation point, then the vehicle <b>106</b> may determine the separation point based at least in part on the energy. As described herein, the vehicle <b>106</b> may determine the separation point to include the potential separation point that is associated with the minimum energy. This is because the potential separation point that is associated with the minimum energy may be located at a location that is associated with the curb within the environment.</p><p id="p-0093" num="0092">Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather, the specific features and acts are disclosed as example forms of implementing the claims.</p><p id="p-0094" num="0093">The components described herein represent instructions that may be stored in any type of computer-readable medium and may be implemented in software and/or hardware. All of the methods and processes described above may be embodied in, and fully automated via, software code components and/or computer-executable instructions executed by one or more computers or processors, hardware, or some combination thereof. Some or all of the methods may alternatively be embodied in specialized computer hardware.</p><p id="p-0095" num="0094">Conditional language such as, among others, &#x201c;may,&#x201d; &#x201c;could,&#x201d; &#x201c;may&#x201d; or &#x201c;might,&#x201d; unless specifically stated otherwise, are understood within the context to present that certain examples include, while other examples do not include, certain features, elements and/or steps. Thus, such conditional language is not generally intended to imply that certain features, elements and/or steps are in any way required for one or more examples or that one or more examples necessarily include logic for deciding, with or without user input or prompting, whether certain features, elements and/or steps are included or are to be performed in any particular example.</p><p id="p-0096" num="0095">Conjunctive language such as the phrase &#x201c;at least one of X, Y or Z,&#x201d; unless specifically stated otherwise, is to be understood to present that an item, term, etc. may be either X, Y, or Z, or any combination thereof, including multiples of each element. Unless explicitly described as singular, &#x201c;a&#x201d; means singular and plural.</p><p id="p-0097" num="0096">Any routine descriptions, elements or blocks in the flow diagrams described herein and/or depicted in the attached figures should be understood as potentially representing modules, segments, or portions of code that include one or more computer-executable instructions for implementing specific logical functions or elements in the routine. Alternate implementations are included within the scope of the examples described herein in which elements or functions may be deleted, or executed out of order from that shown or discussed, including substantially synchronously, in reverse order, with additional operations, or omitting operations, depending on the functionality involved as would be understood by those skilled in the art.</p><p id="p-0098" num="0097">Many variations and modifications may be made to the above-described examples, the elements of which are to be understood as being among other acceptable examples. All such modifications and variations are intended to be included herein within the scope of this disclosure and protected by the following claims.</p><heading id="h-0004" level="1">EXAMPLE CLAUSES</heading><p id="p-0099" num="0098">A: A system comprising: one or more processors; and one or more computer-readable media storing instructions that, when executed by the one or more processors, cause the one or more processors to perform operations comprising: receiving sensor data from a lidar sensor, the sensor data representing first points associated with a drivable surface and second points associated with a sidewalk; generating spatial lines using the first points associated with the drivable surface and the second points associated with the sidewalk; determining, for the spatial lines, separation points between the first points associated with the drivable surface and the second points associated with the sidewalk; generating, based at least in part on the separation points, a curve that represents a curb between the drivable surface and the sidewalk; and controlling a vehicle based at least in part on the curve.</p><p id="p-0100" num="0099">B: The system as recited in paragraph A, wherein receiving the sensor data occurs at a first time, and wherein the operations further comprise: receiving, at a second time, additional sensor data from the lidar sensor, the additional sensor data representing third points associated with the drivable surface and fourth points associated with the sidewalk, and wherein generating the spatial lines further uses the third points and the fourth points.</p><p id="p-0101" num="0100">C: The vehicle as recited in either paragraph A or paragraph B, the operations further comprising: determining, for a spatial line from the spatial lines, a distance between a first point associated with the drivable surface and a second point associated with the sidewalk; determining that the distance is equal to or greater than a threshold distance; and determining that a separation point associated with the spatial line is associated with the curb based at least in part on the distance being equal to or greater than the threshold distance.</p><p id="p-0102" num="0101">D: The vehicle as recited in any one of paragraphs A-C, wherein determining a separation point from the separation points comprises at least: determining, based at least in part a candidate separation point, a first surface line associated with a first portion of the sensor data and a second surface line associated with a second portion of the sensor data; determining, based at least in part on a first difference between the first surface line and a first point of the first portion of the sensor data, a first energy; determining, based at least in part on a second difference between the second surface line an second point of the second portion of the sensor data, a second energy; and determining that the candidate separation point is a separation point based at least in part on the first energy and the second energy.</p><p id="p-0103" num="0102">E: The vehicle as recited in any one of paragraphs A-D, wherein generating a spatial line from the spatial lines comprises at least: analyzing the sensor data to identify at least a portion of the first points that are located within a distance range from the vehicle; analyzing the sensor data to identify at least a portion of the second points that are located within the distance range; and generating the spatial line to include at least the portion of the first points and the portion of the second points.</p><p id="p-0104" num="0103">F: A method comprising: receiving sensor data from a sensor associated with a vehicle, the sensor data representing a first point associated with a drivable surface and a second point associated with a non-drivable surface at a different height than the drivable surface; determining, based on the sensor data, a first surface line representing the drivable surface and a second surface line representing the non-drivable surface; determining, based on the first surface line and the second surface line, a separation point between the first surface line and the second surface line; and controlling the vehicle based at least in part on the separation point.</p><p id="p-0105" num="0104">G: The method as recited in paragraph F, wherein: the first surface line and the second surface line are determined based at least in part on the first point and the second point; the sensor data further represents a third point associated with the drivable surface and a fourth point associated with the non-drivable surface; the method further comprises: determining, based on the third point and the fourth point, a third surface line representing the drivable surface and a fourth surface line representing the non-drivable surface; and determining, based on the third surface line and the fourth surface line, an additional separation point between the third surface line and the fourth surface line; and controlling the vehicle is further based at least in part on the additional separation point.</p><p id="p-0106" num="0105">H: The method as recited in either paragraph F or paragraph G, further comprising: generating a spatial line using the first point and the second point, wherein determining the first surface line and the second surface line are based at least in part on the spatial line.</p><p id="p-0107" num="0106">I: The method as recited in paragraph H, further comprising: determining that the first point is located within a distance range from the vehicle; and determining that the second point is located within the distance range, and wherein generating the spatial line using the first point and the second point is based at least in part on the first point being located within the distance range and the second point being located within the distance range.</p><p id="p-0108" num="0107">J: The method as recited in any one of paragraphs F-I, wherein receiving the sensor data occurs at a first time, and wherein the method further comprises: receiving, at a second time, additional sensor data from the sensor, the additional sensor data representing at least a third point associated with the drivable surface and a fourth point associated with the non-drivable surface, and wherein determining the first surface line and the second surface line is further based at least in part on the additional sensor data.</p><p id="p-0109" num="0108">K: The method as recited in any one of paragraphs F-J, further comprising: determining a distance between the first point associated with the drivable surface and the second point associated with the non-drivable surface; determining that the distance is equal to or greater than a threshold distance; and determining that the separation point is associated with a curb based at least in part on the distance being equal to or greater than the threshold distance.</p><p id="p-0110" num="0109">L: The method as recited in any one of paragraphs F-K, further comprising: determining a first probability associated with the first point; determining that the first probability is equal to or greater than a threshold probability; determining a second probability associated with the second point; and determining that the second probability is equal to or greater than the threshold probability, and wherein determining the first surface line and the second surface line uses at least the first point and the second point based at least in part on the first probability being equal to or greater than the threshold probability and the second probability being equal to or greater than the threshold probability.</p><p id="p-0111" num="0110">M: The method as recited in any one of paragraphs F-L, further comprising: determining that the first point is associated with the driving surface; and determining that the second point is associated with the non-drivable surface, and wherein determining the first surface line and the second surface line uses at least the first point and the second point based at least in part on the first point being associated with the driving surface and the second point being associated with the non-drivable surface.</p><p id="p-0112" num="0111">N: The method as recited in any one of paragraphs F-M further comprising: determining that the first point is located within a threshold distance to the vehicle; and determining that the second point is located within the threshold distance, and wherein determining the first surface line and the second surface line uses at least the first point and the second point based at least in part on the first point being located within the threshold distance and the second point being located within the threshold distance.</p><p id="p-0113" num="0112">O: The method as recited in any one of paragraphs F-N, wherein determining the separation point comprises at least: determining a first difference between a first location associated with the first point the first surface line; determining a second difference between a second location associated with the second point and the second surface line; determining an energy based at least in part on the first difference and the second difference; and determining the separation point based at least in part on the energy.</p><p id="p-0114" num="0113">P: The method as recited in any one of paragraphs F-O, further comprising: determining, based at least in part on the sensor data, a third surface line representing the drivable surface and a fourth surface line representing the non-drivable surface; determining, based on the third surface line and the fourth surface line, an additional separation point between the third surface line and the fourth surface line; and generating, based at least in part on the separation point and the additional separation point, a curve that represents a curb, wherein controlling the vehicle is based at least in part on the curve.</p><p id="p-0115" num="0114">Q: The method as recited in any one of paragraph F-P, further comprising: generating an additional curve based at least in part on the separation point and the additional separation point; determining a first position associated with the separation point on the additional curve; determining an energy based at least in part on the first position associated with the separation point and a second position associated with the additional separation point; determining a third position for the separation point based at least in part on the energy; and generating the curve based at least in part on the third position.</p><p id="p-0116" num="0115">R: The method as recited in any one of paragraphs F-Q, further comprising: determining a first coordinate associated with the first point; determining a second coordinate associated with the second point; and determining a height of the curb based at least in part on the first coordinate and the second coordinate.</p><p id="p-0117" num="0116">S: One or more non-transitory computer-readable media storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising: receiving sensor data from a sensor associated with a vehicle, the sensor data representing a first point associated with a drivable surface and a second point associated with a non-drivable surface at a different height than the drivable surface; determining, based on the sensor data, a first surface line representing the drivable surface and a second surface line representing the non-drivable surface; determining, based on the first surface line and the second surface line, a separation point between the first surface line and the second surface line; and controlling the vehicle based at least in part on the separation point.</p><p id="p-0118" num="0117">T: The one or more non-transitory computer-readable media as recited in paragraph S, wherein: the first surface line and the second surface line are determined based at least in part on the first point and the second point; the sensor data further represents a third point associated with the drivable surface and a fourth point associated with the non-drivable surface; the operations further comprise: determining, based on the third point and the fourth point, a third surface line representing the drivable surface and a fourth surface line representing the non-drivable surface; and determining, based on the third surface line and the fourth surface line, an additional separation point between the third surface line and the fourth surface line; and controlling the vehicle is further based at least in part on the additional separation point.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system comprising:<claim-text>one or more processors; and</claim-text><claim-text>one or more computer-readable media storing instructions that, when executed by the one or more processors, cause the one or more processors to perform operations comprising:<claim-text>receiving sensor data from a lidar sensor, the sensor data representing first points associated with a drivable surface and second points associated with a sidewalk;</claim-text><claim-text>generating spatial lines using the first points associated with the drivable surface and the second points associated with the sidewalk;</claim-text><claim-text>determining, for the spatial lines, separation points between the first points associated with the drivable surface and the second points associated with the sidewalk;</claim-text><claim-text>generating, based at least in part on the separation points, a curve that represents a curb between the drivable surface and the sidewalk; and</claim-text><claim-text>controlling a vehicle based at least in part on the curve.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein receiving the sensor data occurs at a first time, and wherein the operations further comprise:<claim-text>receiving, at a second time, additional sensor data from the lidar sensor, the additional sensor data representing third points associated with the drivable surface and fourth points associated with the sidewalk,</claim-text><claim-text>and wherein generating the spatial lines further uses the third points and the fourth points.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The vehicle as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, the operations further comprising:<claim-text>determining, for a spatial line from the spatial lines, a distance between a first point associated with the drivable surface and a second point associated with the sidewalk;</claim-text><claim-text>determining that the distance is equal to or greater than a threshold distance; and</claim-text><claim-text>determining that a separation point associated with the spatial line is associated with the curb based at least in part on the distance being equal to or greater than the threshold distance.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The vehicle as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining a separation point from the separation points comprises at least:<claim-text>determining, based at least in part a candidate separation point, a first surface line associated with a first portion of the sensor data and a second surface line associated with a second portion of the sensor data;</claim-text><claim-text>determining, based at least in part on a first difference between the first surface line and a first point of the first portion of the sensor data, a first energy;</claim-text><claim-text>determining, based at least in part on a second difference between the second surface line an second point of the second portion of the sensor data, a second energy; and</claim-text><claim-text>determining that the candidate separation point is a separation point based at least in part on the first energy and the second energy.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The vehicle as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating a spatial line from the spatial lines comprises at least:<claim-text>analyzing the sensor data to identify at least a portion of the first points that are located within a distance range from the vehicle;</claim-text><claim-text>analyzing the sensor data to identify at least a portion of the second points that are located within the distance range; and</claim-text><claim-text>generating the spatial line to include at least the portion of the first points and the portion of the second points.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. A method comprising:<claim-text>receiving sensor data from a sensor associated with a vehicle, the sensor data representing a first point associated with a drivable surface and a second point associated with a non-drivable surface at a different height than the drivable surface;</claim-text><claim-text>determining, based on the sensor data, a first surface line representing the drivable surface and a second surface line representing the non-drivable surface;</claim-text><claim-text>determining, based on the first surface line and the second surface line, a separation point between the first surface line and the second surface line; and</claim-text><claim-text>controlling the vehicle based at least in part on the separation point.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein:<claim-text>the first surface line and the second surface line are determined based at least in part on the first point and the second point;</claim-text><claim-text>the sensor data further represents a third point associated with the drivable surface and a fourth point associated with the non-drivable surface;</claim-text><claim-text>the method further comprises:<claim-text>determining, based on the third point and the fourth point, a third surface line representing the drivable surface and a fourth surface line representing the non-drivable surface; and</claim-text><claim-text>determining, based on the third surface line and the fourth surface line, an additional separation point between the third surface line and the fourth surface line; and</claim-text></claim-text><claim-text>controlling the vehicle is further based at least in part on the additional separation point.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>generating a spatial line using the first point and the second point,</claim-text><claim-text>wherein determining the first surface line and the second surface line are based at least in part on the spatial line.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>determining that the first point is located within a distance range from the vehicle; and</claim-text><claim-text>determining that the second point is located within the distance range,</claim-text><claim-text>and wherein generating the spatial line using the first point and the second point is based at least in part on the first point being located within the distance range and the second point being located within the distance range.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein receiving the sensor data occurs at a first time, and wherein the method further comprises:<claim-text>receiving, at a second time, additional sensor data from the sensor, the additional sensor data representing at least a third point associated with the drivable surface and a fourth point associated with the non-drivable surface,</claim-text><claim-text>and wherein determining the first surface line and the second surface line is further based at least in part on the additional sensor data.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>determining a distance between the first point associated with the drivable surface and the second point associated with the non-drivable surface;</claim-text><claim-text>determining that the distance is equal to or greater than a threshold distance; and</claim-text><claim-text>determining that the separation point is associated with a curb based at least in part on the distance being equal to or greater than the threshold distance.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>determining a first probability associated with the first point;</claim-text><claim-text>determining that the first probability is equal to or greater than a threshold probability;</claim-text><claim-text>determining a second probability associated with the second point; and</claim-text><claim-text>determining that the second probability is equal to or greater than the threshold probability,</claim-text><claim-text>and wherein determining the first surface line and the second surface line uses at least the first point and the second point based at least in part on the first probability being equal to or greater than the threshold probability and the second probability being equal to or greater than the threshold probability.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>determining that the first point is associated with the driving surface; and</claim-text><claim-text>determining that the second point is associated with the non-drivable surface,</claim-text><claim-text>and wherein determining the first surface line and the second surface line uses at least the first point and the second point based at least in part on the first point being associated with the driving surface and the second point being associated with the non-drivable surface.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>determining that the first point is located within a threshold distance to the vehicle; and</claim-text><claim-text>determining that the second point is located within the threshold distance,</claim-text><claim-text>and wherein determining the first surface line and the second surface line uses at least the first point and the second point based at least in part on the first point being located within the threshold distance and the second point being located within the threshold distance.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein determining the separation point comprises at least:<claim-text>determining a first difference between a first location associated with the first point the first surface line;</claim-text><claim-text>determining a second difference between a second location associated with the second point and the second surface line;</claim-text><claim-text>determining an energy based at least in part on the first difference and the second difference; and</claim-text><claim-text>determining the separation point based at least in part on the energy.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>determining, based at least in part on the sensor data, a third surface line representing the drivable surface and a fourth surface line representing the non-drivable surface;</claim-text><claim-text>determining, based on the third surface line and the fourth surface line, an additional separation point between the third surface line and the fourth surface line; and</claim-text><claim-text>generating, based at least in part on the separation point and the additional separation point, a curve that represents a curb,</claim-text><claim-text>wherein controlling the vehicle is based at least in part on the curve.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method as recited in <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising:<claim-text>generating an additional curve based at least in part on the separation point and the additional separation point;</claim-text><claim-text>determining a first position associated with the separation point on the additional curve;</claim-text><claim-text>determining an energy based at least in part on the first position associated with the separation point and a second position associated with the additional separation point;</claim-text><claim-text>determining a third position for the separation point based at least in part on the energy; and</claim-text><claim-text>generating the curve based at least in part on the third position.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method as recited in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>determining a first coordinate associated with the first point;</claim-text><claim-text>determining a second coordinate associated with the second point; and</claim-text><claim-text>determining a height of the curb based at least in part on the first coordinate and the second coordinate.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. One or more non-transitory computer-readable media storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:<claim-text>receiving sensor data from a sensor associated with a vehicle, the sensor data representing a first point associated with a drivable surface and a second point associated with a non-drivable surface at a different height than the drivable surface;</claim-text><claim-text>determining, based on the sensor data, a first surface line representing the drivable surface and a second surface line representing the non-drivable surface;</claim-text><claim-text>determining, based on the first surface line and the second surface line, a separation point between the first surface line and the second surface line; and</claim-text><claim-text>controlling the vehicle based at least in part on the separation point.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The one or more non-transitory computer-readable media as recited in <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein:<claim-text>the first surface line and the second surface line are determined based at least in part on the first point and the second point;</claim-text><claim-text>the sensor data further represents a third point associated with the drivable surface and a fourth point associated with the non-drivable surface;</claim-text><claim-text>the operations further comprise:<claim-text>determining, based on the third point and the fourth point, a third surface line representing the drivable surface and a fourth surface line representing the non-drivable surface; and</claim-text><claim-text>determining, based on the third surface line and the fourth surface line, an additional separation point between the third surface line and the fourth surface line; and</claim-text></claim-text><claim-text>controlling the vehicle is further based at least in part on the additional separation point.</claim-text></claim-text></claim></claims></us-patent-application>