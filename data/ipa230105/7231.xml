<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007232A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007232</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782522</doc-number><date>20201023</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-228694</doc-number><date>20191218</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>383</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>117</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>31</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>383</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>117</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>31</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>303</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION PROCESSING DEVICE AND INFORMATION PROCESSING METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>TOKYO</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>YOSHIDA</last-name><first-name>AKIRA</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>ARAI</last-name><first-name>RYOTA</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/040017</doc-number><date>20201023</date></document-id><us-371c12-date><date>20220603</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Provided is an information processing device that performs processing on a content. An information processing device is provided with an estimation unit that estimates sounding coordinates at which a sound image is generated on the basis of a video stream and an audio stream, a video output control unit that controls an output of the video stream, and an audio output control unit that controls an output of the audio stream so as to generate the sound image at the sounding coordinates. A discrimination unit that discriminates a gazing point of a user who views video and audio is further provided, in which the estimation unit estimates the sounding coordinates at which the sound image of the object gazed by the user is generated on the basis of a discrimination result.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="100.75mm" wi="158.75mm" file="US20230007232A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="215.39mm" wi="149.27mm" orientation="landscape" file="US20230007232A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="223.18mm" wi="147.32mm" orientation="landscape" file="US20230007232A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="130.05mm" wi="152.82mm" file="US20230007232A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="229.19mm" wi="146.47mm" file="US20230007232A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="202.78mm" wi="111.68mm" orientation="landscape" file="US20230007232A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="208.20mm" wi="151.72mm" orientation="landscape" file="US20230007232A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="208.11mm" wi="151.72mm" orientation="landscape" file="US20230007232A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="208.20mm" wi="151.72mm" orientation="landscape" file="US20230007232A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="184.49mm" wi="97.45mm" file="US20230007232A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="202.86mm" wi="156.13mm" orientation="landscape" file="US20230007232A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="208.62mm" wi="151.72mm" orientation="landscape" file="US20230007232A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="208.70mm" wi="151.72mm" orientation="landscape" file="US20230007232A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="179.75mm" wi="151.81mm" file="US20230007232A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="175.51mm" wi="111.68mm" orientation="landscape" file="US20230007232A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="208.87mm" wi="151.72mm" orientation="landscape" file="US20230007232A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="209.47mm" wi="106.85mm" file="US20230007232A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="218.27mm" wi="153.16mm" orientation="landscape" file="US20230007232A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="175.18mm" wi="144.44mm" orientation="landscape" file="US20230007232A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="203.28mm" wi="146.81mm" orientation="landscape" file="US20230007232A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="176.95mm" wi="134.11mm" orientation="landscape" file="US20230007232A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="207.35mm" wi="132.08mm" orientation="landscape" file="US20230007232A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="102.36mm" wi="147.91mm" file="US20230007232A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="198.20mm" wi="130.98mm" orientation="landscape" file="US20230007232A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="208.36mm" wi="151.55mm" orientation="landscape" file="US20230007232A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="208.70mm" wi="130.98mm" orientation="landscape" file="US20230007232A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The technology disclosed in this specification (hereinafter, &#x201c;the present disclosure&#x201d;) relates to an information processing device and an information processing method that perform processing related to a content viewed by a user.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">Currently, research and development related to a technology for controlling viewing processing of free viewpoint video on the basis of posture information of a viewer wearing a head-mounted display are being conducted. For example, Patent Document 1 discloses a technology enabling viewing of viewpoint video corresponding to a direction of a line of sight of a user by detecting a posture of a head or a body of a viewer and drawing the viewpoint video corresponding to the same.</p><p id="p-0004" num="0003">Furthermore, in recent years, research and development have also been conducted on a technology of combining a display device and a sensing technology to detect a position of an eye and a line of sight of a viewer to perform display processing on a video content. For example, in Patent Document 2, detected eye position and line of sight of a viewer are used for display control of a stereoscopic image. As in these examples, by performing display control of the video on the basis of the detection result of the positional relationship between the viewer and the display video, the expression of the more extended video content is implemented.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Document</heading><p id="p-0005" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0004">Patent Document 1: Japanese Patent No. 6572893</li>    <li id="ul0001-0002" num="0005">Patent Document 2: International Publication No. 2018/116580</li></ul></p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Problems to be Solved by the Invention</heading><p id="p-0006" num="0006">An object of the present disclosure is to provide an information processing device and an information processing method that perform processing on the basis of a gazing point of a user for a content.</p><heading id="h-0007" level="1">Solutions to Problems</heading><p id="p-0007" num="0007">A first aspect of the present disclosure is</p><p id="p-0008" num="0008">an information processing device provided with:</p><p id="p-0009" num="0009">an estimation unit that estimates sounding coordinates at which a sound image is generated on the basis of a video stream and an audio stream;</p><p id="p-0010" num="0010">a video output control unit that controls an output of the video stream; and</p><p id="p-0011" num="0011">an audio output control unit that controls an output of the audio stream so as to generate the sound image at the sounding coordinates.</p><p id="p-0012" num="0012">The information processing device according to the first aspect is provided with a discrimination unit that discriminates a gazing point of a user who views video and audio. Then, the estimation unit estimates the sounding coordinates at which the sound image of the object gazed by the user is generated on the basis of a discrimination result of the discrimination unit. The discrimination unit discriminates the gazing point of the user on the basis of a result of detecting a state related to the user.</p><p id="p-0013" num="0013">Furthermore, the video output control unit performs rendering of video including at least one of framing or zooming processing of the video on the basis of a result of discriminating a gazing degree of the user. The video output control unit performs the rendering on the basis of a result of tracking the object gazed by the user.</p><p id="p-0014" num="0014">Furthermore, the information processing device according to the first aspect is provided with an acquisition unit that acquires related information of the object discriminated on the basis of a feature of the object corresponding to the gazing point, and a related information output control unit that controls an output of the acquired related information. Then, the video output control unit controls to output the related information together with the video stream.</p><p id="p-0015" num="0015">Furthermore, a second aspect of the present disclosure is</p><p id="p-0016" num="0016">an information processing method provided with:</p><p id="p-0017" num="0017">an estimation step of estimating sounding coordinates at which a sound image is generated on the basis of a video stream and an audio stream;</p><p id="p-0018" num="0018">a video output control step of controlling an output of the video stream; and</p><p id="p-0019" num="0019">an audio output control step of controlling an output of the audio stream so as to generate the sound image at the sounding coordinates.</p><heading id="h-0008" level="1">Effects of the Invention</heading><p id="p-0020" num="0020">According to the present disclosure, it is possible to provide an information processing device and an information processing method that perform reproduction control of a content based on a gazing point of a user using artificial intelligence.</p><p id="p-0021" num="0021">Note that, the effect described in this specification is illustrative only and the effect by the present invention is not limited to this. Furthermore, there also is a case in which the present disclosure further has an additional effect in addition to the above-described effect.</p><p id="p-0022" num="0022">Still another object, feature, and advantage of the present disclosure will become clear by further detailed description with reference to an embodiment to be described later and the attached drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0023" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a view illustrating a configuration example of a system for viewing a video content.</p><p id="p-0024" num="0024"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view illustrating a configuration example of a content reproduction device <b>100</b>.</p><p id="p-0025" num="0025"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a view illustrating an example of a content viewing environment in which a 3D display and a sound image localization technology are combined.</p><p id="p-0026" num="0026"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a view illustrating a configuration example of a sensor unit <b>109</b>.</p><p id="p-0027" num="0027"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a view illustrating a functional configuration example of a video signal processing unit <b>105</b>.</p><p id="p-0028" num="0028"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a view illustrating a functional configuration example of an artificial intelligence server <b>600</b> that performs deep learning on a neural network for gazing point discrimination processing.</p><p id="p-0029" num="0029"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a view illustrating a functional configuration example of an artificial intelligence server <b>700</b> that performs deep learning on a neural network for object feature extraction processing.</p><p id="p-0030" num="0030"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a view illustrating a functional configuration example of an artificial intelligence server <b>800</b> that performs deep learning on a neural network for video output control.</p><p id="p-0031" num="0031"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating a procedure of video signal processing performed in the video signal processing unit <b>105</b> illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0032" num="0032"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a view illustrating another functional configuration example of the video signal processing unit <b>105</b>.</p><p id="p-0033" num="0033"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a view illustrating a functional configuration example of an artificial intelligence server <b>1100</b> that performs deep learning on a neural network for object discrimination processing.</p><p id="p-0034" num="0034"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a view illustrating a functional configuration of an artificial intelligence server <b>1200</b> that performs deep learning on a neural network for related information retrieval acquisition processing.</p><p id="p-0035" num="0035"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart illustrating a procedure of video signal processing performed in the video signal processing unit <b>105</b> illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0036" num="0036"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a view illustrating a functional configuration example of the signal processing unit <b>150</b> that performs image sound matching processing.</p><p id="p-0037" num="0037"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a view illustrating a functional configuration example of an artificial intelligence server <b>1500</b> that performs deep learning on a neural network for sounding coordinate estimation processing.</p><p id="p-0038" num="0038"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a flowchart illustrating a procedure of the image sound matching processing executed in the signal processing unit <b>150</b>.</p><p id="p-0039" num="0039"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a view illustrating a functional configuration example of the signal processing unit <b>150</b> that performs the image sound matching processing.</p><p id="p-0040" num="0040"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a view illustrating an example of performing framing and zooming processing on video on the basis of a gazing point of a user.</p><p id="p-0041" num="0041"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a view illustrating an example of performing framing and zooming processing on video on the basis of a gazing point of a user.</p><p id="p-0042" num="0042"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a view illustrating an example of performing framing and zooming processing on video on the basis of a gazing point of a user.</p><p id="p-0043" num="0043"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a view illustrating an example of performing framing and zooming processing on video on the basis of a gazing point of a user.</p><p id="p-0044" num="0044"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a view illustrating an example of performing framing and zooming processing on video on the basis of a gazing point of a user.</p><p id="p-0045" num="0045"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a view illustrating a functional configuration example of an artificial intelligence server <b>2300</b> that performs image creation on the basis of the gazing point of the user.</p><p id="p-0046" num="0046"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a view illustrating a functional configuration example of an artificial intelligence server <b>2400</b> that performs the image creation and image sound matching processing on the basis of the gazing point of the user.</p><p id="p-0047" num="0047"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a view illustrating a functional configuration example of an artificial intelligence server <b>2500</b> that outputs related information of the object gazed by the user.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">MODE FOR CARRYING OUT THE INVENTION</heading><p id="p-0048" num="0048">An embodiment according to the present disclosure is hereinafter described in detail with reference to the drawings.</p><p id="p-0049" num="0049">A. System Configuration</p><p id="p-0050" num="0050"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates a configuration example of a system for viewing a video content.</p><p id="p-0051" num="0051">A content reproduction device <b>100</b> is, for example, a television receiver installed in a living room where a family has a happy family circle, a private room of a user and the like in a home. Note that, the content reproduction device <b>100</b> is not necessarily limited to a stationary device such as the television receiver, and may be, for example, a small or portable device such as a personal computer, a smartphone, a tablet, and a head-mounted display. Furthermore, in this embodiment, unless otherwise specified, the simple term &#x201c;user&#x201d; refers to a viewer who views a video content displayed on the content reproduction device <b>100</b> (including a case where the viewer has a plan to view).</p><p id="p-0052" num="0052">The content reproduction device <b>100</b> is equipped with a display that displays the video content and a speaker that outputs a sound. The content reproduction device <b>100</b> includes, for example, a built-in tuner for selecting and receiving a broadcast signal, or an externally-connected set top box having a tuner function, and may use a broadcast service provided by a television station. The broadcast signal may be either a terrestrial wave or a satellite wave.</p><p id="p-0053" num="0053">Furthermore, the content reproduction device <b>100</b> may also use a moving image distribution service using a network such as IPTV, OTT, or a moving image sharing service, for example. Therefore, the content reproduction device <b>100</b> is equipped with a network interface card, and is interconnected to an external network such as the Internet via a router or an access point using communication based on an existing communication standard such as Ethernet (registered trademark) or Wi-Fi (registered trademark). The content reproduction device <b>100</b> also is, in a functional aspect thereof, a content acquisition device, a content reproduction device, or a display device equipped with a display having a function of acquiring or reproducing various types of contents to acquire various reproduction contents such as video and audio by streaming or downloading via a broadcast wave or the Internet to present to the user. Furthermore, although not illustrated, a medium reproduction device is connected to the content reproduction device <b>100</b> via a high-definition multimedia interface (HDMI (registered trademark)) interface, and a content reproduced from a recording medium such as a hard disk drive (HDD) or Blu-ray is input thereto.</p><p id="p-0054" num="0054">A stream distribution server that distributes video streams is installed on the Internet, and provides a broadcast type moving image distribution service to the content reproduction device <b>100</b>.</p><p id="p-0055" num="0055">Furthermore, an infinite number of servers that provide various services are installed on the Internet. An example of the server is, for example, a stream distribution server that provides a distribution service of moving image streams using a network such as IPTV, OTT, and a moving image sharing service. A side of the content reproduction device <b>100</b> may activate a browser function, and issue a hyper text transfer protocol (HTTP) request, for example, to the stream distribution server, thereby using the stream distribution service.</p><p id="p-0056" num="0056">Furthermore, in this embodiment, it is assumed that there also is an artificial intelligence server that provides a function of artificial intelligence to a client on the Internet (alternatively, on a cloud). The artificial intelligence is, for example, a function of artificially implementing, by software or hardware, a function exhibited by a human brain such as learning, inference, data creation, and planning. The function of the artificial intelligence may be implemented using a machine learning model represented by a neural network that simulates a human cranial nerve circuit.</p><p id="p-0057" num="0057">The machine learning model is a calculation model having variability used for the artificial intelligence that changes a model structure through learning (training) accompanied with an input of learning data. In the neural network, in a case of using a brain-type (neuromorphic) computer, a node is also referred to as an artificial neuron (or simply &#x201c;neuron&#x201d;) via a synapse. The neural network has a network structure formed by connection between nodes (neurons), and generally includes an input layer, a hidden layer, and an output layer. Learning of the machine learning model represented by the neural network is performed through processing of changing the neural network by inputting data (learning data) to the neural network to perform learning of a connection degree (hereinafter, also referred to as a &#x201c;connection weight coefficient&#x201d;) between the nodes (neurons). By using a learned machine learning model, an optimal solution (output) for a question (input) may be estimated. The machine learning model is handled as, for example, set data of the connection weight coefficients between the nodes (neurons).</p><p id="p-0058" num="0058">Here, the neural network may have various algorithms, forms, and structures according to purposes such as a convolutional neural network (CNN), a recurrent neural network (RNN), a generative adversarial network, a variational autoencoder, a self-organizing feature map, and a spiking neural network (SNN), and they may be combined in any manner.</p><p id="p-0059" num="0059">The artificial intelligence server applied to the present disclosure is assumed to be equipped with a multi-stage neural network capable of performing deep learning (DL). In a case of performing deep learning, the number of learning data and the number of nodes (neurons) are also large. Therefore, it is considered appropriate to perform deep learning using huge computer resources such as the cloud.</p><p id="p-0060" num="0060">The &#x201c;artificial intelligence server&#x201d; mentioned in this specification is not limited to a single server device, and may have, for example, a form of the cloud that provides a cloud computing service to the user via another device, and outputs and provides a service result (product) to the other device.</p><p id="p-0061" num="0061">Furthermore, a &#x201c;client&#x201d; (hereinafter, also referred to as a terminal, a sensor device, or an edge device) in this specification is at least characterized in downloading from the artificial intelligence server the machine learning model learning of which is finished by the artificial intelligence server as the result of the service by the artificial intelligence server to perform processing such as inference and object detection using the downloaded machine learning model, or receiving sensor data inferred by the artificial intelligence server using the machine learning model as the product of the service to perform processing such as inference and object detection. The client may further have a learning function using a relatively small-scale neural network, thereby performing deep learning in cooperation with the artificial intelligence server.</p><p id="p-0062" num="0062">Note that, the above-described technology of the brain-type computer and the technology of other artificial intelligence are not independent from each other, and may be used in cooperation with each other. For example, a representative technology in the neuromorphic computer includes SNN (described above). By using the SNN technology, output data from an image sensor and the like, for example, may be used as data provided as an input of deep learning in a format differentiated along time axis on the basis of input data series. Therefore, in this specification, unless otherwise specified, the neural network is handled as a type of the technology of the artificial intelligence using the brain-type computer technology.</p><p id="p-0063" num="0063">B. Device Configuration</p><p id="p-0064" num="0064"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a configuration example of the content reproduction device <b>100</b>. The illustrated content reproduction device <b>100</b> is provided with an external interface unit <b>110</b> that performs data exchange with the outside, such as reception of the content. The external interface unit <b>110</b> herein mentioned is equipped with a tuner for selecting and receiving a broadcast signal, an HDMI (registered trademark) interface for inputting a reproduction signal from a medium reproduction device, and a network interface (NIC) for connecting to a network, and has functions such as data reception from a medium such as broadcasting and the cloud, and reading and retrieval of data from the cloud.</p><p id="p-0065" num="0065">The external interface unit <b>110</b> has a function of acquiring the content provided to the content reproduction device <b>100</b>. As a mode in which the content is provided to the content reproduction device <b>100</b>, the broadcast signal of terrestrial broadcasting, satellite broadcasting and the like, the reproduction signal reproduced from the recording medium such as the hard disk drive (HDD) or Blu-ray, a streaming content distributed from the stream distribution server on the cloud and the like is assumed. The broadcast type moving image distribution service using the network may include IPTV, OTT, the moving image sharing service and the like. Then, these contents are supplied to the content reproduction device <b>100</b> as a multiplexed bit stream obtained by multiplexing bit streams of respective media data such as video, audio, and auxiliary data (subtitles, text, graphics, program information and the like). In the multiplexed bit stream, for example, it is assumed that data of respective media such as video and audio are multiplexed in compliance with the MPEG-2 Systems standard.</p><p id="p-0066" num="0066">Note that, the video streams provided from the broadcasting station, the stream distribution server, or the recording medium are assumed to include both 2D and 3D. The 3D video may be free viewpoint video. The 2D video may include a plurality of videos imaged from a plurality of viewpoints. Furthermore, it is assumed that audio streams provided from the broadcasting station, the stream distribution server, or the recording medium include object-based audio in which individual sounding objects are not mixed.</p><p id="p-0067" num="0067">Furthermore, in this embodiment, it is assumed that the external interface unit <b>110</b> acquires a machine learning model learning of which is performed by deep learning and the like by the artificial intelligence server on the cloud. For example, the external interface unit <b>110</b> acquires the machine learning model for video signal processing and the machine learning model for audio signal processing (to be described later).</p><p id="p-0068" num="0068">The content reproduction device <b>100</b> is provided with a demultiplexer <b>101</b>, a video decoding unit <b>102</b>, an audio decoding unit <b>103</b>, an auxiliary data decoding unit <b>104</b>, a video signal processing unit <b>105</b>, an audio signal processing unit <b>106</b>, an image display unit <b>107</b>, and an audio output unit <b>108</b>. Note that, the content reproduction device <b>100</b> may be a terminal device such as a set top box and configured to process the received multiplexed bit stream and output processed video and audio signals to another device provided with the image display unit <b>107</b> and the audio output unit <b>108</b>.</p><p id="p-0069" num="0069">The demultiplexer <b>101</b> demultiplexes the multiplexed bit stream externally received as the broadcast signal, reproduction signal, or streaming data into a video bit stream, an audio bit stream, and an auxiliary bit stream, and distributes them to the video decoding unit <b>102</b>, the audio decoding unit <b>103</b>, and the auxiliary data decoding unit <b>104</b>, respectively, on a subsequent stage.</p><p id="p-0070" num="0070">The video decoding unit <b>102</b> decodes, for example, an MPEG-encoded video bit stream, and outputs a baseband video signal. Note that, it is also conceivable that the video signal output from the video decoding unit <b>102</b> is low resolution or standard resolution video, or low dynamic range (LDR) or standard dynamic range (SDR) video.</p><p id="p-0071" num="0071">The audio decoding unit <b>103</b> decodes the audio bit stream encoded by an encoding system such as MPEG Audio Layer 3 (MP3) or High Efficiency MPEG4 Advanced Audio Coding (HE-AAC), for example, and outputs a baseband audio signal. Note that, the audio signal output from the audio decoding unit <b>103</b> is assumed to be a low resolution or standard resolution audio signal in which a partial band such as a high range is removed or compressed.</p><p id="p-0072" num="0072">The auxiliary data decoding unit <b>104</b> decodes the encoded auxiliary bit stream and outputs the subtitles, text, graphics, program information and the like.</p><p id="p-0073" num="0073">The content reproduction device <b>100</b> is provided with a signal processing unit <b>150</b> that performs signal processing and the like of the reproduction content. The signal processing unit <b>150</b> includes the video signal processing unit <b>105</b> and the audio signal processing unit <b>106</b>.</p><p id="p-0074" num="0074">The video signal processing unit <b>105</b> applies video signal processing to the video signal output from the video decoding unit <b>102</b> and the subtitles, text, graphics, program information and the like output from the auxiliary data decoding unit <b>104</b>. The video signal processing herein mentioned may include image quality enhancement processing such as noise reduction, resolution conversion processing such as super resolution processing, dynamic range conversion processing, and gamma processing. In a case where the video signal output from the video decoding unit <b>102</b> is the low resolution or standard resolution video or the low dynamic range or standard dynamic range video, the video signal processing unit <b>105</b> performs the image quality enhancement processing such as super resolution processing of generating a high resolution video signal from a low resolution or standard resolution video signal, dynamic range expansion and the like. The video signal processing unit <b>105</b> may perform the video signal processing after synthesizing the video signal of a main story output from the video decoding unit <b>102</b> and the auxiliary data such as the subtitles output from the auxiliary data decoding unit <b>104</b>, or may perform synthesis processing after individually performing the image quality enhancement processing on the video signal of the main story and the auxiliary data. In any case, the video signal processing unit <b>105</b> performs the video signal processing such as the super resolution processing and dynamic range expansion within a range of screen resolution or a luminance dynamic range allowed by the image display unit <b>107</b>, which is an output destination of the video signal.</p><p id="p-0075" num="0075">Furthermore, the video signal processing unit <b>105</b> further performs processing such as tracking, framing, and zooming of a specific object on the video on the basis of a gazing point of the user and the like. The framing may include processing such as viewpoint switching and line-of-sight change.</p><p id="p-0076" num="0076">In this embodiment, the video signal processing unit <b>105</b> is assumed to perform the video signal processing as described above by the machine learning model. It is expected to implement optimal video signal processing by using the machine learning model preliminary learning of which by deep learning is performed by the artificial intelligence server on the cloud.</p><p id="p-0077" num="0077">The audio signal processing unit <b>106</b> applies audio signal processing to the audio signal output from the audio decoding unit <b>103</b>. The audio signal output from the audio decoding unit <b>103</b> is a low resolution or standard resolution audio signal in which a partial band such as a high range is removed or compressed. The audio signal processing unit <b>106</b> may perform sound quality enhancement processing of performing band extension of the low resolution or standard resolution audio signal to a high resolution audio signal including a removed or compressed band. Furthermore, the audio signal processing unit <b>106</b> performs processing of applying effects such as reflection, diffraction, and interference of an output sound. Furthermore, the audio signal processing unit <b>106</b> may perform sound image localization processing using a plurality of speakers in addition to sound quality enhancement such as band extension. The sound image localization processing is implemented by determining a direction and a volume of the sound at a position of a sound image to be localized (hereinafter, also referred to as &#x201c;sounding coordinates&#x201d;) and determining a combination of the speakers for generating the sound image and directivity and volume of each speaker. Then, the audio signal processing unit <b>106</b> outputs the audio signal from each speaker.</p><p id="p-0078" num="0078">Note that, the audio signal handled in this embodiment may be &#x201c;object-based audio&#x201d; obtained by supplying individual sounding objects without mixing and rendering on a reproduction device side. In the object-based audio, data of the object-based audio includes a waveform signal for the sounding object (object as a sound source in a video frame (an object hidden from the video may be included) and localization information of the sounding object represented by a relative position from a listening position as a predetermined reference as meta information. The wave signal of the sounding object is rendered to audio signals of the desired number of channels by vector based amplitude panning (VBAP), for example, on the basis of the meta information to be reproduced. The audio signal processing unit <b>106</b> may designate the position of the sounding object by using the audio signal conforming to the object-based audio, and easily implement more robust stereophonic sound.</p><p id="p-0079" num="0079">In this embodiment, it is assumed that the audio signal processing unit <b>106</b> performs the audio signal processing such as the band extension, effects, and sound image localization by the machine learning model. It is expected to implement optimal audio signal processing by using the machine learning model preliminary learning of which by deep learning is performed by the artificial intelligence server on the cloud.</p><p id="p-0080" num="0080">Furthermore, a single machine learning model that performs the video signal processing and audio signal processing together may be used in the signal processing unit <b>150</b>. For example, in a case of performing processing such as tracking, framing (including viewpoint switching and line-of-sight change), and zooming of an object as the video signal processing using the machine learning model in the signal processing unit <b>150</b> (as described above), the sound image position may be controlled in conjunction with the change in position of the object in the frame.</p><p id="p-0081" num="0081">The image display unit <b>107</b> presents a screen on which video subjected to the video signal processing such as image quality enhancement by the video signal processing unit <b>105</b> is displayed to the user (the viewer of the content and the like). The image display unit <b>107</b> is a display device including, for example, a liquid crystal display, an organic electro-luminescence (EL) display, a self-luminous display using fine light emitting diode (LED) elements as pixels or the like.</p><p id="p-0082" num="0082">Furthermore, the image display unit <b>107</b> may be a display device to which a partial driving technology of dividing the screen into a plurality of areas and controlling brightness for each area is applied. In a case of the display using a transmissive liquid crystal panel, luminance contrast may be improved by brightly lighting a backlight corresponding to an area with a high signal level and darkly lighting a backlight corresponding to an area with a low signal level. In this type of partial driving display device, it is possible to implement a high dynamic range by increasing the luminance in a case where white display is partially performed (while keeping output power of an entire backlight constant) by further utilizing a push-up technology of allocating power suppressed in a dark portion to the area with a high signal level to intensively emit light.</p><p id="p-0083" num="0083">Alternatively, the image display unit <b>107</b> may be a 3D display or a display capable of switching between 2D video display and 3D video display. Furthermore, the 3D display may be a display provided with a screen enabling stereoscopic viewing, such as a 3D display with naked eyes or glasses, a holographic display enabling viewing of different videos according to a line-of-sight direction and improving depth perception, or a light-field display. Note that, examples of the naked-eye 3D display include, for example, a display using binocular parallax such as a parallax barrier system or a lenticular lens system, and a multilayer display (MLD) that enhances a depth effect using a plurality of liquid crystal displays. In a case where the 3D display is used for the image display unit <b>107</b>, the user may enjoy stereoscopic video, so that a more effective viewing experience may be provided.</p><p id="p-0084" num="0084">Alternatively, the image display unit <b>107</b> may be a projector (or a movie theater that projects video using a projector). A projection mapping technique of projecting video on a wall surface having any shape or a projector stacking technique of superimposing projection videos of a plurality of projectors may be applied to the projector. If the projector is used, the video may be enlarged to be displayed on a relatively large screen, so that there is an advantage that the same video may be simultaneously presented to a plurality of persons. The audio output unit <b>108</b> outputs audio subjected to the audio signal processing such as sound quality enhancement by the audio signal processing unit <b>106</b>. The audio output unit <b>108</b> includes a sound generating element such as a speaker. For example, the audio output unit <b>108</b> may be a speaker array (multichannel speaker or super-multichannel speaker) obtained by combining a plurality of speakers.</p><p id="p-0085" num="0085">In addition to a cone speaker, a flat-panel speaker may be used as the audio output unit <b>108</b>. It goes without saying that a speaker array obtained by combining different types of speakers may be used as the audio output unit <b>108</b>. Furthermore, the speaker array may include one that performs audio output by vibrating the image display unit <b>107</b> by one or more vibrators (actuators) that generate vibration. The vibrator (actuator) may be retrofitted to the image display unit <b>107</b>.</p><p id="p-0086" num="0086">Furthermore, a part or all of the speakers forming the audio output unit <b>108</b> may be externally connected to the content reproduction device <b>100</b>. The external speaker may be installed in front of a television as with a sound bar, or may be wirelessly connected to the television as with a wireless speaker. Furthermore, the speaker may be connected to another audio product via an amplifier and the like. Alternatively, the external speaker may be a smart speaker equipped with a speaker to which audio may be input, a wired or wireless headphone/headset, a tablet, a smartphone, a personal computer (PC), or a so-called smart home appliance such as a refrigerator, a washing machine, an air conditioner, a vacuum cleaner, or a lighting fixture, or an Internet of things (IoT) home appliance.</p><p id="p-0087" num="0087">In a case where the audio output unit <b>108</b> is provided with a plurality of speakers, sound image localization may be performed by individually controlling the audio signals output from a plurality of output channels, respectively. Furthermore, by increasing the number of channels and multiplexing the speakers, it is possible to control a sound field with high resolution. For example, it is possible to generate a sound image at desired sounding coordinates by using a plurality of directional speakers in combination or arranging a plurality of speakers in an annular shape and adjusting a direction and volume of sound emitted from each speaker.</p><p id="p-0088" num="0088"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of a content viewing environment in which a 3D display and a sound image localization technology are combined. Under the illustrated environment, a 3D display <b>302</b> is arranged in an interior <b>301</b> serving as the content viewing environment. Furthermore, it is assumed that a plurality of directional speakers <b>311</b> to <b>315</b> is arranged so as to surround the user who views the content. Furthermore, directional speakers <b>321</b> and <b>322</b> are also arranged on left and right sides of the 3D display <b>302</b>. For example, the speakers <b>311</b> to <b>315</b> are used for outputting space sound, and the speakers <b>321</b> and <b>322</b> are used for outputting stage sound. Furthermore, it is possible to generate a sound image at desired sounding coordinates by changing the combination of the speakers that output or adjusting the direction and volume of the sound emitted from each speaker. Note that, <figref idref="DRAWINGS">FIG. <b>3</b></figref> is an example schematically illustrating the arrangement of the speakers, and there is no limitation. Furthermore, a non-directional speaker may also be arranged in place of the directional speaker.</p><p id="p-0089" num="0089">As the 3D display <b>302</b>, a display with improved depth perception such as a holographic display is assumed. In the illustrated example, video in which an object (person) <b>302</b> moves in the 3D space <b>301</b> is displayed. The user may view different videos according to a line-of-sight direction of viewing the 3D space displayed by the 3D display <b>302</b>. Furthermore, it is also assumed that the user moves in the interior <b>301</b> while viewing the 3D video in order to change the line-of-sight direction of viewing the 3D space. When an object (audio object) serving as a sound source such as a performer of a drama moves in the 3D space, the sound image localization processing is executed in synchronization with a position where the object moves, and a sound image of voice uttered by the object at each time is localized at a position where the object is present at that time. Furthermore, a volume of the sound image of the object is controlled according to a distance between the user who moves in the interior <b>301</b> and the audio object (for example, when the object comes to a near side of the user in the 3D space, the volume of the voice increases, and when the object moves backward in the 3D space, the volume of the voice decreases).</p><p id="p-0090" num="0090">The configuration of the content reproduction device <b>100</b> is continuously described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref> again.</p><p id="p-0091" num="0091">A sensor unit <b>109</b> includes both a sensor provided inside a main body of the content reproduction device <b>100</b> and a sensor externally connected to the content reproduction device <b>100</b>. The externally connected sensor also includes a sensor built in another consumer electronics (CE) device or IoT device present in the same space as the content reproduction device <b>100</b>. In this embodiment, it is assumed that sensor information acquired from the sensor unit <b>109</b> becomes input information of the neural network used in the video signal processing unit <b>105</b> and the audio signal processing unit <b>106</b>. Note that, the neural network is described later in detail.</p><p id="p-0092" num="0092">C. Sensing Function</p><p id="p-0093" num="0093"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically illustrates a configuration example of the sensor unit <b>109</b> mounted on the content reproduction device <b>100</b>. The sensor unit <b>109</b> includes a camera unit <b>410</b>, a user state sensor unit <b>420</b>, an environment sensor unit <b>430</b>, a device state sensor unit <b>440</b>, and a user profile sensor unit <b>450</b>. In this embodiment, the sensor unit <b>109</b> is used to acquire various pieces of information regarding a viewing status of the user.</p><p id="p-0094" num="0094">The camera unit <b>410</b> includes a camera <b>411</b> that images the user who is viewing the video content displayed on the image display unit <b>107</b>, a camera <b>412</b> that images the video content displayed on the image display unit <b>107</b>, and a camera <b>413</b> that images an interior (or an installation environment) in which the content reproduction device <b>100</b> is installed.</p><p id="p-0095" num="0095">The camera <b>411</b> is installed, for example, in the vicinity of the center of an upper edge of a screen of the image display unit <b>107</b>, and suitably images the user who is viewing the video content. The camera <b>412</b> is installed, for example, so as to face the screen of the image display unit <b>107</b>, and images the video content that the user is viewing. Alternatively, the user may wear goggles equipped with the camera <b>412</b>. Furthermore, the camera <b>412</b> has a function of recording the voice of the video content together. Furthermore, the camera <b>413</b> includes, for example, a whole-sky camera or a wide-angle camera, and images the interior (or the installation environment) in which the content reproduction device <b>100</b> is installed. Alternatively, the camera <b>413</b> may be, for example, a camera mounted on a camera table (camera platform) rotatable about each axis of roll, pitch, and yaw. Note that, in a case where sufficient environment data may be acquired by the environment sensor <b>430</b> or in a case where the environment data itself is unnecessary, the camera <b>410</b> is unnecessary.</p><p id="p-0096" num="0096">The user state sensor unit <b>420</b> includes one or more sensors that acquire state information regarding a state of the user. The user state sensor unit <b>420</b> intends to acquire, as the state information, for example, a work state of the user (whether or not the user views the video content), an action state of the user (moving state such as a stationary, walking, or running state, an opening/closing state of eyelids, a line-of-sight direction, a size of pupil), a mental state (a degree of impression, a degree of excitement, a degree of wakefulness as to whether the user is immersed or concentrated in the video content, feelings, emotions and the like), and a physiological state. The user state sensor unit <b>420</b> may be provided with various sensors such as a perspiration sensor, a myoelectric potential sensor, an eye potential sensor, a brain wave sensor, an exhalation sensor, a gas sensor, an ion concentration sensor, and an inertial measurement unit (IMU) that measures a behavior of the user, an audio sensor (such as a microphone) that collects the utterance of the user, and a position information detection sensor (such as a proximity sensor) that detects a position of an object such as a finger of the user. Note that, the microphone is not necessarily integrated with the content reproduction device <b>100</b>, and may be a microphone mounted on a product installed in front of a television such as a sound bar. Furthermore, an external microphone mounted device connected by wire or wirelessly may be used. The external microphone mounted device may be a smart speaker equipped with a microphone to which audio may be input, a wireless headphone/headset, a tablet, a smartphone, a PC, or a so-called smart home appliance such as a refrigerator, a washing machine, an air conditioner, a vacuum cleaner, or a lighting fixture, or an IoT home appliance. The position information detection sensor may be configured as a touch sensor for detecting a user operation on the image display unit <b>107</b>.</p><p id="p-0097" num="0097">The environment sensor unit <b>430</b> includes various sensors that measure information regarding an environment such as the interior in which the content reproduction device <b>100</b> is installed. For example, the environment sensor unit <b>430</b> includes a temperature sensor, a humidity sensor, an optical sensor, an illuminance sensor, an airflow sensor, an odor sensor, an electromagnetic wave sensor, a geomagnetic sensor, a global positioning system (GPS) sensor, an audio sensor (microphone and the like) that collects ambient sound and the like. Furthermore, the environment sensor unit <b>430</b> may acquire information such as a size of a room in which the content reproduction device <b>100</b> is placed, a position of the user, and brightness of the room.</p><p id="p-0098" num="0098">The device state sensor unit <b>440</b> includes one or more sensors that acquire an internal state of the content reproduction device <b>100</b>. Alternatively, circuit components of the video decoding unit <b>102</b> and the audio decoding unit <b>103</b> may have a function of externally outputting a state of an input signal, a processing status of the input signal and the like, and may serve as a sensor that detects the internal state of the device. Furthermore, the device state sensor unit <b>440</b> may detect the operation performed by the user on the content reproduction device <b>100</b> and other devices, or may store a past operation history of the user. The operation of the user may include a remote control operation on the content reproduction device <b>100</b> and other devices. The other devices herein mentioned may be a tablet, a smartphone, a PC, or a so-called smart home appliance such as a refrigerator, a washing machine, an air conditioner, a vacuum cleaner, or a lighting fixture, or an IoT home appliance. Furthermore, the device state sensor unit <b>440</b> may acquire information regarding performance and specifications of the device. The device state sensor unit <b>440</b> may be a memory such as a built-in read only memory (ROM) in which the information regarding the performance and specifications of the device is recorded, or a reader that reads the information from such memory.</p><p id="p-0099" num="0099">The user profile sensor unit <b>450</b> detects profile information regarding the user who views the video content with the content reproduction device <b>100</b>. It is not necessary that the user profile sensor unit <b>450</b> includes a sensor element. For example, a user profile such as age and sex of the user may be estimated on the basis of a face image of the user imaged by the camera <b>411</b>, the utterance of the user collected by the audio sensor and the like. Furthermore, the user profile acquired on a multifunctional information terminal carried by the user such as a smartphone may be acquired by cooperation between the content reproduction device <b>100</b> and the smartphone. Note that, the user profile sensor unit does not need to detect even sensitive information related to privacy and secret information of the user. Furthermore, it is not necessary to detect the profile of the same user every time the video content is viewed, and a memory such as an electrically erasable and programmable ROM (EEPROM) that stores the user profile information acquired once may be used.</p><p id="p-0100" num="0100">Furthermore, the multifunctional information terminal carried by the user such as the smartphone may be used as the user state sensor unit <b>420</b>, the environment sensor unit <b>430</b>, or the user profile sensor unit <b>450</b> by cooperation between the content reproduction device <b>100</b> and the smartphone. For example, sensor information acquired by a sensor built in a smartphone, and data managed by an application such as a health care function (pedometer and the like), a calendar or a schedule book, a memorandum, an e-mail, a browser history, and a posting and browsing history of a social network service (SNS) may be added to the state data of the user and the environment data. Furthermore, a sensor built in another CE device or IoT device present in the same space as the content reproduction device <b>100</b> may be used as the user state sensor unit <b>420</b> or the environment sensor unit <b>430</b>. Furthermore, a visitor may be detected by detecting a sound of an interphone, or by communication with an interphone system. Furthermore, a luminance meter or a spectrum analysis unit that acquires the video or audio output from the content reproduction device <b>100</b> and analyzes the same may be provided as a sensor.</p><p id="p-0101" num="0101">D. Reproduction Control of Content Based on Gazing Point</p><p id="p-0102" num="0102">The content reproduction device <b>100</b> according to the present disclosure is configured to perform reproduction control of the content on the basis of a gazing point of the user. The reproduction control of the content is performed on one or both of image creation and sound creation. The gazing point is a value representing who is viewing which content (for example, a television program) in which manner. In this specification, in order to simplify the description, an embodiment is described in which the reproduction content is processed on the basis of the gazing point of the user.</p><p id="p-0103" num="0103">Specifically, the content reproduction device <b>100</b> according to the present disclosure extracts an object in which the user is highly interested from the video on the basis of the gazing point of the user, tracks the object, and further performs the video signal processing such as framing and zooming based on the object gazed by the user on the video signal of the content. Furthermore, in a case where the target video is free viewpoint video, processing such as switching to a viewpoint position suitable for observing the object gazed by the user and line-of-sight change is also performed. With the framing and zooming processing, it becomes possible to provide video with which it is easy to observe the object gazed by the user, and an effect that the content further attracts the user's interest and that realistic feeling increases may be obtained.</p><p id="p-0104" num="0104">Furthermore, in a case where the content reproduction device <b>100</b> may execute the image quality enhancement processing such as super resolution processing and dynamic range expansion as the video signal processing, such image quality processing may be performed with reference to the gazed object. For example, sharp video signal processing according to user's interest is performed; for example, the gazed object is rendered at high resolution and with high dynamic range, whereas the resolution and luminance dynamic range of other surrounding objects are suppressed.</p><p id="p-0105" num="0105">Moreover, the content reproduction device <b>100</b> may perform processing of acquiring related information related to the object gazed by the user by automatic retrieval and displaying the same as auxiliary data. For example, in a case where the object is an athlete, related information such as a profile, results, and a related book of the athlete are retrieved. Furthermore, in a case where the object is an entertainer who appears in a movie or a drama, related information such as a movie or a television program in which the entertainer has appeared in the past or a related book are retrieved. Furthermore, when the object is a car, related information such as specifications and dealers of the car are retrieved.</p><p id="p-0106" num="0106">Furthermore, the content reproduction device <b>100</b> performs the audio signal processing on the audio signal on the basis of the gazing point of the user in addition to the video signal. For example, in a case where framing and zooming based on the object gazed by the user is performed in the video signal processing, the sound image localization processing is performed as the audio signal processing such that a sound image of the audio signal of the voice uttered from the gazed object and the like is adapted to a display position of the object.</p><p id="p-0107" num="0107">Furthermore, in a case where there is a plurality of objects in the frame of the video, not only the sound image of the audio signal for each object is localized at the display position of each object, but also volume adjustment according to the degree of interest of the user may be performed. For example, a volume of an audio signal emitted from a certain object in which the user is highly interested may be increased, and a volume of an audio signal emitted from an object in which the user is not much interested may be decreased. In a case where the content reproduction device <b>100</b> performs the sound quality enhancement processing such as band extension as the audio signal processing, the processing of the audio signal of each object may be performed according to the degree of interest of the user; for example, only the audio signal emitted from the object gazed by the user is subjected to the sound quality enhancement processing.</p><p id="p-0108" num="0108">In a case where the audio signal is the object-based audio, since the individual sounding objects are supplied without being mixed, and the localization information of the sounding object is supplied as the meta information, it is possible to easily implement the audio signal processing such as the sound image localization and the sound volume adjustment for each object in accordance with the display of the object when performing the framing and zooming processing on the video.</p><p id="p-0109" num="0109">D-1. Image Creation Based on Gazing Point (1)</p><p id="p-0110" num="0110"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a functional configuration example of the video signal processing unit <b>105</b> that performs image creation on the basis of the gazing point of the user. The illustrated video signal processing unit <b>105</b> is provided with a gazing point discrimination unit <b>501</b>, an object feature extraction unit <b>502</b>, a video tracking unit <b>503</b>, and a video output control unit <b>504</b>.</p><p id="p-0111" num="0111">The gazing point discrimination unit <b>501</b> receives a decoded video stream from the video decoding unit <b>102</b>, receives the sensor information from the sensor unit <b>109</b>, and discriminates the gazing point gazed by the user in the video on the basis of the sensor information. The gazing point discrimination unit <b>501</b> may extract only one gazing point from the video, or may extract a plurality of gazing points simultaneously.</p><p id="p-0112" num="0112">The gazing point discrimination unit <b>501</b> may discriminate the gazing point on the basis of motion of the line of sight of the user. For example, the gazing point discrimination unit <b>501</b> may discriminate a position where the line of sight remains for a certain period of time as the gazing point. Alternatively, the gazing point discrimination unit <b>501</b> may discriminate, as the gazing point, a position in the line-of-sight direction when a change in feelings or emotions when the degree of excitement of the user increases is detected.</p><p id="p-0113" num="0113">The gazing point discrimination unit <b>501</b> may discriminate the gazing point by using the position of the user (relative position with respect to a display screen of the content reproduction device <b>100</b>) and a direction of a face in addition to the line of sight of the user. In a case where there is a plurality of users, the gazing point discrimination unit <b>501</b> may extract a principal user and discriminate the gazing point from the line of sight of the user, or may discriminate the gazing point on the basis of an average line of sight of a plurality of users. Furthermore, the gazing point discrimination unit <b>501</b> may discriminate the gazing point in consideration of a viewing environment such as brightness of the room.</p><p id="p-0114" num="0114">Furthermore, the gazing point discrimination unit <b>501</b> may discriminate the gazing point on the basis of an instruction by the user using a finger or a remote controller. For example, it is possible to detect the user's finger by the camera unit <b>410</b> or the user state sensor <b>420</b> (for example, the position information detection sensor), and discriminate a position on the screen indicated by the user with the finger as the gazing point. Furthermore, a position on the screen indicated by a pointer function of the remote controller may be detected and discriminated as the gazing point.</p><p id="p-0115" num="0115">Furthermore, the gazing point discrimination unit <b>501</b> may discriminate the gazing point by using information regarding the feature of the object discriminated by the object feature extraction unit <b>502</b>. For example, the information regarding the feature of the object already extracted by the object feature extraction unit <b>502</b> may be stored in a storage unit not illustrated, and the gazing point discrimination unit <b>501</b> may discriminate the gazing point using the feature information. In this case, for example, the gazing point discrimination unit <b>501</b> may determine the object gazed by the user by comparing feature information (for example, a person holding a ball) included in the user utterance acquired by the sensor unit <b>109</b> with the feature information of the object extracted by the object feature extraction unit <b>502</b>. Furthermore, in a case where the meta information transmitted together with the video stream includes the information regarding the object, the gazing point discrimination unit <b>501</b> may discriminate the gazing point by using the meta information. Note that, the meta information may be included in the video stream to be transmitted, or may be acquired from an external server or a recording medium through a path different from that of the video stream.</p><p id="p-0116" num="0116">Furthermore, the gazing point discrimination unit <b>501</b> may discriminate the gazing point of the user (or a position at which the user should gaze) on the basis of a context such as story development of the content in addition to the sensor information. For example, in a case of video of sport game watching, a player to be focused on according to the progress of the game may be discriminated as the gazing point. In a case of a video stream of baseball relay broadcasting, it is estimated that the gazing point should be moved from a pitcher who pitches a ball to a batter who hits the ball and further in a direction of the hit ball from moment to moment. In a case of a video stream of football or rugby relay broadcasting, it is estimated that a player keeping a ball or a player to whom the ball is passed should be gazed. Furthermore, in a case of a video stream of a movie or a drama, it is estimated that a main character or a performer who is currently speaking should be gazed. Note that, the gazing point is not limited to a person, and may be an object (a signboard (including digital signage) arranged in a stadium, a car, studio set furniture, furnishings and the like) appearing in the video.</p><p id="p-0117" num="0117">The gazing point discrimination unit <b>501</b> may perform gazing point discrimination processing using a machine learning model of which deep learning is performed so as to estimate the gazing point of the user from the sensor information and the video stream.</p><p id="p-0118" num="0118">The object feature extraction unit <b>502</b> extracts the feature of the object corresponding to the gazing point discriminated by the gazing point discrimination unit <b>501</b>. For example, in a case of the video of sport game watching, the feature of the player discriminated as the gazing point is extracted, and in a case of the video of the movie or drama, the feature of the performer discriminated as the gazing point is extracted. Note that, the object discriminated as the gazing point is not limited to a person, and may be an object (a signboard (including digital signage) arranged in a stadium, a car, studio set furniture, furnishings and the like) appearing in the video. The object feature extraction unit <b>502</b> may perform feature extraction processing of the object by using the machine learning model of which deep learning is performed so as to estimate the feature of the object corresponding to the gazing point from the gazing point and the video stream.</p><p id="p-0119" num="0119">The video tracking unit <b>503</b> tracks the object at the gazing point in the video stream on the basis of the feature of the object at the gazing point extracted by the object feature extraction unit <b>502</b>, and outputs coordinates of the object at the gazing point in the video frame. The video tracking unit <b>503</b> may output the coordinates of the object for each frame, or may output the coordinates of the object at predetermined frame intervals. Furthermore, in a case where there is a plurality of gazing points, the video tracking unit <b>503</b> may track the object at each gazing point and output the coordinates of each object.</p><p id="p-0120" num="0120">The video output control unit <b>504</b> performs output control of the video stream on the basis of the coordinates of the object output from the video tracking unit <b>503</b>. Specifically, the video output control unit <b>504</b> performs the framing and zooming processing based on the coordinates of the object gazed by the user. For example, the video output control unit <b>504</b> performs the framing processing such that the coordinates of the object gazed by the user are at the center of the frame, and further zooms in to the object gazed by the user. Furthermore, in a case where there is a plurality of gazing points, the video output control unit <b>504</b> may perform the framing and zoom processing so that all objects (or more objects) gazed by the user are displayed. Furthermore, in a case where the target video stream is a free viewpoint video, the video output control unit <b>504</b> performs the framing and zooming processing including viewpoint change and line-of-sight change. Note that, the framing and zooming processing (or rendering processing) is basically performed on the basis of the line of sight of a specific user, but different framing and zooming processing may be performed according to the number of users who view simultaneously, or the framing and zooming processing may be switched according to the environment of the room.</p><p id="p-0121" num="0121">Furthermore, in a case where the video output control unit <b>504</b> performs the image quality enhancement processing such as super resolution processing or dynamic range expansion, this may perform the image quality enhancement processing based on the coordinates of the object gazed by the user. For example, the video output control unit <b>504</b> performs sharp video signal processing according to user's interest; for example, the gazed object is rendered at high resolution and with high dynamic range, whereas the resolution and luminance dynamic range of other surrounding objects are suppressed. While the vicinity of the coordinates of the object gazed by the user are converted at high resolution and with high dynamic range, in an area separated from the object gazed by the user, the video with suppressed resolution and dynamic range is obtained, so that the video that emphasizes or enhances the object gazed by the user is obtained.</p><p id="p-0122" num="0122">The video output control unit <b>504</b> may perform framing, zooming, and other video signal processing on the video stream by using a machine learning model of which deep learning is performed so as to perform optimum framing, zooming, and other video signal processing on the object at the gazing point and the objects around the gazing point.</p><p id="p-0123" num="0123">Various types of display devices such as a liquid crystal display, an organic EL display, a self-luminous display, a 3D display, and a holographic display are assumed as the image display unit <b>107</b>. Therefore, the video output control unit <b>504</b> performs adjustment processing according to the type of the display on the video stream subjected to framing, zooming, and other image quality processing based on the gazed object. Then, the video processed by the video output control unit <b>504</b> is output by the image display unit <b>107</b>.</p><p id="p-0124" num="0124">Although not illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in a case where the framing and zooming processing based on the object gazed by the user is performed in the video signal processing unit <b>105</b>, a position, a size, and a direction in which each object is displayed in the video frame change from those in an original video frame. The objects displayed in the video also include the sounding object that emits the audio signal. Therefore, in a case where the framing and zooming processing is performed on the video stream, it is preferable to perform processing of generating the sound image so that the audio signal of each sounding object is heard from the display position (sounding coordinates) in the video frame after the framing and zooming processing, that is, image sound matching processing.</p><p id="p-0125" num="0125"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates a functional configuration example of the signal processing unit <b>150</b> configured to perform the image creation including the framing and zooming processing on the basis of the gazing point of the user and to perform the image sound matching processing. Note that, functional modules having the same names and similar functions as those illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> are denoted by the same reference numerals, and detailed description thereof is herein omitted or the description is minimized.</p><p id="p-0126" num="0126">A sounding coordinate estimation unit <b>521</b> receives the video stream after being subjected to the framing and zooming processing from the video output control unit <b>102</b>, and receives a decoded audio stream from the audio decoding unit <b>103</b>. Then, the sounding coordinate estimation unit <b>521</b> estimates the display position of the sounding object serving as the sound source in the video after being subjected to the framing and zooming processing, and outputs the same as the sounding coordinates. The sounding object is the object and the like discriminated as the gazing point by the gazing point discrimination unit <b>501</b>. In a case where the audio signals of a plurality of objects are superimposed on the audio stream, the sounding coordinate estimation unit <b>521</b> separates the waveform of the audio signal for each object from an original audio stream, and estimates the sounding coordinates for each separated object. Note that, in a case where the audio stream follows the object-based audio, since the individual sounding objects are not mixed, the sounding coordinate estimation unit <b>521</b> does not need to perform separation processing of the audio signal for each object, and may easily calculate the display position, size, and direction of each object after the framing and zooming processing on the basis of the localization information included in the meta information.</p><p id="p-0127" num="0127">In a case where the image output unit <b>107</b> uses the 2D display, the sounding coordinate estimation unit <b>521</b> estimates two-dimensional sounding coordinates. Furthermore, in a case where the image output unit <b>107</b> is the 3D display having depth information such as the holographic display and a light-field display, the sounding coordinate estimation unit <b>521</b> estimates three-dimensional sounding coordinates. Furthermore, in a case where a plurality of sounding objects is estimated, the sounding coordinate estimation unit <b>521</b> may estimate the volume (or a volume ratio between the sounding objects) for each sounding object.</p><p id="p-0128" num="0128">Here, in a case where the image output unit <b>107</b> is the 3D display, for example, the sounding coordinate estimation unit <b>521</b> may estimate a 3D depth (projecting amount), a size, and a direction of 3D display of the sounding object in the 3D video, and estimate the three-dimensional sounding coordinates, the volume of the output sound, and the direction of the output sound so as to correspond to the 3D depth, the size, and the direction of the 3D display. Specifically, for example, the sound image of the 3D sounding object 3D-displayed from a certain position on the image output unit <b>107</b> toward the user who views the content in front of the image output unit <b>107</b> may be generated so as to be heard by the user from the display position on the image output unit <b>107</b>. Furthermore, in a case where the 3D sounding object is displayed at a position closer to the user (in a case where the projecting amount is large), the sound image may be generated so as to be heard louder by the user.</p><p id="p-0129" num="0129">The sounding coordinate estimation unit <b>521</b> performs estimation processing of the sounding coordinates of each sounding object by using a machine learning model of which deep learning is performed so as to estimate the sounding coordinates of the object that is the sound source in the video from the input video stream and audio stream. Note that, there also is a method of detecting the sound source appearing in the video by video analysis and audio analysis of the input video stream and audio stream, and calculating the display position of the sound source on the screen, but it is difficult to perform calculation processing in real time. Therefore, as described above, in this embodiment, the sounding coordinate estimation unit <b>521</b> uses the machine learning model.</p><p id="p-0130" num="0130">The audio output control unit <b>522</b> controls drive of the audio output unit <b>108</b> for generating the sound image of each sounding object in the video according to the sounding coordinates estimated by the sounding coordinate estimation unit <b>521</b>. Specifically, for example, in a case where the audio output unit <b>108</b> includes a plurality of speakers, the audio output control unit <b>522</b> determines the direction and the volume of the sound at the sounding position for each sounding object for which the sounding coordinates are estimated, determines a combination of the speakers for generating the sound image, and sets the volume and the direction of the sound to be output from each speaker.</p><p id="p-0131" num="0131">Furthermore, the audio output control unit <b>522</b> may perform the audio signal processing weighted on the basis of a gazing degree of the user; for example, sound quality enhancement processing such as band extension is applied or a special effect is applied to the audio signal emitted from the sounding object gazed by the user.</p><p id="p-0132" num="0132">Here, a specific example of the framing and zooming processing of the video based on the gazing point of the user is described.</p><p id="p-0133" num="0133">For example, in a case of a video stream of baseball relay broadcasting, the gazing point of the user moves from a pitcher who pitches a ball to a batter who hits the ball and further in a direction of the hit ball from moment to moment. <figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates an imaged video of a camera installed on a backscreen. Here, when the gazing point discrimination unit <b>501</b> discriminates that the user wants to gaze from a third base side or from the side of the pitcher, the video output control unit <b>504</b> performs the framing and zooming processing so as to obtain video in which the pitcher is viewed from the third base side as illustrated in <figref idref="DRAWINGS">FIG. <b>19</b></figref>. Furthermore, when the gazing point discrimination unit <b>501</b> discriminates that the user wants to gaze a home base side or the pitcher from the front, the video output control unit <b>504</b> performs the framing and zooming processing so as to obtain video in which the pitcher is viewed from the home base side as illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>.</p><p id="p-0134" num="0134">Furthermore, in a case of a video stream of football or rugby relay broadcasting, it is assumed that the user gazes at a player keeping a ball or a player to whom the ball is passed. Furthermore, in a case of a video stream of a movie or a drama, it is assumed that a main character or a performer who is currently speaking is gazed. <figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates video in which a long pass is made from a penalty area of one's own side toward an enemy side. In such a case, when the gazing point discrimination unit <b>501</b> discriminates that the user is gazing at the ball kicked to the enemy side, the video output control unit <b>504</b> performs the framing and zooming processing so as to focus on the ball and the player following the ball as illustrated in <figref idref="DRAWINGS">FIG. <b>22</b></figref>.</p><p id="p-0135" num="0135">Furthermore, it is not required that the zooming and framing processing is performed instantaneously, and this may be performed stepwise up to a target framing image or a target zooming image. For example, when the zooming and framing processing is performed at a high speed, there is a possibility that the user may get motion sickness or the user cannot properly follow the target object. Therefore, such a problem may be reduced by performing display control so as to approach target viewpoint video at a predetermined speed or performing display control so as to approach the target viewpoint video stepwise. Furthermore, when such display control is performed, by performing the image sound matching processing, the sound image and the volume are also generated so as to change at a predetermined speed or change stepwise, so that stereoscopic voice output control may be implemented.</p><p id="p-0136" num="0136">Subsequently, the machine learning model used in the image creation based on the gazing point is described.</p><p id="p-0137" num="0137">The machine learning model used in the gazing point discrimination processing by the gazing point discrimination unit <b>501</b> is represented by, for example, the neural network. Learning of the machine learning model represented by the neural network is performed through processing of changing the neural network by inputting learning data to the neural network to perform learning of a connection weight coefficient between nodes (neurons). The learning of the neural network may be performed in the content reproduction device <b>100</b>, but it is also possible to perform deep learning using enormous learning data on the cloud. <figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically illustrates a functional configuration example of an artificial intelligence server <b>600</b> that performs deep learning on the neural network used by the gazing point discrimination unit <b>501</b>. As an example, the artificial intelligence server <b>600</b> is constructed on the cloud, but may be constructed with edge computing such as an edge or a mobile edge.</p><p id="p-0138" num="0138">In a learning data database <b>601</b>, enormous learning data uploaded from a large number of content reproduction devices <b>100</b> (for example, television reception devices of respective homes) are accumulated. The learning data includes the sensor information input to the neural network as the gazing point discrimination unit <b>501</b> in the content reproduction device <b>100</b>, the video stream, the gazing point discriminated at that time (output from the gazing point discrimination unit <b>501</b>), and an evaluation value for a discrimination result. The evaluation value may be, for example, a simple evaluation (whether it is OK or NG) of the user for the discriminated gazing point (or video output on the basis of the discrimination result).</p><p id="p-0139" num="0139">A neural network <b>602</b> for discriminating the gazing point receives a combination of the sensor information and the video stream read from the learning data database <b>601</b> as the learning data, and outputs a result of discriminating the gazing point of the user at that time.</p><p id="p-0140" num="0140">An evaluation unit <b>603</b> evaluates a learning result of the neural network <b>602</b>. Specifically, the evaluation unit <b>603</b> receives the discrimination result of the gazing point output from the neural network <b>602</b>, the discrimination result of the gazing point (teacher data) and the evaluation of the user combined with the input to the neural network <b>602</b> read from the learning data database <b>601</b>, and defines a loss function based on a difference between the output from the neural network <b>602</b> and the teacher data. Note that, it is possible to calculate the loss function by weighting so as to increase the weight of the difference from the teacher data having a high evaluation result of the user and decrease the weight of the difference from the teacher data having a low evaluation result of the user. Then, the evaluation unit <b>603</b> performs learning of the neural network <b>602</b> by back propagation so as to minimize the loss function.</p><p id="p-0141" num="0141">Furthermore, the machine learning model used for the feature extraction processing of the object by the object feature extraction unit <b>502</b> is represented by the neural network. <figref idref="DRAWINGS">FIG. <b>7</b></figref> schematically illustrates a functional configuration example of an artificial intelligence server <b>700</b> that performs deep learning on the neural network used by the object feature extraction unit <b>502</b>. As an example, the artificial intelligence server <b>700</b> is constructed on the cloud, but this may be constructed with edge computing such as an edge or a mobile edge.</p><p id="p-0142" num="0142">In a learning data database <b>701</b>, enormous learning data uploaded from a large number of content reproduction devices <b>100</b> (for example, television reception devices of respective homes) are accumulated. The learning data includes information of the gazing point discriminated by the gazing point discrimination unit <b>501</b> in the content reproduction device <b>100</b>, the video stream, the feature of the object discriminated at that time (output from the object feature extraction unit <b>502</b>), and an evaluation value for an extraction result. The evaluation value may be, for example, a simple evaluation (whether it is OK or NG) of the user for the extracted feature of the object (or video output on the basis of the extraction result).</p><p id="p-0143" num="0143">The neural network <b>702</b> for extracting the object feature receives a combination of the information of the gazing point and the video stream read from the learning data database <b>701</b> as the learning data, and extracts the object corresponding to the gazing point.</p><p id="p-0144" num="0144">An evaluation unit <b>703</b> evaluates a learning result of the neural network <b>702</b>. Specifically, the evaluation unit <b>703</b> receives the feature of the object output from the neural network <b>702</b>, the feature of the object (teacher data) and the user evaluation combined with the input to the neural network <b>702</b> read from the learning data database <b>701</b>, and defines a loss function based on a difference between the output from the neural network <b>702</b> and the teacher data. Note that, it is possible to define the loss function by weighting so as to increase the weight of the difference from the teacher data having a high evaluation result of the user and decrease the weight of the difference from the teacher data having a low evaluation result of the user. Then, the evaluation unit <b>703</b> performs learning of the neural network <b>702</b> by back propagation so as to minimize the loss function.</p><p id="p-0145" num="0145">Furthermore, the machine learning model used in video output processing by the video output control unit <b>504</b> is also represented by the neural network. <figref idref="DRAWINGS">FIG. <b>8</b></figref> schematically illustrates a functional configuration example of an artificial intelligence server <b>800</b> that performs deep learning on the neural network used by the video output control unit <b>504</b>. As an example, the artificial intelligence server <b>800</b> is constructed on the cloud, but this may be constructed with edge computing such as an edge or a mobile edge.</p><p id="p-0146" num="0146">In a learning data database <b>801</b>, enormous learning data uploaded from a large number of content reproduction devices <b>100</b> (for example, television reception devices of respective homes) are accumulated. The learning data includes the coordinates of the object tracked by the video tracking unit <b>503</b> in the content reproduction device <b>100</b>, the video stream, video output from the video output control unit <b>504</b> at that time, and an evaluation value for the output video. The evaluation value may be, for example, a simple evaluation (OK or NG) of the user for the output video.</p><p id="p-0147" num="0147">A neural network <b>802</b> for video output control receives a combination of the coordinates of the object and the video stream read from the learning data database <b>801</b> as the learning data, and performs the video output control on the video stream. The video output control herein mentioned includes framing, zooming, resolution conversion, and luminance dynamic range conversion for the object at the gazing point and objects around the gazing point.</p><p id="p-0148" num="0148">An evaluation unit <b>803</b> evaluates a learning result of the neural network <b>802</b>. Specifically, the evaluation unit <b>803</b> receives the video output from the neural network <b>802</b>, the video stream (teacher data) and the evaluation of the user combined with the coordinates of the object read from the learning data database <b>801</b>, and defines a loss function based on the difference from the video stream output from the neural network <b>802</b>. Note that, it is possible to define the loss function by weighting so as to increase the weight of the difference from the teacher data having a high evaluation result of the user and increase the weight of the difference from the teacher data having a low evaluation result of the user. Then, the evaluation unit <b>803</b> performs learning of the neural network <b>802</b> by back propagation so as to minimize the loss function.</p><p id="p-0149" num="0149">Furthermore, the machine learning model used in sounding coordinate estimation processing by the sounding coordinate estimation unit <b>521</b> is represented by the neural network. <figref idref="DRAWINGS">FIG. <b>15</b></figref> schematically illustrates a functional configuration example of an artificial intelligence server <b>1500</b> that performs deep learning on the neural network used by the sounding coordinate estimation unit <b>521</b>. As an example, the artificial intelligence server <b>1500</b> is constructed on the cloud, but this may be constructed with edge computing such as an edge or a mobile edge.</p><p id="p-0150" num="0150">In a learning data database <b>1501</b>, enormous learning data uploaded from a large number of content reproduction devices <b>100</b> (for example, television reception devices of respective homes) are accumulated. The learning data includes the video stream decoded by the video decoding unit <b>102</b> in the content reproduction device <b>100</b> (or after the framing and zooming processing), the video stream decoded by the audio decoding unit <b>103</b>, sounding coordinates estimated by a sounding coordinate estimation unit <b>1401</b> at that time, and an evaluation value for the sounding coordinates. The evaluation value may be, for example, a simple evaluation (OK or NG) of the user for the audio output (or the generated sound image) based on the estimated sounding coordinates.</p><p id="p-0151" num="0151">A neural network <b>1502</b> for the sounding coordinate estimation processing receives a combination of the video stream and the audio stream read from the learning data database <b>1501</b> as the learning data, estimates the display position of the object serving as the sound source in the video, and outputs the same as the sounding coordinates. In a case where audio signals of a plurality of objects are superimposed on the audio stream, the neural network <b>1502</b> separates a waveform of the audio signal for each object from an original audio stream, and estimates the sounding coordinates for each separated object.</p><p id="p-0152" num="0152">The evaluation unit <b>1503</b> evaluates a learning result of the neural network <b>1502</b>. Specifically, the evaluation unit <b>1503</b> receives the sounding coordinates output from the neural network <b>1502</b>, the sounding coordinates (teacher data) and the user evaluation combined with the video stream and the audio stream read from the learning data database <b>1501</b>, and defines a loss function based on a difference from the video stream output from the neural network <b>1502</b>. Note that, it is possible to define the loss function by weighting so as to increase the weight of the difference from the teacher data having a high evaluation result of the user and increase the weight of the difference from the teacher data having a low evaluation result of the user. Then, the evaluation unit <b>1503</b> performs learning of the neural network <b>1502</b> by back propagation so as to minimize the loss function.</p><p id="p-0153" num="0153"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a procedure of the video signal processing performed in the video signal processing unit <b>105</b> illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> as a flowchart.</p><p id="p-0154" num="0154">First, the gazing point discrimination unit <b>501</b> receives the decoded video stream from the video decoding unit <b>102</b>, receives the sensor information from the sensor unit <b>109</b>, and discriminates the gazing point gazed by the user in the video on the basis of the sensor information (step S<b>901</b>).</p><p id="p-0155" num="0155">The gazing point discrimination unit <b>501</b> basically discriminates a position in which the user is interested and gazing from the video frame on the basis of motion of the line of sight of the user and the like. Note that, the gazing point discrimination unit <b>501</b> may discriminate not the position gazed by the user but the position at which the user should gaze on the basis of a context of the content being reproduced and the like.</p><p id="p-0156" num="0156">In a case where the gazing point discrimination unit <b>501</b> may discriminate the gazing point of the user (Yes at step S<b>902</b>), the object feature extraction unit <b>502</b> extracts the feature of the object corresponding to the gazing point discriminated by the gazing point discrimination unit <b>501</b> (step S<b>903</b>).</p><p id="p-0157" num="0157">Next, the video tracking unit <b>503</b> tracks the object at the gazing point in the video stream on the basis of the feature of the object at the gazing point extracted by the object feature extraction unit <b>502</b>, and outputs the coordinates of the object at the gazing point in the video frame (step S<b>904</b>).</p><p id="p-0158" num="0158">Next, the video output control unit <b>504</b> performs the output control of the video stream on the basis of the coordinates of the object output from the video tracking unit <b>503</b> (step S<b>905</b>). The video output control herein mentioned includes framing, zooming, resolution conversion, and luminance dynamic range conversion for the object at the gazing point and objects around the gazing point.</p><p id="p-0159" num="0159">Various types of display devices such as a liquid crystal display, an organic EL display, a self-luminous display, a 3D display, and a holographic display are assumed as the image display unit <b>107</b>. Therefore, the video output control unit <b>504</b> performs adjustment processing according to the type of the display on the video stream subjected to the framing, zooming, and other image quality processing based on the gazed object (step S<b>906</b>).</p><p id="p-0160" num="0160">Then, the video processed by the video output control unit <b>504</b> is output by the image display unit <b>107</b> (step S<b>907</b>).</p><p id="p-0161" num="0161"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates a procedure of the image sound matching processing executed by the signal processing unit <b>150</b> illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref> as a flowchart. Note that, the video signal processing such as the framing and zooming based on the gazing point of the user is implemented according to the procedure illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, so that, in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, the description of the video signal processing is omitted or minimized.</p><p id="p-0162" num="0162">First, the gazing point discrimination unit <b>501</b> receives the decoded video stream from the video decoding unit <b>102</b>, receives the sensor information from the sensor unit <b>109</b>, and discriminates the gazing point gazed by the user in the video on the basis of the sensor information (step S<b>1601</b>).</p><p id="p-0163" num="0163">In a case where the gazing point discrimination unit <b>501</b> may discriminate the gazing point of the user (Yes at step S<b>1602</b>), the video output control unit <b>504</b> performs the framing and zooming processing on the video stream on the basis of the result of tracking the object corresponding to the gazing point. Then, the sounding coordinate estimation unit <b>521</b> calculates the video position of each sounding object from the video after being subjected to the framing and zooming processing (step S<b>1603</b>), and determines the video position of each sounding object, and the direction and size of the object (step S<b>1604</b>).</p><p id="p-0164" num="0164">Next, the sounding coordinate estimation unit <b>521</b> calculates the position of the sound image for each sounding object on the basis of the video position of the object, and the direction and size of the object determined at preceding step S<b>1604</b> (step S<b>1605</b>), determines the sounding position of the sound image, and the direction and volume of the sound (step S<b>1606</b>), and outputs the same as information of the sounding coordinates. In a case where the audio signal is the object-based audio, the individual sounding objects are supplied without being mixed, and the localization information of the sounding object is supplied as the meta information, so that it is possible to easily implement the calculation of the sound image position for each sounding object and the processing of determining the sounding position, and the direction and volume of the sound.</p><p id="p-0165" num="0165">The video output control unit <b>504</b> performs output control of the video stream on the basis of the coordinates of the object output from the video tracking unit <b>503</b>. The video output control herein mentioned includes framing, zooming, resolution conversion, and luminance dynamic range conversion for the object at the gazing point and objects around the gazing point. Then, the video output control unit <b>504</b> performs adjustment processing according to the type of the display (step S<b>1607</b>).</p><p id="p-0166" num="0166">Furthermore, the audio output control unit <b>522</b> applies an effect (reflection, diffraction, interference and the like) to the sound image of each sounding object on the basis of the video position of each sounding object, and the direction and size of the object determined at step S<b>1606</b> (step S<b>1608</b>). Next, the audio output control unit <b>522</b> determines a combination of speakers for generating the sound image on the basis of the direction of the sound and the volume of the sound at the sounding position and the effect determined at preceding steps S<b>1606</b> and S<b>1608</b> (step S<b>1609</b>).</p><p id="p-0167" num="0167">Then, the video processed by the video output control unit <b>504</b> is output by the image display unit <b>107</b>. Furthermore, the audio output control unit <b>522</b> outputs the audio signal from each speaker to generate the sound image localized at the display position of the object at the gazing point (step S<b>1610</b>).</p><p id="p-0168" num="0168"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates the functional configuration example in which the image creation processing based on the gazing point of the user is executed by the edge, that is, the content reproduction device <b>100</b>. As a modification, the image creation processing based on the gazing point of the user may be performed by a server (for example, an artificial intelligence server) on the cloud. One effect of performing the processing on the server side is that deep learning and relearning may be performed by collecting learning data from a large number of edges while providing the content created on the basis of the gazing point of the user to the edge.</p><p id="p-0169" num="0169"><figref idref="DRAWINGS">FIG. <b>23</b></figref> illustrates a functional configuration example of an artificial intelligence server <b>2300</b> that performs the image creation on the basis of the gazing point of the user. The artificial intelligence server <b>2300</b> is assumed to be installed on the cloud. Furthermore, a video source <b>2301</b> as a target of the image creation processing by the artificial intelligence server <b>2300</b> may be any of a broadcasting station, a stream distribution server, or a recording medium.</p><p id="p-0170" num="0170">On the content reproduction device <b>100</b> side, for example, the framing or scaling processing of the video occurs by a manual operation such as an operation of a remote controller or another controller and a voice command by the user. Alternatively, the framing or scaling processing is automatically activated on the basis of an analysis result of source video from the video source <b>2301</b>. The sensor information detected by the sensor unit <b>109</b> on the content reproduction device <b>100</b> side is input to a gazing point discrimination unit <b>2311</b> in the artificial intelligence server <b>2300</b> via a network. The gazing point discrimination unit <b>2311</b> discriminates the gazing point gazed by the user in the video input from the video source <b>2301</b>. The gazing point discrimination unit <b>2311</b> performs the gazing point discrimination processing using the machine learning model of which deep learning is performed so as to estimate the gazing point of the user from the sensor information and the video stream.</p><p id="p-0171" num="0171">The object feature extraction unit <b>2312</b> extracts the feature of the object corresponding to the gazing point discriminated by the gazing point discrimination unit <b>2311</b>. The object feature extraction unit <b>2312</b> performs the feature extraction processing of the object using the machine learning model of which deep learning is performed so as to estimate the feature of the object corresponding to the gazing point from the gazing point and the video stream.</p><p id="p-0172" num="0172">The video tracking unit <b>2313</b> tracks the object at the gazing point in the video stream on the basis of the feature of the object at the gazing point extracted by the object feature extraction unit <b>2312</b>, and outputs the coordinates of the object at the gazing point in the video frame.</p><p id="p-0173" num="0173">A framing/scaling processing unit <b>2314</b> performs the framing and zooming processing based on the coordinates of the object gazed by the user on the basis of the coordinates of the object output from the video tracking unit <b>2313</b>. For example, the framing/scaling processing unit <b>2314</b> performs the framing processing such that the coordinates of the object gazed by the user are at the center of the frame, and further zooms in to the object gazed by the user (refer to <figref idref="DRAWINGS">FIGS. <b>18</b> to <b>22</b></figref>).</p><p id="p-0174" num="0174">Then, the video stream subjected to the image creation processing in the artificial intelligence server <b>2300</b> is subjected to encoding processing in compliance with a predetermined encoding standard such as MPEG-2 systems by a video encoding unit <b>2302</b>, and then distributed to the content reproduction device <b>100</b>.</p><p id="p-0175" num="0175">On the content reproduction device <b>100</b> side, the received encoded video stream is subjected to decoding processing by the video decoding unit <b>102</b>, subjected to the video signal processing including image quality enhancement and the like by the video signal processing unit <b>105</b>, and then displayed by the image display unit <b>107</b>.</p><p id="p-0176" num="0176">Furthermore, <figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates the functional configuration example in which the image sound matching processing when performing the image creation based on the gazing point of the user by the edge, that is, the content reproduction device <b>100</b>. As a modification, the image sound matching processing may be performed by a server (for example, an artificial intelligence server) on the cloud. One effect of performing the processing on the server side is that deep learning and relearning may be performed by collecting learning data from a large number of edges while providing the content of which image creation or image sound matching processing is performed on the basis of the gazing point of the user to the edge.</p><p id="p-0177" num="0177"><figref idref="DRAWINGS">FIG. <b>24</b></figref> illustrates a functional configuration example of an artificial intelligence server <b>2400</b> that performs the image creation and image sound matching processing on the basis of the gazing point of the user. Note that, functional modules having the same names and similar functions as those illustrated in <figref idref="DRAWINGS">FIG. <b>23</b></figref> are denoted by the same reference numerals, and description thereof is herein omitted or minimized. The artificial intelligence server <b>2400</b> is assumed to be installed on the cloud. Furthermore, an audio source <b>2401</b> as a target of the image sound matching processing by the artificial intelligence server <b>2400</b> is a content integrated with the video source <b>2301</b> and provided from a broadcasting station, a stream distribution server, or a recording medium.</p><p id="p-0178" num="0178">The sounding coordinate estimation unit <b>2411</b> receives the video stream after being subjected to the framing and zooming processing from the framing/scaling processing unit <b>2314</b>, and receives the audio stream from the audio source <b>2401</b>. Then, the sounding coordinate estimation unit <b>2411</b> estimates the display position of the sounding object serving as the sound source in the video after being subjected to the framing and zooming processing, and outputs the same as the sounding coordinates. The sounding coordinate estimation unit <b>2411</b> performs estimation processing of the sounding coordinates of each sounding object by using the machine learning model of which deep learning is performed so as to estimate the sounding coordinates of the object serving as the sound source in the video from the input video stream and audio stream.</p><p id="p-0179" num="0179">In a case where the audio signals of a plurality of objects are superimposed on the audio stream, the sounding coordinate estimation unit <b>2411</b> separates the waveform of the audio signal for each object from the original audio stream, and estimates the sounding coordinates for each separated object. In a case where the audio stream follows the object-based audio, since the individual sounding objects are not mixed, separation processing of the audio signals for each object is not necessary, and the sounding coordinate estimation unit <b>2411</b> may easily calculate the display position, size, and direction of each object after the framing and zooming processing on the basis of the localization information included in the meta information.</p><p id="p-0180" num="0180">It is also possible to transmit specification information of the display used by the image output unit <b>107</b> from the edge, that is, the content reproduction device <b>100</b> side to the artificial intelligence server <b>2400</b>. In a case where the image output unit <b>107</b> uses the 2D display, the sounding coordinate estimation unit <b>2411</b> estimates two-dimensional sounding coordinates. Furthermore, in a case where the image output unit <b>107</b> is a 3D display having depth information such as a holographic display, the sounding coordinate estimation unit <b>2411</b> estimates three-dimensional sounding coordinates. Furthermore, in a case where a plurality of sounding objects is estimated, the sounding coordinate estimation unit <b>2411</b> may also estimate the volume for each sounding object (or a volume ratio between the sounding objects).</p><p id="p-0181" num="0181">The audio signal processing unit <b>2412</b> performs the audio signal processing for generating the sound image of each sounding object in the video according to the sounding coordinates estimated by the sounding coordinate estimation unit <b>2411</b>. For example, the configuration of the speaker used in the audio output unit <b>108</b> may be transmitted from the edge, that is, the content reproduction device <b>100</b> side. The audio signal processing unit <b>2412</b> determines the direction and the volume of the sound at the sounding position for each sounding object for which the sounding coordinates are estimated, determines the combination of speakers for generating the sound image, and sets the volume and the direction of the sound output from each speaker. In a case where the audio signal is the object-based audio, the audio signal processing unit <b>2412</b> does not mix individual sounding objects, and outputs information such as the sounding coordinates of each sounding object as meta information.</p><p id="p-0182" num="0182">Then, the audio stream subjected to the sound image matching processing in the artificial intelligence server <b>2400</b> is subjected to encoding processing in compliance with a predetermined encoding standard such as MP3 or HE-AAC by an audio encoding unit <b>2413</b>, and then distributed to the content reproduction device <b>100</b>.</p><p id="p-0183" num="0183">On the content reproduction device <b>100</b> side, the received encoded audio stream is subjected to the decoding processing by the audio decoding unit <b>103</b>, subjected to the audio signal processing including the sound quality enhancement and the like by the audio signal processing unit <b>106</b>, and then output by the audio output unit <b>108</b>.</p><p id="p-0184" num="0184">Furthermore, the video subjected to the processing such as framing and zooming by the video output control unit <b>504</b> at step S<b>905</b> or step S<b>1607</b> may be returned to a state before the video output processing is performed. Specifically, for example, in a case where the user performs a predetermined operation, the framed or zoomed video may be returned to the input video before the processing is performed. These operations may include, for example, a user's gesture such as moving a hand in front of the screen display unit <b>107</b> or moving a face away from the screen display unit <b>107</b>. Furthermore, control of the line of sight by the user including bringing the line of sight to a predetermined position or a predetermined object inside and outside the screen display unit <b>107</b> may be included. Furthermore, the utterance including a predetermined command by the user may be included. Furthermore, an operation on a predetermined object displayed on the screen, an operation of an operation unit provided in the main body of the content reproduction device <b>100</b>, or an operation of a remote controller for operating the content reproduction device <b>100</b> may be included.</p><p id="p-0185" num="0185">D-2. Image Creation Based on Gazing Point (2)</p><p id="p-0186" num="0186"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates another functional configuration example of the video signal processing unit <b>105</b> that performs the image creation on the basis of the gazing point of the user. The illustrated video signal processing unit <b>105</b> is provided with the gazing point discrimination unit <b>501</b>, the object feature extraction unit <b>502</b>, the video tracking unit <b>503</b>, the video output control unit <b>504</b>, an object discrimination unit <b>511</b>, a related information retrieval acquisition unit <b>512</b>, and a related information output control unit <b>513</b>. Note that, since the gazing point discrimination unit <b>501</b>, the object feature extraction unit <b>502</b>, the video tracking unit <b>503</b>, and the video output control unit <b>504</b> are the same as the functional modules having the same name and the same reference number in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a detailed description thereof is herein omitted or the description is minimized.</p><p id="p-0187" num="0187">The object discrimination unit <b>511</b> discriminates the object on the basis of the feature of the object at the gazing point extracted by the object feature extraction unit <b>502</b>. The object discrimination unit <b>511</b> may access information resources on the cloud as necessary to perform the object discrimination processing. For example, in a case of the video of sport game watching, it is discriminated who a player discriminated as the gazing point is, and in a case of the video of a movie or a drama, it is discriminated who a performer discriminated as the gazing point is. Note that, the object is not limited to a person, and may be an object (a signboard (including digital signage) arranged in a stadium, a car, studio set furniture, furnishings and the like) appearing in the video.</p><p id="p-0188" num="0188">The object discrimination unit <b>511</b> may perform the object discrimination processing by using the machine learning model of which deep learning is performed so as to estimate the object from the feature of the object.</p><p id="p-0189" num="0189">The related information retrieval acquisition unit <b>512</b> performs processing of retrieving and acquiring the related information of the object discriminated by the object discrimination unit <b>511</b> on the cloud through the external interface unit <b>110</b>. For example, in a case where the object is an athlete, related information such as a profile, results, and a related book of the athlete are retrieved. Furthermore, in a case where the object is an entertainer who appears in a movie or a drama, related information such as a movie or a television program in which the entertainer has appeared in the past or a related book are retrieved. Furthermore, when the object is a car, related information such as specifications and dealers of the car are retrieved.</p><p id="p-0190" num="0190">The related information retrieval acquisition unit <b>512</b> may perform retrieval acquisition processing of the related information by using the machine learning model of which deep learning is performed so as to perform retrieval processing by estimating a retrieval method (retrieval keyword or retrieval conditional expression) and the like of the related information according to the object. Furthermore, although the processing in which the related information retrieval acquisition unit <b>512</b> retrieves on the cloud to acquire the related information of the object is herein described, the related information retrieval acquisition unit <b>512</b> may perform the retrieval acquisition processing of acquiring the related information from a storage unit of the content reproduction device <b>100</b> not illustrated or an external storage area connected to the content reproduction device <b>100</b>. Furthermore, the related information regarding the object may be acquired from the meta information transmitted together with the video stream.</p><p id="p-0191" num="0191">The related information output control unit <b>513</b> performs output control for presenting the related information of the object acquired by retrieving on the cloud by the related information retrieval acquisition unit <b>512</b> to the user. There are various methods of presenting the related information to the user. For example, the related information may be superimposed on the video stream as auxiliary data such as on screen display (OSD) or subtitles to be displayed. Alternatively, the related information output control unit <b>513</b> may convert the related information into voice data, superimpose the same on the audio stream of the content, and output the audio data. In this embodiment, it is assumed that the related information is displayed using an OSD function. The related information output control unit <b>513</b> generates the OSD from the related information of the object acquired by the related information retrieval acquisition unit <b>512</b> and outputs the same to the video output control unit <b>504</b>.</p><p id="p-0192" num="0192">The video output control unit <b>504</b> performs the output control of the video stream on the basis of the coordinates of the object output from the video tracking unit <b>503</b> as described above. The video output control herein mentioned includes the framing, zooming, resolution conversion, and luminance dynamic range conversion for the object at the gazing point of the user and objects around the gazing point.</p><p id="p-0193" num="0193">Furthermore, the video output control unit <b>504</b> superimposes the OSD generated by the related information output control unit <b>513</b> on the video stream. In a case where the framing and zooming is performed on the object at the gazing point of the user or the objects around the gazing point, the video output control unit <b>504</b> performs OSD superimposition processing in consideration of the position of the object that moves in accordance with the framing and zooming.</p><p id="p-0194" num="0194">Then, the video output control unit <b>504</b> performs the adjustment processing according to the type of the display on the video stream on which other image quality processing is performed. The video processed by the video output control unit <b>504</b> is output by the image display unit <b>107</b>.</p><p id="p-0195" num="0195">Note that, a mode of displaying the related information of the object is not limited to the OSD. For example, a balloon, a pop-up window, and a band for displaying the related information on any of upper, lower, left, and right edges of the screen may be generated. Furthermore, in a case where the related information is displayed by the OSD and the like on the 3D display, a 3D depth, a size, and a direction of a display position of the OSD are set with reference to the display position of the target object. In any display mode, it is desirable to display the related information for any object in an easy-to-understand manner while preventing interference with the video of the object at the gazing point of the user and hindrance of the gazing of the user.</p><p id="p-0196" num="0196">Furthermore, the gazing point discrimination unit <b>501</b> may discriminate the gazing point by using discrimination information regarding the object discriminated by the object discrimination unit <b>511</b>. For example, information regarding the discrimination result of the object already discriminated by the object feature extraction unit <b>502</b> may be stored in a storage unit not illustrated, and the gazing point discrimination unit <b>501</b> may discriminate the gazing point using the discrimination information. In this case, for example, the gazing point discrimination unit <b>501</b> may determine the object gazed by the user by using the name of the object included in the user utterance acquired by the sensor unit <b>109</b>. Furthermore, in a case where the meta information transmitted together with the video stream includes the information regarding the object, the gazing point discrimination unit <b>501</b> may discriminate the gazing point by using the meta information. Note that, the meta information may be included in the video stream to be transmitted, or may be acquired from an external server or a recording medium through a path different from that of the video stream. Similarly, the gazing point discrimination unit <b>501</b> may discriminate the gazing point by using the related information of the object acquired by the related information retrieval acquisition unit <b>512</b>.</p><p id="p-0197" num="0197">The machine learning model used in the object discrimination processing by the object discrimination unit <b>511</b> is represented by, for example, the neural network. <figref idref="DRAWINGS">FIG. <b>11</b></figref> schematically illustrates a functional configuration example of an artificial intelligence server <b>1100</b> that performs deep learning on the neural network used by the object discrimination unit <b>511</b>. As an example, the artificial intelligence server <b>1100</b> is constructed on the cloud, but this may be constructed with edge computing such as an edge or a mobile edge.</p><p id="p-0198" num="0198">In a learning data database <b>1101</b>, enormous learning data uploaded from a large number of content reproduction devices <b>100</b> (for example, television reception devices of respective homes) are accumulated. The learning data includes the feature of the object at the gazing point discriminated by the object feature extraction unit <b>502</b> in the content reproduction device <b>100</b>, the object discriminated by the object discrimination unit <b>511</b> at that time, and an evaluation value for the discrimination result. The evaluation value may be, for example, a simple evaluation (whether it is OK or NG) of the user for the discriminated object (or the video output on the basis of the discrimination result).</p><p id="p-0199" num="0199">A neural network <b>1102</b> for the object discrimination processing receives the feature of the object read from the learning data database <b>1101</b> as the learning data, and outputs a discrimination result of the object at that time. The neural network <b>1102</b> accesses information resources on the cloud as necessary to perform the object discrimination processing.</p><p id="p-0200" num="0200">An evaluation unit <b>1103</b> evaluates a learning result of the neural network <b>1102</b>. Specifically, the evaluation unit <b>1103</b> receives the discrimination result of the object output from the neural network <b>1102</b>, the discrimination result of the object (teacher data) and the user evaluation combined with the input to the neural network <b>1102</b> read from the learning data database <b>1101</b>, and defines a loss function based on a difference between the output from the neural network <b>1102</b> and the teacher data. Note that, it is possible to calculate the loss function by weighting so as to increase the weight of the difference from the teacher data having a high evaluation result of the user and decrease the weight of the difference from the teacher data having a low evaluation result of the user. Then, the evaluation unit <b>1103</b> performs learning of the neural network <b>1102</b> by back propagation so as to minimize the loss function.</p><p id="p-0201" num="0201">Furthermore, the machine learning model used for the processing of retrieving the related information of the object on the cloud to acquire by the related information retrieval acquisition unit <b>512</b> is represented by, for example, the neural network. <figref idref="DRAWINGS">FIG. <b>12</b></figref> schematically illustrates a functional configuration example of an artificial intelligence server <b>1200</b> that performs deep learning on the neural network used by the related information retrieval acquisition unit <b>512</b>. As an example, the artificial intelligence server <b>1200</b> is constructed on the cloud, but this may be constructed with edge computing such as an edge or a mobile edge.</p><p id="p-0202" num="0202">In a learning data database <b>1201</b>, enormous learning data uploaded from a large number of content reproduction devices <b>100</b> (for example, television reception devices of respective homes) are accumulated. The learning data includes the object discriminated by the object discrimination unit <b>511</b> in the content reproduction device <b>100</b> on the basis of the feature of the object, the related information retrieved on the cloud to be acquired by the related information retrieval acquisition unit <b>512</b> for the object, and an evaluation value for a retrieval acquisition result. The evaluation value may be, for example, a simple evaluation (whether it is OK or NG) of the user for the related information retrieved and acquired (or the OSD displayed on the basis of the related information).</p><p id="p-0203" num="0203">A neural network <b>1202</b> for related information retrieval and acquisition receives the object read from the learning data database as the learning data, and outputs the related information of the object. The neural network <b>1202</b> performs processing of retrieving on the cloud the related information of the object to acquire.</p><p id="p-0204" num="0204">An evaluation unit <b>1203</b> evaluates a learning result of the neural network <b>1202</b>. Specifically, the evaluation unit <b>1203</b> receives the related information output from the neural network <b>1202</b>, the related information (teacher data) and the evaluation of the user combined with the input to the neural network <b>1202</b> read from the learning data database <b>1201</b>, and defines a loss function based on a difference between the output from the neural network <b>1202</b> and the teacher data. Note that, it is possible to calculate the loss function by weighting so as to increase the weight of the difference from the teacher data having a high evaluation result of the user and decrease the weight of the difference from the teacher data having a low evaluation result of the user. Then, the evaluation unit <b>1203</b> performs learning of the neural network <b>1202</b> by back propagation so as to minimize the loss function.</p><p id="p-0205" num="0205"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates a procedure of the video signal processing performed by the video signal processing unit <b>105</b> illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> as a flowchart.</p><p id="p-0206" num="0206">First, the gazing point discrimination unit <b>501</b> receives the decoded video stream from the video decoding unit <b>102</b>, receives the sensor information from the sensor unit <b>109</b>, and discriminates the gazing point gazed by the user in the video on the basis of the sensor information (step S<b>1301</b>).</p><p id="p-0207" num="0207">In a case where the gazing point discrimination unit <b>501</b> may discriminate the gazing point of the user (Yes at step S<b>1302</b>), the object feature extraction unit <b>502</b> extracts the feature of the object corresponding to the gazing point discriminated by the gazing point discrimination unit <b>501</b> (step S<b>1303</b>).</p><p id="p-0208" num="0208">Next, the video tracking unit <b>503</b> tracks the object at the gazing point in the video stream on the basis of the feature of the object at the gazing point extracted by the object feature extraction unit <b>502</b>, and outputs the coordinates of the object at the gazing point in the video frame (step S<b>1304</b>).</p><p id="p-0209" num="0209">Furthermore, the object discrimination unit <b>511</b> discriminates the object on the basis of the feature of the object at the gazing point extracted by the object feature extraction unit <b>502</b>. The object discrimination unit <b>511</b> accesses information resources on the cloud as necessary to perform the object discrimination processing (step S<b>1308</b>).</p><p id="p-0210" num="0210">The related information retrieval acquisition unit <b>512</b> performs processing of retrieving the related information of the object discriminated by the object discrimination unit <b>511</b> on the cloud to acquire (step S<b>1309</b>).</p><p id="p-0211" num="0211">The related information output control unit <b>513</b> performs output control for presenting the related information of the object acquired by retrieving on the cloud by the related information retrieval acquisition unit <b>512</b> to the user by the OSD, for example (step S<b>1310</b>).</p><p id="p-0212" num="0212">Next, the video output control unit <b>504</b> performs the output control of the video stream on the basis of the coordinates of the object output from the video tracking unit <b>503</b> (step S<b>1305</b>). The video output control herein mentioned includes framing, zooming, resolution conversion, and luminance dynamic range conversion for the object at the gazing point and objects around the gazing point. Furthermore, the video output control unit <b>504</b> superimposes the OSD generated by the related information output control unit <b>513</b> on the video stream.</p><p id="p-0213" num="0213">Various types of display devices such as a liquid crystal display, an organic EL display, a self-luminous display, a 3D display, and a holographic display are assumed as the image display unit <b>107</b>. Therefore, the video output control unit <b>504</b> performs adjustment processing according to the type of the display on the video stream subjected to the framing, zooming, and other image quality processing based on the gazed object (step S<b>1306</b>).</p><p id="p-0214" num="0214">Then, the video processed by the video output control unit <b>504</b> is output by the image display unit <b>107</b> (step S<b>1307</b>).</p><p id="p-0215" num="0215"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates the configuration example of the video signal processing unit <b>105</b> that performs the processing of retrieving the information related to the object gazed by the user to output; note that, as a modification, a server (for example, an artificial intelligence server) on the cloud may perform similar processing. One effect of performing the processing on the server side is that deep learning and relearning may be performed by collecting learning data from a large number of edges while providing the related information of the object to the edge together with the content.</p><p id="p-0216" num="0216"><figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates a functional configuration example of an artificial intelligence server <b>2500</b> that outputs the related information of the object gazed by the user. The artificial intelligence server <b>2500</b> is assumed to be installed on the cloud. Furthermore, a video source <b>2501</b> as a target of the image creation processing by the artificial intelligence server <b>2500</b> may be any of a broadcasting station, a stream distribution server, or a recording medium.</p><p id="p-0217" num="0217">The sensor information detected by the sensor unit <b>109</b> on the content reproduction device <b>100</b> side is input to a gazing point discrimination unit <b>2511</b> in the artificial intelligence server <b>2500</b> via a network. The gazing point discrimination unit <b>2511</b> discriminates the gazing point gazed by the user in the video input from the video source <b>2501</b>. Note that, since the gazing point discrimination unit <b>2511</b>, an object feature extraction unit <b>2512</b>, a video tracking unit <b>2513</b>, and a video output control unit <b>2514</b> are the same as the functional modules having the same name and the same reference number in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a detailed description thereof is herein omitted or the description is minimized.</p><p id="p-0218" num="0218">The object feature extraction unit <b>2512</b> extracts the feature of the object corresponding to the gazing point discriminated by the gazing point discrimination unit <b>2511</b>. The object discrimination unit <b>2515</b> discriminates an object on the basis of the feature of the object at the gazing point extracted by the object feature extraction unit <b>502</b>. The object discrimination unit <b>2515</b> may perform the object discrimination processing by using a machine learning model of which deep learning is performed so as to estimate the object from the feature of the object.</p><p id="p-0219" num="0219">A related information retrieval acquisition unit <b>2516</b> performs processing of retrieving the related information of the object discriminated by the object discrimination unit <b>2515</b> on the cloud to acquire. The related information retrieval acquisition unit <b>2516</b> may perform retrieval acquisition processing of the related information by using the machine learning model of which deep learning is performed so as to perform retrieval processing by estimating a retrieval method (retrieval keyword or retrieval conditional expression) and the like of the related information according to the object.</p><p id="p-0220" num="0220">A related information output control unit <b>2517</b> performs output control for presenting the related information of the object acquired by retrieving on the cloud by the related information retrieval acquisition unit <b>2516</b> to the user. There are various methods of presenting the related information to the user. The related information output control unit <b>2517</b> may generate the related information in a format of auxiliary data such as subtitles, meta information of the video stream and the like.</p><p id="p-0221" num="0221">The video output control unit <b>2514</b> performs the output control of the video stream on the basis of the coordinates of the object output from the video tracking unit <b>2513</b> as described above. Furthermore, the video output control unit <b>2514</b> outputs the related information generated by the related information output control unit <b>2517</b> in a format of subtitles, auxiliary data, meta information and the like together with the video stream.</p><p id="p-0222" num="0222">Then, the video stream subjected to the image creation processing in the artificial intelligence server <b>2500</b> is subjected to encoding processing in compliance with a predetermined encoding standard such as MPEG-2 systems by a video encoding unit <b>2502</b>, and then distributed to the content reproduction device <b>100</b>.</p><p id="p-0223" num="0223">On the content reproduction device <b>100</b> side, the video decoding unit <b>102</b> performs the decoding processing on the received encoded video stream, and the auxiliary data decoding unit <b>104</b> performs the decoding processing on the received auxiliary data stream. Then, the decoded video stream is subjected to the video signal processing including the image quality enhancement and the like by the video signal processing unit <b>105</b>, and then displayed by the image display unit <b>107</b> together with the related information.</p><p id="p-0224" num="0224">D-3. Image Sound Matching Processing</p><p id="p-0225" num="0225">For example, in the content of a movie or a drama, there is a plurality of sound sources such as a plurality of characters appearing in the video frame. Furthermore, even in a scene in which only one character appears, when the character moves, the sound source moves. Furthermore, in a case where the framing and zooming is performed on the object at the gazing point of the user or the objects around the gazing point as the video signal processing, the position of the object that moves in accordance with the framing and zooming moves from the position in an original content (as described above). In short, the position of the sound source present in the video frame is indefinite. For example, in a 2D display and a 3D display of a large screen, an image sound mismatch phenomenon in which an image of the object (sound source) appearing on the screen and a sound image thereof do not match is remarkably perceived by the user. For example, in a case of using a display with improved depth perception such as a holographic display, the mismatch between the image and the sound image in the depth direction becomes remarkable. Due to the image sound mismatch phenomenon, it becomes difficult for the user to identify the sound source (of which object the voice is), and the user feels uncomfortable.</p><p id="p-0226" num="0226">Therefore, the content reproduction device <b>100</b> according to this embodiment performs processing (hereinafter, also referred to as &#x201c;image sound matching processing&#x201d;) of matching the image of the object serving as the sound source with the sound image thereof. Specifically, in the content reproduction device <b>100</b> according to this embodiment, the image of the object serving as the sound source appearing in the video is detected, and optimal sounding coordinates for emitting the audio signal of the sound source are estimated on the basis of the display position of the object in the video frame. Then, the audio output unit <b>108</b> is allowed to perform processing of generating the sound image of the sound source at the estimated sounding position. The sound image generation processing includes processing of determining a direction and a volume of the sound at the sounding position and determining a combination of speakers for generating the sound image. By performing the image sound matching processing, reality of the content to be reproduced increases, and realistic feeling may be provided to the user. When the image sound matching processing is applied to the 3D display, the effect thereof is large.</p><p id="p-0227" num="0227"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a functional configuration example of the signal processing unit <b>150</b> that performs the image sound matching processing. The illustrated signal processing unit <b>150</b> is a functional module in which the functions of the video signal processing unit <b>105</b> and the audio signal processing unit <b>106</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref> are integrated, and is provided with a sounding coordinate estimation unit <b>1401</b>, a video output control unit <b>1402</b>, and an audio output control unit <b>1403</b>. The decoded video stream from the video decoding unit <b>102</b> and the decoded audio stream from the audio decoding unit <b>103</b> are input to the signal processing unit <b>150</b>.</p><p id="p-0228" num="0228">The decoded video stream from the video decoding unit <b>102</b> and the decoded audio stream from the audio decoding unit <b>103</b> are input to the sounding coordinate estimation unit <b>1401</b>. Note that, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in a case where the framing and zooming are performed on the object at the gazing point of the user and the objects around the gazing point, in the sounding coordinate estimation unit <b>1401</b>, the framing and zooming processing of the video is performed on the basis of the sensor information and the like, and estimation processing of the sounding coordinates is performed on the basis of the processed video stream.</p><p id="p-0229" num="0229">Then, the sounding coordinate estimation unit <b>1401</b> estimates the display position of the sounding object serving as the sound source in the video and outputs the same as the sounding coordinates. In a case where the audio signals of a plurality of objects are superimposed on the audio stream, the sounding coordinate estimation unit <b>1401</b> separates the waveform of the audio signal for each object from an original audio stream, and estimates the sounding coordinates for each separated object. Note that, in a case where the audio stream follows the object-based audio, since the individual sounding objects are not mixed, the sounding coordinate estimation unit <b>1401</b> does not need to perform separation processing of the audio signals for each object, and may easily calculate the display position, size, and direction of each sounding object on the basis of the localization information included in the meta information.</p><p id="p-0230" num="0230">Furthermore, as described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> and the like, in a case where the framing and zooming are performed on the object at the gazing point of the user and the objects around the gazing point, the sounding coordinate estimation unit <b>1401</b> estimates the gazing point on the basis of the sensor information, and estimates the sounding coordinates of each sounding object from the video in consideration of a result of framing and zooming on the basis of the gazing point.</p><p id="p-0231" num="0231">In a case where the image output unit <b>107</b> uses the 2D display, the sounding coordinate estimation unit <b>1401</b> estimates two-dimensional sounding coordinates. Furthermore, in a case where the image output unit <b>107</b> is a 3D display having depth information such as a holographic display, the sounding coordinate estimation unit <b>1401</b> estimates three-dimensional sounding coordinates. Furthermore, in a case where a plurality of sounding objects is estimated, the sounding coordinate estimation unit <b>1401</b> may also estimate the volume for each sounding object (or a volume ratio between the sounding objects).</p><p id="p-0232" num="0232">The sounding coordinate estimation unit <b>1401</b> performs estimation processing of the sounding coordinates of each sounding object by using the machine learning model of which deep learning is performed so as to estimate the sounding coordinates of the object serving as the sound source in the video from the input video stream and audio stream. Note that, there also is a method of detecting the sound source appearing in the video by video analysis and audio analysis of the input video stream and audio stream, and calculating the display position of the sound source on the screen, but it is difficult to perform calculation processing in real time. Therefore, as described above, in this embodiment, the sounding coordinate estimation unit <b>1401</b> uses the machine learning model.</p><p id="p-0233" num="0233">The video output control unit <b>1402</b> performs the image quality enhancement processing such as super resolution processing or dynamic range expansion on the video stream. In a case where the framing and zooming of the video is determined by the sounding coordinate estimation unit <b>1401</b>, the video output control unit <b>1402</b> performs the framing or zooming processing on the original video frame. Furthermore, the video output control unit <b>1402</b> performs adjustment processing according to the type of the display such as a liquid crystal display, an organic EL display, a self-luminous display, a 3D display, and a holographic display. Then, the video processed by the video output control unit <b>1402</b> is output by the image display unit <b>107</b>.</p><p id="p-0234" num="0234">The audio output control unit <b>1403</b> controls drive of the audio output unit <b>108</b> for generating the sound image of each sounding object in the video according to the sounding coordinates estimated by the sounding coordinate estimation unit <b>1401</b>. Specifically, for example, in a case where the audio output unit <b>108</b> includes a plurality of speakers, the audio output control unit <b>1403</b> determines the direction and the volume of the sound at the sounding position for each sounding object for which the sounding coordinates are estimated, determines a combination of the speakers for generating the sound image, and sets the volume and the direction of the sound to be output from each speaker.</p><p id="p-0235" num="0235">Note that, although not illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, in a case of performing the framing and zooming, and image quality processing on the basis of the object gazed by the user in the video signal processing unit <b>105</b>, the audio output control unit <b>1403</b> may perform the audio signal processing weighted on the basis of a gazing degree of the user; for example, sound quality enhancement processing such as band extension is applied or a special effect is applied to the audio signal emitted from the sounding object gazed by the user.</p><p id="p-0236" num="0236">The machine learning model used in the sounding coordinate estimation processing by the sounding coordinate estimation unit <b>1401</b> is represented by the neural network. The deep learning of the neural network used by the sounding coordinate estimation unit <b>1401</b> may be performed on the artificial intelligence server <b>1500</b> illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref>. Since the configuration and the operation of the artificial intelligence server <b>1500</b> are already described, the description thereof is herein omitted.</p><p id="p-0237" num="0237">A procedure of the image sound matching processing executed in the signal processing unit <b>150</b> illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref> is similar to that in <figref idref="DRAWINGS">FIG. <b>16</b></figref>. Since the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref> is already described, the description thereof is herein omitted.</p><p id="p-0238" num="0238"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates the functional configuration example in which the image sound matching processing of the content is executed by the edge, that is, the content reproduction device <b>100</b> side. As a modification, the image sound matching processing may be performed by a server (for example, an artificial intelligence server) on the cloud. For example, by using the artificial intelligence server <b>2400</b> illustrated in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the image sound matching processing may be performed on the content before the distribution to the edge. Since the artificial intelligence server <b>2400</b> is already described, the description thereof is herein omitted.</p><heading id="h-0011" level="1">INDUSTRIAL APPLICABILITY</heading><p id="p-0239" num="0239">The present disclosure is heretofore described in detail with reference to the specific embodiment. However, it is obvious that one skilled in the art may modify or substitute the embodiment without departing from the scope of the present disclosure.</p><p id="p-0240" num="0240">In this specification, the embodiment in which the present disclosure is applied to the television receiver has been mainly described, but the gist of the present disclosure is not limited thereto. The present disclosure may be similarly applied to various types of devices, for example, a personal computer, a smartphone, a tablet, a head-mounted display, a media player and the like that present, to a user, contents acquired by streaming or downloading via a broadcast wave or the Internet, or contents reproduced from a recording medium.</p><p id="p-0241" num="0241">In short, the present disclosure is heretofore described in a form of an example and the content described in this specification should not be interpreted in a limited manner. In order to determine the scope of the present technology, claims should be taken into consideration.</p><p id="p-0242" num="0242">Note that, the present disclosure may also have the following configuration.</p><p id="p-0243" num="0243">(1) An information processing device provided with:</p><p id="p-0244" num="0244">an estimation unit that estimates sounding coordinates at which a sound image is generated on the basis of a video stream and an audio stream;</p><p id="p-0245" num="0245">a video output control unit that controls an output of the video stream; and</p><p id="p-0246" num="0246">an audio output control unit that controls an output of the audio stream so as to generate the sound image at the sounding coordinates.</p><p id="p-0247" num="0247">(2) The information processing device according to (1) above, in which</p><p id="p-0248" num="0248">the estimation unit estimates the sounding coordinates using a machine learning model.</p><p id="p-0249" num="0249">(3) The information processing device according to (1) or (2) above, in which</p><p id="p-0250" num="0250">the estimation unit estimates the sounding coordinates for generating the sound image of a predetermined object serving as a sound source in video.</p><p id="p-0251" num="0251">(4) The information processing device according to any one of (1) to (3) above, in which</p><p id="p-0252" num="0252">the estimation unit estimates the sounding coordinates for generating the sound image of an object gazed by a user who views video and audio.</p><p id="p-0253" num="0253">(5) The information processing device according to (4) above, further provided with:</p><p id="p-0254" num="0254">a discrimination unit that discriminates a gazing point of the user who views video and audio, in which</p><p id="p-0255" num="0255">the estimation unit estimates the sounding coordinates at which the sound image of the object gazed by the user is generated on the basis of a discrimination result of the discrimination unit.</p><p id="p-0256" num="0256">(5-1) The information processing device according to (5) above, in which</p><p id="p-0257" num="0257">the discrimination unit discriminates the gazing point of the user using a machine learning model.</p><p id="p-0258" num="0258">(5-2) The information processing device according to (5) or (6) above, in which</p><p id="p-0259" num="0259">the discrimination unit discriminates the gazing point of the user on the basis of a result of detecting a state related to the user.</p><p id="p-0260" num="0260">(6) The information processing device according to (5) above, in which</p><p id="p-0261" num="0261">the state includes at least one of a position of the user, a direction of a face or a line of sight of the user, the number of users, or an indoor environment in which viewing is performed.</p><p id="p-0262" num="0262">(7) The information processing device according to (5) or (6) above, in which</p><p id="p-0263" num="0263">the video output control unit performs rendering of the video on the basis of a result of discriminating a gazing degree of the user.</p><p id="p-0264" num="0264">(8) The information processing device according to (7) above, in which</p><p id="p-0265" num="0265">the video output control unit performs the rendering of the video using a machine learning model.</p><p id="p-0266" num="0266">(9) The information processing device according to (7) or (8) above, in which</p><p id="p-0267" num="0267">the rendering includes at least one of framing or zooming processing of the video.</p><p id="p-0268" num="0268">(10) The information processing device according to any one of (7) to (9) above, in which</p><p id="p-0269" num="0269">the video output control unit performs the rendering on the basis of a result of tracking the object gazed by the user.</p><p id="p-0270" num="0270">(11) The information processing device according to any one of (7) to (10) above, in which</p><p id="p-0271" num="0271">the estimation unit estimates the sounding coordinates on the basis of the video subjected to framing or zooming processing.</p><p id="p-0272" num="0272">(11-1) The information processing device according to any one of (3) to (11) above, in which</p><p id="p-0273" num="0273">the estimation unit estimates a sounding position of a sound image, and a direction and a volume of sound on the basis of a display position, a direction, or a size of an object in video.</p><p id="p-0274" num="0274">(11-2) The information processing device according to any one of (1) to (11) above, in which</p><p id="p-0275" num="0275">the audio output control unit generates the sound image by controlling a combination of speakers to be used among a plurality of usable speakers, and a direction and a volume of sound output from each speaker.</p><p id="p-0276" num="0276">(11-3) The information processing device according to any one of (1) to (11) above, in which</p><p id="p-0277" num="0277">the video output control unit controls an output of the video stream to a 2D or 3D display.</p><p id="p-0278" num="0278">(12) The information processing device according to (9) above, in which</p><p id="p-0279" num="0279">the framing or zooming processing of the video is performed stepwise or at a predetermined speed to a target value.</p><p id="p-0280" num="0280">(13) The information processing device according to any one of (1) to (12) above, in which</p><p id="p-0281" num="0281">the video stream is a stream of 3D video.</p><p id="p-0282" num="0282">(14) The information processing device according to (13) above, in which</p><p id="p-0283" num="0283">the estimation unit generates the sound image according to a 3D depth or a direction of 3D display of a predetermined object serving as a sound source included in the video stream.</p><p id="p-0284" num="0284">(15) The information processing device according to (13) above, in which</p><p id="p-0285" num="0285">the estimation unit generates the sound image according to a 3D depth and a direction of 3D display of a predetermined object serving as a sound source included in the video stream.</p><p id="p-0286" num="0286">(16) The information processing device according to any one of (1) to (15) above, further provided with:</p><p id="p-0287" num="0287">a display unit capable of performing 3D display using binocular parallax.</p><p id="p-0288" num="0288">(17) The information processing device according to any one of (5) to (13) above, further provided with:</p><p id="p-0289" num="0289">an acquisition unit that acquires related information of the object discriminated on the basis of a feature of the object corresponding to the gazing point; and</p><p id="p-0290" num="0290">a related information output control unit that controls an output of the acquired related information, in which</p><p id="p-0291" num="0291">the video output control unit controls to output the related information together with the video stream.</p><p id="p-0292" num="0292">(18) The information processing device according to (17) above that extracts the feature of the object or acquires the related information using a machine learning model.</p><p id="p-0293" num="0293">(19) The information processing device according to any one of (1) to (18) above, in which</p><p id="p-0294" num="0294">the audio stream includes meta information of object-based audio.</p><p id="p-0295" num="0295">(20) An information processing method provided with:</p><p id="p-0296" num="0296">an estimation step of estimating sounding coordinates at which a sound image is generated on the basis of a video stream and an audio stream;</p><p id="p-0297" num="0297">a video output control step of controlling an output of the video stream; and</p><p id="p-0298" num="0298">an audio output control step of controlling an output of the audio stream so as to generate the sound image at the sounding coordinates.</p><heading id="h-0012" level="1">REFERENCE SIGNS LIST</heading><p id="p-0299" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0299"><b>100</b> Content reproduction device</li>    <li id="ul0002-0002" num="0300"><b>101</b> Demultiplexer</li>    <li id="ul0002-0003" num="0301"><b>102</b> Video decoding unit</li>    <li id="ul0002-0004" num="0302"><b>103</b> Audio decoding unit</li>    <li id="ul0002-0005" num="0303"><b>104</b> Auxiliary data decoding unit</li>    <li id="ul0002-0006" num="0304"><b>105</b> Image signal processing unit</li>    <li id="ul0002-0007" num="0305"><b>106</b> Audio signal processing unit</li>    <li id="ul0002-0008" num="0306"><b>107</b> Image display unit</li>    <li id="ul0002-0009" num="0307"><b>108</b> Audio output unit</li>    <li id="ul0002-0010" num="0308"><b>109</b> Sensor unit</li>    <li id="ul0002-0011" num="0309"><b>110</b> External user interface unit</li>    <li id="ul0002-0012" num="0310"><b>150</b> Signal processing unit</li>    <li id="ul0002-0013" num="0311"><b>410</b> Camera unit</li>    <li id="ul0002-0014" num="0312"><b>411</b> to <b>413</b> Camera</li>    <li id="ul0002-0015" num="0313"><b>420</b> User state sensor unit</li>    <li id="ul0002-0016" num="0314"><b>430</b> Environment sensor unit</li>    <li id="ul0002-0017" num="0315"><b>440</b> Device state sensor unit</li>    <li id="ul0002-0018" num="0316"><b>450</b> User profile sensor unit</li>    <li id="ul0002-0019" num="0317"><b>501</b> Gazing point discrimination unit</li>    <li id="ul0002-0020" num="0318"><b>502</b> Object feature extraction unit</li>    <li id="ul0002-0021" num="0319"><b>503</b> Video tracking unit</li>    <li id="ul0002-0022" num="0320"><b>504</b> Video output control unit</li>    <li id="ul0002-0023" num="0321"><b>511</b> Object discrimination unit</li>    <li id="ul0002-0024" num="0322"><b>512</b> Related information retrieval acquisition unit</li>    <li id="ul0002-0025" num="0323"><b>513</b> Related information output control unit</li>    <li id="ul0002-0026" num="0324"><b>521</b> Sounding coordinate estimation unit</li>    <li id="ul0002-0027" num="0325"><b>522</b> Audio output control unit</li>    <li id="ul0002-0028" num="0326"><b>600</b> Artificial intelligence server (for gazing point discrimination)</li>    <li id="ul0002-0029" num="0327"><b>601</b> Learning data database</li>    <li id="ul0002-0030" num="0328"><b>602</b> Neural network</li>    <li id="ul0002-0031" num="0329"><b>603</b> Evaluation unit</li>    <li id="ul0002-0032" num="0330"><b>700</b> Artificial intelligence server (for object feature extraction)</li>    <li id="ul0002-0033" num="0331"><b>701</b> Learning data database</li>    <li id="ul0002-0034" num="0332"><b>702</b> Neural network</li>    <li id="ul0002-0035" num="0333"><b>703</b> Evaluation unit</li>    <li id="ul0002-0036" num="0334"><b>800</b> Artificial intelligence server (for video output control)</li>    <li id="ul0002-0037" num="0335"><b>801</b> Learning data database</li>    <li id="ul0002-0038" num="0336"><b>802</b> Neural network</li>    <li id="ul0002-0039" num="0337"><b>803</b> Evaluation unit</li>    <li id="ul0002-0040" num="0338"><b>1100</b> Artificial intelligence server (for object discrimination processing)</li>    <li id="ul0002-0041" num="0339"><b>1101</b> Learning data database</li>    <li id="ul0002-0042" num="0340"><b>1102</b> Neural network</li>    <li id="ul0002-0043" num="0341"><b>1103</b> Evaluation unit</li>    <li id="ul0002-0044" num="0342"><b>1200</b> Artificial intelligence server (for related information retrieval acquisition processing)</li>    <li id="ul0002-0045" num="0343"><b>1201</b> Learning data database</li>    <li id="ul0002-0046" num="0344"><b>1202</b> Neural network</li>    <li id="ul0002-0047" num="0345"><b>1203</b> Evaluation unit</li>    <li id="ul0002-0048" num="0346"><b>1401</b> Sounding coordinate estimation unit</li>    <li id="ul0002-0049" num="0347"><b>1402</b> Video output control unit</li>    <li id="ul0002-0050" num="0348"><b>1403</b> Audio output control unit</li>    <li id="ul0002-0051" num="0349"><b>1501</b> Learning data database</li>    <li id="ul0002-0052" num="0350"><b>1502</b> Neural network</li>    <li id="ul0002-0053" num="0351"><b>1503</b> Evaluation unit</li>    <li id="ul0002-0054" num="0352"><b>2300</b> Artificial intelligence server</li>    <li id="ul0002-0055" num="0353"><b>2301</b> Video source</li>    <li id="ul0002-0056" num="0354"><b>2302</b> Video decoding unit</li>    <li id="ul0002-0057" num="0355"><b>2311</b> Gazing point discrimination unit</li>    <li id="ul0002-0058" num="0356"><b>2312</b> Object feature extraction unit</li>    <li id="ul0002-0059" num="0357"><b>2313</b> Video tracking unit</li>    <li id="ul0002-0060" num="0358"><b>2314</b> Framing/scaling processing unit</li>    <li id="ul0002-0061" num="0359"><b>2400</b> Artificial intelligence server</li>    <li id="ul0002-0062" num="0360"><b>2401</b> Audio source</li>    <li id="ul0002-0063" num="0361"><b>2411</b> Sounding coordinate estimation unit</li>    <li id="ul0002-0064" num="0362"><b>2412</b> Audio signal processing unit</li>    <li id="ul0002-0065" num="0363"><b>2413</b> Audio encoding unit</li>    <li id="ul0002-0066" num="0364"><b>2500</b> Artificial intelligence server</li>    <li id="ul0002-0067" num="0365"><b>2501</b> Video source</li>    <li id="ul0002-0068" num="0366"><b>2502</b> Video encoding unit</li>    <li id="ul0002-0069" num="0367"><b>2511</b> Gazing point discrimination unit</li>    <li id="ul0002-0070" num="0368"><b>2512</b> Object feature extraction unit</li>    <li id="ul0002-0071" num="0369"><b>2513</b> Video tracking unit</li>    <li id="ul0002-0072" num="0370"><b>2514</b> Video output control unit</li>    <li id="ul0002-0073" num="0371"><b>2515</b> Object discrimination unit</li>    <li id="ul0002-0074" num="0372"><b>2516</b> Related information retrieval acquisition unit</li>    <li id="ul0002-0075" num="0373"><b>2517</b> Related information output control unit</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing device comprising:<claim-text>an estimation unit that estimates sounding coordinates at which a sound image is generated on a basis of a video stream and an audio stream;</claim-text><claim-text>a video output control unit that controls an output of the video stream; and</claim-text><claim-text>an audio output control unit that controls an output of the audio stream so as to generate the sound image at the sounding coordinates.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the estimation unit estimates the sounding coordinates using a machine learning model.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the estimation unit estimates the sounding coordinates for generating the sound image of a predetermined object serving as a sound source in video.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the estimation unit estimates the sounding coordinates for generating the sound image of an object gazed by a user who views video and audio.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processing device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:<claim-text>a discrimination unit that discriminates a gazing point of the user who views video and audio, wherein</claim-text><claim-text>the estimation unit estimates the sounding coordinates at which the sound image of the object gazed by the user is generated on a basis of a discrimination result of the discrimination unit.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processing device according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the discrimination unit discriminates the gazing point of the user using a machine learning model.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processing device according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the video output control unit performs rendering of the video on a basis of a result of discriminating a gazing degree of the user.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processing device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the video output control unit performs the rendering of the video using a machine learning model.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The information processing device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the rendering includes at least one of framing or zooming processing of the video.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The information processing device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the video output control unit performs the rendering on a basis of a result of tracking the object gazed by the user.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The information processing device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the estimation unit estimates the sounding coordinates on a basis of the video subjected to framing or zooming processing.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The information processing device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the framing or zooming processing of the video is performed stepwise or at a predetermined speed to a target value.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the video stream is a stream of 3D video.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The information processing device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein<claim-text>the estimation unit generates the sound image according to a 3D depth or a direction of 3D display of a predetermined object serving as a sound source included in the video stream.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The information processing device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein<claim-text>the estimation unit generates the sound image according to a 3D depth and a direction of 3D display of a predetermined object serving as a sound source included in the video stream.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a display unit capable of performing 3D display using binocular parallax.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The information processing device according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:<claim-text>an acquisition unit that acquires related information of the object discriminated on a basis of a feature of the object corresponding to the gazing point; and</claim-text><claim-text>a related information output control unit that controls an output of the acquired related information, wherein</claim-text><claim-text>the video output control unit controls to output the related information together with the video stream.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The information processing device according to <claim-ref idref="CLM-00017">claim 17</claim-ref> that extracts the feature of the object or acquires the related information using a machine learning model.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the audio stream includes meta information of object-based audio.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. An information processing method comprising:<claim-text>an estimation step of estimating sounding coordinates at which a sound image is generated on a basis of a video stream and an audio stream;</claim-text><claim-text>a video output control step of controlling an output of the video stream; and</claim-text><claim-text>an audio output control step of controlling an output of the audio stream so as to generate the sound image at the sounding coordinates.</claim-text></claim-text></claim></claims></us-patent-application>