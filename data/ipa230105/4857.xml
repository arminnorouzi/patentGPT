<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004858A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004858</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364360</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SOFTWARE BUILD MANAGEMENT USING MACHINE LEARNING MODEL</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only"><addressbook><orgname>Oracle International Corporation</orgname><address><city>Redwood Shores</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Santhanagopal</last-name><first-name>Harish</first-name><address><city>Fremont</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Jiun-Cheng</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Oracle International Corporation</orgname><role>02</role><address><city>Redwood Shores</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques for managing a software build using a machine learning model are disclosed. A system obtains historical data associated with historical software builds. The historical data includes attribute data for a plurality of development stages associated with a historical software build and labels indicating success or failure for the plurality of development stages. The system trains a machine learning model using the historical data associated with the historical software builds to generate predictions of success or failure of the plurality of development stages. The system receives attributes of a target software build and a selection of a first target development stage of the target software build. The system applies the machine learning model to the target software build to generate a first prediction of success or failure of the first target development stage.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="168.06mm" wi="158.75mm" file="US20230004858A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="230.80mm" wi="184.32mm" file="US20230004858A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="218.19mm" wi="177.72mm" file="US20230004858A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="192.28mm" wi="140.55mm" file="US20230004858A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="228.09mm" wi="174.84mm" file="US20230004858A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="228.09mm" wi="174.84mm" file="US20230004858A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="228.09mm" wi="174.84mm" file="US20230004858A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="252.65mm" wi="188.98mm" file="US20230004858A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to managing a software build process using a machine learning model. In particular, the present disclosure relates to applying a machine learning model to predict the success or failure of development stages of a software build and modify attributes of the software build to avoid failures.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Software build development involves a number of stages, some of which must be performed sequentially, and others which can overlap. Resource, including both human resources, such as code programmers, and computing resources, such as processors and servers, may be shared among multiple stages. A failure in any one software build development stage may result in a failure of the entire software build. For example, even if a software build passes a compile stage, the software build will still fail if it does not pass a security scan stage. Given the variety of potential software build specifications, software build characteristics, and resource allocation among development stages of a software build, it may be difficult for an entity interested in the success of the software build to identify potential sources of failure in the software build.</p><p id="p-0004" num="0003">The approaches described in this section are approaches that could be pursued, but not necessarily approaches that have been previously conceived or pursued. Therefore, unless otherwise indicated, it should not be assumed that any of the approaches described in this section qualify as prior art merely by virtue of their inclusion in this section.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0005" num="0004">The embodiments are illustrated by way of example and not by way of limitation in the figures of the accompanying drawings. It should be noted that references to &#x201c;an&#x201d; or &#x201c;one&#x201d; embodiment in this disclosure are not necessarily to the same embodiment, and they mean at least one. In the drawings:</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a system in accordance with one or more embodiments;</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example set of operations for managing a software build using a machine learning model in accordance with one or more embodiments;</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example set of operations for training a machine learning model in accordance with one or more embodiments;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example set of operations according to one embodiment;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example set of operations according to another embodiment;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example set of operations according to yet another embodiment; and</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a block diagram that illustrates a computer system in accordance with one or more embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0013" num="0012">In the following description, for the purposes of explanation, numerous specific details are set forth to provide a thorough understanding. One or more embodiments may be practiced without these specific details. Features described in one embodiment may be combined with features described in a different embodiment. In some examples, well-known structures and devices are described with reference to a block diagram form in order to avoid unnecessarily obscuring the present invention.</p><p id="p-0014" num="0013">1. GENERAL OVERVIEW</p><p id="p-0015" num="0014">2. SYSTEM ARCHITECTURE</p><p id="p-0016" num="0015">3. MACHINE LEARNING MANAGEMENT OF A SOFTWARE BUILD</p><p id="p-0017" num="0016">4. TRAINING A MACHINE LEARNING MODEL</p><p id="p-0018" num="0017">5. EXAMPLE EMBODIMENTS</p><p id="p-0019" num="0018">6. COMPUTER NETWORKS AND CLOUD NETWORKS</p><p id="p-0020" num="0019">7. MISCELLANEOUS; EXTENSIONS</p><p id="p-0021" num="0020">8. HARDWARE OVERVIEW</p><heading id="h-0005" level="1">1. General Overview</heading><p id="p-0022" num="0021">A software build development process includes multiple stages. A compiling stage converts a high-level code into machine code. In a testing stage, a system tests whether the software meets specification standards. In a security stage, a system identifies security vulnerabilities in the software. A failure at any stage may result in an overall failure of the software build and may affect the timing of dependent development stages. A failure as referred to herein may include failure to complete, failure to complete without errors, failure to complete within a particular time period, failure to complete within a set budget. In an example, two stages may share computing resources. A failure to complete one stage within a particular time may mean the other subsequent stage may not have access to the computing resources within a desired timetable.</p><p id="p-0023" num="0022">In one or more embodiments, a machine learning model predicts the success or failure of a software build by identifying the success or failure of the development stages of the software build development process. The system trains a machine learning model with a dataset including attributes of historical software builds and indications of whether the historical software builds were successful or not. The trained machine learning model is applied to a target set of software build attributes to predict the success or failure of the software build as a whole. In other words, the trained machine learning model predicts whether the software build would successfully pass each stage of the software build development process. Alternatively, or additionally, the trained machine learning model may predict the success or failure of individual development stages of the software build development process. The machine learning model may also identify attributes of the software build that may be modified to avoid a failure.</p><p id="p-0024" num="0023">For example, if the machine learning model determines that additional resources would be required to perform a compilation process within a specified time frame, the system may assign the additional human resources and computing resources to perform the code compilation. In addition, the machine learning model may identify one or more hardware or software upgrades required to successfully pass a development stage. For example, the machine learning model may identify a database upgrade required to successfully compile code for a particular software build. Alternatively, the machine learning model may identify a particular virtual machine upgrade required to successfully complete software testing. In addition, the system may modify a timing of two or more development stages. For example, the machine learning model may recommend allowing for more time to perform a compilation development stage.</p><p id="p-0025" num="0024">The system may change a ratio at which resources are assigned to two different development stages. For example, if a stress testing stage and a security scan stage overlap, the system may assign a higher priority for accessing computing resources to one stage over the other to reduce a likelihood of failure of the software build.</p><p id="p-0026" num="0025">In another example, the system modifies attributes of the software build by modifying a sequence in which two or more development stages or events (such as tests) within a single development stage occur.</p><p id="p-0027" num="0026">In one embodiment, the system modifies attributes of the software build by comparing the attributes of the software build to the attributes of other software builds that were successful. For example, the machine learning model may embed the attributes of a target software build as a vector in an n-dimensional space. If the model predicts a failure of the target software build, the system may compare the data point associated with a target software build to neighboring data points associated with historical software builds which were successful. The system may then identify attribute values of the neighboring data points that may be applied to the target software build to reduce the likelihood of failure of the target software build.</p><p id="p-0028" num="0027">One or more embodiments include notifying entities associated with different development stages of the software build of the success/failure predictions of the machine learning model. The system may associate different entities with different development stages. For example, a software coder may be associated with a pre-compile development stage and a compile development stage. A tester may be associated with a code testing stage. A supervisor may be associated with every stage. The system may generate a notification for the respective monitoring entity for each development stage. The notification may include the prediction of success or failure and any recommended modifications to the software build attributes.</p><p id="p-0029" num="0028">One or more embodiments described in this Specification and/or recited in the claims may not be included in this General Overview section.</p><heading id="h-0006" level="1">2. System Architecture</heading><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a system <b>100</b> in accordance with one or more embodiments. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, system <b>100</b> includes a software build engine <b>110</b>. The software build engine includes software build engines corresponding to development stages of a software build development process. The software build engines include a pre-compiler engine <b>111</b>, a compiler <b>112</b>, a software test engine <b>113</b>, a security scan engine <b>114</b>, a code quality test engine <b>115</b>, and a build stress test engine <b>116</b>.</p><p id="p-0031" num="0030">The pre-compiler engine <b>111</b> uses computing resources <b>117</b> to generate software code, analyze software attributes to identify dependencies within the software or between the software and one or more separate applications, and analyze software attributes to identify required upgrades to applications used by the software. The computing resources <b>117</b> include processors, memory, data transmission elements, and any other computing components required to execute software code and applications. The computing resources <b>117</b> may be shared among the software build development stages. For example, the computing resources <b>117</b> may be allocated by resource (e.g., a software build development stage has full access to the computing resource for the duration of the software build development stage), by time (e.g. two or more stages may be assigned different periods of time to access the same computing resource <b>117</b> over the duration of the software build development stages), and by priority (e.g. two or more stages may be allowed access the same computing resource <b>117</b> over the duration of the software build development stages, and one stage is given access priority over another).</p><p id="p-0032" num="0031">The compiler <b>112</b> uses computing resources <b>117</b> to convert software code from one programming language to another. For example, a programmer may utilize computing resources <b>117</b> assigned by the pre-compiler engine <b>111</b> to generate high-level software code. The compiler <b>112</b> may convert the high-level software code to machine code readable by a computer to execute the software. The compiler <b>112</b> may perform pre-processing to call or translate particular information prior to compiling; lexical analysis to convert one sequence of characters into a sequence of tokens or strings having pre-defined meaning for compilation; parsing of the computer code; semantic analysis of the computer code; conversion of the computer code to an intermediate representation prior to generating a final, translated code; code optimization, by selecting an optimal translation for the computer code to allow it to run faster, more efficiently, consume fewer resources, etc.; and generating an output translated computer code.</p><p id="p-0033" num="0032">The software test engine <b>113</b> uses the computing resources <b>117</b> to analyze the functionality of the software build, isolate bugs, and identify any defects. The software test engine <b>113</b> executes components of the software build and compares the result with a specification for the software build.</p><p id="p-0034" num="0033">The security scan engine <b>114</b> uses the computing resources <b>117</b> to analyze the software build to identify security weaknesses and determine whether the software meets a specified security level.</p><p id="p-0035" num="0034">The code quality test engine <b>115</b> uses the computing resources <b>117</b> to determine whether the functionality of the software build matches the desired specifications. The code quality test engine <b>115</b> may identify, for example: whether particular components meet a specified functionality, whether the software build includes each specified component, and whether additional computing resources <b>117</b> are needed to meet build specifications.</p><p id="p-0036" num="0035">The stress test engine <b>116</b> uses the computing resources <b>117</b> to test the performance of the software build under stress. For example, the stress test engine <b>116</b> may test performance of the software build when subjected to a data load beyond a threshold level or when provided with a number of access requests exceeding a threshold number of requests.</p><p id="p-0037" num="0036">In one or more embodiments, the software build engine <b>110</b> communicates with monitoring entities <b>140</b> regarding the progress, completion, success, or failure of the software build stages and the software build as a whole. The software build engine <b>110</b> may identify different monitoring entities <b>140</b> to be notified regarding different software build stages. For example, the monitoring entities may include: a team of programmers, a team of software testers, a security specialist, a company administrator, and two customers. The company administrator and the two customers may be designated to be notified regarding the success or failure of the software build as a whole. One of the two customers may be designated to be additionally notified of any delays in a schedule of completing any of the software build development stages. The team of programmers may be designated to be notified only regarding the success or failure of the pre-compiler stage, the compiler stage, and the software test stage. The team of testers may be designated to be notified only regarding the success or failure of the software test development stage. The security specialist may be designated to be notified regarding the success or failure of the security scan stage. The company administrator may be designated to be notified regarding the success or failure of each development stage and delays in completing any of the stages.</p><p id="p-0038" num="0037">A data repository <b>130</b> stores data attributes of the software build. In particular, the data repository <b>130</b> stores historical software build data <b>131</b>. The historical software build data <b>131</b> includes historical software build attribute data <b>132</b> and labels <b>133</b> associated with the attribute data <b>132</b>. The historical software build attribute data <b>132</b> includes attributes of a software build as a whole, as well as attributes of the individual development stages associated with the software build. Examples of attributes of a software build include functions, components, size, type, and specifications of the software build, as well as crashes, bugs, and failures identified during the development of the software build. Examples of attributes of the development stages associated with the software build include computing and human resources allocated to complete a particular development stage, a time required to complete the development stage, resources shared between two or more development stages, any conflicts among resources shared among the two or more development stages, and a sequence in which the development stages are carried out. The historical software build data <b>131</b> further includes labels <b>133</b> identifying a success or failure of individual software build stages and a software build as a whole.</p><p id="p-0039" num="0038">In one or more embodiments, a data repository <b>130</b> is any type of storage unit and/or device (e.g., a file system, database, collection of tables, or any other storage mechanism) for storing data. Further, a data repository <b>130</b> may include multiple different storage units and/or devices. The multiple different storage units and/or devices may or may not be of the same type or located at the same physical site. Further, a data repository <b>130</b> may be implemented or may execute on the same computing system as the software build engine <b>110</b>. Alternatively, or additionally, a data repository <b>130</b> may be implemented or executed on a computing system separate from the software build engine <b>110</b>. A data repository <b>130</b> may be communicatively coupled to the software build engine <b>110</b> via a direct connection or via a network.</p><p id="p-0040" num="0039">Information describing historical software build data <b>131</b> may be implemented across any of components within the system <b>100</b>. However, this information is illustrated within the data repository <b>130</b> for purposes of clarity and explanation.</p><p id="p-0041" num="0040">The software build engine <b>110</b> includes a machine learning engine <b>118</b>. The machine learning engine <b>118</b> trains one or more machine learning models <b>119</b>, based on the historical software build data <b>131</b>, to predict the success or failure of software build stages and/or the success or failure of an entire software build. In one embodiment, the machine learning model <b>119</b> may predict a likely success or failure of a software build stage or software build without identifying the specific cause of the software build failure. For example, training the machine learning model <b>119</b> may cause the machine learning model to identify a relationship between (1) a combination of a software function and particular allocation of resources, and (2) a failure of a security scan stage of the software build. When the machine learning model <b>119</b> is applied to a set of target software build attributes having the software function and the particular allocation of resources, the model <b>119</b> may generate a failure prediction. However, the machine learning model <b>119</b> may not be able to identify the particular attributes in the software that cause the security scan failure. In an alternative embodiment, the machine learning model <b>119</b> identifies one or more software build attribute modifications to reduce a likelihood of failure of the machine learning model.</p><p id="p-0042" num="0041">A software build attribute modification engine <b>120</b> may analyze one or both of: (1) the historical software attribute data <b>132</b> and (2) a prediction output from the machine learning engine <b>118</b> for a particular software build stage or software build to modify software build attributes or to generate recommendations for modification of software build attributes.</p><p id="p-0043" num="0042">For example, in one embodiment, software attribute data <b>132</b> is converted into a vector to be provided to a machine learning model <b>119</b> to generate a prediction of success or failure for a software build and a particular software build stage. The software build attribute modification engine <b>120</b> may locate vectors associated with different software builds in an n-dimensional space. If the machine learning model <b>119</b> predicts a failure of a target software build, the software build attribute modification engine <b>120</b> may identify one or more neighboring data points associated with other software builds. The software build attribute modification engine <b>120</b> may identify the closest data points associated with successful software builds. The engine <b>120</b> may identify software build attribute differences between the target software build and a data point associated with a success-type software build. The software build attribute modification engine <b>120</b> may then generate one or more recommendations for modifying attributes of the target software build based on the comparison.</p><p id="p-0044" num="0043">In one or more embodiments, the software build attribute modification engine <b>120</b> identifies the neighboring data points associated with success-type software builds by calculating a Euclidian distance between the data point associated with the target software build and the data points associated with the successful software builds in the n-dimensional space.</p><p id="p-0045" num="0044">In the above example in which the machine learning model predicts a failure of a security scan for a target software build, the software build attribute modification engine <b>120</b> may identify a neighboring data point associated with a successful software build. The software build attribute modification engine <b>120</b> may recommend increasing computing resources <b>117</b> available to the security scan engine <b>114</b>, modifying a ratio of resources between software stages or between different in-development software builds, modifying a sequence of development tasks by one or more software stages, or modifying a duration of one or more software build development stages. In one example, neither the machine learning model <b>119</b> nor the software build attribute modification engine <b>120</b> may identify the particular software attributes of the software build that may result in a failure of the software build. However, the modification of computing resources <b>117</b> allocated to the security scan engine <b>114</b> may result in the security scan engine <b>114</b> identifying and correcting a software attribute of the software build that causes the failure.</p><p id="p-0046" num="0045">The monitoring entities <b>140</b> may interact with the software build engine <b>110</b> via the user interface <b>111</b>. For example, the software build engine <b>110</b> may notify the monitoring entities <b>140</b> of the predictions of success or failure of a software build development stage, or of the entire software build, via the user interface <b>111</b>. In one embodiment, the machine learning engine <b>118</b> generates a success/failure prediction of both an entire software build and of individual development stages in the software build process. The software build engine <b>110</b> may notify a monitoring entity associated with a particular software build development stage of the success/failure prediction for the individual stage, as well as of any recommendations for modifying attributes associated with the particular development stage. The software build engine <b>110</b> may notify monitoring entities <b>140</b> via email, enterprise-based messaging applications, publicly-sold messaging applications, or any other communication medium. For example, an enterprise may utilize a messaging application in which users may belong to one or more different groups, each group composed of different sets of members. The software build engine <b>110</b> may be configured to notify particular groups, via the messaging application, of the predicted or actual failure of a software build stage. The software build engine <b>110</b> may further be configured to notify the particular groups of suggested recommended software build attribute modifications. The software build engine <b>110</b> may further notify one group when a software build attribute modification associated with a different development stage affects resources available to, or timing for completion of, the stage associated with the group.</p><p id="p-0047" num="0046">In one or more embodiments, the system <b>100</b> may include more or fewer components than the components illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The components illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be local to or remote from each other. The components illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be implemented in software and/or hardware. Each component may be distributed over multiple applications and/or machines. Multiple components may be combined into one application and/or machine. Operations described with respect to one component may instead be performed by another component.</p><p id="p-0048" num="0047">Additional embodiments and/or examples relating to computer networks are described below in Section <b>6</b>, titled &#x201c;Computer Networks and Cloud Networks.&#x201d;</p><p id="p-0049" num="0048">In one or more embodiments, software build engine <b>110</b> refers to hardware and/or software configured to perform operations described herein for managing development of a software build, predicting success or failure of the software build, and modifying software build attributes. Examples of operations for applying a machine learning model to manage a software build are described below with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0050" num="0049">In an embodiment, the software build engine <b>110</b> is implemented on one or more digital devices. The term &#x201c;digital device&#x201d; generally refers to any hardware device that includes a processor. A digital device may refer to a physical device executing an application or a virtual machine. Examples of digital devices include a computer, a tablet, a laptop, a desktop, a netbook, a server, a web server, a network policy server, a proxy server, a generic machine, a function-specific hardware device, a hardware router, a hardware switch, a hardware firewall, a hardware firewall, a hardware network address translator (NAT), a hardware load balancer, a mainframe, a television, a content receiver, a set-top box, a printer, a mobile handset, a smartphone, a personal digital assistant (&#x201c;PDA&#x201d;), a wireless receiver and/or transmitter, a base station, a communication management device, a router, a switch, a controller, an access point, and/or a client device.</p><heading id="h-0007" level="1">3. Software Build Management Using Machine Learning Model</heading><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example set of operations for managing a software build using a machine learning model in accordance with one or more embodiments. One or more operations illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be modified, rearranged, or omitted all together. Accordingly, the particular sequence of operations illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> should not be construed as limiting the scope of one or more embodiments.</p><p id="p-0052" num="0051">A system obtains target software build attributes (Operation <b>202</b>). Software build attributes include characteristics of the software build code, specifications for which the software build is designed, and computing and human resources assigned to software build development stages. For example, one set of software build attributes may include a set of specifications provided by a customer specifying desired functionality of a software application, a number of human programmers, testers, and managers assigned to code, test, and supervise the development of the software build, hardware resources assigned to different stages of the software build development, and characteristics of the software build code, including a size, application and hardware compatibility features, computing requirements, and application programming interface (API) specifications.</p><p id="p-0053" num="0052">The system applies a machine learning model to the target software build attributes to generate a prediction of a success or failure of the target software build (Operation <b>204</b>). The machine learning model has been trained using historical software build attribute data to identify correlations among software build attributes and the success or failure of the software builds. In one embodiment, the system predicts a success or failure of separate software build development stages together with a prediction associated with the entire build. For example, the model may predict the software build will compile successfully and test successfully, but the build will fail a code quality test and a security scan.</p><p id="p-0054" num="0053">Examples of measures of success or failure include: whether a stage or build is completed on time; whether a build is susceptible to crashes or errors; whether a build performs specified functions within specified periods of time; whether a build is developed within a specified period of time; whether a build generates accurate results; whether a build meets specified compatibility criteria with other applications or with specified hardware; whether a build meets specified security criteria; whether a build meets specified code structure or format criteria; and whether the build operates within specified hardware constraints (such as providing a particular performance level when operating on a particular class of hardware).</p><p id="p-0055" num="0054">The system provides the success or failure prediction to a monitoring entity (Operation <b>206</b>). The system may identify particular monitoring entities associated with particular software build stages, and provide the success or failure predictions only to the monitoring entities associated with the particular software build stages. For example, if two customers are interested in a software build, one customer may request notifications only for a final success or failure prediction for the entire software build. The other customer may request notifications for predicted failures of the security scan of the software build.</p><p id="p-0056" num="0055">The system determines whether a target software build development stage is associated with a failure prediction by the machine learning model (Operation <b>208</b>). If the target software build development stage is associated with a failure prediction, the system may modify attributes of the target development stage or of a related stage. Modifying the attributes of the target stage or a related stage may include, for example: identifying components of the software build that have a high likelihood of failure; notifying monitoring entities of software components that are identified as having a high likelihood of contributing to build failure; assigning additional human resources (such as programmer-hours) to a particular stage of the software build; and assigning additional computing resources, such as server access time, cloud-based virtual machines, bandwidth, or processing power, to a particular development stage.</p><p id="p-0057" num="0056">In one or more embodiments, the system automatically modifies software build attributes based on identifying a failure of one or more stages of the software build development process. In an alternative embodiment, the system generates a notification to monitoring entities prior to modifying any software build attributes. The notification may include recommendations for modifying the software build attributes.</p><p id="p-0058" num="0057">In one or more embodiments, the system generates a confidence score associated with the prediction of success or failure of the software build. The system may determine whether to modify attributes of the software build based on the confidence score. For example, if the system determines that the data set used to train the machine learning model did not include data points similar to a data point associated with a target software build, the system may assign a low confidence score to the resulting prediction of failure. As a result, the system may adjust the modifications to the software build attributes based on the confidence score. If the confidence score is sufficiently low, the system may refrain from modifying attributes of the software build. Alternatively, the system may provide the recommended attribute modifications to a monitoring entity, together with the confidence score. If the confidence score is relatively high, the system may proceed to modify, or recommend modification of, the attributes of the software build.</p><p id="p-0059" num="0058">The system generates a revised target software build based on the modified software build attributes (Operation <b>212</b>). The revised target software build may include, for example, modifications to a software build specification, modifications to a timeline for completing development of the software build, and a modification of resources used to produce the software build.</p><p id="p-0060" num="0059">If a software build success is predicted (Operation <b>208</b>), the system initiates or continues the software build development process (Operation <b>214</b>). The system may update the target build software attributes and re-run the machine learning analysis at any point in the software build development process. For example, the system may apply the machine learning model to a set of software build attributes prior to generating code to develop the software build, and the model may predict a successful build. However, in the course of development, a new security threat may arise, resulting in a change in the software build specification. The system may apply the machine learning model to the updated software build attributes to determine whether the software build would still pass a security scan. Additional modifications may include: delays in programming or testing, changes in available hardware resources, and changes in customer-defined requirements for the software build. For example, a customer may request new functionality from the software build. The system may re-apply the machine learning model to modified or updated sets of software build attributes throughout a software build development process.</p><heading id="h-0008" level="1">4. Training a Machine Learning Model</heading><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example set of operations for training a machine learning model in accordance with one or more embodiments. One or more operations illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be modified, rearranged, or omitted all together. Accordingly, the particular sequence of operations illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> should not be construed as limiting the scope of one or more embodiments.</p><p id="p-0062" num="0061">A system obtains historical data associated with historical software builds (Operation <b>302</b>). The historical data includes data attributes of the historical software builds, including, for example: a specification for the software build specifying a desired functionality of the software build, actual functionality of the software build, components, size, and type of software code, crashes, bugs, and failures identified during the development of the software build, computing and human resources allocated to complete a particular development stage for a software build, a time required to complete the development stage, resources shared between two or more development stages, any conflicts among resources shared among the two or more development stages, and a sequence in which the development stages are carried out.</p><p id="p-0063" num="0062">The system generates a training data set using the historical data (Operation <b>304</b>). The training data set includes a set of software build attributes and labels indicating the success or failure of one or both of the stages of the software build development process and success or failure of the entire software build.</p><p id="p-0064" num="0063">The system applies a machine learning algorithm to the training set to train a machine learning model to predict the success or failure of a software build based on the software build attributes (Operation <b>306</b>). Examples of measures of success or failure include: whether a stage or build is completed on time; whether a build is susceptible to crashes or errors; whether a build performs specified functions within specified periods of time; whether a build generates accurate results; whether a build meets specified compatibility criteria with other applications; whether a build meets specified security criteria; whether a build meets specified code structure or format criteria; and whether the build operates within specified hardware constraints (such as providing a particular performance level when operating on a particular class of hardware). In one embodiment, the machine learning model is trained to identify particular attributes that are likely to contribute to the failure of a software build or of a particular development stage of the software build. In another embodiment, the machine learning model does not identify particular attributes that are likely to contribute to the failure of a software build or of a particular development stage of the software build. Instead, the machine learning model is trained to identify correlations between entire sets of attributes (not necessarily individual attributes) and the success or failure of the software build.</p><p id="p-0065" num="0064">In one or more embodiments, the prediction of success or failure is associated with a confidence level. For example, if the system determines that the data set used to train the machine learning model does not include any data points within a predefined threshold distance from a data point associated with the target software build, the system may assign a low confidence level to the prediction of success or failure.</p><p id="p-0066" num="0065">In one or more embodiments, the system predicts the success or failure of the entire software build. In other words, the system determines whether the target software build, having a particular set of software build attributes, would successfully pass each stage of the software build development process. In addition, or in the alternative, the system may predict the success or failure of individual software build stages. For example, the system may determine whether the build would successfully compile, successfully pass testing, and successfully pass a security scan.</p><p id="p-0067" num="0066">In addition to a prediction of success or failure, the system may generate one or more recommendations for modifying software build attributes based on the output from the machine learning model. For example, the model may identify a lack of assigned hardware resources at a testing stage as a contributing cause of a software build failure. The system may generate a recommendation to increase the hardware resources assigned to the testing stage of the software build development process.</p><p id="p-0068" num="0067">The system receives user feedback based on the prediction of success or failure and the any recommendations for modifying the software build attributes (Operation <b>308</b>). For example, the feedback may indicate that a prediction of failure associated with a low confidence score should be labeled a prediction of &#x201c;success.&#x201d;</p><p id="p-0069" num="0068">The system updates the machine learning model based on the feedback (Operation <b>310</b>). For example, if a user indicates that a particular set of software build attributes should be associated with a &#x201c;success&#x201d; prediction, the system re-trains the machine learning model by adjusting parameters of the model such that the particular attributes will result in a &#x201c;success&#x201d; prediction.</p><heading id="h-0009" level="1">4. Example Embodiment</heading><p id="p-0070" num="0069">Detailed examples are described below for purposes of clarity. Components and/or operations described below should be understood as one specific example which may not be applicable to certain embodiments. Accordingly, components and/or operations described below should not be construed as limiting the scope of any of the claims.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example in which a system applies a machine learning model to a set of software build attributes to generate a prediction of success or failure for different stages of a software build development process.</p><p id="p-0072" num="0071">A system is provided with a set of software build attributes <b>401</b> for a software build. The system applies a trained machine learning model <b>402</b> to the software build attributes <b>401</b> to generate, for the entire build or for one or more development stages, one or more of: (1) a prediction of success or failure, (2) a recommendation for modifying software build attributes, and (3) a modification to the software build attributes <b>401</b>.</p><p id="p-0073" num="0072">In the embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the software build development process includes a code pre-compile analysis <b>404</b>, a code compile stage <b>405</b>, a code testing stage <b>406</b>, a stress test stage <b>407</b>, and a security scan stage <b>408</b>. Upon applying the machine learning model <b>402</b> to the software build attributes <b>401</b>, the system identifies a database version upgrade <b>410</b> associated with the pre-compile analysis <b>404</b>. The database version upgrade may be required for the software build to succeed (resulting in a &#x201c;failure&#x201d; prediction for the software build) or may be a recommendation to improve performance of the software build. The system alerts monitoring entities <b>418</b> associated with the code pre-compile analysis <b>404</b>.</p><p id="p-0074" num="0073">In addition, the machine learning model <b>402</b> may identify required components <b>411</b> and required resources <b>412</b> associated with a code compile stage <b>405</b>. The system may notify monitoring entities <b>418</b> associated with the code compile stage <b>405</b>. The machine learning model <b>402</b> may identify required testing <b>413</b> and required resources <b>414</b> associated with a code testing stage <b>406</b>. For example, the machine learning model <b>402</b> may determine that an additional cloud instance should be provisioned to the code testing stage <b>406</b> to increase the speed of the code testing stage <b>406</b>. The system may determine that the increase in speed is required for the software build development to maintain a specified schedule. Alternatively, the system may determine that the increase in speed will improve the likelihood of detecting software code failures in the code testing stage <b>406</b>. The system may notify monitoring entities <b>418</b> associated with the code testing stage <b>406</b>. The machine learning model <b>402</b> may identify required resources <b>415</b> associated with a code stress stage <b>407</b>. For example, the machine learning model may determine that the suite of stress testing provided in the software build is inadequate to ensure proper stress testing. The model <b>402</b> may identify one or more additional stress tests, or may recommend different stress metrics, for the stress test stage <b>407</b>. The system may notify monitoring entities <b>418</b> associated with the code stress stage <b>407</b>. The machine learning model <b>402</b> may identify areas of impact <b>416</b> and required testing <b>417</b> associated with a security scan stage <b>408</b>. The system may notify monitoring entities <b>418</b> associated with the security scan stage <b>408</b>.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates another example embodiment in which a system applies a machine learning model to a set of software build attributes to generate a prediction of success or failure for different stages of a software build development process.</p><p id="p-0076" num="0075">A software build development process according to <figref idref="DRAWINGS">FIG. <b>5</b></figref> may include a dependency scan stage <b>419</b> to identify dependencies between the software build and any other applications or builds. The machine learning model <b>402</b> may identify one or more third-party dependencies <b>420</b> required to perform or successfully pass the dependency scan stage <b>419</b>. The system may notify monitoring entities <b>418</b> associated with the dependency scan stage <b>419</b>.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates another example embodiment in which a system applies a machine learning model to a set of software build attributes to generate a prediction of success or failure for different stages of a software build development process.</p><p id="p-0078" num="0077">As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a machine learning model <b>402</b> may identify a software upgrade <b>422</b> associated with the pre-compile analysis <b>404</b> that may be required to perform the pre-compile analysis <b>404</b>. The machine learning model <b>402</b> may also identify areas of impact <b>422</b>, code quality metrics <b>423</b>, and required resources <b>424</b> required to perform a quality scan <b>421</b> of the software build. For example, the machine learning model <b>402</b> may identify an additional deprecation test and an additional performance test that should be performed in association with the quality scan <b>421</b> to improve a likelihood of success of the quality scan <b>421</b> stage. In addition, the machine learning model <b>402</b> may recommend provisioning the quality scan <b>421</b> with an additional cloud instance to allow for additional code quality scanning. The system may notify monitoring entities <b>418</b> associated with the pre-compile analysis <b>404</b> and the quality scan <b>421</b>, respectively.</p><heading id="h-0010" level="1">6. Computer Networks and Cloud Networks</heading><p id="p-0079" num="0078">In one or more embodiments, a software build engine is implemented in a computer network that provides connectivity among a set of nodes. The nodes may be local to and/or remote from each other. The nodes are connected by a set of links. Examples of links include a coaxial cable, an unshielded twisted cable, a copper cable, an optical fiber, and a virtual link.</p><p id="p-0080" num="0079">A subset of nodes implements the computer network. Examples of such nodes include a switch, a router, a firewall, and a network address translator (NAT). Another subset of nodes uses the computer network. Such nodes (also referred to as &#x201c;hosts&#x201d;) may execute a client process and/or a server process. A client process makes a request for a computing service (such as, execution of a particular application, and/or storage of a particular amount of data). A server process responds by executing the requested service and/or returning corresponding data.</p><p id="p-0081" num="0080">A computer network may be a physical network, including physical nodes connected by physical links. A physical node is any digital device. A physical node may be a function-specific hardware device, such as a hardware switch, a hardware router, a hardware firewall, and a hardware NAT. Additionally or alternatively, a physical node may be a generic machine that is configured to execute various virtual machines and/or applications performing respective functions. A physical link is a physical medium connecting two or more physical nodes. Examples of links include a coaxial cable, an unshielded twisted cable, a copper cable, and an optical fiber.</p><p id="p-0082" num="0081">A computer network may be an overlay network. An overlay network is a logical network implemented on top of another network (such as, a physical network). Each node in an overlay network corresponds to a respective node in the underlying network. Hence, each node in an overlay network is associated with both an overlay address (to address to the overlay node) and an underlay address (to address the underlay node that implements the overlay node). An overlay node may be a digital device and/or a software process (such as, a virtual machine, an application instance, or a thread) A link that connects overlay nodes is implemented as a tunnel through the underlying network. The overlay nodes at either end of the tunnel treat the underlying multi-hop path between them as a single logical link. Tunneling is performed through encapsulation and decapsulation.</p><p id="p-0083" num="0082">In an embodiment, a client, including clients operated by one or more monitoring entities, may be local to and/or remote from a computer network. The client may access the computer network over other computer networks, such as a private network or the Internet. The client may communicate requests to the computer network using a communications protocol, such as Hypertext Transfer Protocol (HTTP). The requests are communicated through an interface, such as a client interface (such as a web browser), a program interface, or an application programming interface (API).</p><p id="p-0084" num="0083">In an embodiment, a computer network provides connectivity between clients and network resources. Network resources include hardware and/or software configured to execute server processes. Examples of network resources include a processor, a data storage, a virtual machine, a container, and/or a software application. Network resources are shared amongst multiple clients. Clients request computing services from a computer network independently of each other. Network resources are dynamically assigned to the requests and/or clients on an on-demand basis. Network resources assigned to each request and/or client may be scaled up or down based on, for example, (a) the computing services requested by a particular client, (b) the aggregated computing services requested by a particular tenant, and/or (c) the aggregated computing services requested of the computer network. Such a computer network may be referred to as a &#x201c;cloud network.&#x201d;</p><p id="p-0085" num="0084">In an embodiment, a service provider provides a cloud network to one or more end users. Various service models may be implemented by the cloud network, including but not limited to Software-as-a-Service (SaaS), Platform-as-a-Service (PaaS), and Infrastructure-as-a-Service (IaaS). In SaaS, a service provider provides end users the capability to use the service provider's applications, which are executing on the network resources. In PaaS, the service provider provides end users the capability to deploy custom applications onto the network resources. The custom applications may be created using programming languages, libraries, services, and tools supported by the service provider. In IaaS, the service provider provides end users the capability to provision processing, storage, networks, and other fundamental computing resources provided by the network resources. Any arbitrary applications, including an operating system, may be deployed on the network resources.</p><p id="p-0086" num="0085">In an embodiment, various deployment models may be implemented by a computer network, including but not limited to a private cloud, a public cloud, and a hybrid cloud. In a private cloud, network resources are provisioned for exclusive use by a particular group of one or more entities (the term &#x201c;entity&#x201d; as used herein refers to a corporation, organization, person, or other entity). The network resources may be local to and/or remote from the premises of the particular group of entities. In a public cloud, cloud resources are provisioned for multiple entities that are independent from each other (also referred to as &#x201c;tenants&#x201d; or &#x201c;customers&#x201d;). The computer network and the network resources thereof are accessed by clients corresponding to different tenants. Such a computer network may be referred to as a &#x201c;multi-tenant computer network.&#x201d; Several tenants may use a same particular network resource at different times and/or at the same time. The network resources may be local to and/or remote from the premises of the tenants. In a hybrid cloud, a computer network comprises a private cloud and a public cloud. An interface between the private cloud and the public cloud allows for data and application portability. Data stored at the private cloud and data stored at the public cloud may be exchanged through the interface. Applications implemented at the private cloud and applications implemented at the public cloud may have dependencies on each other. A call from an application at the private cloud to an application at the public cloud (and vice versa) may be executed through the interface.</p><p id="p-0087" num="0086">In an embodiment, tenants of a multi-tenant computer network are independent of each other. For example, a business or operation of one tenant may be separate from a business or operation of another tenant. Different tenants may demand different network requirements for the computer network. Examples of network requirements include processing speed, amount of data storage, security requirements, performance requirements, throughput requirements, latency requirements, resiliency requirements, Quality of Service (QoS) requirements, tenant isolation, and/or consistency. The same computer network may need to implement different network requirements demanded by different tenants.</p><p id="p-0088" num="0087">In one or more embodiments, in a multi-tenant computer network, tenant isolation is implemented to ensure that the applications and/or data of different tenants are not shared with each other. Various tenant isolation approaches may be used.</p><p id="p-0089" num="0088">In an embodiment, each tenant is associated with a tenant ID. Each network resource of the multi-tenant computer network is tagged with a tenant ID. A tenant is permitted access to a particular network resource only if the tenant and the particular network resources are associated with a same tenant ID.</p><p id="p-0090" num="0089">In an embodiment, each tenant is associated with a tenant ID. Each application, implemented by the computer network, is tagged with a tenant ID. Additionally or alternatively, each data structure and/or dataset, stored by the computer network, is tagged with a tenant ID. A tenant is permitted access to a particular application, data structure, and/or dataset only if the tenant and the particular application, data structure, and/or dataset are associated with a same tenant ID.</p><p id="p-0091" num="0090">As an example, each database implemented by a multi-tenant computer network may be tagged with a tenant ID. Only a tenant associated with the corresponding tenant ID may access data of a particular database. As another example, each entry in a database implemented by a multi-tenant computer network may be tagged with a tenant ID. Only a tenant associated with the corresponding tenant ID may access data of a particular entry. However, the database may be shared by multiple tenants.</p><p id="p-0092" num="0091">In an embodiment, a subscription list indicates which tenants have authorization to access which applications. For each application, a list of tenant IDs of tenants authorized to access the application is stored. A tenant is permitted access to a particular application only if the tenant ID of the tenant is included in the subscription list corresponding to the particular application.</p><p id="p-0093" num="0092">In an embodiment, network resources (such as digital devices, virtual machines, application instances, and threads) corresponding to different tenants are isolated to tenant-specific overlay networks maintained by the multi-tenant computer network. As an example, packets from any source device in a tenant overlay network may only be transmitted to other devices within the same tenant overlay network. Encapsulation tunnels are used to prohibit any transmissions from a source device on a tenant overlay network to devices in other tenant overlay networks. Specifically, the packets, received from the source device, are encapsulated within an outer packet. The outer packet is transmitted from a first encapsulation tunnel endpoint (in communication with the source device in the tenant overlay network) to a second encapsulation tunnel endpoint (in communication with the destination device in the tenant overlay network). The second encapsulation tunnel endpoint decapsulates the outer packet to obtain the original packet transmitted by the source device. The original packet is transmitted from the second encapsulation tunnel endpoint to the destination device in the same particular overlay network.</p><heading id="h-0011" level="1">7. Miscellaneous; Extensions</heading><p id="p-0094" num="0093">Embodiments are directed to a system with one or more devices that include a hardware processor and that are configured to perform any of the operations described herein and/or recited in any of the claims below.</p><p id="p-0095" num="0094">In an embodiment, a non-transitory computer readable storage medium comprises instructions which, when executed by one or more hardware processors, causes performance of any of the operations described herein and/or recited in any of the claims.</p><p id="p-0096" num="0095">Any combination of the features and functionalities described herein may be used in accordance with one or more embodiments. In the foregoing specification, embodiments have been described with reference to numerous specific details that may vary from implementation to implementation. The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense. The sole and exclusive indicator of the scope of the invention, and what is intended by the applicants to be the scope of the invention, is the literal and equivalent scope of the set of claims that issue from this application, in the specific form in which such claims issue, including any subsequent correction.</p><heading id="h-0012" level="1">8. Hardware Overview</heading><p id="p-0097" num="0096">According to one embodiment, the techniques described herein are implemented by one or more special-purpose computing devices. The special-purpose computing devices may be hard-wired to perform the techniques, or may include digital electronic devices such as one or more application-specific integrated circuits (ASICs), field programmable gate arrays (FPGAs), or network processing units (NPUs) that are persistently programmed to perform the techniques, or may include one or more general purpose hardware processors programmed to perform the techniques pursuant to program instructions in firmware, memory, other storage, or a combination. Such special-purpose computing devices may also combine custom hard-wired logic, ASICs, FPGAs, or NPUs with custom programming to accomplish the techniques. The special-purpose computing devices may be desktop computer systems, portable computer systems, handheld devices, networking devices or any other device that incorporates hard-wired and/or program logic to implement the techniques.</p><p id="p-0098" num="0097">For example, <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram that illustrates a computer system <b>700</b> upon which an embodiment of the invention may be implemented. Computer system <b>700</b> includes a bus <b>702</b> or other communication mechanism for communicating information, and a hardware processor <b>704</b> coupled with bus <b>702</b> for processing information. Hardware processor <b>704</b> may be, for example, a general purpose microprocessor.</p><p id="p-0099" num="0098">Computer system <b>700</b> also includes a main memory <b>706</b>, such as a random access memory (RAM) or other dynamic storage device, coupled to bus <b>702</b> for storing information and instructions to be executed by processor <b>704</b>. Main memory <b>706</b> also may be used for storing temporary variables or other intermediate information during execution of instructions to be executed by processor <b>704</b>. Such instructions, when stored in non-transitory storage media accessible to processor <b>704</b>, render computer system <b>700</b> into a special-purpose machine that is customized to perform the operations specified in the instructions.</p><p id="p-0100" num="0099">Computer system <b>700</b> further includes a read only memory (ROM) <b>708</b> or other static storage device coupled to bus <b>702</b> for storing static information and instructions for processor <b>704</b>. A storage device <b>710</b>, such as a magnetic disk or optical disk, is provided and coupled to bus <b>702</b> for storing information and instructions.</p><p id="p-0101" num="0100">Computer system <b>700</b> may be coupled via bus <b>702</b> to a display <b>712</b>, such as a cathode ray tube (CRT), for displaying information to a computer user. An input device <b>714</b>, including alphanumeric and other keys, is coupled to bus <b>702</b> for communicating information and command selections to processor <b>704</b>. Another type of user input device is cursor control <b>716</b>, such as a mouse, a trackball, or cursor direction keys for communicating direction information and command selections to processor <b>704</b> and for controlling cursor movement on display <b>712</b>. This input device typically has two degrees of freedom in two axes, a first axis (e.g., x) and a second axis (e.g., y), that allows the device to specify positions in a plane.</p><p id="p-0102" num="0101">Computer system <b>700</b> may implement the techniques described herein using customized hard-wired logic, one or more ASICs or FPGAs, firmware and/or program logic which in combination with the computer system causes or programs computer system <b>700</b> to be a special-purpose machine. According to one embodiment, the techniques herein are performed by computer system <b>700</b> in response to processor <b>704</b> executing one or more sequences of one or more instructions contained in main memory <b>706</b>. Such instructions may be read into main memory <b>706</b> from another storage medium, such as storage device <b>710</b>. Execution of the sequences of instructions contained in main memory <b>706</b> causes processor <b>704</b> to perform the process steps described herein. In alternative embodiments, hard-wired circuitry may be used in place of or in combination with software instructions.</p><p id="p-0103" num="0102">The term &#x201c;storage media&#x201d; as used herein refers to any non-transitory media that store data and/or instructions that cause a machine to operate in a specific fashion. Such storage media may comprise non-volatile media and/or volatile media. Non-volatile media includes, for example, optical or magnetic disks, such as storage device <b>710</b>. Volatile media includes dynamic memory, such as main memory <b>706</b>. Common forms of storage media include, for example, a floppy disk, a flexible disk, hard disk, solid state drive, magnetic tape, or any other magnetic data storage medium, a CD-ROM, any other optical data storage medium, any physical medium with patterns of holes, a RAM, a PROM, and EPROM, a FLASH-EPROM, NVRAM, any other memory chip or cartridge, content-addressable memory (CAM), and ternary content-addressable memory (TCAM).</p><p id="p-0104" num="0103">Storage media is distinct from but may be used in conjunction with transmission media. Transmission media participates in transferring information between storage media. For example, transmission media includes coaxial cables, copper wire and fiber optics, including the wires that comprise bus <b>702</b>. Transmission media can also take the form of acoustic or light waves, such as those generated during radio-wave and infra-red data communications.</p><p id="p-0105" num="0104">Various forms of media may be involved in carrying one or more sequences of one or more instructions to processor <b>704</b> for execution. For example, the instructions may initially be carried on a magnetic disk or solid state drive of a remote computer. The remote computer can load the instructions into its dynamic memory and send the instructions over a telephone line using a modem. A modem local to computer system <b>700</b> can receive the data on the telephone line and use an infra-red transmitter to convert the data to an infra-red signal. An infra-red detector can receive the data carried in the infra-red signal and appropriate circuitry can place the data on bus <b>702</b>. Bus <b>702</b> carries the data to main memory <b>706</b>, from which processor <b>704</b> retrieves and executes the instructions. The instructions received by main memory <b>706</b> may optionally be stored on storage device <b>710</b> either before or after execution by processor <b>704</b>.</p><p id="p-0106" num="0105">Computer system <b>700</b> also includes a communication interface <b>718</b> coupled to bus <b>702</b>. Communication interface <b>718</b> provides a two-way data communication coupling to a network link <b>720</b> that is connected to a local network <b>722</b>. For example, communication interface <b>718</b> may be an integrated services digital network (ISDN) card, cable modem, satellite modem, or a modem to provide a data communication connection to a corresponding type of telephone line. As another example, communication interface <b>718</b> may be a local area network (LAN) card to provide a data communication connection to a compatible LAN. Wireless links may also be implemented. In any such implementation, communication interface <b>718</b> sends and receives electrical, electromagnetic or optical signals that carry digital data streams representing various types of information.</p><p id="p-0107" num="0106">Network link <b>720</b> typically provides data communication through one or more networks to other data devices. For example, network link <b>720</b> may provide a connection through local network <b>722</b> to a host computer <b>724</b> or to data equipment operated by an Internet Service Provider (ISP) <b>726</b>. ISP <b>726</b> in turn provides data communication services through the worldwide packet data communication network now commonly referred to as the &#x201c;Internet&#x201d; <b>728</b>. Local network <b>722</b> and Internet <b>728</b> both use electrical, electromagnetic or optical signals that carry digital data streams. The signals through the various networks and the signals on network link <b>720</b> and through communication interface <b>718</b>, which carry the digital data to and from computer system <b>700</b>, are example forms of transmission media.</p><p id="p-0108" num="0107">Computer system <b>700</b> can send messages and receive data, including program code, through the network(s), network link <b>720</b> and communication interface <b>718</b>. In the Internet example, a server <b>730</b> might transmit a requested code for an application program through Internet <b>728</b>, ISP <b>726</b>, local network <b>722</b> and communication interface <b>718</b>.</p><p id="p-0109" num="0108">The received code may be executed by processor <b>704</b> as it is received, and/or stored in storage device <b>710</b>, or other non-volatile storage for later execution.</p><p id="p-0110" num="0109">In the foregoing specification, embodiments of the invention have been described with reference to numerous specific details that may vary from implementation to implementation. The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense. The sole and exclusive indicator of the scope of the invention, and what is intended by the applicants to be the scope of the invention, is the literal and equivalent scope of the set of claims that issue from this application, in the specific form in which such claims issue, including any subsequent correction.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A non-transitory computer readable medium comprising instructions which, when executed by one or more hardware processors, causes performance of operations comprising:<claim-text>obtaining historical data associated with historical software builds, the historical data including:<claim-text>attribute data for a plurality of development stages associated with a historical software build, and</claim-text><claim-text>labels indicating success or failure for the plurality of development stages,</claim-text></claim-text><claim-text>training a machine learning model using the historical data associated with the historical software builds to generate predictions of success or failure of the plurality of development stages;</claim-text><claim-text>receiving attributes of a target software build and a selection of a first target development stage of the target software build;</claim-text><claim-text>applying the machine learning model to the target software build to generate a first prediction of success or failure of the first target development stage.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operations further comprise:<claim-text>modifying, based on the predictions of success or failure of the plurality of development stages, attributes of one or more of the plurality of development stages.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The medium of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the attributes of the one or more of the plurality of development stages include timing relationships between two or more development stages of the plurality of development stages.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The medium of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the attributes of the one or more of the plurality of development stages include a sequence of operations performed by the first target development stage.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The medium of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the attributes of the one or more of the plurality of development stages include resources shared by the one or more of the plurality of development stages, and<claim-text>modifying the attributes of the one or more of the plurality of development stages includes modifying a ratio of resource usage of the resources by the first target development stage and a second development stage.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The medium of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the machine learning model further generates a recommendation for modifying the attributes of the one or more of the plurality of development stages.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The medium of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein generating the recommendation for modifying the attributes of the one or more of the plurality of development stages comprises:<claim-text>identifying, in an n-dimensional space, a data point associated with the target software build and associated with a failure prediction;</claim-text><claim-text>identifying, in the n-dimensional space, a neighboring data point associated with a software build and associated with a success prediction;</claim-text><claim-text>generating the recommendation for modifying the attributes of the one or more of the plurality of development stages based on at least one difference in attribute values between the first data point and the neighboring data point.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operations further comprise:<claim-text>identifying a first entity associated with the first target development stage and a second entity associated with a second target development stage;</claim-text><claim-text>wherein applying the machine learning model to the target software results in generating the first prediction of the success or failure of the first target development stage and a second prediction of success or failure of the second target development stage;</claim-text><claim-text>transmitting, to the first entity, a first notification including first prediction of the success or failure of the first target development stage, without including the second prediction of the success or failure of the second target development stage; and</claim-text><claim-text>transmitting, to the second entity, a second notification including second prediction of the success or failure of the second target development stage, without including the first prediction of the success or failure of the first target development stage.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the machine learning model further generates a third prediction of success or failure of the target software build,<claim-text>wherein the first notification and the second notification both include the third prediction of the success or failure of the target software build.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operations further comprise:<claim-text>calculating a confidence score associated with the first prediction, the confidence score associated with a likelihood that the success or failure indicated by first prediction will occur.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A non-transitory computer readable medium comprising instructions which, when executed by one or more hardware processors, causes performance of operations comprising:<claim-text>obtaining historical data associated with historical software builds, the historical data including:<claim-text>attribute data for a plurality of development stages associated with a historical software build, and</claim-text><claim-text>labels indicating success or failure for the plurality of development stages,</claim-text></claim-text><claim-text>training a machine learning model using the historical data associated with the historical software builds to generate predictions of success or failure of the software builds;</claim-text><claim-text>receiving attributes of a target software build;</claim-text><claim-text>applying the machine learning model to the target software build to generate a first prediction of success or failure of the target software build.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A method comprising:<claim-text>obtaining historical data associated with historical software builds, the historical data including:<claim-text>attribute data for a plurality of development stages associated with a historical software build, and</claim-text><claim-text>labels indicating success or failure for the plurality of development stages,</claim-text></claim-text><claim-text>training a machine learning model using the historical data associated with the historical software builds to generate predictions of success or failure of the plurality of development stages;</claim-text><claim-text>receiving attributes of a target software build and a selection of a first target development stage of the target software build;</claim-text><claim-text>applying the machine learning model to the target software build to generate a first prediction of success or failure of the first target development stage.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>modifying, based on the predictions of success or failure of the plurality of development stages, attributes of one or more of the plurality of development stages.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the attributes of the one or more of the plurality of development stages include timing relationships between two or more development stages of the plurality of development stages.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the attributes of the one or more of the plurality of development stages include a sequence of operations performed by the first target development stage.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the attributes of the one or more of the plurality of development stages include resources shared by the one or more of the plurality of development stages, and<claim-text>modifying the attributes of the one or more of the plurality of development stages includes modifying a ratio of resource usage of the resources by the first target development stage and a second development stage.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the machine learning model further generates a recommendation for modifying the attributes of the one or more of the plurality of development stages.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein generating the recommendation for modifying the attributes of the one or more of the plurality of development stages comprises:<claim-text>identifying, in an n-dimensional space, a data point associated with the target software build and associated with a failure prediction;</claim-text><claim-text>identifying, in the n-dimensional space, a neighboring data point associated with a software build and associated with a success prediction;</claim-text><claim-text>generating the recommendation for modifying the attributes of the one or more of the plurality of development stages based on at least one difference in attribute values between the first data point and the neighboring data point.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>identifying a first entity associated with the first target development stage and a second entity associated with a second target development stage;</claim-text><claim-text>wherein applying the machine learning model to the target software results in generating the first prediction of the success or failure of the first target development stage and a second prediction of success or failure of the second target development stage;</claim-text><claim-text>transmitting, to the first entity, a first notification including first prediction of the success or failure of the first target development stage, without including the second prediction of the success or failure of the second target development stage; and</claim-text><claim-text>transmitting, to the second entity, a second notification including second prediction of the success or failure of the second target development stage, without including the first prediction of the success or failure of the first target development stage.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A system, comprising:<claim-text>one or more processors; and</claim-text><claim-text>memory storing instructions that, when executed by the one or more processors, cause the system to perform:</claim-text><claim-text>obtaining historical data associated with historical software builds, the historical data including:<claim-text>attribute data for a plurality of development stages associated with a historical software build, and</claim-text><claim-text>labels indicating success or failure for the plurality of development stages,</claim-text></claim-text><claim-text>training a machine learning model using the historical data associated with the historical software builds to generate predictions of success or failure of the plurality of development stages;</claim-text><claim-text>receiving attributes of a target software build and a selection of a first target development stage of the target software build;</claim-text><claim-text>applying the machine learning model to the target software build to generate a first prediction of success or failure of the first target development stage.</claim-text></claim-text></claim></claims></us-patent-application>