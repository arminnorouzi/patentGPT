<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005244A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005244</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17781510</doc-number><date>20201030</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-220172</doc-number><date>20191205</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>082</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0088</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">NEURAL NETWORK COMPRESSION DEVICE AND METHOD FOR SAME</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Hitachi Astemo, Ltd.</orgname><address><city>Hitachinaka-shi, Ibaraki</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MURATA</last-name><first-name>Daichi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Hitachi Astemo, Ltd.</orgname><role>03</role><address><city>Hitachinaka-shi, Ibaraki</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/040924</doc-number><date>20201030</date></document-id><us-371c12-date><date>20220601</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">When it is assumed that a large-scale Deep Neural Network for autonomous driving applied compression, there are problems of a decrease in recognition accuracy of a post-compression Neural Network (NN) model and an increase in a compression design period, due to a large number of harmful or unnecessary training images (invalid training images). A training image selection unit B<b>100</b> calculates an influence value on an inference, and generates an indexed training image set <b>1004</b>-<b>1</b> necessary for an NN compression design, by using the influence value. A neural network compression unit P<b>200</b> notified of the result via a memory P<b>300</b> compresses the NN.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="58.25mm" wi="143.59mm" file="US20230005244A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="212.51mm" wi="158.58mm" file="US20230005244A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="213.87mm" wi="106.17mm" file="US20230005244A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="133.77mm" wi="160.53mm" file="US20230005244A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="185.76mm" wi="156.13mm" file="US20230005244A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="222.00mm" wi="150.88mm" file="US20230005244A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="152.74mm" wi="150.20mm" file="US20230005244A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="171.70mm" wi="149.01mm" file="US20230005244A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="148.34mm" wi="148.34mm" file="US20230005244A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="227.25mm" wi="149.52mm" file="US20230005244A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="216.41mm" wi="151.72mm" file="US20230005244A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="173.74mm" wi="155.96mm" file="US20230005244A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="186.44mm" wi="153.08mm" file="US20230005244A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="144.61mm" wi="158.16mm" file="US20230005244A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="143.43mm" wi="159.85mm" file="US20230005244A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="172.55mm" wi="159.00mm" file="US20230005244A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="225.04mm" wi="158.83mm" file="US20230005244A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="226.57mm" wi="160.19mm" file="US20230005244A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="218.95mm" wi="155.45mm" file="US20230005244A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="230.12mm" wi="160.53mm" file="US20230005244A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="214.04mm" wi="160.36mm" file="US20230005244A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="229.79mm" wi="160.02mm" file="US20230005244A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to compression of a neural network. In the specification of the present application, the compression means reduction of an arithmetic operation amount.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">In the brain. of an organism, there are a large number of neurons (nerve cells), and each neuron performs a motion such as an input of a signal from a large number of other neurons and an output of a signal to a large number of other neurons. An attempt to realize such a brain mechanism by a computer is a Deep Neural Network (DNN), which is an engineering model that mimics the behavior of a biological nerve cell network.</p><p id="p-0004" num="0003">As an example of the DNN, there is a convolutional neural network (CNN) valid for object recognition and behavior prediction. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of a structure of a CNN. The CNN includes an input layer, one or more intermediate layers, and multilayer convolution. operation. layer called an output layer. In the N-th convolution. operation layer, a value output from the (N&#x2212;1)th layer is used as an input, and a result obtained by convolving a weight filter with the input value is output to the input of the (N30 1)th layer. At this time, it is possible to obtain high generalization performance by setting (training) the kernel coefficient (weight coefficient) of the weight filter to an appropriate value in accordance with an application.</p><p id="p-0005" num="0004">In recent years, by mounting a CNN on an in-vehicle electronic control unit (ECU), development of a technology for realizing autonomous driving and driving support has been accelerated. The arithmetic operation amount required to implement a large-scale CNN for autonomous driving is 100 TOPs (Tera Operations) or more. On the other hand, the arithmetic capability of a processor that can be mounted on an autonomous driving-electronic control unit (AD-ECU) being one kind of in-vehicle ECU is about several 10 TOPS (Tera Operation per Sec), and it is difficult to implement real-time processing. Thus, as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, it is necessary to reduce (compress) the arithmetic operation amount of the CNN. However, it takes a long period to make a compression design for searching for a compression condition capable of achieving both suppression of a decrease in recognition accuracy due to compression and adaptation to the arithmetic capability of the in-vehicle processor. From the above viewpoint, in the known compression design method, it is difficult to achieve both (1) suppression of a decrease in recognition accuracy in compression and (2) shortening of a compression design period, with respect to a large-scale CNN for autonomous driving.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0006" num="0005">PTL 1: JP 2005-100121 A In PTL 1, the type of the feature and an identification condition used in identification processing are determined in accordance with a desired balance between the identification accuracy and the burden of the calculation amount. PTL 1 discloses that an identification accuracy specifying unit and a calculation amount specifying unit respectively specify an identification accuracy index value and a calculation amount index value, and select a favorable discriminator.</p><heading id="h-0005" level="1">SUMMARY OF INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0007" num="0006">Here, <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a conventional compression design flow. This processing is executed by a conventional neural network compression device.</p><p id="p-0008" num="0007">First, a training data set <b>1001</b> is received as an input, and an initial compression condition including a compression location and a compression rate for a pre-compression neural network model (pre-compression NN model) <b>1003</b> is determined (S<b>3</b>). Then, adjustment reflecting an optimum compression condition is performed on the initial compression condition, to calculate a compression condition <b>1005</b> (S<b>4</b>). Compressing processing is executed by using the compression condition <b>1005</b> (S<b>5</b>). Therefore, the provisional compression neural network model (provisional compression NN model) <b>1006</b> is specified. Then, re-training (tuning) is executed by using the provisional compression NN model and the training data set <b>1001</b> (S<b>7</b>). It is determined whether the re-training (S<b>6</b>) of the last training data set <b>1001</b> and an inference data set <b>1002</b> has ended (S<b>7</b>). As a result, if the re-training has ended, the result of the re-training is stored as a post-compression neural network model (post-compression NN model) <b>1007</b>. If the re-training has not ended, optimum compression condition search processing is executed on the result of the re-training (S<b>8</b>). Then, an optimum compression condition that is the result of the optimum compression condition search processing is used in S<b>4</b>.</p><p id="p-0009" num="0008">When such processing is performed, there are problems as follows. For example, in-vehicle sensor data and the training data set include a large number of images that induce erroneous recognition in inference using the post-compression NN model <b>1007</b>. Such images are represented by, for example, an image including a minor invisible noise that induces erroneous recognition added by an accidental or malicious third party attack or an image lacking generalizability mapping a limited special environment. In the conventional CNN compression design, the harmful images (invalid training image) are included in the training images used for compression. Thus, (1) a decrease in recognition accuracy of the post-compression DNN model and (2) prolongation of a compression design due to the enormous number of training images have been problems.</p><heading id="h-0007" level="1">Solution To Problem</heading><p id="p-0010" num="0009">In order to solve the above problem, in the present invention, a training data set is classified into valid training data necessary or valid for a compression design and invalid training data unnecessary or harmful, and training regarding a data compression design using at least one of the valid training data and the invalid training data is executed in accordance with a result of the classification.</p><p id="p-0011" num="0010">More specifically, there is provided a neural network compression device that compresses a neural network by using a training data set that has been input. The neural network compression device includes a training image selection unit that calculates an influence value on an inference result calculated by using an inference data set and a neural network model for the training data set, that classifies the training data set into valid training data necessary for the compression and invalid training data unnecessary for the compression, based on the influence value, and that generates an indexed training data set, and a neural network compression unit that compresses the neural network model based on the indexed training data set and the neural network model. Furthermore, the present invention also includes a compression method in the neural network compression device.</p><heading id="h-0008" level="1">Advantageous Effects of Invention</heading><p id="p-0012" num="0011">According to the present invention, it is possible to perform repetitive training (re-training) capable of improving recognition performance such as accuracy and shortening of a training time.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating a structure of a convolutional neural network (CNN).</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an overall configuration diagram of an in-vehicle ECU (AD-ECU <b>1</b>) to which an embodiment of the present invention is applied.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining a relationship between compression and recognition accuracy.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for explaining a relationship between the compression and a compression design period.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a conventional compression design flow.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating a compression design flow in each embodiment of the present invention.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram for explaining determination of an initial compression condition by using training image selection.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> s a diagram illustrating an effect of shortening a training time in the embodiment of the present invention.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating an effect of improving the recognition accuracy in the embodiment of the present invention.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an overall configuration diagram of an NN compression device <b>121</b> in Embodiment 1.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram illustrating a processing flow of Embodiment 1.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating a configuration of a training image selection unit B<b>100</b> in Embodiment 1.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram illustrating a processing flow of the training image selection unit B<b>100</b> in Embodiment 1.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating a configuration of a neural network compression unit B<b>200</b> in Embodiment 1.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram illustrating a processing flow of the neural network compression unit B<b>200</b> in Embodiment 1.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram illustrating a configuration of a PCA/clustering unit B<b>150</b> in Embodiment 1.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram illustrating a processing flow of the PCA/clustering unit B<b>150</b> in Embodiment 1.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram illustrating a configuration of a compression condition determination unit B<b>210</b>.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram illustrating a processing flow of the compression condition determination unit B<b>210</b>.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram illustrating a configuration of a training image selection unit B<b>100</b> in Embodiment 2.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram illustrating a processing flow of the training image selection unit B<b>100</b> in Embodiment 2.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram illustrating a configuration of an NN compression device <b>121</b> in an AD-ECU <b>1</b> in Embodiment 3.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram illustrating a configuration of an NN compression. device <b>121</b> in an AD-ECU <b>1</b> in Embodiment 4.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a diagram illustrating a configuration of an NN compression device <b>121</b> in an AD-ECU <b>1</b> in Embodiment 5.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0037" num="0036">Hereinafter, before description of each Embodiment (1 to 5) of the present invention, a compression design flow common to the embodiments of the present invention will be described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. This processing flow is executed by a neural network compression device <b>212</b> described later. Reference signs common to the reference signs in <figref idref="DRAWINGS">FIG. <b>5</b></figref> indicate similar processing or data. Here, the similarity includes processing or data that is not completely the same depending on the performed processing or used data.</p><p id="p-0038" num="0037">First, an influence value of each piece of data included in a training data set <b>1001</b> on an inference result is estimated by using the training data set <b>1001</b> that has been input and an inference data set <b>1002</b>. Each piece of training data is classified into valid training data or invalid training data, and an indexed training data set <b>1004</b> is created (S<b>1</b>). As generation of the indexed training data set <b>1004</b>, for example, an index is added. The assignment of the index is assignment to at least one. In a form of assigning the index to one (for example, valid), it is possible to determine a training data set to which no index is assigned, to be the other (for example, invalid). As another example, it is also possible to realize the classification by storing the classification result in a storage medium of an address corresponding to the result.</p><p id="p-0039" num="0038">Then, the valid training data is selected based on the index of the indexed training data set <b>1004</b> (S<b>2</b>). An initial compression condition for a pre-compression neural network model (pre-compression NN model) <b>1003</b> is determined by using the selected valid training data (S<b>3</b>). Here, in the initial compression condition determination (S<b>3</b>), the index may be used. Specifically, as illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a priority order of compression is determined for each neuron based on the index and an activation state. The compression of each neuron is repeated in descending order of priority until the post-compression NN model <b>1007</b> satisfies an arithmetic operation amount constraint set by a user. Here, the activation of the neuron indicates that the neuron takes a non-zero value.</p><p id="p-0040" num="0039">To summarize the above description, compression is performed to remove a neuron having a low valid training image and a high invalid training image. A neuron in which both the valid training image and the invalid training image are low is also removed. In addition, if the valid training image is high, the compression of the neuron is not observed regardless of the activation frequency for the invalid training image. Furthermore, it is desirable to compress the corresponding neuron in accordance with the priority illustrated in this drawing. As described above, it is more preferable to perform compression by using a combination of the activation frequencies of neurons of the valid training image and the invalid training image. Here, high and low can be performed by comparison with a predetermined reference value. Furthermore, in this drawing, only the results of the compression priorities 1 and 2 may be used.</p><p id="p-0041" num="0040">Then, adjustment reflecting an optimum compression condition is performed on the initial compression condition, to calculate a compression condition <b>1005</b> (S<b>4</b>). Compressing processing is executed by using the compression condition <b>1005</b> (S<b>5</b>). Thus, a provisional compression NN model <b>1006</b> is specified.</p><p id="p-0042" num="0041">Then re-training is executed by using the provisional compression NN model <b>1005</b> and <b>1006</b> and the selected valid training image (S<b>6</b>). It is determined whether the re-training (S<b>6</b>) of the last training data set <b>1001</b> and an inference data set <b>1002</b> has ended (S<b>7</b>). As a result, if the re-training has ended, the result of the re-training is stored as a post-compression neural network model (post-compression NN model) <b>1007</b>. If the re-training has not ended, optimum compression condition search processing is executed on the result of the re-training (S<b>9</b>). Then, an optimum compression condition that is the result of the optimum compression condition search processing is used in S<b>4</b>.</p><p id="p-0043" num="0042">In each of the following embodiments, an image is used as a training data set, but each of the embodiments can also be applied to other types of data.</p><p id="p-0044" num="0043">The embodiments of the present invention will be described below with reference to the drawings.</p><heading id="h-0011" level="1">Embodiment 1</heading><p id="p-0045" num="0044">An outline con figuration. and an outline processing flow of Embodiment 1 will be described. with reference to <figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref>.</p><p id="p-0046" num="0045">First, <figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates each component requirement constituting a neural network compression device (NN compression device <b>121</b>) in the present embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the neural network compression device includes a training image selection unit B<b>100</b>, a neural network compression unit (NN compression unit) B<b>200</b>, and a memory B<b>300</b>.</p><p id="p-0047" num="0046">The neural network compression device can be mounted on an arithmetic operation device. For example, it can be realized as a field-programmable gate array (FPGA) mounted on an integrated circuit. In addition, each component and submodule described later may be realized as hardware or may be realized as software (program). Processing will be described below.</p><p id="p-0048" num="0047">Next, an outline processing flow of Embodiment 1 will be described with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0049" num="0048">First, in Step S<b>100</b>, the training image selection unit B<b>100</b> receives a training image set <b>1001</b>-<b>1</b>, an inference image set <b>1002</b>-<b>1</b>, and a pre-compression NN model <b>1003</b> from the memory B<b>300</b>. The training image selection unit B<b>100</b> generates an indexed training image set <b>1004</b>-<b>1</b> from the received data. This process corresponds to Step S<b>1</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0050" num="0049">An example in which training target data is an image will be described below in each embodiment. Therefore, the training image set <b>1001</b>-<b>1</b> is one type of the training data set <b>1001</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Furthermore, the inference image set <b>1002</b>-<b>1</b> is one type of the inference data set <b>1002</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The training image set <b>1001</b>-<b>1</b> includes a plurality of training images, and the inference image set <b>1002</b>-<b>1</b> includes a plurality of inference images.</p><p id="p-0051" num="0050">In Step S<b>200</b>, the training image selection unit B<b>100</b> selects a valid training image <b>1008</b> from the indexed training image set <b>1004</b>-<b>1</b>. This process corresponds to Step S<b>2</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. This process corresponds to Step S<b>2</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0052" num="0051">In Step S<b>300</b>, the NN compression. unit B<b>200</b> receives the indexed training image set <b>1004</b>-<b>1</b> and the pre-compression NN model <b>1003</b> from which the valid training image <b>1008</b> has been selected, and then performs compression processing. This corresponds to S<b>3</b> to S<b>8</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0053" num="0052">Next, a detailed configuration and a detailed flow of Embodiment 1 will be described.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating a configuration of the training image selection unit B<b>100</b>. The training image selection unit B<b>100</b> includes the following submodules. B<b>110</b> is an influence value calculation unit B<b>110</b> that calculates an influence value from the training image set <b>1001</b>-<b>1</b>, the inference data set <b>1002</b>, and the pre-compression NN model <b>1003</b>. B<b>120</b> is an unbiased fraction calculation unit B<b>120</b> that calculates an unbiased variance by using the input from the influence unit calculation unit B<b>110</b>. B<b>130</b> is a selector B<b>130</b> that selects a submodule that executes processing on the input data. B<b>140</b> is an averaging unit B<b>140</b> that calculates an average value of the input data. B<b>150</b> is a PCA/clustering unit B<b>150</b> that executes clustering processing on the input data. B<b>160</b> is an index addition unit B<b>160</b> that assigns an index to or classifies the training image. B<b>170</b> and B<b>180</b> are read only memories (ROMs) B<b>170</b> and B<b>180</b> that store data.</p><p id="p-0055" num="0054">With such submodules, the training image selection unit B<b>100</b> executes the following processing. <figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates the contents.</p><p id="p-0056" num="0055">In S<b>110</b>, the influence value calculation unit B<b>110</b> receives the pre-compression NN model <b>1003</b>, the inference data set <b>1002</b>, and the training image set <b>1001</b>-<b>1</b>. The influence value calculation unit B<b>110</b> calculates the influence value of each training image included in the training image set <b>1001</b>-<b>1</b> on the inference result of an inference image k and outputs the influence value as the influence value on the inference image k.</p><p id="p-0057" num="0056">Then, in S<b>120</b>, the unbiased variance calculation unit B<b>120</b> receives the influence value on the inference image k calculated in S<b>110</b>, and calculates the unbiased variance of the influence value for a difference in the inference image. In S<b>130</b>, the unbiased variance calculation unit S<b>120</b> calculates the maximum value of the unbiased variance calculated in S<b>120</b>.</p><p id="p-0058" num="0057">Then, in S<b>140</b>, the selector B<b>130</b> selects the submodule to which the influence value on the inference image k is transmitted, based on the maximum value of the unbiased variance calculated in S<b>130</b>. Specifically, when the maximum value var of the unbiased variance is more than a certain threshold value th_var read from the ROM B<b>180</b>, the PCA/clustering unit B<b>150</b> is selected. On the other hand, when the maximum value var of the unbiased variance is equal to or less than the certain threshold th_var read from the ROM B<b>180</b>, the averaging unit B<b>140</b> is selected.</p><p id="p-0059" num="0058">Then, in S<b>150</b>, the PCA/clustering unit (B<b>150</b>) calculates the influence value on each inference image by applying a main component analysis and clustering processing to the influence value on the inference image k calculated in S<b>110</b>. The calculation of the influence value here is more preferably executed for all the inference images.</p><p id="p-0060" num="0059">In S<b>160</b>, the averaging unit (B<b>140</b>) calculates the influence value on each inference image by applying averaging processing to the influence value on the inference image k calculated in S<b>110</b>. The calculation of the influence value here is more preferably executed for all the inference images.</p><p id="p-0061" num="0060">Then, in S<b>170</b>, the index addition unit B<b>160</b> classifies each training image into the valid training image <b>1008</b> or the invalid training image <b>1009</b> based on the influence value calculated in S<b>150</b> or S<b>160</b>. Such classification may be realized by adding an index indicating whether the image is the valid training image <b>1008</b> or the invalid training image <b>1009</b>. As a result, the index addition unit B<b>160</b> performs an output as the indexed training image set <b>1004</b>-<b>1</b>. Specifically, when the influence value on each inference images is more than a threshold value th_inf read from the ROM B<b>180</b>, the image is determined as the valid training image <b>1008</b>. Conversely, when the influence value is equal to or less than the threshold value th_inf, the image is determined as the invalid training image <b>1009</b>. Here, the invalid training image <b>1009</b> refers to a training image with poor generalization or a training image to which noise inducing erroneous determination by a malicious third party is added. The valid training image <b>1008</b> refers to an image that does not belong to the invalid training image <b>1009</b> among the training images.</p><p id="p-0062" num="0061">Next, a configuration and a detailed processing flow of the compression unit B<b>200</b> will be described with reference to <figref idref="DRAWINGS">FIGS. <b>14</b> and <b>15</b></figref>.</p><p id="p-0063" num="0062">First, the detailed configuration of the NN compression unit B<b>200</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>. The NN compression unit B<b>200</b> executes Step S<b>3</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The NN compression unit B<b>200</b> also includes the following submodules. B<b>210</b> is a compression condition determination unit B<b>210</b> that determines a compression condition from the invalid training image <b>1009</b> and the valid training image <b>1008</b>. B<b>220</b> is a compression unit B<b>220</b> that executes compression processing in accordance with the compression condition. B<b>203</b> is a re-training unit B<b>230</b> that inputs the valid training image <b>1008</b> and. executes re-training by using the provisional compression NN model <b>1006</b>.</p><p id="p-0064" num="0063">With such submodules, the NN compression unit B<b>200</b> executes the following processing. <figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates the contents.</p><p id="p-0065" num="0064">In Step S<b>210</b>, the compression condition determination unit B<b>210</b> receives the pre-compression NN model <b>1003</b> and the indexed training image set <b>1004</b>-<b>1</b> including the invalid training image <b>1009</b> and the valid training image <b>1008</b>. The compression condition determination unit B<b>210</b> determines the compression condition including a compression location and a compression rate based on the compression priority of the neuron.</p><p id="p-0066" num="0065">Then, in S<b>220</b>, the compression unit B<b>220</b> executes compression of the NN model by using the compression condition determined in Step S<b>210</b>. As a result, the compression unit B<b>220</b> outputs a provisional compression NN model.</p><p id="p-0067" num="0066">Then, in Step S<b>230</b>, the re-training unit B<b>230</b> re-trains the provisional compression NN model output in S<b>220</b> by using the valid training image <b>1008</b>. As a result, the post-compression NN model <b>1007</b> is output.</p><p id="p-0068" num="0067">Next, a configuration and a detailed processing flow of the PGA/clustering unit B<b>150</b> will be described with reference to <figref idref="DRAWINGS">FIGS. <b>16</b> and <b>17</b></figref>.</p><p id="p-0069" num="0068">First, submodules constituting the PCA/clustering unit B<b>150</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref>. B<b>151</b> is a PCA unit B<b>151</b> that receives the influence value and outputs the cumulative. contribution degree and the main component score. B<b>152</b> is a dimension reducing unit B<b>152</b> that calculates the main component feature by using the cumulative contribution degree, the main component score, and th_pca that is the lower limit value (threshold value) of the contribution degree. B<b>153</b> is a clustering unit B<b>153</b> that receives the main. component feature and calculates the center of the main component feature as an influence value for each inference image by clustering processing. A ROM B<b>154</b> stores th_pca.</p><p id="p-0070" num="0069">Next, a detailed processing flow of the PCA/clustering unit B<b>150</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>17</b></figref>.</p><p id="p-0071" num="0070">First, in Step S<b>151</b>, the PCA unit B<b>151</b> receives the influence value on the inference image k and outputs the cumulative contribution degree and the main component score.</p><p id="p-0072" num="0071">Then, in Step S<b>152</b>, the dimension reducing unit B<b>152</b> receives the cumulative contribution degree and the main component score output in Step S<b>151</b> and the lower limit value th_pca of the cumulative contribution degree read from the ROM B<b>154</b>. The dimension reducing unit B<b>152</b> performs dimension reduction within a range in which the cumulative contribution degree does not fall below the lower limit value th_pca, and outputs the main component feature.</p><p id="p-0073" num="0072">Then, in Step S<b>153</b>, the clustering unit B<b>153</b> performs clustering on the main component feature. At this time, the centroid of each cluster is output as an influence value for each inference image. The influence value for each inference image may be an influence value for all the inference images.</p><p id="p-0074" num="0073">Then, a configuration and a detailed processing flow of the compression condition determination unit B<b>200</b> constituting the NN compression unit B<b>210</b> will be described with reference to <figref idref="DRAWINGS">FIGS. <b>18</b> and <b>19</b></figref>.</p><p id="p-0075" num="0074">First, <figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates submodules constituting the compression condition determination unit B<b>210</b>. B<b>211</b> is a neural network forward propagation unit B<b>211</b> that receives the pre-compression NN model <b>1003</b>, the invalid training image <b>1009</b>, and the valid training image <b>1008</b>, and outputs hidden layer activation data. B<b>212</b> is a nest y nine-combination condition calculation unit B<b>212</b> that inputs the hidden layer activation data and each threshold value (th_) and outputs the compression condition <b>1005</b>. B<b>213</b> is a ROM B<b>213</b> that stores each threshold value.</p><p id="p-0076" num="0075">Next, <figref idref="DRAWINGS">FIG. <b>19</b></figref> illustrates a detailed processing flow of the compression condition determination unit B<b>210</b>.</p><p id="p-0077" num="0076">In Step S<b>211</b>, the neural network forward propagation unit B<b>211</b> receives the pre-compression NN model <b>1003</b>, the valid training image <b>1008</b>, and the invalid training image <b>1009</b>. The neural network forward propagation unit B<b>211</b> causes each of the valid training image <b>1008</b> and the invalid training image <b>1009</b> to be separately propagated forward to the NN. Therefore, the neural network forward propagation unit B<b>211</b> outputs hidden layer activation data act_val at the time of valid training image propagation and hidden layer activation data act_inval at the time of invalid training image propagation, respectively. Here, the hidden layer activation data indicates a frequency at which neurons constituting a hidden layer take a non-zero value.</p><p id="p-0078" num="0077">Then, in Step S<b>212</b> and S<b>213</b>, the compression condition calculation unit B<b>212</b> determines the relationship between the hidden layer activation data and the threshold value. Specifically, in Step S<b>212</b>, when act_val is equal to or more than a certain threshold. value th_val, the processing proceeds to the process of S<b>213</b>. When act_val is less than the certain threshold value th_val, don't care is set (the processing is ended). In Step S<b>213</b>, when act_val is less than a certain threshold value th_inval, the compression condition calculation unit B<b>212</b> determines the neuron as a first compression priority order neuron. Conversely, when act_inval is equal to or more than the certain threshold value th_inval, the neuron determined as a second compression priority order neuron. The processing order of Steps S<b>212</b> and S<b>213</b> is not limited. That is, Step S<b>213</b> may be processed first, or both steps may be processed in parallel.</p><p id="p-0079" num="0078">Then, in Step S<b>214</b>, the compression condition calculation unit B<b>212</b> receives the first compression priority order neuron and the second compression priority order neuron. The compression condition calculation unit B<b>212</b> sets each neuron as a compression target neuron in descending order of priority, and outputs the compression condition including the compression location and the compression rate.</p><heading id="h-0012" level="1">Embodiment 2</heading><p id="p-0080" num="0079">Next, Embodiment 2 will be described. Embodiment 2 is different from Embodiment 1 in that the threshold value th_inf input to the index addition unit B<b>160</b> of the training image selection unit B<b>100</b> is automatically determined.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram illustrating a configuration of the training image selection unit B<b>100</b> in Embodiment 2. In the present configuration, an inferring unit B<b>180</b> and a threshold value determination unit B<b>170</b> are provided instead of the ROM B<b>180</b> in Embodiment 1 (<figref idref="DRAWINGS">FIG. <b>12</b></figref>). <figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates a processing flow of the training image selection unit B<b>100</b> in Embodiment 2. In this processing flow, Step S<b>180</b> is added to Embodiment 2 (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). Since other components and processing flows are similar to those of Embodiment 1, only the above-described differences will be described.</p><p id="p-0082" num="0081">In Step S<b>180</b>, the inferring unit B<b>180</b> receives the inference data set <b>1002</b> and the pre-compression NN model <b>1003</b>. The inferring unit B<b>181</b> executes inference on the received data and outputs the certainty of the inference result. The threshold value determination unit B<b>170</b> determines the value of the threshold value th_inf based on the certainty of the present inference result. The threshold value th_inf is used for adding an index in Step S<b>170</b>.</p><heading id="h-0013" level="1">Embodiment 3</heading><p id="p-0083" num="0082">Next, Embodiment 3 will be described. Embodiment 3 is an example in which each of the above-described embodiments is applied to the AD-ECU <b>1</b>. In Embodiment 3, compression of the NN is online tuned in the AD-ECU <b>1</b>. The AD-ECU <b>1</b> is an autonomous driving or driving support ECU (also simply referred to as an autonomous driving ECU).</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an overall configuration of the AD-ECU <b>1</b>. The AD-ECU <b>1</b> is one type of in-vehicle ECU installed is a moving object such as a car, and executes information processing for autonomous driving and driving support. Specifically, external field information is acquired from sensors such as a camera <b>21</b>, a light detection and ranging (LiDAR, Laser Imaging Detection and Ranging) <b>22</b>, and a Rader <b>23</b>. Regarding the external field information, information detected by the LIDAR <b>22</b> and the Rader <b>23</b> is merged to obtain the external field information. The external field information is stored in a storage unit <b>11</b>. Compression in the NN compression device <b>121</b> is executed together with NN inference (neural network inference) and DNN inference by using the trained NN and the external field information stored in the storage unit <b>11</b>. A behavior plan indicating control contents is output to a control system that controls an actuator, an engine, and the like, and autonomous driving and driving support are realized.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates a device configuration of the NN compression device <b>121</b> in the AD-ECU <b>1</b>. The NN compression device <b>121</b> illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the training image selection unit B<b>100</b>, the NW compression unit B<b>200</b>, and the memory B<b>300</b> are common as the components. In addition, a behavior plan inferring unit X<b>100</b> (output device) that outputs a behavior plan (inference result) to the control system is provided. The other components are similar to those in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. In the NN compression device <b>121</b>, a behavior plan (inference result) is output to the control system. by using the inference image set <b>1002</b>-<b>1</b> being information acquired from the sensors, a situation of an engine of the moving object, and the like, and information. input via a communication I/F. At this time, the compression processing and the re-training processing in Embodiments 1 and 2 are performed. Therefore, it possible to appropriately tune the NW in accordance with a travel environment, and it is possible to suppress a decrease in recognition accuracy of the post-compression NW model <b>1007</b> due to a change in the travel environment.</p><p id="p-0086" num="0085">Furthermore, by applying each embodiment to autonomous driving and driving support, it is expected to improve the accuracy of recognizing an object such as pedestrians and automobiles, and to enhance security against a cyberattack of a third party using training image that induces erroneous recognition.</p><heading id="h-0014" level="1">Embodiment 4</heading><p id="p-0087" num="0086">Next, Embodiment 4 will be described. In Embodiment 4, similarly to Embodiment 3, the functions of the respective embodiments are applied to the AD-ECU <b>1</b>. In Embodiment 4, fine tuning of the NN is executed. in the AD-ECU <b>1</b>.</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>23</b></figref> illustrates a device. configuration. an NN compression device <b>121</b> in Embodiment 4. In the present configuration, in Embodiment 3, the re-training unit B<b>230</b> calculates a post-tuning NN model by using (the valid training image of) a pre-tuning NN model before fine-tuning and the indexed training image set <b>1004</b>-<b>1</b>. This makes it possible to perform fine correction in accordance with the travel environment of the moving object. Therefore, in autonomous driving and driving support, it is expected to improve the accuracy of recognizing an object such as pedestrians and automobiles, and to enhance security against a cyberattack of a third party using a training image that induces erroneous recognition.</p><heading id="h-0015" level="1">Embodiment 5</heading><p id="p-0089" num="0088">Next, Embodiment 5 will be described. Embodiment 5 is an example in which compression of an NN is tuned by focusing on a specific &#x201c;interested object&#x201d; in the AD-ECU by using the present invention. <figref idref="DRAWINGS">FIG. <b>24</b></figref> illustrates a device configuration of an NN compression device <b>121</b> in Embodiment 5. In the present configuration, information regarding the &#x201c;interested object&#x201d; is also input to the training image selection unit B<b>100</b> and used for selection processing. In autonomous driving and driving support, a person, another vehicle, a bicycle, or the like is used as the interested object. By processing the images separately from other images, it is expected to improve the accuracy of recognizing an interested object such as pedestrians and automobiles, and to enhance security against a cyberattack of a third party using a training image that induces erroneous recognition.</p><p id="p-0090" num="0089">According to the above embodiments, the following effects are obtained. <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref> illustrate evaluations of the recognition accuracy and the training time when the training of an identification problem is performed by ResNet <b>110</b>. By applying each embodiment of the present invention, the training time until the same degree of recognition accuracy is achieved is reduced to 1/10. Furthermore, by removing the invalid image using the present invention, it is possible to obtain an effect of improving the recognition accuracy by 4.2 pt when the same number of iterations is trained. Furthermore, it is possible to suppress (1) erroneous recognition of the post-compression NN model <b>1007</b> due to the presence of the invalid training image. Since the invalid training image can be excluded from the training data set used for re-training, it is possible to simultaneously suppress (2) prolongation of the compression design period due to the enormous number of training images.</p><p id="p-0091" num="0090">According to the embodiments of the present invention, as illustrated in <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>, it is possible to shorten recognition accuracy (suppression of recognition degradation) and a design period compared with the first and second conventional techniques.</p><p id="p-0092" num="0091">The present technology can be extended not only to deep training but also to a classifier of classical machine training such as a support vector machine (SVM).</p><heading id="h-0016" level="1">REFERENCE SIGNS LIST</heading><p id="p-0093" num="0092">B<b>100</b> training image selection unit</p><p id="p-0094" num="0093">B<b>200</b> neural network compression unit</p><p id="p-0095" num="0094">B<b>300</b> memory</p><p id="p-0096" num="0095">B<b>110</b> influence value calculation unit</p><p id="p-0097" num="0096">B<b>160</b> index addition unit</p><p id="p-0098" num="0097">B<b>120</b> unbiased variance calculation unit</p><p id="p-0099" num="0098">B<b>130</b> selector</p><p id="p-0100" num="0099">B<b>140</b> averaging unit</p><p id="p-0101" num="0100">B<b>150</b> PCA/clustering unit</p><p id="p-0102" num="0101">B<b>220</b> compression. unit</p><p id="p-0103" num="0102">B<b>230</b> re-training unit</p><p id="p-0104" num="0103">B<b>210</b> compression condition determination unit</p><p id="p-0105" num="0104">B<b>151</b> PCA unit</p><p id="p-0106" num="0105">B<b>152</b> dimension reducing unit</p><p id="p-0107" num="0106">B<b>153</b> clustering unit</p><p id="p-0108" num="0107">B<b>211</b>, S<b>211</b> neural network forward propagation unit</p><p id="p-0109" num="0108">B<b>170</b>, B<b>180</b>, B<b>154</b>, B<b>213</b>, B<b>190</b> ROM</p><p id="p-0110" num="0109">B<b>212</b> compression condition calculation unit</p><p id="p-0111" num="0110">B<b>181</b> inferring unit</p><p id="p-0112" num="0111">X<b>100</b> behavior plan inferring unit</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A neural network compression device that compresses a neural network by using a training data set that has been input, the neural network compression device comprising:<claim-text>a training image selection unit that calculates an influence value on an inference result calculated by using an inference data set and a neural network model for the training data set, that classifies the training data set into valid training data necessary for the compression and invalid training data unnecessary for the compression, based on the influence value, and that generates an indexed training data set; and</claim-text><claim-text>a neural network compression unit that compresses the neural network model, based on the indexed training data set and the neural network model.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The neural network compression device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the training image selection unit generates the indexed training data set by assigning an index indicating the image selection unit to at least one of the valid training data and the invalid training data.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The neural network compression device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the training image selection unit generates the indexed training data set by assigning an index indicating the training image selection unit to both the valid training data and the invalid training data.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The neural network compression device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the training image selection unit generates the indexed training data set by storing the valid training data and the invalid training data in storage media having different addresses.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The neural network compression device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the neural network compression unit executes re-training of the neural network that has been compressed, by using the valid training data.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The neural network compression device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the neural network compression unit uses a combination of activation frequencies of the valid training data and the invalid training data for neurons constituting the neural network to determine whether to remove the neuron, and executes the compression.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The neural network compression device according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the neural network compression unit</claim-text><claim-text>removes the neuron having the activation frequency that is low in the valid training data and is high in the invalid training data, in preference to removal of the neuron having the activation frequency that is low in the valid training data and is low in the invalid training data.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. An autonomous driving ECU installed in a moving object, the auto autonomous driving ECU comprising:<claim-text>a storage device that stores external field information acquired from a sensor and a neural network;</claim-text><claim-text>the neural network compression device according to <claim-ref idref="CLM-00001">claim 1</claim-ref> that compresses the neural network stored in the storage device; and</claim-text><claim-text>an output device that outputs behavior plan of the moving object to a control system of the moving object, by using the neural network that has been compressed.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A neural network compression method using a neural network compression device that compresses a neural network by using a training data set that has been input, the neural network compression method comprising:<claim-text>calculating an influence value on an inference result calculated by using an inference data set and a neural network model for the training data set;</claim-text><claim-text>classifying the training data set into valid training data necessary for the compression and invalid training data unnecessary for the compression based on the influence value;</claim-text><claim-text>generating an indexed training data set; and</claim-text><claim-text>compressing the neural network model based on the indexed training data set and the neural network model.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The neural network compression method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the indexed training data set is generated by a signing an index indicating the training image selection unit to at least one of the valid training data and the invalid training data.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The neural network compression method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>the indexed training data set is generated by assigning an index indicating the training image selection unit to both the valid training data and the invalid training data.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The neural network compression method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the indexed training data set is generated by storing the valid training data and the invalid training data in storage media having different addresses.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The neural network compression method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>re-training of the neural network that has been compressed is executed by using the valid training data.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The neural network compression method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>a combination of activation frequencies of the valid training data and the invalid training data for neurons constituting the neural network is used to determine whether to remove the neuron, and</claim-text><claim-text>the compression is executed.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The neural network compression method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein<claim-text>the neuron having the activation frequency that is low in the valid training data and is high in the invalid training data is removed in preference to removal of the neuron having the activation frequency that is low in the valid training data and is low in the invalid training data.</claim-text></claim-text></claim></claims></us-patent-application>