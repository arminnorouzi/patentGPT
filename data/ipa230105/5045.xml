<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005046A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005046</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17731796</doc-number><date>20220428</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>Q</subclass><main-group>30</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>Q</subclass><main-group>30</main-group><subgroup>0635</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>Q</subclass><main-group>30</main-group><subgroup>0613</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>Q</subclass><main-group>30</main-group><subgroup>0639</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>167</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>016</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MOBILE-ASSISTED PICKER TECHNIQUES FOR IN-STORE NAVIGATION</invention-title><us-related-documents><continuation-in-part><relation><parent-doc><document-id><country>US</country><doc-number>17366451</doc-number><date>20210702</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17731796</doc-number></document-id></child-doc></relation></continuation-in-part></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NCR Corporation</orgname><address><city>Atlanta</city><state>GA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>McDaniel</last-name><first-name>Christian Lee</first-name><address><city>Atlanta</city><state>GA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An optimized route to pick a list of items through a store is obtained. Sensor data for a mobile device of a user is evaluated in real time to provide fine-grain orientation, direction, location, and behaviors of the user along the route during a picking session. A determination is made based on the sensor data and a current portion of the route that the user has picked a current item from the store and a next item along with its route guidance is provided to the user without any user action being required. In an embodiment, tactile, speech, and/or audible feedback is provided from the device when the determination is made that an item was picked by the user. In an embodiment, predefined movements of the device are identified as user-provided route commands and processed on behalf of the user during the session.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="169.67mm" wi="158.75mm" file="US20230005046A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="204.98mm" wi="165.69mm" file="US20230005046A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="194.14mm" wi="180.59mm" file="US20230005046A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="158.67mm" wi="179.24mm" file="US20230005046A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to, is co-pending with, and is a Continuation-In-Part (CIP) of application Ser. No. 17/366,451 entitled&#x201d; &#x201c;Methods and a System for In-Store Navigation&#x201d; and filed Jul. 2, 2021; the disclosure of which is incorporated by reference herein in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Profession pickers are a major source of income for grocery stores. If this was not already the case, it is now the cash in view of the COVID19 pandemic where many consumers began ordering their groceries through picking services. A picker is an individual that fulfills shopping lists for customers. Some pickers may be employees of a store that fulfills online shopping orders for its customer. The fulfilled orders can be subsequently picked up at the store or delivered directly to the homes of the customers. Other pickers are third-party services that contract out individuals to fulfill and deliver shopping orders to customers who order through their third-party applications.</p><p id="p-0004" num="0003">The quicker a picker can navigate a store; the more orders can be fulfilled. Finding unfamiliar items in an unknown store can be a difficult, a chaotic, and a time-consuming exercise (causing shopping delays). Often, pickers (especially third-party contract pickers) are looking for items they are not familiar with in unfamiliar stores. Finding those items as part of a larger list can be difficult and frustrating.</p><p id="p-0005" num="0004">To complicate matters, some pickers may be simultaneously picking multiple orders within a given store. Unless the picker manually pre-organizes and pre-plans, the picker may find that they are traversing the store or traversing same locations within the store multiple times, which is inefficient. Most third-party pickers receive the orders via a mobile application, such that opening one order, closing it, and opening another order on their phone while inside the store is not practical.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">In various embodiments, methods and a system for mobile-assisted picker techniques for in-store navigation are presented.</p><p id="p-0007" num="0006">According to an aspect, a method for mobile-assisted picker techniques for in-store navigation is presented. A current item is obtained from a list of items to pick within a store along a route within the store for picking the list of items. Sensor data on a user device is monitored and a determination is made that the current item was picked by a user based on the sensor data without the user affirmatively indicating the current item was picked through a user interface of the device. A next item is obtained from the list of items and route guidance is provided through the user device for the user to travel from the current item within the store to the next item.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of a system for mobile-assisted picker techniques for in-store navigation, according to an example embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram of a method for mobile-assisted picker techniques for in-store navigation, according to an example embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of another method for mobile-assisted picker techniques for in-store navigation, according to an example embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of a system <b>100</b> for mobile-assisted picker techniques for in-store navigation, according to an example embodiment. It is to be noted that the components are shown schematically in greatly simplified form, with only those components relevant to understanding of the embodiments being illustrated.</p><p id="p-0012" num="0011">Furthermore, the various components (that are identified in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) are illustrated and the arrangement of the components is presented for purposes of illustration only. It is to be noted that other arrangements with more or less components are possible without departing from the teachings of providing mobile-assisted picker techniques for in-store navigation presented herein and below.</p><p id="p-0013" num="0012">System <b>100</b> and the methods that follow permit a truly hands-free picking experience that can be utilized by customers having a shopping list or by professional pickers (pickers employed by a store and/or third-party contract-based pickers). Sensors and components of the user's mobile devices are tracked when the picker (user) enters a given store. A route for traversing the store to optimally pick the items of a shopping list along the route is pushed to the mobile device. The movements, directions, and pauses of the user are tracked relative to the route and inferences are made when the user is believed to have acquired an item from the list, the directions to the next item in the list is immediately rendered on the mobile device. The navigational guidance can be visual, audible, and/or tactile based through the user's mobile device so that the user can focus on navigating the store for items in the list without focusing on the mobile device. In an embodiment, specific movements by the user of the mobile device causes a previous navigation instruction to be loaded and presented to the user and/or a navigation instruction to be repeated for the user.</p><p id="p-0014" num="0013">System <b>100</b> provides a fine-grain, rather than coarse-grain, interactive in-store navigation session with a user/picker for purposes of picking items from a shopping list from a given store. The navigational guidance during the session is intuitive and non-obtrusive, such that affirmative operation of the user's mobile device is not required, and the user can passively interact with the mobile device.</p><p id="p-0015" num="0014">As used herein the terms &#x201c;picker&#x201d; and/or &#x201c;user may be used interchangeably and synonymously. This refers to a customer of a store that is at a store with a mobile device to pick items off a shopping list or this refers to a professional individual employed by the store to fulfill online orders or a professional individual that contracts with a third-party ordering service to fulfilling online customer shopping lists for delivery to the customers.</p><p id="p-0016" num="0015">System <b>100</b> comprises a cloud or server <b>110</b>, a user-operated device <b>120</b>, and optionally, includes a plurality of third-party servers and/or retail <b>130</b>.</p><p id="p-0017" num="0016">Cloud/Server <b>110</b> comprises at least one processor <b>111</b> and a non-transitory computer-readable storage medium <b>112</b>. Medium <b>112</b> comprises executable instructions for a navigation instructions generator <b>113</b> and an application (app) interface <b>114</b>. The executable instructions when executed by processor <b>111</b> from the medium <b>112</b> cause processor <b>111</b> to perform operations discussed herein and below with navigation instructions generator <b>113</b> and app interface <b>114</b>.</p><p id="p-0018" num="0017">Each user-operated device <b>120</b> comprises a processor <b>121</b> and a non-transitory computer-readable storage medium <b>122</b>. Medium <b>122</b> comprises executable instructions for a mobile app <b>123</b>-<b>1</b> and/or a navigator <b>123</b>-<b>2</b>. The executable instructions when executed by processor <b>121</b> from medium <b>122</b> cause processor <b>121</b> to perform operations discussed herein and below with respect to <b>123</b>-<b>1</b> and/or <b>123</b>-<b>2</b>. Each user-operated device <b>120</b> also comprises a touch display <b>124</b>, an accelerometer <b>125</b>, one or more wireless transceivers <b>126</b>, a compass <b>127</b>, at least one camera <b>128</b>, a microphone <b>129</b>-<b>1</b>, and a speaker or a speaker input jack <b>129</b>-<b>2</b>.</p><p id="p-0019" num="0018">Each optional third-party and/or retail server <b>130</b> comprises a processor <b>131</b> and a non-transitory computer-readable storage medium <b>132</b>. Medium <b>132</b> comprises executable instructions for a third-party app service <b>133</b>, a retail order service <b>134</b>, and an Application Programming Interface (API) <b>135</b>. The executable instructions when executed by processor <b>131</b> from medium <b>132</b> cause processor <b>131</b> to perform operations discussed herein and below with respect to <b>133</b>-<b>135</b>.</p><p id="p-0020" num="0019">Navigation instructions generator <b>113</b> receives as input a list of items provided by app <b>123</b> or, optionally, provided via API <b>135</b> from third-party app service <b>133</b> and/or retail order service <b>134</b> to app interface <b>114</b>. App <b>123</b> or API <b>135</b> also provides a store identifier or store name associated with a store for which the items on the list are to be purchased by a shopper (the shopper operates device <b>120</b>).</p><p id="p-0021" num="0020">Navigation instructions generator <b>113</b> inspects the map data (such as a planogram or modified and enhanced planogram) for a given store and connects the items on the list. Navigation instructions generator <b>113</b> constructs hierarchical map data from information maintained for the store using the store's items, regions, and endpoints as the nodes of the graph/map data. Each item in the list is then connected to that item's nearest endpoint and a pathfinding algorithm is processed to find an intermediate path between the endpoints. This results in a strongly-connect graph of the relevant endpoints (adjacency matrix).</p><p id="p-0022" num="0021">Navigation instructions manager <b>113</b> then uses a modified Traveling Sales Person (TSP) algorithm to generate an optimized, ordered-list of items based on the optimal path through each endpoint.</p><p id="p-0023" num="0022">Navigation instructions manager <b>113</b> produces the instructions from getting from item A to item B by analyzing each segment of the journey independently, such as: entrance-&#x3e;item <b>0</b>, item <b>0</b>-&#x3e;item <b>1</b>, item <b>1</b>-&#x3e;item <b>2</b>, . . . item n-&#x3e;checkout. For each segment a Dijkstra algorithm is processed on the full graph, with item A as the start and item B as the destination.</p><p id="p-0024" num="0023">Next, navigation instructions manager <b>113</b> groups the lists of endpoints by region and feeds a new instruction only when changing regions. This reduces the list of endpoints to contain only the necessary nodes for producing the navigation instructions for the shopper. For example, before reduction:</p><p id="p-0025" num="0024">The above-referenced approach is repeated by the navigation instructions manager <b>113</b> for every pair of items within the list of items.</p><p id="p-0026" num="0025">Finally, the list of regions are then translated into a list of intuitive instructions in text that can be read to the shopper and can be provided in any number of manners, such as through app <b>123</b>, through an existing third-party app service <b>133</b> and/or retail order service <b>134</b>, through navigator <b>123</b>-<b>2</b>.</p><p id="p-0027" num="0026">Once the route and the shopping list of items for an in-store session is know and delivered to navigator <b>123</b>-<b>2</b> and/or app <b>123</b>-<b>1</b>. Navigator <b>123</b>-<b>2</b> monitors a current geographical location of the user for a location that corresponds with the store associated with the shopping list and the route. App <b>123</b>-<b>1</b> may also include a user-facing interface option for the user to indicate that the user is at the entrance of the store and desires to being the picking session along the route for the items on the shopping list. Still further, the user may scan a barcode placed at the entrance of the store from within a user-interface screen of app <b>123</b>-<b>1</b> which initiates the picking session.</p><p id="p-0028" num="0027">Once the picking session is initiated either through user affirmative action or no affirmative action of the user by mapping a current location of device <b>120</b> to a location known to be associated with the store, navigator <b>123</b>-<b>2</b> begins to track and monitor with various sensors (accelerometer <b>125</b> and compass <b>127</b>) and/or interact and monitor other integrated devices (wireless transceivers <b>126</b>, camera <b>128</b>, microphone <b>129</b>-<b>1</b>, and speaker <b>129</b>-<b>2</b>).</p><p id="p-0029" num="0028">Accelerometer <b>125</b> provides data on acceleration forces on device <b>120</b>, such that the device's position in space, vibration, and movement of the device <b>120</b> can be tracked and monitored by navigator <b>123</b>-<b>2</b>. The accelerometer's data also allows for the orientation of the device to be derived based on acceleration due to Earth's gravity on device <b>120</b>.</p><p id="p-0030" num="0029">Compass <b>127</b> provides more detailed data on a current orientation of the device <b>120</b> being operated by the user during the picking session.</p><p id="p-0031" num="0030">During the picking session, navigator <b>123</b>-<b>2</b> processes a next item on the list based on its route data and location data within the store while monitoring a near exact physical location of device <b>120</b> and its orientation using the data provided by the accelerometer <b>125</b> and/or compass <b>127</b>. When a user is detected as stopping and reaching, this can be ascertained from the accelerometer data and/or compass data and mapped to a corresponding known location of an item on the shopping list. Once the user is stopped at a location where the next item is known to be and reaches or bends down, navigator <b>123</b>-<b>2</b> infers or assumes that the user has picked that item and immediately begins to provide navigational route guidance for a next item on the shopping list defined in the route. The user does not have to interact with the user-facing interface of app <b>123</b>-<b>1</b> to press an interface button indicating that the user is ready to move on to a next item.</p><p id="p-0032" num="0031">In this way, the user is not required to actively interact with app or affirmatively operate app <b>123</b>-<b>1</b> during the picking session, rather since fine-grain details about the user is being tracked with respect to a current item that is being picked and the route through the sensor data (accelerometer <b>125</b> and/or compass <b>127</b>) include fine-grain location data, orientation data for (reaching or bending down), navigator <b>123</b>-<b>2</b> identifies when an item is picked and immediately advances in the route to the next item that is to be picked. Turning, stopping, stepping, bending, current directions of the user within the store, current orientations of the user within the store, and reaching are all behaviors that navigator <b>123</b>-<b>2</b> tracks for the user through device <b>120</b> based on the data provided by the accelerometer <b>125</b> and/or compass <b>127</b>. The user's attention is directed to identifying an item that is to be picked and picking it without interacting with app <b>123</b>-<b>1</b>. This creates a seamless and efficient picking session and one in which the user does not have to handle the picked items from the list while at the same time attempting to interact with the user-facing interface of app <b>123</b>-<b>1</b>.</p><p id="p-0033" num="0032">The above-referenced navigation guidance continues until the last item (no more next items to pick) is picked from the shopping list and the route through the store concludes.</p><p id="p-0034" num="0033">In an embodiment, during the picking session navigator <b>123</b>-<b>2</b> and/or app <b>123</b>-<b>1</b> can provide feedback and/or guidance to the user in a number of unobtrusive manners. For example, speech guidance can be provided through a speaker <b>129</b>-<b>2</b> that provides meaningful guidance to the user along the route, such as at the beginning of the session providing speech through speaker <b>129</b>-<b>2</b> indicating that the first item is a 12 pack of Coke&#xae; located in aisle <b>10</b> reached by turning left past the registers and 10 feet thereafter turn right into aisle <b>10</b>. When navigator <b>123</b>-<b>2</b> determines an item was picked a bell or beep can be played through speaker <b>129</b> or speech may indicate coke has been picked, still further, navigator <b>123</b>-<b>2</b> can cause the device <b>120</b> to vibrate as tactile feedback that the item being picked was determined to have been picked by the user and navigator <b>123</b>-<b>2</b> is moving to the next item along the route to pick from the shopping list. In addition to predefined sounds, beeps, and/or tactile-based feedback, a user-facing interface of app <b>123</b>-<b>1</b> may be updated visual to indicate a checkmark next to the item that was determined to be picked and the route to the next item in the list displayed along with written navigational guidance. In fact, navigational guidance and feedback for picked items can be done in any combination of or in all the manners that was discussed above via speech, visual information, predefined audible tones, and/tactile notifications.</p><p id="p-0035" num="0034">In an embodiment, navigator <b>123</b>-<b>2</b> also responds and processes user-received command which may be provided through speech through microphone <b>129</b>-<b>1</b> or by the user moving device <b>120</b> in predefined manners that are mapped to user commands. For example, if the user shakes device <b>120</b> or moves device <b>120</b> is a particular way (Up and down, side to side, etc.), the accelerometer's data for the device <b>120</b> is detected as being the movement reserved as a known user command and navigator <b>123</b>-<b>2</b> processes the command. For example, a command may be to repeat a current navigation instruction to a next item in the list, go back to the last picked item instruction, move head a predefined number of items in the list and start route guidance from that point. The user can use predefined movements of device <b>120</b> to communicate a route guidance command and/or the user can use speech spoken into microphone <b>129</b>-<b>1</b>, which is translated into text and mapped to the route guidance command. In this way, the picking session is interactive and yet still largely passive because the user does not have to interact with any user-facing interface option through touch display <b>124</b> to interact during the picking session with navigator <b>123</b>-<b>2</b>; rather, the user can speak a command and/or move/shake device <b>120</b> in a reserved and predefined manner for navigator <b>123</b>-<b>2</b> to identify a user-directed command and process the command during the session. The user is not required to provide input or direction through touches to touch display <b>124</b> during the session, which allows the user to adjust the session based on the needs of the user while only minimally interacting with device <b>120</b>.</p><p id="p-0036" num="0035">In an embodiment, navigator <b>123</b>-<b>2</b> may be subsumed within and processed as part of app <b>123</b>-<b>1</b>.</p><p id="p-0037" num="0036">In an embodiment, navigator <b>123</b>-<b>2</b> is a plug-in or enhanced to an existing app <b>123</b>-<b>1</b>.</p><p id="p-0038" num="0037">In an embodiment, navigator <b>123</b>-<b>2</b> is processed on a remote device remote from device <b>120</b> (such as cloud/server <b>110</b>) and interacts in real time with device <b>120</b>, components <b>124</b>-<b>129</b>-<b>2</b> and app <b>123</b>-<b>1</b>. In this embodiment, all accelerometer data, location data for location services of device <b>120</b>, and compass data are streamed in real time to a remotely operating version of navigator <b>123</b>-<b>2</b>.</p><p id="p-0039" num="0038">In an embodiment, navigator <b>123</b>-<b>2</b> utilizes wireless signals to enhance the accuracy of pinpointing an exact location at any given point in time of device <b>120</b> within the store during the picking session.</p><p id="p-0040" num="0039">In an embodiment, navigator <b>123</b>-<b>2</b> utilizes video frames (when provided or available) from camera <b>128</b> to calibrate any given current location of device <b>120</b> within the store during a picking session.</p><p id="p-0041" num="0040">In an embodiment, the device <b>120</b> is a phone, a tablet, a laptop, or a wearable processing device.</p><p id="p-0042" num="0041">The above-referenced embodiments and other embodiments are now discussed with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram of a method <b>200</b> for mobile-assisted picker techniques for in-store navigation, according to an example embodiment. The software module(s) that implements the method <b>200</b> is referred to as a &#x201c;in-store picklist route guidance manager.&#x201d; The in-store picklist route guidance manager is implemented as executable instructions programmed and residing within memory and/or a non-transitory computer-readable (processor-readable) storage medium and executed by one or more processors of one or more devices. The processor(s) of the device(s) that executes the in-store picklist route guidance manager are specifically configured and programmed to process the in-store picklist route guidance manager. The in-store picklist route guidance manager has access to one or more network connections during its processing. The connections can be wired, wireless, or a combination of wired and wireless.</p><p id="p-0044" num="0043">In an embodiment, the device that executes the in-store picklist route guidance manager is cloud <b>110</b>. In an embodiment, the device that executes in-store picklist route guidance manager is user-operated device <b>120</b>.</p><p id="p-0045" num="0044">In an embodiment, the in-store picklist route guidance manager is all of, or some combination of navigation instructions manager <b>113</b>, app interface <b>114</b>, app <b>123</b>-<b>1</b>, and/or navigator <b>123</b>-<b>2</b>.</p><p id="p-0046" num="0045">At <b>210</b>, in-store picklist route guidance manager obtains a current item from a list of items to pick within a store along a route within a store. The route may be optimally produced by navigation instructions generator <b>113</b> as discussed above.</p><p id="p-0047" num="0046">In an embodiment, at <b>211</b>, the in-store picklist route guidance manager obtains the list (picklist) from a third-party network service <b>133</b> or a retail ordering service <b>134</b>.</p><p id="p-0048" num="0047">In an embodiment of <b>211</b> and at <b>212</b>, the in-store picklist route guidance manager uses a store identifier for the store and the list (picklist) and obtains the route from a cloud service, such as navigation instructions generator <b>113</b>.</p><p id="p-0049" num="0048">At <b>220</b>, the in-store picklist route guidance manager monitors sensor data generated by and from a user device <b>120</b>.</p><p id="p-0050" num="0049">In an embodiment, at <b>221</b>, the in-store picklist route guidance manager obtains location data from location services of the user device <b>120</b>, obtains accelerometer data from an accelerometer <b>125</b> of the user device <b>120</b>, and obtains compass data from a compass <b>125</b> of the user device <b>120</b>.</p><p id="p-0051" num="0050">At <b>230</b>, the in-store picklist route guidance manager determines the current item was picked by a user based on the sensor data without the user affirmatively indicating the current item was picked through a user interface of the user device <b>120</b>.</p><p id="p-0052" num="0051">In an embodiment of <b>221</b> and <b>230</b>, at <b>231</b>, the in-store picklist route guidance manager detects from the location data and the accelerometer data that the user has stopped in a predefined location of the store that is known to be associated with the current item.</p><p id="p-0053" num="0052">In an embodiment of <b>231</b> and at <b>232</b>, the in-store picklist route guidance manager detected from the accelerometer data that the user is reaching or bending at the predefined location.</p><p id="p-0054" num="0053">In an embodiment of <b>232</b> and at <b>233</b>, the in-store picklist route guidance manager detects from the accelerometer and the compass data that the user has turned away from the current item at the predefined location in a direction toward the next item in the route.</p><p id="p-0055" num="0054">In an embodiment of <b>233</b> and at <b>234</b>, the in-store picklist route guidance manager assumes or infers that the user is in possession of the current item based on <b>233</b>.</p><p id="p-0056" num="0055">In an embodiment, at <b>235</b>, the in-store picklist route guidance manager causes the device to provide an audible or a tactile alert indicating to the user of the user device <b>120</b> the user is believed to have picked the current item and route guidance is moving on to the next item in the picklist (list) associated with the route.</p><p id="p-0057" num="0056">At <b>240</b>, the in-store picklist route guidance manager obtains a next item from the list using the route.</p><p id="p-0058" num="0057">At <b>250</b>, the in-store picklist route guidance manager provides route guidance via the user device <b>120</b> for the user to travel from the current item within the store to the next item of the list.</p><p id="p-0059" num="0058">In an embodiment, at <b>260</b>, the in-store picklist route guidance manager iterates back to <b>220</b> through <b>250</b> until a last item is determined to have been picked by the user.</p><p id="p-0060" num="0059">In an embodiment, at <b>270</b>, the in-store picklist route guidance manager identifies a route guidance command based on user movement of the user device <b>120</b> that is detected in accelerometer data from an accelerometer sensor <b>125</b> of the user device <b>120</b>.</p><p id="p-0061" num="0060">In an embodiment of <b>270</b> and at <b>271</b>, the in-store picklist route guidance manager processes the route guidance command to alter the route guidance based on <b>270</b>, for example by repeating route guidance for the current item or the next item, skipping ahead to a further item beyond the next item, etc.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of another method <b>300</b> for mobile-assisted picker techniques for in-store navigation, according to an example embodiment. The software module(s) that implements the method <b>300</b> is referred to as a &#x201c;route guidance manager.&#x201d; The route guidance manager is implemented as executable instructions programmed and residing within memory and/or a non-transitory computer-readable (processor-readable) storage medium and executed by one or more processors of one or more devices. The processor(s) of the device(s) that executes the route guidance manager are specifically configured and programmed to process the route guidance manager. The route guidance manager has access to one or more network connections during its processing. The network connections can be wired, wireless, or a combination of wired and wireless.</p><p id="p-0063" num="0062">In an embodiment, the device that executes the route guidance manager is cloud <b>110</b>. In an embodiment, the device that executes the route guidance manager is user-operated device <b>120</b>.</p><p id="p-0064" num="0063">In an embodiment, the route guidance manager is all of, or some combination of navigation instructions manager <b>113</b>, app interface <b>114</b>, app <b>123</b>-<b>1</b>, navigator <b>123</b>-<b>2</b>, and/or method <b>200</b>.</p><p id="p-0065" num="0064">The processing of the route guidance manager represents another and, in some ways, enhanced processing perspective from that which was discussed above with system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and method <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0066" num="0065">At <b>310</b>, the route guidance manager activates or establishes a picking session for a user for picking items of a list (picklist) within a given store based on a route through the store to each of the items.</p><p id="p-0067" num="0066">At <b>320</b>, the route guidance manager provides route guidance to the user through a user device <b>120</b> for each of the items using the route and current locations of the user (based on locations tracked for the user device <b>120</b>) within the store.</p><p id="p-0068" num="0067">In an embodiment, at <b>321</b>, the route guidance manager provides the route guidance as spoken speech to the user through a speaker <b>129</b>-<b>2</b> of the user device <b>120</b> or through a headset interfaced to the user device <b>120</b>.</p><p id="p-0069" num="0068">At <b>330</b>, the route guidance manager infers a current item is picked from the list (picklist) based on sensor data provided by the user device <b>120</b> during the picking session and without the user affirmatively providing any input through a user-facing interface of the user device <b>120</b>.</p><p id="p-0070" num="0069">In an embodiment of <b>321</b> and <b>330</b>, at <b>331</b>, the route guidance manager provides one or more of distinctive audio feedback and distinctive tactile feedback (distinctive user device <b>120</b> vibrations) to the user through the user device <b>120</b> indicating when the current item is inferred as being picked by the user during the picking session.</p><p id="p-0071" num="0070">In an embodiment, at <b>332</b>, the route guidance manager repeats <b>320</b> of the route guidance for the current item based on a predefined movement of the user device <b>120</b> being detected as being made by the user from the sensor data. That is, the user issues a route-based command by moving and/or orienting the user-device <b>120</b> in a predefined manner, which is then detected in the sensor data and processed by route guidance manager.</p><p id="p-0072" num="0071">In an embodiment, at <b>333</b>, the route guidance manager determines from the sensor data when the user has stopped, turned, reached, and bended at a predefined location along the route within the store that is associated with the current item.</p><p id="p-0073" num="0072">At <b>340</b>, the route guidance manager automatically obtains a next item as the current item from the list (picklist) based on the route and iterated back to <b>320</b> to continue with the picking session until a last item is inferred as being picked by the user based on the sensor data.</p><p id="p-0074" num="0073">In an embodiment, at <b>350</b>, the route guidance manager completes the route guidance for the last item without the user looking at a touch display <b>124</b> of the user device <b>120</b> during the picking session and without the user touching the touch display <b>124</b> during the picking session.</p><p id="p-0075" num="0074">It should be appreciated that where software is described in a particular form (such as a component or module) this is merely to aid understanding and is not intended to limit how software that implements those functions may be architected or structured. For example, modules are illustrated as separate modules, but may be implemented as homogenous code, as individual components, some, but not all of these modules may be combined, or the functions may be implemented in software structured in any other convenient manner.</p><p id="p-0076" num="0075">Furthermore, although the software modules are illustrated as executing on one piece of hardware, the software may be distributed over multiple processors or in any other convenient manner.</p><p id="p-0077" num="0076">The above description is illustrative, and not restrictive. Many other embodiments will be apparent to those of skill in the art upon reviewing the above description. The scope of embodiments should therefore be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled.</p><p id="p-0078" num="0077">In the foregoing description of the embodiments, various features are grouped together in a single embodiment for the purpose of streamlining the disclosure. This method of disclosure is not to be interpreted as reflecting that the claimed embodiments have more features than are expressly recited in each claim. Rather, as the following claims reflect, inventive subject matter lies in less than all features of a single disclosed embodiment. Thus, the following claims are hereby incorporated into the Description of the Embodiments, with each claim standing on its own as a separate exemplary embodiment.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>obtaining a current item from a list of items to pick within a store along a route within the store for picking the list of items;</claim-text><claim-text>monitoring sensor data on a user device;</claim-text><claim-text>determining the current item was picked by a user based on the sensor data without the user affirmatively indicating the current item was picked through a user interface of the device;</claim-text><claim-text>obtaining a next item from the list of items; and</claim-text><claim-text>providing route guidance through the user device for the user to travel from the current item within the store to the next item.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining further includes obtaining the list of items from a third-party picking network service of a retail ordering network service.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein obtaining further includes using a store identifier for the store and the list of items and obtaining the route from a cloud service.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein monitoring further includes obtaining location data from location services of the device, obtaining accelerometer data from an accelerometer of the device, and obtaining compass data from a compass of the device, wherein the sensor data comprises the location data, the accelerometer data, and the compass data.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein determining further includes detecting from the location data and the accelerometer data that the user has stopped in a predefined location of the store associated with current item.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein detecting further includes detecting from the accelerometer data that the user is reaching or bending at the predefined location.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein detecting further includes detecting from the accelerometer data and the compass data that the user has turned away from the current item at the predefined location in a direction towards the next item on the route.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein detecting further includes assuming or inferring that the current item is in the possession of the user based on the detecting that the user has turned away from the current item at the predefined location towards the next item on the route.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining further includes causing the device to provide an audible or a tactile alert indicating the user picked the current item from the list of items.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising, iterating back to the monitoring until a last item of the list of items is determined to have been picked by the user</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising, identifying a predefined route guidance command based on user movement of the device detected in accelerometer data from an accelerometer sensor of the device.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising, processing the predefined route guidance command to alter the route guidance based on the identifying.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A method, comprising:<claim-text>activating a picking session with a user for picking items of a list within a store based on a route through the store for each of the items;</claim-text><claim-text>providing route guidance to a user through a user device for each of the items using the route and current locations of the user within the store;</claim-text><claim-text>inferring a current item is picked for the list based on sensor data provided by the user device during the picking session and without the user affirmatively providing any input through a user-interface of the user device; and</claim-text><claim-text>automatically obtaining a next item as the current item from the list based on the route and iterating back to the providing to continue with the picking session until a last item is inferred as being picked by the user based on the sensor data.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein providing further includes providing the route guidance as spoken speech to the user through a speaker of the user device or through a headset interfaced to the user device.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein inferring further includes providing one or more of distinctive audible feedback and distinctive tactile feedback to the user through the user device indicating when the current item was inferred as being picked by the user.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein inferring further includes repeating the providing of the route guidance for the current item based on a predefined movement of the user device being detected as having been made by the user from the sensor data.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein inferring further includes determining from the sensor data when the user has stopped, turned, reached, and bended at a predefined location along the route within the store that is associated with the current item.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref> further comprising, completing the route guidance for the last item without the user looking at a touching display of the user device during any of the picking session and without the user touching the touch display during any of the picking session.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A device, comprising:<claim-text>an accelerometer;</claim-text><claim-text>a processor and a non-transitory computer-readable storage medium;</claim-text><claim-text>the non-transitory computer-readable storage medium comprises executable instructions; and</claim-text><claim-text>the executable instructions executed by the processor from the non-transitory computer-readable storage medium causing the processor to perform operations comprising:<claim-text>providing speech-based route guidance to a user to pick a list of items along a route within a store;</claim-text><claim-text>inferring a current item within the list of items was picked by the user based on data provided by the accelerometer; and</claim-text><claim-text>automatically obtaining a next item of the list along the route based on the inferring and iterating back to the providing until a last item of the list is inferred as having been picked by the user.</claim-text></claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The device of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the device is a phone, a laptop, a tablet, or a wearable processing device.</claim-text></claim></claims></us-patent-application>