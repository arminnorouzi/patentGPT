<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005250A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005250</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17792550</doc-number><date>20210217</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-026277</doc-number><date>20200219</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>30</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>7747</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>30</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">LEARNING DATASET GENERATION DEVICE AND LEARNING DATASET GENERATION METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>FANUC CORPORATION</orgname><address><city>Yamanashi</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ANDO</last-name><first-name>Toshiyuki</first-name><address><city>Yamanashi</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>FANUC CORPORATION</orgname><role>03</role><address><city>Yamanashi</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2021/005864</doc-number><date>20210217</date></document-id><us-371c12-date><date>20220713</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A learning dataset generation device includes: a memory that stores three-dimensional CAD data of a workpiece and a container; and one or more processors including hardware, wherein the one or more processors are configured to use the three-dimensional CAD data of the workpiece and the container, stored in the memory, to generate, in a three-dimensional virtual space, a plurality of imaging objects in which a plurality of the workpieces are bulk-loaded in different forms inside the container, acquire a plurality of virtual distance images by measuring each of the generated imaging objects by means of a virtual three-dimensional measurement machine disposed in the three-dimensional virtual space, accept at least one teaching position for each of the acquired virtual distance images, and generate a learning dataset by associating the accepted teaching position with each of the virtual distance images.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="187.96mm" wi="143.51mm" file="US20230005250A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="206.93mm" wi="155.28mm" orientation="landscape" file="US20230005250A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="130.89mm" wi="136.57mm" file="US20230005250A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="212.17mm" wi="145.54mm" file="US20230005250A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="239.52mm" wi="152.40mm" file="US20230005250A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This is a National Stage Entry into the United States Patent and Trademark Office from International Patent Application No. PCT/JP2021/005864, filed on Feb. 17, 2021, which claims priority to Japanese Patent Application No. 2020-026277, filed on Feb. 19, 2020, the entire contents of both of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE INVENTION</heading><p id="p-0003" num="0002">The present disclosure relates to a learning dataset generation device and a learning dataset generation method.</p><heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0004" num="0003">There is a known method of generating a dataset to be used in machine learning by acquiring distance images of a plurality of workpieces by means of a three-dimensional measurement machine, and by storing teaching data comprising the acquired distance images in association with a label map indicating a teaching position (for example, see Japanese Unexamined Patent Application, Publication No. 2019-58960).</p><p id="p-0005" num="0004">The method in Japanese Unexamined Patent Application, Publication No. 2019-58960 is used, for example, for generating a learned model that estimates a take-out position when a plurality of workpieces bulk-loaded in a container are taken out one by one with a hand attached to a robot. In order to generate a highly precise learned model, it is necessary to prepare a huge number of datasets. In other words, every time a dataset is generated, it is necessary to bulk-load the plurality of workpieces in a different form and acquire distance images by means of the three-dimensional measurement machine.</p><heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0006" num="0005">An aspect of the present disclosure is a learning dataset generation device including: a memory that stores three-dimensional CAD data of a workpiece and a container; and one or more processors including hardware, wherein the one or more processors are configured to use the three-dimensional CAD data of the workpiece and the container, stored in the memory, to generate, in a three-dimensional virtual space, a plurality of imaging objects in which a plurality of the workpieces are bulk-loaded in different forms inside the container, acquire a plurality of virtual distance images by measuring each of the generated imaging objects by means of a virtual three-dimensional measurement machine disposed in the three-dimensional virtual space, accept at least one teaching position for each of the acquired virtual distance images, and generate a learning dataset by associating the accepted teaching position with each of the virtual distance images.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an overall configuration diagram showing a robot system employing a learning dataset generation device and a learning dataset generation method according to an embodiment of the present disclosure.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram showing the learning dataset generation device in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart for explaining the learning dataset generation method using the learning dataset generation device in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart for explaining a modification of the learning dataset generation method in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENT(S) OF THE INVENTION</heading><p id="p-0011" num="0010">A learning dataset generation device <b>1</b> and a learning dataset generation method according to an embodiment of the present disclosure will be described below with reference to the drawings.</p><p id="p-0012" num="0011">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the learning dataset generation device <b>1</b> according to this embodiment is applied to a robot system <b>100</b> including: a robot <b>110</b> having a hand <b>111</b> at the distal end thereof; and a three-dimensional measurement machine <b>120</b> that is precisely positioned with respect to the robot <b>110</b> and that has a measurement range vertically therebelow.</p><p id="p-0013" num="0012">This robot system <b>100</b> uses a learned model that estimates a take-out position for the robot <b>110</b> to take out workpieces W one by one from a container X, which is disposed within the measurement range of the three-dimensional measurement machine <b>120</b> and in which a large number of workpieces W are accommodated in a bulk-loaded state.</p><p id="p-0014" num="0013">The learning dataset generation device <b>1</b> according to this embodiment is a device that generates a huge number of learning datasets to be used for generating the learned model.</p><p id="p-0015" num="0014">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the learning dataset generation device <b>1</b> is a computer including a memory <b>2</b>, a processor <b>3</b>, a monitor <b>4</b>, and a keyboard and a mouse serving as an input device <b>5</b>. The memory <b>2</b> stores three-dimensional CAD data of the workpieces W to be actually handled and three-dimensional CAD data of the container X to be actually used.</p><p id="p-0016" num="0015">The processor <b>3</b> generates a plurality of virtual imaging objects in which the plurality of workpieces W consisting of the three-dimensional CAD data are bulk-loaded in different forms inside the container X consisting of the three-dimensional CAD data stored in the memory <b>2</b>. By randomly changing the positions and orientations of the workpieces W consisting of the three-dimensional CAD data, it is possible to easily generate, in a short period of time, a plurality of virtual imaging objects in which the workpieces W are bulk-loaded in different forms.</p><p id="p-0017" num="0016">The processor <b>3</b> places each of the generated imaging objects consisting of the three-dimensional CAD data in a three-dimensional virtual space and acquires, by employing a publicly known method, distance images of the respective imaging objects by means of a virtual three-dimensional measurement machine installed in the same three-dimensional virtual space.</p><p id="p-0018" num="0017">Every time a distance image is acquired, the processor <b>3</b> displays the acquired distance image on the monitor <b>4</b> and allows a user to select a workpiece W that can be taken out by the robot <b>110</b>.</p><p id="p-0019" num="0018">In other words, the user who has visually confirmed the distance image displayed on the monitor <b>4</b> designates a workpiece W that can be taken out, by means of the mouse or the keyboard serving as the input device <b>5</b>. By doing so, the processor <b>3</b> accepts the positions (for example, the center-of-gravity positions) of one or more workpieces W designated by the user as one or more teaching positions for the robot <b>110</b> to take out those workpieces W.</p><p id="p-0020" num="0019">Then, the processor <b>3</b> associates the accepted teaching positions with the previously acquired distance image, thereby generating a learning dataset, and stores the learning dataset in the memory <b>2</b>. By repeating the same processing for each of the imaging objects, it is possible to easily generate a huge number of learning datasets in a short period of time.</p><p id="p-0021" num="0020">The learning dataset generation method using the thus-configured learning dataset generation device <b>1</b> according to this embodiment will be described below.</p><p id="p-0022" num="0021">First, a three-dimensional model of a virtual robot and a virtual three-dimensional measurement machine are installed in a three-dimensional virtual space defined by the processor <b>3</b> such that the positional relationship therebetween matches the positional relationship between the robot <b>110</b> and the three-dimensional measurement machine <b>120</b> installed in the three-dimensional real space.</p><p id="p-0023" num="0022">In addition, three-dimensional CAD data of workpieces W to be actually handled and three-dimensional CAD data of a container X for accommodating the workpieces W in a bulk-loaded state are stored in advance in the memory <b>2</b>.</p><p id="p-0024" num="0023">Next, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, bulk-load parameters are set (step S<b>1</b>). The parameters are, for example, the number of workpieces W to be accommodated in the container X and the positions and orientations of a plurality of workpieces W to be arranged on a bottom surface of the container X.</p><p id="p-0025" num="0024">Then, the plurality of workpieces W consisting of the three-dimensional CAD data are accommodated, in a bulk-loaded state, in the container X consisting of the three-dimensional CAD data by using the set parameters. By doing so, a virtual imaging object that consists of the three-dimensional CAD data in which the plurality of workpieces W are accommodated in a bulk-loaded state inside the container X (virtual imaging object) is generated (step S<b>2</b>).</p><p id="p-0026" num="0025">The generated virtual imaging object consisting of the three-dimensional CAD data is measured by means of the virtual three-dimensional measurement machine set in the three-dimensional virtual space, and a distance image of the virtual imaging object (virtual distance image) is acquired (step S<b>3</b>).</p><p id="p-0027" num="0026">Then, the acquired distance image is displayed on the monitor <b>4</b> (step S<b>4</b>), and with respect to the displayed distance image, a user is prompted to designate a workpiece W that can be taken out (step S<b>5</b>). By doing so, the center-of-gravity position of the designated workpiece W is accepted as a teaching position.</p><p id="p-0028" num="0027">It is determined whether or not the designation of workpieces W that can be taken out has been completed (step S<b>6</b>), and in the case in which the designation has not been completed, the designation of a workpiece W in step S<b>5</b> is repeated. In the case in which the designation has been completed, one or more accepted teaching positions are associated with the distance image, whereby a learning dataset is generated (step S<b>7</b>). In the case in which a learning dataset has been generated for one imaging object, the user is prompted to input whether or not to end the generation (step S<b>8</b>), and in the case in which the generation is not ended, the parameters are changed (step S<b>9</b>) and the process from step S<b>2</b> is repeated.</p><p id="p-0029" num="0028">With this configuration, there is an advantage in that it is possible to easily generate, in a short period of time, learning datasets for a large number of imaging objects in which the plurality of workpieces W are accommodated in the container X in a state in which the workpieces W are bulk-loaded in different forms. In other words, the user does not need to handle actual heavy workpieces W to bulk-load the workpieces W in a different form every time a learning dataset is generated, and thus, it is possible to reduce the burden on the user and also to reduce the time required for generating learning datasets.</p><p id="p-0030" num="0029">In addition, as a result of performing machine learning by using the large number of generated learning datasets, it is possible to generate a highly precise learned model.</p><p id="p-0031" num="0030">In the case in which a workpiece W is taken out from inside the container X by means of the robot <b>110</b>, the three-dimensional measurement machine <b>120</b> disposed in the three-dimensional real space acquires a distance image of the container X, which is disposed in the measurement range of the three-dimensional measurement machine <b>120</b> and in which actual workpieces W are accommodated in a bulk-loaded state. Then, as a result of inputting the acquired distance image to a learned model, it is possible to estimate a take-out position of at least one workpiece W that can be taken out by the robot <b>110</b>.</p><p id="p-0032" num="0031">Note that this embodiment has been described, assuming that the positional relationship between the virtual three-dimensional measurement machine and the container X that are disposed in the three-dimensional virtual space precisely matches the positional relationship between the three-dimensional measurement machine <b>120</b> and the container X that are disposed in the three-dimensional real space. However, it is difficult to precisely position the container X with respect to the three-dimensional measurement machine <b>120</b> in the three-dimensional real space.</p><p id="p-0033" num="0032">Accordingly, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the three-dimensional measurement machine <b>120</b> is fixed in the three-dimensional real space prior to generation of a learning dataset, and the container X is disposed within the measurement range of the three-dimensional measurement machine <b>120</b> to acquire an actual distance image of the container X (actual distance image) (step S<b>10</b>). The relative position between the three-dimensional measurement machine <b>120</b> and the container X (actual relative position) is calculated from the acquired actual distance image of the container X (step S<b>11</b>).</p><p id="p-0034" num="0033">Then, at least one of the virtual three-dimensional measurement machine and the container X is moved in the three-dimensional virtual space. By doing so, the relative position between the virtual three-dimensional measurement machine and the container X (virtual relative position) is matched with the actual relative position between the three-dimensional measurement machine <b>120</b> and the container X in the three-dimensional real space (step S<b>12</b>), and the process proceeds to step S<b>1</b>.</p><p id="p-0035" num="0034">By performing such processing prior to generation of a learning dataset, it is possible to approximate the virtual distance image, which is displayed on the monitor <b>4</b> to allow the user to designate a workpiece W that can be taken out, to the actual distance image acquired by means of the actual three-dimensional measurement machine <b>120</b>. Therefore, there is an advantage in that it is possible to enhance the precision of a teaching position associated with the virtual distance image and to generate a learning dataset capable of enhancing the precision of a take-out position estimated on the basis of the actual distance image.</p><heading id="h-0007" level="1">TECHNICAL FIELD</heading><p id="p-0036" num="0035">The present disclosure relates to a learning dataset generation device and a learning dataset generation method.</p><heading id="h-0008" level="1">BACKGROUND ART</heading><p id="p-0037" num="0036">There is a known method of generating a dataset to be used in machine learning by acquiring distance images of a plurality of workpieces by means of a three-dimensional measurement machine, and by storing teaching data comprising the acquired distance images in association with a label map indicating a teaching position (for example, see Patent Literature 1).</p><p id="p-0038" num="0037">The method in Patent Literature 1 is used, for example, for generating a learned model that estimates a take-out position when a plurality of workpieces bulk-loaded in a container are taken out one by one with a hand attached to a robot. In order to generate a highly precise learned model, it is necessary to prepare a huge number of datasets. In other words, every time a dataset is generated, it is necessary to bulk-load the plurality of workpieces in a different form and acquire distance images by means of the three-dimensional measurement machine.</p><heading id="h-0009" level="1">CITATION LIST</heading><heading id="h-0010" level="1">Patent Literature</heading><p id="p-0039" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0038">{PTL 1} Japanese Unexamined Patent Application, Publication No. 2019-58960</li></ul></p><heading id="h-0011" level="1">SUMMARY OF INVENTION</heading><heading id="h-0012" level="1">Technical Problem</heading><p id="p-0040" num="0039">Because each workpiece handled by a robot has a large weight, when performing the work of bulk-loading a plurality of workpieces in a different form every time each dataset is generated, large amounts of time and labor are required. Therefore, it is desired that a learning dataset be generated without performing the work of bulk-loading actual workpieces again.</p><heading id="h-0013" level="1">Solution to Problem</heading><p id="p-0041" num="0040">An aspect of the present disclosure is a learning dataset generation device including: a memory that stores three-dimensional CAD data of a workpiece and a container; and one or more processors including hardware, wherein the one or more processors are configured to use the three-dimensional CAD data of the workpiece and the container, stored in the memory, to generate, in a three-dimensional virtual space, a plurality of imaging objects in which a plurality of the workpieces are bulk-loaded in different forms inside the container, acquire a plurality of virtual distance images by measuring each of the generated imaging objects by means of a virtual three-dimensional measurement machine disposed in the three-dimensional virtual space, accept at least one teaching position for each of the acquired virtual distance images, and generate a learning dataset by associating the accepted teaching position with each of the virtual distance images.</p><heading id="h-0014" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an overall configuration diagram showing a robot system employing a learning dataset generation device and a learning dataset generation method according to an embodiment of the present disclosure.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram showing the learning dataset generation device in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart for explaining the learning dataset generation method using the learning dataset generation device in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart for explaining a modification of the learning dataset generation method in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><heading id="h-0015" level="1">DESCRIPTION OF EMBODIMENT</heading><p id="p-0046" num="0045">A learning dataset generation device <b>1</b> and a learning dataset generation method according to an embodiment of the present disclosure will be described below with reference to the drawings.</p><p id="p-0047" num="0046">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the learning dataset generation device <b>1</b> according to this embodiment is applied to a robot system <b>100</b> including: a robot <b>110</b> having a hand <b>111</b> at the distal end thereof; and a three-dimensional measurement machine <b>120</b> that is precisely positioned with respect to the robot <b>110</b> and that has a measurement range vertically therebelow.</p><p id="p-0048" num="0047">This robot system <b>100</b> uses a learned model that estimates a take-out position for the robot <b>110</b> to take out workpieces W one by one from a container X, which is disposed within the measurement range of the three-dimensional measurement machine <b>120</b> and in which a large number of workpieces W are accommodated in a bulk-loaded state.</p><p id="p-0049" num="0048">The learning dataset generation device <b>1</b> according to this embodiment is a device that generates a huge number of learning datasets to be used for generating the learned model.</p><p id="p-0050" num="0049">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the learning dataset generation device <b>1</b> is a computer including a memory <b>2</b>, a processor <b>3</b>, a monitor <b>4</b>, and a keyboard and a mouse serving as an input device <b>5</b>. The memory <b>2</b> stores three-dimensional CAD data of the workpieces W to be actually handled and three-dimensional CAD data of the container X to be actually used.</p><p id="p-0051" num="0050">The processor <b>3</b> generates a plurality of virtual imaging objects in which the plurality of workpieces W consisting of the three-dimensional CAD data are bulk-loaded in different forms inside the container X consisting of the three-dimensional CAD data stored in the memory <b>2</b>. By randomly changing the positions and orientations of the workpieces W consisting of the three-dimensional CAD data, it is possible to easily generate, in a short period of time, a plurality of virtual imaging objects in which the workpieces W are bulk-loaded in different forms.</p><p id="p-0052" num="0051">The processor <b>3</b> places each of the generated imaging objects consisting of the three-dimensional CAD data in a three-dimensional virtual space and acquires, by employing a publicly known method, distance images of the respective imaging objects by means of a virtual three-dimensional measurement machine installed in the same three-dimensional virtual space.</p><p id="p-0053" num="0052">Every time a distance image is acquired, the processor <b>3</b> displays the acquired distance image on the monitor <b>4</b> and allows a user to select a workpiece W that can be taken out by the robot <b>110</b>.</p><p id="p-0054" num="0053">In other words, the user who has visually confirmed the distance image displayed on the monitor <b>4</b> designates a workpiece W that can be taken out, by means of the mouse or the keyboard serving as the input device <b>5</b>. By doing so, the processor <b>3</b> accepts the positions (for example, the center-of-gravity positions) of one or more workpieces W designated by the user as one or more teaching positions for the robot <b>110</b> to take out those workpieces W.</p><p id="p-0055" num="0054">Then, the processor <b>3</b> associates the accepted teaching positions with the previously acquired distance image, thereby generating a learning dataset, and stores the learning dataset in the memory <b>2</b>. By repeating the same processing for each of the imaging objects, it is possible to easily generate a huge number of learning datasets in a short period of time.</p><p id="p-0056" num="0055">The learning dataset generation method using the thus-configured learning dataset generation device <b>1</b> according to this embodiment will be described below.</p><p id="p-0057" num="0056">First, a three-dimensional model of a virtual robot and a virtual three-dimensional measurement machine are installed in a three-dimensional virtual space defined by the processor <b>3</b> such that the positional relationship therebetween matches the positional relationship between the robot <b>110</b> and the three-dimensional measurement machine <b>120</b> installed in the three-dimensional real space.</p><p id="p-0058" num="0057">In addition, three-dimensional CAD data of workpieces W to be actually handled and three-dimensional CAD data of a container X for accommodating the workpieces W in a bulk-loaded state are stored in advance in the memory <b>2</b>.</p><p id="p-0059" num="0058">Next, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, bulk-load parameters are set (step S<b>1</b>). The parameters are, for example, the number of workpieces W to be accommodated in the container X and the positions and orientations of a plurality of workpieces W to be arranged on a bottom surface of the container X.</p><p id="p-0060" num="0059">Then, the plurality of workpieces W consisting of the three-dimensional CAD data are accommodated, in a bulk-loaded state, in the container X consisting of the three-dimensional CAD data by using the set parameters. By doing so, a virtual imaging object that consists of the three-dimensional CAD data in which the plurality of workpieces W are accommodated in a bulk-loaded state inside the container X (virtual imaging object) is generated (step S<b>2</b>).</p><p id="p-0061" num="0060">The generated virtual imaging object consisting of the three-dimensional CAD data is measured by means of the virtual three-dimensional measurement machine set in the three-dimensional virtual space, and a distance image of the virtual imaging object (virtual distance image) is acquired (step S<b>3</b>).</p><p id="p-0062" num="0061">Then, the acquired distance image is displayed on the monitor <b>4</b> (step S<b>4</b>), and with respect to the displayed distance image, a user is prompted to designate a workpiece W that can be taken out (step S<b>5</b>). By doing so, the center-of-gravity position of the designated workpiece W is accepted as a teaching position.</p><p id="p-0063" num="0062">It is determined whether or not the designation of workpieces W that can be taken out has been completed (step S<b>6</b>), and in the case in which the designation has not been completed, the designation of a workpiece W in step S<b>5</b> is repeated. In the case in which the designation has been completed, one or more accepted teaching positions are associated with the distance image, whereby a learning dataset is generated (step S<b>7</b>). In the case in which a learning dataset has been generated for one imaging object, the user is prompted to input whether or not to end the generation (step S<b>8</b>), and in the case in which the generation is not ended, the parameters are changed (step S<b>9</b>) and the process from step S<b>2</b> is repeated.</p><p id="p-0064" num="0063">With this configuration, there is an advantage in that it is possible to easily generate, in a short period of time, learning datasets for a large number of imaging objects in which the plurality of workpieces W are accommodated in the container X in a state in which the workpieces W are bulk-loaded in different forms. In other words, the user does not need to handle actual heavy workpieces W to bulk-load the workpieces W in a different form every time a learning dataset is generated, and thus, it is possible to reduce the burden on the user and also to reduce the time required for generating learning datasets.</p><p id="p-0065" num="0064">In addition, as a result of performing machine learning by using the large number of generated learning datasets, it is possible to generate a highly precise learned model.</p><p id="p-0066" num="0065">In the case in which a workpiece W is taken out from inside the container X by means of the robot <b>110</b>, the three-dimensional measurement machine <b>120</b> disposed in the three-dimensional real space acquires a distance image of the container X, which is disposed in the measurement range of the three-dimensional measurement machine <b>120</b> and in which actual workpieces W are accommodated in a bulk-loaded state. Then, as a result of inputting the acquired distance image to a learned model, it is possible to estimate a take-out position of at least one workpiece W that can be taken out by the robot <b>110</b>.</p><p id="p-0067" num="0066">Note that this embodiment has been described, assuming that the positional relationship between the virtual three-dimensional measurement machine and the container X that are disposed in the three-dimensional virtual space precisely matches the positional relationship between the three-dimensional measurement machine <b>120</b> and the container X that are disposed in the three-dimensional real space. However, it is difficult to precisely position the container X with respect to the three-dimensional measurement machine <b>120</b> in the three-dimensional real space.</p><p id="p-0068" num="0067">Accordingly, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the three-dimensional measurement machine <b>120</b> is fixed in the three-dimensional real space prior to generation of a learning dataset, and the container X is disposed within the measurement range of the three-dimensional measurement machine <b>120</b> to acquire an actual distance image of the container X (actual distance image) (step S<b>10</b>). The relative position between the three-dimensional measurement machine <b>120</b> and the container X (actual relative position) is calculated from the acquired actual distance image of the container X (step S<b>11</b>).</p><p id="p-0069" num="0068">Then, at least one of the virtual three-dimensional measurement machine and the container X is moved in the three-dimensional virtual space. By doing so, the relative position between the virtual three-dimensional measurement machine and the container X (virtual relative position) is matched with the actual relative position between the three-dimensional measurement machine <b>120</b> and the container X in the three-dimensional real space (step S<b>12</b>), and the process proceeds to step S<b>1</b>.</p><p id="p-0070" num="0069">By performing such processing prior to generation of a learning dataset, it is possible to approximate the virtual distance image, which is displayed on the monitor <b>4</b> to allow the user to designate a workpiece W that can be taken out, to the actual distance image acquired by means of the actual three-dimensional measurement machine <b>120</b>. Therefore, there is an advantage in that it is possible to enhance the precision of a teaching position associated with the virtual distance image and to generate a learning dataset capable of enhancing the precision of a take-out position estimated on the basis of the actual distance image.</p><heading id="h-0016" level="1">REFERENCE SIGNS LIST</heading><p id="p-0071" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0070"><b>1</b> learning dataset generation device</li>    <li id="ul0002-0002" num="0071"><b>2</b> memory</li>    <li id="ul0002-0003" num="0072"><b>3</b> processor</li>    <li id="ul0002-0004" num="0073"><b>120</b> three-dimensional measurement machine</li>    <li id="ul0002-0005" num="0074">X container</li>    <li id="ul0002-0006" num="0075">W workpiece</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A learning dataset generation device comprising:<claim-text>a memory that stores three-dimensional CAD data of a workpiece and a container; and</claim-text><claim-text>one or more processors comprising hardware,</claim-text><claim-text>wherein the one or more processors are configured to:<claim-text>use the three-dimensional CAD data of the workpiece and the container, stored in the memory, to generate, in a three-dimensional virtual space, a plurality of imaging objects in which a plurality of the workpieces are bulk-loaded in different forms inside the container,</claim-text><claim-text>acquire a plurality of virtual distance images by measuring each of the generated imaging objects by means of a virtual three-dimensional measurement machine disposed in the three-dimensional virtual space,</claim-text><claim-text>accept at least one teaching position for each of the acquired virtual distance images, and</claim-text><claim-text>generate a learning dataset by associating the accepted teaching position with each of the virtual distance images.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The learning dataset generation device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors are further configured to use a distance image of the container in a three-dimensional real space, which is acquired by means of a three-dimensional measurement machine installed in the three-dimensional real space, to set the position of the virtual three-dimensional measurement machine relative to the container in the three-dimensional virtual space to a position matching the arrangement of the three-dimensional measurement machine relative to the container.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. A learning dataset generation method comprising:<claim-text>using three-dimensional CAD data of a workpiece and a container to generate, in a three-dimensional virtual space, a plurality of imaging objects in which a plurality of the workpieces are bulk-loaded in different forms inside the container;</claim-text><claim-text>acquiring a plurality of virtual distance images by measuring each of the generated imaging objects by means of a virtual three-dimensional measurement machine disposed in the three-dimensional virtual space;</claim-text><claim-text>accepting at least one teaching position for each of the acquired virtual distance images; and</claim-text><claim-text>generating a learning dataset by associating the accepted teaching position with each of the virtual distance images.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The learning dataset generation method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising using a distance image of the container in a three-dimensional real space, which is acquired by means of a three-dimensional measurement machine installed in the three-dimensional real space, to set the position of the virtual three-dimensional measurement machine relative to the container in the three-dimensional virtual space to a position matching the arrangement of the three-dimensional measurement machine relative to the container.</claim-text></claim></claims></us-patent-application>