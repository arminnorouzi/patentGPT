<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005104A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005104</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17939614</doc-number><date>20220907</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>IN</country><doc-number>202041009901</doc-number><date>20200307</date></priority-claim><priority-claim sequence="02" kind="national"><country>IN</country><doc-number>202041009901</doc-number><date>20210119</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>4046</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>4053</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e79">METHOD AND ELECTRONIC DEVICE FOR PERFORMING AI BASED ZOOM OF IMAGE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/KR2021/002721</doc-number><date>20210305</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17939614</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SAMSUNG ELECTRONICS CO., LTD.</orgname><address><city>Suwon-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Pasupuleti</last-name><first-name>Sirish Kumar</first-name><address><city>Bengaluru</city><country>IN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Gadde</last-name><first-name>Raj Narayana</first-name><address><city>Bengaluru</city><country>IN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Akula</last-name><first-name>Sri Nitchith</first-name><address><city>Bengaluru</city><country>IN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Saha</last-name><first-name>Avinab</first-name><address><city>Bengaluru</city><country>IN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>SAMSUNG ELECTRONICS CO., LTD.</orgname><role>03</role><address><city>Suwon-si</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method and an electronic device for performing an AI based zoom of an image in an electronic device are provided. The method includes receiving the image; obtaining, through a pixel domain neural network (NN) block, a first set of feature maps of the image based on pixels of the image; obtaining, through a frequency domain NN block, a second set of feature maps of the image based on frequencies of the image; and obtaining, through a joint refinement NN block, a final image with a resolution higher than a resolution of the image, based on the first set of feature maps and the second set of feature maps.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="123.70mm" wi="142.16mm" file="US20230005104A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="162.39mm" wi="144.19mm" file="US20230005104A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="128.61mm" wi="134.62mm" file="US20230005104A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="155.70mm" wi="156.21mm" file="US20230005104A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="223.77mm" wi="163.32mm" orientation="landscape" file="US20230005104A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="153.50mm" wi="156.21mm" file="US20230005104A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="216.07mm" wi="154.01mm" orientation="landscape" file="US20230005104A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="222.59mm" wi="157.23mm" orientation="landscape" file="US20230005104A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="223.60mm" wi="156.97mm" orientation="landscape" file="US20230005104A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="138.35mm" wi="156.46mm" file="US20230005104A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="106.09mm" wi="114.05mm" file="US20230005104A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="158.83mm" wi="156.21mm" file="US20230005104A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="142.75mm" wi="155.96mm" file="US20230005104A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="134.45mm" wi="142.07mm" file="US20230005104A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application is a continuation application of International Application No. PCT/KR2021/002721, filed on Mar. 5, 2021, which is based on and claims priority to Indian Complete Patent Application No. 202041009901, filed on Jan. 19, 2021, and Indian Provisional Patent Application No. 202041009901, filed on Mar. 7, 2020, the disclosures of which are incorporated by reference herein in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">1. Field</heading><p id="p-0003" num="0002">The present disclosure relates to image processing, and more specifically to a method and an electronic device for performing an artificial intelligence (AI) based zoom of an image.</p><heading id="h-0004" level="1">2. Description of Related Art</heading><p id="p-0004" num="0003">A zooming feature of an imaging system such as a camera allows the imaging system to change smoothly from a long shot of a scene to a close-up shot (zooming in), and vice-versa (zooming out). In other words, zooming in decreases and/or narrows an apparent angle view of a digital photograph or video, and zooming out increases and/or widens an apparent angle view of the digital photograph or video. Digital zoom is a zooming technique that enables the imaging system to zoom in and zoom out of the scene without physically focusing the scene using lenses of the imaging system. In order to digitally zoom in to the scene, the imaging system enlarges an image of the scene or a portion of the image of the scene using conventional image processing techniques such as bilinear upscaling, bicubic upscaling, etc. Major shortcoming with the conventional technologies in zooming the image are a presence of artifacts such as jaggy edges, blurred details, overly smoothened edges, and washed-out appearances in a zoomed image. A Deep Neural Network (DNN) may be used as an alternative method for producing a high quality image for zooming. However, a computational complexity for producing the high quality images using the DNN is very high, e.g., more than 1 million operations per pixel, which is practically not feasible to implement in the imaging system with limited computational resources. Thus, it is desired to provide a useful alternative for digitally zooming the image.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0005" num="0004">Provided are a method, an electronic device, and a non-transitory computer readable medium including instructions for performing an AI based zoom of an image. The method may allow the electronic device to produce a high resolution image of the image without generating artifacts such as jaggy edges, blurred details, overly smoothened edges, and washed-out appearances, etc. in the high resolution image.</p><p id="p-0006" num="0005">The electronic device may include Edge Synthesis Blocks (ESBs) that may each include One Dimensional (1D) Convolution Neural Networks (CNNs) for performing an operation with lower complexity on the image or feature maps of the image. Therefore, the electronic device may easily perform operations for generating the high resolution image by using the ESBs, even with limited computational resources.</p><p id="p-0007" num="0006">The method may allow the electronic device to analyze features of the image in multiple domains (e.g., pixel domain and frequency domain), extract the features of the image in the multiple domains, and choose optimal features from the extracted features for generating the high resolution image. In response to zooming a portion of the image, the electronic device may display a corresponding portion of the high resolution image. The image details in the corresponding portion will be very clear compared to an output of conventional image processing techniques, which improves a user experience in zooming the image.</p><p id="p-0008" num="0007">According to an aspect of the disclosure, a method for performing an artificial intelligence (AI) based zoom of an image in an electronic device, includes: receiving the image; obtaining, through a pixel domain neural network (NN) block, a first set of feature maps of the image based on pixels of the image; obtaining, through a frequency domain NN block, a second set of feature maps of the image based on frequencies of the image; and obtaining, through a joint refinement NN block, a final image with a resolution higher than a resolution of the image, based on the first set of feature maps and the second set of feature maps.</p><p id="p-0009" num="0008">The joint refinement NN block may learn kernel weights during a training phase, and update the kernel weights based on image characteristics in the first set of feature maps and the second set of feature maps, and the joint refinement NN block may include a set of edge synthesis blocks (ESBs).</p><p id="p-0010" num="0009">The obtaining the first set of feature maps of the image may include: identifying a third set of feature maps of the image using a first set of edge synthesis blocks (ESBs) of the pixel domain NN block; extracting edges in the image; upscaling the edges to obtain a high resolution (HR) edge map; and obtaining the first set of feature maps of the image by filtering the third set of feature maps and the HR edge map using a second set of ESBs of the pixel domain NN block.</p><p id="p-0011" num="0010">The first set of ESBs of the pixel domain NN block may learn a horizontal edge feature and a vertical edge feature of the image in separate network branches with one-dimensional (1D) convolution neural networks (CNNs).</p><p id="p-0012" num="0011">The first set of ESBs of the pixel domain NN block may include a variable number of ESBs in cascaded arrangement with a local and global feature concatenation to learn edges in 360&#xb0; directions.</p><p id="p-0013" num="0012">The second set of ESBs of the pixel domain NN block may guide learned edge features in the third set of feature maps using the HR edge map to improve an edge consistency.</p><p id="p-0014" num="0013">The obtaining the second set of feature maps of the image may include: converting the image to frequency components; classifying the frequency components of the image to at least one set of frequency components based on a similarity of the frequency components; determining a fourth set of feature maps for each set of frequency components using a first set of edge synthesis blocks (ESBs) in the frequency domain NN block; obtaining a fifth set of feature maps by filtering the fourth set of feature maps and frequency components of the first set of feature maps using a second set of ESBs in the frequency domain NN block; and converting the fifth set of feature maps to the second set of feature maps based on pixels of the image.</p><p id="p-0015" num="0014">The frequency components may be classified to enable a deep learning model for learning in the frequency domain and extracting important features in local neighborhood characteristics.</p><p id="p-0016" num="0015">The first set of ESBs of the frequency domain NN block may learn a local neighborhood frequency characteristics to reduce noise artefacts.</p><p id="p-0017" num="0016">Each of the ESBs may include a plurality of cascaded one-dimensional (1D) convolution NNs neural networks (CNNs), a plurality of depth-wise separable CNNs, and a concatenate layer.</p><p id="p-0018" num="0017">Outputs of each set of cascaded 1D CNNs may be input to corresponding depth-wise separable CNNs, and outputs of the corresponding depth-wise separable CNNs may be input to a final depth-wise separable CNN after performing concatenation on the outputs of the corresponding depth-wise separable CNNs using the concatenate layer.</p><p id="p-0019" num="0018">According to an aspect of the disclosure, an electronic device for performing an AI based zoom of an image, includes: a memory; a processor; and an image zoom controller operably coupled to the memory and the processor, and configured to: receive the image, obtain, through a pixel domain neural network (NN) block, a first set of feature maps of the image based on pixels of the image, obtain, through a frequency domain NN block, a second set of feature maps of the image based on frequencies of the image, and obtain, through a joint refinement NN block, a final image with a resolution higher than the image, based on the first set of feature maps and the second set of feature maps.</p><p id="p-0020" num="0019">The joint refinement NN block may learn kernel weights during a training phase, and update the kernel weights based on image characteristics in the first set of feature maps and the second set of feature maps, wherein the joint refinement NN block may include a set of edge synthesis blocks (ESBs).</p><p id="p-0021" num="0020">The image zoom controller may be further configured to: obtain a third set of feature maps of the image using a first set of edge synthesis blocks (ESBs) of the pixel domain NN block; extract edges in the image; upscale the edges to obtain a high resolution (HR) edge map; and obtain the first set of feature maps of the image by filtering the third set of feature maps and the HR edge map using a second set of ESBs of the pixel domain NN block.</p><p id="p-0022" num="0021">The image zoom controller may be further configured to: convert the image to frequency components; classify the frequency components of the image to at least one set of frequency components based on a similarity of the frequency components; obtain a fourth set of feature maps for each set of frequency components using a first set of edge synthesis blocks (ESBs) in the frequency domain NN block; obtain a fifth set of feature maps by filtering the fourth set of feature maps and frequency components of the first set of feature maps using a second set of ESBs in the frequency domain NN block; and convert the fifth set of feature maps to the second set of feature maps based on pixels of the image According to an aspect of the disclosure, a method for performing an artificial intelligence (AI) based zoom of an image in an electronic device, includes: receiving the image; obtaining, through a pixel domain neural network (NN) block, a first set of feature maps of the image based on pixels of the image; obtaining, through a frequency domain NN block, a second set of feature maps of the image based on frequencies of the image; and obtaining, through a joint refinement NN block, a final image with a resolution higher than a resolution of the image, based on the first set of feature maps and the second set of feature maps.</p><p id="p-0023" num="0022">The joint refinement NN block may learn kernel weights during a training phase, and update the kernel weights based on image characteristics in the first set of feature maps and the second set of feature maps, and the joint refinement NN block may include a set of edge synthesis blocks (ESBs).</p><p id="p-0024" num="0023">The obtaining the first set of feature maps of the image may include: identifying a third set of feature maps of the image using a first set of edge synthesis blocks (ESBs) of the pixel domain NN block; extracting edges in the image; upscaling the edges to obtain a high resolution (HR) edge map; and obtaining the first set of feature maps of the image by filtering the third set of feature maps and the HR edge map using a second set of ESBs of the pixel domain NN block.</p><p id="p-0025" num="0024">The first set of ESBs of the pixel domain NN block may learn a horizontal edge feature and a vertical edge feature of the image in separate network branches with one-dimensional (1D) convolution neural networks (CNNs).</p><p id="p-0026" num="0025">The first set of ESBs of the pixel domain NN block may include a variable number of ESBs in cascaded arrangement with a local and global feature concatenation to learn edges in 360&#xb0; directions.</p><p id="p-0027" num="0026">The second set of ESBs of the pixel domain NN block may guide learned edge features in the third set of feature maps using the HR edge map to improve an edge consistency.</p><p id="p-0028" num="0027">The obtaining the second set of feature maps of the image may include: converting the image to frequency components; classifying the frequency components of the image to at least one set of frequency components based on a similarity of the frequency components; determining a fourth set of feature maps for each set of frequency components using a first set of edge synthesis blocks (ESBs) in the frequency domain NN block; obtaining a fifth set of feature maps by filtering the fourth set of feature maps and frequency components of the first set of feature maps using a second set of ESBs in the frequency domain NN block; and converting the fifth set of feature maps to the second set of feature maps based on pixels of the image.</p><p id="p-0029" num="0028">The frequency components may be classified to enable a deep learning model for learning in the frequency domain and extracting important features in local neighborhood characteristics.</p><p id="p-0030" num="0029">The first set of ESBs of the frequency domain NN block may learn a local neighborhood frequency characteristics to reduce noise artefacts.</p><p id="p-0031" num="0030">Each of the ESBs may include a plurality of cascaded one-dimensional (1D) convolution NNs neural networks (CNNs), a plurality of depth-wise separable CNNs, and a concatenate layer.</p><p id="p-0032" num="0031">Outputs of each set of cascaded 1D CNNs may be input to corresponding depth-wise separable CNNs, and outputs of the corresponding depth-wise separable CNNs may be input to a final depth-wise separable CNN after performing concatenation on the outputs of the corresponding depth-wise separable CNNs using the concatenate layer.</p><p id="p-0033" num="0032">According to an aspect of the disclosure, an electronic device for performing an AI based zoom of an image, includes: a memory; a processor; and an image zoom controller operably coupled to the memory and the processor, and configured to: receive the image, obtain, through a pixel domain neural network (NN) block, a first set of feature maps of the image based on pixels of the image, obtain, through a frequency domain NN block, a second set of feature maps of the image based on frequencies of the image, and obtain, through a joint refinement NN block, a final image with a resolution higher than the image, based on the first set of feature maps and the second set of feature maps.</p><p id="p-0034" num="0033">The joint refinement NN block may learn kernel weights during a training phase, and update the kernel weights based on image characteristics in the first set of feature maps and the second set of feature maps, wherein the joint refinement NN block may include a set of edge synthesis blocks (ESBs).</p><p id="p-0035" num="0034">The image zoom controller may be further configured to: obtain a third set of feature maps of the image using a first set of edge synthesis blocks (ESBs) of the pixel domain NN block; extract edges in the image; upscale the edges to obtain a high resolution (HR) edge map; and obtain the first set of feature maps of the image by filtering the third set of feature maps and the HR edge map using a second set of ESBs of the pixel domain NN block.</p><p id="p-0036" num="0035">The image zoom controller may be further configured to: convert the image to frequency components; classify the frequency components of the image to at least one set of frequency components based on a similarity of the frequency components; obtain a fourth set of feature maps for each set of frequency components using a first set of edge synthesis blocks (ESBs) in the frequency domain NN block; obtain a fifth set of feature maps by filtering the fourth set of feature maps and frequency components of the first set of feature maps using a second set of ESBs in the frequency domain NN block; and convert the fifth set of feature maps to the second set of feature maps based on pixels of the image.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0037" num="0036">The above and other aspects, features, and advantages of certain embodiments of the present disclosure will be more apparent from the following description taken in conjunction with the accompanying drawings, in which:</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of an electronic device for performing an AI based zoom of an image, according to an embodiment;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a flow diagram of a method for performing an AI based zoom of an image, according to an embodiment;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an overview of a method for learning weight adjustments for a CNN to generate a high resolution image from a low resolution image, according to an embodiment;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an architectural diagram of an image zoom controller for generating a high resolution image from a low resolution image, according to an embodiment;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an architectural diagram of a CNN, according to an embodiment;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an architectural diagram of an ESB for generating a feature map, according to an embodiment;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrate a flow diagram of a method for creating a second set of feature maps of a low resolution image by modifying the low resolution image using a frequency domain NN block, according to an embodiment;</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a receptive field of a CNN with multiple layers, according to an embodiment;</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>9</b>B</figref> illustrate a comparison of complexity of computations of a two dimensional convolution layer and an ESB for generating an output feature map, according to an embodiment; and</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrate a comparison of complexity of computations of a depth-wise separable convolution layer and an ESB for generating an output feature map, according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0048" num="0047">The embodiments herein and the various features and advantageous details thereof are explained more fully with reference to the non-limiting embodiments that are illustrated in the accompanying drawings and detailed in the following description. Descriptions of well-known components and processing techniques are omitted so as to not unnecessarily obscure the embodiments herein. Also, the various embodiments described herein are not necessarily mutually exclusive, as some embodiments may be combined with one or more other embodiments to form new embodiments. The term &#x201c;or&#x201d; as used herein, refers to a non-exclusive or, unless otherwise indicated. The examples used herein are intended merely to facilitate an understanding of ways in which the embodiments herein may be practiced and to further enable those skilled in the art to practice the embodiments herein. Accordingly, the examples should not be construed as limiting the scope of the embodiments herein.</p><p id="p-0049" num="0048">As is traditional in the field, embodiments may be described and illustrated in terms of blocks which carry out a described function or functions. These blocks, which may be referred to herein as managers, units, modules, hardware components or the like, are physically implemented by analog and/or digital circuits such as logic gates, integrated circuits, microprocessors, microcontrollers, memory circuits, passive electronic components, active electronic components, optical components, hardwired circuits and the like, and may optionally be driven by firmware. The circuits may, for example, be embodied in one or more semiconductor chips, or on substrate supports such as printed circuit boards and the like. The circuits constituting a block may be implemented by dedicated hardware, or by a processor (e.g., one or more programmed microprocessors and associated circuitry), or by a combination of dedicated hardware to perform some functions of the block and a processor to perform other functions of the block. Each block of the embodiments may be physically separated into two or more interacting and discrete blocks without departing from the scope of the disclosure. Likewise, the blocks of the embodiments may be physically combined into more complex blocks without departing from the scope of the disclosure.</p><p id="p-0050" num="0049">Accordingly, the embodiments herein provide a method for performing an AI based zoom of an image (e.g., a low resolution (LR) image) in an electronic device. The method includes receiving, by the electronic device, the image. The method includes modifying, by the electronic device, the image using a pixel domain neural network (NN) block to create a first set of feature maps of the image. The method includes modifying, by the electronic device, the image using a frequency domain NN block to create a second set of feature maps of the image. The method includes generating, by the electronic device, the first set of feature maps and the second set of feature maps using edge synthesis blocks (ESBs) at a lower computational complexity. The method includes generating, by the electronic device, a final image (e.g., a high resolution (HR) image) with a resolution higher than the image by passing the first set of feature maps and the second set of feature maps to a joint refinement NN block.</p><p id="p-0051" num="0050">Accordingly, the embodiments herein provide the electronic device for performing the AI based zoom of the image. The electronic device includes an image zoom controller, a memory, a processor, where the image zoom controller is coupled to the memory and the processor. The image zoom controller is configured to receive the image. The image zoom controller is configured to modify the image using the pixel domain NN block to create the first set of feature maps of the image. The image zoom controller is configured to modify the image using the frequency domain NN block to create the second set of feature maps of the image. The image zoom controller is configured to generate the first set of feature maps and the second set of feature maps using the ESBs at the lower computational complexity. The image zoom controller is configured to generate the final image with the resolution higher than the image by passing the first set of feature maps and the second set of feature maps to the joint refinement NN block.</p><p id="p-0052" num="0051">According to an aspect of the present disclosure, the electronic device may produce a high resolution image of a low resolution image without generating artifacts such as jaggy edges, blurred details, overly smoothened edges, and washed-out appearances, etc. in the high resolution image.</p><p id="p-0053" num="0052">According to an aspect of the present disclosure, the electronic device may include the ESBs for performing image processing with lower complexity using the 1D CNNs to zoom the image. Therefore, the electronic device with limited computational resources may easily perform operations for generating the high resolution image.</p><p id="p-0054" num="0053">According to an aspect of the present disclosure, the electronic device analyzes features of the image in multiple domains, e.g., pixel domain and frequency domain. The method allows the electronic device to extract the features of the image in the multiple domains and choose optimal features from the extracted features for generating the high resolution image. In response to zooming a portion of the image, the electronic device displays a corresponding portion of the high resolution image. The image details in the corresponding portion of the high resolution image generated by the electronic device will be very clear compared to an output of conventional image processing techniques, which improves a user experience in zooming the image.</p><p id="p-0055" num="0054">Hereinafter, embodiments of the present disclosure will be described in detail with reference to the accompanying drawings.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of an electronic device (<b>100</b>) for performing an AI based zoom of an image (e.g., LR image <b>303</b>), according to an embodiment. Examples of the electronic device (<b>100</b>) may include, but are not limited to, a smart phone, a tablet computer, a personal digital assistance (PDA), a desktop computer, a wearable device, a camera, a projector, an Internet of Things (IoT) device, etc. In an embodiment, the electronic device (<b>100</b>) includes an image zoom controller (<b>110</b>), a memory (<b>120</b>), a processor (<b>130</b>), and a communicator (<b>140</b>). The image zoom controller (<b>110</b>) is implemented by processing circuitry such as logic gates, integrated circuits, microprocessors, microcontrollers, memory circuits, passive electronic components, active electronic components, optical components, hardwired circuits, or the like, and may optionally be driven by a firmware. The circuits may, for example, be embodied in one or more semiconductor chips, or on substrate supports such as printed circuit boards and the like.</p><p id="p-0057" num="0056">In an embodiment, the image zoom controller (<b>110</b>) includes a pixel domain NN block (<b>111</b>), a frequency domain NN block (<b>112</b>), and a joint refinement NN block (<b>113</b>). The pixel domain NN block (<b>111</b>), the frequency domain NN block (<b>112</b>), and the joint refinement NN block (<b>113</b>) are implemented by processing circuitry such as logic gates, integrated circuits, microprocessors, microcontrollers, memory circuits, passive electronic components, active electronic components, optical components, hardwired circuits, or the like, and may optionally be driven by a firmware. The circuits may, for example, be embodied in one or more semiconductor chips, or on substrate supports such as printed circuit boards and the like.</p><p id="p-0058" num="0057">The pixel domain NN block (<b>111</b>) refers to a block for obtaining a first set of feature maps of the image based on pixels of the image. The frequency domain NN block (<b>112</b>) refers to a block for obtaining a second set of feature maps of the image based on frequencies of the image. The joint refinement NN block (<b>113</b>) refers to a block for generating a final image (e.g., HR image <b>305</b>) with a resolution higher than the image based on the first set of feature maps and the second set of feature maps. In the disclosure, the pixel domain NN block (<b>111</b>), the frequency domain NN block (<b>112</b>) and the joint refinement NN block (<b>113</b>) may be referred to as &#x2018;a first NN block&#x2019;, &#x2018;a second NN block&#x2019;, and &#x2018;a third NN block&#x2019;, respectively.</p><p id="p-0059" num="0058">The image zoom controller (<b>110</b>) is configured to receive the image (<b>303</b>). In an embodiment, the image zoom controller (<b>110</b>) is configured to receive the image (<b>303</b>) from an imaging sensor (e.g. complementary metal oxide Semiconductor (CMOS) imaging sensor) of the electronic device (<b>100</b>) in real time. In another embodiment, the image zoom controller (<b>110</b>) is configured to receive the image (<b>303</b>) from the memory (<b>120</b>). The image zoom controller (<b>110</b>) is configured to modify the image (<b>303</b>) using the pixel domain NN block (<b>111</b>) (refer to <figref idref="DRAWINGS">FIG. <b>4</b></figref> for more details) to create a first set of feature maps of the image (<b>303</b>). The image zoom controller (<b>110</b>) is configured to determine a third set of feature maps of the image (<b>303</b>) using a first set of ESBs (<b>401</b>-<b>403</b>) of the pixel domain NN block (<b>111</b>). In an embodiment, the first set of ESBs (<b>401</b>-<b>403</b>) of the pixel domain NN block (<b>111</b>) explicitly learns a horizontal edge feature and a vertical edge feature of the image (<b>303</b>) in separate network branches (<b>610</b>, <b>611</b>) with 1D CNNs (<b>601</b>-<b>604</b>) (refer to <figref idref="DRAWINGS">FIG. <b>6</b></figref> for more details).</p><p id="p-0060" num="0059">In the disclosure, an ESB refers to a block for extracting and classifying various edge features of an image for synthesizing a high resolution image according to various embodiments of the disclosure. The electronic device (<b>100</b>) may comprise a first set of ESBs (<b>401</b>-<b>403</b>) (a first set of ESBs in the pixel domain NN block (<b>111</b>)), a second set of ESBs (<b>404</b>-<b>406</b>) (a second set of ESBs in the pixel domain NN block (<b>111</b>)), a third set of ESBs (<b>407</b>-<b>409</b>) (a first set of ESBs in the frequency domain NN block (<b>112</b>)), a fourth set of ESBs (<b>410</b>-<b>412</b>) (a second set of ESBs in the frequency domain NN block (<b>112</b>)), and a fifth set of ESBs (<b>413</b>-<b>415</b>) (a set of ESBs in the joint refinement NN block (<b>113</b>)). Each set of ESBs will be described in detail in the description of each block including each set of ESBs. The ESBs may comprise One Dimensional (1D) Convolution Neural Networks (CNNs) configured to obtain output data using input data and a 1D kernel.</p><p id="p-0061" num="0060">The image zoom controller (<b>110</b>) is configured to extract edges in the image (<b>303</b>). The image zoom controller (<b>110</b>) is configured to upscale the edges to generate a HR edge map. The image zoom controller (<b>110</b>) is configured to generate the first set of feature maps of the image (<b>303</b>) by filtering the third set of feature maps and the HR edge map using the second set of ESBs (<b>404</b>-<b>406</b>) of the pixel domain NN block (<b>111</b>). In an embodiment, the second set of ESBs (<b>404</b>-<b>406</b>) of the pixel domain NN block (<b>111</b>) guides learned edge features in the third set of feature maps using the HR edge map to improve an edge consistency. The HR edge map helps the learning process by providing explicit edge details present in an input LR image, so that the learned features may not miss these important features present in the current input image while learning various other features learned over a training dataset. Hence, the HR edge map generated from the LR image acts as a guide to the learning process to make use of these and enhance the learning process.</p><p id="p-0062" num="0061">The image zoom controller (<b>110</b>) is configured to modify the image (<b>303</b>) using the frequency domain NN block (<b>112</b>) to create a second set of feature maps of the image (<b>303</b>). In an embodiment, the image zoom controller (<b>110</b>) is configured to convert the image (<b>303</b>) to frequency components. In an embodiment, the frequency components are classified to enable a deep learning model for learning in the frequency domain and extracting important features adaptively based on the local neighborhood characteristics. A pixel to frequency domain conversion using a block based transform will result in various frequency components arranged in a Two Dimensional (2D) fashion for 2D input images. Each adjacent frequency coefficient represents a different frequency component extracted from the pixel domain block. Processing these dissimilar frequency components spatially together may limit the feature extraction ability and thereby limit the learning process, particularly for NNs with less complexity/parameters. To enhance the learning capability, the frequency coefficients are rearranged such that similar frequency coefficients are spatially grouped together.</p><p id="p-0063" num="0062">An image may have different regions containing various characteristics such as plain, texture, etc., and the proposed frequency domain processing may enhance the NN ability to extract important features adaptive to the characteristics present in a local spatial region of the image. The image zoom controller (<b>110</b>) is configured to classify the frequency components of the image (<b>303</b>) to different set of frequency components based on a similarity of the frequency components. The image zoom controller (<b>110</b>) is configured to determine a fourth set of feature maps for each set of frequency components using a first set of ESBs (<b>407</b>-<b>409</b>) in the frequency domain NN block (<b>112</b>). In an embodiment, the first set of ESBs (<b>407</b>-<b>409</b>) of the frequency domain NN block (<b>112</b>) learns a local neighborhood frequency characteristics to reduce noise artefacts.</p><p id="p-0064" num="0063">The image zoom controller (<b>110</b>) is configured to generate a fifth set of feature maps by filtering the fourth set of feature maps and frequency components of the first set of feature maps using a second set of ESBs (<b>410</b>-<b>412</b>) in the frequency domain NN block (<b>112</b>). The image zoom controller (<b>110</b>) is configured to convert the fifth set of feature maps to the second set of feature maps comprising pixels. The image zoom controller (<b>110</b>) is configured to generate a final image (<b>305</b>) with a resolution higher than the image (<b>303</b>) by passing the first set of feature maps and the second set of feature maps to the joint refinement NN block (<b>113</b>). In an embodiment, the joint refinement NN block (<b>113</b>) learns kernel weights during a training phase, and updates the kernel weight based on a best feature in the first set of feature maps and the second set of feature maps, wherein the joint refinement NN block (<b>113</b>) comprises a set of ESBs (<b>413</b>-<b>415</b>). In the disclosure, best feature refers to important image characteristics which are present in the ground truth images and are learned in training phase via supervised learning manner using an objective loss function. An objective loss function measures the error between CNN output and ground truth, wherein, the objective of supervised training is to minimize the loss function and thereby the error. The image zoom controller (<b>110</b>) is configured to receive an input to zoom a portion of the image (<b>303</b>). The image zoom controller (<b>110</b>) is configured to display a portion of the final image (<b>305</b>) corresponds to the portion of the image (<b>303</b>). In an example, a display such as a Light Emitting Diode (LED) screen may be used to display the portion of the final image (<b>305</b>) by including the display to the electronic device (<b>100</b>). When the electronic device (<b>100</b>) is other than the projector, the projector may be used to display the portion of the final image (<b>305</b>) by connecting the projector to the electronic device (<b>100</b>). In an embodiment, the first set of ESBs (<b>401</b>-<b>403</b>), the second set of ESBs (<b>404</b>-<b>406</b>), the first set of ESBs (<b>407</b>-<b>409</b>), the second set of ESBs (<b>410</b>-<b>412</b>), and the set of ESBs (<b>413</b>-<b>415</b>) include a variable number of ESBs in cascaded arrangement with a local and global feature concatenation to learn edges in 360&#xb0; direction (refer to <figref idref="DRAWINGS">FIG. <b>4</b></figref> for more details). Each ESB (<b>401</b>-<b>415</b>) explicitly learns a horizontal edge feature and a vertical edge feature of the image (<b>303</b>) or an input feature map in separate network branches (<b>610</b>, <b>611</b>) with the 1D CNNs (<b>601</b>-<b>604</b>) (refer to <figref idref="DRAWINGS">FIG. <b>6</b></figref> for more details).</p><p id="p-0065" num="0064">A function associated with ESB (<b>401</b>-<b>415</b>) may be performed through the memory (<b>120</b>), and the processor (<b>130</b>). The processor (<b>130</b>) is configured to execute instructions stored in the memory (<b>120</b>). The processor (<b>130</b>) may include multiple cores to execute the instructions. The processor (<b>130</b>) may include one or a plurality of processors. At this time, one or a plurality of processors may be a general-purpose processor, such as a Central Processing Unit (CPU), an Application Processor (AP), or the like, a graphics-only processing unit such as a Graphics Processing Unit (GPU), a Visual Processing Unit (VPU), and/or an AI-dedicated processor such as a Neural Processing Unit (NPU).</p><p id="p-0066" num="0065">In an embodiment, the one or a plurality of processors control processing of the input data in accordance with a predefined operating rule or ESB (<b>401</b>-<b>415</b>) stored in the memory (<b>120</b>). The predefined operating rule or ESB (<b>401</b>-<b>415</b>) is provided through training or learning.</p><p id="p-0067" num="0066">Here, being provided through learning means that, by applying a learning method to a plurality of learning data, a predefined operating rule or ESB (<b>401</b>-<b>415</b>) of a desired characteristic is made. The learning may be performed in the electronic device (<b>100</b>) itself in which the ESB (<b>401</b>-<b>415</b>) according to an embodiment is performed, and/or may be implemented through a separate server/system. The learning method is a method for training a predetermined target device (for example, a robot). The electronic device (<b>100</b>) uses a plurality of learning data to cause, allow, or control the target device to make a determination or prediction. Examples of learning methods include, but are not limited to, supervised learning, unsupervised learning, semi-supervised learning, or reinforcement learning.</p><p id="p-0068" num="0067">The ESB (<b>401</b>-<b>415</b>) may consist of a plurality of neural network layers. Each layer has a plurality of weight values and performs a layer operation through calculation of a previous layer and an operation of a plurality of weights. Examples of neural networks include, but are not limited to, Convolutional Neural Network (CNN), Deep Neural Network (DNN), Recurrent Neural Network (RNN), Restricted Boltzmann Machine (RBM), Deep Belief Network (DBN), Bidirectional Recurrent Deep Neural Network (BRDNN), Generative Adversarial Networks (GAN), and deep Q-networks.</p><p id="p-0069" num="0068">The memory (<b>120</b>) stores the final image (<b>305</b>) and the image (<b>303</b>). The memory (<b>120</b>) may include non-volatile storage elements. Examples of such non-volatile storage elements may include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of an Electrically Programmable Memory (EPROM) or an Electrically Erasable and Programmable Memory (EEPROM). In addition, the memory (<b>120</b>) may, in some examples, be a non-transitory storage medium. The term &#x201c;non-transitory&#x201d; indicates that the storage medium is not embodied in a carrier wave or a propagated signal. However, the term &#x201c;non-transitory&#x201d; should not be interpreted that the memory (<b>120</b>) is non-movable. In certain examples, a non-transitory storage medium may store data that can, over time, change (e.g., in Random Access Memory (RAM) or cache).</p><p id="p-0070" num="0069">The communicator (<b>140</b>) is configured to communicate internally between hardware components in the electronic device (<b>100</b>). Further, the communicator (<b>140</b>) is configured to facilitate the communication between the electronic device (<b>100</b>) and other devices. The communicator (<b>140</b>) includes an electronic circuit specific to a standard that enables wired or wireless communication.</p><p id="p-0071" num="0070">Although the <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows the hardware components of the electronic device (<b>100</b>) but it is to be understood that other embodiments are not limited thereon. In other embodiments, the electronic device (<b>100</b>) may include less or more number of components. Further, the labels or names of the components are used only for illustrative purpose and does not limit the scope of the invention. One or more components may be combined together to perform same or substantially similar function for performing the AI based zoom of the image (<b>303</b>).</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a flow diagram (<b>200</b>) of a method for performing an AI based zoom of an image (e.g., LR image <b>303</b>), according to an embodiment. At step <b>201</b>, the method includes receiving the image (<b>303</b>). In an embodiment, the method allows the pixel domain NN block (<b>111</b>) and the frequency domain NN block (<b>112</b>) to receive the image (<b>303</b>). At step <b>202</b>, the method includes obtaining, through the pixel domain NN block (<b>111</b>), the first set of feature maps of the image (<b>303</b>) based on pixels of the image. In other words, the electronic device (<b>100</b>) may modify the image (<b>303</b>) using the pixel domain NN block (<b>111</b>) to create the first set of feature maps of the image (<b>303</b>). At step <b>203</b>, the method includes obtaining, through the frequency domain NN block (<b>112</b>), the second set of feature maps of the image (<b>303</b>) based on frequencies of the image. In other words, the electronic device (<b>100</b>) may modify the image (<b>303</b>) using the frequency domain NN block (<b>112</b>) to create the second set of feature maps of the image. At step <b>204</b>, the method includes generating, through the joint refinement NN block (<b>113</b>), a final image (e.g., HR image <b>305</b>) with a resolution higher than the image (<b>303</b>) based on the first set of feature maps and the second set of feature maps. In other words, the electronic device (<b>100</b>) may generate the final image (<b>305</b>) with the resolution higher than the image (<b>303</b>) by passing the first set of feature maps and the second set of feature maps to the joint refinement NN block (<b>113</b>).</p><p id="p-0073" num="0072">The various actions, acts, blocks, steps, or the like in the flow diagram <b>200</b> may be performed in the order presented, in a different order or simultaneously. Further, in some embodiments, some of the actions, acts, blocks, steps, or the like may be omitted, added, modified, skipped, or the like without departing from the scope of the invention.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an overview of a method for learning weight adjustments for a CNN (<b>304</b>) to generate a high resolution image (e.g., HR image <b>305</b>) from a low resolution image (e.g., LR image <b>303</b>), according to an embodiment. A resolution of the LR image (<b>303</b>) is lower than a resolution of the HR image (<b>305</b>). The CNN (<b>304</b>) may refer to any one of the 1D CNNs (<b>601</b>-<b>604</b>) of each ESB (<b>401</b>-<b>415</b>).</p><p id="p-0075" num="0074">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the electronic device (<b>100</b>) receives a HR ground truth image (<b>301</b>). At step <b>302</b>, the electronic device (<b>100</b>) downscales and degrades the HR ground truth image (<b>301</b>) to generate the LR image (<b>303</b>). The electronic device (<b>100</b>) initially generates the HR image (<b>305</b>) based on the LR image (<b>303</b>) using the CNN (<b>304</b>) with default weights. At step <b>306</b>, the electronic device (<b>100</b>) determines a loss (e.g., pixel loss) in a quality occurred in the HR image (<b>305</b>) compared to the HR ground truth image (<b>301</b>). At step <b>307</b>, the electronic device (<b>100</b>) learns new weights and modifies the default weights of the CNN (<b>304</b>) to a different value. Further, the electronic device (<b>100</b>) repeats steps of generating the HR image (<b>305</b>) using the CNN (<b>304</b>) with the modified weights, determining the loss in a quality, and modifying the weights for learning optimal weights to produce the HR image (<b>305</b>) without the loss compared to the HR ground truth image (<b>301</b>). The method ends in response to determining by the electronic device (<b>100</b>) the optimal weights of the CNN for generating the HR image (<b>305</b>) without the loss compared to the HR ground truth image (<b>301</b>).</p><p id="p-0076" num="0075">According to an aspect of the present disclosure, the learned kernel of the CNN (<b>304</b>) may be used to extract wide range of edge features. The learned kernel is robust and invariant of image rotation, scale, brightness changes, etc. The learned kernel achieves high quality images for wide range of input images with low quality. The electronic device (<b>100</b>) learns the kernel of the CNN (<b>304</b>) by training over a training dataset (e.g., a plurality of different HR ground truth images) against a loss function, and updating the weights using backpropagation.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an architectural diagram of an image zoom controller (<b>110</b>) for generating a high resolution image (e.g., HR image <b>305</b>) from a low resolution image (e.g., LR image <b>303</b>), according to an embodiment. In another embodiment, the image zoom controller (<b>110</b>) includes the pixel domain NN block (<b>111</b>), the frequency domain NN block (<b>112</b>), the joint refinement NN block (<b>113</b>), and an upscaler (<b>427</b>). The pixel domain NN block (<b>111</b>) includes a CNN with cascaded ESBs (<b>416</b>), an edge detector (<b>417</b>), an edge upscaler (<b>418</b>), an edge guidance CNN (<b>419</b>), and a pixel to frequency transformer (<b>420</b>). The CNN with cascaded ESBs (<b>416</b>) includes the first set of ESBs (<b>401</b>-<b>403</b>). The CNN with cascaded ESBs (<b>416</b>) receives the LR image (<b>303</b>) and generates the third set of feature maps of the LR image (<b>303</b>). Further, the CNN with cascaded ESBs (<b>416</b>) provides the third set of feature maps to the edge guidance CNN (<b>419</b>).</p><p id="p-0078" num="0077">The edge detector (<b>417</b>) receives the LR image (<b>303</b>) and extracts edges of objects in the LR image (<b>303</b>). Further, the edge upscaler (<b>418</b>) upscales or up-samples the edges of the objects using methods such as bilinear upscaling, bicubic upscaling, etc. to generate the HR edge map. Further, the edge upscaler (<b>418</b>) provides the upscaled/up-sampled edges to the edge guidance CNN (<b>419</b>). The edge guidance CNN (<b>419</b>) includes the second set of ESBs (<b>404</b>-<b>406</b>). The edge guidance CNN (<b>419</b>) generates the first set of feature maps by filtering the third set of feature maps and the HR edge map using the second set of ESBs (<b>404</b>-<b>406</b>). The edge guidance CNN (<b>419</b>) uses the HR edge map for determining missing high frequency information in the third set of feature maps and improves overall zoom quality at low complexity.</p><p id="p-0079" num="0078">Further, the edge guidance CNN (<b>419</b>) provides the first set of feature maps to the joint refinement NN block (<b>113</b>) and the pixel to frequency transformer (<b>420</b>). The pixel to frequency transformer (<b>420</b>) converts pixels of the first set of feature maps to the frequency components. The joint refinement NN block (<b>113</b>) includes the fifth set of ESBs (<b>413</b>-<b>415</b>).</p><p id="p-0080" num="0079">The frequency domain NN block (<b>112</b>) includes a pixel to frequency transformer (<b>421</b>), a frequency data classifier (<b>422</b>), a multi branch CNN (<b>423</b>), a refinement CNN (<b>425</b>), and a frequency to pixel transformer (<b>426</b>). The multi branch CNN (<b>423</b>) includes an upscaler (<b>424</b>) and the third set of ESBs (<b>407</b>-<b>409</b>). The pixel to frequency transformer (<b>421</b>) receives the LR image (<b>303</b>) and converts pixels of the LR image (<b>303</b>) to the frequency components. Further, the frequency data classifier (<b>422</b>) classifies the frequency components of the image (<b>303</b>) to different set of frequency components based on the similarity of the frequency components. The multi branch CNN (<b>423</b>) determines the fourth set of feature maps for each set of frequency components using the third set of ESBs (<b>407</b>-<b>409</b>). The upscaler (<b>424</b>) upscales the set of frequency components. The multi branch CNN (<b>423</b>) uses output of the upscaler (<b>424</b>) for learning and obtaining the optimal weights of the third set of ESBs (<b>407</b>-<b>409</b>) for generating the fourth set of feature maps. The multi branch CNN (<b>423</b>) provides the fourth set of feature maps to the refinement CNN (<b>425</b>).</p><p id="p-0081" num="0080">The refinement CNN (<b>425</b>) includes the fourth set of ESBs (<b>410</b>-<b>412</b>). The refinement CNN (<b>425</b>) generate the fifth set of feature maps by filtering the fourth set of feature maps using the fourth set of ESBs (<b>410</b>-<b>412</b>) and the frequency components of the first set of feature maps. The frequency to pixel transformer (<b>426</b>) converts the frequency components of the fifth set of feature maps to the pixels to form the second set of feature maps. Further, the frequency to pixel transformer (<b>426</b>) provides the second set of feature maps to the joint refinement NN block (<b>113</b>). The joint refinement NN block (<b>113</b>) generates the HR image (<b>305</b>) using the first set of feature maps and the second set of feature maps. The joint refinement NN block (<b>113</b>) processes the first set of feature maps and the second set of feature maps and updates the kernel weights based on important features of both feature maps to enhances output quality of the HR image (<b>305</b>). The joint refinement NN block (<b>113</b>) produces a weighted combination of both feature maps, where the weight is learned during the training phase.</p><p id="p-0082" num="0081">The upscaler (<b>427</b>) upscales/up-samples the LR image (<b>303</b>) to generate an upscaled/up-sampled image corresponding to the LR image (<b>303</b>) using the methods such as bilinear upscaling, bicubic upscaling, etc. The joint refinement NN block (<b>113</b>) uses the output (e.g., upscaled/up-sampled image) of the upscaler (<b>427</b>) for learning and obtaining the optimal weights of the fifth set of ESBs (<b>413</b>-<b>415</b>). In an embodiment, the first set of ESBs (<b>401</b>-<b>403</b>), the second set of ESBs (<b>404</b>-<b>406</b>), the third set of ESBs (<b>407</b>-<b>409</b>), the fourth set of ESBs (<b>410</b>-<b>412</b>), and the fifth set of ESBs (<b>413</b>-<b>415</b>) each include a variable number of ESBs in cascaded arrangement with a local and global feature concatenation to learn edges in 360&#xb0; direction. Each ESB (<b>401</b>-<b>415</b>) explicitly learns the horizontal edge feature and the vertical edge feature of an input image or an input feature map in separate network branches (<b>610</b>, <b>611</b>) with the 1D CNNs (<b>601</b>-<b>604</b>).</p><p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an architectural diagram of a CNN (<b>500</b>), according to an embodiment. The CNN (<b>500</b>) may refer to any one of the CNN with cascaded ESBs (<b>416</b>), the edge guidance CNN (<b>419</b>), the multi branch CNN (<b>423</b>), the refinement CNN (<b>425</b>), and the joint refinement CNN (<b>113</b>). As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a general architecture (<b>500</b>) includes convolution layers (<b>502</b>, <b>503</b>, <b>512</b>), a Depth-to-Space (D2S) layer (<b>511</b>), an upscaler (<b>514</b>), and ESB blocks (<b>510</b>A, <b>510</b>B). The convolution layers (<b>502</b>, <b>503</b>) are cascaded, where the convolution layer (<b>502</b>) receives the LR image (<b>303</b>) or an LR feature map (<b>501</b>). The convolution layers (<b>502</b>, <b>503</b>) perform convolution on the LR image (<b>303</b>) or the LR feature map (<b>501</b>), generates feature maps, and provides the feature maps to the ESB block (<b>510</b>A). A number of cascaded ESB blocks (<b>510</b>A, <b>510</b>B) in the CNN may be different for each of the CNN with cascaded ESBs (<b>416</b>), the edge guidance CNN (<b>419</b>), the multi branch CNN (<b>423</b>), the refinement CNN (<b>425</b>), and the joint refinement CNN (<b>113</b>). For example, the CNN (<b>500</b>) may include one or more additional ESB blocks cascaded between the ESB blocks (<b>510</b>A, <b>510</b>B).</p><p id="p-0084" num="0083">The ESB block (<b>510</b>A) includes an ESB (<b>504</b>), a concatenation layer (<b>505</b>), and a convolution layer (<b>506</b>), where the ESB (<b>504</b>), the concatenation layer (<b>505</b>), and the convolution layer (<b>506</b>) are connected in series. The ESB (<b>504</b>) and the concatenation layer (<b>505</b>) receive the feature maps from the convolution layer (<b>503</b>). The ESB (<b>504</b>) generates feature maps using the feature maps of the concatenation layer (<b>505</b>). Further, the concatenation layer (<b>505</b>) performs concatenation on the feature maps of the ESB (<b>504</b>) using the feature maps of the convolution layer (<b>503</b>). Further, the convolution layer (<b>506</b>) performs convolution on the concatenated feature maps from the concatenation layer (<b>505</b>) and generates feature maps. The ESB block (<b>510</b>A) provides the feature maps from the convolution layer (<b>506</b>) to the ESB block (<b>510</b>B). The ESB block (<b>510</b>B) includes an ESB (<b>507</b>), a concatenation layer (<b>508</b>), and a convolution layer (<b>509</b>), where the ESB (<b>507</b>), the concatenation layer (<b>508</b>), and the convolution layer (<b>509</b>) are connected in series.</p><p id="p-0085" num="0084">The ESB (<b>507</b>) and the concatenation layer (<b>508</b>) receives the feature maps from the convolution layer (<b>506</b>). The ESB (<b>507</b>) generates feature maps using the feature maps of the concatenation layer (<b>506</b>). Further, the concatenation layer (<b>508</b>) performs concatenation on the feature maps of the ESB (<b>507</b>) using the feature maps of the convolution layer (<b>506</b>). Further, the convolution layer (<b>509</b>) performs convolution on the concatenated feature maps from the concatenation layer (<b>508</b>) and generates feature maps. The ESB block (<b>510</b>A) provides the feature maps from the convolution layer (<b>509</b>) to the D2S layer (<b>511</b>). The D2S layer (<b>511</b>) and the convolution layer (<b>512</b>) reconstructs the HR image (<b>305</b>) or the HR feature map (<b>516</b>) using the feature map of the convolution layer (<b>509</b>).</p><p id="p-0086" num="0085">The upscaler (<b>514</b>) generates the high resolution of the of the LR image (<b>303</b>) or the LR feature map (<b>501</b>) using the methods such as bilinear upscaling, bicubic upscaling, etc. The ESBs (<b>504</b>, <b>507</b>) modifies weights of the CNNs of the ESBs (<b>504</b>, <b>507</b>) for obtaining the optimal weights based on the loss in quality between the high resolution image and the reconstructed HR image (<b>305</b>). The ESBs (<b>504</b>, <b>507</b>) uses a global residual learning method for determining the optimal weights. In response to obtaining the optimal weights, the ESBs (<b>504</b>, <b>507</b>) generates feature maps based on the optimal weights. Therefore, the D2S layer (<b>511</b>) and the convolution layer (<b>512</b>) may reconstruct the HR image (<b>305</b>) with an optimal quality.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an architectural diagram of an ESB (<b>600</b>) for generating a feature map (e.g., output feature map <b>612</b>), according to an embodiment. The ESB (<b>600</b>) may refer to any one of the ESBs (<b>504</b>, <b>507</b>). The ESB (<b>600</b>) includes a horizontal edge synthesis branch (<b>610</b>), a vertical edge synthesis branch (<b>611</b>), a concatenation layer (<b>608</b>), and a depth-wise separable convolution layer (<b>607</b>) of size k&#xd7;k, where k is a natural number. The horizontal edge synthesis branch (<b>610</b>) and the vertical edge synthesis branch (<b>611</b>) receive an input feature map (<b>609</b>) (e.g., the feature map from the convolution layer (<b>503</b>) or the convolution layer (<b>506</b>)). The horizontal edge synthesis branch (<b>610</b>) extracts the edge in the input feature map (<b>609</b>) in a horizontal direction. In an embodiment, the horizontal edge synthesis branch (<b>610</b>) includes convolution layers (<b>601</b>-<b>602</b>) of size l&#xd7;k and a depth-wise separable convolution layer (<b>605</b>) of size k&#xd7;k. The convolution layers (<b>601</b>-<b>602</b>) are connected in series. The convolution layer (<b>601</b>) receives the input feature map (<b>609</b>) and performs convolution on the input feature map (<b>609</b>) to generate feature maps. The convolution layer (<b>602</b>) receives the feature maps of the convolution layer (<b>601</b>) and performs convolution on the feature map of the convolution layer (<b>601</b>) to generate feature maps. The depth-wise separable convolution layer (<b>605</b>) receives the feature maps of the convolution layer (<b>602</b>) and performs a depth-wise separable convolution on the feature map of the convolution layer (<b>602</b>) to generate the edge features.</p><p id="p-0088" num="0087">The vertical edge synthesis branch (<b>611</b>) extracts the edge in the input feature map (<b>609</b>) in a vertical direction. In an embodiment, the vertical edge synthesis branch (<b>611</b>) includes convolution layers (<b>603</b>-<b>604</b>) of size k&#xd7;l and a depth-wise separable convolution layer (<b>606</b>) of size k&#xd7;k. The convolution layers (<b>603</b>-<b>604</b>) are connected in series. The convolution layer (<b>603</b>) receives the input feature map (<b>609</b>) and performs convolution on the input feature map (<b>609</b>) to generate feature maps. The convolution layer (<b>604</b>) receives the feature maps of the convolution layer (<b>603</b>) and performs convolution on the feature map of the convolution layer (<b>603</b>) to generate feature maps. The depth-wise separable convolution layer (<b>606</b>) receives the feature maps of the convolution layer (<b>604</b>) and performs the depth-wise separable convolution on the feature map of the convolution layer (<b>604</b>) to generate the edge features. The concatenation layer (<b>608</b>) performs concatenation on the edge features of the horizontal edge synthesis branch (<b>610</b>) and the edge features of the vertical edge synthesis branch (<b>610</b>) to learn edges features in all directions and generate feature maps. Further, the depth-wise separable convolution layer (<b>607</b>) performs the depth-wise separable convolution on the feature maps of the concatenation layer (<b>608</b>) to generate the output feature map (<b>612</b>).</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrate a flow diagram of a method for creating a second set of feature maps of a low resolution image (e.g., LR image <b>303</b>) by modifying the low resolution image using a frequency domain NN block (<b>112</b>), according to an embodiment. As shown in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, the LR image (<b>303</b>) is converted to a feature map (<b>701</b>) (e.g., by the pixel to frequency transformer (<b>421</b>)). The feature map (<b>701</b>) includes frequency components, e.g., Direct Current (DC) frequency components (<b>702</b>) and Alternate Current (AC) frequency components (<b>703</b>). Values of the DC frequency components (<b>702</b>) are zero, whereas values of the AC frequency components (<b>703</b>) are different, e.g., frequency components (<b>704</b>-<b>710</b>). Each block of the feature map (<b>701</b>) is a corresponding frequency component of each pixel in the LR image (<b>303</b>).</p><p id="p-0090" num="0089">As shown in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, the feature map (<b>701</b>) is split into frequency data blocks (<b>711</b>-<b>714</b>) (e.g., by the frequency data classifier (<b>422</b>)) where a size of each frequency data blocks (<b>711</b>-<b>714</b>) is of same size. For example, a size of the feature map (<b>701</b>) is 16&#xd7;16, then the feature map (<b>701</b>) may be split into 4 frequency data blocks (<b>711</b>-<b>714</b>) and the size of each frequency data blocks (<b>711</b>-<b>714</b>) will be 8&#xd7;8. The frequency data classifier (<b>422</b>) classifies the AC frequency components (<b>703</b>) in the frequency data blocks (<b>711</b>-<b>714</b>) to multiple sets of frequency components (<b>716</b>-<b>719</b>) in a frequency data arrangement (<b>715</b>) based on the similarity of the value of the AC frequency components (<b>703</b>) in the frequency data blocks (<b>711</b>-<b>714</b>). The set of frequency components (<b>716</b>) includes the frequency components (<b>704</b>) with lower value of frequency (e.g. 20 Hz). The set of frequency components (<b>717</b>) includes the frequency components (<b>705</b>) with value of frequency higher than the frequency components (<b>704</b>) (e.g. 30 Hz). Thus, the set of frequency components (<b>719</b>) includes the frequency components (<b>710</b>) with value of frequency higher than all other frequency components (<b>704</b>-<b>709</b>) (e.g. 500 Hz).</p><p id="p-0091" num="0090">The multi branch CNN (<b>423</b>) includes different CNN branches (<b>720</b>-<b>722</b>), where each CNN branch (<b>720</b>-<b>722</b>) is assigned to each set of frequency components (<b>716</b>-<b>719</b>) for generating a fourth set of feature maps (<b>724</b>) based on the value of the frequency components (<b>716</b>-<b>719</b>). For example, the Low Frequency (LF) CNN branch (<b>720</b>) is assigned to the set of frequency components (<b>716</b>) that includes the frequency components (<b>704</b>) with lower value of frequency (e.g. 20 Hz). Similarly, the High Frequency (HF) CNN branch (<b>720</b>) is assigned to the set of frequency components (<b>719</b>) that includes the frequency components (<b>710</b>) with higher value of frequency (e.g. 20 Hz). The LF CNN branch (<b>720</b>) generates the feature map of the set of frequency components (<b>716</b>) using a plurality of convolution layers (<b>721</b>) and the ESB (<b>407</b>). Similarly, the HF CNN branch generates the feature map of the set of frequency components (<b>719</b>) using a plurality of convolution layers (<b>723</b>) and the ESB (<b>409</b>).</p><p id="p-0092" num="0091">The multi branch CNN (<b>423</b>) combines the feature maps from the CNN branches (<b>720</b>-<b>722</b>) to form the fourth set of feature maps (<b>724</b>). Further, a multi branch CNN (<b>423</b>) generates a rearranged feature maps (<b>706</b>) by performing inverse frequency data arrangement on the fourth set of feature maps (<b>724</b>). It's not possible to visualize the actual image content with the use of just the frequency domain data. Hence, the frequency domain data needs to be converted in to pixel domain to visualize the image content. Since the frequency coefficients are rearranged to enable enhanced learning by CNNs, these frequency coefficients needs to be rearranged back to the original spatial positions after the processing. This rearranging back step is for faithful inverse transform and faithful image content for visualization. Further, the multi branch CNN (<b>423</b>) provides the rearranged feature maps (<b>706</b>) to the refinement CNN for further processing on the rearranged feature maps (<b>706</b>). An image contains multiple frequencies, for example, a texture region contains high frequencies, a smooth region contains low frequencies, etc. So, learning filters of the CNN for the high and low frequencies separately will allow the CNN to generate the HR image with the optimal quality. The proposed method allows the ESBs (<b>407</b>-<b>409</b>) of each CNN branch (<b>720</b>-<b>722</b>) to learn features related to each AC frequency components (<b>703</b>).</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a receptive field of a CNN with multiple layers, according to an embodiment. A receptive field of a layer in the CNN is a size of a region in an input image or input feature map seen by a pixel in that layer, and a receptive field of the CNN is a size of a region in the input image or input feature map seen by a pixel in a last layer of the CNN. As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, each block in a first layer <b>801</b>, second layer <b>802</b>, and third layer <b>803</b> indicates a pixel. In an example, a pixel (<b>804</b>-<b>2</b>) in the second layer (<b>802</b>) may see a 3&#xd7;3 region containing the pixels (<b>804</b>-<b>1</b>) in the first layer (<b>801</b>). Therefore, the receptive field of the second layer (<b>802</b>) is 3. A pixel (<b>805</b>-<b>3</b>) in the third layer (<b>803</b>) may see a 5&#xd7;5 region containing the pixels (<b>805</b>-<b>1</b>) in the first layer (<b>801</b>). Therefore, the receptive field of the third layer (<b>803</b>) is 5. So, a deep-learning network with more depth (i.e., more layers) will have a bigger receptive field. A CNN with a bigger receptive field sees a bigger region in the input image or input feature map, which allows the CNN to learn important features of the image/feature map and generate the HR image (<b>305</b>) with optimal quality.</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>9</b>B</figref> illustrate a comparison of complexity of computations of a two dimensional convolution layer and an ESB for generating an output feature map, according to an embodiment. As shown in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, an output frequency map (FM) (<b>904</b>) is generated by a two dimensional convolution layer (<b>903</b>) (Conv3&#xd7;3 layer) from an input FM (<b>901</b>), where the input FM (<b>901</b>) includes 16 input channels. The Conv3&#xd7;3 layer (<b>903</b>) performs a two dimensional convolution on the input FM (<b>901</b>), where the Conv3&#xd7;3 layer (<b>903</b>) includes 16 nodes. Each input channel provides an input to all the 16 nodes with 3&#xd7;3 weights (<b>902</b>). An example of 3&#xd7;3 weights is given below.</p><p id="p-0095" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="98pt" align="char"/><colspec colname="2" colwidth="14pt" align="char"/><colspec colname="3" colwidth="105pt" align="char"/><thead><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>+1</entry><entry>+2</entry><entry>+1</entry></row><row><entry>0</entry><entry>0</entry><entry>0</entry></row><row><entry>&#x2212;1</entry><entry>&#x2212;2</entry><entry>&#x2212;1</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0096" num="0094">A total multiplication for generating the output FM (<b>904</b>) with 16 channels using the two dimensional convolution layer (<b>903</b>)==weights&#xd7;number of input channel&#xd7;number of output channel=(3&#xd7;3)&#xd7;16&#xd7;16=2304.</p><p id="p-0097" num="0095">As shown in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>, the output FM (<b>904</b>) is generated by the ESBs (<b>504</b>, <b>507</b>) from the input FM (<b>901</b>), where the input FM (<b>901</b>) includes 16 input channels. Conv-1D layers (<b>906</b>, <b>908</b>) are the 1D convolution layers of the ESBs (<b>504</b>, <b>507</b>). The cony-1D layers (<b>906</b>, <b>908</b>) performs 1D convolution on the input FM (<b>901</b>), where each cony-1D layer (<b>906</b>, <b>908</b>) includes 8 nodes. Each input channel provides the input to all the 8 nodes of the cony-1D layer (<b>906</b>) with 1&#xd7;3 weights (<b>905</b>). Each input channel provides the input to all the 8 nodes of the cony-1D layer (<b>908</b>) with 3&#xd7;1 weights (<b>907</b>). Each cony-1D layers (<b>906</b>, <b>908</b>) generates an output FM (<b>909</b>) of 8 channels. Further, the output FM (<b>909</b>) of the cony-1D layers (<b>906</b>, <b>908</b>) are concatenated to form the output FM (<b>904</b>). An example of 1&#xd7;3 weights is given below.</p><p id="p-0098" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="105pt" align="center"/><colspec colname="2" colwidth="7pt" align="center"/><colspec colname="3" colwidth="105pt" align="center"/><thead><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>&#x2212;5</entry><entry>1</entry><entry>3</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0099" num="0096">An example of 3&#xd7;1 weights is given below.</p><p id="p-0100" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="char"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>2</entry></row><row><entry>-1</entry></row><row><entry>4</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0101" num="0097">A total multiplication for generating the output FM (<b>904</b>) with 16 channels using the ESBs (<b>504</b>, <b>507</b>)=(weights&#xd7;number of input channel&#xd7;number of output channel)&#xd7;2=((1&#xd7;3)&#xd7;16&#xd7;8)&#xd7;2=768. Therefore, the computation for obtaining the output FM (<b>904</b>) with the ESBs (<b>504</b>, <b>507</b>) is very less compared to the two dimensional convolution layer (<b>903</b>). The ESBs (<b>504</b>, <b>507</b>) with the 1D convolution layers (<b>906</b>, <b>908</b>) achieves similar or better quality compared to the two dimensional convolution layer (<b>903</b>) at a lower computational complexity.</p><p id="p-0102" num="0098"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrate a comparison of complexity of computations of a depth-wise separable convolution layer and an ESB for generating an output feature map, according to an embodiment. As shown in <figref idref="DRAWINGS">FIG. <b>10</b>A</figref>, an output FM (<b>1007</b>) is generated by a depth-wise separable convolution layer (<b>1003</b>) from an input FM (<b>1001</b>), where the input FM (<b>1001</b>) includes 3 input channels. The depth-wise separable convolution layer (<b>1003</b>) includes a first convolution layer (<b>1004</b>) (conv3&#xd7;3 layer) with a size 3&#xd7;3 and a second convolution layer (<b>1006</b>) (conv1&#xd7;1 layer) with a size 1&#xd7;1. The conv3&#xd7;3 layer (<b>1004</b>) performs the one-one 3&#xd7;3 convolution on the input FM (<b>901</b>), where the conv3&#xd7;3 layer (<b>1004</b>) includes 3 nodes. Each input channel provides an input to corresponding node of the conv3&#xd7;3 layer (<b>1004</b>) with 3&#xd7;3 weights (<b>1002</b>). All output of the conv3&#xd7;3 layer (<b>1004</b>) is transferred to the conv1&#xd7;1 layer (<b>1006</b>) with a 1&#xd7;1 weights (<b>1005</b>). The conv1&#xd7;1 layer (<b>1006</b>) performs fully connected pointwise 1&#xd7;1 convolution on feature maps from the conv3&#xd7;3 layer (<b>1004</b>) and generates the output FM (<b>1007</b>) contains 3 output values. A total multiplication for generating the output FM (<b>1007</b>) with 3 channels using the depth-wise separable convolution layer (<b>1003</b>)=(weights&#xd7;number of output channel of the conv3&#xd7;3 layer (<b>1004</b>))+(weights&#xd7;number of input channel&#xd7;number of output channel of the conv1&#xd7;1 layer (<b>1006</b>)))=((3&#xd7;3)&#xd7;3)+((1&#xd7;1)&#xd7;3&#xd7;3)=36.</p><p id="p-0103" num="0099">As shown in <figref idref="DRAWINGS">FIG. <b>10</b>B</figref>, the output FM (<b>1007</b>) is generated by the ESBs (<b>504</b>, <b>507</b>) from the input FM (<b>1001</b>), where the input FM (<b>1007</b>) includes 3 input channels. A cony 1&#xd7;3 layer (<b>1009</b>) is the 1D convolution layer of the ESBs (<b>504</b>, <b>507</b>). The cony 1&#xd7;3 layer (<b>1009</b>) performs 1D convolution on the input FM (<b>1001</b>), where the cony 1&#xd7;3 layer (<b>1009</b>) includes 3 nodes. Each input channel provides the input to all the 3 nodes of the cony 1&#xd7;3 layer (<b>1009</b>) with 1&#xd7;3 weights (<b>1008</b>). The cony 1&#xd7;3 layer (<b>1009</b>) generates the output FM (<b>1007</b>) of 3 channels. A total multiplication for generating the output FM (<b>1007</b>) with 3 channels using the ESB (<b>504</b>, <b>507</b>)=(weights&#xd7;number of input channel&#xd7;number of output channel)=(1&#xd7;3)&#xd7;3&#xd7;3=27. Therefore, the computation for obtaining the output FM (<b>1007</b>) in the ESBs (<b>504</b>, <b>507</b>) is very less compared to the depth-wise separable convolution layer (<b>1003</b>). The ESBs (<b>504</b>, <b>507</b>) with the 1D convolution layers (<b>1009</b>) achieves similar or better quality compared to the depth-wise separable convolution layer (<b>1003</b>) at the lower computational complexity by an intelligent structure of spatial and depth separable techniques. A comparison of various attributes of existing convolution methods and the ESB (<b>504</b>, <b>507</b>) is given in table <b>1</b>.</p><p id="p-0104" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="49pt" align="left"/><colspec colname="3" colwidth="35pt" align="left"/><colspec colname="4" colwidth="56pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="4" rowsep="1">TABLE 1</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry/><entry>Complexity</entry><entry>Quality</entry><entry>Receptive field</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Two dimensional</entry><entry>High</entry><entry>High</entry><entry>High</entry></row><row><entry/><entry>convolution</entry><entry/><entry/><entry/></row><row><entry/><entry>Depth-wise</entry><entry>Medium</entry><entry>Medium</entry><entry>High</entry></row><row><entry/><entry>separable</entry><entry/><entry/><entry/></row><row><entry/><entry>convolution layer</entry><entry/><entry/><entry/></row><row><entry/><entry>ESB</entry><entry>Low</entry><entry>High</entry><entry>High</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0105" num="0100">The above-described embodiments may be implemented as programs executable on a computer, and be implemented by a general-purpose digital computer for operating the programs by using a non-transitory computer-readable medium. Data structures used in the above-described embodiments may be recorded on the computer-readable medium via a variety of means. The above-described embodiments of the disclosure may be implemented in the form of a non-transitory computer-readable recording medium including instructions executable by the computer, such as a program module executed by the computer. For example, methods implemented by software modules or algorithms may be stored in a computer-readable medium as computer-readable codes or program commands executable by the computer.</p><p id="p-0106" num="0101">The non-transitory computer-readable recording medium may be any recording medium that are accessible by the computer, and examples thereof may include both volatile and non-volatile media and both detachable and non-detachable media. Examples of the computer-readable medium may include magnetic storage media (e.g., ROM, floppy disks, and hard disks) and optical recording media (e.g., compact disc-ROM (CD-ROM) and digital versatile discs (DVDs)), but are not limited thereto. Furthermore, the computer-readable recording medium may include a computer storage medium and a communication medium. A plurality of computer-readable recording media may be distributed over network-coupled computer systems, and data, e.g., program instructions and codes, stored in the distributed recording media may be executed by at least one computer.</p><p id="p-0107" num="0102">The foregoing description of the specific embodiments will so fully reveal the general nature of the embodiments herein that others can, by applying current knowledge, readily modify and/or adapt for various applications such specific embodiments without departing from the generic concept, and, therefore, such adaptations and modifications should and are intended to be comprehended within the meaning and range of equivalents of the disclosed embodiments. It is to be understood that the phraseology or terminology employed herein is for the purpose of description and not of limitation. Therefore, those skilled in the art will recognize that the embodiments herein may be practiced with modification within the scope of the disclosure as described herein.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for performing an artificial intelligence (AI) based zoom of an image in an electronic device, the method comprising:<claim-text>receiving the image;</claim-text><claim-text>obtaining, through a pixel domain neural network (NN) block, a first set of feature maps of the image based on pixels of the image;</claim-text><claim-text>obtaining, through a frequency domain NN block, a second set of feature maps of the image based on frequencies of the image; and</claim-text><claim-text>obtaining, through a joint refinement NN block, a final image with a resolution higher than a resolution of the image, based on the first set of feature maps and the second set of feature maps.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the joint refinement NN block learns kernel weights during a training phase, and updates the kernel weights based on image characteristics in the first set of feature maps and the second set of feature maps, and wherein the joint refinement NN block comprises a set of edge synthesis blocks (ESBs).</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the obtaining the first set of feature maps of the image comprises:<claim-text>identifying a third set of feature maps of the image using a first set of edge synthesis blocks (ESBs) of the pixel domain NN block;</claim-text><claim-text>extracting edges in the image;</claim-text><claim-text>upscaling the edges to obtain a high resolution (HR) edge map; and</claim-text><claim-text>obtaining the first set of feature maps of the image by filtering the third set of feature maps and the HR edge map using a second set of ESBs of the pixel domain NN block.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the first set of ESBs of the pixel domain NN block learns a horizontal edge feature and a vertical edge feature of the image in separate network branches with one-dimensional (1D) convolution neural networks (CNNs).</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the first set of ESBs of the pixel domain NN block comprises a variable number of ESBs in cascaded arrangement with a local and global feature concatenation to learn edges in 360&#xb0; directions.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the second set of ESBs of the pixel domain NN block guides learned edge features in the third set of feature maps using the HR edge map to improve an edge consistency.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the obtaining the second set of feature maps of the image comprises:<claim-text>converting the image to frequency components;</claim-text><claim-text>classifying the frequency components of the image to at least one set of frequency components based on a similarity of the frequency components;</claim-text><claim-text>determining a fourth set of feature maps for each set of frequency components using a first set of edge synthesis blocks (ESBs) in the frequency domain NN block;</claim-text><claim-text>obtaining a fifth set of feature maps by filtering the fourth set of feature maps and frequency components of the first set of feature maps using a second set of ESBs in the frequency domain NN block; and</claim-text><claim-text>converting the fifth set of feature maps to the second set of feature maps based on pixels of the image.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the frequency components are classified to enable a deep learning model for learning in a frequency domain and extracting important features in local neighborhood characteristics.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the first set of ESBs of the frequency domain NN block learns a local neighborhood frequency characteristics to reduce noise artefacts.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein each of the ESBs comprises a plurality of cascaded one-dimensional (1D) convolution NNs neural networks (CNNs), a plurality of depth-wise separable CNNs, and a concatenate layer.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein outputs of each set of cascaded 1D CNNs are input to corresponding depth-wise separable CNNs, and<claim-text>wherein outputs of the corresponding depth-wise separable CNNs are input to a final depth-wise separable CNN after performing concatenation on the outputs of the corresponding depth-wise separable CNNs using the concatenate layer.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. An electronic device for performing an artificial intelligence (AI) based zoom of an image, the electronic device comprising:<claim-text>a memory;</claim-text><claim-text>a processor; and</claim-text><claim-text>an image zoom controller operably coupled to the memory and the processor, and configured to:</claim-text><claim-text>receive the image,</claim-text><claim-text>obtain, through a pixel domain neural network (NN) block, a first set of feature maps of the image based on pixels of the image,</claim-text><claim-text>obtain, through a frequency domain NN block, a second set of feature maps of the image based on frequencies of the image, and</claim-text><claim-text>obtain, through a joint refinement NN block, a final image with a resolution higher than the image, based on the first set of feature maps and the second set of feature maps.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The electronic device of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the joint refinement NN block learns kernel weights during a training phase, and updates the kernel weights based on image characteristics in the first set of feature maps and the second set of feature maps, wherein the joint refinement NN block comprises a set of edge synthesis blocks (ESBs).</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The electronic device of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the image zoom controller is further configured to:<claim-text>obtain a third set of feature maps of the image using a first set of edge synthesis blocks (ESBs) of the pixel domain NN block;</claim-text><claim-text>extract edges in the image;</claim-text><claim-text>upscale the edges to obtain a high resolution (HR) edge map; and</claim-text><claim-text>obtain the first set of feature maps of the image by filtering the third set of feature maps and the HR edge map using a second set of ESBs of the pixel domain NN block.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The electronic device of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the image zoom controller is further configured to:<claim-text>convert the image to frequency components;</claim-text><claim-text>classify the frequency components of the image to at least one set of frequency components based on a similarity of the frequency components;</claim-text><claim-text>obtain a fourth set of feature maps for each set of frequency components using a first set of edge synthesis blocks (ESBs) in the frequency domain NN block;</claim-text><claim-text>obtain a fifth set of feature maps by filtering the fourth set of feature maps and frequency components of the first set of feature maps using a second set of ESBs in the frequency domain NN block; and</claim-text><claim-text>convert the fifth set of feature maps to the second set of feature maps based on pixels of the image.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A non-transitory computer readable medium for storing computer readable program code or instructions for carrying out operations, when executed by a processor, for performing an artificial intelligence (AI) based zoom of an image, the operations comprising:<claim-text>receiving the image;</claim-text><claim-text>obtaining a first set of feature maps of the image based on pixels of the image, as an output of a pixel domain neural network (NN) block by inputting the image to the pixel domain NN block;</claim-text><claim-text>obtaining a second set of feature maps of the image based on frequencies of the image, as an output of a frequency domain NN block by inputting the image to the frequency domain NN block; and</claim-text><claim-text>obtaining a final image having a higher resolution than the image, as an output of a joint refinement NN block by inputting the first set of feature maps of the image and the second set of feature maps of the image to the joint refinement NN block.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the obtaining the first set of feature maps of the image comprises:<claim-text>obtaining a third set of feature maps of the image using a first set of edge synthesis blocks (ESBs) of the pixel domain NN block;</claim-text><claim-text>extracting edges in the image;</claim-text><claim-text>upscaling the edges to obtain a high resolution (HR) edge map; and</claim-text><claim-text>obtaining the first set of feature maps of the image by filtering the third set of feature maps and the HR edge map using a second set of ESBs of the pixel domain NN block.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer readable medium in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the obtaining the second set of feature maps of the image comprises:<claim-text>converting the image to frequency components;</claim-text><claim-text>classifying the frequency components of the image to at least one set of frequency components based on a similarity of the frequency components;</claim-text><claim-text>obtaining a fourth set of feature maps for each set of frequency components using a first set of edge synthesis blocks (ESBs) in the frequency domain NN block;</claim-text><claim-text>obtaining a fifth set of feature maps by filtering the fourth set of feature maps and frequency components of the first set of feature maps using a second set of Edge Synthesis Blocks (ESBs) in the frequency domain NN block; and</claim-text><claim-text>converting the fifth set of feature maps to the second set of feature maps based on pixels of the image.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the obtaining the final image having a higher resolution than the image comprises:<claim-text>producing a weighted combination of the first set of feature maps and the second set of feature maps.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the producing the weighted combination of the first set of feature maps and the second set of feature maps comprise:<claim-text>processing the first set of feature maps and the second set of feature maps to determine one or more features to enhance output quality of the final image;</claim-text><claim-text>updating kernel weights based on the determined one or more features; and</claim-text><claim-text>producing the weighted combination of the first set of feature maps and the second set of feature maps based on the updated kernel weights.</claim-text></claim-text></claim></claims></us-patent-application>