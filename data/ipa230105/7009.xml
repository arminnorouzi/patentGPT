<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007010A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007010</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17865711</doc-number><date>20220715</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>GB</country><doc-number>1610421.8</doc-number><date>20160615</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>9</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>W</subclass><main-group>4</main-group><subgroup>021</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>W</subclass><main-group>12</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>W</subclass><main-group>12</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>63</main-group><subgroup>107</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>W</subclass><main-group>4</main-group><subgroup>021</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>W</subclass><main-group>12</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>W</subclass><main-group>12</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>63</main-group><subgroup>0853</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20210101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>W</subclass><main-group>12</main-group><subgroup>63</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">LOCATION BASED AUTHENTICATION</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16585350</doc-number><date>20190927</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11425142</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17865711</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15624001</doc-number><date>20170615</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10484395</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16585350</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NAGRAVISION S.A.</orgname><address><city>CHESEAUX-SUR-LAUSANNE</city><country>CH</country></address></addressbook><residence><country>CH</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Schaer</last-name><first-name>Olivier</first-name><address><city>CHESEAUX-SUR-LAUSANNE</city><country>CH</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Karoui</last-name><first-name>Sami</first-name><address><city>CHESEAUX-SUR-LAUSANNE</city><country>CH</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Schlaeppi</last-name><first-name>Florent</first-name><address><city>CHESEAUX-SUR-LAUSANNE</city><country>CH</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>NAGRAVISION S.A.</orgname><role>03</role><address><city>CHESEAUX-SUR-LAUSANNE</city><country>CH</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of enabling a user to access recorded data associated with an event, the method comprising determining the location of a user's device at a control unit, confirming the location is within a predetermined vicinity and that the user's device was at the location within a predetermined period and the control unit enabling access for the user to the data if the location of the user's device in the predetermined period is confirmed.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="116.33mm" wi="136.99mm" file="US20230007010A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="129.12mm" wi="139.02mm" file="US20230007010A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="153.92mm" wi="94.23mm" file="US20230007010A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="225.30mm" wi="101.60mm" file="US20230007010A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="212.26mm" wi="124.12mm" orientation="landscape" file="US20230007010A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 16/585,350 filed Sep. 27, 2019, which is a continuation of U.S. patent application Ser. No. 15/624,001 filed Jun. 15, 2017, which claims priority to GB Patent Application No. 1610421.8, filed Jun. 15, 2016, all of which are incorporated herein by reference in their entirety for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The disclosure relates to providing data associated with an event to a user, based on authentication of the user.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">During a live event, such as a sporting event or concert, a spectator at the venue may wish to record footage or images of the event as a memento or to share with friends. The spectator uses his or her personal camera or smartphone to make a recording of all or part of the event. This recording can then be shared online or otherwise so that it can be accessed by others. However, the quality of the video may be poor due to such things as inadequate recording equipment, distance of the spectator from the action and/or poor camerawork by the spectator.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0005" num="0004">Exemplary embodiments of the disclosure shall now be described with reference to the accompanying drawings in which:</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an environment in which a live event is taking place;</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a display on a user device to request access to video content;</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a flow chart of a method of providing data associated with a live event to a user;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a flow chart of a method of accessing a data stream associated with a live event from a user device; and</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a block diagram of one implementation of a computing device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0011" num="0010">Throughout the description and the drawings, like reference numerals refer to like parts.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an environment <b>100</b> in which an arranged live event is taking place. The environment has a performance area <b>102</b> in which the live event such as a sporting, musical, political or other event is being performed. The live event is captured by installed video cameras <b>104</b><i>a</i>-<i>c </i>directed at the performance area <b>102</b> and possibly the crowd and other contributory aspects of the environment. Each video camera <b>104</b><i>a</i>-<i>c </i>has a defined location in the environment <b>100</b>. In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, three video cameras are shown, although it will be appreciated that any number of video cameras could be used in order to appropriately record the live event. The cameras may be part of a system set up to record the event for later or concurrent transmission, for example as a scheduled television program. Alternatively, the cameras may be dedicated to the purpose of implementing the system disclosed herein.</p><p id="p-0013" num="0012">The live event is observed by a crowd of spectators in the environment <b>100</b>. A spectator in the crowd has a user device <b>106</b>, for example a smartphone, tablet, personal computer, smartwatch, or clicker, connected to the internet or other supporting communications network through a smartphone or radio frequency RF link such as Wi-Fi or Bluetooth.</p><p id="p-0014" num="0013">The environment <b>100</b> also has a number of communication beacons <b>108</b><i>a</i>-<i>e </i>distributed about the environment. In this embodiment, the user device <b>106</b> communicates with the beacons <b>108</b><i>a</i>-<i>e </i>using Bluetooth, although other local and pico network communication schemes can be used to similar effect. Each beacon <b>108</b><i>a</i>-<i>e </i>is installed at a predefined position in the environment <b>100</b> and has its own ID. Each beacon <b>108</b><i>a</i>-<i>e </i>has a configurable broadcast range which allows it to connect with a device <b>106</b> and receive data via Bluetooth from anywhere within its broadcast range in the environment <b>100</b>. The broadcast range of each beacon is adjusted so that the set of beacons provides a more or less comprehensive coverage for the spectating area of the environment. Five beacons are shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, although it will be appreciated that any number of beacons can be used. It will also be appreciated that increasing the number of beacons increases the granularity of the location information as described below. It is also possible to implement the disclosed method/system with a single beacon.</p><p id="p-0015" num="0014">The system and method according to the embodiment described provides the user with the ability to request access to video or other content recorded by one or more cameras <b>104</b><i>a</i>-<i>c</i>. The system and method disclosed enable the event organiser to provide a value added service to the user as customer by providing them with access to the content recorded at the event. The content may be linked to the approximate position of the user at the event so that the user has a professionally produced record of the event from the vicinity of their own position, or the user can choose one or more camera angles. The choice is extended to selecting parts of the event, such as songs at a concert or a speech at a lecture, or time limited clips as samples of the event. Such parts can be automatically selected coincident with the time of the request during the event. For example, if a user makes a request halfway through a song, the user can automatically be provided with the video recording of the complete song starting from before the request was made.</p><p id="p-0016" num="0015">A control unit <b>110</b> is configured to communicate with the user device <b>106</b>. The control unit <b>110</b> may be located within the environment <b>100</b> or remote from it. The control unit <b>110</b> has memory storage containing a database of the locations of the video cameras <b>104</b><i>a</i>-<i>c</i>, beacon IDs of the beacons <b>108</b><i>a</i>-<i>e </i>and their locations in the environment <b>100</b>. The control unit <b>110</b> also stores details of the event, for example, event type, time and duration. The control unit <b>110</b> is configured to communicate with a media processing unit MPU <b>112</b> which is located within the environment <b>100</b>, but may also be remote from it. The control unit <b>110</b> and the MPU <b>112</b> may be amalgamated into a single entity. The MPU <b>112</b> receives video streams from each of the video cameras <b>104</b><i>a</i>-<i>c </i>during the event. In this embodiment, the video and/or audio data in the stream is coded according to the Moving Picture Experts Group MPEG standard. Other coding formats can be used to similar effect. The streams are stored in the memory in the MPU <b>112</b> for consumption by a user, as will be described below.</p><p id="p-0017" num="0016">The MPU <b>112</b> communicates with the control unit <b>110</b> and the video cameras <b>104</b><i>a</i>-<i>c </i>via Wi-Fi. Other communication schemes having a bandwidth capable of supporting the streamed content can be used. For example, if the control unit <b>110</b> and/or the MPU <b>112</b> are located remotely from the environment <b>100</b>, communication may be enabled over a network such as the internet or cable.</p><p id="p-0018" num="0017">During an event, the video cameras <b>104</b><i>a</i>-<i>c </i>record the live event that is occurring in the performance area <b>102</b>. The video stream of the recorded event from each of the cameras is sent to the MPU <b>112</b> and tagged to the particular event, according to location of the event, time stamps and a camera source. The video content and tag data are stored at the MPU <b>112</b>. The time stamps are applied to the recorded video at various points during the event to enable retrieval of the video based on a point in the duration of the event.</p><p id="p-0019" num="0018">A user attending the event has, for example, a smartphone as a form of the user device <b>106</b>. This has an application stored on it that enables the user to gain access to and order the recorded content stored at the MPU <b>112</b>. There are two levels of security for access. First, the user has to login to the application by providing an email or other identifying handle and a password or PIN that is registered for the service. The user device then has to be validated for the event the user is attending. Once validated the user has access to the recorded content for the event on request as described below. The two levels of security are optional. The login may not be needed in certain situations. However, the validation of the user actually attending the event to gain access to the content is the desirable authentication step.</p><p id="p-0020" num="0019">The application installed in the user's smartphone <b>106</b> is illustrated by way of a screenshot in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The user having logged in at some point before or during the event, the application is then running on the device until it is disabled. In this embodiment the application constantly or intermittently monitors for transmissions of the beacons <b>108</b> so that the current position of the device <b>106</b> relative to the beacons is always known as the closest beacon in the network at any time is captured by the device <b>106</b>. In the network of beacons, capture is effected by the device opting to retrieve beacon ID information from the beacon providing the highest signal strength. This will generally equate to the closest beacon. The information on beacon capture is stored on the device for use should the user decide at some point to request recorded content associated with the live event. The location information required by the system to verify the device is supplemented by the time of the beacon ID capture. In this way the device is validated as being in the correct location at the correct time for the event. If either the location information or the time information does not correspond to the event details stored at the control unit <b>110</b>, the validation is unsuccessful and access to the recordings of the event is denied. To perform the validation the location information derived from the captured beacon ID and the time of capture are sent to the control unit <b>110</b> as evidence that the user device <b>106</b> is or was in the correct vicinity at the correct time. If the validation is successful a confirmation is sent back to the device <b>106</b> which is then arranged to display on the screen a validation symbol <b>204</b>. This informs the user that he/she is able to make a request for recorded content associated with the attended event.</p><p id="p-0021" num="0020">It is preferable for the user to enable the application on the user device and perform the login either at or close to the start of the event. The device is then ready to execute a request without undue delay. However, the application could be enabled at any time during the event as required by the user. This may be preferable if the user is concerned about battery life being inadequate for the duration of the live event. Once an indication <b>204</b> appears on the screen on the device <b>106</b> the application is ready to process a request. In an alternative embodiment the user can provide verification of the presence of the device at the event by using a built-in global positioning system GPS capability. The co-ordinates for a device with this functionality can provide the necessary location information with sufficient accuracy and the beacons <b>108</b><i>a</i>-<i>e </i>are then not needed as part of the system.</p><p id="p-0022" num="0021">By running the application for a sustained period it is likely in some events that the user will move around the venue. The location information used for validation is the latest available either by beacon capture or GPS co-ordinates.</p><p id="p-0023" num="0022">With the login and validation procedure carried out, the control unit <b>110</b> is able to confirm from its own database of events that the device is or was present at a particular event. It then enables access to recorded content for that event and sends a confirmation back to the device <b>106</b> which responds by illuminating the indication <b>204</b>. In this state a button <b>206</b> on the screen is enabled. Pressing the button causes a request to be sent from the device to the control unit for recorded content. If the control unit is in or near the environment it is possible for the request to be sent via Wi-Fi or other local transmission networks. If the control unit <b>110</b> is remote from the environment the request can be sent, for example, via a mobile telecommunications network.</p><p id="p-0024" num="0023">In an alternative embodiment the time and location information for validation of the device <b>106</b> and the request itself are transmitted to the control unit <b>110</b> in the same or consecutive transmissions but without the need to wait for validation before making a request. The control unit <b>110</b> then determines that the device is authorised to make the request at the time the request is made and sends an acknowledgment back to the device <b>106</b>. This does not require a confirmatory indication <b>204</b>, but this can instead be used by way of subsequent confirmation that the request has been received by the control unit and is deemed valid.</p><p id="p-0025" num="0024">Once a valid request is received by the control unit <b>110</b>, the user is offered recorded video from at least one selected camera position <b>104</b><i>a</i>-<i>c</i>. The video cameras each offer a different perspective of the recorded event. By using the beacon ID of the beacon captured by the user device <b>106</b> or the transmitted GPS information, and the known locations of the cameras <b>104</b><i>a</i>-<i>c</i>, the control unit <b>110</b> can offer by default the recorded video from the camera closest to the user location in the environment <b>100</b> of the event. This gives the user recorded content from a similar perspective to that of his position in the environment. Other perspectives provided by other video cameras may also be selected. For example, a choice may be offered of a main feed, an on-stage feed, a user perspective and a feed from elsewhere in the crowd. In this case the device location information is not needed to fix the position of the device <b>106</b> at or near a particular camera. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, windows <b>208</b><i>a</i>-<i>f </i>are provided to show either thumbnails or short videos of the various camera perspectives if the user has a choice. By pressing one of the thumbnails, the request is supplemented with a specifier of the video of interest to the user. This is transmitted back to the control unit. The user then has the option of accessing the video content itself according to the selected perspective(s) at the time or later. This will be described below.</p><p id="p-0026" num="0025">The control unit <b>110</b> can also use the time information of the user request to identify a specific part of the event, such as a song at a concert, or provide access to a defined clip of the song according to the selected camera angle. This is particularly applicable to discrete content at a live event such as a song, lecture or speech. However, the technique is equally applicable to delivering content that constitutes simply a portion of the event straddling the time of the request. Thus, the user is offered selections in the windows <b>208</b> <i>a</i>-<i>f </i>of different camera positions of the complete event, specific parts of the event straddling the request time and/or clips starting from or straddling the time of the request. The content can be made available immediately upon request or it can be reserved for later consumption.</p><p id="p-0027" num="0026">Once the user has selected one or more of the offered recordings, the completion of the request is sent back to the control unit. The control unit <b>110</b> communicates with the MPU <b>112</b> and requests that the MPU <b>112</b> extracts from the stored content the video or portion associated with the time of the request. The MPU may extract a video clip that has been requested by a user device <b>106</b> in different ways. The video clip duration may be any length within the event. It may be a particular song at a musical event during which the request was received. For a defined part of the event such as a song in a concert, the clip can be arranged to coincide with the beginning and end of the part so that what is provided to the user is the complete song. Alternatively, discrete clips with a predetermined clip duration are marked in the stored video recording. The length can be predetermined by the control unit <b>110</b> or the MPU <b>112</b>. For example, the stored video recording is marked in discrete clips of 30 seconds resulting in 120 clips per hour for the event. The MPU then retrieves the most appropriate clip for each request, that is, the clip during which the user request was made. Alternatively, the user can be enabled via the application on the user device <b>106</b> to define the clip start and length.</p><p id="p-0028" num="0027">In another form, the clips are extracted from the stored content on the basis of a configurable time window that is applied for each individual request and its timestamp. For example, for a request at time &#x201c;1&#x201d;, a clip may begin at t&#x2212;10 seconds and last until t+20 seconds.</p><p id="p-0029" num="0028">This ensures that the user receives content that has just been witnessed, as well as content following the input for context.</p><p id="p-0030" num="0029">Alternatively, the size and/or duration of a video file may be determined dynamically based on the number of requests received within a given period. For example, a clip may have a nominal duration of 15 seconds and a timeout window of 10 seconds. That is to say, for a request of time &#x201c;t&#x201d;, a clip may begin at t&#x2212;15 seconds and last until t. If a second request is received before t+10 seconds, the duration of the video clip to be delivered is extended to include video content up to the second request. For example, if a second request is received at t+3 seconds, the final video clip will have a duration of 15 seconds+3 seconds=18 seconds. The clip duration is extended as further requests are received, up to a predetermined clip length, for example 30 seconds. This avoids producing large files and long video clips to be processed. In this example, the minimum duration of a clip is 15 seconds and the maximum duration of a clip is 30 seconds. In this way, a single clip can address one or more requests. The dynamic determination of video clip duration allows multiple requests received in a predetermined period to be mapped to the same video clip and the duplication of a given video moment across different video files is prevented. Furthermore, with this approach, a single request during an hour-long event would only materially generate a 15 seconds video file. This reduces storage requirements and processing time required.</p><p id="p-0031" num="0030">In another form, the system may be configured to operate using a time limited buffer storing video content on a first-in-first-out FIFO basis. In this case, recorded content is stored in the buffer and then overwritten as newly recorded content becomes available once the buffer is full, using the FIFO approach. For example, only the previous 60 seconds of recorded content may be stored in the buffer at any given time. The buffer may be implemented at the MPU <b>112</b> or in a local memory in each respective camera <b>104</b><i>a</i>-<i>c</i>. The MPU <b>112</b> or the local memory could, of course, be configured to store a longer or shorter duration of recorded content depending on the context. The buffer provides an immediate starting point for a user request but without requiring all content for the entirety of the event to be stored. The storage is based on user demand.</p><p id="p-0032" num="0031">In this mode, a request user to access stored content triggers the recording of further content, so that an extended duration of recorded content is provided beyond that which can be stored in the buffer. The extension of the duration of recorded content could be in line with one of the extraction methods discussed above. For example, the 60 seconds of data preceding a request are stored in the buffer and all or part of it may be copied to the memory in the MPU <b>112</b>, with 20 seconds of recorded content following the request also stored in the memory in the MPU <b>112</b>. In this way, the user can access content similar to that described above, although video is only streamed from the cameras <b>104</b><i>a</i>-<i>c </i>to the MPU <b>112</b> and/or stored at the MPU <b>112</b> over the duration of a live event in response to a request.</p><p id="p-0033" num="0032">In the case that the buffer is implemented in a memory local to each respective camera <b>104</b><i>a</i>-<i>c</i>, no recorded content will even be streamed to the MPU <b>112</b> unless a user request is received.</p><p id="p-0034" num="0033">Due to the reduction in streaming and storing requirements for video content, the system more efficiently manages video content. Such efficiency gains are particularly useful in the case that video cameras <b>104</b><i>a</i>-<i>c </i>have limited storage space and upload bandwidth, which would limit full stream recording or constant live stream uploading. When communicating with the MPU <b>112</b>, the control unit <b>110</b> uses optimisation rules to cluster requests received from multiple user devices <b>106</b> for the same defined content within a predetermined time window and can consolidate them into a single request for the MPU <b>112</b>. For example, if multiple users click to request the same content, such as a song, within a given window, only a single media file is requested from the stored content and generated by the MPU <b>112</b> to be accessed by the requesting user devices. By combining multiple requests from multiple users into a single file, storage and bandwidth efficiencies are realised.</p><p id="p-0035" num="0034">Different request clustering methods may be performed. For example, one video file may be sent for all requests received over a given period. In the case that the format of the recorded video is HTTP Live Streaming HLS, a stream of video content is delivered through a series of consecutive discrete video clips which is stored at the MPU <b>112</b>. One of these video clips may be sent for all requests received over a given period. Alternatively, one video clip may be sent once a given number of requests has been received. Alternatives to HLS for streaming the recorded video can be used to similar effect. The advantage of such streaming techniques as HLS is that they break the streamed video into discrete portions or clips that can be readily identified for purposes of selection.</p><p id="p-0036" num="0035">Once a video clip is extracted from a stored video recording, the MPU <b>112</b> generates a file of the video clip and uploads it to a predefined storage platform for access. The storage platform is located remotely from the environment. Alternatively, or additionally, the storage platform may be located within the environment <b>100</b>. The MPU <b>112</b> sends an access link, for example a URL, for the video clip to the control unit <b>110</b>. The control unit <b>110</b> obfuscates the URL of the media clip through the use of a token based URL, where generation of a URL does not allow derivation of another valid URL. The control unit <b>110</b> then sends the obfuscated URL to the user device <b>106</b> where it is displayed for selection as a screen of the application. The user is then in possession of a link to the requested content.</p><p id="p-0037" num="0036">The user can then access the video clip by navigating to the URL. The user can share the clip complemented with additional data, such as text comments, audio comments or images. The control unit <b>110</b> applies usage rules for the video clip, for example, a limit on the number of playbacks of the video content. In this way, the video clip may be limited to a certain number of retrievals, shares, replays and/or downloads. The limitation may be based on the identity of the device being used to access the video clip. For example, only user devices <b>106</b> authenticated as having been present at the event might be allowed to access the video clip, so that the clip can only be shared amongst attendees. Alternatively, other devices may be allowed to access the video clip, for example on a subscription or registration basis.</p><p id="p-0038" num="0037">The limitations may be applied during the event and/or for a limited time after the event. After this time, the video clip may be arranged to no longer be available. Alternatively, the limitations may be applied for an unlimited time after the event.</p><p id="p-0039" num="0038">A user who attended the event and had their user device validated before or during the event can be permitted access to view a convenient collection of the most popular content. Popularity can be defined in different ways. For example, the number of clip requests received during the event can be assessed at the control unit <b>110</b> to determine the most popular content. The control unit <b>110</b> then collates data on the usage of the service for an event and compiles information on audience behaviour that is of potential use to the event organisers and third parties for promotional purposes. The traffic between the user devices and the system is also useful data for deriving information for other promotional purposes, such as voting for a particular song at a concert.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a flow chart of a method <b>300</b> of enabling a user to access data associated with a live event. In this embodiment, the data is a video recording of the event. At step <b>302</b>, the control unit <b>110</b> determines a location of the user device <b>106</b> from location data. The location data is derived from information based on the capture of the beacon ID of one of the beacons <b>108</b><i>a</i>-<i>e </i>by the device <b>106</b>. At step <b>304</b>, the control unit <b>110</b> confirms that the location derived from the location data is within a predetermined vicinity coincident with the event and that the device is at the location during the event. Alternatively, the time validation maybe within a period that starts before and ends after the scheduled time of the event or a specific window of time during the event according to the service provider's choice. At step <b>306</b>, the control unit <b>110</b> enables access for the user to the recorded content if the location of the device in the period is confirmed.</p><p id="p-0041" num="0040">In this embodiment, the user makes a request during the event. Rather than extracting a video clip based on the time of the user request, the user accesses any video content associated with the event that is available from the MPU <b>112</b>. This is done either directly in response to the request or later, for example via a web browser. In this way, the control unit <b>110</b> confirms that the user was physically present at the event and enables the user to access the content, without communicating with the MPU <b>112</b> at the time. The user can then select preferred video content from any part of the event after the event has taken place, for example a particular song or the whole concert and desired camera position(s).</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a flow chart of a method <b>400</b> of accessing data associated with a live event by the user device <b>106</b>. At step <b>402</b>, the user requests access to data using the user device <b>106</b>. In this embodiment, the data is a stored video recording of the event. As above, the request includes providing location information and the time of the request. In this embodiment, the location information is the beacon ID of one of the beacons <b>108</b><i>a</i>-<i>e</i>. At step <b>404</b>, the user is enabled to access the content on the user device <b>106</b> must be present within the vicinity of the event in order to be validated to access the relevant video content.</p><p id="p-0043" num="0042">The system is adaptable to different contexts as it is applicable to different sources of media content. The system can therefore be applied to various different contexts, such as concerts, sports events, lectures, experimental laboratories, or any live event which can be recorded and shared.</p><p id="p-0044" num="0043">Video cameras <b>104</b><i>a</i>-<i>c </i>can be professional cameras each operated by a cameraman or remotely from a control station. They, or some of them, may also be static. In other implementations, the video cameras can be replaced by any other suitable recording devices capable of providing a media feed to the MPU <b>112</b>. For example, the amateur cameras based on smartphones, camcorders or action cameras could be used, or simply a microphone for audio recording only. In this way, other media content than video, such as images or audio, could be recorded and accessed by the user according to these disclosed embodiments. In the case that a smartphone, or other recording device with limited storage and bandwidth capability, is used, the buffer approach to recording content discussed above may be preferable.</p><p id="p-0045" num="0044">Instead of video cameras at fixed locations, the video cameras <b>104</b><i>a</i>-<i>c </i>could be moveable. In this instance, the location of the video cameras is tracked and determined by the control unit <b>110</b> or the MPU <b>112</b> based on GPS data that is derived by each of the video cameras using installed GPS functionality and transmitted from there such that the control unit or the MPU <b>112</b> is able to monitor their positions. Similarly, the beacons <b>108</b><i>a</i>-<i>e </i>may also be moveable to cater, for example, for any event in which the live action moves from one place to another. The beacons are tracked using location information such as GPS. In one form, the beacons are each located on a respective video camera. The moveable beacons may each simply track an aspect of the event such that absolute location information is less important than the fact that a particular beacon is tracking a given aspect of the event.</p><p id="p-0046" num="0045">Each beacon <b>108</b><i>a</i>-<i>e </i>can be associated with a different video feed for the same live event. For example, beacons <b>108</b><i>a </i>and <i>b </i>are associated with video camera <b>104</b><i>a</i>, while beacon <b>108</b><i>c </i>is associated with video camera <b>104</b><i>b </i>and beacons <b>108</b><i>d </i>and <i>e </i>are associated with video camera <b>104</b><i>c</i>. The control unit <b>110</b> can then identify a particular video associated with each beacon location using the mappings between the video cameras and the beacons.</p><p id="p-0047" num="0046">Instead of Bluetooth, the user device <b>106</b> can communicate with the beacons via any wireless protocol that can interface the user device with each beacon. Instead of Wi-Fi, the user device <b>106</b> can communicate with the control unit <b>110</b> via a mobile network or Global System for Mobile GSM networks. Similarly, the position of the user device can be located through its own communications networks such as GSM, 3G, 4G or 5G instead of the beacon system referred to above or GPS.</p><p id="p-0048" num="0047">When only links are shared between user device <b>106</b> and the control unit <b>110</b> for accessing recorded content, bandwidth efficiency is realised. A video file is only shared when accessed by the user by navigating to the associated URL. User recording of a video is no longer required as a user can access media content of the event through the control unit <b>110</b> and the recording devices, without using the processing and storage necessary to record content on the user device <b>106</b> itself.</p><p id="p-0049" num="0048">The quality of captured content may also be improved as higher resolution recording devices may be used to capture the media content, rather than lower quality user devices having a recording function. Furthermore, as well as picture quality, angle of view and quality of direction is likely to be improved over user controlled handheld user devices.</p><p id="p-0050" num="0049">A user does not need to invest personal time and/or have specialised knowledge to perform content capture and editing. The click of a button <b>206</b> in the application on the user device <b>106</b> can perform both the capture of the content and the editing at the same time. A user is not restricted to requesting a single recorded video of an event but can request multiple content items from multiple recording devices in sequence or simultaneously.</p><p id="p-0051" num="0050">Disclosed herein is a method and system for supplementing a live event with the option for the attendee to access recorded content of the event. The attendee is validated as having been present at the event by location information processed through his/her user device. The attendee is able to make a request for content or a request for content options from which a selection can be made. The request can be made after access to the content has been enabled, at the same time or as a sequence as part of the same user action. The content is not accessed directly but is provided with a link to the requested content which can be retrieved at the attendee's convenience but possibly subject to usage restrictions imposed by the service provider.</p><p id="p-0052" num="0051">A user may log into the service through an application as described above or a web page. The authentication process records the login information provided by the user, for example an email address, and also the unique device ID associated with that user device. This provides an extra layer of security to the process as only known subscribers may then use the service.</p><p id="p-0053" num="0052">In accordance with an aspect of the disclosure there is provided a method of enabling a user to access data associated with an event to a user. The method comprises determining the location of a user's device at a control unit, confirming the location is within a predetermined vicinity and that the user's device was at the location within a predetermined period, and the control unit enabling access for the user to the data if the location of the user's device in the predetermined period is confirmed.</p><p id="p-0054" num="0053">The data associated with the event may be a recording of at least part of the event recorded on a recording device. The recording may be a video recording recorded by a camera. The video recording may be captured by at least one camera, the at least one camera being located at a defined camera location in the vicinity of the event. The defined camera location may be mobile relative to the live event.</p><p id="p-0055" num="0054">Location information may be sent for the control unit to determine the location of the user's device during the event. The location information may be sent to the control unit by the user's device. The method may include deriving the location information from a location device within the vicinity of the event. The user's device may communicate with the closest one of a plurality of location devices in the vicinity to receive the location information. The user's device may communicate with the location devices wirelessly and the closest location device may be determined by relative signal strength. The user's device may receive location information from the one location device. The user's device may generate its own location information.</p><p id="p-0056" num="0055">The at least one location device may be at a fixed location relative to the live event. The at least one location device may be a wireless-enabled beacon. The user's device may be configured to monitor for the presence of the at least one location device.</p><p id="p-0057" num="0056">The access to the data may be enabled in response to a request sent from the user's device. The request may be sent during the event. The data may be stored in a buffer configured to store data for a predefined period. The method may further comprise transferring, to a memory, at least some of the data stored in the buffer at the time of the request, and storing subsequent data recorded after the request in the memory. The request may be sent after the event.</p><p id="p-0058" num="0057">In accordance with another aspect of the disclosure there is provided a method of enabling a user to access recorded data associated with an event. The method comprises determining the location of a user's device at a control unit, confirming the location is within a predetermined vicinity and that the user's device was at the location within a predetermined period, the control unit enabling access for the user to the data if the location of the user's device in the period are confirmed and receiving a request for the data.</p><p id="p-0059" num="0058">The request may be sent during the event. The data may be stored in a buffer configured to store data for a predefined period. The method may further comprise transferring, to a memory, at least some of the data stored in the buffer at the time of the request, and storing subsequent data recorded after the request in the memory. The user's device may send location information for determining the location of the user's device. The location information may be sent with the request. The request may be sent after the location information.</p><p id="p-0060" num="0059">The access to the data may be provided by a link received by the user's device. The link may be transmitted from the control unit in response to a request. The access to the data may comprise extracting a portion of the data associated with the event in response to the request. The portion of the data may be coincident with the time of the request. The portion of the data may be a predefined set of data on a portion of the event corresponding to a period within which the request is made. The portion of the data may be a set of data running from a predetermined time before the time of the request to a predetermined time after the request. The portion of the data may be determined based on the location of the user's device. The extracted data may be assigned an address for the user which is incorporated into the link.</p><p id="p-0061" num="0060">Usage rules may be applied to the data such that access is restricted. The usage rules may comprise limiting the data to a number of retrievals, shares, playbacks and/or downloads. The data may be accessed via the link by another device determined to have been within the vicinity and within the period.</p><p id="p-0062" num="0061">The user's device may be enabled to access the data for a limited period. The limited period may be the duration of the live event. The method may further comprise receiving a plurality of requests for data associated with the event from a plurality of devices, and compiling a single package of data for the plurality of devices. The compilation may be performed on a plurality of requests received in a predetermined period. The predetermined vicinity and period time may be coincident with the live event.</p><p id="p-0063" num="0062">In accordance with another aspect of the disclosure there is provided a system for enabling a user to access data associated with a live event, the system comprising a control unit arranged to determine the location of a device, confirm the location is within a predetermined vicinity and that the user's device was at the location within a predetermined period, and enable access for the user to the data if the location of the user's device in the predetermined period is confirmed.</p><p id="p-0064" num="0063">The system may further comprise at least one recording device, in which the data are a recording of at least part of the event recorded on the at least one recording device. The at least one recording device may be a camera and the recording may be a video recording. The video recording may be a recording of at least part of the event captured by at least one camera, the at least one camera being located at a defined camera location in the vicinity of the event.</p><p id="p-0065" num="0064">Location information may be sent to the control unit which may be operable to determine the location of the user's device during the event. The location information may be sent to the control unit by the user's device. The user's device may be operable to derive the location information from a location device within the vicinity of the event. The system may further comprise a plurality of location devices in the vicinity to receive the location information. The user's device may be operable to communicate with the closest one of the plurality of location devices. The user's device may be operable to communicate with the location devices wirelessly and may be operable to determine the closest location device by relative signal strength. The user's device may be operable to receive location information from the one location device. The user's device may be operable to generate its own location information.</p><p id="p-0066" num="0065">The at least one location device may be at a fixed location relative to the live event. The at least one location device may be a wireless-enabled beacon. The device may be configured to monitor for the presence of the at least one location device.</p><p id="p-0067" num="0066">The access to the data may be enabled by the control unit in response to a request sent from the user's device. The request may be sent during the event. The system may further comprise a buffer configured to store data for a predefined period. The data may be stored in the buffer. The system may further comprise a memory. At least some of the data stored in the buffer at the time of the request may be transferred to the memory, and subsequent data recorded after the request may be stored in the memory. The request may be sent after the event.</p><p id="p-0068" num="0067">In accordance with another aspect of the disclosure there is provided a system for enabling a user to access data associated with a live event, the system comprising a control unit arranged to determine the location of a device, confirm the location is within a predetermined vicinity and that the user's device was at the location within a predetermined period, and enable access for the user to the data if the location of the user's device in the period are confirmed and receive a request for the data.</p><p id="p-0069" num="0068">The request may be sent during the event. The data may be stored in a buffer configured to store data for a predefined period. At least some of the data stored in the buffer at the time of the request may be transferred to a memory, and subsequent data recorded after the request may be stored in the memory.</p><p id="p-0070" num="0069">The user's device may send location information for determining the location of the user's device. The location information may be sent with the request. The request may be sent after the location information.</p><p id="p-0071" num="0070">The access to the data may be provided by a link sent by the user's device. The link may be transmitted from the control unit in response to a request. The access to the data may comprise extracting a portion of the data associated with the event in response to the request. The portion of the data may be coincident with the time of the request. The portion of the data may be a predefined set of data on a portion of the event corresponding to a period within which the request is made. The portion of the data may be a set of data running from a predetermined time before the time of the request to a predetermined time after the request. The portion of the data may be determined based on the location of the user's device. The extracted data may be assigned an address for the user which is incorporated into the link.</p><p id="p-0072" num="0071">The system may further comprise a media processing unit &#x201c;MPU&#x201d; configured to extract a portion of the data associated with the event. The control unit may be configured to enable access for the user to the portion of the data. The MPU may be configured to extract the portion of the data based on the time of the request.</p><p id="p-0073" num="0072">Usage rules may be applied to the data such that access is restricted. The usage rules comprise limiting the data to a number of retrievals, shares, playbacks and/or downloads.</p><p id="p-0074" num="0073">The data may be accessed via the link by another device confirmed to have been within the vicinity and within the period. The control unit may be further configured to enable access to the data for a limited period. The limited period may be the duration of the live event. The control unit may be further configured to receive a plurality of requests for data associated with the event from a plurality of devices, and compile a single package of data to be accessed by the plurality of devices. The control unit may be configured to perform the compilation on a plurality of requests received in a predetermined period. The predetermined vicinity and period time may be coincident with the live event. The control unit may be located remote from an environment in which the live event is taking place. The MPU may be located remote from an environment in which the live event is taking place.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a block diagram of one implementation of a computing device <b>500</b> within which a set of instructions, for causing the computing device to perform any one or more of the methodologies discussed herein, may be executed. In alternative implementations, the computing device may be connected e.g., networked to other machines in a Local Area Network LAN, an intranet, an extranet, or the Internet. The computing device may operate in the capacity of a server or a client machine in a client-server network environment, or as a peer machine in a peer-to-peer or distributed network environment. The computing device may be a personal computer PC, a tablet computer, a set-top box STB, a Personal Digital Assistant PDA, a cellular telephone, a web appliance, a server, a network router, switch or bridge, or any machine capable of executing a set of instructions, sequential or otherwise, that specify actions to be taken by that machine. Further, while only a single computing device is illustrated, the term &#x201c;computing device&#x201d; shall also be taken to include any collection of machines e.g., computers, that individually or jointly execute a set, or multiple sets, of instructions to perform any one or more of the methodologies discussed herein.</p><p id="p-0076" num="0075">The example computing device <b>500</b> includes a processing device <b>502</b>, a main memory <b>504</b> e.g., read-only memory ROM, flash memory, dynamic random access memory DRAM such as synchronous DRAM SDRAM or Rambus DRAM RDRAM, etc., a static memory <b>506</b> e.g., flash memory, static random access memory SRAM, etc., and a secondary memory e.g., a data storage device <b>518</b>, which communicate with each other via a bus <b>530</b>.</p><p id="p-0077" num="0076">Processing device <b>502</b> represents one or more general-purpose processors such as a microprocessor, central processing unit, or the like. More particularly, the processing device <b>502</b> may be a complex instruction set computing CISC microprocessor, reduced instruction set computing RISC microprocessor, very long instruction word VLIW microprocessor, processor implementing other instruction sets, or processors implementing a combination of instruction sets. Processing device <b>502</b> may also be one or more special-purpose processing devices such as an application specific integrated circuit ASIC, a field programmable gate array FPGA, a digital signal processor DSP, network processor, or the like. Processing device <b>502</b> is configured to execute the processing logic instructions <b>522</b> for performing the operations and steps discussed herein.</p><p id="p-0078" num="0077">The computing device <b>500</b> may further include a network interface device <b>508</b>. The computing device <b>500</b> also may include a video display unit <b>510</b> e.g., a liquid crystal display LCD or a cathode ray tube CRT, an alphanumeric input device <b>512</b> e.g., a keyboard or touchscreen, a cursor control device <b>514</b> e.g., a mouse or touchscreen, and an audio device <b>516</b> e.g., a speaker.</p><p id="p-0079" num="0078">The data storage device <b>518</b> may include one or more machine-readable storage media, or more specifically one or more non-transitory computer-readable storage media <b>528</b> on which is stored one or more sets of instructions <b>522</b> embodying any one or more of the methodologies or functions described herein. The instructions <b>522</b> may also reside, completely or at least partially, within the main memory <b>504</b> and/or within the processing device <b>502</b> during execution thereof by the computer system <b>500</b>, the main memory <b>504</b> and the processing device <b>502</b> also constituting computer-readable storage media.</p><p id="p-0080" num="0079">The various methods described above may be implemented by a computer program. The computer program product may include computer code arranged to instruct a computer to perform the functions of one or more of the various methods described above. The computer program and/or the code for performing such methods may be provided to an apparatus, such as a computer, on one or more computer readable media or, more generally, a computer program product. The computer readable media may be transitory or non-transitory. The one or more computer readable media could be, for example, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, or a propagation medium for data transmission, for example for downloading the code over the Internet. Alternatively, the one or more computer readable media could take the form of one or more physical computer readable media such as semiconductor or solid state memory, magnetic tape, a removable computer diskette, a random access memory RAM, a read-only memory ROM, a rigid magnetic disc, and an optical disk, such as a CD-ROM, CD-R/W or DVD.</p><p id="p-0081" num="0080">In an implementation, the modules, components and other features described herein, for example control unit <b>110</b> in relation to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, can be implemented as discrete components or integrated in the functionality of hardware components such as ASICS, FPGAs, DSPs or similar devices as part of an individualization server.</p><p id="p-0082" num="0081">A &#x201c;hardware component&#x201d; is a tangible e.g., non-transitory physical component e.g., a set of one or more processors capable of performing certain operations and may be configured or arranged in a certain physical manner. A hardware component may include dedicated circuitry or logic that is permanently configured to perform certain operations. A hardware component may be or include a special-purpose processor, such as a field programmable gate array FPGA or an ASIC. A hardware component may also include programmable logic or circuitry that is temporarily configured by software to perform certain operations.</p><p id="p-0083" num="0082">Accordingly, the phrase &#x201c;hardware component&#x201d; should be understood to encompass a tangible entity that may be physically constructed, permanently configured e.g., hardwired, or temporarily configured e.g., programmed, to operate in a certain manner or to perform certain operations described herein.</p><p id="p-0084" num="0083">In addition, the modules and components can be implemented as firmware or functional circuitry within hardware devices. Further, the modules and components can be implemented in any combination of hardware devices and software components, or only in software e.g., code stored or otherwise embodied in a machine-readable medium or in a transmission medium.</p><p id="p-0085" num="0084">Unless specifically stated otherwise, as apparent from the following discussion, it is appreciated that throughout the description, discussions utilizing terms such as &#x201c;receiving,&#x201d; &#x201c;determining,&#x201d; &#x201c;comparing,&#x201d; &#x201c;enabling,&#x201d; &#x201c;maintaining,&#x201d; &#x201c;identifying,&#x201d; or the like, refer to the actions and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical, electronic, quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission or display devices.</p><p id="p-0086" num="0085">It is to be understood that the above description is intended to be illustrative, and not restrictive. Many other implementations will be apparent to those of skill in the art upon reading and understanding the above description. Although the present disclosure has been described with reference to specific example implementations, it will be recognized that the disclosure is not limited to the implementations described, but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly, the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense. The scope of the disclosure should, therefore, be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled.</p><p id="p-0087" num="0086">Also disclosed is a method of enabling a user to access recorded data associated with an event, the method comprising determining the location of a user's device at a control unit, confirming the location is within a predetermined vicinity and that the user's device was at the location within a predetermined period, and the control unit enabling access for the user to the data if the location of the user's device in the predetermined period is confirmed.</p><p id="p-0088" num="0087">Optionally, the data are a recording of at least part of the event recorded on at least one recording device. Optionally, the recording is a video recording recorded by at least one camera. Optionally, the video recording is a recording of at least part of the event captured by the at least one camera, the at least one camera being located at a defined camera location in the vicinity of the event.</p><p id="p-0089" num="0088">Optionally, location information is sent for the control unit to determine the location of the user's device during the event. Optionally, the location information is sent to the control unit by the user's device. Optionally, the method includes deriving the location information from a location device within the vicinity of the event. Optionally, the user's device communicates with the closest one of a plurality of location devices in the vicinity to receive the location information. Optionally, the user's device receives location information from the one location device. Optionally, the user's device generates its own location information.</p><p id="p-0090" num="0089">Optionally, the access to the data is enabled in response to a request sent from the user's device. Optionally, the request is sent during the event. Optionally, the data are stored in a buffer configured to store data for a predefined period. Optionally, the method further comprises transferring, to a memory, at least some of the data stored in the buffer at the time of the request, and storing subsequent data recorded after the request in the memory. Optionally, the request is sent after the event.</p><p id="p-0091" num="0090">Optionally, the access to the data is provided by a link received by the user's device. Optionally, the link is transmitted from the control unit in response to a request. Optionally, the access to the data comprises extracting a portion of the data associated with the event in response to the request. Optionally, the portion of the data is coincident with the time of the request. Optionally, the portion of the data is a predefined set of data on a portion of the event corresponding to a period within which the request is made. Optionally, the portion of the data is a set of data running from a predetermined time before the time of the request to a predetermined time after the request. Optionally, the portion of the data is determined based on the location of the user's device. Optionally, the extracted data is assigned an address for the user which is incorporated into the link.</p><p id="p-0092" num="0091">Optionally, usage rules are applied to the data such that access is restricted. Optionally, the usage rules comprise limiting the data to a number of retrievals, shares, playbacks and/or downloads.</p><p id="p-0093" num="0092">Also disclosed is a system for enabling a user to access recorded data associated with an event, the system comprising a control unit arranged to determine the location of a user's device, confirm the location is within a predetermined vicinity and that the user's device was at the location within a predetermined period, and enable access for the user to the data if the location of the user's device in the predetermined period is confirmed.</p><p id="p-0094" num="0093">Optionally, the system further comprises at least one recording device, in which the data are a recording of at least part of the event recorded on the at least one recording device. Optionally, the at least one recording device is at least one camera and the recording is a video recording recorded by the at least one camera. Optionally, the video recording is a recording of at least part of the event captured by the at least one camera, the at least one camera being located at a defined camera location in the vicinity of the event.</p><p id="p-0095" num="0094">Optionally, location information is sent to the control unit which is operable to determine the location of the user's device during the event. Optionally, the location information is sent to the control unit by the user's device. Optionally, the user's device is operable to derive the location information from a location device within the vicinity of the event. Optionally, the system further comprises a plurality of location devices in the vicinity to receive the location information, in which the user's device is operable to communicate with the closest one of the plurality of location devices. Optionally, the user's device is operable to receive location information from the one location device. Optionally, the user's device is operable to generate its own location information.</p><p id="p-0096" num="0095">Optionally, the access to the data is enabled by the control unit in response to a request sent from the user's device. Optionally, the request is sent during the event. Optionally, the system further comprises a buffer configured to store data for a predefined period, in which the data are stored in the buffer. Optionally, the system further comprises a memory, in which at least some of the data stored in the buffer at the time of the request is transferred to the memory, and subsequent data recorded after the request is stored in the memory. Optionally, the request is sent after the event.</p><p id="p-0097" num="0096">Optionally, the access to the data is provided by a link sent by the user's device. Optionally, the link is transmitted from the control unit in response to a request. Optionally, the enabling comprises extracting a portion of the data associated with the event in response to the request. Optionally, the portion of the data is coincident with the time of the request. Optionally, the portion of the data is a predefined set of data on a portion of the event corresponding to a period within which the request is made. Optionally, the portion of the data is a set of data running from a predetermined time before the time of the request to a predetermined time after the request. Optionally, the portion of the data is determined based on the location of the user's device. Optionally, the extracted data is assigned an address for the user which is incorporated into the link.</p><p id="p-0098" num="0097">Optionally, usage rules are applied to the data such that access is restricted. Optionally, the usage rules comprise limiting the data to a number of retrievals, shares, playbacks and/or downloads.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. (canceled)</claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. A method of providing access to data associated with one or more events, the method comprising:<claim-text>receiving a first request for a first portion of stored data associated with an event;</claim-text><claim-text>receiving a second request for a second portion of the stored data;</claim-text><claim-text>determining a time associated with the first request overlaps with at least a portion of a time associated with the second request;</claim-text><claim-text>consolidating the first request and the second request into a single request based on the time associated with the first request overlapping with at least the portion of the time associated with the second request;</claim-text><claim-text>extracting a portion of the stored data based on the single request; and</claim-text><claim-text>generating, for the single request, a media file for the extracted portion of the stored data.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising extending a duration of the media file based on the second request.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the extracted portion of the stored data corresponds to a time window relative to the time associated with the first request or the time associated with the second request.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the time window includes a first time duration before the time associated with the first request and a second time duration after the time associated with the first request.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the time window includes a first time duration before the time associated with the first request and a second time duration after the time associated with the second request.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the media file includes data from the stored data within the time window.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising providing at least one of a first device of a first user or a second device of a second user with access to the media file.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the stored data is based on a video of at least part of the event captured by at least one camera, the at least one camera being located at a location of the event.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An apparatus for providing access to data associated with one or more events, comprising:<claim-text>a memory configured to store media data; and</claim-text><claim-text>one or more processors coupled to the memory and configured to:<claim-text>receive a first request for a first portion of stored data associated with an event;</claim-text><claim-text>receive a second request for a second portion of the stored data;</claim-text><claim-text>determine a time associated with the first request overlaps with at least a portion of a time associated with the second request;</claim-text><claim-text>consolidate the first request and the second request into a single request based on the time associated with the first request overlapping with at least the portion of the time associated with the second request;</claim-text><claim-text>extract a portion of the stored data based on the single request; and</claim-text><claim-text>generate, for the single request, a media file for the extracted portion of the stored data.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the one or more processors are configured to extend a duration of the media file based on the second request.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the extracted portion of the stored data corresponds to a time window relative to the time associated with the first request or the time associated with the second request.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the time window includes a first time duration before the time associated with the first request and a second time duration after the time associated with the first request.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the time window includes a first time duration before the time associated with the first request and a second time duration after the time associated with the second request.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the media file includes data from the stored data within the time window.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the one or more processors are configured to provide at least one of a first device of a first user or a second device of a second user with access to the media file.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the stored data is based on a video of at least part of the event captured by at least one camera, the at least one camera being located at a location of the event.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A non-transitory computer-readable medium having stored thereon instructions that, when executed by one or more processors, cause the one or more processors to:<claim-text>receive a first request for a first portion of stored data associated with an event;</claim-text><claim-text>receive a second request for a second portion of the stored data;</claim-text><claim-text>determine a time associated with the first request overlaps with at least a portion of a time associated with the second request;</claim-text><claim-text>consolidate the first request and the second request into a single request based on the time associated with the first request overlapping with at least the portion of the time associated with the second request;</claim-text><claim-text>extract a portion of the stored data based on the single request; and</claim-text><claim-text>generate, for the single request, a media file for the extracted portion of the stored data.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising instructions that, when executed by the one or more processors, cause the one or more processors to extend a duration of the media file based on the second request.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the extracted portion of the stored data corresponds to a time window relative to the time associated with the first request or the time associated with the second request.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the time window includes one of:<claim-text>a first time duration before the time associated with the first request and a second time duration after the time associated with the first request; or</claim-text><claim-text>a third time duration before the time associated with the first request and a fourth time duration after the time associated with the second request.</claim-text></claim-text></claim></claims></us-patent-application>