<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004787A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004787</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17779736</doc-number><date>20191127</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Distributed Deep Learning System</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Nippon Telegraph and Telephone Corporation</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Tanaka</last-name><first-name>Kenji</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Arikawa</last-name><first-name>Yuki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Ito</last-name><first-name>Tsuyoshi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Terada</last-name><first-name>Kazuhiko</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Sakamoto</last-name><first-name>Takeshi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2019/046373</doc-number><date>20191127</date></document-id><us-371c12-date><date>20220525</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A distributed deep learning system includes nodes (<b>1</b>-<i>n</i>, n=1, . . . , 4) and a network. The node (<b>1</b>-<i>n</i>) includes GPUs (<b>11</b>-<i>n</i>-<b>1</b> and <b>11</b>-<i>n</i>-<b>2</b>), and an FPGA (<b>12</b>-<i>n</i>). The FPGA (<b>12</b>-<i>n</i>) includes a plurality of GPU reception buffers, a plurality of network transmission buffers that store data transferred from the GPU reception buffers, a plurality of network reception buffers that store aggregated data received from other nodes, and a plurality of GPU transmission buffers that store data transferred from the network reception buffers. The GPUs (<b>11</b>-<i>n</i>-<b>1</b> and <b>11</b>-<i>n</i>-<b>2</b>) DMA-transfer data to the FPGA (<b>12</b>-<i>n</i>). The data stored in the GPU transmission buffers is DMA-transferred to the GPUs (<b>11</b>-<i>n</i>-<b>1</b> and <b>11</b>-<i>n</i>-<b>2</b>).</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="100.25mm" wi="158.75mm" file="US20230004787A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="233.51mm" wi="145.20mm" orientation="landscape" file="US20230004787A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="234.61mm" wi="114.30mm" orientation="landscape" file="US20230004787A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="194.99mm" wi="154.60mm" orientation="landscape" file="US20230004787A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="206.76mm" wi="154.26mm" orientation="landscape" file="US20230004787A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="154.86mm" wi="148.51mm" file="US20230004787A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="235.88mm" wi="117.94mm" file="US20230004787A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="237.07mm" wi="116.67mm" file="US20230004787A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="224.79mm" wi="148.76mm" file="US20230004787A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="175.26mm" wi="148.76mm" file="US20230004787A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="232.75mm" wi="153.75mm" orientation="landscape" file="US20230004787A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="234.36mm" wi="110.32mm" orientation="landscape" file="US20230004787A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="198.29mm" wi="154.94mm" orientation="landscape" file="US20230004787A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="207.09mm" wi="154.77mm" orientation="landscape" file="US20230004787A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="233.93mm" wi="135.04mm" orientation="landscape" file="US20230004787A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="233.93mm" wi="114.64mm" orientation="landscape" file="US20230004787A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="197.70mm" wi="154.69mm" orientation="landscape" file="US20230004787A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="207.18mm" wi="154.86mm" orientation="landscape" file="US20230004787A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="129.54mm" wi="148.84mm" file="US20230004787A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="233.76mm" wi="135.38mm" orientation="landscape" file="US20230004787A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="190.16mm" wi="154.69mm" orientation="landscape" file="US20230004787A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="196.09mm" wi="154.43mm" orientation="landscape" file="US20230004787A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="232.41mm" wi="133.69mm" orientation="landscape" file="US20230004787A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="190.33mm" wi="154.43mm" orientation="landscape" file="US20230004787A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="194.82mm" wi="154.60mm" orientation="landscape" file="US20230004787A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="237.49mm" wi="118.28mm" file="US20230004787A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="235.63mm" wi="116.08mm" file="US20230004787A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="106.09mm" wi="147.32mm" file="US20230004787A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="233.85mm" wi="133.18mm" orientation="landscape" file="US20230004787A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="185.76mm" wi="154.18mm" orientation="landscape" file="US20230004787A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="195.66mm" wi="154.94mm" orientation="landscape" file="US20230004787A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a national phase entry of PCT Application No. PCT/JP2019/046373, filed on Nov. 27, 2019, which application is hereby incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present invention relates to a distributed deep learning system that executes deep learning, which is machine learning using a neural network, by using a plurality of nodes in a distributed and collaborative manner.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Deep learning is to learn models adapted to input data by alternately performing forward propagation and back propagation. In recent years, accelerators such as a graphics processing unit (GPU) are used to efficiently perform the forward propagation and the back propagation. In recent years, there exist enormous amounts of input data, processing of which by one computing device causes storage and I/O (input/output) bottlenecks to occur, and thus, data parallel distributed deep learning has been proposed in which data is distributed and processed in a plurality of computing devices (see NPL 1).</p><p id="p-0005" num="0004">In the data parallel distributed deep learning, computing devices performs forward propagations and back propagations different from each other, and resulting weight data after the back propagations is shared using communications. This sharing is a collective communication process called Allreduce. In Allreduce, the weight data calculated by each computing device is reduced (summed) and broadcast (distributed). It is known that Allreduce has an important role in the data parallel distributed deep learning but is a bottleneck.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system of related art. The distributed deep learning system includes N nodes <b>100</b>-<i>n </i>(n=1, . . . , N) and a network <b>200</b> connecting the N nodes <b>100</b>-<i>n </i>to each other (where N is an integer of 2 or more, and here N=4).</p><p id="p-0007" num="0006">A master node <b>100</b>-<b>1</b> includes a central processing unit (CPU) <b>101</b>-<b>1</b>, a GPU <b>102</b>-<b>1</b>, and an FPGA <b>103</b>-<b>1</b>.</p><p id="p-0008" num="0007">A slave node <b>100</b>-<i>k </i>(k=2, . . . , N) includes a CPU <b>101</b>-<i>k</i>, a GPU <b>102</b>-<i>k</i>-<b>1</b>, and an FPGA <b>103</b>-<i>k. </i></p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a functional block diagram of the FPGA <b>103</b>-<b>1</b> of the master node <b>100</b>-<b>1</b>. The FPGA <b>103</b>-<b>1</b> functions as a GPU reception buffer <b>120</b>, a GPU transmission buffer <b>121</b>, network transmission buffers <b>122</b> and <b>123</b>, network reception buffers <b>124</b> and <b>125</b>, a transmission unit <b>126</b>, a transmission unit <b>128</b>, and a reception unit <b>129</b>.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a functional block diagram of the FPGA <b>103</b>-<i>k </i>of the slave node <b>100</b>-<i>k </i>(k=2, . . . , N). The FPGA <b>103</b>-<i>k </i>functions as the GPU reception buffer <b>120</b>, the GPU transmission buffer <b>121</b>, the network transmission buffers <b>122</b> and <b>123</b>, the network reception buffers <b>124</b> and <b>125</b>, a transmission unit <b>126</b>, a reception unit <b>127</b>, the transmission unit <b>128</b>, and the reception unit <b>129</b>.</p><p id="p-0011" num="0010">Hereinafter, an Allreduce process will be described. The GPU <b>102</b>-<i>n </i>of each node <b>100</b>-<i>n </i>calculates gradients for weights of a model to be learned, and calculates distributed data D by totaling the gradients for each weight. The GPU <b>102</b>-<i>n </i>of each node <b>100</b>-<i>n </i>direct memory access (DMA)-transfers the distributed data D to the GPU reception buffer <b>120</b> in the FPGA <b>103</b>-<i>n </i>of the node <b>100</b>-<i>n</i>. Data stored in the GPU reception buffer <b>120</b> is transferred to either the network transmission buffer <b>122</b> or <b>123</b> having an available space.</p><p id="p-0012" num="0011">In the FPGA <b>103</b>-<i>n </i>of each node <b>100</b>-<i>n</i>, in a case that the data is stored in the network transmission buffer <b>122</b> or <b>123</b>, and either the network reception buffer <b>124</b> or <b>125</b> of the FPGA <b>103</b>-<i>n </i>is empty, a check flag is set.</p><p id="p-0013" num="0012">In a case that the check flag is set in every node <b>100</b>-<i>n </i>including the master node <b>100</b>-<b>1</b>, the transmission unit <b>126</b> in the FPGA <b>103</b>-<b>1</b> of the master node <b>100</b>-<b>1</b> retrieves the distributed data D stored in the network transmission buffer <b>122</b> or <b>123</b> in the FPGA <b>103</b>-<b>1</b>, and transmits the retrieved data as intermediate aggregated data Rt[i] to the next numbered node <b>100</b>-<b>2</b> via a communication path <b>201</b>.</p><p id="p-0014" num="0013">The reception unit <b>127</b> in the FPGA <b>103</b>-<i>k </i>of the slave node <b>100</b>-<i>k </i>(k=2, . . . , N) receives the intermediate aggregated data Rt[k&#x2212;1] from the node <b>100</b>-(<i>k</i>&#x2212;1) via the communication path <b>201</b>.</p><p id="p-0015" num="0014">An addition unit <b>131</b> in the FPGA <b>103</b>-<i>k </i>of the slave node <b>100</b>-<i>k </i>retrieves the distributed data D stored in the network transmission buffer <b>122</b> or <b>123</b> in the FPGA <b>103</b>-<i>k</i>. Then, the addition unit <b>131</b> calculates a sum of the retrieved distributed data D and the intermediate aggregated data Rt[k&#x2212;1] received from the communication path <b>201</b> to generate the intermediate aggregated data Rt[k].</p><p id="p-0016" num="0015">The transmission unit <b>126</b> in the FPGA <b>103</b>-<i>k </i>of the slave node <b>100</b>-<i>k </i>transmits the intermediate aggregated data Rt[k] generated by the addition unit <b>131</b> in the FPGA <b>103</b>-<i>k </i>to the next numbered node <b>100</b>-<i>k</i><sup>+</sup> (k<sup>+</sup>=k+1, where k<sup>+</sup>=1 in a case of k=N) via the communication path <b>201</b>.</p><p id="p-0017" num="0016">The reception unit <b>129</b> in the FPGA <b>103</b>-<b>1</b> of the master node <b>100</b>-<b>1</b> receives the intermediate aggregated data Rt[N] from the node <b>100</b>-N via the communication path <b>201</b>.</p><p id="p-0018" num="0017">The transmission unit <b>128</b> in the FPGA <b>103</b>-<b>1</b> of the master node <b>100</b>-<b>1</b> transmits the received intermediate aggregated data Rt[N] as aggregated data R to the next numbered node <b>100</b>-<b>2</b> via the communication path <b>201</b>.</p><p id="p-0019" num="0018">The reception unit <b>129</b> in the FPGA <b>103</b>-<b>1</b> of the master node <b>100</b>-<b>1</b> transfers the aggregated data R received from the node <b>100</b>-N via the communication path <b>201</b> to either the network reception buffer <b>124</b> or <b>125</b> having an available space in the FPGA <b>103</b>-<b>1</b>. The data stored in the network reception buffer <b>124</b> or <b>125</b> is transferred to the GPU transmission buffer <b>121</b> in the FPGA <b>103</b>-<b>1</b>. The data stored in the GPU transmission buffer <b>121</b> is DMA-transferred to the GPU <b>102</b>-<b>1</b>.</p><p id="p-0020" num="0019">The reception unit <b>129</b> in the FPGA <b>103</b>-<i>k </i>of the slave node <b>100</b>-<i>k </i>(k=2, . . . , N) receives the aggregated data R from the node <b>100</b>-(<i>k</i>&#x2212;1) via the communication path <b>201</b>.</p><p id="p-0021" num="0020">The transmission unit <b>128</b> in the FPGA <b>103</b>-<i>k </i>of the slave node <b>100</b>-<i>k </i>transmits the received aggregated data R to the next numbered node <b>100</b>-<i>k</i><sup>+</sup> (k<sup>+</sup>=n+1, where n+=1 in a case of k=N) via the communication path <b>201</b>.</p><p id="p-0022" num="0021">The reception unit <b>129</b> in the FPGA <b>103</b>-<i>k </i>of the slave node <b>100</b>-<i>k </i>transfers the aggregated data R received from the node <b>100</b>-(<i>k</i>&#x2212;1) via the communication path <b>201</b> to either the network reception buffer <b>124</b> or <b>125</b> having an available space in the FPGA <b>103</b>-<i>k</i>. The data stored in the network reception buffer <b>124</b> or <b>125</b> is transferred to the GPU transmission buffer <b>121</b> in the FPGA <b>103</b>-<i>k</i>. The data stored in the GPU transmission buffer <b>121</b> is DMA-transferred to the GPU <b>102</b>-<i>k. </i></p><p id="p-0023" num="0022">In the above Allreduce process, a file descriptor in the DMA transfer needs to be specified in a one-to-one manner. For this reason, in the distributed deep learning system of related art illustrated in <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the file descriptors needs to be specified with times being shifted for performing the DMA transfer in order to perform the Allreduce process by a plurality of GPUs using the FPGAs, leading to a problem of large communication overhead.</p><heading id="h-0004" level="1">CITATION LIST</heading><heading id="h-0005" level="1">Non Patent Literature</heading><p id="p-0024" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0023">NPL 1: Kenji Tanaka, et al., &#x201c;Research Poster: (RP04) Distributed Deep Learning with FPGA Ring Allreduce&#x201d;, ISC 2019, 2019, https://2019.isc-program.com/presentation/?id=post120&#x26;sess=sess182.</li></ul></p><heading id="h-0006" level="1">SUMMARY</heading><heading id="h-0007" level="1">Technical Problem</heading><p id="p-0025" num="0024">Embodiments of the present invention are made to solve the above problem and has an object to provide a distributed deep learning system capable of reducing overhead of Allreduce process.</p><heading id="h-0008" level="1">Means for Solving the Problem</heading><p id="p-0026" num="0025">A distributed deep learning system according to embodiments of the present invention (first to fifth embodiments) includes a plurality of nodes connected with each other via a network, wherein each node of the nodes including a plurality of GPUs configured to generate distributed data per weight of a model to be learned, a plurality of first reception buffers configured to store the distributed data from the GPUs, a plurality of first transmission buffers configured to store the distributed data transferred from the first reception buffers, a plurality of second reception buffers configured to store aggregated data received from another node, a second transmission buffer configured to store the aggregated data transferred from any of the second reception buffers, a monitoring unit configured to set a check flag when data is stored in any of the first transmission buffers and any of the second reception buffers has an available space, a first transmission unit configured to transmit, when the check flag is set in the node itself and every other node in a case that the node functions as the first numbered node among the plurality of nodes, the distributed data stored in any of the first transmission buffers as first aggregated data to the next numbered node, and transmit, in a case that the node functions as a node except for the first numbered node among the plurality of nodes, updated first aggregated data to the next numbered node, a first reception unit configured to receive, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, the first aggregated data from another node, an addition unit configured to calculate, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, a sum of the distributed data stored in the first transmission buffer and the first aggregated data received by the first reception unit per weight to generate the updated first aggregated data, a second reception unit configured to receive the updated first aggregated data in the case that the node functions as the first numbered node among the plurality of nodes, and receives second aggregated data in the case that the node functions as the node except for the first numbered node among the plurality of nodes, a second transmission unit configured to transmit, in the case that the node functions as the first numbered node among the plurality of nodes, the first aggregated data received by the second reception unit as the second aggregated data to the next numbered node, and transmit, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, the second aggregated data received by the second reception unit to the next numbered node, a first transfer unit configured to transfer the distributed data stored in the first reception buffers to the first transmission buffers, and DMA-transfer the aggregated data stored in the second transmission buffer to the plurality of GPUs, and a second transfer unit configured to transfer the aggregated data stored in the second reception buffers to the second transmission buffer, and the plurality of GPUs DMA-transfer the distributed data to the plurality of first reception buffers.</p><p id="p-0027" num="0026">In an exemplary configuration of the distributed deep learning system according to embodiments of the present invention (second embodiment), a plurality of communication paths are configured in the network, each node includes the plurality of GPUs, the first reception buffers the number of which is the same as the number of the communication paths, the plurality of first transmission buffers provided per one communication path, the plurality of second reception buffers provided per one communication path, the second transmission buffers the number of which is the same as the number of the communication paths, the monitoring unit, the first and second transmission units, the first and second reception units, the addition unit, the first transfer unit, and the second transfer unit, each of the GPUs includes a third transmission unit configured to DMA-transfer the distributed data to respective corresponding first reception buffers, a third reception unit configured to receive the second aggregated data DMA-transferred by the first transfer unit, a fourth transmission unit configured to transmit the second aggregated data received by the third reception unit to another GPU, a fourth reception unit configured to receive the second aggregated data transmitted from another GPU, an aggregation processing unit configured to calculate a sum of the second aggregated data received by the third reception unit and the second aggregated data received by the fourth reception unit per weight to generated third aggregated data, and an updating unit configured to update the model in accordance with the third aggregated data, the first transfer unit transfers the distributed data stored in the first reception buffer corresponding to one communication path to the first transmission buffer corresponding to the communication path, and DMA-transfers the second aggregated data stored in the second transmission buffer corresponding to one communication path to the GPU corresponding to the communication path, the second transfer unit transfers the second aggregated data stored in the second reception buffer corresponding to one communication path to the second transmission buffer corresponding to the communication path, when the data is stored in the first transmission buffer and the second reception buffer has an available space, the first transmission buffer and the second reception buffer corresponding to the identical communication path, the monitoring unit sets the check flag corresponding to the communication path, in the case that the node functions as the first numbered node among the plurality of nodes when the check flag corresponding to the identical communication path is set in the node itself and every other node, and the check flag corresponding to another communication path is not set in at least one node, the first transmission unit transmits the distributed data stored in the first transmission buffer corresponding to the identical communication path as the first aggregated data to the next numbered node via the identical communication path, and the addition unit calculates a sum of the distributed data stored in the first transmission buffer corresponding to one communication path and the first aggregated data received from the communication path by the first reception unit per weight to generate the updated first aggregated data.</p><p id="p-0028" num="0027">In an exemplary configuration of the distributed deep learning system according to embodiments of the present invention (third embodiment), a plurality of communication paths are configured in the network, each node includes the plurality of GPUs, the first reception buffers the number of which is the same as the number of the communication paths, the plurality of first transmission buffers provided per one communication path, the plurality of second reception buffers provided per one communication path, the second transmission buffers the number of which is the same as the number of the communication paths, the monitoring unit, the first and second transmission units, the first and second reception units, the addition unit, the first transfer unit, and the second transfer unit, each of the GPUs includes a third transmission unit configured to DMA-transfer the distributed data to any of the plurality of first reception buffers, a third reception unit configured to receive the second aggregated data DMA-transferred by the first transfer unit, a fourth transmission unit configured to transmit the second aggregated data received by the third reception unit to another GPU, a fourth reception unit configured to receive the second aggregated data transmitted from another GPU, an aggregation processing unit configured to calculate a sum of the second aggregated data received by the third reception unit and the second aggregated data received by the fourth reception unit per weight to generated third aggregated data, and an updating unit configured to update the model in accordance with the third aggregated data, the first transfer unit transfers the distributed data stored in the first reception buffer corresponding to one communication path to the first transmission buffer corresponding to the communication path, and DMA-transfers the second aggregated data stored in the second transmission buffer corresponding to one communication path to the GPU corresponding to the second aggregated data, the second transfer unit transfers the second aggregated data stored in the second reception buffer corresponding to one communication path to the second transmission buffer corresponding to the communication path, when the data is stored in the first transmission buffer and the second reception buffer has an available space, the first transmission buffer and the second reception buffer corresponding to the identical communication path, the monitoring unit sets the check flag corresponding to the communication path, in the case that the node functions as the first numbered node among the plurality of nodes when the check flag corresponding to the identical communication path is set in the node itself and every other node, and the check flag corresponding to another communication path is not set in at least one node, the first transmission unit transmits the distributed data stored in the first transmission buffer corresponding to the identical communication path as the first aggregated data to the next numbered node via the identical communication path, and in a case that the GPU deriving the first aggregated data received from another node by the first reception unit is in the same combination with the GPU generating the distributed data and the distributed data is stored in the first transmission buffer, the addition unit calculates a sum of the distributed data and the first aggregated data received by the first reception unit per weight to generate the updated first aggregated data.</p><p id="p-0029" num="0028">In an exemplary configuration of the distributed deep learning system according to embodiments of the present invention (fourth embodiment), a plurality of communication paths are configured in the network, each node includes the plurality of GPUs, the first reception buffers the number of which is the same as the number of the communication paths, the plurality of first transmission buffers provided per one communication path, the plurality of second reception buffers provided per one communication path, the second transmission buffers the number of which is the same as the number of the communication paths, the monitoring unit, the first and second transmission units, the first and second reception units, the addition unit, the first transfer unit, and the second transfer unit, each of the GPUs includes a third transmission unit configured to DMA-transfer the distributed data to the first reception buffer not busy among the plurality of reception buffers, a third reception unit configured to receive the second aggregated data DMA-transferred by the first transfer unit, and an updating unit configured to update the model in accordance with the second aggregated data received by the third reception unit, the first transfer unit transfers the distributed data stored in the first reception buffer corresponding to one communication path to the first transmission buffer corresponding to the communication path, and DMA-transfers the second aggregated data stored in the second transmission buffer corresponding to one communication path to the GPU corresponding to the communication path, the second transfer unit transfers the second aggregated data stored in the second reception buffer corresponding to one communication path to the second transmission buffer corresponding to the communication path, when the data is stored in the first transmission buffer and the second reception buffer has an available space, the first transmission buffer and the second reception buffer corresponding to the identical communication path, the monitoring unit sets the check flag corresponding to the communication path, in the case that the node functions as the first numbered node among the plurality of nodes when all check flags are set in the node itself and every other node, the first transmission unit transmits the distributed data stored in the plurality of first transmission buffers as the first aggregated data to the next numbered node via the communication paths corresponding to the first transmission buffers storing the distributed data, and the addition unit calculates a sum of the distributed data stored in the plurality of first transmission buffers corresponding to the plurality of communication paths and the first aggregated data received from the plurality of communication paths by the first reception unit per weight to generate the updated first aggregated data.</p><p id="p-0030" num="0029">In an exemplary configuration of the distributed deep learning system according to embodiments of the present invention (fifth embodiment), a plurality of communication paths are configured in the network, each node includes the plurality of GPUs, the first reception buffers the number of which is the same as the number of the communication paths, the plurality of first transmission buffers provided per one communication path, the plurality of second reception buffers provided common to the plurality of communication paths, the second transmission buffer provided common to the plurality of communication paths, the monitoring unit, the first and second transmission units, the first and second reception units, the addition unit, the first transfer unit, and the second transfer unit, each of the GPUs includes a third transmission unit configured to DMA-transfer the distributed data to the first reception buffer not busy among the plurality of reception buffers, a third reception unit configured to receive the second aggregated data DMA-transferred by the first transfer unit, and an updating unit configured to update the model in accordance with the second aggregated data received by the third reception unit, the first transfer unit transfers the distributed data stored in the first reception buffer corresponding to one communication path to the first transmission buffer corresponding to the communication path, and DMA-transfers the second aggregated data stored in the second transmission buffer to the plurality of GPUs, the second transfer unit transfers the second aggregated data stored in any of the plurality of second reception buffers to the second transmission buffer, when the data is stored in the first transmission buffer and the second reception buffer has an available space, the first transmission buffer and the second reception buffer corresponding to the identical communication path, the monitoring unit sets the check flag corresponding to the communication path, in the case that the node functions as the first numbered node among the plurality of nodes when all check flags are set in the node itself and every other node, the first transmission unit transmits the distributed data stored in the plurality of first transmission buffers as the first aggregated data to the next numbered node via the communication paths corresponding to the first transmission buffers storing the distributed data, and the addition unit calculates a sum of the distributed data stored in the plurality of first transmission buffers corresponding to the plurality of communication paths and the first aggregated data received from the plurality of communication paths by the first reception unit per weight to generate the updated first aggregated data.</p><p id="p-0031" num="0030">A distributed deep learning system according to embodiments of the present invention (sixth embodiment) includes a plurality of nodes connected with each other via a network, each of the nodes includes a plurality of GPUs configured to generate distributed data per weight of a model to be learned, a plurality of first reception buffers configured to store the distributed data from the GPUs, a first addition unit configured to calculate a sum of a plurality of pieces of the distributed data transferred from the plurality of first reception buffers per weight to generate a first aggregated data, a plurality of first transmission buffers configured to store the first aggregated data, a plurality of second reception buffers configured to store aggregated data received from another node, a second transmission buffer configured to store the aggregated data transferred from any of the second reception buffers, a monitoring unit configured to set a check flag when data is stored in any of the first transmission buffers and any of the second reception buffers has an available space, a first transmission unit configured to transmit, when the check flag is set in the node itself and every other node in a case that the node functions as the first numbered node among the plurality of nodes, the first aggregated data stored in any of the first transmission buffers as second aggregated data to the next numbered node, and transmit, in a case that the node functions as a node except for the first numbered node among the plurality of nodes, updated second aggregated data to the next numbered node, a first reception unit configured to receive, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, the second aggregated data from another node, a second addition unit configured to calculate, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, a sum of the first aggregated data stored in the first transmission buffer and the second aggregated data received by the first reception unit per weight to generate the updated first aggregated data, a second reception unit configured to receive the updated second aggregated data in the case that the node functions as the first numbered node among the plurality of nodes, and receives third aggregated data in the case that the node functions as the node except for the first numbered node among the plurality of nodes, a second transmission unit configured to transmit, in the case that the node functions as the first numbered node among the plurality of nodes, the second aggregated data received by the second reception unit as the third aggregated data to the next numbered node, and transmit, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, the third aggregated data received by the second reception unit to the next numbered node, a first transfer unit configured to transfer the distributed data stored in the first reception buffers to the first addition unit, and DMA-transfer the third aggregated data stored in the second transmission buffer to the plurality of GPUs, and a second transfer unit configured to transfer the third aggregated data stored in the second reception buffers to the second transmission buffer, wherein the plurality of GPUs DMA-transfer the distributed data to the plurality of first reception buffers, and updates the model in accordance with the third aggregated data.</p><p id="p-0032" num="0031">In an exemplary configuration of the distributed deep learning system according to embodiments of the present invention (sixth embodiment), one communication path is configured in the network, each node includes the plurality of GPUs, the first reception buffers the number of which is the same as the number of the GPUs, the plurality of first reception buffers, the plurality of second reception buffers, the second transmission buffers the number of which is the same as the number of the communication path, the monitoring unit, the first and second transmission units, the first and second reception units, the addition unit, the first transfer unit, and the second transfer unit, each of the GPUs includes a third transmission unit configured to DMA-transfer the distributed data to the first reception buffer not busy among the plurality of reception buffers, a third reception unit configured to receive the third aggregated data DMA-transferred by the first transfer unit, and an updating unit configured to update the model in accordance with the third aggregated data received by the third reception unit, the second transfer unit transfers the third aggregated data stored in any of the plurality of second reception buffers to the second transmission buffer, when the data is stored in the first transmission buffer and the second reception buffer has an available space, the first transmission buffer and the second reception buffer corresponding to the identical communication path, the monitoring unit sets the check flag corresponding to the communication path, and the second addition unit calculates a sum of the first aggregated data stored in any of the plurality of first transmission buffers and the second aggregated data received from the communication path by the first reception unit per weight to generate the updated second aggregated data.</p><heading id="h-0009" level="1">Effects of Embodiments of the Invention</heading><p id="p-0033" num="0032">According to embodiments of the present invention, a DMA wait time is reduced in each GPU of each node, and thus, each GPU can perform other processing by a reduced DMA wait time. In embodiments of the present invention, a band of the network can be effectively used by increasing a first transmission buffer than in the current system. As a result, embodiments of the present invention can reduce overhead of the Allreduce process.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0010" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system according to a first embodiment of the present invention.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram of a GPU according to the first embodiment of the present invention.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a functional block diagram of an FPGA of a master node according to the first embodiment of the present invention.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a functional block diagram of an FPGA of a slave node according to the first embodiment of the present invention.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart illustrating a sample data input process, a gradient calculation process, and an intra-GPU aggregation process of each GPU of the node according to the first embodiment of the present invention.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating an inter-node Allreduce process for the master node according to the first embodiment of the present invention.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating an inter-node Allreduce process for the slave node according to the first embodiment of the present invention.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart illustrating an inter-GPU Allreduce process and a weight updating process in each node according to the first embodiment of the present invention.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating an inter-GPU Allreduce process in each node according to the first embodiment of the present invention.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system according to a third embodiment of the present invention.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a functional block diagram of a GPU according to the third embodiment of the present invention.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a functional block diagram of an FPGA of a master node according to the third embodiment of the present invention.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a functional block diagram of an FPGA of a slave node according to the third embodiment of the present invention.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system according to a fourth embodiment of the present invention.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a functional block diagram of a GPU according to the fourth embodiment of the present invention.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a functional block diagram of an FPGA of a master node according to the fourth embodiment of the present invention.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a functional block diagram of an FPGA of a slave node according to the fourth embodiment of the present invention.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a flowchart illustrating a weight updating process in a node according to the fourth embodiment of the present invention.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system according to a fifth embodiment of the present invention.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a functional block diagram of an FPGA of a master node according to the fifth embodiment of the present invention.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a functional block diagram of an FPGA of a slave node according to the fifth embodiment of the present invention.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system according to a sixth embodiment of the present invention.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a functional block diagram of an FPGA of a master node according to the sixth embodiment of the present invention.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a functional block diagram of an FPGA of a slave node according to the sixth embodiment of the present invention.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a flowchart illustrating an inter-node Allreduce process for the master node according to the sixth embodiment of the present invention.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart illustrating an inter-node Allreduce process for the slave node according to the sixth embodiment of the present invention.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a block diagram illustrating an exemplary configuration of a computer that implements the nodes according to the first to sixth embodiments of the present invention.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system of related art.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a functional block diagram of an FPGA of a master node of the distributed deep learning system of related art.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a functional block diagram of an FPGA of a slave node of the distributed deep learning system of related art.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0011" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading><heading id="h-0012" level="1">First Embodiment</heading><p id="p-0064" num="0063">Hereinafter, embodiments of the present invention will be described with reference to the drawings. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system according to a first embodiment of the present invention. The distributed deep learning system includes N nodes <b>1</b>-<i>n </i>(n=1, . . . , N) and a network <b>2</b> connecting the N nodes <b>1</b>-<i>n </i>to each other (where N is an integer of 2 or more, and N=4 in the present embodiment).</p><p id="p-0065" num="0064">In the present embodiment, the node <b>1</b>-<b>1</b> is a master node and the nodes <b>1</b>-<b>2</b> to <b>1</b>-<b>4</b> are slave nodes. Two communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> are configured in the network <b>2</b>. Note that, in embodiments of the present invention, a &#x201c;node&#x201d; refers to a device such as a server distributively disposed on a network.</p><p id="p-0066" num="0065">The master node <b>1</b>-<b>1</b> includes a CPU <b>10</b>-<b>1</b>, GPUs <b>11</b>-<b>1</b>-<b>1</b> and <b>11</b>-<b>1</b>-<b>2</b>, and an FPGA <b>12</b>-<b>2</b>.</p><p id="p-0067" num="0066">The slave node <b>1</b>-<i>k </i>(k=2, . . . , N) includes a CPU <b>10</b>-<i>k</i>, GPUs <b>11</b>-<i>k</i>-<b>1</b> and <b>11</b>-<i>k</i>&#x2212;2, and an FPGA <b>12</b>-<i>k. </i></p><p id="p-0068" num="0067">In the present embodiment, each node is provided with J GPUs (where J is an integer of 2 or more, and J=2 in the present embodiment). <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram of the GPU <b>11</b>-<i>n</i>-<i>j </i>(n=1, . . . , N, j=1, . . . , J). The GPU <b>11</b>-<i>n</i>-<i>j </i>functions as a sample input unit <b>110</b> that receives sample data for learning from a data collection node (not illustrated), a gradient calculation processing unit <b>11</b> that calculates a gradient of a loss function of a model <b>13</b>-<i>n </i>(neural network) to be learned per sample data piece with respect to each of weights of the model <b>13</b>-<i>n </i>when the sample data is input, an aggregation processing unit <b>112</b> that generates and holds distribution data per weight, the distribution data being a numerical value obtained by aggregating gradients per sample data piece, a weight updating processing unit <b>113</b> that updates the weights of the model <b>13</b>-<i>n</i>, a transmission unit <b>114</b> (third transmission unit), a reception unit <b>115</b> (third reception unit), a transmission unit <b>116</b> (fourth transmission unit), a reception unit <b>117</b> (fourth reception unit), and an aggregation processing unit <b>118</b>.</p><p id="p-0069" num="0068">The model <b>13</b>-<i>n </i>(neural network) is a mathematical model built by the CPU <b>10</b>-<i>n </i>in a software manner.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a functional block diagram of the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b>. The FPGA <b>12</b>-<b>1</b> functions as GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b> (first reception buffers), GPU transmission buffers <b>121</b>-<b>1</b> and <b>121</b>-<b>2</b> (second transmission buffers), network transmission buffers <b>122</b>-<b>1</b>, <b>122</b>-<b>2</b>, <b>123</b>-<b>1</b>, and <b>123</b>-<b>2</b> (first transmission buffers), network reception buffers <b>124</b>-<b>1</b>, <b>124</b>-<b>2</b>, <b>125</b>-<b>1</b>, and <b>125</b>-<b>2</b> (second reception buffers), a transmission unit <b>126</b> (first transmission unit), a transmission unit <b>128</b> (second transmission unit), a reception unit <b>129</b> (second reception unit), a monitoring unit <b>130</b>, a transfer unit <b>132</b> (first transfer unit), and a transfer unit <b>133</b> (second transfer unit).</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a functional block diagram of the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>(k=2, . . . , N). The FPGA <b>12</b>-<i>k </i>functions as the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b>, the GPU transmission buffers <b>121</b>-<b>1</b> and <b>121</b>-<b>2</b>, the network transmission buffers <b>122</b>-<b>1</b>, <b>122</b>-<b>2</b>, <b>123</b>-<b>1</b>, and <b>123</b>-<b>2</b>, the network reception buffers <b>124</b>-<b>1</b>, <b>124</b>-<b>2</b>, <b>125</b>-<b>1</b>, and <b>125</b>-<b>2</b>, the transmission unit <b>126</b>, a reception unit <b>127</b> (first reception unit), the transmission unit <b>128</b>, the reception unit <b>129</b>, the monitoring unit <b>130</b>, an addition unit <b>131</b>, the transfer unit <b>132</b>, and the transfer unit <b>133</b>.</p><p id="p-0072" num="0071">In the present embodiment, the number of GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b> in the FPGA <b>12</b>-<i>n </i>of each node <b>1</b>-<i>n </i>is the same as the number of number of communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> configured in the network <b>2</b>. The number of GPU transmission buffers <b>121</b>-<b>1</b> and <b>121</b>-<b>2</b> in the FPGA <b>12</b>-<i>n </i>of each node <b>1</b>-<i>n </i>is the same as the number of number of communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b>.</p><p id="p-0073" num="0072">The FPGA <b>12</b>-<i>n </i>of each node <b>1</b>-<i>n </i>is provided with two network transmission buffers <b>122</b>-<b>1</b> and <b>123</b>-<b>1</b> corresponding to the communication path <b>20</b>-<b>1</b> and two network reception buffers <b>124</b>-<b>1</b> and <b>125</b>-<b>1</b> corresponding to the communication path <b>20</b>-<b>1</b>. Furthermore, the FPGA <b>12</b>-<i>n </i>of each node <b>1</b>-<i>n </i>is provided with two network transmission buffers <b>122</b>-<b>2</b> and <b>123</b>-<b>2</b> corresponding to the communication path <b>20</b>-<b>2</b> and two network reception buffers <b>124</b>-<b>2</b> and <b>125</b>-<b>2</b> corresponding to the communication path <b>20</b>-<b>2</b>.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart illustrating a sample data input process, a gradient calculation process, and an intra-GPU aggregation process in each GPU <b>11</b>-<i>n</i>-<i>j </i>(n=1, . . . , N, j=1, . . . , J) of the node <b>1</b>-<i>n. </i></p><p id="p-0075" num="0074">The sample input unit <b>11</b><i>o </i>in each GPU <b>11</b>-<i>n</i>-<i>j </i>of the node <b>1</b>-<i>n </i>inputs different S pieces of sample data x[n, s] (s=1, . . . , S) (S is an integer of 2 or more) per mini batch from a data collecting node (not illustrated) to the gradient calculation processing unit in (step S<b>100</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>).</p><p id="p-0076" num="0075">Note that the present invention is not limited to a sample data collecting method performed by a data collecting node and a method of dividing collected sample data into N&#xd7;J sets and broadcasting the sets to the GPU <b>11</b>-<i>n</i>-<i>j </i>of the node <b>1</b>-<i>n</i>, and any method can be applied.</p><p id="p-0077" num="0076">When sample data x[n, s] is input, the gradient calculation processing unit in in each GPU <b>11</b>-<i>n</i>-<i>j </i>of the node <b>1</b>-<i>n </i>calculates a gradient Gj[m, n, s] of a loss function of the mode <b>13</b>-<i>n </i>per sample data piece x[n, s] with respect to each of M weights w[m] (m=1, . . . , M) (M is an integer of 2 or more) of the model <b>13</b>-<i>n </i>to be learned (step S<b>101</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>).</p><p id="p-0078" num="0077">The weights w[m] of the model <b>13</b>-<i>n</i>, the loss function that is an indicator indicating the degree of poorness of performance of the model <b>13</b>-<i>n</i>, and the gradient Gj[m, n, s] of the loss function are well-known techniques, and thus, detailed description thereof will be omitted.</p><p id="p-0079" num="0078">Subsequently, the aggregation processing unit <b>112</b> in each GPU <b>11</b>-<i>n</i>-<i>j </i>of the node <b>1</b>-<i>n </i>generates and holds distributed data Dj[m, n] per weight w[m], the distributed data Dj[m, n] being a numerical value obtained by aggregating a gradient G[m, n, s] per sample data piece (step S<b>102</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>). A calculation equation for the distributed data Dj[m, n] is as follows.</p><p id="p-0080" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Math 1<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0081" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Dj</i>[<i>m,n</i>]=&#x3a3;<sub>s=1, . . . ,S</sub><i>Gj</i>[<i>m,n,s</i>]&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0082" num="0079">Note that the gradient calculation process performed by the gradient calculation processing unit <b>111</b> and the intra-GPU aggregation process performed by the aggregation processing unit <b>112</b> can be performed in a pipelined manner in units of sample data (the gradient calculation process for any sample data piece and the intra-GPU aggregation process of aggregating gradients obtained from sample data piece immediately prior to the former sample data piece can be performed at the same time).</p><p id="p-0083" num="0080">Furthermore, each node <b>1</b>-<i>n </i>performs an inter-node Allreduce process after generating the distributed data Dj [m, n].</p><p id="p-0084" num="0081"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating the inter-node Allreduce process for the master node <b>1</b>-<b>1</b>, and <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating the inter-node Allreduce process for the slave node <b>1</b>-<i>k </i>(k=2, . . . , N).</p><p id="p-0085" num="0082">The transmission unit <b>114</b> in each GPU <b>11</b>-<b>1</b>-<i>j </i>of the master node <b>1</b>-<b>1</b> direct memory access (DMA)-transfers M pieces of distribution data Dj[m, 1] (m=1, . . . , M, j=1, . . . , J) generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b>-<b>1</b>-<i>j </i>to any one of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> (step S<b>200</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The respective GPUs <b>11</b>-<b>1</b>-<i>j </i>asynchronously DMA transfer data to the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b> different from each other. In a case that the DMA-transferring is congested, DMA-transferring thereafter is queued, and then, is started as soon as the prior DMA-transferring ends.</p><p id="p-0086" num="0083">The transfer unit <b>132</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> monitors the network transmission buffers <b>122</b>-<b>1</b>, <b>122</b>-<b>2</b>, <b>123</b>-<b>1</b>, and <b>123</b>-<b>2</b> in the FPGA <b>12</b>-<b>1</b>. In a case that data is stored in the GPU reception buffer <b>120</b>-<b>1</b> in the FPGA <b>12</b>-<b>1</b> and any of the network transmission buffers <b>122</b>-<b>1</b> and <b>123</b>-<b>1</b> is empty, the transfer unit <b>132</b> transfers the data stored in the GPU reception buffer <b>120</b>-<b>1</b> to either the network transmission buffer <b>122</b>-<b>1</b> or <b>123</b>-<b>1</b> having an available space (step S<b>201</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). In a case that data is stored in the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b>-<b>1</b> and any of the network transmission buffers <b>122</b>-<b>2</b> and <b>123</b>-<b>2</b> is empty, the transfer unit <b>132</b> in the FPGA <b>12</b>-<b>1</b> transfers the data stored in the GPU reception buffer <b>120</b>-<b>2</b> to either the network transmission buffer <b>122</b>-<b>2</b> or <b>123</b>-<b>2</b> having an available space (step S<b>201</b>).</p><p id="p-0087" num="0084">Similarly, the transmission unit <b>114</b> in each GPU <b>11</b>-<i>k</i>-<i>j </i>of the slave node <b>1</b>-<i>k </i>DMA-transfers M pieces of distribution data Dj[m, k] (m=1, . . . , M, j=1, . . . , J) generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b>-<i>k</i>-<i>j </i>to any one of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>(step S<b>300</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0088" num="0085">The present embodiment gives a description assuming that the transmission unit <b>114</b> in each GPU <b>11</b>-<i>n</i>-<b>1</b> of the node <b>1</b>-<i>n </i>transfers the distributed data D<b>1</b>[<i>m, n</i>] to the GPU reception buffer <b>120</b>-<b>1</b> in the FPGA <b>12</b>-<i>n</i>, and the transmission unit <b>114</b> in each GPU <b>11</b>-<i>n</i>-<b>2</b> of the node <b>1</b>-<i>n </i>transfers distributed data D<b>2</b>[<i>m, n</i>] to the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b>-<i>n. </i></p><p id="p-0089" num="0086">In a case that data is stored in the GPU reception buffer <b>120</b>-<b>1</b> in the FPGA <b>12</b>-<i>k </i>and any of the network transmission buffers <b>122</b>-<b>1</b> and <b>123</b>-<b>1</b> is empty, the transfer unit <b>132</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>transfers the data stored in the GPU reception buffer <b>120</b>-<b>1</b> to either the network transmission buffer <b>122</b>-<b>1</b> or <b>123</b>-<b>1</b> having an available space (step S<b>301</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). In a case that data is stored in the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b>-<i>k </i>and any of the network transmission buffers <b>122</b>-<b>2</b> and <b>123</b>-<b>2</b> is empty, the transfer unit <b>132</b> in the FPGA <b>12</b>-<i>k </i>transfers the data stored in the GPU reception buffer <b>120</b>-<b>2</b> to either the network transmission buffer <b>122</b>-<b>2</b> or <b>123</b>-<b>2</b> having an available space (step S<b>301</b>).</p><p id="p-0090" num="0087">In a case that data is stored in the network transmission buffer <b>122</b>-<b>1</b> or <b>123</b>-<b>1</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> and any of the network reception buffers <b>124</b>-<b>1</b> and <b>125</b>-<b>1</b> in the FPGA <b>12</b>-<b>1</b> is empty (YES in step S<b>202</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>), the monitoring unit <b>130</b> in the FPGA <b>12</b>-<b>1</b> sets a check flag F<b>1</b> corresponding to the communication path <b>20</b>-<b>1</b> (step S<b>203</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). In a case that data is stored in the network transmission buffer <b>122</b>-<b>2</b> or <b>123</b>-<b>2</b> in the FPGA <b>12</b>-<b>1</b> and any of the network reception buffers <b>124</b>-<b>2</b> and <b>125</b>-<b>2</b> in the FPGA <b>12</b>-<b>1</b> is empty (YES in step S<b>202</b>), the monitoring unit <b>130</b> in the FPGA <b>12</b>-<b>1</b> sets a check flag F<b>2</b> corresponding to the communication path <b>20</b>-<b>2</b> (step S<b>203</b>).</p><p id="p-0091" num="0088">Similarly, in a case that data is stored in the network transmission buffer <b>122</b>-<b>1</b> or <b>123</b>-<b>1</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>and any of the network reception buffers <b>124</b>-<b>1</b> and <b>125</b>-<b>1</b> in the FPGA <b>12</b>-<i>k </i>is empty (YES in step S<b>302</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>), the monitoring unit <b>130</b> in the FPGA <b>12</b>-<i>k </i>sets the check flag F<b>1</b> corresponding to the communication path <b>20</b>-<b>1</b> (step S<b>303</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). In a case that data is stored in the network transmission buffer <b>122</b>-<b>2</b> or <b>123</b>-<b>2</b> in the FPGA <b>12</b>-<i>k </i>and any of the network reception buffers <b>124</b>-<b>2</b> and <b>125</b>-<b>2</b> in the FPGA <b>12</b>-<i>k </i>is empty (YES in step S<b>302</b>), the monitoring unit <b>130</b> in the FPGA <b>12</b>-<i>k </i>sets the check flag F<b>2</b> corresponding to the communication path <b>20</b>-<b>2</b> (step S<b>303</b>).</p><p id="p-0092" num="0089">The monitoring unit <b>130</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> monitors the check flag that is managed by the monitoring unit <b>130</b> in the FPGA <b>12</b>-<i>k </i>of each slave node <b>1</b>-<i>k</i>, and instructs the transmission unit <b>126</b> in the FPGA <b>12</b>-<b>1</b> to transmit the data in a case that the check flag F<b>1</b> is set in every node <b>1</b>-<i>n </i>including the master node <b>1</b>-<b>1</b> itself (YES in step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The transmission unit <b>126</b> in the FPGA <b>12</b>-<b>1</b> retrieves the distributed data D<b>1</b>[<i>m, </i>1] stored in the network transmission buffer <b>122</b>-<b>1</b> or <b>123</b>-<b>1</b> in the FPGA <b>12</b>-<b>1</b>, and transmits the retrieved data as intermediate aggregated data Rt<b>1</b>[<i>m, </i>1] to the next numbered node <b>1</b>-<b>2</b> via the communication path <b>20</b>-<b>1</b> (step S<b>205</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The intermediate aggregated data Rt<b>1</b>[<i>m, </i>1] at this time is the same as the distributed data D<b>1</b>[<i>m, </i>1].</p><p id="p-0093" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Rt</i>1[<i>m,</i>1]=<i>D</i>1[<i>m,</i>1]&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0094" num="0090">The monitoring unit <b>130</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> instructs the transmission unit <b>126</b> in the FPGA <b>12</b>-<b>1</b> to transmit the data in a case that the check flag F<b>2</b> is set in every node <b>1</b>-<i>n </i>including the master node <b>1</b>-<b>1</b> itself (YES in step S<b>204</b>). The transmission unit <b>126</b> in the FPGA <b>12</b>-<b>1</b> retrieves the distributed data D<b>2</b>[<i>m, </i>1] stored in the network transmission buffer <b>122</b>-<b>2</b> or <b>123</b>-<b>2</b> in the FPGA <b>12</b>-<b>1</b>, and transmits the retrieved data as intermediate aggregated data Rt<b>2</b>[<i>m, </i>1] to the next numbered node <b>1</b>-<b>2</b> via the communication path <b>20</b>-<b>2</b> (step S<b>205</b>).</p><p id="p-0095" num="0091">Next, the reception unit <b>127</b> in the FPGA <b>12</b>-<i>i </i>of the node <b>1</b>-<i>i </i>(i=2, . . . , N&#x2212;1) that is an intermediate one of the plurality of slave nodes <b>1</b>-<i>k </i>(k=2, . . . , N) excluding the N-th node receives the intermediate aggregated data Rt<b>1</b>[<i>m, i&#x2212;</i>1] (m=1, . . . , M) from the node <b>1</b>-(<i>i</i>&#x2212; 1) via the communication path <b>20</b>-<b>1</b> (step S<b>304</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0096" num="0092">The addition unit <b>131</b> in the FPGA <b>12</b>-<i>i </i>of the slave node <b>1</b>-<i>i </i>(i=2, . . . , N&#x2212;1) retrieves the distributed data D<b>1</b>[<i>m, i</i>] stored in the network transmission buffer <b>122</b>-<b>1</b> or <b>123</b>-<b>1</b> in the FPGA <b>12</b>-<i>i</i>. Then, the addition unit <b>131</b> calculates a sum of the retrieved distributed data D<b>1</b>[<i>m, i</i>] and the intermediate aggregated data Rt<b>1</b>[<i>m, i&#x2212;</i>1] received from the communication path <b>20</b>-<b>1</b> per corresponding weight w[m] to generate the intermediate aggregated data Rt<b>1</b>[<i>m, i</i>] (step S<b>305</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). That is, the intermediate aggregated data Rt<b>1</b>[<i>m, i</i>] is constituted by M numerical values. A calculation equation for the intermediate aggregated data Rt<b>1</b>[<i>m, i</i>] is as follows.</p><p id="p-0097" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Rt</i>1[<i>m,i</i>]=<i>Rt</i>1[<i>m,i&#x2212;</i>1]+<i>D</i>1[<i>m,i</i>]&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0098" num="0093">Then, the transmission unit <b>126</b> in the FPGA <b>12</b>-<i>i </i>of the slave node <b>1</b>-<i>i </i>transmits the intermediate aggregated data Rt<b>1</b>[<i>m, i</i>] generated by the addition unit <b>131</b> in the FPGA <b>12</b>-<i>i </i>in response to the data reception from the communication path <b>20</b>-<b>1</b>, to the next numbered node <b>1</b>-(<i>i</i>+1) via the communication path <b>20</b>-<b>1</b> (step S<b>306</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0099" num="0094">Similarly, the reception unit <b>127</b> in the FPGA <b>12</b>-<i>i </i>of the slave node <b>1</b>-<i>i </i>receives the intermediate aggregated data Rt<b>2</b>[<i>m, i&#x2212;</i>1] from the node <b>1</b>-(<i>i</i>&#x2212; 1) via the communication path <b>20</b>-<b>2</b> (step S<b>304</b>). The addition unit <b>131</b> in the FPGA <b>12</b>-<i>i </i>of the slave node <b>1</b>-<i>i </i>retrieves the distributed data D<b>2</b>[<i>m, i</i>] stored in the network transmission buffer <b>122</b>-<b>2</b> or <b>123</b>-<b>2</b> in the FPGA <b>12</b>-<i>i</i>. Then, the addition unit <b>131</b> calculates a sum of the retrieved distributed data D<b>2</b>[<i>m, i</i>] and the intermediate aggregated data Rt<b>2</b>[<i>m, i&#x2212;</i>1] received from the communication path <b>20</b>-<b>2</b> per corresponding weight w[m] to generate the intermediate aggregated data Rt<b>2</b>[<i>m, i</i>] (step S<b>305</b>).</p><p id="p-0100" num="0095">Then, the transmission unit <b>126</b> in the FPGA <b>12</b>-<i>i </i>of the slave node <b>1</b>-<i>i </i>transmits the intermediate aggregated data Rt<b>2</b>[<i>m, i</i>] generated by the addition unit <b>131</b> in the FPGA <b>12</b>-<i>i </i>in response to the data reception from the communication path <b>20</b>-<b>2</b>, to the next numbered node <b>1</b>-(<i>i</i>+1) via the communication path <b>20</b>-<b>2</b> (step S<b>306</b>).</p><p id="p-0101" num="0096">On the other hand, the reception unit <b>127</b> in the FPGA <b>12</b>-N of the slave node <b>1</b>-N receives the intermediate aggregated data Rt<b>1</b>[<i>m, N&#x2212;</i>1] from the node <b>1</b>-(N&#x2212;1) via the communication path <b>20</b>-<b>1</b> (step S<b>304</b>).</p><p id="p-0102" num="0097">The addition unit <b>131</b> in the FPGA <b>12</b>-N of the slave node <b>1</b>-N retrieves the distributed data D<b>1</b>[<i>m, N</i>] stored in the network transmission buffer <b>122</b>-<b>1</b> or <b>123</b>-<b>1</b> in the FPGA <b>12</b>-N. Then, the addition unit <b>131</b> calculates a sum of the retrieved distributed data D<b>1</b>[<i>m, N</i>] and the intermediate aggregated data Rt<b>1</b>[<i>m, N&#x2212;</i>1] received from the communication path <b>20</b>-<b>1</b> per corresponding weight w[m] to generate the intermediate aggregated data Rt<b>1</b>[<i>m, N</i>] (step S<b>305</b>). That is, the intermediate aggregated data Rt<b>1</b>[<i>m, N</i>] is constituted by M numerical values. A calculation equation for the intermediate aggregated data Rt<b>1</b>[<i>m, N</i>] is as follows.</p><p id="p-0103" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Rt</i>[<i>m,N</i>]=<i>Rt</i>1[<i>m,N&#x2212;</i>1]+<i>D</i>1[<i>m,N</i>]&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0104" num="0098">Then, the transmission unit <b>126</b> in the FPGA <b>12</b>-N of the slave node <b>1</b>-N transmits the intermediate aggregated data Rt<b>1</b>[<i>m, N</i>] generated by the addition unit <b>131</b> in the FPGA <b>12</b>-N in response to the data reception from the communication path <b>20</b>-<b>1</b>, to the master node <b>1</b>-<b>1</b> via the communication path <b>20</b>-<b>1</b> (step S<b>306</b>).</p><p id="p-0105" num="0099">In this manner, the intermediate aggregated data Rt<b>1</b>[<i>m, N</i>] constituted by M numerical values, which is calculated using the equations (2), (3), and (4), is calculated based on the distributed data D<b>1</b>[<i>m, n</i>] constituted by M numerical values generated at each node <b>1</b>-<i>n</i>. A value of the intermediate aggregated data Rt<b>1</b>[<i>m, N</i>] can be expressed by the following equation.</p><p id="p-0106" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Math 2<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0107" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Rt</i>1[<i>m,N</i>]=&#x3a3;<sub>n=1, . . . ,N</sub><i>D</i>1[<i>m,n</i>]&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0108" num="0100">Similarly, the reception unit <b>127</b> in the FPGA <b>12</b>-N of the slave node <b>1</b>-N receives the intermediate aggregated data Rt<b>2</b>[<i>m, N&#x2212;</i>1] from the node <b>1</b>-(N&#x2212;1) via the communication path <b>20</b>-<b>2</b> (step S<b>304</b>). The addition unit <b>131</b> in the FPGA <b>12</b>-N of the slave node <b>1</b>-N retrieves the distributed data D<b>2</b>[<i>m, N</i>] stored in the network transmission buffer <b>122</b>-<b>2</b> or <b>123</b>-<b>2</b> in the FPGA <b>12</b>-N. Then, the addition unit <b>131</b> calculates a sum of the retrieved distributed data D<b>2</b>[<i>m, N</i>] and the intermediate aggregated data Rt<b>2</b>[<i>m, N&#x2212;</i>1] received from the communication path <b>20</b>-<b>2</b> per corresponding weight w[m] to generate the intermediate aggregated data Rt<b>2</b>[<i>m, N</i>] (step S<b>305</b>).</p><p id="p-0109" num="0101">Then, the transmission unit <b>126</b> in the FPGA <b>12</b>-N of the slave node <b>1</b>-N transmits the intermediate aggregated data Rt<b>1</b>[<i>m, N</i>] generated by the addition unit <b>131</b> in the FPGA <b>12</b>-N in response to the data reception from the communication path <b>20</b>-<b>2</b>, to the master node <b>1</b>-<b>1</b> via the communication path <b>20</b>-<b>2</b> (step S<b>306</b>).</p><p id="p-0110" num="0102">Next, the reception unit <b>129</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> receives the intermediate aggregated data Rt<b>1</b>[<i>m, N</i>] from the node <b>1</b>-N via the communication path <b>20</b>-<b>1</b> (step S<b>206</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0111" num="0103">The transmission unit <b>128</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> transmits the received intermediate aggregated data Rt<b>1</b>[<i>m, N</i>] as aggregated data R<b>1</b>[<i>m</i>] to the next numbered node <b>1</b>-<b>2</b> via the communication path <b>20</b>-<b>1</b> (step S<b>207</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The aggregated data R<b>1</b>[<i>m</i>] is the same as the intermediate aggregated data Rt[m, N].</p><p id="p-0112" num="0104">Similarly, the transmission unit <b>128</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> transmits, in a case that the reception unit <b>129</b> receives the intermediate aggregated data Rt<b>2</b>[<i>m, N</i>] from the node <b>1</b>-N via the communication path <b>20</b>-<b>2</b>, the received intermediate aggregated data Rt<b>2</b>[<i>m, N</i>] as aggregated data R<b>2</b>[<i>m</i>] to the next numbered node <b>1</b>-<b>2</b> via the communication node <b>20</b>-<b>2</b> (step S<b>207</b>).</p><p id="p-0113" num="0105">The reception unit <b>129</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> transfers the aggregated data R<b>1</b>[<i>m</i>] (or the intermediate aggregated data Rt<b>1</b>[<i>m, N</i>]) received from the node <b>1</b>-N via the communication path <b>20</b>-<b>1</b> to either the network reception buffer <b>124</b>-<b>1</b> or <b>125</b>-<b>1</b> having an available space in the FPGA <b>12</b>-<b>1</b> (S<b>208</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). Similarly, the reception unit <b>129</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> transfers the aggregated data R<b>2</b>[<i>m</i>] received from the node <b>1</b>-N via the communication path <b>20</b>-<b>2</b> to either the network reception buffer <b>124</b>-<b>2</b> or <b>125</b>-<b>2</b> having an available space in the FPGA <b>12</b>-<b>1</b> (step S<b>208</b>).</p><p id="p-0114" num="0106">The transfer unit <b>133</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> retrieves, once any of the network reception buffers <b>124</b>-<b>1</b> and <b>125</b>-<b>1</b> in the FPGA <b>12</b>-<b>1</b> is full, the data from the full network reception buffer to transfer the retrieved data to the GPU transmission buffer <b>121</b>-<b>1</b> in the FPGA <b>12</b>-<b>1</b> (step S<b>209</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). Similarly, the transfer unit <b>133</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> retrieves, once any of the network reception buffers <b>124</b>-<b>2</b> and <b>125</b>-<b>2</b> in the FPGA <b>12</b>-<b>1</b> is full, the data from the full network reception buffer to transfer the retrieved data to the GPU transmission buffer <b>121</b>-<b>2</b> in the FPGA <b>12</b>-<b>1</b> (step S<b>209</b>).</p><p id="p-0115" num="0107">The transfer unit <b>132</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> DMA-transfers the data stored in the GPU transmission buffer <b>121</b>-<b>1</b> in the FPGA <b>12</b>-<b>1</b> to the GPU <b>11</b>-<b>1</b>-<b>1</b> (step S<b>210</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). Similarly, the transfer unit <b>132</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> DMA-transfers the data stored in the GPU transmission buffer <b>121</b>-<b>2</b> in the FPGA <b>12</b>-<b>1</b> to the GPU <b>11</b>-<b>1</b>-<b>2</b> (step S<b>210</b>).</p><p id="p-0116" num="0108">As described above, aggregated data Rj[m] received from the node <b>1</b>-N via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> is transferred to the GPUs <b>11</b>-<b>1</b>-<b>1</b> and <b>11</b>-<b>1</b>-<b>2</b>.</p><p id="p-0117" num="0109">On the other hand, the reception unit <b>129</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>(k=2, . . . , N) receives the aggregated data R<b>1</b>[<i>m</i>] from the node <b>1</b>-(<i>k</i>&#x2212;1) via the communication path <b>20</b>-<b>1</b> (step S<b>307</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0118" num="0110">The transmission unit <b>128</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>transmits the received aggregated data R<b>1</b>[<i>m</i>] to the next numbered node <b>1</b>-<i>k</i><sup>+</sup> (k<sup>+</sup>=k+1, where k<sup>+</sup>=1 in a case of k=N) via the communication path <b>20</b>-<b>1</b> (step S<b>308</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0119" num="0111">Similarly, the transmission unit <b>128</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>transmits, in a case that the reception unit <b>129</b> receives the aggregated data R<b>2</b>[<i>m</i>] from the node <b>1</b>-(<i>k</i>&#x2212;1) via the communication path <b>20</b>-<b>2</b>, the received aggregated data R<b>2</b>[<i>m</i>] to the next numbered node <b>1</b>-<i>k</i><sup>+</sup> via the communication node <b>20</b>-<b>2</b> (step S<b>308</b>).</p><p id="p-0120" num="0112">The reception unit <b>129</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>transfers the aggregated data R<b>1</b>[<i>m</i>] received from the node <b>1</b>-(<i>k</i>&#x2212;1) via the communication path <b>20</b>-<b>1</b> to either the network reception buffer <b>124</b>-<b>1</b> or <b>125</b>-<b>1</b> having an available space in the FPGA <b>12</b>-<i>k </i>(step S<b>309</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). Similarly, the reception unit <b>129</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>transfers the aggregated data R<b>2</b>[<i>m</i>] received from the node <b>1</b>-(<i>k</i>&#x2212;1) via the communication path <b>20</b>-<b>2</b> to either the network reception buffer <b>124</b>-<b>2</b> or <b>125</b>-<b>2</b> having an available space in the FPGA <b>12</b>-<i>k </i>(step S<b>309</b>).</p><p id="p-0121" num="0113">The transfer unit <b>133</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>retrieves, once any of the network reception buffers <b>124</b>-<b>1</b> and <b>125</b>-<b>1</b> in the FPGA <b>12</b>-<i>k </i>is full, the data from the full network reception buffer to transfer the retrieved data to the GPU transmission buffer <b>121</b>-<b>1</b> in the FPGA <b>12</b>-<i>k </i>(step S<b>310</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). Similarly, the transfer unit <b>133</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>retrieves, once any of the network reception buffers <b>124</b>-<b>2</b> and <b>125</b>-<b>2</b> in the FPGA <b>12</b>-<i>k </i>is full, the data from the full network reception buffer to transfer the retrieved data to the GPU transmission buffer <b>121</b>-<b>1</b> in the FPGA <b>12</b>-<i>k </i>(step S<b>310</b>).</p><p id="p-0122" num="0114">The transfer unit <b>132</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>DMA-transfers the data stored in the GPU transmission buffer <b>121</b>-<b>1</b> in the FPGA <b>12</b>-<i>k </i>to the GPU <b>11</b>-<i>k</i>&#x2212;1 (step S<b>311</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). Similarly, the transfer unit <b>132</b> in the FPGA <b>12</b>-<i>k </i>of the slave node <b>1</b>-<i>k </i>DMA-transfers the data stored in the GPU transmission buffer <b>121</b>-<b>2</b> in the FPGA <b>12</b>-<i>k </i>to the GPU <b>11</b>-<i>k</i>&#x2212;2 (step S<b>311</b>).</p><p id="p-0123" num="0115">As described above, the aggregated data Rj[m] received from the node <b>1</b>-(<i>k</i>&#x2212;1) via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> is transferred to the GPUs <b>11</b>-<i>k</i>&#x2212;1 and <b>11</b>-<i>k</i>&#x2212;2.</p><p id="p-0124" num="0116">Next, the GPU <b>11</b>-<i>n</i>-<i>j </i>of each node <b>1</b>-<i>n </i>performs the inter-GPU Allreduce process and weight updating process in the node. <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart illustrating the inter-GPU Allreduce process and weight updating process of the GPU <b>11</b>-<i>n</i>-<b>1</b> in each node <b>1</b>-<i>n</i>, and <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating the inter-GPU Allreduce process of the GPU <b>11</b>-<i>n</i>-<i>p </i>(p=2, . . . , J) in each node <b>1</b>-<i>n</i>. Note that here, the GPU <b>11</b>-<i>n</i>-<b>1</b> in each node <b>1</b>-<i>n </i>performs, as the representative GPU of the node, the weighting update process.</p><p id="p-0125" num="0117">The reception unit <b>115</b> in the GPU <b>11</b>-<i>n</i>-<b>1</b> of each node <b>1</b>-<i>n </i>receives the aggregated data R<b>1</b>[<i>m</i>] stored in the GPU transmission buffer <b>121</b>-<b>1</b> in the FPGA <b>12</b>-<i>n </i>(step S<b>400</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>).</p><p id="p-0126" num="0118">The transmission unit <b>116</b> in the GPU <b>11</b>-<i>n</i>-<b>1</b> of each node <b>1</b>-<i>n </i>transmits the aggregated data R<b>1</b>[<i>m</i>] received by the reception unit <b>115</b> in the GPU <b>11</b>-<i>n</i>-<b>1</b> to another GPU <b>11</b>-<i>n</i>-<b>2</b> (step S<b>401</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>).</p><p id="p-0127" num="0119">On the other hand, the reception unit <b>115</b> in the GPU <b>11</b>-<i>n</i>-<b>2</b> of each node <b>1</b>-<i>n </i>receives the aggregated data R<b>2</b>[<i>m</i>] stored in the GPU transmission buffer <b>121</b>-<b>2</b> in the FPGA <b>12</b>-<i>n </i>(step S<b>500</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>).</p><p id="p-0128" num="0120">The transmission unit <b>116</b> in the GPU <b>11</b>-<i>n</i>-<b>2</b> of each node <b>1</b>-<i>n </i>transmits the aggregated data R<b>2</b>[<i>m</i>] received by the reception unit <b>115</b> in the GPU <b>11</b>-<i>n</i>-<b>2</b> to another GPU <b>11</b>-<i>n</i>-<b>1</b> (step S<b>501</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>).</p><p id="p-0129" num="0121">The reception unit <b>117</b> in the GPU <b>11</b>-<i>n</i>-<b>1</b> of each node <b>1</b>-<i>n </i>receives the aggregated data R<b>2</b>[<i>m</i>] transmitted from the GPU <b>11</b>-<i>n</i>-<b>2</b> (step S<b>402</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>).</p><p id="p-0130" num="0122">The reception unit <b>117</b> in the GPU <b>11</b>-<i>n</i>-<b>2</b> of each node <b>1</b>-<i>n </i>receives the aggregated data R<b>1</b>[<i>m</i>] transmitted from the GPU <b>11</b>-<i>n</i>-<b>1</b> (step S<b>502</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>).</p><p id="p-0131" num="0123">Next, the aggregation processing unit <b>118</b> in the GPU <b>11</b>-<i>n</i>-<b>1</b> of each node <b>1</b>-<i>n </i>calculates a sum of the aggregated data R<b>1</b>[<i>m</i>] received by the reception unit <b>115</b> in the GPU <b>11</b>-<i>n</i>-<b>1</b> and the aggregated data R<b>2</b>[<i>m</i>] received by the reception unit <b>117</b> per corresponding weight w[m] to generate aggregated data U[m] (step S<b>403</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>).</p><p id="p-0132" num="0124">In this way, the sum of the data R<b>1</b>[<i>m</i>] obtained by aggregating the distributed data D<b>1</b>[<i>m, n</i>] calculated by the GPU <b>11</b>-<i>n</i>-<b>1</b> of each node <b>1</b>-<i>n </i>and the data R<b>2</b>[<i>m</i>] obtained by aggregating the distributed data D<b>2</b>[<i>m, n</i>] calculated by the GPU <b>11</b>-<i>n</i>-<b>2</b> of each node <b>1</b>-<i>n </i>can be determined as the aggregated data U[m].</p><p id="p-0133" num="0125">The weight updating processing unit <b>113</b> in the GPU <b>11</b>-<i>n</i>-<b>1</b> of each node <b>1</b>-<i>n </i>performs weight updating process to update the weight w [m] of the model <b>13</b>-<i>n </i>in the node <b>1</b>-<i>n </i>itself in accordance with the aggregated data U[m] (step S<b>404</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>). In the weight updating process, the weight w[m] is updated per number m so that a loss function is minimized on the basis of a gradient of the loss function which is indicated by the aggregated data U[m]. The updating of a weight w[m] is a well-known technique, and thus detailed description thereof will be omitted.</p><p id="p-0134" num="0126">When one mini batch learning is terminated due to the termination of the weight updating process, each node <b>1</b>-<i>n </i>continuously performs the next mini batch learning process on the basis of the updated weight w[m]. That is, each node <b>1</b>-<i>n </i>receives sample data for the next mini batch learning from a data collecting node (not illustrated), and repeats the above-described mini batch learning process to improve the accuracy of inference of the model of the node <b>1</b>-<i>n </i>itself.</p><p id="p-0135" num="0127">In the present embodiment, a DMA wait time is reduced in each GPU <b>11</b>-<i>n</i>-<i>j </i>of each node <b>1</b>-<i>n</i>, and thus, each GPU <b>11</b>-<i>n</i>-<i>j </i>can perform other processes by a reduced DMA wait time. In the present embodiment, a band of the GPU-FPGA bus can be effectively used by using the DMA transfer queue. In the present embodiment, a band of the network can be effectively used by increased network transmission buffer.</p><heading id="h-0013" level="1">Second Embodiment</heading><p id="p-0136" num="0128">Next, a second embodiment of the present invention will be described. In the present embodiment as well, the configuration of the distributed deep learning system and the process flow thereof are the same as those in the first embodiment, and thus, the description will be given using the reference signs in <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>9</b></figref>.</p><p id="p-0137" num="0129">In the first embodiment, each GPU <b>11</b>-<i>n</i>-<i>j </i>(j=1, . . . , J) of the node <b>1</b>-<i>n </i>(n=1, . . . , N) DMA-transfers the generated distributed data Dj[m, n] to either the GPU reception buffer <b>120</b>-<b>1</b> or the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b>-<i>n </i>of the node <b>1</b>-<i>n. </i></p><p id="p-0138" num="0130">In contrast, in the present embodiment, each GPU <b>11</b>-<b>1</b>-<b>1</b> of the node <b>1</b>-<i>n </i>exclusively uses the GPU reception buffer <b>120</b>-<b>1</b> and GPU transmission buffer <b>121</b>-<b>1</b> in the FPGA <b>12</b>-<i>n </i>of the node <b>1</b>-<i>n</i>. Each GPU <b>11</b>-<b>1</b>-<b>2</b> of the node <b>1</b>-<i>n </i>exclusively uses the GPU reception buffer <b>120</b>-<b>2</b> and GPU transmission buffer <b>121</b>-<b>2</b> in the FPGA <b>12</b>-<i>n </i>of the node <b>1</b>-<i>n. </i></p><p id="p-0139" num="0131">Accordingly, the transmission unit <b>114</b> in each GPU <b>11</b>-<i>n</i>-<b>1</b> of the node <b>1</b>-<i>n </i>DMA-transfers the distribution data D<b>1</b>[<i>m, n</i>] generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b>-<i>n</i>-<b>1</b> to the GPU reception buffer <b>120</b>-<b>1</b> in the FPGA <b>12</b>-<i>n </i>of the node <b>1</b>-<i>n </i>(step S<b>200</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). Similarly, the transmission unit <b>114</b> in each GPU <b>11</b>-<i>n</i>-<b>2</b> of the node <b>1</b>-<i>n </i>DMA-transfers the distribution data D<b>2</b>[<i>m, n</i>] generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b>-<i>n</i>-<b>2</b> to the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b>-<i>n </i>of the node <b>1</b>-<i>n </i>(step S<b>200</b>).</p><p id="p-0140" num="0132">The monitoring unit <b>130</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> instructs the transmission unit <b>126</b> in the FPGA <b>12</b>-<b>1</b> to transmit the data in a case that the check flag F<b>1</b> is set in every node <b>1</b>-<i>n </i>including the master node <b>1</b>-<b>1</b> itself and the check flag F<b>2</b> is not set in at least one node (YES in step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The transmission unit <b>126</b> in the FPGA <b>12</b>-<b>1</b> retrieves the distributed data D<b>1</b>[<i>m, </i>1] stored in the network transmission buffer <b>122</b>-<b>1</b> or <b>123</b>-<b>1</b> in the FPGA <b>12</b>-<b>1</b>, and transmits the retrieved data as intermediate aggregated data Rt<b>1</b>[<i>m, </i>1] to the next numbered node <b>1</b>-<b>2</b> via the communication path <b>20</b>-<b>1</b> (step S<b>205</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0141" num="0133">Similarly, the monitoring unit <b>130</b> in the FPGA <b>12</b>-<b>1</b> of the master node <b>1</b>-<b>1</b> instructs the transmission unit <b>126</b> in the FPGA <b>12</b>-<b>1</b> to transmit the data in a case that the check flag F<b>2</b> is set in every node <b>1</b>-<i>n </i>including the master node <b>1</b>-<b>1</b> itself and the check flag F<b>1</b> is not set in at least one node (YES in step S<b>204</b>). The transmission unit <b>126</b> in the FPGA <b>12</b>-<b>1</b> retrieves the distributed data D<b>2</b>[<i>m, </i>1] stored in the network transmission buffer <b>122</b>-<b>2</b> or <b>123</b>-<b>2</b> in the FPGA <b>12</b>-<b>1</b>, and transmits the retrieved data as intermediate aggregated data Rt<b>2</b>[<i>m, </i>1] to the next numbered node <b>1</b>-<b>2</b> via the communication path <b>20</b>-<b>2</b> (step S<b>205</b>).</p><p id="p-0142" num="0134">Other processing is the same as that described in the first embodiment. In this way, the present embodiment can realize the inter-node Allreduce process to aggregate the distributed data D<b>1</b>[<i>m, n</i>] calculated by the GPU <b>11</b>-<i>n</i>-<b>1</b> of each node <b>1</b>-<i>n </i>to broadcast to the GPU <b>11</b>-<i>n</i>-<b>1</b> of each node <b>1</b>-<i>n</i>, and the inter-node Allreduce process to aggregate the distributed data D<b>2</b>[<i>m, n</i>] calculated by the GPU <b>11</b>-<i>n</i>-<b>2</b> of each node <b>1</b>-<i>n </i>to broadcast to the GPU <b>11</b>-<i>n</i>-<b>2</b> of each node <b>1</b>-<i>n. </i></p><p id="p-0143" num="0135">In the present embodiment, a DMA wait time is reduced in each GPU <b>11</b>-<i>n</i>-<i>j </i>of each node <b>1</b>-<i>n</i>, and thus, each GPU <b>11</b>-<i>n</i>-<i>j </i>can perform other processes by a reduced DMA wait time. In the present embodiment, a band of the GPU-FPGA bus can be effectively used by using the DMA transfer queue. In the present embodiment, a band of the network can be effectively used by increased network transmission buffer. In the present embodiment, the inter-node Allreduce process can be performed by one FPGA of each node <b>1</b>-<i>n</i>, allowing power saving and space-saving to be achieved.</p><heading id="h-0014" level="1">Third Embodiment</heading><p id="p-0144" num="0136">Next, a third embodiment of the present invention will be described. <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system according to a third embodiment of the present invention. The distributed deep learning system in the present embodiment includes N nodes <b>1</b><i>a</i>-<i>n </i>(n=1, . . . , N) and a network <b>2</b> connecting N nodes <b>1</b><i>a</i>-<i>n </i>to each other.</p><p id="p-0145" num="0137">A patent node <b>1</b><i>a</i>-<i>i </i>includes a CPU <b>10</b>-<b>1</b>, GPUs <b>11</b><i>a</i>-<b>1</b>-<b>1</b> to <b>11</b><i>a</i>-<b>1</b>-<b>4</b>, and an FPGA <b>12</b><i>a</i>-<b>1</b>.</p><p id="p-0146" num="0138">A slave node <b>1</b><i>a</i>-<i>k </i>(k=2, . . . , N) includes a CPU <b>10</b>-<i>k</i>, GPUs <b>11</b><i>a</i>-<i>k</i>&#x2212;1 to <b>11</b><i>a</i>-<i>k</i>&#x2212;4, and an FPGA <b>12</b><i>a</i>-<i>k. </i></p><p id="p-0147" num="0139">In the present embodiment, each node <b>1</b><i>a</i>-<i>n </i>is provided with four GPUs (that is, J=4). <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a functional block diagram of the GPU <b>11</b><i>a</i>-<i>n</i>-<i>j </i>(n=1, . . . , N, j=1, . . . , J). The GPU <b>11</b><i>a</i>-<i>n</i>-<i>j </i>functions as the sample input unit <b>110</b>, the gradient calculation processing unit <b>11</b>, the aggregation processing unit <b>112</b>, the weight updating processing unit <b>113</b>, a transmission unit <b>114</b><i>a</i>, the reception unit <b>115</b>, the transmission unit <b>116</b>, the reception unit <b>117</b>, and the aggregation processing unit <b>118</b>.</p><p id="p-0148" num="0140"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a functional block diagram of the FPGA <b>12</b><i>a</i>-<b>1</b> of the master node <b>1</b><i>a</i>-<b>1</b>. The FPGA <b>12</b><i>a</i>-<b>1</b> functions as the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b>, the GPU transmission buffers <b>121</b>-<b>1</b> and <b>121</b>-<b>2</b>, the network transmission buffers <b>122</b>-<b>1</b>, <b>122</b>-<b>2</b>, <b>123</b>-<b>1</b>, and <b>123</b>-<b>2</b>, the network reception buffers <b>124</b>-<b>1</b>, <b>124</b>-<b>2</b>, <b>125</b>-<b>1</b>, and <b>125</b>-<b>2</b>, the transmission unit <b>126</b>, the transmission unit <b>128</b>, the reception unit <b>129</b>, the monitoring unit <b>130</b>, a transfer unit <b>132</b><i>a</i>, and the transfer unit <b>133</b>.</p><p id="p-0149" num="0141"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a functional block diagram of the FPGA <b>12</b><i>a</i>-<i>k </i>of the slave node <b>1</b><i>a</i>-<i>k </i>(k=2, . . . , N). The FPGA <b>12</b><i>a</i>-<i>k </i>functions as the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b>, the GPU transmission buffers <b>121</b>-<b>1</b> and <b>121</b>-<b>2</b>, the network transmission buffers <b>122</b>-<b>1</b>, <b>122</b>-<b>2</b>, <b>123</b>-<b>1</b>, and <b>123</b>-<b>2</b>, the network reception buffers <b>124</b>-<b>1</b>, <b>124</b>-<b>2</b>, <b>125</b>-<b>1</b>, and <b>125</b>-<b>2</b>, the transmission unit <b>126</b>, the reception unit <b>127</b>, the transmission unit <b>128</b>, the reception unit <b>129</b>, the monitoring unit <b>130</b>, an addition unit <b>131</b><i>a</i>, the transfer unit <b>132</b><i>a</i>, and the transfer unit <b>133</b>.</p><p id="p-0150" num="0142">The sample data input process, the gradient calculation process, and the intra-GPU aggregation process in each GPU <b>11</b><i>a</i>-<i>n</i>-<i>j </i>(n=1, . . . , N, j=1, . . . , J) of the node <b>1</b><i>a</i>-<i>n </i>are the same as those described in the first embodiment.</p><p id="p-0151" num="0143">The flow of the inter-node Allreduce process for the node <b>1</b><i>a</i>-<i>n</i>, which is similar to that in the first embodiment, will be described using the reference signs in <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>.</p><p id="p-0152" num="0144">Similar to the first embodiment, the transmission unit <b>114</b><i>a </i>in each GPU <b>11</b><i>a</i>-<b>1</b>-<i>j </i>of the master node <b>1</b><i>a</i>-<b>1</b> DMA-transfers the distribution data Dj[m, 1] generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b><i>a</i>-<b>1</b>-<i>j </i>to any one of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b><i>a</i>-<b>1</b> of the master node <b>1</b><i>a</i>-<b>1</b> (step S<b>200</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). In a case that the DMA-transferring is congested, DMA-transferring thereafter is queued, and then, is started as soon as the prior DMA-transferring ends. At this time, the transmission unit <b>114</b><i>a </i>adds an identifier of the GPU <b>11</b><i>a</i>-<b>1</b>-<i>j </i>generating the distributed data Dj[m, 1] to the distributed data Dj[m, 1]. Processing in steps S<b>201</b> to S<b>203</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is the same as that described in the first embodiment.</p><p id="p-0153" num="0145">Similarly, the transmission unit <b>114</b><i>a </i>in each GPU <b>11</b><i>a</i>-<i>k</i>-<i>j </i>of the slave node <b>1</b><i>a</i>-<i>k </i>DMA-transfers the distribution data Dj[m, k] generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b><i>a</i>-<i>k</i>-<i>j </i>to any one of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b><i>a</i>-<i>k </i>of the slave node <b>1</b><i>a</i>-<i>k </i>(step S<b>300</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). At this time, the transmission unit <b>114</b><i>a </i>adds an identifier of the GPU <b>11</b><i>a</i>-<i>k</i>-<i>j </i>generating the distributed data Dj[m, k] to the distributed data Dj[m, k]. Processing in steps S<b>301</b> to S<b>303</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is the same as that described in the first embodiment.</p><p id="p-0154" num="0146">The present embodiment gives a description assuming that the transmission units <b>114</b><i>a </i>in the GPU <b>11</b><i>a</i>-<i>n</i>-<b>1</b> and the GPU <b>11</b><i>a</i>-<i>n</i>-<b>3</b> of the node <b>1</b><i>a</i>-<i>n </i>transfer the distributed data D<b>1</b>[<i>m, n</i>] and D<b>3</b>[<i>m, n</i>] to the GPU reception buffer <b>120</b>-<b>1</b> in the FPGA <b>12</b><i>a</i>-<i>n</i>, and the transmission units <b>114</b><i>a </i>in the GPU <b>11</b><i>a</i>-<i>n</i>-<b>2</b> and the GPU <b>11</b><i>a</i>-<i>n</i>-<b>4</b> of the node <b>1</b><i>a</i>-<i>n </i>transfer the distributed data D<b>2</b>[<i>m, n</i>] and D<b>4</b>[<i>m, n</i>], respectively, to the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b><i>a</i>-<i>n. </i></p><p id="p-0155" num="0147">The monitoring unit <b>130</b> in the FPGA <b>12</b><i>a</i>-<b>1</b> of the master node <b>1</b><i>a</i>-<b>1</b> instructs the transmission unit <b>126</b> in the FPGA <b>12</b><i>a</i>-<b>1</b> to transmit the data in a case that the check flag F<b>1</b> is set in every node <b>1</b><i>a</i>-<i>n </i>including the master node <b>1</b><i>a</i>-<b>1</b> itself and the check flag F<b>2</b> is not set in at least one node (YES in step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The transmission unit <b>126</b> in the FPGA <b>12</b><i>a</i>-<b>1</b> retrieves the distributed data D<b>1</b>[<i>m, </i>1] or D<b>3</b>[<i>m, </i>1] stored in the network transmission buffer <b>122</b>-<b>1</b> or <b>123</b>-<b>1</b> in the FPGA <b>12</b><i>a</i>-<b>1</b>, and transmits the retrieved data as intermediate aggregated data Rt<b>1</b>[<i>m, </i>1] or Rt<b>3</b>[<i>m, </i>1] to the next numbered node <b>1</b><i>a</i>-<b>2</b> via the communication path <b>20</b>-<b>1</b> (step S<b>205</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0156" num="0148">The monitoring unit <b>130</b> in the FPGA <b>12</b><i>a</i>-<b>1</b> of the master node <b>1</b><i>a</i>-<b>1</b> instructs the transmission unit <b>126</b> in the FPGA <b>12</b><i>a</i>-<b>1</b> to transmit the data in a case that the check flag F<b>2</b> is set in every node <b>1</b><i>a</i>-<i>n </i>including the master node <b>1</b><i>a</i>-<b>1</b> itself and the check flag F<b>1</b> is not set in at least one node (YES in step S<b>204</b>). The transmission unit <b>126</b> in the FPGA <b>12</b><i>a</i>-<b>1</b> retrieves the distributed data D<b>2</b>[<i>m, </i>1] or D<b>4</b>[<i>m, </i>1] stored in the network transmission buffer <b>122</b>-<b>2</b> or <b>123</b>-<b>2</b> in the FPGA <b>12</b><i>a</i>-<b>1</b>, and transmits the retrieved data as intermediate aggregated data Rt<b>2</b>[<i>m, </i>1] or Rt<b>4</b>[<i>m, </i>1] to the next numbered node <b>1</b><i>a</i>-<b>2</b> via the communication path <b>20</b>-<b>2</b> (step S<b>205</b>).</p><p id="p-0157" num="0149">Next, the reception unit <b>127</b> in the FPGA <b>12</b><i>a</i>-<i>i </i>of the node <b>1</b><i>a</i>-<i>i </i>(i=2, . . . , N&#x2212;1) that is an intermediate one of the plurality of slave nodes <b>1</b><i>a</i>-<i>k </i>(k=2, . . . , N) excluding the N-th node receives the intermediate aggregated data Rt<b>1</b>[<i>m, i&#x2212;</i>1] or Rt<b>3</b>[<i>m, i&#x2212;</i>1] from the node <b>1</b><i>a</i>-(i&#x2212;1) via the communication path <b>20</b>-<b>1</b> (step S<b>304</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). The reception unit <b>127</b> in the FPGA <b>12</b><i>a</i>-<i>i </i>of the node <b>1</b><i>a</i>-<i>i </i>receives the intermediate aggregated data Rt<b>2</b>[<i>m, i&#x2212;</i>1] or Rt<b>4</b>[<i>m, i&#x2212;</i>1] from the node <b>1</b><i>a</i>-(i&#x2212;1) via the communication path <b>20</b>-<b>2</b> (step S<b>304</b>).</p><p id="p-0158" num="0150">The addition unit <b>131</b><i>a </i>in the FPGA <b>12</b><i>a</i>-<i>i </i>of the slave nodes <b>1</b><i>a</i>-<i>i </i>transitorily stores the intermediate aggregated data Rt<b>1</b> [m, i&#x2212;1], Rt<b>2</b> [m, i&#x2212;1], Rt<b>3</b> [m, i&#x2212;1], and Rt<b>4</b> [m, i&#x2212;1] received from the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b>. Then, in a case that the GPU <b>11</b><i>a</i>-(i&#x2212;1)-j deriving the intermediate aggregated data Rtj[m, i&#x2212;1] received by the addition unit <b>131</b><i>a </i>in the FPGA <b>12</b><i>a</i>-<i>i </i>of the slave nodes <b>1</b><i>a</i>-<i>i </i>is in the same combination with the GPU <b>11</b><i>a</i>-<i>i</i>-<i>j </i>generating the distributed data Dj[m, i], and the distributed data Dj[m, i] is stored in any of the network transmission buffers <b>122</b>-<b>1</b>, <b>123</b>-<b>1</b>, <b>122</b>-<b>2</b>, and <b>123</b>-<b>2</b> in the FPGA <b>12</b>-<i>i</i>, the addition unit <b>131</b><i>a </i>retrieves the distributed data Dj[m, i]. Then, the addition unit <b>131</b><i>a </i>calculates a sum of the retrieved distributed data Dj[m, i] and the intermediate aggregated data Rtj[m, i&#x2212;1] received from the communication path <b>20</b>-<b>1</b> or <b>20</b>-<b>2</b> per corresponding weight w[m] to generate the intermediate aggregated data Rtj[m, i] (step S<b>305</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0159" num="0151">Note that the GPU <b>11</b><i>a</i>-(i&#x2212;1)-j deriving the intermediate aggregated data Rtj[m, i&#x2212;1] can be identified by the identifier added to the intermediate aggregated data Rtj[m, i&#x2212;1]. Similarly, the GPU <b>11</b><i>a</i>-<i>i</i>-<i>j </i>deriving the distributed data Dj[m, i] can be identified by the identifier added to the distributed data Dj[m, i].</p><p id="p-0160" num="0152">The transmission unit <b>126</b> in the FPGA <b>12</b>-<i>i </i>of the slave node <b>1</b><i>a</i>-<i>i </i>transmits the intermediate aggregated data Rt<b>1</b>[<i>m, i</i>] or Rt<b>3</b>[<i>m, i</i>] generated by the addition unit <b>131</b><i>a </i>in the FPGA <b>12</b>-<i>i </i>to the next numbered node <b>1</b><i>a</i>-(i+1) via the communication path <b>20</b>-<b>1</b> (step S<b>306</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). The transmission unit <b>126</b> in the FPGA <b>12</b>-<i>i </i>of the slave node <b>1</b><i>a</i>-<i>i </i>transmits the intermediate aggregated data Rt<b>2</b>[<i>m, i</i>] or Rt<b>4</b>[<i>m, i</i>] generated by the addition unit <b>131</b><i>a </i>in the FPGA <b>12</b>-<i>i </i>to the next numbered node <b>1</b><i>a</i>-(i+1) via the communication path <b>20</b>-<b>2</b> (step S<b>306</b>).</p><p id="p-0161" num="0153">[oio<b>8</b>] On the other hand, the reception unit <b>127</b> in the FPGA <b>12</b><i>a</i>-N of the slave node <b>1</b><i>a</i>-N receives the intermediate aggregated data Rt<b>1</b>[<i>m, N&#x2212;</i>1] or Rt<b>3</b>[<i>m, N&#x2212;</i>1] from the node <b>1</b><i>a</i>-(N&#x2212;1) via the communication path <b>20</b>-<b>1</b> (step S<b>304</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). The reception unit <b>127</b> in the FPGA <b>12</b><i>a</i>-N of the node <b>1</b><i>a</i>-N receives the intermediate aggregated data Rt<b>2</b>[<i>m, N&#x2212;</i>1] or Rt<b>4</b>[<i>m, N&#x2212;</i>1] from the node <b>1</b><i>a</i>-(N&#x2212;1) via the communication path <b>20</b>-<b>2</b> (step S<b>304</b>).</p><p id="p-0162" num="0154">The addition unit <b>131</b><i>a </i>in the FPGA <b>12</b><i>a</i>-N of the slave nodes <b>1</b><i>a</i>-N transitorily stores the intermediate aggregated data Rt<b>1</b> [m, N&#x2212;1], Rt<b>2</b> [m, N&#x2212;1], Rt<b>3</b> [m, N&#x2212;1], and Rt<b>4</b> [m, N&#x2212;1] received from the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b>. Then, in a case that the GPU <b>11</b><i>a</i>-(N&#x2212;1)-j deriving the intermediate aggregated data Rtj[m, N&#x2212; i] received by the addition unit <b>131</b><i>a </i>in the FPGA <b>12</b><i>a</i>-N of the slave nodes <b>1</b><i>a</i>-N is in the same combination with the GPU <b>11</b><i>a</i>-N-j generating the distributed data Dj[m, N], and the distributed data Dj[m, N] is stored in any of the network transmission buffers <b>122</b>-<b>1</b>, <b>123</b>-<b>1</b>, <b>122</b>-<b>2</b>, and <b>123</b>-<b>2</b> in the FPGA <b>12</b>-N, the addition unit <b>131</b><i>a </i>retrieves the distributed data Dj[m, N]. Then, the addition unit <b>131</b><i>a </i>calculates a sum of the retrieved distributed data Dj[m, N] and the intermediate aggregated data Rtj[m, N&#x2212;1] received from the communication path <b>20</b>-<b>1</b> or <b>20</b>-<b>2</b> per corresponding weight w[m] to generate the intermediate aggregated data Rtj[m, N] (step S<b>305</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0163" num="0155">The transmission unit <b>126</b> in the FPGA <b>12</b>-N of the slave node <b>1</b><i>a</i>-N transmits the intermediate aggregated data Rt<b>1</b>[<i>m, N</i>] or Rt<b>3</b>[<i>m, N</i>] generated by the addition unit <b>131</b><i>a </i>in the FPGA <b>12</b>-N to the master node <b>1</b><i>a</i>-<b>1</b> via the communication path <b>20</b>-<b>1</b> (step S<b>306</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). The transmission unit <b>126</b> in the FPGA <b>12</b>-N of the slave node <b>1</b><i>a</i>-N transmits the intermediate aggregated data Rt<b>2</b>[<i>m, N</i>] or Rt<b>4</b>[<i>m, N</i>] generated by the addition unit <b>131</b><i>a </i>in the FPGA <b>12</b>-N to the master node <b>1</b><i>a</i>-<b>1</b> via the communication path <b>20</b>-<b>2</b> (step S<b>306</b>).</p><p id="p-0164" num="0156">Next, the reception unit <b>129</b> in the FPGA <b>12</b><i>a</i>-<b>1</b> of the master node <b>1</b><i>a</i>-<b>1</b> receives the intermediate aggregated data Rt<b>1</b>[<i>m, N</i>], Rt<b>2</b>[<i>m, N</i>], Rt<b>3</b>[<i>m, N</i>], and Rt<b>4</b>[<i>m, N</i>] from the node <b>1</b><i>a</i>-N via the communication path <b>20</b>-<b>1</b> or <b>20</b>-<b>2</b> (step S<b>206</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0165" num="0157">The transmission unit <b>128</b> in the FPGA <b>12</b><i>a</i>-<b>1</b> of the master node <b>1</b><i>a</i>-<b>1</b> transmits the received intermediate aggregated data Rt<b>1</b>[<i>m, N</i>] or Rt<b>3</b>[<i>m, N</i>] as aggregated data R<b>1</b>[<i>m</i>] or R<b>3</b>[<i>m</i>] to the next numbered node <b>1</b><i>a</i>-<b>2</b> via the communication path <b>20</b>-<b>1</b> (step S<b>207</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The transmission unit <b>128</b> in the FPGA <b>12</b><i>a</i>-<b>1</b> of the master node <b>1</b><i>a</i>-<b>1</b> transmits the received intermediate aggregated data Rt<b>2</b>[<i>m, N</i>] or Rt<b>4</b>[<i>m, N</i>] as aggregated data R<b>2</b>[<i>m</i>] or R<b>4</b>[<i>m</i>] to the next numbered node <b>1</b><i>a</i>-<b>2</b> via the communication path <b>20</b>-<b>2</b> (step S<b>207</b>).</p><p id="p-0166" num="0158">The reception unit <b>129</b> in the FPGA <b>12</b><i>a</i>-<b>1</b> of the master node <b>1</b><i>a</i>-<b>1</b> transfers the aggregated data R<b>1</b>[<i>m</i>], R<b>2</b>[<i>m</i>], R<b>3</b>[<i>m</i>], and R<b>4</b>[<i>m</i>] received from the node <b>1</b><i>a</i>-N via the communication path <b>20</b>-<b>1</b> or <b>20</b>-<b>2</b> to any of the network reception buffers <b>124</b>-<b>1</b>, <b>125</b>-<b>1</b>, <b>124</b>-<b>2</b>, and <b>125</b>-<b>2</b> having an available space in the FPGA <b>12</b><i>a</i>-<b>1</b> (S<b>208</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0167" num="0159">Processing in step S<b>209</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is the same as that described in the first embodiment. The transfer unit <b>132</b><i>a </i>in the FPGA <b>12</b><i>a</i>-<b>1</b> of the master node <b>1</b><i>a</i>-1 DMA-transfers, in a case that the aggregated data Rj[m] is stored in the GPU transmission buffer <b>121</b>-<b>1</b> or <b>12</b>-<b>2</b> in the FPGA <b>12</b><i>a</i>-<b>1</b>, the aggregated data Rj[m] to the corresponding GPU <b>11</b><i>a</i>-<b>1</b>-<i>j </i>(step S<b>210</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0168" num="0160">As is obvious from the above description, the correspondence between the aggregated data Rj[m] and the GPU <b>11</b><i>a</i>-<b>1</b>-<i>j </i>can be identified by the identifier added to the aggregated data Rj[m].</p><p id="p-0169" num="0161">As described above, the aggregated data Rj[m] received from the node <b>1</b><i>a</i>-N via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> is transferred to the GPU <b>11</b><i>a</i>-<b>1</b>-<i>j. </i></p><p id="p-0170" num="0162">On the other hand, the reception unit <b>129</b> in the FPGA <b>12</b><i>a</i>-<i>k </i>of the slave node <b>1</b><i>a</i>-<i>k </i>(k=2, . . . , N) receives the aggregated data R<b>1</b>[<i>m</i>], R<b>2</b>[<i>m</i>], R<b>3</b>[<i>m</i>], and R<b>4</b>[<i>m</i>] from the node <b>1</b><i>a</i>-(k&#x2212;1) via the communication path <b>20</b>-<b>1</b> or <b>20</b>-<b>2</b> (step S<b>307</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0171" num="0163">The transmission unit <b>128</b> in the FPGA <b>12</b><i>a</i>-<i>k </i>of the slave node <b>1</b><i>a</i>-<i>k </i>transmits the received aggregated data R<b>1</b>[<i>m</i>] or R<b>3</b>[<i>m</i>] to the next numbered node <b>1</b><i>a</i>-<i>k</i>+(k<sup>+</sup>=k<sup>+</sup>1, where k+=1 in a case of k=N) via the communication path <b>20</b>-<b>1</b> (step S<b>308</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). The transmission unit <b>128</b> in the FPGA <b>12</b><i>a</i>-<i>k </i>of the slave node <b>1</b><i>a</i>-<i>k </i>transmits the received aggregated data R<b>2</b>[<i>m</i>] or R<b>4</b>[<i>m</i>] to the next numbered node <b>1</b><i>a</i>-<i>k</i><sup>+</sup> via the communication path <b>20</b>-<b>2</b> (step S<b>308</b>).</p><p id="p-0172" num="0164">The reception unit <b>129</b> in the FPGA <b>12</b><i>a</i>-<i>k </i>of the slave node <b>1</b><i>a</i>-<i>k </i>transfers the aggregated data R<b>1</b>[<i>m</i>], R<b>2</b>[<i>m</i>], R<b>3</b>[<i>m</i>], and R<b>4</b>[<i>m</i>] received from the node <b>1</b><i>a</i>-(k&#x2212;1) via the communication path <b>20</b>-<b>1</b> or <b>20</b>-<b>2</b> to any of the network reception buffers <b>124</b>-<b>1</b>, <b>125</b>-<b>1</b>, <b>124</b>-<b>2</b>, and <b>125</b>-<b>2</b> having an available space in the FPGA <b>12</b><i>a</i>-<i>k </i>(S<b>309</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0173" num="0165">Processing in step S<b>310</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is the same as that described in the first embodiment. The transfer unit <b>132</b><i>a </i>in the FPGA <b>12</b><i>a</i>-<i>k </i>of the slave node <b>1</b><i>a</i>-<i>k </i>DMA-transfers, in a case that the aggregated data Rj[m] is stored in the GPU transmission buffer <b>121</b>-<b>1</b> or <b>12</b>-<b>2</b> in the FPGA <b>12</b><i>a</i>-<i>k</i>, the aggregated data Rj[m] to the corresponding GPU <b>11</b><i>a</i>-<i>k</i>-<i>j </i>(step S<b>311</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0174" num="0166">As described above, the aggregated data Rj[m] received from the node <b>1</b><i>a</i>-(k&#x2212;1) via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> is transferred to the GPU <b>11</b><i>a</i>-<i>k</i>-<i>j. </i></p><p id="p-0175" num="0167">Next, the GPU <b>11</b><i>a</i>-<i>n</i>-<i>j </i>of each node <b>1</b><i>a</i>-<i>n </i>performs the inter-GPU Allreduce process and weight updating process in the node. The flows of the inter-GPU Allreduce process and the weight updating process, which are similar to those in the first embodiment, will be described using the reference signs in <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref>.</p><p id="p-0176" num="0168">The reception unit <b>115</b> in the GPU <b>11</b><i>a</i>-<i>n</i>-<b>1</b> of each node <b>1</b><i>a</i>-<i>n </i>receives the aggregated data R<b>1</b>[<i>m</i>] from the FPGA <b>12</b><i>a</i>-<i>n </i>(step S<b>400</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>).</p><p id="p-0177" num="0169">The transmission unit <b>116</b> in the GPU <b>11</b><i>a</i>-<i>n</i>-<b>1</b> of each node <b>1</b><i>a</i>-<i>n </i>transmits the aggregated data R<b>1</b>[<i>m</i>] received by the reception unit <b>115</b> in the GPU <b>11</b><i>a</i>-<i>n</i>-<b>1</b> to other GPUs <b>11</b><i>a</i>-<i>n</i>-<i>p </i>(p=2, . . . , J)(step S<b>401</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>).</p><p id="p-0178" num="0170">On the other hand, the reception unit <b>115</b> in each of the GPUs <b>11</b><i>a</i>-<i>n</i>-<i>p </i>(p=2, . . . , J) of each node <b>1</b><i>a</i>-<i>n </i>receives the aggregated data Rp[m] transmitted from the FPGA <b>12</b><i>a</i>-<i>n </i>(step S<b>500</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>).</p><p id="p-0179" num="0171">The transmission unit <b>116</b> in each of the GPUs <b>11</b><i>a</i>-<i>n</i>-<i>p </i>of each node <b>1</b><i>a</i>-<i>n </i>transmits the aggregated data Rp[m] received by the reception unit <b>115</b> in the GPU <b>11</b><i>a</i>-<i>n</i>-<i>p </i>to other GPUs <b>11</b><i>a</i>-<i>n</i>-<i>q </i>(q is a natural number equal to or less than J, and p&#x2260;q)(step S<b>501</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>).</p><p id="p-0180" num="0172">The reception unit <b>117</b> in the GPU <b>11</b><i>a</i>-<i>n</i>-<b>1</b> of each node <b>1</b><i>a</i>-<i>n </i>receives the aggregated data Rp[m] transmitted from the GPU <b>11</b><i>a</i>-<i>n</i>-<i>p </i>(step S<b>402</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>).</p><p id="p-0181" num="0173">The reception unit <b>117</b> in the GPU <b>11</b><i>a</i>-<i>n</i>-<i>p </i>of each node <b>1</b><i>a</i>-<i>n </i>receives the aggregated data Rq[m] transmitted from the GPU <b>11</b><i>a</i>-<i>n</i>-<i>q </i>(step S<b>502</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>).</p><p id="p-0182" num="0174">Next, the aggregation processing unit <b>118</b> in the GPU <b>11</b><i>a</i>-<i>n</i>-<b>1</b> of each node <b>1</b><i>a</i>-<i>n </i>calculates a sum of the aggregated data R<b>1</b>[<i>m</i>] received by the reception unit <b>115</b> in the GPU <b>11</b><i>a</i>-<i>n</i>&#x2212;1 and the aggregated data Rp[m] received by the reception unit <b>117</b> per corresponding weight w[m] to generate the aggregated data U[m] (step S<b>403</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>).</p><p id="p-0183" num="0175">In this way, the sum of the data R<b>1</b>[<i>m</i>] obtained by aggregating the distributed data D<b>1</b>[<i>m, n</i>] calculated by the GPU <b>11</b><i>a</i>-<i>n</i>-<b>1</b> of each node <b>1</b><i>a</i>-<i>n</i>, the data R<b>2</b>[<i>m</i>] obtained by aggregating the distributed data D<b>2</b>[<i>m, n</i>] calculated by the GPU <b>11</b><i>a</i>-<i>n</i>&#x2212;2 of each node <b>1</b><i>a</i>-<i>n</i>, the data R<b>3</b>[<i>m</i>] obtained by aggregating the distributed data D<b>3</b>[<i>m, n</i>] calculated by the GPU <b>11</b><i>a</i>-<i>n</i>-<b>3</b> of each node <b>1</b><i>a</i>-<i>n</i>, and the data R<b>4</b>[<i>m</i>] obtained by aggregating the distributed data D<b>4</b>[<i>m, n</i>] calculated by the GPU <b>11</b><i>a</i>-<i>n</i>&#x2212;4 of each node <b>1</b><i>a</i>-<i>n </i>can be determined as the aggregated data U[m].</p><p id="p-0184" num="0176">Processing in step S<b>404</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is the same as that described in the first embodiment.</p><p id="p-0185" num="0177">In the present embodiment, a DMA wait time is reduced in each GPU <b>11</b><i>a</i>-<i>n</i>-<i>j </i>of each node <b>1</b><i>a</i>-<i>n</i>, and thus, each GPU <b>11</b><i>a</i>-<i>n</i>-<i>j </i>can perform other processes by a reduced DMA wait time. In the present embodiment, a band of the GPU-FPGA bus can be effectively used by using the DMA transfer queue. In the present embodiment, a band of the network can be effectively used by increased network transmission buffer. In the present embodiment, an aggregate throughput in the node can be improved by operating the GPUs <b>11</b><i>a</i>-<i>n</i>-<i>j </i>in parallel. In the present embodiment, each GPU <b>11</b><i>a</i>-<i>n</i>-<i>j </i>creates a Allreduce queue in parallel, and thus, the bus band and the network band can be more effectively used. In the present embodiment, the inter-node Allreduce process can be performed by one FPGA of each node <b>1</b><i>a</i>-<i>n</i>, allowing power saving and space-saving to be achieved.</p><p id="p-0186" num="0178">In the past, the Allreduce process which is the slowest process in collective communication, has occurred in a node and between nodes. In contrast, in the present embodiment, the Allreduce process in the node is sped up by the number of parallel GPUs, and the Allreduce process between the nodes is also sped up by the number of parallel GPUs.</p><heading id="h-0015" level="1">Fourth Embodiment</heading><p id="p-0187" num="0179">Next, a fourth embodiment of the present invention will be described. <figref idref="DRAWINGS">FIG. <b>14</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system according to a fourth embodiment of the present invention. The distributed deep learning system in the present embodiment includes N nodes <b>1</b><i>b</i>-<i>n </i>(n=1, . . . , N) and a network <b>2</b> connecting N nodes <b>1</b><i>b</i>-<i>n </i>to each other.</p><p id="p-0188" num="0180">A patent node <b>1</b><i>b</i>-<b>1</b> includes a CPU <b>10</b>-<b>1</b>, GPUs <b>11</b><i>b</i>-<b>1</b>-<b>1</b> and <b>11</b><i>b</i>-<b>1</b>-<b>2</b>, and an FPGA <b>12</b><i>b</i>-<b>1</b>.</p><p id="p-0189" num="0181">A slave node <b>1</b><i>b</i>-<i>k </i>(k=2, . . . , N) includes a CPU <b>10</b>-<i>k</i>, GPUs <b>11</b><i>b</i>-<i>k</i>-<b>1</b> and <b>11</b><i>a</i>-<i>k</i>-<b>2</b>, and an FPGA <b>12</b><i>b</i>-<i>k. </i></p><p id="p-0190" num="0182">In the present embodiment, each node <b>1</b><i>b</i>-<i>n </i>is provided with two GPUs (that is, J=2). <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a functional block diagram of the GPU <b>11</b><i>b</i>-<i>n</i>-<i>j </i>(n=1, . . . , N, j=1, . . . , J). The GPU <b>11</b><i>b</i>-<i>n</i>-<i>j </i>functions as the sample input unit no, the gradient calculation processing unit <b>111</b>, the aggregation processing unit <b>112</b>, the weight updating processing unit <b>113</b>, a transmission unit <b>114</b><i>b</i>, and the reception unit <b>115</b>.</p><p id="p-0191" num="0183"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a functional block diagram of the FPGA <b>12</b><i>b</i>-<b>1</b> of the master node <b>1</b><i>b</i>-<b>1</b>. The FPGA <b>12</b><i>b</i>-<b>1</b> functions as the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b>, the GPU transmission buffers <b>121</b>-<b>1</b> and <b>121</b>-<b>2</b>, the network transmission buffers <b>122</b>-<b>1</b>, <b>122</b>-<b>2</b>, <b>123</b>-<b>1</b>, and <b>123</b>-<b>2</b>, the network reception buffers <b>124</b>-<b>1</b>, <b>124</b>-<b>2</b>, <b>125</b>-<b>1</b>, and <b>125</b>-<b>2</b>, the transmission unit <b>126</b>, the transmission unit <b>128</b>, the reception unit <b>129</b>, a monitoring unit <b>130</b><i>b</i>, a transfer unit <b>132</b><i>b</i>, and the transfer unit <b>133</b>.</p><p id="p-0192" num="0184"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a functional block diagram of the FPGA <b>12</b><i>b</i>-<i>k </i>of the slave node <b>1</b><i>b</i>-<i>k </i>(k=2, . . . , N). The FPGA <b>12</b><i>b</i>-<i>k </i>functions as the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b>, the GPU transmission buffers <b>121</b>-<b>1</b> and <b>121</b>-<b>2</b>, the network transmission buffers <b>122</b>-<b>1</b>, <b>122</b>-<b>2</b>, <b>123</b>-<b>1</b>, and <b>123</b>-<b>2</b>, the network reception buffers <b>124</b>-<b>1</b>, <b>124</b>-<b>2</b>, <b>125</b>-<b>1</b>, and <b>125</b>-<b>2</b>, the transmission unit <b>126</b>, the reception unit <b>127</b>, the transmission unit <b>128</b>, the reception unit <b>129</b>, the monitoring unit <b>130</b><i>b</i>, an addition unit <b>131</b><i>b</i>, the transfer unit <b>132</b><i>b</i>, and the transfer unit <b>133</b>.</p><p id="p-0193" num="0185">The sample data input process, the gradient calculation process, and the intra-GPU aggregation process in each GPU <b>11</b><i>b</i>-<i>n</i>-<i>j </i>(n=1, . . . , N, j=1, . . . , J) of the node <b>1</b><i>b</i>-<i>n </i>are the same as those described in the first embodiment.</p><p id="p-0194" num="0186">The flow of the inter-node Allreduce process for the node <b>1</b><i>b</i>-<i>n</i>, which is similar to that in the first embodiment, will be described using the reference signs in <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>.</p><p id="p-0195" num="0187">Similar to the first embodiment, the transmission unit <b>114</b><i>b </i>in each GPU <b>11</b><i>b</i>-<b>1</b>-<i>j </i>of the master node <b>1</b><i>b</i>-1 DMA-transfers the distribution data Dj[m, 1] generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b><i>b</i>-<i>i</i>-<i>j </i>to any one of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b><i>b</i>-<b>1</b> of the master node <b>1</b><i>b</i>-<b>1</b> (step S<b>200</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0196" num="0188">The transmission unit <b>114</b><i>b </i>in each GPU <b>11</b><i>b</i>-<i>i</i>-<i>j </i>selects any one of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> that is not currently busy (or, not used by another GPU) and DMA-transfers the distributed data Dj[m, 1].</p><p id="p-0197" num="0189">Processing in steps S<b>201</b> to S<b>203</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is the same as that described in the first embodiment.</p><p id="p-0198" num="0190">Similarly, the transmission unit <b>114</b><i>b </i>in each GPU <b>11</b><i>b</i>-<i>k</i>-<i>j </i>of the slave node <b>1</b><i>b</i>-<i>k </i>DMA-transfers the distribution data Dj[m, k] generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b><i>b</i>-<i>k</i>-<i>j </i>to any of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> that is not currently busy in the FPGA <b>12</b><i>b</i>-<i>k </i>of the slave node <b>1</b><i>b</i>-<i>k </i>(step S<b>300</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0199" num="0191">The present embodiment gives a description assuming that the transmission unit <b>114</b><i>b </i>in each GPU <b>11</b><i>b</i>-<i>n</i>-<b>1</b> of the node <b>1</b><i>b</i>-<i>n </i>transfers the distributed data D<b>1</b>[<i>m, n</i>] to the GPU reception buffer <b>120</b>-<b>1</b> in the FPGA <b>12</b><i>b</i>-<i>n</i>, and the transmission unit <b>114</b><i>b </i>in each GPU <b>11</b><i>b</i>-<i>n</i>-<b>2</b> of the node <b>1</b><i>b</i>-<i>n </i>transfers distributed data D<b>2</b>[<i>m, n</i>] to the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b><i>b</i>-<i>n. </i></p><p id="p-0200" num="0192">Processing in steps S<b>301</b> to S<b>303</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is the same as that described in the first embodiment.</p><p id="p-0201" num="0193">The monitoring unit <b>130</b><i>b </i>in the FPGA <b>12</b><i>b</i>-<b>1</b> of the master node <b>1</b><i>b</i>-<b>1</b> instructs the transmission unit <b>126</b> in the FPGA <b>12</b><i>b</i>-<b>1</b> to transmit the data in a case that the check flag F<b>1</b> and the check flag F<b>2</b> are set in every node <b>1</b><i>b</i>-<i>n </i>including the master node <b>1</b><i>b</i>-<b>1</b> itself (YES in step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The transmission unit <b>126</b> in the FPGA <b>12</b><i>b</i>-<b>1</b> retrieves the distributed data D<b>1</b>[<i>m, </i>1] stored in the network transmission buffer <b>122</b>-<b>1</b> or <b>123</b>-<b>1</b> in the FPGA <b>12</b><i>b</i>-<b>1</b>, and transmits the retrieved data as the intermediate aggregated data Rt<b>1</b>[<i>m, </i>1] to the next numbered node <b>1</b><i>b</i>-<b>2</b> via the communication path <b>20</b>-<b>1</b> (step S<b>205</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The transmission unit <b>126</b> in the FPGA <b>12</b><i>b</i>-<b>1</b> retrieves the distributed data D<b>2</b>[<i>m, </i>1] stored in the network transmission buffer <b>122</b>-<b>2</b> or <b>123</b>-<b>2</b> in the FPGA <b>12</b><i>b</i>-<b>1</b>, and transmits the retrieved data as the intermediate aggregated data Rt<b>2</b>[<i>m, </i>1] to the next numbered node <b>1</b><i>b</i>-<b>2</b> via the communication path <b>20</b>-<b>2</b> (step S<b>205</b>).</p><p id="p-0202" num="0194">Next, the reception unit <b>127</b> in the FPGA <b>12</b><i>b</i>-<b>2</b> of the slave node <b>1</b><i>b</i>-<b>2</b> receives the intermediate aggregated data Rt<b>1</b>[<i>m, </i>1] from the master node <b>1</b><i>b</i>-<b>1</b> via the communication path <b>20</b>-<b>1</b> (step S<b>304</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). The reception unit <b>127</b> in the FPGA <b>12</b><i>b</i>-<b>2</b> of the slave node <b>1</b><i>b</i>-<b>2</b> receives the intermediate aggregated data Rt<b>2</b>[<i>m, </i>1] from the master node <b>1</b><i>b</i>-<b>1</b> via the communication path <b>20</b>-<b>2</b> (step S<b>304</b>).</p><p id="p-0203" num="0195">The addition unit <b>131</b><i>b </i>in the FPGA <b>12</b><i>b</i>-<b>2</b> of the slave node <b>1</b><i>b</i>-<b>2</b> transitorily stores the intermediate aggregated data Rt<b>1</b> [m, i] and Rt<b>2</b> [m, 1] received from the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b>. The addition unit <b>131</b><i>b </i>retrieves the distributed data D<b>1</b>[<i>m, </i>2] and D<b>2</b>[<i>m</i>, <b>2</b>] generated by the GPUs <b>11</b><i>b</i>-<b>2</b>-<b>1</b> and <b>11</b><i>b</i>-<b>2</b>-<b>2</b> from any of the network transmission buffers <b>122</b>-<b>1</b>, <b>123</b>-<b>1</b>, <b>122</b>-<b>2</b>, and <b>123</b>-<b>2</b> in the FPGA <b>12</b><i>b</i>-<b>2</b>. Then, the addition unit <b>131</b><i>b </i>calculates a sum of the retrieved distributed data D<b>1</b>[<i>m, </i>2] and D<b>2</b>[<i>m, </i>2], and the intermediate aggregated data Rt<b>1</b>[<i>m, </i>1] and Rt<b>2</b>[<i>m, </i>1] received from the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> per corresponding weight w[m] to generate the intermediate aggregated data Rt[m, 2] (step S<b>305</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0204" num="0196">The transmission unit <b>126</b> in the FPGA <b>12</b><i>b</i>-<b>2</b> of the slave node <b>1</b><i>b</i>-<b>2</b> transmits the intermediate aggregated data Rt[m, 2] generated by the addition unit <b>131</b><i>b </i>in the FPGA <b>12</b><i>b</i>-<b>2</b> to the next numbered node <b>1</b><i>b</i>-<b>3</b> via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> (step S<b>306</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0205" num="0197">The reception unit <b>127</b> in the FPGA <b>12</b><i>b</i>-<i>r </i>of the slave node <b>1</b><i>b</i>-<i>r </i>(r=3, . . . , N) receives the intermediate aggregated data Rt[m, r&#x2212;1] from the node <b>1</b><i>b</i>-(r&#x2212;1) via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> (step S<b>304</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0206" num="0198">The addition unit <b>131</b><i>b </i>in the FPGA <b>12</b><i>b</i>-<i>r </i>of the slave node <b>1</b><i>b</i>-<i>r </i>transitorily stores the intermediate aggregated data Rt[m, r&#x2212;1] received from the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b>. The addition unit <b>131</b><i>b </i>retrieves the distributed data D<b>1</b>[<i>m, </i>2] and D<b>2</b>[<i>m, </i>2] generated by the GPUs <b>11</b><i>b</i>-<i>r</i>-<b>1</b> and <b>11</b><i>b</i>-<i>r</i>-<b>2</b> from any of the network transmission buffers <b>122</b>-<b>1</b>, <b>123</b>-<b>1</b>, <b>122</b>-<b>2</b>, and <b>123</b>-<b>2</b> in the FPGA <b>12</b><i>b</i>-<i>r</i>. Then, the addition unit <b>131</b><i>b </i>calculates a sum of the retrieved distributed data D<b>1</b>[<i>m, </i>2] and D<b>2</b>[<i>m, </i>2], and the intermediate aggregated data Rt[m, r&#x2212;1] received from the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> per corresponding weight w[m] to generate the intermediate aggregated data Rt[m, r] (step S<b>305</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). At this time, the data from only any one of the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> is used for the intermediate aggregated data Rt[m, r&#x2212;1] used for the addition.</p><p id="p-0207" num="0199">The transmission unit <b>126</b> in the FPGA <b>12</b><i>b</i>-<i>r </i>of the slave node <b>1</b><i>b</i>-<i>r </i>transmits the intermediate aggregated data Rt[m, r] generated by the addition unit <b>131</b><i>b </i>in the FPGA <b>12</b><i>b</i>-<i>r </i>to the next numbered node <b>1</b><i>b</i>-<i>r</i><sup>+</sup> (r<sup>+</sup>=r+1, where r<sup>+</sup>=1 in a case of r=N) via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> (step S<b>306</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0208" num="0200">Next, the reception unit <b>129</b> in the FPGA <b>12</b><i>b</i>-<b>1</b> of the master node <b>1</b><i>b</i>-<b>1</b> receives the intermediate aggregated data Rt[m, N] from the node <b>1</b><i>b</i>-N via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> (step S<b>206</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0209" num="0201">The transmission unit <b>128</b> in the FPGA <b>12</b><i>b</i>-<b>1</b> of the master node <b>1</b><i>b</i>-<b>1</b> transmits the received intermediate aggregated data Rt[m, N] as the aggregated data U[m] to the next numbered node <b>1</b><i>b</i>-<b>2</b> via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> (step S<b>207</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0210" num="0202">The reception unit <b>129</b> in the FPGA <b>12</b><i>b</i>-<b>1</b> of the master node <b>1</b><i>b</i>-<b>1</b> transfers the aggregated data U[m] received from the node <b>1</b><i>b</i>-N via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> to any of the network reception buffers <b>124</b>-<b>1</b> and <b>125</b>-<b>1</b> having an available space, and any of the network reception buffers <b>124</b>-<b>2</b> and <b>125</b>-<b>2</b> having an available space in the FPGA <b>12</b><i>b</i>-<b>1</b> (step S<b>208</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). At this time, the reception unit <b>129</b> transfers the aggregated data U[m] that is from only any one of the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b>.</p><p id="p-0211" num="0203">Processing in step S<b>209</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is the same as that described in the first embodiment. The transfer unit <b>132</b><i>b </i>in the FPGA <b>12</b><i>b</i>-<b>1</b> of the master node <b>1</b><i>b</i>-1 DMA-transfers, in a case that the aggregated data U[m] is stored in the GPU transmission buffer <b>121</b>-<b>1</b> in the FPGA <b>12</b><i>b</i>-<b>1</b>, the aggregated data U[m] to the GPU <b>11</b><i>b</i>-<b>1</b>-<b>1</b> (step S<b>210</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The transfer unit <b>132</b><i>b </i>in the FPGA <b>12</b><i>b</i>-<b>1</b> of the master node <b>1</b><i>b</i>-<b>1</b> DMA-transfers, in a case that the aggregated data U[m] is stored in the GPU transmission buffer <b>121</b>-<b>2</b> in the FPGA <b>12</b><i>b</i>-<b>1</b>, the aggregated data U[m] to the GPU <b>11</b><i>b</i>-<b>1</b>-<b>2</b> (step S<b>210</b>).</p><p id="p-0212" num="0204">As described above, the aggregated data U[m] received from the node <b>1</b><i>b</i>-N via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> is transferred to the GPU <b>11</b><i>b</i>-<b>1</b>-<i>j. </i></p><p id="p-0213" num="0205">On the other hand, the reception unit <b>129</b> in the FPGA <b>12</b><i>b</i>-<i>k </i>of the slave node <b>1</b><i>b</i>-<i>k </i>(k=2, . . . , N) receives the aggregated data U[m] from the node <b>1</b><i>b</i>-(k&#x2212;1) via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> (step S<b>307</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0214" num="0206">The transmission unit <b>128</b> in the FPGA <b>12</b><i>b</i>-<i>k </i>of the slave node <b>1</b><i>b</i>-<i>k </i>transmits the received aggregated data U[m] to the next numbered node <b>1</b><i>b</i>-<i>k</i>+(k<sup>+</sup>=k+1, where k<sup>+</sup>=1 in a case of k=N) via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> (step S<b>308</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0215" num="0207">The reception unit <b>129</b> in the FPGA <b>12</b><i>b</i>-<b>1</b> of the master node <b>1</b><i>b</i>-<b>1</b> transfers the aggregated data U[m] received from the node <b>1</b><i>b</i>-(k&#x2212;1) via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> to any of the network reception buffers <b>124</b>-<b>1</b> and <b>125</b>-<b>1</b> having an available space, and any of the network reception buffers <b>124</b>-<b>2</b> and <b>125</b>-<b>2</b> having an available space in the FPGA <b>12</b><i>b</i>-<i>k </i>(step S<b>309</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0216" num="0208">Processing in step S<b>310</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is the same as that described in the first embodiment. The transfer unit <b>132</b><i>b </i>in the FPGA <b>12</b><i>b</i>-<i>k </i>of the slave node <b>1</b><i>b</i>-<i>k </i>DMA-transfers, in a case that the aggregated data U[m] is stored in the GPU transmission buffer <b>121</b>-<b>1</b> in the FPGA <b>12</b><i>b</i>-<i>k</i>, the aggregated data U[m] to the GPU <b>11</b><i>b</i>-<i>k</i>-<b>1</b> (step S<b>311</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). The transfer unit <b>132</b><i>b </i>in the FPGA <b>12</b><i>b</i>-<i>k </i>of the master node <b>1</b><i>b</i>-<i>k </i>DMA-transfers, in a case that the aggregated data U[m] is stored in the GPU transmission buffer <b>121</b>-<b>2</b> in the FPGA <b>12</b><i>b</i>-<i>k</i>, the aggregated data U[m] to the GPU <b>11</b><i>b</i>-<i>k</i>-<b>2</b> (step S<b>311</b>).</p><p id="p-0217" num="0209">As described above, the aggregated data U[m] received from the node <b>1</b><i>b</i>-(k&#x2212;1) via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> is transferred to the GPU <b>11</b><i>b</i>-<i>k</i>-<i>j. </i></p><p id="p-0218" num="0210">Next, the GPU <b>11</b><i>b</i>-<i>n</i>-<i>j </i>of each node <b>1</b><i>b</i>-<i>n </i>performs the weight updating process. <figref idref="DRAWINGS">FIG. <b>18</b></figref> is a flowchart illustrating the weight updating process by the GPU <b>11</b><i>b</i>-<i>n</i>-<b>1</b> of the node <b>1</b><i>b</i>-<i>n</i>. Note that here, the GPU <b>11</b><i>b</i>-<i>n</i>-<b>1</b> in each node <b>1</b><i>b</i>-<i>n </i>performs, as the representative GPU of the node, the weighting update process.</p><p id="p-0219" num="0211">The reception unit <b>115</b> in the GPU <b>11</b><i>b</i>-<i>n</i>-<b>1</b> of each node <b>1</b><i>b</i>-<i>n </i>receives the aggregated data U[m] from the FPGA <b>12</b><i>b</i>-<i>n </i>(step S<b>600</b> in <figref idref="DRAWINGS">FIG. <b>18</b></figref>).</p><p id="p-0220" num="0212">The weight updating processing unit <b>113</b> in the GPU <b>11</b><i>b</i>-<i>n</i>-<b>1</b> of each node <b>1</b><i>b</i>-<i>n </i>performs the weight updating process to update the weight w[m] of the model <b>13</b>-<i>n </i>in the node <b>1</b><i>b</i>-<i>n </i>itself in accordance with the aggregated data U[m] (step S<b>6</b><i>oi </i>in <figref idref="DRAWINGS">FIG. <b>18</b></figref>).</p><p id="p-0221" num="0213">In the present embodiment, a DMA wait time is reduced in each GPU <b>11</b><i>b</i>-<i>n</i>-<i>j </i>of each node <b>1</b><i>b</i>-<i>n</i>, and thus, each GPU <b>11</b><i>b</i>-<i>n</i>-<i>j </i>can perform other processes by a reduced DMA wait time. In the present embodiment, a band of the GPU-FPGA bus can be effectively used by using the DMA transfer queue. In the present embodiment, a band of the network can be effectively used by increased network transmission buffer. In the present embodiment, the inter-node Allreduce process can be performed by one FPGA of each node <b>1</b><i>b</i>-<i>n</i>, allowing power saving and space-saving to be achieved.</p><p id="p-0222" num="0214">In the present embodiment, the all aggregation processes in the Allreduce process which is the slowest process in collective communication are performed in hardware of the FPGA <b>12</b><i>b</i>-<i>n</i>, and thus, processing on the GPU side is lightened and a processing latency is also sped up. Each GPU <b>11</b><i>b</i>-<i>n</i>-<i>j </i>can select the GPU reception buffer that is not busy, and thus, a GPU reception buffer free wait time can be reduced, allowing the entire processing time to be shortened.</p><heading id="h-0016" level="1">Fifth Embodiment</heading><p id="p-0223" num="0215">Next, a fifth embodiment of the present invention will be described. <figref idref="DRAWINGS">FIG. <b>19</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system according to a fifth embodiment of the present invention. The distributed deep learning system in the present embodiment includes N nodes <b>1</b><i>c</i>-<i>n </i>(n=1, . . . , N) and a network <b>2</b> connecting N nodes <b>1</b><i>c</i>-<i>n </i>to each other.</p><p id="p-0224" num="0216">A patent node <b>1</b><i>c</i>-<b>1</b> includes a CPU <b>10</b>-<b>1</b>, GPUs <b>11</b><i>c</i>-<b>1</b>-<b>1</b> and <b>11</b><i>c</i>-<b>1</b>-<b>2</b>, and an FPGA <b>12</b><i>c</i>-<b>1</b>.</p><p id="p-0225" num="0217">A slave node <b>1</b><i>c</i>-<i>k </i>(k=2, . . . , N) includes a CPU <b>10</b>-<i>k</i>, GPUs <b>11</b><i>c</i>-<i>k</i>-<b>1</b> and <b>11</b><i>a</i>-<i>k</i>-<b>2</b>, and an FPGA <b>12</b><i>c</i>-<i>k. </i></p><p id="p-0226" num="0218">In the present embodiment, each node <b>1</b><i>c</i>-<i>n </i>is provided with two GPUs (that is, J=2). A configuration of the GPU <b>11</b><i>c</i>-<i>n</i>-<i>j</i>, which is similar to that of the GPU <b>11</b><i>b</i>-<i>n</i>-<i>j </i>in the fourth embodiment, is described using the reference signs in <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0227" num="0219"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a functional block diagram of the FPGA <b>12</b><i>c</i>-<b>1</b> of the master node <b>1</b><i>c</i>-<b>1</b>. The FPGA <b>12</b><i>c</i>-<b>1</b> functions as the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b>, the GPU transmission buffer <b>121</b>, the network transmission buffers <b>122</b>-<b>1</b>, <b>122</b>-<b>2</b>, <b>123</b>-<b>1</b>, and <b>123</b>-<b>2</b>, the network reception buffers <b>124</b> and <b>125</b>, the transmission unit <b>126</b>, the transmission unit <b>128</b>, the reception unit <b>129</b>, the monitoring unit <b>130</b><i>b</i>, a transfer unit <b>132</b><i>c</i>, and a transfer unit <b>133</b><i>c. </i></p><p id="p-0228" num="0220"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a functional block diagram of the FPGA <b>12</b><i>c</i>-<i>k </i>of the slave node <b>1</b><i>c</i>-<i>k </i>(k=2, . . . , N). The FPGA <b>12</b><i>c</i>-<i>k </i>functions as the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b>, the GPU transmission buffer <b>121</b>, the network transmission buffers <b>122</b>-<b>1</b>, <b>122</b>-<b>2</b>, <b>123</b>-<b>1</b>, and <b>123</b>-<b>2</b>, the network reception buffers <b>124</b> and <b>125</b>, the transmission unit <b>126</b>, the reception unit <b>127</b>, the transmission unit <b>128</b>, the reception unit <b>129</b>, the monitoring unit <b>130</b><i>b</i>, the addition unit <b>131</b><i>b</i>, the transfer unit <b>132</b><i>c</i>, and the transfer unit <b>133</b><i>c. </i></p><p id="p-0229" num="0221">In the present embodiment, the FPGA <b>12</b><i>c</i>-<i>n </i>of each node <b>1</b><i>c</i>-<i>n </i>is provided with the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b> the number of which is the same as the number of communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b>, and the GPU transmission buffer <b>121</b> common to the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b>. The FPGA <b>12</b><i>c</i>-<i>n </i>of each node <b>1</b><i>c</i>-<i>n </i>is provided with two network transmission buffers <b>122</b>-<b>1</b> and <b>123</b>-<b>1</b> corresponding to the communication path <b>20</b>-<b>1</b>. The FPGA <b>12</b><i>c</i>-<i>n </i>of each node <b>1</b><i>c</i>-<i>n </i>is provided with two network transmission buffers <b>122</b>-<b>2</b> and <b>123</b>-<b>2</b> corresponding to the communication path <b>20</b>-<b>2</b>. Furthermore, the FPGA <b>12</b><i>c</i>-<i>n </i>of each node <b>1</b><i>c</i>-<i>n </i>is provided with two network reception buffers <b>124</b> and <b>125</b> corresponding to the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b>.</p><p id="p-0230" num="0222">The sample data input process, the gradient calculation process, and the intra-GPU aggregation process in each GPU <b>11</b><i>c</i>-<i>n</i>-<i>j </i>(n=1, . . . , N, j=1, . . . , J) of the node <b>1</b><i>c</i>-<i>n </i>are the same as those described in the first embodiment.</p><p id="p-0231" num="0223">The flow of the inter-node Allreduce process for the node <b>1</b><i>c</i>-<i>n</i>, which is similar to that in the first embodiment, will be described using the reference signs in <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>.</p><p id="p-0232" num="0224">The transmission unit <b>114</b><i>b </i>in each GPU <b>11</b><i>c</i>-<b>1</b>-<i>j </i>of the master node <b>1</b><i>c</i>-1 DMA-transfers the distribution data Dj[m, i] generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b><i>c</i>-<b>1</b>-<i>j </i>to any one of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b><i>c</i>-<i>i </i>of the master node <b>1</b><i>c</i>-<b>1</b> (step S<b>200</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0233" num="0225">Similar to the fourth embodiment, the transmission unit <b>114</b><i>b </i>in each GPU <b>11</b><i>c</i>-<b>1</b>-<i>j </i>selects any one of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> that is not currently busy (or, not used by another GPU) and DMA-transfers the distributed data Dj[m, i].</p><p id="p-0234" num="0226">Processing in steps S<b>201</b> to S<b>207</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is the same as that described in the fourth embodiment.</p><p id="p-0235" num="0227">The transmission unit <b>114</b><i>b </i>in each GPU <b>11</b><i>c</i>-<i>k</i>-<i>j </i>of the slave node <b>1</b><i>c</i>-<i>k </i>DMA-transfers the distribution data Dj[m, k] generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b><i>c</i>-<i>k</i>-<i>j </i>to any of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> that is not currently busy in the FPGA <b>12</b><i>c</i>-<i>k </i>of the slave node <b>1</b><i>c</i>-<i>k </i>(step S<b>300</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0236" num="0228">Processing in steps S<b>301</b> to S<b>308</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is the same as that described in the fourth embodiment.</p><p id="p-0237" num="0229">The reception unit <b>129</b> in the FPGA <b>12</b><i>c</i>-<i>i </i>of the master node <b>1</b><i>c</i>-<b>1</b> transfers the aggregated data U[m] received from the node <b>1</b><i>c</i>-N via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> to either the network reception buffer <b>124</b> or <b>125</b> having an available space in the FPGA <b>12</b><i>c</i>-<b>1</b> (step S<b>208</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). At this time, the reception unit <b>129</b> transfers the aggregated data U[m] that is from only any one of the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b>.</p><p id="p-0238" num="0230">The transfer unit <b>133</b><i>c </i>in the FPGA <b>12</b><i>c</i>-<i>i </i>of the master node <b>1</b><i>c</i>-<b>1</b> retrieves, once any of the network reception buffers <b>124</b> and <b>125</b> in the FPGA <b>12</b><i>c</i>-<i>i </i>is full, the data from the full network reception buffer to transfer the retrieved data to the GPU transmission buffer <b>121</b> in the FPGA <b>12</b><i>c</i>-<b>1</b> (step S<b>209</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0239" num="0231">The transfer unit <b>132</b><i>c </i>in the FPGA <b>12</b><i>c</i>-<b>1</b> of the master node <b>1</b><i>c</i>-1 DMA-transfers the data stored in the GPU transmission buffer <b>121</b> in the FPGA <b>12</b><i>c</i>-<i>i </i>to the GPU <b>11</b><i>c</i>-<b>1</b>-<b>1</b> and the GPU <b>11</b><i>c</i>-<b>1</b>-<b>2</b> (step S<b>210</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0240" num="0232">As described above, the aggregated data U[m] received from the node <b>1</b><i>c</i>-N via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> is broadcast-transferred to the GPUs <b>11</b><i>c</i>-<b>1</b>-<b>1</b> and <b>11</b><i>c</i>-<b>1</b>-<b>2</b>.</p><p id="p-0241" num="0233">The reception unit <b>129</b> in the FPGA <b>12</b><i>c</i>-<i>k </i>of the slave node <b>1</b><i>c</i>-<i>k </i>transfers the aggregated data U[m] received from the node <b>1</b><i>c</i>-(k&#x2212;1) via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> to either the network reception buffer <b>124</b> or <b>125</b> having an available space in the FPGA <b>12</b><i>c</i>-<i>k </i>(step S<b>309</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). At this time, the reception unit <b>129</b> transfers the aggregated data U[m] that is from only any one of the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b>.</p><p id="p-0242" num="0234">The transfer unit <b>133</b><i>c </i>in the FPGA <b>12</b><i>c</i>-<i>k </i>of the slave node <b>1</b><i>c</i>-<i>k </i>retrieves, once any of the network reception buffers <b>124</b> and <b>125</b> in the FPGA <b>12</b><i>c</i>-<i>k </i>is full, the data from the full network reception buffer to transfer the retrieved data to the GPU transmission buffer <b>121</b> in the FPGA <b>12</b><i>c</i>-<i>k </i>(step S<b>310</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0243" num="0235">The transfer unit <b>132</b><i>c </i>in the FPGA <b>12</b><i>c</i>-<i>k </i>of the slave node <b>1</b><i>c</i>-<i>k </i>DMA-transfers the data stored in the GPU transmission buffer <b>121</b> in the FPGA <b>12</b><i>c</i>-<i>k </i>to the GPU <b>11</b><i>c</i>-<i>k</i>-<b>1</b> and the GPU <b>11</b><i>c</i>-<i>k</i>&#x2212;2 (step S<b>311</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0244" num="0236">As described above, the aggregated data U[m] received from the node <b>1</b><i>c</i>-(k&#x2212;1) via the communication paths <b>20</b>-<b>1</b> and <b>20</b>-<b>2</b> is broadcast-transferred to the GPUs <b>11</b><i>c</i>-<i>k</i>&#x2212;1 and <b>11</b><i>c</i>-<i>k</i>-<b>2</b>.</p><p id="p-0245" num="0237">The weight updating process of the GPU <b>11</b><i>c</i>-<i>n</i>-<i>j </i>in each node <b>1</b><i>c</i>-<i>n </i>is similar to that in the fourth embodiment.</p><p id="p-0246" num="0238">In the present embodiment, a DMA wait time is reduced in each GPU <b>11</b><i>c</i>-<i>n</i>-<i>j </i>of each node <b>1</b><i>c</i>-<i>n</i>, and thus, each GPU <b>11</b><i>c</i>-<i>n</i>-<i>j </i>can perform other processes by a reduced DMA wait time. In the present embodiment, a band of the GPU-FPGA bus can be effectively used by using the DMA transfer queue. In the present embodiment, a band of the network can be effectively used by increased network transmission buffer. In the present embodiment, the inter-node Allreduce process can be performed by one FPGA of each node <b>1</b><i>c</i>-<i>n</i>, allowing power saving and space-saving to be achieved. In the present embodiment, the number of network reception buffers and GPU transmission buffers in the FPGA can be reduced compared to the first to fourth embodiments, which makes it possible to reduce a circuit area and reduce costs.</p><p id="p-0247" num="0239">In the present embodiment, the all aggregation processes in the Allreduce process which is the slowest process in collective communication are performed in hardware of the FPGA <b>12</b><i>c</i>-<i>n</i>, and thus, processing on the GPU side is lightened and a processing latency is also sped up. Each GPU <b>11</b><i>c</i>-<i>n</i>-<i>j </i>can select the GPU reception buffer that is not busy, and thus, a GPU reception buffer free wait time can be reduced, allowing the entire processing time to be shortened.</p><heading id="h-0017" level="1">Sixth Embodiment</heading><p id="p-0248" num="0240">Next, a sixth embodiment of the present invention will be described. <figref idref="DRAWINGS">FIG. <b>22</b></figref> is a block diagram illustrating a configuration of a distributed deep learning system according to the sixth embodiment of the present invention. The distributed deep learning system in the present embodiment includes N nodes <b>1</b><i>d</i>-<i>n </i>(n=1, . . . , N) and a network <b>2</b><i>d </i>connecting N nodes <b>1</b><i>d</i>-<i>n </i>to each other. One communication path <b>20</b> is configured in the network <b>2</b><i>d. </i></p><p id="p-0249" num="0241">A patent node <b>1</b><i>d</i>-<b>1</b> includes a CPU <b>10</b>-<b>1</b>, GPUs <b>11</b><i>d</i>-<b>1</b>-<b>1</b> and <b>11</b><i>d</i>-<b>1</b>-<b>2</b>, and an FPGA <b>12</b><i>d</i>-<b>1</b>.</p><p id="p-0250" num="0242">A slave node <b>1</b><i>d</i>-<i>k </i>(k=2, . . . , N) includes a CPU <b>10</b>-<i>k</i>, GPUs <b>11</b><i>d</i>-<i>k</i>-<b>1</b> and <b>11</b><i>d</i>-<i>k</i>-<b>2</b>, and an FPGA <b>12</b><i>d</i>-<i>k. </i></p><p id="p-0251" num="0243">In the present embodiment, each node <b>1</b><i>d</i>-<i>n </i>is provided with two GPUs (that is, J=2). A configuration of the GPU <b>11</b><i>d</i>-<i>n</i>-<i>j</i>, which is similar to that of the GPU <b>11</b><i>b</i>-<i>n</i>-<i>j </i>in the fourth embodiment, is described using the reference signs in <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0252" num="0244"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a functional block diagram of the FPGA <b>12</b><i>d</i>-<b>1</b> of the master node <b>1</b><i>d</i>-<b>1</b>. The FPGA <b>12</b><i>d</i>-<b>1</b> functions as the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b>, the GPU transmission buffer <b>121</b>, the network transmission buffers <b>122</b> and <b>123</b>, the network reception buffers <b>124</b> and <b>125</b>, the transmission unit <b>126</b>, the transmission unit <b>128</b>, the reception unit <b>129</b>, a monitoring unit <b>130</b><i>d</i>, a transfer unit <b>132</b><i>d</i>, and a transfer unit <b>133</b><i>d </i>and an addition unit <b>134</b> (first addition unit).</p><p id="p-0253" num="0245"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a functional block diagram of the FPGA <b>12</b><i>d</i>-<i>k </i>of the slave node <b>1</b><i>d</i>-<i>k </i>(k=2, . . . , N). The FPGA <b>12</b><i>d</i>-<i>k </i>functions as the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b>, the GPU transmission buffer <b>121</b>, the network transmission buffers <b>122</b> and <b>123</b>, the network reception buffers <b>124</b> and <b>125</b>, the transmission unit <b>126</b>, the reception unit <b>127</b>, the transmission unit <b>128</b>, the reception unit <b>129</b>, the monitoring unit <b>130</b><i>d</i>, an addition unit <b>131</b><i>d </i>(second addition unit), the transfer unit <b>132</b><i>d</i>, the transfer unit <b>133</b><i>d</i>, and the addition unit <b>134</b> (first addition unit).</p><p id="p-0254" num="0246">In the present embodiment, the FPGA <b>12</b><i>d</i>-<i>n </i>of each node <b>1</b><i>d</i>-<i>n </i>is provided with the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b> the number of which is the same as the number of GPUs <b>11</b><i>d</i>-<i>n</i>-<i>j</i>, and the GPU transmission buffers <b>121</b> the number of which is the same as the number of communication paths <b>20</b>. The FPGA <b>12</b><i>d</i>-<i>n </i>of each node <b>1</b><i>d</i>-<i>n </i>is provided with two network transmission buffers <b>122</b> and <b>123</b> and two network reception buffers <b>124</b> and <b>125</b>.</p><p id="p-0255" num="0247">The sample data input process, the gradient calculation process, and the intra-GPU aggregation process in each GPU <b>11</b><i>d</i>-<i>n</i>-<i>j </i>(n=1, . . . , N, j=1, . . . , J) of the node <b>1</b><i>d</i>-<i>n </i>are the same as those described in the first embodiment.</p><p id="p-0256" num="0248"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a flowchart illustrating the inter-node Allreduce process for the master node <b>1</b><i>d</i>-<b>1</b>, and <figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart illustrating the inter-node Allreduce process for the slave node <b>1</b><i>d</i>-<i>k </i>(k=2, . . . , N).</p><p id="p-0257" num="0249">The transmission unit <b>114</b><i>b </i>in each GPU <b>11</b><i>d</i>-<i>i</i>-<i>j </i>of the master node <b>1</b><i>d</i>-<b>1</b> DMA-transfers the distribution data Dj[m, i] generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b><i>d</i>-<i>i</i>-<i>j </i>to any one of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> of the master node <b>1</b><i>d</i>-<b>1</b> (step S<b>700</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>).</p><p id="p-0258" num="0250">Similar to the fourth embodiment, the transmission unit <b>114</b><i>b </i>in each GPU <b>11</b><i>d</i>-<i>i</i>-<i>j </i>selects any one of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> that is not currently busy (or, not used by another GPU) and DMA-transfers the distributed data Dj[m, i].</p><p id="p-0259" num="0251">In a case that data is stored in the both GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> of the master node <b>1</b><i>d</i>-<b>1</b> and any of the network transmission buffers <b>122</b> and <b>123</b> is empty, the transfer unit <b>132</b><i>d </i>in the FPGA <b>12</b><i>d</i>-<b>1</b> transfers the data stored in the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b> to the addition unit <b>134</b> (step S<b>701</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>).</p><p id="p-0260" num="0252">The addition unit <b>134</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> of the master node <b>1</b><i>d</i>-<b>1</b> calculates a sum of the distributed data D<b>1</b>[<i>m, </i>1] and D<b>2</b>[<i>m, </i>1] received from the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b> per corresponding weight w[m] to generate the intermediate aggregated data Rt[m, 1](step S<b>702</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>). The addition unit <b>134</b> transfers the intermediate aggregated data Rt[m, 1] to either the network transmission buffer <b>122</b> or <b>123</b> having an available space in the FPGA <b>12</b><i>d</i>-<b>1</b> (step S<b>703</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>).</p><p id="p-0261" num="0253">The transmission unit <b>114</b><i>b </i>in each GPU <b>11</b><i>d</i>-<i>k</i>-<i>j </i>of the slave node <b>1</b><i>d</i>-<i>k </i>DMA-transfers the distribution data Dj[m, k] generated by the aggregation processing unit <b>112</b> in the GPU <b>11</b><i>d</i>-<i>k</i>-<i>j </i>to any of the GPU reception buffer <b>120</b>-<b>1</b> and the GPU reception buffer <b>120</b>-<b>2</b> that is not currently busy in the FPGA <b>12</b><i>d</i>-<i>k </i>of the slave node <b>1</b><i>d</i>-<i>k </i>(step S<b>800</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>).</p><p id="p-0262" num="0254">In a case that data is stored in the both GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b> in the FPGA <b>12</b><i>d</i>-<i>k </i>of the slave node <b>1</b><i>d</i>-<i>k </i>and any of the network transmission buffers <b>122</b> and <b>123</b> is empty, the transfer unit <b>132</b><i>d </i>in the FPGA <b>12</b><i>d</i>-<i>k </i>transfers the data stored in the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b> to the addition unit <b>134</b> (step S<b>8</b><i>oi </i>in <figref idref="DRAWINGS">FIG. <b>26</b></figref>).</p><p id="p-0263" num="0255">The addition unit <b>134</b> in the FPGA <b>12</b><i>d</i>-<i>k </i>of the slave node <b>1</b><i>d</i>-<i>k </i>calculates a sum of the distributed data D<b>1</b>[<i>m, k</i>] and D<b>2</b>[<i>m, k</i>] received from the GPU reception buffers <b>120</b>-<b>1</b> and <b>120</b>-<b>2</b> per corresponding weight w[m] to generate the intermediate aggregated data Rt[m, k](step S<b>802</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>). The addition unit <b>134</b> transfers the intermediate aggregated data Rt[m, k] to either the network transmission buffer <b>122</b> or <b>123</b> having an available space in the FPGA <b>12</b><i>d</i>-<i>k </i>(step S<b>803</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>).</p><p id="p-0264" num="0256">In a case that data is stored in the network transmission buffer <b>122</b> or <b>123</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> of the master node <b>1</b><i>d</i>-<b>1</b> and any of the network reception buffers <b>124</b> and <b>125</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> is empty (YES in step S<b>704</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>), the monitoring unit <b>130</b><i>d </i>in the FPGA <b>12</b><i>d</i>-<b>1</b> sets a check flag F (step S<b>705</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>).</p><p id="p-0265" num="0257">Similarly, in a case that data is stored in the network transmission buffer <b>122</b> or <b>123</b> in the FPGA <b>12</b><i>d</i>-<i>k </i>of the slave node <b>1</b><i>d</i>-<i>k </i>and any of the network reception buffers <b>124</b> and <b>125</b> in the FPGA <b>12</b><i>d</i>-<i>k </i>is empty (YES in step S<b>804</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>), the monitoring unit <b>130</b><i>f </i>in the FPGA <b>12</b><i>d</i>-<i>k </i>sets the check flag F (step S<b>805</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>).</p><p id="p-0266" num="0258">The monitoring unit <b>130</b><i>d </i>in the FPGA <b>12</b><i>d</i>-<b>1</b> of the master node <b>1</b><i>d</i>-<b>1</b> instructs the transmission unit <b>126</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> to transmit the data in a case that the check flag F is set in every node <b>1</b><i>d</i>-<i>n </i>including the master node <b>1</b><i>d</i>-<b>1</b> itself (YES in step S<b>706</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>). The transmission unit <b>126</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> retrieves the intermediate aggregated data Rt[m, 1]stored in the network transmission buffer <b>122</b> or <b>123</b> in the FPGA <b>12</b><i>d</i>-<b>1</b>, and transmits the retrieved data as intermediate aggregated data Rz[m, 1] to the next numbered node <b>1</b><i>d</i>-<b>2</b> via the communication path <b>20</b> (step S<b>707</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>).</p><p id="p-0267" num="0259">Next, the reception unit <b>127</b> in the FPGA <b>12</b><i>d</i>-<i>i </i>of the node <b>1</b><i>d</i>-<i>i </i>(i=2, . . . , N&#x2212;1) that is an intermediate one of the plurality of slave nodes <b>1</b><i>d</i>-<i>k </i>excluding the N-th node receives the intermediate aggregated data Rz[m, i&#x2212;1] from the node <b>1</b><i>d</i>-(i&#x2212;1) via the communication path <b>20</b> (step S<b>806</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>).</p><p id="p-0268" num="0260">The addition unit <b>131</b><i>d </i>in the FPGA <b>12</b><i>d</i>-<i>i </i>of the slave node <b>1</b><i>d</i>-<i>i </i>retrieves the intermediated aggregated data Rt[m, i] stored in the network transmission buffer <b>122</b> or <b>123</b> in the FPGA <b>12</b><i>d</i>-<i>i</i>. Then, the addition unit <b>131</b><i>d </i>calculates a sum of the retrieved intermediated aggregated data Rt[m, i] and the intermediate aggregated data Rz[m, i&#x2212;1] received from the communication path <b>20</b> per corresponding weight w[m] to generate the intermediate aggregated data Rz[m, i] (step S<b>807</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>).</p><p id="p-0269" num="0261">The transmission unit <b>126</b> in the FPGA <b>12</b><i>d</i>-<i>i </i>of the slave node <b>1</b><i>d</i>-<i>i </i>transmits the intermediate aggregated data Rz[m, i] generated by the addition unit <b>131</b><i>d </i>in the FPGA <b>12</b><i>d</i>-<i>i </i>to the next numbered node <b>1</b><i>d</i>-(i+1) via the communication path <b>20</b> (step S<b>808</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>).</p><p id="p-0270" num="0262">On the other hand, the reception unit <b>127</b> in the FPGA <b>12</b><i>d</i>-N of the slave node <b>1</b><i>d</i>-(N&#x2212; 1) receives the intermediate aggregated data Rz[m, N&#x2212;1] from the node <b>1</b>-(N&#x2212;1) via the communication path <b>20</b> (step S<b>806</b>).</p><p id="p-0271" num="0263">The addition unit <b>131</b><i>d </i>in the FPGA <b>12</b><i>d</i>-N of the slave node <b>1</b><i>d</i>-N retrieves the intermediated aggregated data Rt[m, N] stored in the network transmission buffer <b>122</b> or <b>123</b> in the FPGA <b>12</b><i>d</i>-N. Then, the addition unit <b>131</b><i>d </i>calculates a sum of the retrieved intermediated aggregated data Rt[m, N] and the intermediate aggregated data Rz[m, i&#x2212;1] received from the communication path <b>20</b> per corresponding weight w[m] to generate the intermediate aggregated data Rz[m, N] (step S<b>807</b>).</p><p id="p-0272" num="0264">Then, the transmission unit <b>126</b> in the FPGA <b>12</b><i>d</i>-N of the slave node <b>1</b><i>d</i>-N transmits the intermediate aggregated data Rz[m, N] generated by the addition unit <b>131</b><i>d </i>in the FPGA <b>12</b><i>d</i>-N to the master node <b>1</b><i>d</i>-<b>1</b> via the communication path <b>20</b> (step S<b>808</b>).</p><p id="p-0273" num="0265">Next, the reception unit <b>129</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> of the master node <b>1</b><i>d</i>-<b>1</b> receives the intermediate aggregated data Rz[m, N] from the node <b>1</b><i>d</i>-N via the communication path <b>20</b> (step S<b>708</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>).</p><p id="p-0274" num="0266">The transmission unit <b>128</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> of the master node <b>1</b><i>d</i>-<b>1</b> transmits the received intermediate aggregated data Rz[m, N] as the aggregated data U[m] to the next numbered node <b>1</b><i>d</i>-<b>2</b> (step S<b>709</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>).</p><p id="p-0275" num="0267">The reception unit <b>129</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> of the master node <b>1</b><i>d</i>-<b>1</b> transfers the aggregated data U[m] (or the intermediate aggregated data Rz[m, N]) received from the node <b>1</b><i>d</i>-N via the communication path <b>20</b> to either the network reception buffer <b>124</b> or <b>125</b> having an available space in the FPGA <b>12</b><i>d</i>-<b>1</b> (S<b>710</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>).</p><p id="p-0276" num="0268">The transfer unit <b>133</b><i>d </i>in the FPGA <b>12</b><i>d</i>-<b>1</b> of the master node <b>1</b><i>d</i>-<b>1</b> retrieves, once any of the network reception buffers <b>124</b> and <b>125</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> is full, the data from the full network reception buffer to transfer the retrieved data to the GPU transmission buffer <b>121</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> (step S<b>711</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>).</p><p id="p-0277" num="0269">The transfer unit <b>132</b><i>d </i>in the FPGA <b>12</b><i>d</i>-<b>1</b> of the master node <b>1</b><i>d</i>-1 DMA-transfers the data stored in the GPU transmission buffer <b>121</b> in the FPGA <b>12</b><i>d</i>-<b>1</b> to the GPU <b>11</b><i>d</i>-<b>1</b>-<b>1</b> and the GPU <b>11</b><i>d</i>-<b>1</b>-<b>2</b> (step S<b>712</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref>).</p><p id="p-0278" num="0270">As described above, the aggregated data U[m] received from the node <b>1</b><i>d</i>-N via the communication path <b>20</b> is broadcast-transferred to the GPUs <b>11</b><i>d</i>-<b>1</b>-<b>1</b> and <b>11</b><i>d</i>-<b>1</b>-<b>2</b>.</p><p id="p-0279" num="0271">On the other hand, the reception unit <b>129</b> in the FPGA <b>12</b><i>d</i>-<i>k </i>of the slave node <b>1</b><i>d</i>-<i>k </i>receives the aggregated data U[m] from the node <b>1</b><i>d</i>-(k&#x2212;1) via the communication path <b>20</b> (step S<b>809</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>). The transmission unit <b>128</b> in the FPGA <b>12</b><i>d</i>-<i>k </i>of the slave node <b>1</b><i>d</i>-<i>k </i>transmits the received aggregated data U[m] to the next numbered node <b>1</b><i>d</i>-<i>k</i><sup>+</sup> (k<sup>+</sup>=k+1, where k<sup>+</sup>=1 in a case of k=N) via the communication path <b>20</b> (step S<b>810</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>).</p><p id="p-0280" num="0272">The reception unit <b>129</b> in the FPGA <b>12</b><i>d</i>-<i>k </i>of the slave node <b>1</b><i>d</i>-<i>k </i>transfers the aggregated data U[m] received from the node <b>1</b><i>d</i>-(k&#x2212;1) via the communication path <b>20</b> to either the network reception buffer <b>124</b> or <b>125</b> having an available space in the FPGA <b>12</b><i>d</i>-<i>k </i>(step S<b>811</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>).</p><p id="p-0281" num="0273">The transfer unit <b>133</b><i>d </i>in the FPGA <b>12</b><i>d</i>-K of the slave node <b>1</b><i>d</i>-<i>k </i>retrieves, once any of the network reception buffers <b>124</b> and <b>125</b> in the FPGA <b>12</b><i>d</i>-<i>k </i>is full, the data from the full network reception buffer to transfer the retrieved data to the GPU transmission buffer <b>121</b> in the FPGA <b>12</b><i>d</i>-<i>k </i>(step S<b>812</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>).</p><p id="p-0282" num="0274">The transfer unit <b>132</b><i>d </i>in the FPGA <b>12</b><i>d</i>-<i>k </i>of the slave node <b>1</b><i>d</i>-<i>k </i>DMA-transfers the data stored in the GPU transmission buffer <b>121</b> in the FPGA <b>12</b><i>d</i>-<i>k </i>to the GPU <b>11</b><i>d</i>-<i>k</i>-<b>1</b> and the GPU <b>11</b><i>d</i>-<i>k</i>-<b>2</b> (step S<b>813</b> in <figref idref="DRAWINGS">FIG. <b>26</b></figref>).</p><p id="p-0283" num="0275">As described above, the aggregated data U[m] received from the node <b>1</b><i>d</i>-(k&#x2212;1) via the communication path <b>20</b> is broadcast-transferred to the GPUs <b>11</b><i>d</i>-<i>k</i>-<b>1</b> and <b>11</b><i>d</i>-<i>k</i>-<b>2</b>.</p><p id="p-0284" num="0276">The weight updating process of the GPU <b>11</b><i>d</i>-<i>n</i>-<i>j </i>in each node <b>1</b><i>d</i>-<i>n </i>is similar to that in the fourth embodiment.</p><p id="p-0285" num="0277">In the present embodiment, a DMA wait time is reduced in each GPU <b>11</b><i>d</i>-<i>n</i>-<i>j </i>of each node <b>1</b><i>d</i>-<i>n</i>, and thus, each GPU <b>11</b><i>d</i>-<i>n</i>-<i>j </i>can perform other processes by a reduced DMA wait time. In the present embodiment, a band of the GPU-FPGA bus can be effectively used by using the DMA transfer queue. In the present embodiment, a band of the network can be effectively used by increased network transmission buffer. In the present embodiment, the inter-node Allredude process can be performed by one FPGA of each node <b>1</b><i>d</i>-<i>n</i>, allowing power saving and space-saving to be achieved. In the present embodiment, the number of network reception buffers and GPU transmission buffers in the FPGA can be reduced compared to the first to fourth embodiments, which makes it possible to reduce a circuit area and reduce costs.</p><p id="p-0286" num="0278">In the present embodiment, the all aggregation processes in the Allredude process which is the slowest process in collective communication are performed in hardware of the FPGA <b>12</b><i>d</i>-<i>n</i>, and thus, processing on the GPU side is lightened and a processing latency is also sped up. Each GPU <b>11</b><i>d</i>-<i>n</i>-<i>j </i>can select the GPU reception buffer that is not busy, and thus, a GPU reception buffer free wait time can be reduced, allowing the entire processing time to be shortened. In the present embodiment, the plurality of nodes <b>1</b><i>d</i>-<i>n </i>are connected via one communication path <b>20</b> similarly to the related art, and thus, the number of network ports provided in each node <b>1</b><i>d</i>-<i>n </i>can be the same number as in the related art. In the present embodiment, the number of check flags is less than that in the first to fifth embodiments, and thus, it is possible to reduce the wait time until the all check flags are set, and reduce the processing time.</p><p id="p-0287" num="0279">Each of the nodes described in the first to sixth embodiments can be implemented by a computer including a calculation unit such as CPU and a GPU, a storage apparatus, and an interface, programs for controlling these hardware resources, and an FPGA. An exemplary configuration of the computer is illustrated in <figref idref="DRAWINGS">FIG. <b>27</b></figref>. The computer includes a calculation unit <b>300</b>, a storage device <b>301</b>, and an interface device (I/F) <b>302</b>. The I/F <b>302</b> is connected with a communication circuit, for example. The calculation unit <b>300</b> such as a CPU and a GPU in each node performs the processes described in the first to sixth embodiments in accordance with the programs stored in each storage device <b>301</b>.</p><heading id="h-0018" level="1">INDUSTRIAL APPLICABILITY</heading><p id="p-0288" num="0280">Embodiments of the present invention can be applied to techniques for performing machine learning of a neural network.</p><heading id="h-0019" level="1">REFERENCE SIGNS LIST</heading><p id="p-0289" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0281"><b>1</b>, <b>1</b><i>a </i>to <b>1</b><i>d </i>. . . Node</li>    <li id="ul0002-0002" num="0282"><b>2</b>, <b>2</b><i>d </i>. . . Network</li>    <li id="ul0002-0003" num="0283"><b>10</b> . . . CPU</li>    <li id="ul0002-0004" num="0284"><b>11</b>, <b>11</b><i>a </i>to <b>11</b><i>d </i>. . . GPU</li>    <li id="ul0002-0005" num="0285"><b>12</b>, <b>12</b><i>a </i>to <b>12</b><i>d </i>. . . FPGA</li>    <li id="ul0002-0006" num="0286"><b>13</b> . . . Model</li>    <li id="ul0002-0007" num="0287"><b>110</b> . . . Sample input unit</li>    <li id="ul0002-0008" num="0288"><b>111</b> . . . Gradient calculation processing unit</li>    <li id="ul0002-0009" num="0289"><b>112</b>, <b>118</b> . . . Aggregation processing unit</li>    <li id="ul0002-0010" num="0290"><b>113</b> . . . Weight updating processing unit</li>    <li id="ul0002-0011" num="0291"><b>114</b>, <b>114</b><i>a</i>, <b>114</b><i>b</i>, <b>116</b>, <b>126</b>, <b>128</b> . . . Transmission unit</li>    <li id="ul0002-0012" num="0292"><b>115</b>, <b>117</b>, <b>127</b>, <b>129</b> . . . Reception unit</li>    <li id="ul0002-0013" num="0293"><b>120</b> . . . GPU reception buffer</li>    <li id="ul0002-0014" num="0294"><b>121</b> . . . GPU transmission buffer</li>    <li id="ul0002-0015" num="0295"><b>122</b>, <b>123</b> . . . Network transmission buffer</li>    <li id="ul0002-0016" num="0296"><b>124</b>, <b>125</b> . . . Network reception buffer</li>    <li id="ul0002-0017" num="0297"><b>130</b>, <b>130</b><i>b</i>, <b>130</b><i>d </i>. . . Monitoring unit</li>    <li id="ul0002-0018" num="0298"><b>131</b>, <b>131</b><i>a</i>, <b>131</b><i>b</i>, <b>131</b><i>d</i>, <b>134</b> . . . Addition unit</li>    <li id="ul0002-0019" num="0299"><b>132</b>, <b>132</b><i>a </i>to <b>132</b><i>d</i>, <b>133</b>, <b>133</b><i>c</i>, <b>133</b><i>d </i>. . . Transfer unit.</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-001-7" num="001-7"><claim-text><b>1</b>-<b>7</b>. (canceled)</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A distributed deep learning system comprising:<claim-text>a plurality of nodes connected with each other via a network, wherein each node of the plurality of nodes includes:<claim-text>a plurality of GPUs configured to generate distributed data per weight of a model to be learned;</claim-text><claim-text>a plurality of first reception buffers configured to store the distributed data from the plurality of GPUs, wherein the plurality of GPUs is configured to DMA-transfer the distributed data to the plurality of first reception buffers;</claim-text><claim-text>a plurality of first transmission buffers configured to store the distributed data transferred from the plurality of first reception buffers;</claim-text><claim-text>a plurality of second reception buffers configured to store aggregated data received from another node of the plurality of nodes;</claim-text><claim-text>a second transmission buffer configured to store the aggregated data transferred from the plurality of second reception buffers;</claim-text><claim-text>a monitoring circuit configured to set a check flag when data is stored in any of the plurality of first transmission buffers and any of the plurality of second reception buffers has available space;</claim-text><claim-text>a first transmission circuit configured to transmit, when the check flag is set in the node itself and every other node of the plurality of nodes in a case that the node functions as a first numbered node among the plurality of nodes, the distributed data stored in the plurality of first transmission buffers as first aggregated data to a next numbered node of the plurality of nodes, and transmit, in a case that the node functions as a node except for the first numbered node among the plurality of nodes, updated first aggregated data to the next numbered node;</claim-text><claim-text>a first reception circuit configured to receive, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, the first aggregated data from another node of the plurality of nodes;</claim-text><claim-text>an addition circuit configured to calculate, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, a sum of the distributed data stored in the first transmission buffer and the first aggregated data received by the first reception circuit per weight to generate the updated first aggregated data;</claim-text><claim-text>a second reception circuit configured to receive the updated first aggregated data in the case that the node functions as the first numbered node among the plurality of nodes, and receives second aggregated data in the case that the node functions as the node except for the first numbered node among the plurality of nodes;</claim-text><claim-text>a second transmission circuit configured to transmit, in the case that the node functions as the first numbered node among the plurality of nodes, the first aggregated data received by the second reception circuit as the second aggregated data to the next numbered node, and transmit, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, the second aggregated data received by the second reception circuit to the next numbered node;</claim-text><claim-text>a first transfer circuit configured to transfer the distributed data stored in the plurality of first reception buffers to the plurality of first transmission buffers, and DMA-transfer the aggregated data stored in the second transmission buffer to the plurality of GPUs; and</claim-text><claim-text>a second transfer circuit configured to transfer the aggregated data stored in the plurality of second reception buffers to the second transmission buffer.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The distributed deep learning system according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein:<claim-text>a plurality of communication paths are configured in the network;</claim-text><claim-text>for each node of the plurality of nodes:<claim-text>a quantity of the plurality of first reception buffers equals a quantity of the plurality of communication paths;</claim-text><claim-text>the plurality of first transmission buffers are provided per one communication path;</claim-text><claim-text>the plurality of second reception buffers are provided per one communication path;</claim-text><claim-text>a quantity of the plurality of second transmission buffers equals the quantity of the plurality of communication paths;</claim-text><claim-text>each of the plurality of GPUs includes:<claim-text>a third transmission circuit configured to DMA-transfer the distributed data to respective ones of the plurality of first reception buffers;</claim-text><claim-text>a third reception circuit configured to receive the second aggregated data DMA-transferred by the first transfer circuit;</claim-text><claim-text>a fourth transmission circuit configured to transmit the second aggregated data received by the third reception circuit to another GPU of the plurality of GPUs;</claim-text><claim-text>a fourth reception circuit configured to receive the second aggregated data transmitted from another GPU of the plurality of GPUs;</claim-text><claim-text>an aggregation processing circuit configured to calculate a sum of the second aggregated data received by the third reception circuit and the second aggregated data received by the fourth reception circuit per weight to generated third aggregated data; and</claim-text><claim-text>an updating circuit configured to update the model in accordance with the third aggregated data;</claim-text></claim-text><claim-text>the first transfer circuit is configured to transfer the distributed data stored in a first reception buffer of plurality of first reception buffers corresponding to a first communication path to a first transmission buffer of the plurality of first transmission buffers corresponding to the first communication path, and DMA-transfer the second aggregated data stored in a second transmission buffer of the plurality of second transmission buffers corresponding to a second communication path to a GPU of the plurality of GPUs corresponding to the second communication path;</claim-text><claim-text>the second transfer circuit is configured to transfer the second aggregated data stored in the second reception buffer corresponding to the second communication path to the second transmission buffer corresponding to the second communication path;</claim-text></claim-text><claim-text>when the data is stored in the first transmission buffer and the second reception buffer has available space, the first communication path being identical to the second communication path, the monitoring circuit is configured to set the check flag corresponding to the first communication path;</claim-text><claim-text>in the case that the node functions as the first numbered node among the plurality of nodes when the check flag corresponding to the first communication path is set in the node itself and every other node, and the check flag corresponding to another communication path is not set in at least one node, the first transmission circuit is configured to transmit the distributed data stored in the first transmission buffer corresponding to the first communication path as the first aggregated data to the next numbered node via the first communication path; and</claim-text><claim-text>the addition circuit is configured to calculate a sum of the distributed data stored in the first transmission buffer corresponding to the first communication path and the first aggregated data received from the first communication path by the first reception circuit per weight to generate the updated first aggregated data.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The distributed deep learning system according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein:<claim-text>a plurality of communication paths are configured in the network,</claim-text><claim-text>for each node of the plurality of nodes:<claim-text>a quantity of the plurality of first reception buffers equals a quantity of the plurality of communication paths;</claim-text><claim-text>the plurality of first transmission buffers provided per one communication path;</claim-text><claim-text>the plurality of second reception buffers provided per one communication path;</claim-text><claim-text>a quantity of the plurality of second transmission buffers equals the quantity of the plurality of communication paths;</claim-text><claim-text>each of the plurality of GPUs includes:</claim-text><claim-text>a third transmission circuit configured to DMA-transfer the distributed data to respective ones of the plurality of first reception buffers;</claim-text><claim-text>a third reception circuit configured to receive the second aggregated data DMA-transferred by the first transfer circuit;</claim-text><claim-text>a fourth transmission circuit configured to transmit the second aggregated data received by the third reception circuit to another GPU of the plurality of GPUs;</claim-text><claim-text>a fourth reception circuit configured to receive the second aggregated data transmitted from another GPU of the plurality of GPUs;</claim-text><claim-text>an aggregation processing circuit configured to calculate a sum of the second aggregated data received by the third reception circuit and the second aggregated data received by the fourth reception circuit per weight to generated third aggregated data; and</claim-text><claim-text>an updating circuit configured to update the model in accordance with the third aggregated data;</claim-text></claim-text><claim-text>the first transfer circuit is configured to transfer the distributed data stored in a first reception buffer of plurality of first reception buffers corresponding to a first communication path to a first transmission buffer of the plurality of first transmission buffers corresponding to the first communication path, and DMA-transfer the second aggregated data stored in a second transmission buffer of the plurality of second transmission buffers corresponding to a second communication path to a GPU of the plurality of GPUs corresponding to the second aggregated data;</claim-text><claim-text>the second transfer circuit is configured to transfer the second aggregated data stored in the second reception buffer corresponding to the second communication path to the second transmission buffer corresponding to the second communication path;</claim-text><claim-text>when the data is stored in the first transmission buffer and the second reception buffer has available space, the first communication path being identical to the second communication path, the monitoring circuit is configured to set the check flag corresponding to the first communication path;</claim-text><claim-text>in a case that the node functions as the first numbered node among the plurality of nodes when the check flag corresponding to the first communication path is set in the node itself and every other node, and the check flag corresponding to another communication path is not set in at least one node, the first transmission circuit is configured to transmit the distributed data stored in the first transmission buffer corresponding to the first communication path as the first aggregated data to the next numbered node via the first communication path; and</claim-text><claim-text>in a case that the GPU deriving the first aggregated data received from another node by the first reception circuit is in the same combination with the GPU generating the distributed data and the distributed data is stored in the first transmission buffer, the addition circuit is configured to calculate a sum of the distributed data and the first aggregated data received by the first reception circuit per weight to generate the updated first aggregated data.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The distributed deep learning system according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein:<claim-text>a plurality of communication paths are configured in the network;</claim-text><claim-text>for each node of the plurality of nodes:<claim-text>a quantity of the plurality of first reception buffers equals a quantity of the plurality of communication paths;</claim-text><claim-text>the plurality of first transmission buffers are provided per one communication path;</claim-text><claim-text>the plurality of second reception buffers are provided per one communication path;</claim-text><claim-text>a quantity of the plurality of second transmission buffers equals the quantity of the plurality of communication paths;</claim-text></claim-text><claim-text>each of the plurality of GPUs includes:<claim-text>a third transmission circuit configured to DMA-transfer the distributed data an available first reception buffer that is not busy among the plurality of reception buffers;</claim-text><claim-text>a third reception circuit configured to receive the second aggregated data DMA-transferred by the first transfer circuit, and</claim-text><claim-text>an updating circuit configured to update the model in accordance with the second aggregated data received by the third reception circuit,</claim-text><claim-text>the first transfer circuit is configured to transfer the distributed data stored in a first reception buffer of plurality of first reception buffers corresponding to a first communication path to a first transmission buffer of the plurality of first transmission buffers corresponding to the first communication path, and DMA-transfer the second aggregated data stored in a second transmission buffer of the plurality of second transmission buffers corresponding to a second communication path to a GPU of the plurality of GPUs corresponding to the second communication path;</claim-text></claim-text><claim-text>the second transfer circuit is configured to transfer the second aggregated data stored in the second reception buffer corresponding to the second communication path to the second transmission buffer corresponding to the second communication path;</claim-text><claim-text>when the data is stored in the first transmission buffer and the second reception buffer has available space, the first communication path being identical to the second communication path, the monitoring circuit is configured to set the check flag corresponding to the first communication path;</claim-text><claim-text>in the case that the node functions as the first numbered node among the plurality of nodes when all check flags are set in the node itself and every other node, the first transmission circuit is configured to transmit the distributed data stored in the plurality of first transmission buffers as the first aggregated data to the next numbered node via the communication paths corresponding to the first transmission buffers storing the distributed data; and</claim-text><claim-text>the addition circuit is configured to calculate a sum of the distributed data stored in the plurality of first transmission buffers corresponding to the plurality of communication paths and the first aggregated data received from the plurality of communication paths by the first reception circuit per weight to generate the updated first aggregated data.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The distributed deep learning system according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein:<claim-text>a plurality of communication paths are configured in the network,</claim-text><claim-text>for each node of the plurality of nodes:<claim-text>a quantity of the plurality of first reception buffers equals a quantity of the plurality of communication paths;</claim-text><claim-text>the plurality of first transmission buffers are provided per one communication path,</claim-text><claim-text>the plurality of second reception buffers are provided common to the plurality of communication paths;</claim-text><claim-text>the second transmission buffer are provided common to the plurality of communication paths;</claim-text><claim-text>each of the GPUs includes:<claim-text>a third transmission unit configured to DMA-transfer the distributed data to the first reception buffer not busy among the plurality of reception buffers;</claim-text><claim-text>a third reception circuit configured to receive the second aggregated data DMA-transferred by the first transfer circuit; and</claim-text><claim-text>an updating circuit configured to update the model in accordance with the second aggregated data received by the third reception circuit;</claim-text><claim-text>the first transfer circuit is configured to transfer the distributed data stored in a first reception buffer of plurality of first reception buffers corresponding to a first communication path to a first transmission buffer of the plurality of first transmission buffers corresponding to the first communication path, and DMA-transfer the second aggregated data stored in a second transmission buffer of the plurality of second transmission buffers corresponding to a second communication path to the plurality of GPUs;</claim-text></claim-text></claim-text><claim-text>the second transfer circuit is configured to transfer the second aggregated data stored in the plurality of second reception buffers to the second transmission buffer;</claim-text><claim-text>when the data is stored in the first transmission buffer and the second reception buffer has available space, the first communication path being identical to the second communication path, the monitoring circuit is configured to set the check flag corresponding to the first communication path;</claim-text><claim-text>in the case that the node functions as the first numbered node among the plurality of nodes when all check flags are set in the node itself and every other node, the first transmission circuit is configured to transmit the distributed data stored in the plurality of first transmission buffers as the first aggregated data to the next numbered node via the communication paths corresponding to the first transmission buffers storing the distributed data; and</claim-text><claim-text>the addition circuit is configured to calculate a sum of the distributed data stored in the plurality of first transmission buffers corresponding to the plurality of communication paths and the first aggregated data received from the plurality of communication paths by the first reception circuit per weight to generate the updated first aggregated data.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A distributed deep learning system comprising:<claim-text>plurality of nodes connected with each other via a network, wherein each node of the plurality of nodes includes:<claim-text>a plurality of GPUs configured to generate distributed data per weight of a model to be learned;</claim-text><claim-text>a plurality of first reception buffers configured to store the distributed data from the plurality of GPUs, wherein the plurality of GPUs is configured to DMA-transfer the distributed data to the plurality of first reception buffers;</claim-text><claim-text>a first addition circuit configured to calculate a sum of a plurality of pieces of the distributed data transferred from the plurality of first reception buffers per weight to generate first aggregated data;</claim-text><claim-text>a plurality of first transmission buffers configured to store the first aggregated data;</claim-text><claim-text>a plurality of second reception buffers configured to store aggregated data received from another node of the plurality of nodes;</claim-text><claim-text>a second transmission buffer configured to store the aggregated data transferred from the plurality of second reception buffers;</claim-text><claim-text>a monitoring circuit configured to set a check flag when data is stored in the first transmission buffers and the second reception buffers has available space;</claim-text><claim-text>a first transmission circuit configured to transmit, when the check flag is set in the node itself and every other node in a case that the node functions as a first numbered node among the plurality of nodes, the first aggregated data stored in any of the first transmission buffers as second aggregated data to the next numbered node, and transmit, in a case that the node functions as a node except for the first numbered node among the plurality of nodes, updated second aggregated data to the next numbered node;</claim-text><claim-text>a first reception circuit configured to receive, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, the second aggregated data from another node;</claim-text><claim-text>a second addition circuit configured to calculate, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, a sum of the first aggregated data stored in the first transmission buffer and the second aggregated data received by the first reception circuit per weight to generate the updated first aggregated data;</claim-text><claim-text>a second reception circuit configured to receive the updated second aggregated data in the case that the node functions as the first numbered node among the plurality of nodes, and receives third aggregated data in the case that the node functions as the node except for the first numbered node among the plurality of nodes;</claim-text><claim-text>a second transmission circuit configured to transmit, in the case that the node functions as the first numbered node among the plurality of nodes, the second aggregated data received by the second reception circuit as the third aggregated data to the next numbered node, and transmit, in the case that the node functions as the node except for the first numbered node among the plurality of nodes, the third aggregated data received by the second reception circuit to the next numbered node;</claim-text><claim-text>a first transfer circuit configured to transfer the distributed data stored in the first reception buffers to the first addition circuit, and DMA-transfer the third aggregated data stored in the second transmission buffer to the plurality of GPUs; and</claim-text><claim-text>a second transfer circuit configured to transfer the third aggregated data stored in the second reception buffers to the second transmission buffer, wherein the plurality of GPUs is configured to update the model in accordance with the third aggregated data.</claim-text></claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The distributed deep learning system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein:<claim-text>a communication path is configured in the network,</claim-text><claim-text>for each node of the plurality of nodes:<claim-text>a quantity of the first reception buffers equals a quantity of the plurality of GPUs;</claim-text><claim-text>a quantity of the second transmission buffers equals a quantity of the communication paths in the network,</claim-text><claim-text>each of the plurality of GPUs includes:<claim-text>a third transmission circuit configured to DMA-transfer the distributed data to a first reception buffer that is available among the plurality of reception buffers;</claim-text><claim-text>a third reception circuit configured to receive the third aggregated data DMA-transferred by the first transfer circuit; and</claim-text><claim-text>an updating circuit configured to update the model in accordance with the third aggregated data received by the third reception circuit;</claim-text></claim-text></claim-text><claim-text>the second transfer circuit is configured to transfer the third aggregated data stored in the plurality of second reception buffers to the second transmission buffer,</claim-text><claim-text>when the data is stored in the first transmission buffer and the second reception buffer has an available space, the first transmission buffer and the second reception buffer corresponding to the identical communication path, the monitoring circuit is configured to set the check flag corresponding to the communication path; and</claim-text><claim-text>the second addition circuit is configured to calculate a sum of the first aggregated data stored in any of the plurality of first transmission buffers and the second aggregated data received from the communication path by the first reception circuit per weight to generate the updated second aggregated data.</claim-text></claim-text></claim></claims></us-patent-application>