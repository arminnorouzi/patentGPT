<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005115A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005115</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17783028</doc-number><date>20201230</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>SG</country><doc-number>10201914038S</doc-number><date>20191231</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>33</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>64</class><subclass>C</subclass><main-group>39</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>64</class><subclass>D</subclass><main-group>47</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>337</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0002</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23299</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23218</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23212</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>64</class><subclass>C</subclass><main-group>39</main-group><subgroup>024</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>64</class><subclass>D</subclass><main-group>47</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0094</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30168</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20164</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20216</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10032</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>64</class><subclass>C</subclass><main-group>2201</main-group><subgroup>127</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD, SYSTEM, AND IMAGE PROCESSING DEVICE FOR CAPTURING AND/OR PROCESSING ELECTROLUMINESCENCE IMAGES, AND AN AERIAL VEHICLE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Quantified Energy Labs Pte. Ltd.</orgname><address><city>Singapore</city><country>SG</country></address></addressbook><residence><country>SG</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>BEDRICH</last-name><first-name>Karl Georg</first-name><address><city>Singapore</city><country>SG</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>KHOO</last-name><first-name>Yong Sheng</first-name><address><city>Singapore</city><country>SG</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Yan</first-name><address><city>Singapore</city><country>SG</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Quantified Energy Labs Pte. Ltd.</orgname><role>03</role><address><city>Singapore</city><country>SG</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/SG2020/050790</doc-number><date>20201230</date></document-id><us-371c12-date><date>20220607</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method (<b>400</b>) of capturing and processing electroluminescence (EL) images (<b>1910</b>) of a PV array (<b>40</b>) is disclosed herein. In a described embodiment, the method <b>400</b> includes controlling the aerial vehicle (<b>20</b>) to fly along a flight path to capture EL images (<b>1910</b>) of corresponding PV array subsections (<b>512</b><i>b</i>) of the PV array (<b>40</b>), deriving respective image quality parameters from at least some of the captured EL images, dynamically adjusting a flight speed of the aerial vehicle along the flight path, based on the respective image quality parameters for capturing the EL images (<b>1910</b>) of the PV array subsections (<b>512</b><i>b</i>), extracting a plurality of frames (<b>1500</b>) of the PV array subsection (<b>512</b><i>b</i>) from the EL images (<b>1910</b>); determining a reference frame having a highest image quality of the PV array subsection (<b>512</b><i>b</i>) from among the extracted frames (<b>2100</b>); performing image alignment of the extracted frames (<b>2100</b>) to the reference frame to generate image aligned frames (<b>2130</b>), and processing the image aligned frames (<b>2130</b>) to produce an enhanced image (<b>2140</b>) of the PV array subsection (<b>512</b><i>b</i>) having a higher resolution than the reference frame. A system, image processing device, and aerial vehicle for the method thereof are also disclosed.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="191.85mm" wi="158.75mm" file="US20230005115A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="241.13mm" wi="171.70mm" file="US20230005115A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="245.70mm" wi="164.76mm" file="US20230005115A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="248.41mm" wi="136.74mm" file="US20230005115A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="139.36mm" wi="158.07mm" file="US20230005115A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="234.53mm" wi="162.39mm" file="US20230005115A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="136.99mm" wi="157.65mm" file="US20230005115A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="246.21mm" wi="181.78mm" file="US20230005115A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="240.37mm" wi="163.58mm" file="US20230005115A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="202.10mm" wi="129.79mm" file="US20230005115A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="248.41mm" wi="159.34mm" file="US20230005115A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="238.59mm" wi="166.96mm" file="US20230005115A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="239.86mm" wi="136.74mm" file="US20230005115A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="126.49mm" wi="119.97mm" file="US20230005115A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="236.47mm" wi="182.29mm" file="US20230005115A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="244.43mm" wi="181.44mm" file="US20230005115A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present application relates to methods for capturing and processing electroluminescence (EL) images of a photovoltaic (PV) array, an aerial vehicle for capturing the EL images, and an image processing device for processing the EL images.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Solar panels have found widespread use globally. However, due to high initial capital investment cost, solar panels installed in the field must work properly and efficiently for a period to ensure return on investment. Hence, it is important to maintain the quality of solar panels installed in the field. Due to the mass deployment of solar panels in solar farms (or generally photovoltaic (PV) plants), and the remote deployment of solar panels such as on the roof of houses, it is often difficult to monitor the performance of individual solar panels. Various imaging technologies such as visual, thermal (infrared), ultra-violet (UV) fluorescence, photoluminescence (PL) and electroluminescence (EL) imaging are available to detect defects of solar panels. For example, EL inspection is used during PV manufacturing for quality control.</p><p id="p-0004" num="0003">For EL measurements, PV modules of the solar panels are connected to a power supply and put under forward bias. The emitted near-infrared light is captured with a camera that is sensitive in the near-infrared waveband. For on-site inspection, EL imaging has also been used on a sampling basis. One of the common methods for on-site EL inspection is using a mobile trailer. In this method, the mobile trailer which carries a darkroom is deployed on-site. PV modules are taken down from their installed position for measurement in the darkroom inside the trailer. This method ensures that the EL measurements of PV modules are taken in a controlled environment. However, as the PV modules must be dismounted, large scale inspection using this method is time consuming and not feasible. Additionally, there is the risk of introducing defects during module handling.</p><p id="p-0005" num="0004">Another method of EL inspection is performed at night using a camera mounted on a tripod, respective during the day, with lock-in current control. While this method does not require dismounting of PV modules from their support frame, it is also time consuming and highly labour intensive. Additionally, perspective and intensity distortions of captured images may result from the limitations of capturing images from a camera mounted to a tripod.</p><p id="p-0006" num="0005">Therefore, it is desirable to provide a solution that addresses at least one of the problems mentioned in existing prior art, and/or to provide the public with a useful alternative.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0007" num="0006">According to a first aspect, there is provided a method of processing electroluminescence (EL) images of a PV array. The method includes (i) extracting a plurality of frames of a PV array subsection of the PV array from the EL images, the PV array subsection including one or more PV modules of the PV array, (ii) determining a reference frame having a highest image quality of the PV array subsection from among the extracted frames, (iii) performing image alignment of the extracted frames to the reference frame to generate image aligned frames, and (iv) processing the image aligned frames to produce an enhanced image of the PV array subsection having a higher resolution than the reference frame.</p><p id="p-0008" num="0007">The described embodiment is able to take low resolution, mono-chromatic images, and videos under dim light conditions or in the absence of natural light such as during the night, and yet enhanced resolution and noise reduced images may be produced to identify defective PV modules. In general, and if averaging is used, magnitude of noise in the images processed may be reduced by roughly the square root of the number of images averaged. As a result, in general enhanced images of higher image quality may be achieved.</p><p id="p-0009" num="0008">Specifically, extracting the frames from the images may include determining respective corner points of each PV module in the images, and constructing respective frames for each PV module based on the identified corner points of each PV module.</p><p id="p-0010" num="0009">In a specific embodiment, determining respective corner points of each PV module in the images may include clustering the respective corner points of a particular PV module that is repeated in different images, and computing respective averaged positions for each cluster of respective corner points.</p><p id="p-0011" num="0010">Preferably, determining a reference frame having a highest image quality may include evaluating the image quality of each frame based on at least one of sharpness, signal-to-noise ratio, and completeness of the frames.</p><p id="p-0012" num="0011">The method may also include arranging the extracted frames in a stacked arrangement before performing image alignment. Further, the respective corner points of the PV modules may be stacked in the stacked arrangement, and performing image alignment may include aligning the respective corner points of each PV module in the extracted frames to the corresponding corner points of the PV module in the reference frame</p><p id="p-0013" num="0012">It is envisaged that processing the image aligned frames may include grouping the image aligned frames according to the PV module in each frame, and performing image averaging on each group of image aligned frames to obtain respective enhanced frames for each PV module.</p><p id="p-0014" num="0013">Moreover, the image averaging may be based on weighted image stack averaging, and/or a deep convolutional neural network structure.</p><p id="p-0015" num="0014">The method may further include associating each enhanced frame with a horizontal index and a vertical index according to each PV module's position in the PV array subsection, and arranging the enhanced frames according to its horizontal and vertical index to produce the enhanced image of the PV array subsection.</p><p id="p-0016" num="0015">The method may also include scaling respective image intensities of each enhanced frame. Additionally, the method may also include mapping the enhanced image of the PV array subsection onto a base-map of the PV array subsection. The base-map may include geo-location of each PV module.</p><p id="p-0017" num="0016">In a specific embodiment, mapping the enhanced image onto the base-map may further include orientating the enhanced image to align the PV array subsectionin the enhanced image to the PV array subsection in the base-map.</p><p id="p-0018" num="0017">Furthermore, each EL image of the PV array subsection may include an image identifier, and orientating the enhanced image may include associating the image identifier of a particular EL image with each enhanced frame having its PV module featured in the particular EL image, and determining an orientation of the enhanced image based on the image identifiers associated with each enhanced frame.</p><p id="p-0019" num="0018">The geo-location may include GPS coordinates.</p><p id="p-0020" num="0019">Moreover, the plurality of frames may be consecutive frames of the PV array subsection.</p><p id="p-0021" num="0020">According to a second aspect, there is provided an image processing device for processing EL images of a PV array. The image processing device includes an image processor configured to extract a plurality of frames of a PV array subsection of the PV array from the EL images, the PV array subsection including one or more PV modules of the PV array, determine a reference frame having a highest image quality of the PV array subsection from among the extracted frames, perform image alignment of the extracted frames to the reference frame to generate image aligned frames, and process the image aligned frames to produce an enhanced image of the PV array subsection having a higher resolution and lower noise level than the reference frame.</p><p id="p-0022" num="0021">The image processor may be further configured to extract the frames from the images by determining respective corner points of each PV module in the images, and constructing respective frames for each PV module based on the identified corner points of each PV module.</p><p id="p-0023" num="0022">Preferably, the image processor may be further configured to determine respective corner points of each PV module in the images by clustering the respective corner points of a particular PV module that may be repeated in different images, and computing respective averaged positions for each cluster of respective corner points.</p><p id="p-0024" num="0023">The image processor may also be further configured to determine a reference frame having a highest image quality by evaluating the image quality of each frame based on at least one of sharpness, signal-to-noise ratio, and completeness of the frames.</p><p id="p-0025" num="0024">Additionally, the image processor may be further configured to arrange the extracted frames in a stacked arrangement before performing image alignment.</p><p id="p-0026" num="0025">Furthermore, the respective corner points of the PV modules may be stacked in the stacked arrangement, and the image processor may be further configured to perform image alignment by aligning the respective corner points of each PV module in the extracted frames to the corresponding corner points of the PV module in the reference frame.</p><p id="p-0027" num="0026">In a specific embodiment, the image processor may be further configured to process the image aligned frames by grouping the image aligned frames according to the PV module in each frame, and perform image averaging on each group of image aligned frames to obtain respective enhanced frames for each PV module.</p><p id="p-0028" num="0027">Moreover, the image processor may be configured to perform image averaging based on weighted image stack averaging, and/or a deep convolutional neural network structure.</p><p id="p-0029" num="0028">The image processor may also be configured to associate each enhanced frame with a horizontal index and a vertical index according to each PV module's position in the PV array subsection, and arrange the enhanced frames according to its horizontal and vertical index to produce the enhanced image of the PV array subsection.</p><p id="p-0030" num="0029">The image processor may be configured to scale respective image intensities of each enhanced frame.</p><p id="p-0031" num="0030">The image processor may be further configured to map the enhanced image of the PV array subsection onto a base-map of the PV array subsection. The base-map may then include geo-location of each PV module. In addition, the image processor may be configured to map the enhanced image onto the base-map by orientating the enhanced image to align the PV array subsection in the enhanced image to the PV array subsection in the base-map.</p><p id="p-0032" num="0031">Furthermore, each EL image of the PV array subsection may include an image identifier, and the image processor may be configured to orientate the enhanced image by associating the image identifier of a particular EL image with each enhanced frame having its PV module featured in the particular EL image, and determining an orientation of the enhanced image based on the image identifiers associated with each enhanced frame.</p><p id="p-0033" num="0032">The geo-location may include GPS coordinates. Additionally, the plurality of frames may be consecutive frames of the PV array subsection.</p><p id="p-0034" num="0033">According to a third aspect, there is provided a method of controlling movement of an aerial vehicle having a camera for capturing EL images of a PV array. The method includes controlling the aerial vehicle to fly along a flight path to capture EL images of corresponding PV array subsections of the PV array, deriving respective image quality parameters from at least some of the captured EL images, and dynamically adjusting a flight speed of the aerial vehicle along the flight path, based on the respective image quality parameters for capturing the EL images of the PV array subsections.</p><p id="p-0035" num="0034">Advantageously, by dynamically adjusting the aerial vehicle's flight speed according to image quality parameters derived from at least some of the captured EL images, the aerial vehicle is able to adjust its flights speed so that it is possible to capture EL images with higher image quality such as better signal-to-noise ratio or image clarity.</p><p id="p-0036" num="0035">The image quality parameters may include a SNR scanning factor and a motion blur scanning factor. In a specific example, the SNR scanning factor may be dependent on a target SNR, a measured SNR, and an estimated number of EL images captured that include a particular PV module of the PV array subsection.</p><p id="p-0037" num="0036">On the other hand, the motion blur scanning factor may be a ratio of a measured object deflection to a predefined maximum object deflection.</p><p id="p-0038" num="0037">Preferably, dynamically adjusting a flight speed of the aerial vehicle along the flight path, based on the respective image quality parameters may include deriving a target flight speed based on a minimum of the SNR scanning factor and the motion blur scanning factor, and dynamically adjusting the current flight speed of the aerial vehicle to match the target flight speed.</p><p id="p-0039" num="0038">Additionally, deriving the target flight speed may include applying the minimum of the SNR scanning factor and the motion blur scanning factor to a current flight speed of the aerial vehicle to derive a target scanning speed, and selecting the target scanning speed as the target flight speed if the target scanning speed may be below a maximum flight speed of the aerial vehicle.</p><p id="p-0040" num="0039">The method may further include selecting the maximum flight speed as the target flight speed if the target scanning speed exceeds the maximum flight speed of the aerial vehicle.</p><p id="p-0041" num="0040">The method may also include adjusting the target flight speed based on a user input factor.</p><p id="p-0042" num="0041">Additionally, the method may further include detecting an EL signal emitted by one or more PV modules from the PV array, prior to controlling the aerial vehicle to fly along the flight path to capture EL images of corresponding PV array subsections of the PV array.</p><p id="p-0043" num="0042">The method may also include manoeuvring the aerial vehicle to an initial position wherein the aerial vehicle's yaw axis and the camera's optical axis may be perpendicular to the ground before detecting the EL signal.</p><p id="p-0044" num="0043">Further, the method may include navigating the aerial vehicle to the EL signal's location.</p><p id="p-0045" num="0044">In a specific embodiment, detecting the EL signal emitted by the one or more PV modules of the PV array may include rotating the aerial vehicle about the vehicle's yaw axis while simultaneously increasing the camera's optical axis angle until the EL signal may be detected.</p><p id="p-0046" num="0045">The camera's optical axis angle may be increased from 0&#xb0; to 70&#xb0;. It is envisaged that the camera's optical axis angle may then be increased at a decreasing pitch speed. Preferably, the aerial vehicle may also rotate at a decreasing yaw speed.</p><p id="p-0047" num="0046">The method may further include manoeuvring the aerial vehicle to a predefined elevation before rotating the aerial vehicle.</p><p id="p-0048" num="0047">The method may also include aligning the camera's field-of-view (FOV) to the corresponding PV array subsections by determining respective key points of a reference PV module in the corresponding PV array subsections, deriving target aligned points from the respective key points for the camera's FOV to be aligned to the corresponding PV array subsections, performing a perspective transformation to align the respective key points to the target aligned points, and manoeuvring the aerial vehicle relative to the corresponding PV array subsections based on the perspective transformation.</p><p id="p-0049" num="0048">Furthermore, aligning the camera's FOV to the corresponding PV array subsections further may include manoeuvring the aerial vehicle to an appropriate elevation, wherein the corresponding PV array subsections may be at a predefined size ratio within the camera's FOV at the appropriate elevation.</p><p id="p-0050" num="0049">The corresponding PV array subsections may occupy 80% to 90% of the camera's FOV at the predefined size ratio.</p><p id="p-0051" num="0050">In addition, the method may further include dynamically adjusting the camera's focus according to a measured image sharpness.</p><p id="p-0052" num="0051">The aerial vehicle further may include a light source aligned with the camera's optical axis, and the method further may include powering the light source except while capturing the EL images of the PV array.</p><p id="p-0053" num="0052">According to a fourth aspect, there is provided an aerial vehicle including a camera for capturing EL images of a PV array, a propulsion device for actuating movement of the aerial vehicle, and a controller communicatively coupled to the camera and the propulsion device. The controller is configured to control the aerial vehicle to fly along a flight path to capture EL images of corresponding PV array subsections of the PV array, deriving respective image quality parameters from at least some of the captured EL images, and dynamically adjusting a flight speed of the aerial vehicle along the flight path, based on the respective image quality parameters for capturing the EL images of the PV array subsections.</p><p id="p-0054" num="0053">The image quality parameters may then include a SNR scanning factor and a motion blur scanning factor. Preferably, the SNR scanning factor may be dependent on a target SNR, a measured SNR, and an estimated number of EL images captured that include a particular PV module of the PV array subsection.</p><p id="p-0055" num="0054">On the other hand, the motion blur scanning factor may be a ratio of a measured object deflection to a predefined maximum object deflection.</p><p id="p-0056" num="0055">Preferably, the controller may be further configured to dynamically adjusting a flight speed of the aerial vehicle along the flight path, based on the respective image quality parameters by deriving a target flight speed based on a minimum of the SNR scanning factor and the motion blur scanning factor, and dynamically adjusting the current flight speed of the aerial vehicle to match the target flight speed.</p><p id="p-0057" num="0056">The controller may also be configured to deriving the target flight speed by applying the minimum of the SNR scanning factor and the motion blur scanning factor to a current flight speed of the aerial vehicle to derive a target scanning speed, and selecting the target scanning speed as the target flight speed if the target scanning speed may be below a maximum flight speed of the aerial vehicle.</p><p id="p-0058" num="0057">Additionally, the controller may be configured to select the maximum flight speed as the target flight speed if the target scanning speed exceeds the maximum flight speed of the aerial vehicle.</p><p id="p-0059" num="0058">The controller may also be configured to adjust the target flight speed based on a user input factor.</p><p id="p-0060" num="0059">Moreover, the controller may be further configured to detect an EL signal emitted by one or more PV modules from the PV array, the PV array having an array axis, and the one or more PV modules being aligned to the array axis and may include a planar surface, determine the array axis of the PV array, and control the camera to capture the EL images of the PV array along the array axis while dynamically adjusting the propulsion device to align the camera's optical axis to be perpendicular to the one or more PV module's planar surface.</p><p id="p-0061" num="0060">Furthermore, the controller may be configured to set the aerial vehicle to an initial position before locating the EL signal by dynamically adjusting the propulsion device to set the aerial vehicle's yaw axis to be perpendicular to the ground, and dynamically adjusting the camera's optical axis to be perpendicular to the ground.</p><p id="p-0062" num="0061">The controller may also be configured to dynamically adjust the propulsion device to navigate the aerial vehicle to the EL signal's location.</p><p id="p-0063" num="0062">The controller may be further configured to locate the EL signal emitted by the one or more PV modules of the PV array by dynamically adjust the propulsion device to rotate the aerial vehicle about the vehicle's yaw axis while simultaneously increasing the camera's optical axis angle until the EL signal may be located.</p><p id="p-0064" num="0063">The camera's optical axis angle may be increased from 0&#xb0; to 70&#xb0;. Specifically, the camera's optical axis angle may then be increased at a decreasing pitch speed. Preferably, the aerial vehicle may also rotate at a decreasing yaw speed.</p><p id="p-0065" num="0064">The controller may be further configured to dynamically adjust the propulsion device to maneuver the aerial vehicle to a predefined elevation before rotating the aerial vehicle.</p><p id="p-0066" num="0065">Additionally, the controller may be configured to align the camera's field-of-view (FOV) to the corresponding PV array subsections by determining respective key points of a reference PV module in the corresponding PV array subsections, deriving target aligned points from the respective key points for the camera's FOV to be align to the corresponding PV array subsections, performing a perspective transformation to align the respective key points to the target aligned points, and dynamically adjusting the propulsion device to maneuver the aerial vehicle relative to the corresponding PV array subsection based on the perspective transformation.</p><p id="p-0067" num="0066">The controller may also be configured to align the camera's FOV to the corresponding PV array subsections by dynamically adjusting the propulsion device to maneuver the aerial vehicle to an appropriate elevation, wherein the corresponding PV array subsections may be at a predefined size ratio within the camera's FOV at the appropriate elevation.</p><p id="p-0068" num="0067">The corresponding PV array subsections may occupy 80% to 90% of the camera's FOV at the predefined size ratio.</p><p id="p-0069" num="0068">The controller may be further configured to dynamically adjust the camera's focus according to a measured image sharpness.</p><p id="p-0070" num="0069">Additionally, the aerial vehicle may further include a light source aligned with the camera's optical axis, and the controller may be further configured to power the light source except while capturing the EL images of the PV array.</p><p id="p-0071" num="0070">According to a fifth aspect, there is provided a method of obtaining an enhanced image of a PV array subsection of a PV array from EL images of the PV array subsection captured by an aerial vehicle having a camera. The method includes (i) controlling the aerial vehicle to fly along a flight path to capture EL images of corresponding PV array subsections of the PV array, deriving respective image quality parameters from at least some of the captured EL images, dynamically adjusting a flight speed of the aerial vehicle along the flight path, based on the respective image quality parameters for capturing the EL images of the PV array subsections, (iv) extracting a plurality of frames of the PV array subsection from the EL images, (v) determining a reference frame having a highest image quality of the PV array subsection from among the extracted frames, (vi) performing image alignment of the extracted frames to the reference frame to generate image aligned frames, and (vii) processing the image aligned frames to produce an enhanced image of the PV array subsection having a higher resolution than the reference frame.</p><p id="p-0072" num="0071">According to a sixth aspect, there is provided a system for capturing and processing EL images of a PV array subsection of a PV array. The system includes an aerial vehicle and an image processing device. The aerial vehicle includes a camera for capturing EL images of a PV array, a propulsion device for actuating movement of the aerial vehicle, and a controller communicatively coupled to the camera and the propulsion device, and configured to control the aerial vehicle to fly along a flight path to capture EL images of corresponding PV array subsections of the PV array, deriving respective image quality parameters from at least some of the captured EL images, and dynamically adjusting a flight speed of the aerial vehicle along the flight path, based on the respective image quality parameters for capturing the EL images of the PV array subsections. The image processing device includes an image processor configured to extract a plurality of frames of a PV array subsection of the PV array from the EL images, the PV array subsection including one or more PV modules of the PV array, determine a reference frame having a highest image quality of the PV array subsection from among the extracted frames, perform image alignment of the extracted frames to the reference frame to generate image aligned frames, and process the image aligned frames to produce an enhanced image of the PV array subsection having a higher resolution than the reference frame.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0073" num="0072">Exemplary embodiments will be described with reference to the accompanying drawings in which:</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an exemplary setup for a UAV to capture EL images of a PV array;</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a system architecture of a system for capturing and processing EL images, the system including the UAV of <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an optical sub-system which is a part of the system of <figref idref="DRAWINGS">FIG. <b>2</b></figref>;</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of an exemplary method for capturing and processing EL images as performed by the system of <figref idref="DRAWINGS">FIG. <b>2</b></figref>;</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of the UAV performing a POINT function as part of the exemplary method of <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of the UAV performing a first part of a FIND function as part of the exemplary method of <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram of the UAV performing a second part of the FIND function of <figref idref="DRAWINGS">FIG. <b>6</b></figref>;</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic diagram depicting a process and results of the FIND function of <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>;</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a perspective view of the UAV hovering over a PV string with the camera's FOV misaligned before an ALIGN function is performed as part of the exemplary method of <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a perspective view of the UAV hovering over a PV string with the camera's FOV aligned after an ALIGN function is performed as part of the exemplary method of <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is an EL image from the perspective of the camera with the camera's FOV misaligned according to <figref idref="DRAWINGS">FIG. <b>9</b></figref>;</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an EL image from the perspective of the camera with the camera's FOV aligned according to <figref idref="DRAWINGS">FIG. <b>10</b></figref>;</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a schematic diagram of the EL image from the perspective of the controller with the camera's FOV misaligned according to <figref idref="DRAWINGS">FIG. <b>11</b></figref>;</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a schematic diagram of the EL image from the perspective of the controller with the camera's FOV aligned according to <figref idref="DRAWINGS">FIG. <b>12</b></figref>;</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>15</b></figref>, comprising <figref idref="DRAWINGS">FIGS. <b>15</b>A to <b>15</b>F</figref>, is a series of six consecutive EL images of the PV string captured by the camera during a SCAN function as part of the exemplary method of <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a schematic diagram of the series of EL images of <figref idref="DRAWINGS">FIG. <b>15</b></figref> after image alignment to the PV string is performed;</p><p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a schematic diagram of two consecutive EL images from <figref idref="DRAWINGS">FIG. <b>15</b></figref> showing point-to-point deflection;</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>18</b>A</figref> is a line graph showing a current flight speed of the UAV decreasing over time while performing the SCAN function as part of the exemplary method of <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>18</b>B</figref> is a line graph showing a current flight speed of the UAV increasing over time while performing the SCAN function as part of the exemplary method of <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>19</b></figref> comprising <figref idref="DRAWINGS">FIGS. <b>19</b>A, <b>19</b>B, <b>19</b>C, <b>19</b>D and <b>19</b>E</figref> are schematic diagrams showing a time lapse of the UAV performing an AUTO function as part of the exemplary method of <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a schematic diagram of a file structure for the stored EL images that are captured during the AUTO function of <figref idref="DRAWINGS">FIG. <b>19</b></figref>;</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. <b>21</b></figref>, comprising <figref idref="DRAWINGS">FIGS. <b>21</b>A, <b>21</b>B and <b>21</b>C</figref>, are schematic diagram of frames being extracted from three consecutive EL images during a FREEZE function as part of the exemplary method of <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a schematic diagram of an image enhancement step being performed on the frames extracted as part of the FREEZE function of <figref idref="DRAWINGS">FIG. <b>21</b></figref>;</p><p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a pixel intensity histogram of an enhanced frame obtained from the image enhancement step of <figref idref="DRAWINGS">FIG. <b>22</b></figref>;</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a schematic diagram of the enhanced EL image generated by the FREEZE function of <figref idref="DRAWINGS">FIGS. <b>21</b> to <b>23</b></figref>;</p><p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. <b>25</b>A</figref> is a schematic diagram of an enhanced EL image with PV modules arranged in two rows and two columns during a first part of the MAP function as part of the exemplary method of <figref idref="DRAWINGS">FIG. <b>4</b></figref>;</p><p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. <b>25</b>B</figref> is a schematic diagram of an alternative to <figref idref="DRAWINGS">FIG. <b>25</b>A</figref> with the enhanced EL image having PV modules arranged in two rows and three columns;</p><p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. <b>25</b>C</figref> is a schematic diagram of an alternative to <figref idref="DRAWINGS">FIG. <b>25</b>A</figref> with the enhanced EL image having PV modules arranged in two rows and five columns;</p><p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a schematic diagram of the enhanced EL image being mapped onto a base-map during a second part of the MAP function of <figref idref="DRAWINGS">FIGS. <b>25</b>A-C</figref>;</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a schematic diagram of the UAV during the SCAN function for approximating the center position of the camera's FOV along the PV string in a third part of the MAP function of <figref idref="DRAWINGS">FIG. <b>26</b></figref>;</p><p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. <b>28</b>A</figref> is a schematic diagram of an exemplary enhanced EL image being mapped onto an exemplary base-map using a string alignment method in a fourth part of the MAP function of <figref idref="DRAWINGS">FIG. <b>27</b></figref>; and</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>28</b>B</figref> is a schematic diagram of an exemplary enhanced EL image being mapped onto an exemplary base-map using a module alignment method as an alternative to the fourth part of <figref idref="DRAWINGS">FIG. <b>28</b>A</figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0106" num="0105">The following description includes specific examples for illustrative purposes. The person skilled in the art would appreciate that variations and alterations to the specific examples are possible and within the scope of the present disclosure. The figures and the following description of the particular embodiments should not take away from the generality of the preceding summary.</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an exemplary EL inspection apparatus or setup <b>100</b> for capturing EL images of a PV array <b>10</b> installed on a roof of a building. In this embodiment, the PV array <b>10</b> includes three PV strings <b>12</b>. Each PV string <b>12</b> includes two rows of five PV modules <b>14</b>. The PV strings <b>12</b> are arranged in each row along a longitudinal axis of the PV strings <b>12</b> so that the PV array <b>10</b> has an array axis <b>10</b><i>a </i>that runs along the longitudinal axis of the PV strings <b>12</b>. The PV strings <b>12</b> are connected to a combiner box <b>16</b> which combines the PV strings' electrical output. The combiner box <b>16</b> is connected to an inverter box (not shown) which is then connected to the power grid. The inverter box converts the combined electrical output from DC to AC before feeding the combined electrical output into the power grid. In this way, electricity generated by the PV modules <b>14</b> is fed into the power grid. During EL inspection, the PV array <b>10</b> is disconnected from the power grid.</p><p id="p-0108" num="0107">The setup <b>100</b> further includes a switcher box <b>32</b> that includes three channels <b>34</b>. Each PV string <b>12</b> of the PV array <b>10</b> is connected to a respective channel <b>34</b> of the switcher box <b>32</b>. The setup <b>100</b> further includes a power supply <b>36</b> connected to the switcher box <b>32</b>. The power supply is configured to supply each PV string <b>12</b> with up to 1000 volts of electricity and a minimum current equal to 10% of the short circuit current of the PV modules <b>14</b>. By selectively activating the channels <b>34</b>, an on-site worker <b>30</b> selectively supplies the PV strings <b>12</b> with an electrical current from the power source <b>36</b> which puts the PV strings <b>12</b> under forward bias conditions. When put in the forward bias condition, one or more PV modules <b>14</b> in the PV string <b>12</b> emit light, otherwise known as electroluminescence (EL), and thus produce an EL signal that is detected by an optical sub-system of an aerial vehicle (e.g. an unmanned aerial vehicle (UAV) <b>20</b>).</p><p id="p-0109" num="0108">In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the worker <b>30</b> notices that the PV array <b>10</b> operating normally is generating less electricity than expected. Neither visual, nor infrared inspection indicated a reason for this. After disconnecting the PV array <b>10</b> from the power grid, and electrically connecting the PV array <b>10</b> to the power supply <b>36</b> via the switcher box <b>32</b>, the worker <b>30</b> instructs an assistant <b>22</b> to deploy the UAV <b>20</b> to capture EL images of the PV string <b>12</b> for EL inspection. The UAV <b>20</b> includes a main body <b>210</b>, a propulsion device <b>230</b> attached to the main body <b>210</b> to allow the UAV <b>20</b> to take flight, and the optical sub-system <b>220</b> mounted to the main body <b>210</b> for capturing the EL images.</p><p id="p-0110" num="0109">While the assistant <b>22</b> is provided in this embodiment, it should be clear that the worker <b>30</b> may deploy the UAV <b>20</b> without help from the assistant <b>22</b>. Additionally, it should also be noted that multiple PV strings <b>12</b> may be connected to one channel <b>34</b>. For example, all three PV strings <b>12</b> of the PV array <b>10</b> may be connected to a single channel <b>34</b>. In this scenario, all three PV strings <b>12</b> are simultaneously put under forward bias conditions, and the EL images of the entire PV array <b>10</b> are captured. Notably, the amount of current supplied by the power supply <b>36</b> is lower in this scenario compared to when each channel <b>34</b> is connected to respective PV strings <b>12</b> although this does not affect the PV strings <b>12</b> being put under forward bias conditions.</p><p id="p-0111" num="0110">Furthermore, a larger PV array may include multiple combiner boxes <b>16</b> which are then connected to the inverter box (not shown). Alternatively, the PV array <b>10</b> may not include the combiner box <b>16</b>, and instead, the PV strings <b>12</b> are directly connected to the inverter box.</p><p id="p-0112" num="0111">Preferably, each PV string <b>12</b> is supplied with 100% of the short circuit current of the PV modules <b>14</b>. However, this is not necessary. For example, each PV string <b>12</b> may be supplied with a current equal to 60% of the short circuit current of the PV modules <b>14</b>. A measurement of the same PV array sub section at multiple injection currents may be used to estimate electrical properties of the PV modules <b>14</b> and to identify current-dependent defects.</p><p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a system architecture of a system <b>200</b> of capturing and processing images. The system <b>200</b> includes the unmanned aerial vehicle (UAV) <b>20</b> and an image processing device <b>260</b>. In addition to the optical sub-system <b>220</b>, and the propulsion device <b>230</b>, the UAV <b>20</b> further includes an onboard processing sub-system <b>240</b> and a power source <b>242</b> (e.g. batteries). The power source <b>242</b> is connected to, and powers, the optical sub-system <b>220</b>, the propulsion device <b>230</b>, and the onboard processing sub-system <b>240</b>. The onboard processing sub-system <b>240</b> is communicatively coupled to the optical sub-system <b>220</b> and the propulsion device <b>230</b>, and is configured to control the optical sub-system <b>220</b> and the propulsion device <b>230</b> to perform various functions.</p><p id="p-0114" num="0113">The optical sub-system <b>220</b> is described first with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The optical sub-system <b>220</b> includes a camera <b>222</b> with an optical axis <b>222</b><i>a </i>and in this embodiment, the camera <b>222</b> is a video camera operable to take monochromatic video recordings. The camera <b>222</b> is sensitive in the near- and/or short-infrared (NIR, SWIR) EL waveband, and is suited for capturing EL images in such wavebands. The camera <b>222</b> includes a focusing lens <b>223</b> which is also suitable for use in the NIR/SWIR EL waveband. The lens <b>223</b> (e.g. motorized focus lens, voltage-controlled polymer lens or liquid lens) allows the onboard processing sub-system <b>240</b> to adjust the focus of the lens <b>223</b> depending on the distance of the lens <b>223</b> to the PV array <b>10</b>. The camera <b>222</b> further includes a lens filter (not shown) for filtering out any unwanted spectrum of light.</p><p id="p-0115" num="0114">The optical sub-system <b>220</b> further includes an optical distance measurement device (such as a Light Detection And Ranging device (LIDAR) <b>224</b>). The LIDAR's optical axis (<b>224</b><i>a</i>) is aligned to the optical axis <b>222</b><i>a </i>of the camera <b>222</b>. The LIDAR <b>224</b> is operable to measure distance of the optical sub-system <b>220</b> from the PV array <b>10</b>.</p><p id="p-0116" num="0115">The optical sub-system <b>220</b> further includes a focused light source (such as a laser <b>226</b>). The laser's optical axis (<b>226</b><i>a</i>) is also aligned to the optical axis <b>222</b><i>a </i>of the camera <b>222</b>. The laser <b>226</b> is arranged to emit light in the visible spectrum, and has a beam divergence that is not larger than the camera's field-of-view (FOV) which minimizes optical interference from the laser. Furthermore, the laser <b>226</b> allows for low power operation, emits light in a narrow waveband, and creates focused shapes which are easily identified by the worker <b>30</b>. The focussed shapes are non-symmetrical which beneficially allows the worker <b>30</b> to identify where the camera <b>222</b> is pointing at, and also identify a rotation of the camera's FOV.</p><p id="p-0117" num="0116">The optical sub-system <b>220</b> further includes a single-axis gimbal <b>228</b> (shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) which attaches the optical sub-system <b>220</b> to the main body <b>210</b> of the UAV <b>20</b>. The onboard processing sub-system <b>240</b> controls the gimbal to raise/lower the optical axis <b>222</b><i>a </i>of the camera <b>222</b> with one degree of freedom (i.e. pitch).</p><p id="p-0118" num="0117">Alternatively, the optical sub-system <b>220</b> may be mounted to the main body <b>210</b> via a two-axis or a three-axis gimbal to allow for further degrees of freedom (yaw, roll) for adjusting the optical axis <b>222</b><i>a </i>of the camera <b>222</b> and provide enhanced stability of the FOV. Furthermore, the focused shapes created by the laser <b>226</b> may be symmetrical. An LED may also be used in place of the laser <b>226</b>. The focus of the lens <b>223</b> may be adjustable, either mechanically or electrically driven.</p><p id="p-0119" num="0118">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the propulsion device <b>230</b> is described next. The propulsion device <b>230</b> includes four sets of propellers <b>232</b> driven by respective motors <b>234</b> to allow the UAV to take flight and perform aerial maneuvers such as rotating about the aerial vehicle's yaw axis <b>210</b><i>a </i>(see <figref idref="DRAWINGS">FIG. <b>1</b></figref>). The yaw axis <b>210</b><i>a </i>is a vertical axis that runs through a middle portion of the main body <b>210</b> when the UAV <b>20</b> is upright.</p><p id="p-0120" num="0119">The onboard processing sub-system <b>240</b> includes a controller <b>250</b> and a memory unit <b>252</b>. The controller is configured to execute five functions (FOCUS, POINT, FIND, ALIGN, SCAN, AUTO) according to a set of instructions stored in the memory unit <b>252</b>. The controller <b>250</b> receives information from the optical sub-system <b>220</b> including the distance from the PV array <b>10</b> to the LIDAR <b>224</b>, as well as the camera's visual feed. Using the information received from the optical sub-system <b>220</b>, the controller <b>250</b> is configured to operate the optical sub-system <b>220</b> and the propulsion device <b>230</b> to execute the functions POINT, FIND, FOCUS, ALIGN, SCAN and AUTO algorithms. Once the EL images are captured, the UAV <b>20</b> returns to its base to transfer the EL images to the image processing device <b>260</b> for further processing.</p><p id="p-0121" num="0120">The image processing device <b>260</b> is configured to execute the functions FREEZE and MAP. The image processing device <b>260</b> includes a frame extraction module <b>270</b>, an image enhancement module <b>280</b>, a mapping module <b>290</b>, and an image processor <b>300</b>. The image processing device <b>260</b> takes the EL images as input, and outputs an enhanced EL image of the PV array <b>10</b>.</p><p id="p-0122" num="0121">The operation of each component of the aerial vehicle is described in more detail in the following section.</p><p id="p-0123" num="0122"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram for an exemplary method <b>400</b> of capturing and processing the EL images by the system <b>200</b>. The exemplary method <b>400</b> is described alongside corresponding <figref idref="DRAWINGS">FIGS. <b>5</b> to <b>26</b>B</figref>, where applicable. In this embodiment, the UAV <b>20</b> is deployed to perform EL inspection of a PV array <b>40</b>, preferably performed at night or under low natural light conditions. The PV array <b>40</b> is similar to the PV array <b>10</b>, except the PV array <b>40</b> includes more PV modules. The combiner boxes (not shown) of the respective sets from the PV array <b>10</b> are connected to a switcher box <b>532</b> which is controlled by the onsite worker <b>30</b>.</p><p id="p-0124" num="0123">At step <b>410</b> of the method <b>400</b>, the controller <b>250</b> executes the POINT function. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram <b>500</b> of the UAV <b>20</b> performing the POINT function. Upon deployment of the UAV <b>20</b>, the controller <b>250</b> is configured to control the power source <b>250</b> to supply power to the laser <b>226</b>. The worker <b>30</b> spots the area <b>530</b> illuminated by the laser <b>226</b> and identifies where the camera <b>222</b> is pointing at. The laser has a light intensity that is within a safe range (laser: Class 1 or 2) so that the onsite worker <b>30</b> will not sustain any eye damage even in the event of unintentional direct eye exposure to the laser <b>226</b>. The laser <b>226</b> is switched on throughout most of the operation of the UAV <b>20</b>. This allows the worker <b>30</b> to identify quickly where the camera <b>222</b> is pointing at, especially when it is not obvious which PV string is currently under forward bias. The laser <b>226</b> is turned off right before the UAV <b>20</b> executes the SCAN function so that the laser <b>226</b> does not appear in the EL images captured by the camera <b>222</b>.</p><p id="p-0125" num="0124">The worker <b>30</b> consults a string connection schematic which informs the worker <b>30</b> which PV string is put under forward bias according to the channel that is activated/open. In this embodiment, the string connection schematic contains an error and the worker <b>30</b> is informed that for a particular channel, a PV string <b>512</b><i>a </i>is put under forward bias. In actuality, another PV string <b>512</b><i>b </i>is put under forward bias, and one or more PV modules <b>514</b><i>b </i>of the PV string <b>512</b><i>b </i>emits an EL signal. The PV strings <b>512</b><i>a</i>, <b>512</b><i>b </i>are part of the PV array <b>40</b>, and is also referred to as a PV array subsection <b>512</b><i>a</i>, <b>512</b><i>b </i>of the PV array <b>40</b>.</p><p id="p-0126" num="0125">After activating the particular channel, the worker <b>30</b> manually guides the UAV <b>20</b> to the PV string <b>512</b><i>a </i>along a flight path <b>520</b>. The worker <b>30</b> notices that no EL signal is being emitted by the PV string <b>512</b><i>a </i>and deduces that there is an error in the string connection schematic. In order to determine the location of the PV string <b>512</b><i>b </i>that is under forward bias i.e. emitting an EL signal, the worker <b>30</b> directs the controller <b>250</b> to initiate the FIND function.</p><p id="p-0127" num="0126">It should be noted that it is not necessary for the worker <b>30</b> to manually guide the UAV <b>20</b> to the PV string <b>512</b><i>a</i>. The worker <b>30</b> may initiate the FIND function immediately after deployment of the UAV <b>20</b> thus obviating the POINT function. Alternatively, the controller <b>250</b> may also be configured to initiate the FIND function automatically upon deployment of the UAV <b>20</b>.</p><p id="p-0128" num="0127">At step <b>420</b> of the method <b>400</b>, the controller <b>250</b> executes the FIND function. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram <b>600</b> of the UAV <b>20</b> performing a first part of the FIND function. Notably, the worker <b>30</b> and the illuminated area <b>530</b> are not illustrated in <figref idref="DRAWINGS">FIG. <b>6</b><i>a </i></figref>(and subsequent figures). The first part of the FIND function involves setting the UAV <b>20</b> to an initial position. Upon initiation, the controller <b>250</b> is configured to dynamically adjust the propulsion device <b>220</b> to maneuver the UAV <b>20</b> to the initial position in which the UAV's yaw axis <b>210</b><i>a </i>is perpendicular to the ground. The controller <b>250</b> is further configured to adjust the gimbal <b>228</b> dynamically such that the camera's optical axis <b>222</b><i>a </i>is also perpendicular to the ground, or in other words, the camera's field-of-view shows the area directly below the UAV <b>20</b>. In this position, the optical axis <b>222</b><i>a </i>has an angle of 0&#xb0;.</p><p id="p-0129" num="0128">In addition, the controller <b>250</b> is further configured to adjust dynamically the propulsion device <b>220</b> to maneuver the UAV <b>20</b> (along the UAV's yaw axis <b>210</b><i>a</i>) to a predefined elevation <b>610</b> from the ground.</p><p id="p-0130" num="0129"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram <b>700</b> of the UAV <b>20</b> performing a second part of the FIND function. The second part of the FIND function involves performing a sweep of the PV array <b>40</b> in a sweeping path that starts from the area directly below the UAV <b>20</b> and spirals outwards. To perform the sweep, the controller <b>250</b> is configured to adjust dynamically the propulsion device <b>220</b> to rotate (see arrow <b>710</b>) the UAV <b>20</b> about the UAV's yaw axis <b>210</b><i>a</i>. The controller <b>250</b> is further configured to simultaneously increase <b>720</b> the camera's optical axis angle. This moves the camera's FOV outwards from the UAV <b>20</b>. In combination with the rotation <b>720</b>, the camera's scanning path forms a spiral <b>810</b>. This is illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> which illustrates a schematic diagram <b>800</b> depicting the results of the second part of the FIND function.</p><p id="p-0131" num="0130">To give an example, in the initial position, the UAV hovers at a height of 10 m above the ground. If the camera's optical axis angle is moved from 0&#xb0; to 70&#xb0;, a radial area of 55 meters is observed (using the law of sines:</p><p id="p-0132" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <mn>2</mn>    <mo>&#xb7;</mo>    <mn>10</mn>   </mrow>   <mo>&#x2062;</mo>   <mrow>    <mi>m</mi>    <mo>&#xb7;</mo>    <mfrac>     <mrow>      <mi>sin</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mn>50</mn>       <mo>&#x2062;</mo>       <mo>&#xb0;</mo>      </mrow>      <mo>)</mo>     </mrow>     <mrow>      <mi>sin</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mn>20</mn>       <mo>&#x2062;</mo>       <mo>&#xb0;</mo>      </mrow>      <mo>)</mo>     </mrow>    </mfrac>   </mrow>  </mrow>  <mo>&#x2248;</mo>  <mrow>   <mn>55</mn>   <mo>&#x2062;</mo>   <mi>m</mi>   <mo>&#x2062;</mo>   <mrow>    <mo>)</mo>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0133" num="0000">The camera <b>222</b> has an angle-of-view of 60&#xb0;. This results in a field of view of 10 m for a view pointing directly towards the ground</p><p id="p-0134" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mtext>	</mtext>  <mrow>   <mrow>    <mo>(</mo>    <mrow>     <mrow>      <mrow>       <mn>2</mn>       <mo>&#xb7;</mo>       <mn>10</mn>      </mrow>      <mo>&#x2062;</mo>      <mrow>       <mi>m</mi>       <mo>&#xb7;</mo>       <mrow>        <mi>sin</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mstyle><mtext>?</mtext></mstyle>        <mo>)</mo>       </mrow>      </mrow>     </mrow>     <mo>=</mo>     <mrow>      <mn>10</mn>      <mo>&#x2062;</mo>      <mi>m</mi>     </mrow>    </mrow>    <mo>)</mo>   </mrow>   <mo>.</mo>  </mrow> </mrow></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0135" num="0000">Thus, three rotations are sufficient to cover the PV array <b>40</b> which has a 25 m radius.</p><p id="p-0136" num="0131">Notably, the camera's optical axis angle is increased at a decreasing pitch speed. From a perspective of the gimbal <b>228</b>, the gimbal's pitch speed is decreased with increasing pitch angle. Since the scanning path <b>810</b> increases with 2&#x3c0; multiplied by radius, the camera's FOV travels along a five times larger distance (<b>2</b>&#x3c0;&#xb7;5 m&#x2248;31; 2&#x3c0;&#xb7;25 m&#x2248;157 m) in its last rotation. In consequence, the camera's rotation <b>710</b> or yaw speed is adjusted to be five times lower at the last rotation.</p><p id="p-0137" num="0132">The camera's yaw speed depends on the amount of motion blur that is acceptable in a frame during an exposure time. For a maximum deflection during exposure of 10 pixels, an exposure time of 7 ms and a horizontal sensor resolution of 640px, a yaw speed of 22 m/s is possible</p><p id="p-0138" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mtext>	</mtext>  <mrow>   <mrow>    <mo>(</mo>    <mrow>     <mrow>      <mstyle><mtext>?</mtext></mstyle>      <mn>10</mn>      <mo>&#x2062;</mo>      <mi>m</mi>     </mrow>     <mo>&#x2248;</mo>     <mrow>      <mn>22</mn>      <mo>&#x2062;</mo>      <mrow>       <mi>m</mi>       <mo>/</mo>       <mi>s</mi>      </mrow>     </mrow>    </mrow>    <mo>)</mo>   </mrow>   <mo>.</mo>  </mrow> </mrow></math></maths><maths id="MATH-US-00003-2" num="00003.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0139" num="0000">With an approximated travelled distance for the whole spiral of 282 m (sum of three circles at 5 m, 15 m, 25 m radius) the method FIND may take at maximum 13 seconds if no PV string under forward bias is detected. While the function FIND is in progress, the controller <b>250</b> checks the images captured by the camera <b>222</b> for features of the forward biased PV string <b>512</b><i>b</i>. The function FIND stops when an EL signal is detected from PV string <b>512</b><i>b</i>. Upon EL detection, the controller <b>250</b> is configured to adjust dynamically the propulsion device <b>220</b> to maneuver the UAV <b>20</b> to the EL signal. In this way, the EL signal is being used as an optical marker to guide the UAV <b>20</b>.</p><p id="p-0140" num="0133">At step <b>430</b> of the method <b>400</b>, the controller <b>250</b> executes the FOCUS function. The controller <b>250</b> receives information regarding the distance of the camera <b>222</b> to the one or more PV modules in the PV string <b>512</b><i>b </i>from the LIDAR <b>224</b>. The controller <b>250</b> is configured to adjust dynamically the camera's focus to match the distance between the camera lens <b>223</b> and a focal point to the distance between the camera lens <b>223</b> and imaged object according to the measured distance to maintain the camera lens' focus.</p><p id="p-0141" num="0134">At step <b>440</b> of the method <b>400</b>, the controller <b>250</b> executes the ALIGN function which is described next in relation to <figref idref="DRAWINGS">FIGS. <b>9</b> to <b>14</b></figref>. <figref idref="DRAWINGS">FIGS. <b>9</b> and <b>10</b></figref> are perspective views <b>900</b>, <b>1000</b> of the UAV <b>20</b> hovering over the PV string <b>512</b><i>b </i>with the camera's FOV misaligned and aligned to the PV string <b>512</b><i>b </i>respectively. <figref idref="DRAWINGS">FIGS. <b>11</b> and <b>12</b></figref> are EL images <b>1100</b>, <b>1200</b> from the perspective of the camera <b>222</b> with the camera's FOV misaligned and aligned respectively. <figref idref="DRAWINGS">FIGS. <b>13</b> and <b>14</b></figref> are schematic diagrams <b>1300</b>, <b>1400</b> of the EL images <b>1100</b>, <b>1200</b> from the perspective of the controller <b>250</b>.</p><p id="p-0142" num="0135">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the camera's FOV <b>910</b> is misaligned relative to the PV string <b>512</b><i>b</i>. The camera's FOV <b>910</b> has to be aligned to the PV string <b>512</b><i>b </i>(as depicted in <figref idref="DRAWINGS">FIG. <b>10</b></figref>), before the method <b>400</b> can proceed to the SCAN function.</p><p id="p-0143" num="0136">The controller <b>250</b> receives an EL image <b>1100</b> from the camera <b>222</b> (as depicted in <figref idref="DRAWINGS">FIG. <b>11</b></figref>). The EL image <b>1100</b> includes the PV string <b>512</b><i>b </i>(or a portion of the PV string <b>512</b><i>b</i>) which appears bright (higher light intensity) due to the EL signal emitted by the one or more PV modules <b>514</b><i>b</i>, compared to the background <b>1110</b> i.e. the ground. The controller <b>250</b> applies an algorithm to determine that the camera's FOV <b>910</b> is misaligned relative to the PV string <b>512</b><i>b</i>. The algorithm utilizes an intensity difference between the bright PV string <b>512</b><i>b </i>and the dark background <b>1110</b> to detect a position and orientation of the PV string <b>512</b><i>b </i>(or of a reference PV module <b>514</b><i>b</i>). Specifically, and with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the algorithm detects the PV string's edges <b>1310</b>, and derive key points <b>1320</b> (e.g. module corner points) from the edges <b>1310</b>. The algorithm then determines a set of aligned points <b>1330</b> corresponding to each key point <b>1320</b> which minimizes the misalignment and any angular or perspective distortions. The aligned points <b>1330</b> are set on upper and lower horizontal indicators <b>1340</b>.</p><p id="p-0144" num="0137">Further, the algorithm also determines an appropriate elevation of the UAV <b>20</b> relative to the PV string <b>512</b><i>b </i>which puts the PV string <b>512</b><i>b </i>at a predefined size ratio within the camera's FOV <b>910</b>. The predefined size ratio is set to keep a space of about 5-10% between the top and bottom of the PV string <b>512</b><i>b </i>and image border <b>1410</b> to allow a tolerance to positional oscillations of the UAV <b>20</b>. In other words, the PV string <b>512</b><i>b </i>occupies 80% to 90% of the camera's FOV at the predefined size ratio.</p><p id="p-0145" num="0138">The algorithm then determines a perspective transformation to align the key points <b>1320</b> to the aligned points <b>1330</b> (as depicted in <figref idref="DRAWINGS">FIG. <b>14</b></figref>). The controller <b>250</b> is then configured to make appropriate adjustments to the gimbal <b>228</b> and the propulsion device <b>220</b> based on the perspective transformation. As can be seen in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the camera's FOV <b>910</b> is aligned to the PV string <b>512</b><i>b</i>, and the PV string <b>512</b><i>b </i>is at the predefined size ratio within the camera's FOV <b>910</b>. Once aligned, the camera's optical axis is perpendicular to the PV string's planar surface. Since the PV string includes one or more PV modules <b>514</b><i>b</i>, the PV string's planar surface is made up of the one or more PV modules' planar surface. The one or more PV modules' planar surface is defined as the surface that is arranged to receive the sunlight.</p><p id="p-0146" num="0139">Notably, the controller <b>250</b> is configured to execute the ALIGN function repeated while the SCAN function is in progress. This ensures that the camera's optical axis is perpendicular to the PV string's planar surface while the EL images are being captured during the SCAN function. Advantageously, this minimizes perspective distortion and increases the image resolution of EL images captured by the camera <b>222</b>. Further, this allows the camera <b>222</b> to capture EL images with a more consistent focus across the EL image. In addition, the EL intensity from each PV module <b>514</b><i>b </i>is captured accurately which is important for analysis purposes.</p><p id="p-0147" num="0140">Notably, if the controller <b>250</b> does not detect an end <b>1210</b> of the PV string <b>512</b><i>b </i>in the EL image <b>1200</b>, the controller <b>250</b> is configured to adjust the propulsion device <b>220</b> to manuever the UAV <b>20</b> along the PV string's longitudinal axis <b>10</b><i>a </i>(refer to <figref idref="DRAWINGS">FIG. <b>1</b></figref>) until the end <b>1210</b> of the PV string <b>512</b><i>b </i>is in the EL image <b>1200</b>.</p><p id="p-0148" num="0141">At step <b>450</b>, the controller <b>250</b> executes the SCAN function which is described next alongside <figref idref="DRAWINGS">FIGS. <b>15</b> to <b>17</b></figref>. <figref idref="DRAWINGS">FIG. <b>15</b></figref> includes <figref idref="DRAWINGS">FIGS. <b>15</b>A to <b>15</b>F</figref> which respectively illustrate a series <b>1500</b> of six consecutive EL frames of the PV string <b>512</b><i>b </i>captured by the camera <b>222</b> at different positions along the PV string <b>512</b><i>b</i>. Notably, the EL frames overlap so that a PV module <b>1510</b> is likely to appear more than once, i.e. in <figref idref="DRAWINGS">FIGS. <b>15</b>A to <b>15</b>E</figref>.</p><p id="p-0149" num="0142">When considering large PV installations and a limited flight time of the UAV <b>20</b>, scanning speed becomes a crucial parameter in determining system efficiency. Long camera exposure times typically result in better image quality, i.e. better signal-to-noise ratio (SNR). However, a long camera exposure time coupled with a fast scanning speed causes motion blur which reduces the image quality. On the other hand, a short camera exposure time results in EL images with too much noise, especially when the injected current is low, which also reduces image quality.</p><p id="p-0150" num="0143">Since the PV module <b>1510</b> appears in <figref idref="DRAWINGS">FIGS. <b>15</b>A-<b>15</b>F</figref>, there are five frames of the PV module <b>1510</b> (which will be extracted at a later stage) available for image averaging (again at a later stage). The SNR of an image average <b>1520</b> increases roughly with the square root of the number of frames available for image averaging (n<sub>frames</sub>). In other words, image noise of the image average <b>1520</b> reduces with the number of frames used to create the image average. The number of frames available for image averaging is calculated in real-time by the controller <b>250</b> since it also dependent on the scanning speed.</p><p id="p-0151" num="0144">Furthermore, even with the ALIGN function being executed repeatedly during the SCAN function, it is difficult for the camera's FOV to remain completely stable throughout the SCAN function. This is especially so considering the positional oscillations of the UAV <b>20</b> due to external forces (such as wind) acting on the UAV <b>20</b>. This is evident in <figref idref="DRAWINGS">FIG. <b>16</b></figref> which is a schematic diagram of the series <b>1500</b> after image alignment of the EL images in <figref idref="DRAWINGS">FIGS. <b>15</b>A-<b>15</b>F</figref> to the PV string <b>512</b><i>b </i>is performed. The positional oscillation of the UAV <b>20</b> is evident in a point-to-point deflection between EL images along the PV string <b>512</b><i>b</i>. While the positional oscillations are mitigated (and mostly corrected) by the ALIGN function, any computation of the scanning speed has to take into consideration this deflection, along with limiting noise and motion blur.</p><p id="p-0152" num="0145">Limiting Noise:</p><p id="p-0153" num="0146">The controller <b>250</b> is configured to perform optical flow analysis (e.g. Lucas-Kanade method) during the SCAN function. For each frame in <figref idref="DRAWINGS">FIGS. <b>15</b>A-<b>15</b>F</figref> that is captured by the camera <b>222</b>, the controller <b>250</b> calculates key points in a current frame, and compares the key points in the current frame with the key points in a preceding frame to determine a length of a deflection vector.</p><p id="p-0154" num="0147"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a schematic diagram <b>1700</b> of two consecutive EL images (i.e. <figref idref="DRAWINGS">FIGS. <b>15</b>D and <b>15</b>E</figref>) of the PV string <b>512</b><i>b </i>showing point-to-point deflection. The preceding EL image of <figref idref="DRAWINGS">FIG. <b>15</b>E</figref> is shown in dotted lines while the current EL image F is shown in bold lines. The deflection of an object <b>1710</b> in an image centre <b>1720</b> is calculated from an average deflection of detected key points in the EL images of <figref idref="DRAWINGS">FIGS. <b>15</b>D and <b>15</b>E</figref>. The length of this deflection vector is referred to as d<sub>f2f</sub>.</p><p id="p-0155" num="0148">The controller <b>250</b> calculates a line <b>1730</b> through the image centre <b>1720</b> and at deflection angle. The line <b>1730</b> intersects the image border <b>1740</b> at intersection points <b>1750</b>, <b>1760</b>. The distance between the intersection points <b>1750</b>, <b>1760</b> represents an object distance travelled through the image plane. The controller <b>250</b> then calculates n<sub>frames </sub>by taking the ratio of the length of the deflection vector d<sub>f2f </sub>to the distance between the intersection points <b>1750</b>, <b>1760</b>.</p><p id="p-0156" num="0149">The impact of noise on the image quality can be quantified with the SNR:</p><p id="p-0157" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>SNR</mi>     <mo>=</mo>     <mfrac>      <mi>Signal</mi>      <mi>Noise</mi>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0158" num="0150">In this embodiment, the SNR is calculated in the following manner. The captured Otsu's method is used to obtain a threshold (t<sub>Otsu</sub>) between the dark background <b>1110</b> and the PV string <b>512</b><i>b </i>(or a portion of the PV string <b>512</b><i>b</i>) which appears brighter due to the EL signal emitted by the one or more PV modules <b>514</b><i>b</i>. The &#x2018;Signal&#x2019; value is obtained by averaging the intensity of all pixels brighter than the threshold, t<sub>Otsu</sub>. The &#x2018;Noise&#x2019; value is obtained before EL measurement from an average of the standard deviation of a pixel of multiple images taken with similar or comparable imaging parameters (e.g. exposure time, sensor temperature and gain) in series.</p><p id="p-0159" num="0151">An SNR-dependent scanning speed factor (or simply SNR scanning factor), f<sub>SNR </sub>is applied on the current scanning speed to ensure that the SNR of the image average <b>1520</b>, SNR<sub>frame </sub>matches a target SNR, SNR<sub>target </sub>using Equation (2)</p><p id="p-0160" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>f</mi>      <mi>SNR</mi>     </msub>     <mo>=</mo>     <mfrac>      <mrow>       <msub>        <mi>SNR</mi>        <mi>frame&#xb0;</mi>       </msub>       <mo>&#x2062;</mo>       <msqrt>        <msub>         <mi>n</mi>         <mi>frames</mi>        </msub>       </msqrt>      </mrow>      <msub>       <mi>SNR</mi>       <mi>target</mi>      </msub>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0161" num="0152">For example, SNR<sub>target </sub>is set at 45 for lab measurements. The camera exposure time is adjusted during the SCAN function to keep the SNR<sub>frame </sub>at 5 (minimum requirement for outdoor measurements). The controller <b>250</b> then estimates that twenty-five EL images are available for image averaging (n<sub>frame</sub>=25). Based on Equation (1),</p><p id="p-0162" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mrow>  <mtext>	</mtext>  <mrow>   <msub>    <mi>f</mi>    <mi>SNR</mi>   </msub>   <mo>=</mo>   <mrow>    <mstyle><mtext>?</mtext></mstyle>    <mo>=</mo>    <mrow>     <mstyle><mtext>?</mtext></mstyle>     <mo>&#x2248;</mo>     <mrow>      <mn>56</mn>      <mo>&#x2062;</mo>      <mrow>       <mi>%</mi>       <mo>.</mo>      </mrow>     </mrow>    </mrow>   </mrow>  </mrow> </mrow></math></maths><maths id="MATH-US-00006-2" num="00006.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0163" num="0000">In other words, the current scanning speed should be reduced to 56% of its current value. In essence, lowering the scanning speed increases the number of frames (n<sub>frames</sub>) available for creating the image average <b>1520</b>.</p><heading id="h-0006" level="2">Limiting Motion Blur:</heading><p id="p-0164" num="0153">To avoid effects of motion blur, object deflection during exposure of a frame should also be below a pre-determined maximum value(d<sub>exp_max</sub>). A value of 0.75 pixel per exposure time is suggested. The frame to frame deflection (d<sub>f2f</sub>) can be scaled into exposure time deflection (d<sub>exp</sub>) using the time difference between two frames (t<sub>f2f</sub>) and exposure time (&#x3c4;<sub>exp</sub>) according to Equation (3).</p><p id="p-0165" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>d</mi>      <mi>exp</mi>     </msub>     <mo>=</mo>     <mrow>      <msub>       <mi>d</mi>       <mrow>        <mi>f</mi>        <mo>&#x2062;</mo>        <mn>2</mn>        <mo>&#x2062;</mo>        <mi>f</mi>       </mrow>      </msub>      <mo>&#xb7;</mo>      <mfrac>       <msub>        <mi>t</mi>        <mrow>         <mi>f</mi>         <mo>&#x2062;</mo>         <mn>2</mn>         <mo>&#x2062;</mo>         <mi>f</mi>        </mrow>       </msub>       <msub>        <mi>t</mi>        <mi>exp</mi>       </msub>      </mfrac>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0166" num="0154">A motion blur dependent scanning factor (f<sub>blur</sub>) is equal to a ratio of the maximum object deflection d<sub>exp_max </sub>to the current object deflection d<sub>exp </sub>as shown in Equation (4).</p><p id="p-0167" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>f</mi>      <mi>blur</mi>     </msub>     <mo>=</mo>     <mfrac>      <msub>       <mi>d</mi>       <mi>exp_max</mi>      </msub>      <msub>       <mi>d</mi>       <mi>exp</mi>      </msub>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0168" num="0155">A scanning speed factor (f<sub>scan</sub>) is obtained from a minimum of both factors (f<sub>SNR</sub>, f<sub>blur</sub>) as shown in Equation (5):</p><p id="p-0169" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>f</i><sub>scan</sub>=min(<i>f</i><sub>SNR</sub><i>,f</i><sub>blur</sub>)&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0170" num="0156">To ensure high EL image quality, a maximum set scanning speed, v<sub>quality </sub>is obtained using Equation (6):</p><p id="p-0171" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i><sub>quality</sub>=min(<i>v</i><sub>max</sub><i>,f</i><sub>scan</sub><i>&#xd7;v</i><sub>cur</sub>)&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0172" num="0157">The maximum set scanning speed, v<sub>quality </sub>defines the maximum scanning speed at which a high EL image quality can still be achieved.</p><p id="p-0173" num="0158">According to Equation (6), a target scanning speed is calculated by multiplying the scanning speed factor, f<sub>scan </sub>with the current flight speed, v<sub>cur</sub>. If the target scanning speed is below a maximum flight speed, v<sub>max </sub>of the UAV <b>20</b>, then the target scanning speed is selected as the maximum set scanning speed, v<sub>quality</sub>. In other words, even though the UAV <b>20</b> is able to move faster up to its maximum flight speed, v<sub>max</sub>, since this reduces the image quality of the EL images, the maximum set scanning speed, v<sub>quality </sub>is set below the maximum flight speed v<sub>max</sub>.</p><p id="p-0174" num="0159">If the maximum set scanning speed exceeds the maximum flight speed, v<sub>max </sub>of the UAV <b>20</b>, then the maximum flight speed, v<sub>max </sub>is selected as the maximum set scanning speed, v<sub>quality</sub>.</p><p id="p-0175" num="0160">The target flight speed, v<sub>target </sub>is obtained from the maximum set scanning speed, v<sub>quality</sub>, and a user input factor, f<sub>user </sub>according to Equation (7).</p><p id="p-0176" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i><sub>target</sub><i>=v</i><sub>quality</sub><i>&#xb7;f</i><sub>user</sub>&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0177" num="0161">The user input factor, f<sub>user </sub>is obtained from a deflection of a joystick controlled by the worker <b>30</b> remotely, and ranges from 0% to 100%. At 100%, the target flight speed, v<sub>target </sub>is simply the maximum set scanning speed, v<sub>quality</sub>.</p><p id="p-0178" num="0162">A smoothing technique is applied to the target flight speed, v<sub>target </sub>to minimise jerky movement of the UAV <b>20</b>. In this embodiment, exponential moving average is used to obtain a set speed, v<sub>set </sub>according to Equation (8). &#x3b1; is a smoothness factor within a range of 0 to 100%.</p><p id="p-0179" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i><sub>set</sub>=(&#x3b2;&#xd7;<i>v</i><sub>cur</sub>)+((1&#x2212;&#x3b1;)&#xb7;<i>v</i><sub>target</sub>)&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0180" num="0163">Two exemplary embodiments of the SCAN function are described next with reference to <figref idref="DRAWINGS">FIGS. <b>18</b>A and <b>18</b>B</figref> which are line graphs <b>1800</b><i>a</i>, <b>1800</b><i>b </i>showing the current flight speed, v<sub>cur </sub>of the UAV <b>20</b> decreasing and increasing respectively over time in accordance with the SCAN function. For both embodiments, the user input factor, f<sub>user </sub>is taken to be 100%, and the maximum flight speed, v<sub>max </sub>of the UAV <b>20</b> is taken to be 9 m/s.</p><p id="p-0181" num="0164">Referring to <figref idref="DRAWINGS">FIG. <b>18</b>A</figref>, at time=1s, and with the UAV <b>20</b> moving at a current flight speed, v<sub>cur </sub>of 6 m/s, the controller <b>250</b> obtains a scanning speed factor, f<sub>scan1 </sub>of 50% from Equation (5). If the scanning speed factor is below 100%, this indicates that the UAV <b>20</b> is moving faster than the UAV <b>20</b> should. The controller <b>250</b> then determines the target scanning speed to be 3 m/s. Since the target scanning speed is below the maximum flight speed, v<sub>max </sub>of the UAV <b>20</b>, the target scanning speed is selected as the maximum set scanning speed, v<sub>quality </sub>according to Equation (6).</p><p id="p-0182" num="0165">Notably, since the user input factor, f<sub>user </sub>is 100%, the maximum set scanning speed, v<sub>quality </sub>is also the target flight speed, v<sub>target </sub>according to Equation (7).</p><p id="p-0183" num="0166">The controller <b>250</b> then dynamically decreases the current flight speed of the aerial vehicle until the target flight speed, v<sub>target </sub>is achieved. The smoothing technique according to Equation (8) is applied to minimise the jerky movement of the UAV <b>20</b>, and this can be seen in the smooth transition of the current flight speed, v<sub>cur </sub>of the UAV <b>20</b> from 6 m/s (at time=1s) to 3 m/s (at time=2s).</p><p id="p-0184" num="0167">At time=2s, the controller <b>250</b> obtains a scanning speed factor, f<sub>scan2 </sub>of 100% from Equation (5). At this point, the current flight speed, v<sub>cur </sub>of the UAV <b>20</b> matches the maximum set scanning speed, v<sub>quality</sub>.</p><p id="p-0185" num="0168">Referring to <figref idref="DRAWINGS">FIG. <b>18</b><i>b</i></figref>, at time=1s, and with the UAV <b>20</b> moving at a current flight speed, v<sub>cur </sub>of 3 m/s, the controller <b>250</b> obtains a scanning speed factor, f<sub>scan1 </sub>of 150% from Equation (5). If the scanning speed factor is above 100%, this indicates that the UAV <b>20</b> can move 50% faster while still matching the image quality that is required. The controller <b>250</b> then determines the target scanning speed to be 4.5 m/s.</p><p id="p-0186" num="0169">Since the target scanning speed is below the maximum flight speed, v<sub>max </sub>of the UAV <b>20</b>, instead of selecting the maximum flight speed, v<sub>max </sub>as the maximum set scanning speed, v<sub>quality</sub>, the target scanning speed is selected as the maximum set scanning speed, v<sub>quality </sub>according to Equation (6).</p><p id="p-0187" num="0170">Similarly, since the user input factor, f<sub>user </sub>is 100%, the maximum set scanning speed, v<sub>quality </sub>is also the target flight speed, v<sub>target </sub>according to Equation (7). The smoothing technique according to Equation (8) is also applied to minimise the jerky movement of the UAV <b>20</b>.</p><p id="p-0188" num="0171">The controller <b>250</b> then dynamically increases the current flight speed, v<sub>cur </sub>of the aerial vehicle until the target flight speed, v<sub>target </sub>is achieved. At time=2s, the controller <b>250</b> obtains a scanning speed factor, f<sub>scan2 </sub>of 100% from Equation (5). At this point, the current flight speed, v<sub>cur </sub>of the UAV <b>20</b> matches the maximum set scanning speed, v<sub>quality</sub>.</p><p id="p-0189" num="0172">Notably, the controller <b>250</b> continuously performs the SCAN function until an opposing end of the PV string <b>512</b><i>b </i>is detected. Once the opposing end of the PV string <b>512</b><i>b </i>is detected, the controller <b>250</b> terminates the SCAN function and the EL images are stored in the memory unit <b>252</b>.</p><p id="p-0190" num="0173">At step <b>460</b>, the controller <b>250</b> is configured to execute the AUTO function. <figref idref="DRAWINGS">FIG. <b>19</b></figref>, which includes <figref idref="DRAWINGS">FIGS. <b>19</b>A to <b>19</b>E</figref>, is a schematic diagram showing a time lapse of the UAV <b>20</b> capturing EL video images of the PV array <b>40</b> (using the PV array <b>40</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> as an example). The PV array <b>40</b> is illustrated to include two rows <b>1810</b>, <b>1820</b> of PV strings. The controller <b>250</b> is communicatively coupled to and is able to control the power supply <b>36</b> and the switcher box <b>532</b> (via a wireless connection) which in turn is electrically connected to the PV array <b>40</b>.</p><p id="p-0191" num="0174">At t<b>1</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>, the UAV <b>20</b> starts at an end <b>1810</b><i>a </i>of the first row <b>1810</b> and captures the EL images of the first PV string <b>1812</b><i>a </i>in a scanning direction <b>1830</b>. Upon detecting an end <b>1814</b><i>a </i>of the PV string <b>1812</b><i>a</i>, the controller <b>250</b> is configured to instruct the switcher box <b>532</b> to close the current channel, and open the next channel for the next PV string, <b>1812</b><i>b</i>. Notably, the current injected into the PV string <b>1812</b><i>b </i>is maintained at a same level as the PV string <b>1812</b><i>a</i>. The process continues until the controller <b>250</b> detects that it has reached the last PV string <b>1812</b><i>f </i>at an opposing end <b>1810</b><i>b </i>of the first row <b>1810</b> of PV strings at t<b>2</b> as shown in <figref idref="DRAWINGS">FIG. <b>19</b>B</figref>.</p><p id="p-0192" num="0175">At t<b>3</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>19</b>C</figref>, the controller <b>250</b> is configured to instruct the power supply <b>36</b> to lower the current injected into the PV string <b>1812</b><i>f</i>. The purpose of lowering the injection current is to capture low-current EL images for comparison with the higher current EL images. The controller <b>250</b> then dynamically adjusts the propulsion device <b>220</b> to move the UAV <b>20</b> in a scanning direction <b>1840</b>, which is opposite to the scanning direction <b>1830</b>. The UAV <b>20</b> moves along the scanning direction <b>1840</b> and captures the EL images of the PV strings in the first row <b>1810</b> at a lower injection current.</p><p id="p-0193" num="0176"><figref idref="DRAWINGS">FIG. <b>19</b>D</figref> illustrates that at t<b>4</b>, the controller <b>250</b> detects that the UAV <b>40</b> has reached the end <b>1810</b><i>a </i>of the first row <b>1810</b> of PV strings. At this point, the controller <b>250</b> is configured to instruct the switcher box <b>532</b> to close the current channel and open the next channel to put the first PV string <b>1820</b><i>a </i>of the second row <b>1820</b> under forward bias. The controller <b>250</b> is further configured to execute the FIND function to locate the EL signal emitted by the PV string <b>1820</b><i>a</i>. Notably, the PV string <b>1820</b><i>a </i>is within the camera's FOV during the FIND function, and the controller <b>250</b> is able to locate the PV string <b>1820</b><i>a. </i></p><p id="p-0194" num="0177">At t<b>5</b> in <figref idref="DRAWINGS">FIG. <b>19</b>E</figref>, the controller <b>250</b> is configured to navigate the UAV <b>20</b> to the PV string <b>1820</b><i>a</i>. Once the UAV <b>20</b> has reached the PV string <b>1820</b><i>a</i>, the controller <b>250</b> is configured to execute the SCAN function once again. The EL images of the PV strings in the second row <b>1820</b> are captured using a similar process used to capture the EL images of the PV strings in the first row <b>1810</b> (as detailed in t<b>1</b> to t<b>4</b>). Notably, the controller <b>250</b> is configured to execute the ALIGN function throughout the duration of the SCAN function. The captured EL images are stored in the memory unit <b>252</b>.</p><p id="p-0195" num="0178"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a schematic diagram <b>1900</b> of a file structure for the stored EL images. The file structure includes the stored EL images <b>1910</b>, and an appended block including additional meta data <b>1920</b>. The meta data <b>1920</b> is separated into a header <b>1930</b> and a body <b>1940</b>. The header <b>1922</b> includes information on the image correction methods applied on the EL images <b>1910</b> before the EL images <b>1910</b> are saved. The image correction methods include dark current subtraction, flat filed correction, bad pixel substitution, and lens distortion removal. The body <b>1924</b> stores image-dependent data which include camera exposure time and gain, UAV geo-location, camera orientation (yaw, pitch, roll), injection current, voltage, and channel information.</p><p id="p-0196" num="0179">The camera <b>222</b> captures/digitizes the EL images <b>1910</b> at a bit depth larger than 8-bit (e.g. 14- or 16-bit). This allows resolving an image intensity range more precisely than within the 255 brightness steps of a monochromatic 8-bit sensor. To reduce the file size, an image encoder based on 8-bit images is used. An upper and lower intensity range of each EL image <b>1910</b> is stored in the meta data <b>1920</b>. The upper and lower intensity range is obtained from the effective dynamic range of each EL image <b>1910</b> captured by camera <b>222</b>. The range can be used to scale every 8-bit EL image to respective lower and upper intensity range of the original higher depth camera image.</p><p id="p-0197" num="0180">Once the UAV <b>20</b> returns to its base, the stored EL images are then transferred to the image processing device <b>260</b> for further processing.</p><p id="p-0198" num="0181">At step <b>470</b>, the image processing device <b>260</b> executes the FREEZE function which is described next with reference to <figref idref="DRAWINGS">FIGS. <b>21</b> to <b>24</b></figref>. For the sake of brevity, the FREEZE function is described with reference to processing the EL images that include the PV string <b>512</b><i>b </i>only. It should be understood that the FREEZE function may process every EL image in a similar manner.</p><p id="p-0199" num="0182">The FREEZE function includes (i) a frame extraction step performed by the frame extraction module <b>270</b>; and (ii) an image enhancement step performed by the image enhancement module <b>280</b>.</p><p id="p-0200" num="0183"><figref idref="DRAWINGS">FIG. <b>21</b></figref> includes <figref idref="DRAWINGS">FIGS. <b>21</b>A to <b>21</b>C</figref> which respectively illustrates three consecutive EL frame images <b>2000</b><i>a</i>, <b>2000</b><i>b</i>, <b>2000</b><i>c </i>of the PV string <b>512</b><i>b </i>being processed as part of the frame extraction step. The image processor <b>300</b> instructs the frame extraction module <b>270</b> to determine respective corner points <b>2010</b> of each PV module <b>514</b><i>b </i>in the PV string <b>512</b><i>b </i>from all three EL frame images <b>2000</b><i>a</i>, <b>2000</b><i>b</i>, <b>2000</b><i>c</i>. The detected corner points of each PV module <b>514</b><i>b </i>are shown as black dots <b>2012</b> in each EL frame image <b>2000</b><i>a</i>, <b>2000</b><i>b</i>, <b>2000</b><i>c</i>. Notably, in this embodiment, the frame extraction module <b>270</b> fails to detect a particular corner point <b>2014</b> of a particular PV module <b>2016</b> in the first EL frame image <b>2000</b><i>a </i>in <figref idref="DRAWINGS">FIG. <b>21</b>A</figref>.</p><p id="p-0201" num="0184">All corner points <b>2010</b> detected in the first EL frame image <b>2000</b><i>a </i>of <figref idref="DRAWINGS">FIG. <b>21</b>A</figref> are visualized as empty dots <b>2020</b> in the second EL frame image <b>2000</b><i>b </i>of <figref idref="DRAWINGS">FIG. <b>21</b>B</figref>. Notably the empty dots <b>2030</b> are shifted slightly to the left compared to their corresponding black dots <b>2012</b> in the first EL frame image <b>2000</b><i>a </i>of <figref idref="DRAWINGS">FIG. <b>21</b>A</figref>. This is due to the correction of point-to-point deflection. Once a local corner point density exceeds a certain threshold (two detected corner points <b>2010</b> is sufficient in this embodiment), the image processor <b>300</b> controls the frame extraction module <b>270</b> to generate a cluster point <b>2022</b>. The cluster point's position is obtained by taking an average of all the corner points <b>2010</b> used to generate the cluster point <b>2022</b>. This beneficially reduces the spatial detection error of individual corner points.</p><p id="p-0202" num="0185">Referring to <figref idref="DRAWINGS">FIG. <b>21</b>C</figref>, a cluster point <b>2022</b> is stored once the cluster point <b>2022</b> moves outside the EL frame image's border, as illustrated by the cluster point <b>2030</b> in the EL image <b>2000</b><i>c</i>. Cluster points <b>2022</b> inside the EL frame image's border are discarded once too many EL frame images add no more corner points <b>2010</b> to the cluster point. Cluster points <b>2022</b> inside the EL frame image's border, such as the cluster point <b>2032</b> in the EL image <b>2000</b><i>c</i>, are kept as long as a ratio of the EL frame images adding new corner points <b>2010</b> to the EL frame images adding no corner points <b>2010</b> is above a certain threshold. All cluster points <b>2022</b> are then meshed by the frame extraction module <b>270</b> to generate/construct rectangular-like quadrilaterals or frames. Each frame is constructed from the cluster points <b>2022</b> of the respective PV modules that is contained in each frame.</p><p id="p-0203" num="0186"><figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates the frames <b>2100</b> that are extracted from the frame extraction step being processed in the image enhancement step. The image processor <b>300</b> instructs the image enhancement module <b>280</b> to assign a horizontal and vertical module index <b>2110</b> to each frame <b>2100</b> according to its position in the PV string <b>512</b><i>b</i>. For example, the first frame which is associated with a particular PV module <b>2514</b> is located in the first row, and first column of the PV string <b>512</b><i>b</i>, and is assigned the horizontal and vertical module index [<b>1</b>,<b>1</b>].</p><p id="p-0204" num="0187">The image processor <b>300</b> further controls the image enhancement module <b>280</b> to group the frames <b>2100</b> according to the PV module <b>514</b><i>b </i>contained in each frame (or similarly, according to their module index <b>2110</b>). Each frame <b>2100</b> includes four cluster points <b>2022</b> (respectively marked CA&#x2032; to D&#x2032;). The frames <b>2100</b> in each group are then arranged in a stacked arrangement (referred to as a stack) so that the cluster points <b>2022</b> that are marked with the same alphabet (&#x2018;A&#x2019;-&#x2018;D&#x2019;) are stacked on top of each other. An exemplary stack <b>2120</b> having the module index [1,1] is shown in <figref idref="DRAWINGS">FIG. <b>22</b></figref>.</p><p id="p-0205" num="0188">The image enhancement module <b>280</b> is further configured to discard the area <b>2124</b> that are not part of the frames <b>2100</b>.</p><p id="p-0206" num="0189">Using the exemplary stack <b>2120</b> as an example, the image enhancement module <b>280</b> is further configured to determine a reference frame having a highest image quality from the frames <b>2100</b> within the exemplary stack <b>2120</b>. The image quality is evaluated based on sharpness, SNR and completeness of the PV module <b>514</b><i>b </i>within the frames.</p><p id="p-0207" num="0190">The image enhancement module <b>280</b> is further configured to perform image alignment of the frames <b>2100</b>. This is done using an image alignment algorithm such as &#x2018;Parametric Image Alignment using Enhanced Correlation Coefficient&#x2019;. The image enhancement module <b>280</b> aligns the remaining frames <b>2100</b> in the exemplary stack <b>2120</b> to the reference frame. Image alignment is done by aligning the cluster points <b>2022</b> that are marked &#x2018;A&#x2019; in the remaining frames to the corresponding cluster point <b>2022</b> that is marked &#x2018;A&#x2019; in the reference frame to obtain image-aligned frames <b>2130</b>.</p><p id="p-0208" num="0191">The image enhancement module <b>280</b> is further configured to perform image averaging on the image-aligned frames <b>2130</b> to obtain an enhanced frame <b>2140</b> of the particular PV module <b>2514</b>. Image averaging is performed using a super-resolution routine such as weighted image stack averaging <b>2032</b> and/or a dedicated deep convolutional network structure <b>2034</b>. The enhanced frame <b>2140</b> has higher SNR (i.e. lower noise) and higher resolution (up to a resolution improvement factor of three) than the reference frame.</p><p id="p-0209" num="0192">The same process is repeated for the remaining stacks to obtain respective enhanced frames for the remaining PV modules <b>514</b><i>b </i>in the PV string <b>512</b><i>b</i>. The image processor <b>300</b> further controls the image enhancement module <b>280</b> to determine the respective corner points <b>2010</b> of each enhanced frame and to remove any remaining perspective distortion in the enhanced frame.</p><p id="p-0210" num="0193">The image processor <b>300</b> further controls the image enhancement module <b>280</b> to arrange the enhanced frames according to their module index <b>2110</b> to produce an enhanced EL image of the PV string. If distances between the PV modules <b>514</b><i>b </i>in the PV string <b>512</b><i>b </i>are similar, a single enhanced EL image is produced. If distances vary due to a large gap <b>2112</b> between two PV modules (which indicate that one of the PV modules belong to a separate PV string <b>512</b><i>c</i>), then a separate enhanced EL image is produced for the separate PV string <b>512</b><i>c. </i></p><p id="p-0211" num="0194">The image enhancement module <b>280</b> is further configured to scale the image intensities of each enhanced frame to reflect an intensity spectrum from the darkest to the brightest PV module <b>514</b><i>b </i>in the PV string <b>512</b><i>b</i>. Since the intensity scaling reduces a depth resolution of the PV module's intensity range, the enhanced frames for the respective PV modules <b>514</b><i>b </i>are stored together with the enhanced EL image of the PV string.</p><p id="p-0212" num="0195">During image processing, image intensities are expressed in real or floating-point values. When visually displaying the resulting images, image intensities have to be assigned a brightness value between a darkest and a brightest displayable value. To reduce the influence of pixels with extreme image (or pixel) intensity values, a brightness range is defined by a lowest pixel intensity bin (dotted line <b>2201</b><i>a</i>) and a highest pixel intensity bin (dotted line <b>2201</b><i>b</i>) that contains a minimum number of pixels (referred before as the effective dynamic range).</p><p id="p-0213" num="0196"><figref idref="DRAWINGS">FIG. <b>23</b></figref> displays a pixel intensity histogram of the enhanced frame <b>2140</b>. Two peaks <b>2200</b><i>a</i>,<b>2200</b><i>b </i>can be seen corresponding to pixel intensities of a dark background and a bright EL signal respectively. A brightness range between the lowest pixel intensity bin (dotted line <b>2201</b><i>a</i>) and the highest pixel intensity bin (dotted line <b>2201</b><i>b</i>) is defined to exclude pixel intensities which do not occur in many pixels. In <figref idref="DRAWINGS">FIG. <b>23</b></figref>, pixel intensities below the lowest pixel intensity bin (left of the dotted line <b>2201</b><i>a</i>) is set to the minimum value (0) and pixel intensities above the highest pixel intensity bin (right of the dotted line <b>2201</b><i>b</i>) will be set to the maximum value that can be saved according to a chosen precision (2*&#x2212;1=255, for 8 bit images). Image intensities in-between will be scaled between the minimum value (0) and the maximum value (255) of the defined brightness range.</p><p id="p-0214" num="0197"><figref idref="DRAWINGS">FIG. <b>24</b></figref> illustrates the enhanced EL image <b>2200</b> of the PV string <b>512</b><i>b </i>after the image enhancement step is completed, and it is noted that resolution of the enhanced EL image <b>2200</b> is better than the resolution of the reference frame. Notably, independent of the scanning direction <b>2210</b>, i.e. left-to-right, right-to-left, top-to-bottom, or any other combination, the enhanced EL image <b>2200</b> always aligns with the camera's yaw (i.e. optical axis <b>222</b><i>a</i>) since the EL images <b>2000</b><i>a</i>,<b>2000</b><i>b</i>,<b>2000</b><i>c </i>are captured with the camera's FOV <b>910</b> aligned to the PV string <b>512</b><i>b</i>. For ease of reference, a black triangle <b>2220</b> is used to indicate a bottom-left corner of the enhanced EL image <b>2200</b> with the enhanced EL image <b>2200</b> aligned with the camera's FOV <b>910</b>.</p><p id="p-0215" num="0198">Existing known methods may then be used to process the enhanced EL image <b>2200</b> to identify any defective PV modules of the PV string <b>512</b><i>b </i>based on the EL imaging. With a defective PV module identified, it might be helpful to know the defective PV module's geo-location. For this purpose, the mapping module <b>290</b> can be used.</p><p id="p-0216" num="0199">The image processor <b>300</b> controls the mapping module <b>290</b> to execute the MAP function as illustrated in <figref idref="DRAWINGS">FIGS. <b>25</b> to <b>28</b></figref>. The mapping module <b>290</b> is configured to map the enhanced EL image <b>2200</b> of the PV string <b>512</b><i>b </i>onto a base-map of the PV string <b>2410</b>. In order to identify the location of a PV module <b>514</b><i>b </i>(and thus, any defective PV module) in the PV string <b>512</b><i>b </i>from the enhanced EL image <b>2200</b>, the following information is captured during the SCAN function, processed, and stored: frame-dependent timed geo-location (such as time, latitude, longitude, altitude) and camera orientation (e.g. yaw, pitch, roll). This information is stored in the meta-data <b>1920</b> of every captured EL image/video (refer to <figref idref="DRAWINGS">FIG. <b>20</b></figref>).</p><p id="p-0217" num="0200">The image processor <b>300</b> controls the mapping module <b>290</b> to map the enhanced image onto the base-map by orientating the enhanced image to align the PV array subsection in the enhanced image to the PV array subsection in the base-map. If the geo-location of the PV string <b>512</b><i>b </i>in the enhanced EL image <b>2200</b> is known, but not its orientation (i.e. camera's yaw), there are four possible orientations with respect to the black triangle <b>2220</b> (0&#xb0;, 90&#xb0;, 180&#xb0;, 270&#xb0; of rotation) to align the image with a PV string in a base-map layer. Commercially available PV modules generally have cell grids of 4&#xd7;8, 6&#xd7;10 or 6&#xd7;12 cells, and are generally rectangular and not square. In such cases, only two of the four orientations are plausible: 0&#xb0; and 180&#xb0; of rotation with respect to the black triangle <b>2220</b>. This is explained in further detail with reference to <figref idref="DRAWINGS">FIGS. <b>25</b>A, <b>25</b>B and <b>25</b>C</figref> which illustrate three enhanced EL images <b>2310</b>,<b>2320</b>,<b>2330</b> with PV modules arranged in two rows, and two, three and five columns respectively.</p><p id="p-0218" num="0201">The enhanced EL image <b>2310</b> in <figref idref="DRAWINGS">FIG. <b>25</b>A</figref> has the same number of rows and columns. Due to the shape of the PV module <b>2314</b>, two of the four orientations (90&#xb0; and 270&#xb0; of rotation with respect to the black triangle <b>2220</b>) will result in the enhanced EL image <b>2310</b> being skewed wrongly when the enhanced EL image <b>2310</b> is mapped onto a base-map of its PV string.</p><p id="p-0219" num="0202">Even though the enhanced EL image <b>2320</b> in <figref idref="DRAWINGS">FIG. <b>25</b>B</figref> has a square shape, the enhanced EL image <b>2320</b> has a different number of PV modules <b>2324</b> in its row and column. In two of the four orientations (90&#xb0; and 270&#xb0; of rotation with respect to the black triangle <b>2220</b>), the number of PV modules <b>2324</b> in a row or column would not match the number of PV modules <b>2324</b> in the same row or column of its PV string in the base-map.</p><p id="p-0220" num="0203">The enhanced EL image <b>2330</b> in <figref idref="DRAWINGS">FIG. <b>25</b>C</figref> is depicted with two orientations (0&#xb0; and 180&#xb0; of rotation with respect to the black triangle <b>2220</b>). As can be seen, only these two orientations will result in an equally un-skewed image matching its base-map.</p><p id="p-0221" num="0204"><figref idref="DRAWINGS">FIG. <b>26</b></figref> illustrate the enhanced EL image <b>2200</b> mapped onto a base-map <b>2410</b> of the PV string <b>512</b><i>b</i>. Since the PV string <b>512</b><i>b </i>has a longitudinal axis (i.e. the array axis <b>10</b><i>a </i>in <figref idref="DRAWINGS">FIG. <b>1</b></figref>), and the camera <b>222</b> is arranged to capture the EL images of the PV string <b>512</b><i>b </i>with the predefined size ratio, no single EL image captures the entire PV string <b>512</b><i>b</i>. Specifically, in this embodiment, eleven EL images of the PV string <b>512</b><i>b </i>are captured. Each EL image is associated with a unique image identifier <b>2420</b>. In this embodiment, each EL image is numbered in ascending order from 1 to 11.</p><p id="p-0222" num="0205">When a particular EL image is used for image averaging to produce an enhanced frame, the image processor <b>300</b> further instructs the mapping module <b>290</b> to associate the image identifier <b>2420</b> of the particular EL image with the enhanced frame. For example, the enhanced frame <b>2430</b> is associated with unique number identifiers &#x2018;2&#x2019;,&#x2018;3&#x2019;,&#x2018;4&#x2019;,&#x2018;5&#x2019; and &#x2018;6&#x2019;. In other words, the corresponding frames extracted from the EL images &#x2018;2&#x2019;,&#x2018;3&#x2019;,&#x2018;4&#x2019;,&#x2018;5&#x2019;, and &#x2018;6&#x2019; are used for image averaging to produce the enhanced frame <b>2430</b>.</p><p id="p-0223" num="0206">The mapping module <b>290</b> is then able to determine the orientation (from the two available orientations: 0&#xb0; and 180&#xb0; of rotation with respect to the black triangle <b>2220</b>) of the enhanced image <b>2200</b> based on the image identifiers <b>2420</b> associated with each enhanced frame. The enhanced frames associated with the image identifier &#x2018;1&#x2019; represents the frames that are captured at the start of the SCAN function as opposed to the enhanced frames associated with the image identifier &#x2018;11&#x2019; which represent the frames that are captured at the end of the SCAN function.</p><p id="p-0224" num="0207">Further indicators for position and orientation of the enhanced EL image <b>2200</b> within the base-map <b>2410</b> are discussed. An approximate center position <b>2440</b> of the camera's FOV <b>910</b> along the PV string <b>512</b><i>b </i>can be calculated from the UAV's flight path <b>2450</b> (including flight start <b>2452</b> and flight end <b>2454</b>), flight altitude and camera orientation.</p><p id="p-0225" num="0208"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a schematic diagram <b>2500</b> illustrating a side view of the UAV <b>20</b> during the SCAN function for approximating the center position <b>2440</b> of the camera's FOV <b>10</b> along the PV string <b>512</b><i>b</i>. The UAV <b>20</b> has an altitude <b>2510</b> above ground of d<sub>z</sub>. The camera <b>222</b> is aligned at a pitch angle <b>2520</b> relative to the ground of &#x3b1;. The pitch angle <b>2520</b> is aligned to the tilt angle <b>2530</b> of the PV string <b>512</b><i>b </i>relative to the ground. The distance <b>2540</b> between the camera <b>222</b> and PV string <b>512</b><i>b </i>is d<sub>L</sub>. The distance <b>2540</b> is readily available from a LIDAR reading or as measured by the LIDAR <b>224</b> during the FOCUS function. The horizontal distance <b>2550</b> between the UAV <b>20</b> and the PV string <b>512</b><i>b </i>is d<sub>xy</sub>. The horizontal distance <b>2550</b> can then be calculated either based on d<sub>L </sub>using Equation (3):</p><p id="p-0226" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>xy,L</sub><i>=d</i><sub>L</sub>&#xb7;cos(&#x3b1;)&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0227" num="0209">Or based on the and the height <b>2560</b> of the PV string <b>512</b><i>b </i>from the ground d<sub>PV </sub>using Equation (4):</p><p id="p-0228" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>xy,2</sub>=(<i>d</i><sub>z</sub><i>&#x2212;d</i><sub>PV</sub>)&#xb7;tan(&#x3b1;)&#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0229" num="0210">Equation (9) is preferred over Equation (10) d<sub>xy,2 </sub>because requires the height of the imaged object (d<sub>PV</sub>) to be known or estimated. Further, the UAV <b>20</b> estimates d<sub>z </sub>barometrically which may be erroneous for longer flight times and changing weather.</p><p id="p-0230" num="0211">Referring to <figref idref="DRAWINGS">FIG. <b>27</b></figref>, the center position <b>2440</b> of the camera's FOV <b>910</b> is within the PV string <b>512</b><i>b </i>while that UAV's flight path <b>2450</b> is slightly below. With high quality location data and a simple PV string interconnection (which is the case for PV string <b>512</b><i>b</i>), the mapping module <b>290</b> is able to map the enhanced EL image <b>2200</b> onto the base-map <b>2410</b> accurately without further input. Where there are more complex PV string interconnections and/or low-quality location data, further input on the PV string is obtained through comparing measurement number, switcher box channel and information regarding which inverter box or combiner box <b>16</b> is connected to which switcher box channel and at what time.</p><p id="p-0231" num="0212"><figref idref="DRAWINGS">FIG. <b>28</b>A</figref> illustrates an exemplary enhanced EL image <b>2610</b> being mapped onto an exemplary base-map <b>2620</b> using a string alignment method. <figref idref="DRAWINGS">FIG. <b>28</b>B</figref> illustrates an exemplary enhanced EL image <b>2630</b> being mapped onto an exemplary base-map <b>2640</b> using a module alignment method.</p><p id="p-0232" num="0213">Referring to <figref idref="DRAWINGS">FIG. <b>28</b>A</figref>, for the string alignment method, the corner points <b>2622</b> of the base-map <b>2620</b> is known and the enhanced EL image <b>2610</b> is roughly aligned on top of the base-map <b>2620</b>. If the base-maps contain multiple PV strings below the enhanced EL image <b>2610</b>, the PV array <b>2620</b> sharing most overlapping area with the enhanced EL image <b>2610</b> is chosen. The four corner points <b>2612</b> of the enhanced EL image <b>2610</b> are then affine- or perspective aligned <b>2624</b> by the mapping module <b>290</b> to the corner points <b>2622</b> of the PV string inside the base-map <b>2620</b>, and according to the most similar orientation.</p><p id="p-0233" num="0214">Referring to <figref idref="DRAWINGS">FIG. <b>28</b>B</figref>, in this embodiment, the number of PV modules in the enhanced EL image <b>2630</b> do not match the number of PV modules in the base-map <b>2640</b>. This happens when the enhanced EL image is generated from only a part of a scanned PV string or the interconnection of the PV modules within the PV string does not follow a regular pattern. In this case, in addition to the corner points <b>2642</b> of the base-map <b>2640</b>, the number of modules in each row and column of the PV string must be known. Furthermore, the PV modules within the enhanced EL image <b>2630</b> must be roughly aligned with the PV modules of the base-map <b>2640</b>. The image processor <b>300</b> then instructs the mapping module <b>290</b> to obtain an affine- or perspective image transform from the deflection vectors of the corners points <b>2632</b> in the enhanced EL image <b>2630</b> to all corresponding corner points <b>2642</b> of the PV modules in the base-map <b>2640</b>. The image processor <b>300</b> further instructs the mapping module <b>290</b> to map the enhanced EL image <b>2630</b> onto the base-map <b>2640</b> based on a largest overlap of the corner points <b>2632</b> with the corner points <b>2642</b>, and according to most similar orientation.</p><p id="p-0234" num="0215">Once the enhanced EL image <b>2200</b>,<b>2610</b>,<b>2630</b> is mapped onto the base-map <b>2410</b>,<b>2620</b>,<b>2640</b>, information about the geo-location (such as GPS coordinate) of a PV module defect that is identified in the enhanced EL image <b>2200</b>,<b>2610</b>,<b>2630</b> can be readily identified from the base-map <b>2410</b>,<b>2620</b>,<b>2640</b> for repair works and/or maintenance.</p><p id="p-0235" num="0216">Advantageously, in light of the described embodiment, it is possible for the UAV <b>20</b> to take low resolution, monochromatic videos under dim light conditions and yet enhanced resolution and improved quality images may be produced to identify defective PV modules and to estimate PV module power loss. In particular, the onboard processing sub-system <b>240</b> is capable of autonomously navigating the UAV <b>20</b> and executing the exemplary method <b>400</b>.</p><p id="p-0236" num="0217">Further, since information such as frame-dependent timed geo-location (such as time, latitude, longitude, altitude etc) and camera orientation (e.g. yaw, pitch, roll) are processed, it is possible to reproduce the location of a PV string of a certain EL image reliably and accurately.</p><p id="p-0237" num="0218">It should be noted that the various embodiments described herein should not be construed as limitative. For example, the UAV <b>20</b> may be further equipped with an ultrasound device for additional distance measurements. Furthermore, the camera <b>222</b> may capture still EL images of the PV string under forward bias, or record a video of the PV string instead. Further, instead of a monochromatic sensor, a colour sensor can be used. Although the described embodiment uses &#x2018;PV string&#x2019; as an example, any other PV electrical connections may be used, and broadly, the embodiment may be used with any PV array.</p><p id="p-0238" num="0219">Other types of aerial vehicles, such as drones may be used, and not only UAVs.</p><p id="p-0239" num="0220">While the exemplary method <b>400</b> is described as including all 8 functions: FOCUS, POINT, FIND, ALIGN, SCAN, AUTO, FREEZE, MAP, it is understood that the system <b>200</b> may execute any number of the functions, and in any reasonable order. For example, in an alternative embodiment, the onboard processing sub-system <b>240</b> may not execute the AUTO function as the worker <b>30</b> may want greater control of which PV string to inspect. In this case, the worker manually controls the switcher box <b>32</b> and power supply <b>36</b> after the SCAN function is completed and initiates the FIND or SCAN function accordingly. The image processing device <b>260</b> may also execute the FREEZE function without the MAP function.</p><p id="p-0240" num="0221">Furthermore, the FOCUS function may be executed at all times throughout the method <b>400</b>, especially while the SCAN function is in progress, to ensure the captured EL images have a high quality of sharpness. Alternatively, the FOCUS function need not be executed at all if the distance between UAV <b>20</b> and PV array <b>10</b> can be kept within a narrow range. In such an embodiment, fixed focus lenses without controlled focus adjustments may be used, instead of the focussing lens <b>223</b>.</p><p id="p-0241" num="0222">Moreover, in an alternative embodiment, the UAV <b>20</b> may remotely transfer the captured EL images to the image processing device <b>260</b> without first returning to the base.</p><p id="p-0242" num="0223">Furthermore, the pre-determined maximum value (d<sub>exp_max</sub>) may be set up to 1.5.</p><p id="p-0243" num="0224">In another example, the laser may not need to be turned off if the lens filter is arranged to filter out any optical interference from the laser.</p><p id="p-0244" num="0225">In a further example, during the SCAN function, the predefined size ratio may also be set to keep a space of about 15% to 20% (or even higher, e.g. 20% to 25%) between the top and bottom of the PV string <b>512</b><i>b </i>and image border <b>1410</b> to allow a greater tolerance to positional oscillations of the UAV <b>20</b>, depending on how unstable the UAV <b>20</b> appears to be.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005115A1-20230105-M00001.NB"><img id="EMI-M00001" he="6.01mm" wi="76.20mm" file="US20230005115A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US20230005115A1-20230105-M00002.NB"><img id="EMI-M00002" he="7.37mm" wi="76.20mm" file="US20230005115A1-20230105-M00002.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003 MATH-US-00003-2" nb-file="US20230005115A1-20230105-M00003.NB"><img id="EMI-M00003" he="7.37mm" wi="76.20mm" file="US20230005115A1-20230105-M00003.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230005115A1-20230105-M00004.NB"><img id="EMI-M00004" he="5.67mm" wi="76.20mm" file="US20230005115A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230005115A1-20230105-M00005.NB"><img id="EMI-M00005" he="7.37mm" wi="76.20mm" file="US20230005115A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006 MATH-US-00006-2" nb-file="US20230005115A1-20230105-M00006.NB"><img id="EMI-M00006" he="7.37mm" wi="76.20mm" file="US20230005115A1-20230105-M00006.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230005115A1-20230105-M00007.NB"><img id="EMI-M00007" he="6.01mm" wi="76.20mm" file="US20230005115A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230005115A1-20230105-M00008.NB"><img id="EMI-M00008" he="6.35mm" wi="76.20mm" file="US20230005115A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of processing electroluminescence (EL) images of a PV array, comprising<claim-text>extracting a plurality of frames of a PV array subsection of the PV array from the EL images, the PV array subsection including one or more PV modules of the PV array;</claim-text><claim-text>determining a reference frame having a highest image quality of the PV array subsection from among the extracted frames;</claim-text><claim-text>performing image alignment of the extracted frames to the reference frame to generate image aligned frames by<claim-text>arranging the extracted frames in a stacked arrangement, wherein respective corner points of the PV modules are stacked; and</claim-text><claim-text>aligning the respective corner points of each PV module in the extracted frames to the corresponding corner points of the PV module in the reference frame; and</claim-text></claim-text><claim-text>processing the image aligned frames to produce an enhanced image of the PV array subsection having a higher resolution than the reference frame.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein extracting the frames from the images comprises<claim-text>determining the respective corner points of each PV module in the images; and</claim-text><claim-text>constructing respective frames for each PV module based on the identified corner points of each PV module.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. (canceled)</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining a reference frame having a highest image quality comprises evaluating the image quality of each frame based on at least one of sharpness, signal-to-noise ratio, and completeness of the frames.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the image aligned frames comprises grouping the image aligned frames according to the PV module in each frame; and performing image averaging on each group of image aligned frames to obtain respective enhanced frames for each PV module.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. A method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the image averaging is based on weighted image stack averaging, and/or a deep convolutional neural network structure.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising associating each enhanced frame with a horizontal index and a vertical index according to each PV module's position in the PV array subsection; and arranging the enhanced frames according to its horizontal and vertical index to produce the enhanced image of the PV array subsection.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising scaling respective image intensities of each enhanced frame.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising mapping the enhanced image of the PV array subsection onto a base-map of the PV array subsection, the base-map including geo-location of each PV module.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein mapping the enhanced image onto the base-map comprises orientating the enhanced image to align the PV array subsection in the enhanced image to the PV array subsection in the base-map.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. (canceled)</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. (canceled)</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. (canceled)</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. An image processing device for processing EL images of a PV array, comprising<claim-text>an image processor configured to</claim-text><claim-text>extract a plurality of frames of a PV array subsection of the PV array from the EL images, the PV array subsection including one or more PV modules of the PV array;<claim-text>determine a reference frame having a highest image quality of the PV array subsection from among the extracted frames;</claim-text><claim-text>perform image alignment of the extracted frames to the reference frame to generate image aligned frames by</claim-text></claim-text><claim-text>arranging the extracted frames in a stacked arrangement, wherein respective corner points of the PV modules are stacked; and</claim-text><claim-text>aligning the respective corner points of each PV module in the extracted frames to the corresponding corner points of the PV module in the reference frame; and<claim-text>process the image aligned frames to produce an enhanced image of the PV array subsection having a higher resolution than the reference frame.</claim-text></claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. An image processing device according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the image processor is further configured to extract the frames from the images by<claim-text>determining respective corner points of each PV module in the images; and</claim-text><claim-text>constructing respective frames for each PV module based on the identified corner points of each PV module.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. (canceled)</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. An image processing device according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the image processor is further configured to determine a reference frame having a highest image quality by evaluating the image quality of each frame based on at least one of sharpness, signal-to-noise ratio, and completeness of the frames.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. An image processing device according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the image processor is further configured to process the image aligned frames by grouping the image aligned frames according to the PV module in each frame; and perform image averaging on each group of image aligned frames to obtain respective enhanced frames for each PV module.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. An image processing device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the image processor is further configured to perform image averaging based on weighted image stack averaging, and/or a deep convolutional neural network structure.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. An image processing device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the image processor is further configured to associate each enhanced frame with a horizontal index and a vertical index according to each PV module's position in the PV array subsection; and arrange the enhanced frames according to its horizontal and vertical index to produce the enhanced image of the PV array subsection.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. An image processing device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the image processor is further configured to scale respective image intensities of each enhanced frame.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. An image processing device according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the image processor is further configured to map the enhanced image of the PV array subsection onto a base-map of the PV array subsection, the base-map including geo-location of each PV module.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. An image processing device according to <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the image processor is further configured to map the enhanced image onto the base-map by orientating the enhanced image to align the PV array subsection in the enhanced image to the PV array subsection in the base-map.</claim-text></claim><claim id="CLM-24-64" num="24-64"><claim-text><b>24</b>-<b>64</b>. (canceled)</claim-text></claim><claim id="CLM-00065" num="00065"><claim-text><b>65</b>. A method of obtaining an enhanced image of a PV array subsection of a PV array from EL images of the PV array subsection captured by an aerial vehicle having a camera, the method comprising<claim-text>controlling the aerial vehicle to fly along a flight path to capture EL images of corresponding PV array subsections of the PV array;<claim-text>deriving respective image quality parameters from at least some of the captured EL images;</claim-text><claim-text>dynamically adjusting a flight speed of the aerial vehicle along the flight path, based on the respective image quality parameters for capturing the EL images of the PV array subsections;</claim-text><claim-text>extracting a plurality of frames of the PV array subsection from the EL images;</claim-text></claim-text><claim-text>determining a reference frame having a highest image quality of the PV array subsection from among the extracted frames;</claim-text><claim-text>performing image alignment of the extracted frames to the reference frame to generate image aligned frames by<claim-text>arranging the extracted frames in a stacked arrangement, wherein respective corner points of the PV modules are stacked; and<claim-text>aligning the respective corner points of each PV module in the extracted frames to the corresponding corner points of the PV module in the reference frame; and</claim-text></claim-text><claim-text>processing the image aligned frames to produce an enhanced image of the PV array subsection having a higher resolution than the reference frame.</claim-text></claim-text></claim-text></claim><claim id="CLM-00066" num="00066"><claim-text><b>66</b>. (canceled)</claim-text></claim><claim id="CLM-00067" num="00067"><claim-text><b>67</b>. (canceled)</claim-text></claim><claim id="CLM-00068" num="00068"><claim-text><b>68</b>. (canceled)</claim-text></claim><claim id="CLM-00069" num="00069"><claim-text><b>69</b>. (canceled)</claim-text></claim><claim id="CLM-00070" num="00070"><claim-text><b>70</b>. (canceled)</claim-text></claim><claim id="CLM-00071" num="00071"><claim-text><b>71</b>. (canceled)</claim-text></claim><claim id="CLM-00072" num="00072"><claim-text><b>72</b>. (canceled)</claim-text></claim><claim id="CLM-00073" num="00073"><claim-text><b>73</b>. (canceled)</claim-text></claim><claim id="CLM-00074" num="00074"><claim-text><b>74</b>. (canceled)</claim-text></claim></claims></us-patent-application>