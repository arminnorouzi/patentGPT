<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230003546A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230003546</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17850084</doc-number><date>20220627</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>C</subclass><main-group>21</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20200801</date></cpc-version-indicator><section>G</section><class>01</class><subclass>C</subclass><main-group>21</main-group><subgroup>3837</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200801</date></cpc-version-indicator><section>G</section><class>01</class><subclass>C</subclass><main-group>21</main-group><subgroup>383</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">A SYSTEM AND METHOD OF GENERATING A FLOORPLAN</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217506</doc-number><date>20210701</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>FARO Technologies, Inc.</orgname><address><city>Lake Mary</city><state>FL</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Brenner</last-name><first-name>Mark</first-name><address><city>Asperg</city><country>DE</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Frank</last-name><first-name>Aleksej</first-name><address><city>Kornwestheim</city><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Zweigle</last-name><first-name>Oliver</first-name><address><city>Stuttgart</city><country>DE</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system and method of generating a two-dimensional (2D) image of an environment is provided. The system includes a scanner having a first light source, an image sensor, a second light source and a controller, the second light source emitting a visible light, the controller determining a distance to points based on a beam of light emitted by the first light source and receiving of the reflected beam of light from the points. Processors are operably coupled to the scanner execute a method comprising: generating a map of the environment; emitting light from the second light source towards an edge defined by at least a pair of surfaces; detecting the edge based on emitting a second beam of light and receiving the reflected second beam of light; and defining a room on the map based on the detecting of the corner or the edge.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="98.89mm" wi="126.15mm" file="US20230003546A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="212.94mm" wi="134.20mm" file="US20230003546A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="198.63mm" wi="138.94mm" file="US20230003546A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="203.28mm" wi="149.18mm" file="US20230003546A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="196.60mm" wi="110.83mm" file="US20230003546A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="216.15mm" wi="153.42mm" file="US20230003546A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="198.71mm" wi="174.41mm" orientation="landscape" file="US20230003546A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="234.02mm" wi="105.41mm" file="US20230003546A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="218.19mm" wi="108.03mm" file="US20230003546A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="105.92mm" wi="180.93mm" file="US20230003546A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="240.71mm" wi="159.17mm" file="US20230003546A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="215.98mm" wi="92.88mm" file="US20230003546A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="202.01mm" wi="150.71mm" file="US20230003546A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="242.15mm" wi="121.58mm" file="US20230003546A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="225.72mm" wi="131.40mm" file="US20230003546A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="250.78mm" wi="162.39mm" file="US20230003546A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="169.33mm" wi="157.99mm" file="US20230003546A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="239.44mm" wi="155.70mm" file="US20230003546A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="192.96mm" wi="169.50mm" file="US20230003546A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="272.88mm" wi="186.61mm" file="US20230003546A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="273.98mm" wi="207.18mm" file="US20230003546A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="272.20mm" wi="212.68mm" file="US20230003546A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="271.61mm" wi="204.81mm" file="US20230003546A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="189.82mm" wi="149.69mm" file="US20230003546A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="166.29mm" wi="138.09mm" file="US20230003546A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="179.92mm" wi="146.22mm" file="US20230003546A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Application Ser. No. 63/217,506, filed Jul. 1, 2021, the entire disclosure of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The present application is directed to a system that optically scans an environment, such as a building, and in particular to a portable system that generates two-dimensional floorplans of the scanned environment.</p><p id="p-0004" num="0003">The automated creation of digital two-dimensional floorplans for existing structures is desirable as it allows the size and shape of the environment to be used in many processes. For example, a floorplan may be desirable to allow construction drawings to be prepared during a renovation. Such floorplans may find other uses such as in documenting a building for a fire department or to document a crime scene.</p><p id="p-0005" num="0004">Existing measurement systems typically use a scanning device that determines coordinates of surfaces in the environment by both emitting a light and capturing a reflection to determine a distance or by triangulation using cameras. These scanning device may be mounted to a movable structure, such as a cart, and moved through the building to generate a digital representation of the building. During operation, the scanning equipment generates a plan view map of the building being scanned. However, post processing is needed that includes manual steps to define the boundaries of the rooms. While automated methods of segmenting rooms have been proposed, these were generally unsuccessful (i.e. due to noise or artifacts in the scan data) or required extensive computer resources.</p><p id="p-0006" num="0005">Accordingly, while existing scanners are suitable for their intended purposes, what is needed is a system for having certain features of embodiments of the present invention.</p><heading id="h-0003" level="1">BRIEF DESCRIPTION</heading><p id="p-0007" num="0006">According to one aspect of the invention, a system of generating a two-dimensional (2D) image of an environment is provided. The system includes a 2D scanner having a first light source, an image sensor, a second light source and a controller, the second light source being configured to emit a visible light, the controller being operable to determine a distance value to object points in the environment based at least in part on a beam of light emitted by the first light source and the receiving of the beam of light reflected from the object points. One or more processors are operably coupled to the 2D scanner, the one or more processors being responsive to nontransitory executable instructions to execute a method comprising: generating a plan view map of the environment; emitting light from the second light source towards an edge defined by at least a pair of surfaces; detecting the edge based at least in part on emitting a second beam of light from the light source and receiving the second beam of light reflected from either the edge or from the pair of surfaces; and defining a room on the plan view map based at least in part on the detecting of the corner or the edge.</p><p id="p-0008" num="0007">In addition to one or more of the features described herein, or as an alternative, further embodiments of the system may include the method further comprising associating the detected edge with a location on the plan view map.</p><p id="p-0009" num="0008">In addition to one or more of the features described herein, or as an alternative, further embodiments of the system may include the method further comprising detecting a plurality of edges, and generating a polygon on the plan view map defined by the edges. In addition to one or more of the features described herein, or as an alternative, further embodiments of the system may include the detecting of the edge including the measuring a plurality of first points on a first surface of the pair of surfaces and measuring a plurality of second points on a second surface of the pair of surfaces. In addition to one or more of the features described herein, or as an alternative, further embodiments of the system may include the edge being defined by a first line and a second line, the first line being defined by the plurality of first points, the second line being defined by the plurality of second points.</p><p id="p-0010" num="0009">In addition to one or more of the features described herein, or as an alternative, further embodiments of the system may include the method further comprising: detecting a plurality of second edges based at least in part on emitting the second beam of light from the light source and receiving the reflected second beam of light; and defining a doorway on the plan view map based on the plurality of second edges.</p><p id="p-0011" num="0010">In addition to one or more of the features described herein, or as an alternative, further embodiments of the system may include the method further comprising: detecting a plurality of third edges based at least in part on emitting a second beam of light from the light source and receiving the reflected second beam of light; and defining a doorway on the plan view map based on the plurality of third edges.</p><p id="p-0012" num="0011">According to another aspect of the invention, a method for generating a two-dimensional (2D) image of an environment is provided. The method includes providing a 2D scanner having a first light source, an image sensor, a second light source and a controller, the second light source being configured to emit a visible light, the controller being operable to determine a distance value to object points in the environment based at least in part on a beam of light emitted by the first light source and the receiving of the beam of light reflected from the object points. A plan view map is generated of the environment. Light is emitted from the second light source towards an edge defined by at least a pair of surfaces. The edge is detected based at least in part on emitting a second beam of light from the light source and receiving the second beam of light reflected from either the edge or from the pair of surfaces. A room is defined on the plan view map based at least in part on the detecting of the corner or the edge.</p><p id="p-0013" num="0012">In addition to one or more of the features described herein, or as an alternative, further embodiments of the method may include associating the detected edge with a location on the plan view map.</p><p id="p-0014" num="0013">In addition to one or more of the features described herein, or as an alternative, further embodiments of the method may include detecting a plurality of edges, and generating a polygon on the plan view map defined by the edges. In addition to one or more of the features described herein, or as an alternative, further embodiments of the method may include the detecting of the edge including measuring a plurality of first points on a first surface of the pair of surfaces and measuring a plurality of second points on a second surface of the pair of surfaces. In addition to one or more of the features described herein, or as an alternative, further embodiments of the method may include the edge being defined by a first line and a second line, the first line being defined by the plurality of first points, the second line being defined by the plurality of second points.</p><p id="p-0015" num="0014">In addition to one or more of the features described herein, or as an alternative, further embodiments of the method may include detecting a plurality of second edges based at least in part on emitting the second beam of light from the light source and receiving the reflected second beam of light; and defining a doorway on the plan view map based on the plurality of second edges.</p><p id="p-0016" num="0015">In addition to one or more of the features described herein, or as an alternative, further embodiments of the method may include detecting a plurality of third edges based at least in part on emitting a second beam of light from the light source and receiving the reflected second beam of light; and defining a doorway on the plan view map based on the plurality of third edges.</p><p id="p-0017" num="0016">According to another aspect of the invention, a system of generating a two-dimensional (2D) image of an environment is provided. The system including one or more processors and a 2D scanner sized and weighted to be carried by a single person, having a first light source, an image sensor, an inertial measurement unit having a first plurality of sensors, the first light source steers a beam of light within a first plane to illuminate object points in the environment, the image sensor is arranged to receive light reflected from the object points. A mobile computing device is removably coupled to the 2D scanner, the mobile computing device having a second plurality of sensors. Wherein the one or more processors are responsive to executable instructions which when executed by the one or more processors to: generating a plan view map of the environment; emitting light from the second light source towards an edge defined by at least a pair of surfaces; detecting the edge based at least in part on emitting a second beam of light from the light source and receiving the second beam of light reflected from either the edge or from the pair of surfaces; and defining a room on the plan view map based at least in part on the detecting of the corner or the edge.</p><p id="p-0018" num="0017">In addition to one or more of the features described herein, or as an alternative, further embodiments of the system may include one or more processors that are further responsive to associate the detected edge with a location on the plan view map.</p><p id="p-0019" num="0018">In addition to one or more of the features described herein, or as an alternative, further embodiments of the system may include the one or more processors being further responsive to detect a plurality of edges, and generating a polygon on the plan view map defined by the edges. In addition to one or more of the features described herein, or as an alternative, further embodiments of the system may include the detecting of the edge including the measuring a plurality of first points on a first surface of the pair of surfaces and measuring a plurality of second points on a second surface of the pair of surfaces. In addition to one or more of the features described herein, or as an alternative, further embodiments of the system may include the edge being defined by a first line and a second line, the first line being defined by the plurality of first points, the second line being defined by the plurality of second points.</p><p id="p-0020" num="0019">In addition to one or more of the features described herein, or as an alternative, further embodiments of the system may include the one or more processors being further responsive to: detect a plurality of second edges based at least in part on emitting the second beam of light from the light source and receiving the reflected second beam of light; and define a doorway on the plan view map based on the plurality of second edges.</p><p id="p-0021" num="0020">These and other advantages and features will become more apparent from the following description taken in conjunction with the drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0022" num="0021">The subject matter, which is regarded as the invention, is particularly pointed out and distinctly claimed in the claims at the conclusion of the specification. The foregoing and other features, and advantages of the invention are apparent from the following detailed description taken in conjunction with the accompanying drawings in which:</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. <b>1</b>-<b>3</b></figref> are perspective views of a scanning and mapping system in accordance with an embodiment;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a first end view of the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a side sectional view of the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a side sectional view of the system of a scanning and mapping system in accordance with another embodiment;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a first end view of the system of <figref idref="DRAWINGS">FIG. <b>6</b></figref>;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a top sectional view of the system of <figref idref="DRAWINGS">FIG. <b>6</b></figref>;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an enlarged view of a portion of the second end of <figref idref="DRAWINGS">FIG. <b>7</b></figref>;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram of the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>6</b></figref>;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>11</b>-<b>13</b></figref> are schematic illustrations of the operation of system of <figref idref="DRAWINGS">FIG. <b>9</b></figref> in accordance with an embodiment;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flow diagram of a method of generating a two-dimensional map of an environment;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIGS. <b>15</b>-<b>16</b></figref> are plan views of stages of a two-dimensional map generated with the method of <figref idref="DRAWINGS">FIG. <b>14</b></figref> in accordance with an embodiment;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>17</b>-<b>18</b></figref> are schematic views of the operation of the system of <figref idref="DRAWINGS">FIG. <b>9</b></figref> in accordance with an embodiment;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a flow diagram of a method of generating a two-dimensional map using the system of <figref idref="DRAWINGS">FIG. <b>9</b></figref> in accordance with an embodiment;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a flow diagram of a method of defining rooms or spaces on a two-dimensional map using the system of <figref idref="DRAWINGS">FIG. <b>9</b></figref> in accordance with an embodiment;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>21</b>-<b>25</b>B</figref> are schematic views of a display of the system of <figref idref="DRAWINGS">FIG. <b>9</b></figref> and an image of the environment during operation in accordance of an embodiment; and</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>26</b>A</figref>, <figref idref="DRAWINGS">FIG. <b>26</b>B</figref>, and <figref idref="DRAWINGS">FIG. <b>26</b>C</figref> are a partial view of a display of the system of <figref idref="DRAWINGS">FIG. <b>9</b></figref> while defining a room within the map.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0039" num="0038">The detailed description explains embodiments of the invention, together with advantages and features, by way of example with reference to the drawings.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0040" num="0039">The present invention relates to a device that includes a system having a 2D scanner that works cooperatively with an inertial measurement unit to generate an annotated two-dimensional map of an environment.</p><p id="p-0041" num="0040">Referring now to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>5</b></figref>, an embodiment of a system <b>30</b> having a housing <b>32</b> that includes a body portion <b>34</b> and a handle portion <b>36</b>. In an embodiment, the handle <b>36</b> may include an actuator <b>38</b> that allows the operator to interact with the system <b>30</b>. In the exemplary embodiment, the body <b>34</b> includes a generally rectangular center portion <b>35</b> with a slot <b>40</b> formed in an end <b>42</b>. The slot <b>40</b> is at least partially defined by a pair walls <b>44</b> that are angled towards a second end <b>48</b>. As will be discussed in more detail herein, a portion of a two-dimensional scanner <b>50</b> is arranged between the walls <b>44</b>. The walls <b>44</b> are angled to allow the scanner <b>50</b> to operate by emitting a light over a large angular area without interference from the walls <b>44</b>. As will be discussed in more detail herein, the end <b>42</b> may further include a three-dimensional camera or RGBD camera <b>60</b>.</p><p id="p-0042" num="0041">Extending from the center portion <b>35</b> is a mobile device holder <b>41</b>. The mobile device holder <b>41</b> is configured to securely couple a mobile device <b>43</b> to the housing <b>32</b>. The holder <b>41</b> may include one or more fastening elements, such as a magnetic or mechanical latching element for example, that couples the mobile device <b>43</b> to the housing <b>32</b>. In an embodiment, the mobile device <b>43</b> is coupled to communicate with a controller <b>68</b> (<figref idref="DRAWINGS">FIG. <b>10</b></figref>). The communication between the controller <b>68</b> and the mobile device <b>43</b> may be via any suitable communications medium, such as wired, wireless or optical communication mediums for example.</p><p id="p-0043" num="0042">In the illustrated embodiment, the holder <b>41</b> is pivotally coupled to the housing <b>32</b>, such that it may be selectively rotated into a closed position within a recess <b>46</b>. In an embodiment, the recess <b>46</b> is sized and shaped to receive the holder <b>41</b> with the mobile device <b>43</b> disposed therein.</p><p id="p-0044" num="0043">In the exemplary embodiment, the second end <b>48</b> includes a plurality of exhaust vent openings <b>56</b>. In an embodiment, shown in <figref idref="DRAWINGS">FIGS. <b>6</b>-<b>9</b></figref>, the exhaust vent openings <b>56</b> are fluidly coupled to intake vent openings <b>58</b> arranged on a bottom surface <b>62</b> of center portion <b>35</b>. The intake vent openings <b>58</b> allow external air to enter a conduit <b>64</b> having an opposite opening <b>66</b> (<figref idref="DRAWINGS">FIG. <b>6</b></figref>) in fluid communication with the hollow interior <b>67</b> of the body <b>34</b>. In an embodiment, the opening <b>66</b> is arranged adjacent to a controller <b>68</b> which has one or more processors that is operable to perform the methods described herein. In an embodiment, the external air flows from the opening <b>66</b> over or around the controller <b>68</b> and out the exhaust vent openings <b>56</b>.</p><p id="p-0045" num="0044">The controller <b>68</b> is coupled to a wall <b>70</b> of body <b>34</b>. In an embodiment, the wall <b>70</b> is coupled to or integral with the handle <b>36</b>. The controller <b>68</b> is electrically coupled to the 2D scanner <b>50</b>, the 3D camera <b>60</b>, a power source <b>72</b>, an inertial measurement unit (IMU) <b>74</b>, a visible laser light projector <b>76</b>, and a haptic feedback device <b>77</b>. In some embodiments, the system <b>30</b> includes multiple laser light projectors <b>76</b>.</p><p id="p-0046" num="0045">Referring now to <figref idref="DRAWINGS">FIG. <b>10</b></figref> with continuing reference to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>9</b></figref>, elements are shown of the system <b>30</b> with the mobile device <b>43</b> installed or coupled to the housing <b>32</b>. Controller <b>68</b> is a suitable electronic device capable of accepting data and instructions, executing the instructions to process the data, and presenting the results. The controller <b>68</b> includes one or more processing elements <b>78</b>. The processors may be microprocessors, field programmable gate arrays (FPGAs), digital signal processors (DSPs), and generally any device capable of performing computing functions. The one or more processors <b>78</b> have access to memory <b>80</b> for storing information.</p><p id="p-0047" num="0046">Controller <b>68</b> is capable of converting the analog voltage or current level provided by 2D scanner <b>50</b>, camera <b>60</b> and IMU <b>74</b> into a digital signal to determine a distance from the system <b>30</b> to an object in the environment. In an embodiment, the camera <b>60</b> is a 3D or RGBD type camera. Controller <b>68</b> uses the digital signals that act as input to various processes for controlling the system <b>30</b>. The digital signals represent one or more system <b>30</b> data including but not limited to distance to an object, images of the environment, acceleration, pitch orientation, yaw orientation and roll orientation. As will be discussed in more detail, the digital signals may be from components internal to the housing <b>32</b> or from sensors and devices located in the mobile device <b>43</b>.</p><p id="p-0048" num="0047">In general, when the mobile device <b>43</b> is not installed, controller <b>68</b> accepts data from 2D scanner <b>50</b> and IMU <b>74</b> and is given certain instructions for the purpose of generating a two-dimensional map of a scanned environment. Controller <b>68</b> provides operating signals to the 2D scanner <b>50</b>, the camera <b>60</b>, laser light projector <b>76</b> and haptic feedback device <b>77</b>. Controller <b>68</b> also accepts data from IMU <b>74</b>, indicating, for example, whether the operator is operating in the system in the desired orientation. The controller <b>68</b> compares the operational parameters to predetermined variances (e.g. yaw, pitch or roll thresholds) and if the predetermined variance is exceeded, generates a signal that activates the haptic feedback device <b>77</b>. The data received by the controller <b>68</b> may be displayed on a user interface coupled to controller <b>68</b>. The user interface may be one or more LEDs (light-emitting diodes) <b>82</b>, an LCD (liquid-crystal diode) display, a CRT (cathode ray tube) display, or the like. A keypad may also be coupled to the user interface for providing data input to controller <b>68</b>. In one embodiment, the user interface is arranged or executed on the mobile device <b>43</b>.</p><p id="p-0049" num="0048">The controller <b>68</b> may also be coupled to external computer networks such as a local area network (LAN) and the Internet. A LAN interconnects one or more remote computers, which are configured to communicate with controller <b>68</b> using a well-known computer communications protocol such as TCP/IP (Transmission Control Protocol/Internet({circumflex over (&#x2003;)}) Protocol), RS-232, ModBus, and the like. Additional systems <b>30</b> may also be connected to LAN with the controllers <b>68</b> in each of these systems <b>30</b> being configured to send and receive data to and from remote computers and other systems <b>30</b>. The LAN may be connected to the Internet. This connection allows controller <b>68</b> to communicate with one or more remote computers connected to the Internet.</p><p id="p-0050" num="0049">The processors <b>78</b> are coupled to memory <b>80</b>. The memory <b>80</b> may include random access memory (RAM) device <b>84</b>, a non-volatile memory (NVM) device <b>86</b>, a read-only memory (ROM) device <b>88</b>. In addition, the processors <b>78</b> may be connected to one or more input/output (I/O) controllers <b>90</b> and a communications circuit <b>92</b>. In an embodiment, the communications circuit <b>92</b> provides an interface that allows wireless or wired communication with one or more external devices or networks, such as the LAN discussed above.</p><p id="p-0051" num="0050">Controller <b>68</b> includes operation control methods embodied in application code shown or described with reference to <figref idref="DRAWINGS">FIGS. <b>11</b>-<b>14</b></figref> and <figref idref="DRAWINGS">FIG. <b>19</b></figref>. These methods are embodied in computer instructions written to be executed by processors <b>78</b>, typically in the form of software. The software can be encoded in any language, including, but not limited to, assembly language, VHDL (Verilog Hardware Description Language), VHSIC HDL (Very High Speed IC Hardware Description Language), Fortran (formula translation), C, C++, C#, Objective-C, Visual C++, Java, ALGOL (algorithmic language), BASIC (beginners all-purpose symbolic instruction code), visual BASIC, ActiveX, HTML (HyperText Markup Language), Python, Ruby and any combination or derivative of at least one of the foregoing.</p><p id="p-0052" num="0051">Coupled to the controller <b>68</b> is the 2D scanner <b>50</b>. The 2D scanner <b>50</b> measures 2D coordinates in a plane. In the exemplary embodiment, the scanning is performed by steering light within a plane to illuminate object points in the environment. The 2D scanner <b>50</b> collects the reflected (scattered) light from the object points to determine 2D coordinates of the object points in the 2D plane. In an embodiment, the 2D scanner <b>50</b> scans a spot of light over an angle while at the same time measuring an angle value and corresponding distance value to each of the illuminated object points.</p><p id="p-0053" num="0052">Examples of 2D scanners <b>50</b> include, but are not limited to Model LMS100 scanners manufactured by Sick, Inc of Minneapolis, Minn. and scanner Models URG-04LX-UG01 and UTM-30LX manufactured by Hokuyo Automatic Co., Ltd of Osaka, Japan. The scanners in the Sick LMS100 family measure angles over a 270 degree range and over distances up to 20 meters. The Hoyuko model URG-04LX-UG01 is a low-cost 2D scanner that measures angles over a 240 degree range and distances up to 4 meters. The Hoyuko model UTM-30LX is a 2D scanner that measures angles over a 270 degree range and to distances up to 30 meters. It should be appreciated that the above 2D scanners are exemplary and other types of 2D scanners are also available.</p><p id="p-0054" num="0053">In an embodiment, the 2D scanner <b>50</b> is oriented so as to scan a beam of light over a range of angles in a generally horizontal plane (relative to the floor of the environment being scanned). At instants in time the 2D scanner <b>50</b> returns an angle reading and a corresponding distance reading to provide 2D coordinates of object points in the horizontal plane. In completing one scan over the full range of angles, the 2D scanner returns a collection of paired angle and distance readings. As the system <b>30</b> is moved from place to place, the 2D scanner <b>50</b> continues to return 2D coordinate values. These 2D coordinate values are used to locate the position of the system <b>30</b> thereby enabling the generation of a two-dimensional map or floorplan of the environment.</p><p id="p-0055" num="0054">Also coupled to the controller <b>86</b> is the IMU <b>74</b>. The IMU <b>74</b> is a position/orientation sensor that may include accelerometers <b>94</b> (inclinometers), gyroscopes <b>96</b>, a magnetometers or compass <b>98</b>, and altimeters. In the exemplary embodiment, the IMU <b>74</b> includes multiple accelerometers <b>94</b> and gyroscopes <b>96</b>. The compass <b>98</b> indicates a heading based on changes in magnetic field direction relative to the earth's magnetic north. The IMU <b>74</b> may further have an altimeter that indicates altitude (height). An example of a widely used altimeter is a pressure sensor. By combining readings from a combination of position/orientation sensors with a fusion algorithm that may include a Kalman filter, relatively accurate position and orientation measurements can be obtained using relatively low-cost sensor devices. In the exemplary embodiment, the IMU <b>74</b> determines the pose or orientation of the system <b>30</b> about three-axis to allow a determination of a yaw, roll and pitch parameter.</p><p id="p-0056" num="0055">In the embodiment shown in <figref idref="DRAWINGS">FIGS. <b>6</b>-<b>9</b></figref>, the system <b>30</b> further includes a camera <b>60</b> that is a 3D or RGB-D camera. As used herein, the term 3D camera refers to a device that produces a two-dimensional image that includes distances to a point in the environment from the location of system <b>30</b>. The 3D camera <b>30</b> may be a range camera or a stereo camera. In an embodiment, the 3D camera <b>30</b> includes an RGB-D sensor that combines color information with a per-pixel depth information. In an embodiment, the 3D camera <b>30</b> may include an infrared laser projector <b>31</b> (<figref idref="DRAWINGS">FIG. <b>9</b></figref>), a left infrared camera <b>33</b>, a right infrared camera <b>39</b>, and a color camera <b>37</b>. In an embodiment, the 3D camera <b>60</b> is a RealSense&#x2122; camera model R200 manufactured by Intel Corporation. In still another embodiment, the 3D camera <b>30</b> is a RealSense&#x2122; LIDAR camera model L515 manufactured by Intel Corporation.</p><p id="p-0057" num="0056">In an embodiment, when the mobile device <b>43</b> is coupled to the housing <b>32</b>, the mobile device <b>43</b> becomes an integral part of the system <b>30</b>. In an embodiment, the mobile device <b>43</b> is a cellular phone, a tablet computer or a personal digital assistant (PDA). The mobile device <b>43</b> may be coupled for communication via a wired connection, such as ports <b>100</b>, <b>102</b>. The port <b>100</b> is coupled for communication to the processor <b>78</b>, such as via I/O controller <b>90</b> for example. The ports <b>100</b>, <b>102</b> may be any suitable port, such as but not limited to USB, USB-A, USB-B, USB-C, IEEE 1394 (Firewire), or Lightning&#x2122; connectors.</p><p id="p-0058" num="0057">The mobile device <b>43</b> is a suitable electronic device capable of accepting data and instructions, executing the instructions to process the data, and presenting the results. The mobile device <b>43</b> includes one or more processing elements <b>104</b>. The processors may be microprocessors, field programmable gate arrays (FPGAs), digital signal processors (DSPs), and generally any device capable of performing computing functions. The one or more processors <b>104</b> have access to memory <b>106</b> for storing information.</p><p id="p-0059" num="0058">The mobile device <b>43</b> is capable of converting the analog voltage or current level provided by sensors <b>108</b> and processor <b>78</b>. Mobile device <b>43</b> uses the digital signals that act as input to various processes for controlling the system <b>30</b>. The digital signals represent one or more system <b>30</b> data including but not limited to distance to an object, images of the environment, acceleration, pitch orientation, yaw orientation, roll orientation, global position, ambient light levels, and altitude for example.</p><p id="p-0060" num="0059">In general, mobile device <b>43</b> accepts data from sensors <b>108</b> and is given certain instructions for the purpose of generating or assisting the processor <b>78</b> in the generation of a two-dimensional map or three-dimensional map of a scanned environment. Mobile device <b>43</b> provides operating signals to the processor <b>78</b>, the sensors <b>108</b> and a display <b>110</b>. Mobile device <b>43</b> also accepts data from sensors <b>108</b>, indicating, for example, to track the position of the mobile device <b>43</b> in the environment or measure coordinates of points on surfaces in the environment. The mobile device <b>43</b> compares the operational parameters to predetermined variances (e.g. yaw, pitch or roll thresholds) and if the predetermined variance is exceeded, may generate a signal. The data received by the mobile device <b>43</b> may be displayed on display <b>110</b>. In an embodiment, the display <b>110</b> is a touch screen device that allows the operator to input data or control the operation of the system <b>30</b>.</p><p id="p-0061" num="0060">The controller <b>68</b> may also be coupled to external networks such as a local area network (LAN), a cellular network and the Internet. A LAN interconnects one or more remote computers, which are configured to communicate with controller <b>68</b> using a well-known computer communications protocol such as TCP/IP (Transmission Control Protocol/Internet({circumflex over (&#x2003;)}) Protocol), RS-232, ModBus, and the like. Additional systems <b>30</b> may also be connected to LAN with the controllers <b>68</b> in each of these systems <b>30</b> being configured to send and receive data to and from remote computers and other systems <b>30</b>. The LAN may be connected to the Internet. This connection allows controller <b>68</b> to communicate with one or more remote computers connected to the Internet.</p><p id="p-0062" num="0061">The processors <b>104</b> are coupled to memory <b>106</b>. The memory <b>106</b> may include random access memory (RAM) device, a non-volatile memory (NVM) device, and a read-only memory (ROM) device. In addition, the processors <b>104</b> may be connected to one or more input/output (I/O) controllers <b>112</b> and a communications circuit <b>114</b>. In an embodiment, the communications circuit <b>114</b> provides an interface that allows wireless or wired communication with one or more external devices or networks, such as the LAN or the cellular network discussed above.</p><p id="p-0063" num="0062">Controller <b>68</b> includes operation control methods embodied in application code shown or described with reference to <figref idref="DRAWINGS">FIGS. <b>11</b>-<b>4</b></figref> and <figref idref="DRAWINGS">FIG. <b>19</b></figref>. These methods are embodied in computer instructions written to be executed by processors <b>78</b>, <b>104</b>, typically in the form of software. The software can be encoded in any language, including, but not limited to, assembly language, VHDL (Verilog Hardware Description Language), VHSIC HDL (Very High Speed IC Hardware Description Language), Fortran (formula translation), C, C++, C#, Objective-C, Visual C++, Java, ALGOL (algorithmic language), BASIC (beginners all-purpose symbolic instruction code), visual BASIC, ActiveX, HTML (HyperText Markup Language), Python, Ruby and any combination or derivative of at least one of the foregoing.</p><p id="p-0064" num="0063">Also coupled to the processor <b>104</b> are the sensors <b>108</b>. The sensors <b>108</b> may include but are not limited to: a microphone <b>116</b>; a speaker <b>118</b>; a front or rear facing camera <b>120</b>; accelerometers <b>122</b> (inclinometers), gyroscopes <b>124</b>, a magnetometers or compass <b>126</b>; a global positioning satellite (GPS) module <b>128</b>; a barometer <b>130</b>; a proximity sensor <b>132</b>; and an ambient light sensor <b>134</b>. By combining readings from a combination of sensors <b>108</b> with a fusion algorithm that may include a Kalman filter, relatively accurate position and orientation measurements can be obtained.</p><p id="p-0065" num="0064">It should be appreciated that the sensors <b>60</b>, <b>74</b> integrated into the scanner <b>30</b> may have different characteristics than the sensors <b>108</b> of mobile device <b>43</b>. For example, the resolution of the cameras <b>60</b>, <b>120</b> may be different, or the accelerometers <b>94</b>, <b>122</b> may have different dynamic ranges, frequency response, sensitivity (mV/g) or temperature parameters (sensitivity or range). Similarly, the gyroscopes <b>96</b>, <b>124</b> or compass/magnetometer may have different characteristics. It is anticipated that in some embodiments, one or more sensors <b>108</b> in the mobile device <b>43</b> may be of higher accuracy than the corresponding sensors <b>74</b> in the system <b>30</b>. As described in more detail herein, in some embodiments the processor <b>78</b> determines the characteristics of each of the sensors <b>108</b> and compares them with the corresponding sensors in the system <b>30</b> when the mobile device. The processor <b>78</b> then selects which sensors <b>74</b>, <b>108</b> are used during operation. In some embodiments, the mobile device <b>43</b> may have additional sensors (e.g. microphone <b>116</b>, camera <b>120</b>) that may be used to enhance operation compared to operation of the system <b>30</b> without the mobile device <b>43</b>. In still further embodiments, the system <b>30</b> does not include the IMU <b>74</b> and the processor <b>78</b> uses the sensors <b>108</b> for tracking the position and orientation/pose of the system <b>30</b>. In still further embodiments, the addition of the mobile device <b>43</b> allows the system <b>30</b> to utilize the camera <b>120</b> to perform three-dimensional (3D) measurements either directly (using an RGB-D camera) or using photogrammetry techniques to generate 3D maps. In an embodiment, the processor <b>78</b> uses the communications circuit (e.g. a cellular 4G internet connection) to transmit and receive data from remote computers or devices.</p><p id="p-0066" num="0065">In the exemplary embodiment, the system <b>30</b> is a handheld portable device that is sized and weighted to be carried by a single person during operation. Therefore, the plane <b>136</b> (<figref idref="DRAWINGS">FIG. <b>18</b></figref>) in which the 2D scanner <b>50</b> projects a light beam may not be horizontal relative to the floor or may continuously change as the computer moves during the scanning process. Thus, the signals generated by the accelerometers <b>94</b>, gyroscopes <b>96</b> and compass <b>98</b> (or the corresponding sensors <b>108</b>) may be used to determine the pose (yaw, roll, tilt) of the system <b>30</b> and determine the orientation of the plane <b>51</b>.</p><p id="p-0067" num="0066">In an embodiment, it may be desired to maintain the pose of the system <b>30</b> (and thus the plane <b>136</b>) within predetermined thresholds relative to the yaw, roll and pitch orientations of the system <b>30</b>. In an embodiment, a haptic feedback device <b>77</b> is disposed within the housing <b>32</b>, such as in the handle <b>36</b>. The haptic feedback device <b>77</b> is a device that creates a force, vibration or motion that is felt or heard by the operator. The haptic feedback device <b>77</b> may be, but is not limited to: an eccentric rotating mass vibration motor or a linear resonant actuator for example. The haptic feedback device is used to alert the operator that the orientation of the light beam from 2D scanner <b>50</b> is equal to or beyond a predetermined threshold. In operation, when the IMU <b>74</b> measures an angle (yaw, roll, pitch or a combination thereof), the controller <b>68</b> transmits a signal to a motor controller <b>138</b> that activates a vibration motor <b>140</b>. Since the vibration originates in the handle <b>36</b>, the operator will be notified of the deviation in the orientation of the system <b>30</b>. The vibration continues until the system <b>30</b> is oriented within the predetermined threshold or the operator releases the actuator <b>38</b>. In an embodiment, it is desired for the plane <b>136</b> to be within 10-15 degrees of horizontal (relative to the ground) about the yaw, roll and pitch axes.</p><p id="p-0068" num="0067">In an embodiment, the 2D scanner <b>50</b> makes measurements as the system <b>30</b> is moved about an environment, such from a first position <b>142</b> to a second registration position <b>144</b> as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. In an embodiment, 2D scan data is collected and processed as the system <b>30</b> passes through a plurality of 2D measuring positions <b>146</b>. At each measuring position <b>146</b>, the 2D scanner <b>50</b> collects 2D coordinate data over an effective FOV <b>148</b>. Using methods described in more detail below, the controller <b>68</b> uses 2D scan data from the plurality of 2D scans at positions <b>146</b> to determine a position and orientation of the system <b>30</b> as it is moved about the environment. In an embodiment, the common coordinate system is represented by 2D Cartesian coordinates x, y and by an angle of rotation &#x3b8; relative to the x or y axis. In an embodiment, the x and y axes lie in the plane of the 2D scanner and may be further based on a direction of a &#x201c;front&#x201d; of the 2D scanner <b>50</b>.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows the 2D system <b>30</b> collecting 2D scan data at selected positions <b>108</b> over an effective FOV <b>110</b>. At different positions <b>146</b>, the 2D scanner <b>50</b> captures a portion of the object <b>150</b> marked A, B, C, D, and E. <figref idref="DRAWINGS">FIG. <b>12</b>I</figref> shows 2D scanner <b>50</b> moving in time relative to a fixed frame of reference of the object <b>150</b>.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>13</b></figref> includes the same information as <figref idref="DRAWINGS">FIG. <b>12</b></figref> but shows it from the frame of reference of the system <b>30</b> rather than the frame of reference of the object <b>150</b>. <figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates that in the system <b>30</b> frame of reference, the position of features on the object change over time. Therefore, the distance traveled by the system <b>30</b> can be determined from the 2D scan data sent from the 2D scanner <b>50</b> to the controller <b>68</b>.</p><p id="p-0071" num="0070">As the 2D scanner <b>50</b> takes successive 2D readings and performs best-fit calculations, the controller <b>68</b> keeps track of the translation and rotation of the 2D scanner <b>50</b>, which is the same as the translation and rotation of the system <b>30</b>. In this way, the controller <b>68</b> is able to accurately determine the change in the values of x, y, &#x3b8; as the system <b>30</b> moves from the first position <b>142</b> to the second position <b>144</b>.</p><p id="p-0072" num="0071">In an embodiment, the controller <b>68</b> is configured to determine a first translation value, a second translation value, along with first and second rotation values (yaw, roll, pitch) that, when applied to a combination of the first 2D scan data and second 2D scan data, results in transformed first 2D data that closely matches transformed second 2D data according to an objective mathematical criterion. In general, the translation and rotation may be applied to the first scan data, the second scan data, or to a combination of the two. For example, a translation applied to the first data set is equivalent to a negative of the translation applied to the second data set in the sense that both actions produce the same match in the transformed data sets. An example of an &#x201c;objective mathematical criterion&#x201d; is that of minimizing the sum of squared residual errors for those portions of the scan data determined to overlap. Another type of objective mathematical criterion may involve a matching of multiple features identified on the object. For example, such features might be the edge transitions <b>152</b>, <b>154</b>, and <b>156</b> shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. The mathematical criterion may involve processing of the raw data provided by the 2D scanner <b>50</b> to the controller <b>68</b>, or it may involve a first intermediate level of processing in which features are represented as a collection of line segments using methods that are known in the art, for example, methods based on the Iterative Closest Point (ICP). Such a method based on ICP is described in Censi, A., &#x201c;An ICP variant using a point-to-line metric,&#x201d; IEEE International Conference on Robotics and Automation (ICRA) 2008, which is incorporated by reference herein.</p><p id="p-0073" num="0072">In an embodiment, assuming that the plane <b>136</b> of the light beam from 2D scanner <b>50</b> remains horizontal relative to the ground plane, the first translation value is dx, the second translation value is dy, and the first rotation value d&#x3b8;. If the first scan data is collected with the 2D scanner <b>50</b> having translational and rotational coordinates (in a reference coordinate system) of (x<sub>1</sub>, y<sub>1</sub>, &#x3b8;<sub>1</sub>), then when the second 2D scan data is collected at a second location the coordinates are given by (x<sub>2</sub>, y<sub>2</sub>, &#x3b8;<sub>2</sub>)=(x<sub>1</sub>+dx, y<sub>1</sub>+dy, &#x3b8;<sub>1</sub>+d&#x3b8;). In an embodiment, the controller <b>68</b> is further configured to determine a third translation value (for example, dz) and a second and third rotation values (for example, pitch and roll). The third translation value, second rotation value, and third rotation value may be determined based at least in part on readings from the IMU <b>74</b>.</p><p id="p-0074" num="0073">The 2D scanner <b>50</b> collects 2D scan data starting at the first position <b>142</b> and more 2D scan data at the second position <b>144</b>. In some cases, these scans may suffice to determine the position and orientation of the system <b>30</b> at the second position <b>144</b> relative to the first position <b>142</b>. In other cases, the two sets of 2D scan data are not sufficient to enable the controller <b>68</b> to accurately determine the first translation value, the second translation value, and the first rotation value. This problem may be avoided by collecting 2D scan data at intermediate scan positions <b>146</b>. In an embodiment, the 2D scan data is collected and processed at regular intervals, for example, once per second. In this way, features in the environment are identified in successive 2D scans at positions <b>146</b>. In an embodiment, when more than two 2D scans are obtained, the controller <b>68</b> may use the information from all the successive 2D scans in determining the translation and rotation values in moving from the first position <b>142</b> to the second position <b>144</b>. In another embodiment, only the first and last scans in the final calculation, simply using the intermediate 2D scans to ensure proper correspondence of matching features. In most cases, accuracy of matching is improved by incorporating information from multiple successive 2D scans.</p><p id="p-0075" num="0074">It should be appreciated that as the system <b>30</b> is moved beyond the second position <b>144</b>, a two-dimensional image or map of the environment being scanned may be generated.</p><p id="p-0076" num="0075">Referring now to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, a method <b>160</b> is shown for generating a two-dimensional map with annotations. The method <b>160</b> starts in block <b>162</b> where the facility or area is scanned to acquire scan data <b>170</b>, such as that shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>. The scanning is performed by carrying the system <b>30</b> through the area to be scanned. The system <b>30</b> measures distances from the system <b>30</b> to an object, such as a wall for example, and also a pose of the system <b>30</b> in an embodiment the user interacts with the system <b>30</b> via actuator <b>38</b>. In the illustrated embodiments, the mobile device <b>43</b> provides a user interface that allows the operator to initiate the functions and control methods described herein. Using the registration process desired herein, the two dimensional locations of the measured points on the scanned objects (e.g. walls, doors, windows, cubicles, file cabinets etc.) may be determined. It is noted that the initial scan data may include artifacts, such as data that extends through a window <b>172</b> or an open door <b>174</b> for example. Therefore, the scan data <b>170</b> may include additional information that is not desired in a 2D map or layout of the scanned area.</p><p id="p-0077" num="0076">The method <b>120</b> then proceeds to block <b>164</b> where a 2D map <b>176</b> is generated of the scanned area as shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>. The generated 2D map <b>176</b> represents a scan of the area, such as in the form of a floor plan without the artifacts of the initial scan data. It should be appreciated that the 2D map <b>176</b> may be utilized directly by an architect, interior designer or construction contractor as it represents a dimensionally accurate representation of the scanned area. In the embodiment of FIG. <b>14</b>, the method <b>160</b> then proceeds to block <b>166</b> where optional user-defined annotations are made to the 2D maps <b>176</b> to define an annotated 2D map that includes information, such as dimensions of features, the location of doors, the relative positions of objects (e.g. liquid oxygen tanks, entrances/exits or egresses or other notable features such as but not limited to the location of automated sprinkler systems, knox or key boxes, or fire department connection points (&#x201c;FDC&#x201d;). In some geographic regions, public safety services such as fire departments may keep records of building or facility layouts for use in case of an emergency as an aid to the public safety personnel in responding to an event. It should be appreciated that these annotations may be advantageous in alerting the public safety personnel to potential issues they may encounter when entering the facility, and also allow them to quickly locate egress locations.</p><p id="p-0078" num="0077">Once the annotations of the 2D annotated map are completed, the method <b>160</b> then proceeds to block <b>168</b> where the 2D map is stored in memory, such as nonvolatile memory <b>86</b> for example. The 2D map may also be stored in a network accessible storage device or server so that it may be accessed by the desired personnel.</p><p id="p-0079" num="0078">Referring now to <figref idref="DRAWINGS">FIG. <b>17</b></figref> and <figref idref="DRAWINGS">FIG. <b>18</b></figref> an embodiment is illustrated with the mobile device <b>43</b> coupled to the system <b>20</b>. As described herein, the 2D scanner <b>50</b> emits a beam of light in the plane <b>136</b>. In an embodiment, the system <b>20</b> is the same as the scanner <b>30</b> described herein with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>13</b></figref>. The 2D scanner <b>50</b> has a field of view (FOV) that extends over an angle that is less than 360 degrees. In the exemplary embodiment, the FOV of the 2D scanner is about 270 degrees. In this embodiment, the mobile device <b>43</b> is coupled to the housing <b>32</b> adjacent the end where the 2D scanner <b>50</b> is arranged. The mobile device <b>43</b> includes a forward facing camera <b>120</b>. The camera <b>120</b> is positioned adjacent a top side of the mobile device and has a predetermined field of view <b>180</b>. In the illustrated embodiment, the holder <b>41</b> couples the mobile device <b>43</b> on an obtuse angle <b>182</b>. This arrangement allows the mobile device <b>43</b> to acquire images of the floor and the area directly in front of the system <b>20</b> (e.g. the direction the operator is moving the system <b>20</b>).</p><p id="p-0080" num="0079">In embodiments where the camera <b>120</b> is a RGB-D type camera, three-dimensional coordinates of surfaces in the environment may be directly determined in a mobile device coordinate frame of reference. In an embodiment, the holder <b>41</b> allows for the mounting of the mobile device <b>43</b> in a stable position (e.g. no relative movement) relative to the 2D scanner <b>50</b>. When the mobile device <b>43</b> is coupled to the housing <b>32</b>, the processor <b>78</b> performs a calibration of the mobile device <b>43</b> allowing for a fusion of the data from sensors <b>108</b> with the sensors of system <b>20</b>. As a result, the coordinates of the 2D scanner may be transformed into the mobile device coordinate frame of reference or the 3D coordinates acquired by camera <b>120</b> may be transformed into the 2D scanner coordinate frame of reference.</p><p id="p-0081" num="0080">In an embodiment, the mobile device is calibrated to the 2D scanner <b>50</b> by assuming the position of the mobile device based on the geometry and position of the holder <b>41</b> relative to 2D scanner <b>50</b>. In this embodiment, it is assumed that the holder that causes the mobile device to be positioned in the same manner. It should be appreciated that this type of calibration may not have a desired level of accuracy due to manufacturing tolerance variations and variations in the positioning of the mobile device <b>43</b> in the holder <b>41</b>. In another embodiment, a calibration is performed each time a different mobile device <b>43</b> is used. In this embodiment, the user is guided (such as via the user interface <b>110</b>) to direct the system <b>30</b> to scan a specific object, such as a door, that can be readily identified in the laser readings of the system <b>30</b> and in the camera-sensor <b>120</b> using an object recognition method.</p><p id="p-0082" num="0081">Referring now to <figref idref="DRAWINGS">FIG. <b>19</b></figref>, a method <b>200</b> is provided for generating a 2D map of an environment. The method <b>200</b> begins in block <b>202</b> where the operator couples the mobile device <b>43</b> to the holder <b>41</b>. In an embodiment, the coupling includes forming a communication connection between the processor <b>78</b> and the processor <b>104</b>. This communication connection allows the processors <b>78</b>, <b>104</b> to exchange data, including sensor data, therebetween. The method <b>200</b> then proceeds to block <b>204</b> where information regarding the sensors <b>108</b> is transmitted to the processor <b>78</b>. The information transmitted includes the type of sensors (e.g. accelerometer) and performance characteristics or parameters of the sensor (e.g. dynamic range, frequency response, sensitivity (mV/g) temperature sensitivity, or temperature range).</p><p id="p-0083" num="0082">The method <b>200</b> then proceeds to block <b>206</b> where the processor <b>78</b> compares the sensors <b>108</b> with the corresponding sensors in the system <b>20</b>. In an embodiment, this comparison includes comparing performance characteristics or parameters and determining which sensor would provide a desired accuracy of the scan. It should be appreciated that this comparison is performed on a sensor by sensor basis. In some embodiments, the data used for tracking and pose may be a combination of the sensors from the mobile device <b>43</b> and the system <b>20</b>. For example, the accelerometer <b>122</b> may be used in combination with the gyroscope <b>96</b> and compass <b>98</b> for determining tracking and pose.</p><p id="p-0084" num="0083">In an embodiment, once the sensors are selected the method <b>200</b> a calibration step is performed in block <b>208</b>. As discussed herein, the calibration step allows the transforming of data between the mobile device coordinate frame of reference and the 2D scanner coordinate frame of reference.</p><p id="p-0085" num="0084">The method <b>200</b> then proceeds to block <b>210</b> where the scan is performed by moving the system <b>20</b> (with mobile device <b>43</b> attached) about the environment. As the scan is being performed (e.g. the 2D scanner is emitting and receiving reflected light and determining distances), the method <b>200</b> is transforming data in block <b>212</b> into a common frame of reference, such as the 2D scanner frame of reference for example, so that coordinates of the points of surfaces in the environment may be determined. As the scan is being performed, the position and pose of the system <b>20</b> is determined on a periodic, aperiodic or continuous basis as described herein.</p><p id="p-0086" num="0085">Once the scan is completed, the method <b>200</b> proceeds to block <b>214</b> where the 2D map is generated of the scanned area. It should be appreciated that in embodiments where the camera <b>120</b> is a 3D camera or RGB-D type camera, a 3D map of the environment may be generated.</p><p id="p-0087" num="0086">Referring now to <figref idref="DRAWINGS">FIG. <b>20</b></figref>, an embodiment of a method <b>2000</b> is shown for segmenting the 2D map generated by method <b>200</b> into segmented/defined spaces or rooms. It should be appreciated that the generated 2D map, such as map <b>176</b> (<figref idref="DRAWINGS">FIG. <b>16</b></figref>) for example, represents the boundaries of the spaces within the environment scanned by the system <b>20</b>. In some embodiments it is desirable to segment the 2D-map into rooms or other designated spaces and apply information or meta-data to the segmented area. This information may provide context for other users of the 2D-map, or may be used by other methods to provide additional functionality (e.g. planning using interior design software). One issue that sometimes arises with automated methods of segmenting the rooms is that artifacts in the 2-D map, such as caused by windows, counters, appliances (e.g. refrigerators), file cabinets, or shelving units for example, may prevent the 2D map from accurately displaying the boundaries of the room.</p><p id="p-0088" num="0087">Accordingly, method <b>2000</b> provides a solution to the technical problem of designating rooms or spaces. The method <b>2000</b> starts in block <b>2002</b> where the user selects, such as by using the user interface on display <b>110</b> for example a room segmentation mode of operation on the system <b>20</b>. The operator then, in block <b>2004</b>, directs the light toward an edge that defines an intersection of two or more planes (e.g. walls) in a corner or vertex point of a room/space. The light may be emitted from laser light projector <b>76</b> for example. It should be appreciated that in some embodiments, the system <b>20</b> may have multiple laser light sources allowing multiple points of light to be emitted. The system <b>20</b> then scans in block <b>2006</b>, using the 2D scanner <b>50</b> for example, to measure the at least surfaces that meet to form the edge. Since the 2D scanner <b>50</b> measures in a plane, the scanning should measure points on both surfaces in the plane. The method <b>2000</b> then proceeds to block <b>2008</b> where the edge (e.g. corner or vertex) is detected by determining the intersection of lines formed by the measured points on each surface. The method then proceeds to block <b>2010</b> where the location of the edge on the 2D-map is recorded and stored.</p><p id="p-0089" num="0088">The method <b>2000</b> then proceeds to query block <b>2012</b> where it is determined whether there are additional corners in the room or space being segmented. When the query block <b>2012</b> returns a positive, the method <b>2000</b> loops back to block <b>2004</b> and the method <b>2000</b> continues. When the query block <b>2012</b> returns a negative, the method <b>2000</b> proceeds to block <b>2014</b> and generates a polygon representing the room based on the edges detected in block <b>2008</b>.</p><p id="p-0090" num="0089">Referring now to <figref idref="DRAWINGS">FIGS. <b>21</b>-<b>26</b>C</figref>, a series of images of a user interface <b>2100</b> (such as display <b>110</b> for example) are shown during the method <b>2000</b> for segmenting spaces or rooms within the 2D-map <b>2102</b>. In an embodiment, the operator of system engage activates a segmentation mode of operation. In response, the system <b>20</b> (e.g. a 2D laser scanner) localizes itself in the environment with respect to the 2D map <b>2102</b>. In response to a successful localization, an icon <b>2104</b> is displayed representing the operator and system</p><p id="p-0091" num="0090">It should be appreciated that while embodiments herein describe the performance of operational control methods by the processor <b>78</b>, this is for exemplary purposes and the claims should not be so limited. In other embodiments, the operation control methods may be performed by the processor <b>104</b> or a combination of the processor <b>78</b> and the processor <b>104</b>. In an embodiment, the operator may activate the laser light device <b>76</b> to indicate a doorway, such as doorway <b>2108</b> for example. In an embodiment, the location of the spot of light <b>2106</b> may be indicated on the 2D map <b>2102</b>.</p><p id="p-0092" num="0091">The operator then proceeds to the next edge/corner of the room/space and once again activates the laser light device <b>76</b> which directs a spot of light <b>2110</b> either on the walls <b>2112</b>, <b>2114</b> or the edge <b>2116</b> itself to indicate the location of the edge/corner. The system <b>20</b> localizes itself in the environment with respect to the 2D map <b>2102</b> so that the icon <b>2104</b> is shown on the 2D map <b>2102</b> in approximately the same area that the operator is occupying. In the example embodiment, the system <b>20</b> scans or measures coordinate points within plane, in other words it is a two-dimensional scanning device. In an embodiment, an approximate location of the spot of light <b>2110</b> is also shown on the 2D map <b>2102</b>. With the spot of light <b>2110</b> on or adjacent to the edge <b>2116</b>, the system <b>30</b> projects light in a plane (as described herein) and measures a first plurality of points on the first wall <b>2112</b> and a second plurality of points on the second wall <b>2114</b>. As described By fitting a first line to the first plurality of points and a second line to the second plurality of points, an intersection of the first line and second line may be found to determine the location of the edge <b>2116</b>. The location of the edge <b>2116</b> on the 2D-map <b>2102</b> is then stored.</p><p id="p-0093" num="0092">The operator then proceeds about the room and marks the location of edges/corners in the same manner as described with respect to <figref idref="DRAWINGS">FIG. <b>22</b>A, <b>22</b>B</figref>. It should be appreciated that this allows for corners of rooms to be identified even if the edges/corners are not visible when in the horizontal plane (e.g. parallel to the floor) that was initially used when the system <b>20</b> generated the 2D-map <b>2102</b>. This can be accomplished by pointing the laser light device towards an area near a ceiling, such as is shown in <figref idref="DRAWINGS">FIG. <b>24</b>B</figref> for example. Since the system <b>20</b> may include an IMU <b>74</b>, the tilt or pose of the system <b>20</b> may be determined and the location of the edge/corner relative to the map <b>2102</b> may be determined. Thus, the pose or position of the system <b>20</b> may be altered to direct the laser light from device <b>76</b> around obstacles, appliances or furniture to allow determination of the edge/corner of the room/space.</p><p id="p-0094" num="0093">Once the edges/corners of the room have been determined, the operator may add metadata <b>2120</b> or information, such as but not limited to a room name for example, and save the segregated room/space. In an embodiment, a polygon <b>2122</b> may be placed on the 2D map <b>2102</b>.</p><p id="p-0095" num="0094">It should be appreciated that the map generated by the system <b>20</b> measures the environment to the surface of walls (e.g when scanning a building). However, some post processing systems, such as interior design or planning software for example, typically expect the lines on a floor plan to represent the center of the wall, rather than the outside surface. In an embodiment, the user may optionally modify the 2D-map <b>2102</b> to offset the lines representing walls such that the line represents the center of the wall. The amount of offset may be predetermined or user defined (e.g. the user inputs the standard wall thickness of their local building practices. In other embodiments, the offset may be automatically determined, such as by determining the distance between two adjacent and parallel walls.</p><p id="p-0096" num="0095">The term &#x201c;about&#x201d; is intended to include the degree of error associated with measurement of the particular quantity based upon the equipment available at the time of filing the application.</p><p id="p-0097" num="0096">The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the disclosure. As used herein, the singular forms &#x201c;a&#x201d;, &#x201c;an&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms &#x201c;comprises&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, element components, and/or groups thereof.</p><p id="p-0098" num="0097">While the invention has been described in detail in connection with only a limited number of embodiments, it should be readily understood that the invention is not limited to such disclosed embodiments. Rather, the invention can be modified to incorporate any number of variations, alterations, substitutions or equivalent arrangements not heretofore described, but which are commensurate with the spirit and scope of the invention. Additionally, while various embodiments of the invention have been described, it is to be understood that aspects of the invention may include only some of the described embodiments. Accordingly, the invention is not to be seen as limited by the foregoing description, but is only limited by the scope of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system of generating a two-dimensional (2D) image of an environment, the system comprising:<claim-text>a 2D scanner having a first light source, an image sensor, a second light source and a controller, the second light source being configured to emit a visible light, the controller being operable to determine a distance value to object points in the environment based at least in part on a beam of light emitted by the first light source and the receiving of the beam of light reflected from the object points;</claim-text><claim-text>one or more processors operably coupled to the 2D scanner, the one or more processors being responsive to nontransitory executable instructions to execute a method comprising:<claim-text>generating a plan view map of the environment;</claim-text><claim-text>emitting light from the second light source towards an edge defined by at least a pair of surfaces;</claim-text><claim-text>detecting the edge based at least in part on emitting a second beam of light from the light source and receiving the second beam of light reflected from either the edge or from the pair of surfaces; and</claim-text><claim-text>defining a room on the plan view map based at least in part on the detecting of the corner or the edge.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises associating the detected edge with a location on the plan view map.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises detecting a plurality of edges, and generating a polygon on the plan view map defined by the edges.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the detecting of the edge includes measuring a plurality of first points on a first surface of the pair of surfaces and measuring a plurality of second points on a second surface of the pair of surfaces.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the edge is defined by a first line and a second line, the first line being defined by the plurality of first points, the second line being defined by the plurality of second points.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises<claim-text>detecting a plurality of second edges based at least in part on emitting the second beam of light from the light source and receiving the reflected second beam of light; and</claim-text><claim-text>defining a doorway on the plan view map based on the plurality of second edges.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises<claim-text>detecting a plurality of third edges based at least in part on emitting a second beam of light from the light source and receiving the reflected second beam of light; and</claim-text><claim-text>defining a doorway on the plan view map based on the plurality of third edges.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method for generating a two-dimensional (2D) image of an environment, the method comprising:<claim-text>providing a 2D scanner having a first light source, an image sensor, a second light source and a controller, the second light source being configured to emit a visible light, the controller being operable to determine a distance value to object points in the environment based at least in part on a beam of light emitted by the first light source and the receiving of the beam of light reflected from the object points;</claim-text><claim-text>generating a plan view map of the environment;</claim-text><claim-text>emitting light from the second light source towards an edge defined by at least a pair of surfaces;</claim-text><claim-text>detecting the edge based at least in part on emitting a second beam of light from the light source and receiving the second beam of light reflected from either the edge or from the pair of surfaces; and</claim-text><claim-text>defining a room on the plan view map based at least in part on the detecting of the corner or the edge.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising associating the detected edge with a location on the plan view map.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising detecting a plurality of edges, and generating a polygon on the plan view map defined by the edges.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the detecting of the edge includes measuring a plurality of first points on a first surface of the pair of surfaces and measuring a plurality of second points on a second surface of the pair of surfaces.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the edge is defined by a first line and a second line, the first line being defined by the plurality of first points, the second line being defined by the plurality of second points.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>detecting a plurality of second edges based at least in part on emitting the second beam of light from the light source and receiving the reflected second beam of light; and</claim-text><claim-text>defining a doorway on the plan view map based on the plurality of second edges.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>detecting a plurality of third edges based at least in part on emitting a second beam of light from the light source and receiving the reflected second beam of light; and</claim-text><claim-text>defining a doorway on the plan view map based on the plurality of third edges.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A system of generating a two-dimensional (2D) image of an environment, the system comprising:<claim-text>one or more processors;</claim-text><claim-text>a 2D scanner sized and weighted to be carried by a single person, having a first light source, an image sensor, an inertial measurement unit having a first plurality of sensors, the first light source steers a beam of light within a first plane to illuminate object points in the environment, the image sensor is arranged to receive light reflected from the object points;</claim-text><claim-text>a mobile computing device removably coupled to the 2D scanner, the mobile computing device having a second plurality of sensors;</claim-text><claim-text>wherein the one or more processors are responsive to executable instructions which when executed by the one or more processors to:<claim-text>generating a plan view map of the environment;</claim-text><claim-text>emitting light from the second light source towards an edge defined by at least a pair of surfaces;</claim-text><claim-text>detecting the edge based at least in part on emitting a second beam of light from the light source and receiving the second beam of light reflected from either the edge or from the pair of surfaces; and</claim-text><claim-text>defining a room on the plan view map based at least in part on the detecting of the corner or the edge.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more processors are further responsive to associate the detected edge with a location on the plan view map.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more processors are further responsive to detect a plurality of edges, and generating a polygon on the plan view map defined by the edges.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the detecting of the edge includes measuring a plurality of first points on a first surface of the pair of surfaces and measuring a plurality of second points on a second surface of the pair of surfaces.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the edge is defined by a first line and a second line, the first line being defined by the plurality of first points, the second line being defined by the plurality of second points.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more processors are further responsive to:<claim-text>detect a plurality of second edges based at least in part on emitting the second beam of light from the light source and receiving the reflected second beam of light; and</claim-text><claim-text>define a doorway on the plan view map based on the plurality of second edges.</claim-text></claim-text></claim></claims></us-patent-application>