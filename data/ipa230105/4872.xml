<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004873A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004873</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17859886</doc-number><date>20220707</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND METHOD FOR ARTIFICIAL INTELLIGENCE DRIVEN DOCUMENT ANALYSIS, INCLUDING AUTOMATED REUSE OF PREDICTIVE CODING RULES BASED ON MANAGEMENT AND CURATION OF DATASETS OR MODELS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16881274</doc-number><date>20200522</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11416685</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17859886</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62968659</doc-number><date>20200131</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>CS Disco, Inc.</orgname><address><city>Austin</city><state>TX</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Lockett</last-name><first-name>Alan Justin</first-name><address><city>Georgetown</city><state>TX</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Fischer</last-name><first-name>Verlyn Michael</first-name><address><city>Cedar Park</city><state>TX</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Vestal</last-name><first-name>Richard Alan</first-name><address><city>Jonestown</city><state>TX</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Ramos</last-name><first-name>Jesse Abraham</first-name><address><city>Austin</city><state>TX</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Harrington</last-name><first-name>Robert Duane</first-name><address><city>Austin</city><state>TX</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Luskey</last-name><first-name>Brian Daniel</first-name><address><city>Ausitn</city><state>TX</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Artificial intelligence based document analysis systems and methods are disclosed. Embodiments of document analysis systems may allow the reuse of coded datasets defined in association with a particular code by allowing these datasets to be bundled to define a dataset for another code, where that code may be associated with a target corpus of documents. A model can then be trained based on that dataset and used to provide predictive scores for the documents of the target corpora with respect to the code. Furthermore, this code can be applied not just to the target corpus of documents, but additionally can be applied against any other corpora.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="56.22mm" wi="158.75mm" file="US20230004873A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="237.49mm" wi="170.18mm" orientation="landscape" file="US20230004873A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="240.03mm" wi="169.25mm" orientation="landscape" file="US20230004873A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="243.16mm" wi="170.18mm" orientation="landscape" file="US20230004873A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="242.23mm" wi="168.66mm" orientation="landscape" file="US20230004873A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="243.25mm" wi="170.18mm" orientation="landscape" file="US20230004873A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="244.86mm" wi="170.69mm" orientation="landscape" file="US20230004873A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="244.77mm" wi="166.12mm" file="US20230004873A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="237.15mm" wi="161.12mm" orientation="landscape" file="US20230004873A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="237.15mm" wi="161.12mm" orientation="landscape" file="US20230004873A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="237.15mm" wi="161.12mm" orientation="landscape" file="US20230004873A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="213.19mm" wi="166.20mm" orientation="landscape" file="US20230004873A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="242.65mm" wi="169.16mm" orientation="landscape" file="US20230004873A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="213.19mm" wi="154.18mm" orientation="landscape" file="US20230004873A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of, and claims a benefit of priority under 35 U.S.C. 120 of, U.S. patent application Ser. No. 16/881,274 filed May 22, 2020, entitled &#x201c;SYSTEM AND METHOD FOR ARTIFICIAL INTELLIGENCE DRIVEN DOCUMENT ANALYSIS, INCLUDING AUTOMATED REUSE OF PREDICTIVE CODING RULES BASED ON MANAGEMENT AND CURATION OF DATASETS OR MODELS,&#x201d; issued as U.S. Pat. No. 11,416,685, which claims the benefit of priority under 35 U.S.C. &#xa7; 119 to U.S. Provisional Application No. 62/968,659 filed Jan. 31, 2020, entitled &#x201c;SYSTEM AND METHOD FOR AUTOMATED REUSE OF PREDICTIVE CODING RULES BASED ON MANAGEMENT AND CURATION OF DATASETS OR MODELS,&#x201d; which are hereby fully incorporated by reference herein for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">COPYRIGHT NOTICE</heading><p id="p-0003" num="0002">A portion of the disclosure of this patent document contains material to which a claim for copyright is made. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure, as it appears in the Patent and Trademark Office patent file or records, but reserves all other copyright rights whatsoever.</p><heading id="h-0003" level="1">TECHNICAL FIELD</heading><p id="p-0004" num="0003">This disclosure relates generally to semantic analysis and understanding of electronic documents. In particular, this disclosure relates to the semantic analysis and understanding of a potentially large corpus of documents including the use of machine learning, neural networks, transfer learning or predictive coding within document analysis. Even more specifically, this disclosure relates to predictive coding of documents of a corpus and reuse of data, models or coding decisions developed with respect to one corpus in the coding of another corpus.</p><heading id="h-0004" level="1">BACKGROUND</heading><p id="p-0005" num="0004">In the modern world, the vast majority of documents that are being created, utilized and maintained are in electronic format. A number of different situations commonly arise that require an analysis or identification of certain relevant electronic documents from a relatively large pool of available electronic documents. These types of search problems crop up in a wide variety of contexts. For example, in litigation, an entity's documents may need to be reviewed in order to identify documents that may be relevant to one or more issues in a litigation. In other examples, certain regulatory filings may require review of a number of documents to identify documents that may be relevant to one or more issues in the regulatory filing.</p><p id="p-0006" num="0005">To illustrate in more detail, parties to litigation typically have to share relevant evidence with opposing counsel through the discovery process. In many cases, each party makes a reasonable search of their records based on some set of terms or keywords and produces the results of the search to the other party. Discovery thus typically involves the gathering of potentially relevant materials, much of it digital, and then reviewing such materials to determine what is to be shared with opposite parties. Additionally, d the course of the litigation each party may continually review those documents produced by the opposing party to locate documents relevant to the case at hand.</p><p id="p-0007" num="0006">Litigation thus represents a microcosm of a more general problem raised by the high volume, and increasing presence and use of, electronic documents across a variety of different contexts. Namely, how can a large volume of electronic documents be understood, reviewed, or searched in order that documents relevant to a particular topic or user's interest may be located. To aid users in resolving these problems, a document analysis system may be provided in a given electronic context. A document analysis system is typically used to allow users to analyze, review, navigate or search the electronic information to return responsive electronically stored information.</p><p id="p-0008" num="0007">Accordingly, there is constantly a need for improved systems and methods for document analysis to assist in the analysis, review, navigation, or search of such electronic documents in order to allow such document analysis systems to better aid users engaged in such tasks, including allowing users to better identify relevant electronic documents from a pool of available electronic documents.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0009" num="0008">Attention is thus directed to the embodiments of document analysis systems and methods disclosed herein. Specifically, embodiments may relate previously coded datasets and trained models to new corpora and employ these previously coded datasets and trained models in the context of predictive coding. Thus, embodiments may allow a user to bundle datasets in order to form an aggregate dataset. Accordingly, the previously coded datasets and previously trained models can be reused to provide predictive scores for a new corpus with no human coding. Furthermore, the quality of this set of codes can be higher than the quality of predictive coding without such reuse due to the fact that the previously coded datasets may be much larger in scale or of higher quality. Moreover, by transferring datasets, coding decisions, or models from previous predictive coding applications (e.g., within a defined scope) the amount of labor required by predictive coding may be reduced and the efficiency of such coding improved.</p><p id="p-0010" num="0009">Thus, embodiments may enable the reuse of prior work product across corpora by aggregating data and coding decisions from one or more corpora in order to form boosted codes, where a boosted code represents a new category formed by identifying an enumerated set of codes as a single category and forming a composite dataset accordingly. A predictive coding process may train models incrementally as coding decisions are made across the boosted codes based on the composite datasets. The models for these boosted codes may then be used to augment or preempt predictive coding in another corpora, which may or may not be associated with the boosted code (or any of the boosting codes). Additionally, testing procedures can be used to decide between the use of boosting, hybrid, or native models in predictive coding for boosted codes.</p><p id="p-0011" num="0010">To achieve these capabilities, among others, embodiments of document analysis systems as disclosed herein may (a) record and analyze the meaning of previously observed codes and the quality of datasets and models for previously coded corpora to which predictive coding has been applied; (b) manage the relationships among previously coded datasets and models based on the meaning and quality of previously coded datasets and models in order to offer datasets and models for codes with common meaning; (c) manage these datasets and models in the context of a system in which access to the previously coded data may be ephemeral or temporary; (d) discover potentially relevant codes from among the datasets and models and apply them to a new corpus; and (e) monitor the quality of reused datasets and models as applied to new corpora in order to guarantee that the quality achieved by an overall predictive coding system incorporating reused codes is not less than the quality of the same system without reused codes.</p><p id="p-0012" num="0011">Embodiments as presented herein may thus have a number of advantages. As one advantage, embodiments may provide labor savings due to the fact that machine coding recommendations are available faster (potentially immediately) and may be more accurate due to the reuse of previous coding decisions. These machine coding recommendations may be accurate and rapidly produced even on small corpora by leveraging the &#x201c;knowledge&#x201d; gleaned from other data sets. In addition, a boosted code can be applied to generate predictive coding recommendations before a single document has ever been coded. Furthermore, if several corpora are being coded in parallel, each of which has one or more codes that participate in a common dataset, the coding decisions on one matter can benefit the review process in all the others, resulting in increased learning speed and consequent productivity. Thus, embodiments may employ continuous learning as new coding occurs in other corpora while also providing the opportunity to incorporate data from new corpora into predictive coding for existing corpora. Embodiments may also offer testing and evaluation to assess the value of the reuse of codes and to test boosting or hybrid models (e.g., relative to a native model built solely from a single corpus.</p><p id="p-0013" num="0012">As yet another advantage, embodiments may employ models and associated training processes that protect against the disappearance of data on which these models are built. When data disappears, the model built from the data does not disappear and is still available to be applied to score documents of a corpus. In this manner, embodiments allow a model to convey the benefit or knowledge gleaned from data without the liability of having to actually keep the data around. Such capabilities may be especially useful in certain spaces where data may be more ephemeral or walled off (e.g., in a litigation context).</p><p id="p-0014" num="0013">In one particular embodiment, a system for document analysis may include a data store, comprising a first target corpus of electronic document and a second target corpus of electronic documents. The document analysis system may receive a definition of a first code in association with the first target corpus and create a first dataset for the first code. The document analysis system can also receive an indication that the first code is to be boosted with a second code, wherein the second code is associated with the second target corpus and the second code is associated with a second dataset comprising a first set of positive signals associated with the second code and documents of the second corpus and a first set of negative signals associated with the second code and documents of the second target corpus. This indication may be received, for example through an interface that presents the second code as one of a plurality of codes, each of the plurality of codes presented with an associated textual description in the interface.</p><p id="p-0015" num="0014">The second dataset associated with the second code and the second target corpus can be added to the first dataset of the first code such that the first dataset comprises a boosting dataset including the second dataset comprising the first set of positive signals associated with the second code and documents of the second corpus and the first set of negative signals associated with the second code and documents of the second target corpus</p><p id="p-0016" num="0015">The document analysis system of embodiments can train a first machine learning model for the first code on the boosting dataset of the first dataset and generate predictive scores for the first code for documents of the first target corpus using the first machine learning model. These predictive scores can be presented (e.g., to a user) in association with documents of the first target corpus to a user.</p><p id="p-0017" num="0016">In some embodiments, the first machine learning model is trained only on the boosting dataset. In particular embodiments, the data analysis system may receive coding decisions for documents of the first target corpus with respect to the first code and store the coding decisions in association with the first dataset associated with the first code such that the first dataset comprises a native dataset comprising a second set of positive signals associated with the first code and documents of the first corpus and a second set of negative signals associated with the first code and documents of the first corpus.</p><p id="p-0018" num="0017">A second machine learning model can then be trained for the first code on the native dataset of the first dataset. The first machine learning model and the second machine learning model are evaluated to select a best machine learning model based on a test set of documents of the first corpus and predictive scores for the first code for documents of the first target corpus are generated using the best machine learning model and presented in association with documents of the first target corpus to a user.</p><p id="p-0019" num="0018">According to a specific embodiment, evaluating the first machine learning model and the second machine learning model to select the best machine learning model may include determining a current best machine learning model from the first machine learning model and the second machine learning model, and comparing the current best machine learning model to a previous best machine learning model using the test set of documents of the first target corpus to select the best model. The first machine learning model for the first code may be trained on the boosting dataset of the first dataset or the boosting dataset in combination with the native dataset of the first dataset.</p><p id="p-0020" num="0019">In one embodiment, the boosting dataset includes a plurality of datasets, each dataset having respective positive signals and negative signals, and training the first machine learning model for the first code on the boosting dataset comprises selecting a positive signal and a negative signal from each of the plurality of datasets according to a balancing method.</p><p id="p-0021" num="0020">These, and other, aspects of the disclosure will be better appreciated and understood when considered in conjunction with the following description and the accompanying drawings. It should be understood, however, that the following description, while indicating various embodiments of the disclosure and numerous specific details thereof, is given by way of illustration and not of limitation. Many substitutions, modifications, additions and/or rearrangements may be made within the scope of the disclosure without departing from the spirit thereof, and the disclosure includes all such substitutions, modifications, additions and/or rearrangements.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0022" num="0021">The drawings accompanying and forming part of this specification are included to depict certain aspects of the invention. A clearer impression of the invention, and of the components and operation of systems provided with the invention, will become more readily apparent by referring to the exemplary, and therefore non-limiting, embodiments illustrated in the drawings, wherein identical reference numerals designate the same components. Note that the features illustrated in the drawings are not necessarily drawn to scale.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>, <b>1</b>A-<b>2</b>, <b>1</b>B-<b>1</b>, <b>1</b>B-<b>2</b>, <b>1</b>C-<b>1</b> and <b>1</b>C-<b>2</b></figref> are block diagrams of one embodiment of an architecture including a document analysis system.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow diagram of one embodiment of a method for the automated reuse of datasets or models in document analysis.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. <b>3</b>A, <b>3</b>B and <b>3</b>C</figref> are examples of interfaces that may be utilized by embodiments of a document analysis system.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIGS. <b>4</b>A, <b>4</b>B and <b>4</b>C</figref> are examples of interfaces that may be utilized by embodiments of a document analysis system.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0027" num="0026">The disclosure and various features and advantageous details thereof are explained more fully with reference to the exemplary, and therefore non-limiting, embodiments illustrated in the accompanying drawings and detailed in the following description. It should be understood, however, that the detailed description and specific examples, while indicating the preferred embodiments, are given by way of illustration only and not by way of limitation. Descriptions of known programming techniques, computer software, hardware, operating platforms and protocols may be omitted so as not to unnecessarily obscure the disclosure in detail. Various substitutions, modifications, additions and/or rearrangements within the spirit and/or scope of the underlying inventive concept will become apparent to those skilled in the art from this disclosure.</p><p id="p-0028" num="0027">Before describing embodiments in detail, it may be helpful to discuss some context around document analysis systems. As discussed herein, a number of different situations commonly arise that require an analysis or identification of certain relevant electronic documents from a relatively large pool of available electronic documents. These types of search problems crop up in a wide variety of contexts. For example, parties to litigation typically have to share relevant evidence with opposing counsel through the discovery process. In many cases, each party makes a reasonable search of their records based on some set of terms or keywords and produces the results of the search to the other party. Discovery thus typically involves the gathering of potentially relevant materials, much of it digital, and then reviewing such materials to determine what to be shared with opposite parties. Additionally, during the course of the litigation each party may continually review those documents produced by the opposing party to locate documents relevant to the case at hand.</p><p id="p-0029" num="0028">Litigation thus represents a microcosm of a more general problem raised by the high volume of electronic documents present in a variety of contexts. Namely, how can a large volume of electronic documents be understood, reviewed, or searched in order that documents relevant to a particular topic or user's interest may be located.</p><p id="p-0030" num="0029">To aid users in resolving these problems, a document analysis system may be provided in a given electronic context. A document analysis, document review, information retrieval, or search system (which all will be utilized here substantially interchangeably) is a computer system used to process a corpus of electronically stored information (referred to as the corpus) and allow users to analyze, review or navigate the information, or search the electronic information to return electronically stored information responsive to a search (also referred to as a query). Items of electronic information that form a corpus may be referred to interchangeably as (electronic) documents, items, files, objects, items, content, etc. and may include objects such as files of almost any type including documents for various editing applications, emails, workflows, etc.</p><p id="p-0031" num="0030">In the legal domain, as well as other domains, these document analysis systems may be required to review and analyze a large corpus of documents. In some of the instances it may be desired to review and code the documents of a corpus according to a list of classification criteria that may be arbitrarily nuanced or complex. One standard approach to this task is to engage a team of human reviewers to examine each document in the corpus in order to apply the correct codes (or labels) to those documents. In recent years, several systems have been designed and built to speed up this process with the use of machine learning.</p><p id="p-0032" num="0031">One method, known as predictive coding, requires human reviewers to code (also referred to as label) a set of documents with the coded documents forming a dataset for machine learning that is used to train a predictive model that suggests codes for the remaining documents, with each suggested code having some degree of confidence or strength that may be indicated to the reviewer as a predictive score for the code. These predictive scores may be used to assist in the coding of the remaining documents, for example, by sorting the documents into priority order or searching for those documents that are predicted highly likely for some code. As the set of coded documents grows, new models may be trained as needed to improve the predictive scores. By incorporating statistical methods to exclude documents from review once all documents likely to be coded have been found with high probability, predictive coding can save a substantial fraction of review cost, frequently between 30%-70% and sometimes in excess of 90% for large corpora.</p><p id="p-0033" num="0032">One challenge for predictive coding is the iterative nature of coding in which the human codes documents: the machine models the coded documents, and then the human codes more documents with the machine modeling the new documents ad infinitum. This iterative process must be restarted every time a new review begins. The many iterations in this method are wasteful of the human's time and effort, especially in cases where new corpora are substantially similar to previously coded corpora.</p><p id="p-0034" num="0033">It would instead be desirable to utilize the prior work product or generated machine-learning models to provide, or assist in providing, coding descriptions for the coding of new corpora of documents.</p><p id="p-0035" num="0034">To that end, among others, embodiments of document analysis systems and methods as disclosed herein may allow the reuse of coded datasets defined in association with a particular code (referred to as the boosting code) by allowing these coded datasets to be bundled to define a boosting dataset for another code, where that code (referred to as the boosted code) may be associated with a target (e.g., new or existing) corpus of documents (referred to also herein as the native corpus). A model can then be trained (referred to as the boosting training process) based on that boosting dataset (referred to herein as the boosting model) and used to provide predictive scores for the documents of the target corpora with respect to the boosted code. Furthermore, this boosted code can be applied not just to the target corpus of documents, but additionally can be applied against any other corpora, including corpora from which the boosting codes were originally obtained or associated with, or corpora that may be accessed at a future point.</p><p id="p-0036" num="0035">Accordingly, embodiments may utilize coded datasets available from other (or the same) corpora using a related set of codes. Thus, embodiments may allow a user to bundle datasets associated with different corpora or codes in order to form the aggregate boosting dataset. As such the previously coded datasets can be reused to provide an initial set of predictive scores for that boosted code for a target (e.g., new) corpus, even in the absence of any human coding of documents of the new corpus. Furthermore, the quality of this initial set of scores can be higher than the quality of predictive coding without such boosting (e.g., even though several iterations of predictive coding) due to the fact that the previously coded datasets may be much larger in scale or of higher quality.</p><p id="p-0037" num="0036">Moreover, as the documents of the target corpus are coded with respect to the boosted code, or other input is received with respect to the boosted code, a native dataset comprising documents of the target corpus that have been coded with respect to the boosted code may be formed. Embodiments may thus train a model for the boosted code using the native dataset (referred to as the native model) based on the native dataset alone (e.g., without use of the booting dataset). This training may be referred to as the native training process. To provide the most accurate predictive coding scores for the boosted code, the boosting model trained for the boosted code can be compared (e.g., using an A/B comparison or the like) against the native model for the boosted code trained in this native training process. This comparison may happen, for example, when predictive scores are generated for documents for the boosted code for presentation to a user. The model (e.g., the boosting model or the native model) that wins this comparison may be utilized to provide predictive scores for the documents of the target corpora with respect to the boosted code.</p><p id="p-0038" num="0037">In some embodiments, the native dataset may be added to the boosting data set and a model trained from this combined dataset. For clarity of reference, this combined dataset (e.g., the boosting dataset plus the native dataset) will be referred to as the hybrid dataset, the model trained on this hybrid dataset will be referred to as the hybrid model and the training process referred to as the hybrid training process. In this manner, the model for the boosted code is trained not only on the data of the corpora associated with each of the original boosting codes, but additionally may be trained on data associated with the target corpus. This hybrid model for the boosted code can similarly be compared against the boosting model (e.g., trained on the boosting dataset alone) generated for the boosted code (e.g., if one has been generated) or the native model for the boosted code (e.g., if one has been generated). The model (e.g., the boosting model, hybrid model or the native model) that wins this evaluation may be utilized to provide predictive scores for the documents of the target corpora with respect to the boosted code.</p><p id="p-0039" num="0038">To ensure that the most accurate model is being used for the boosted code (e.g., as applied to documents of the target corpus), in one embodiment only documents (e.g., a test set of documents) from the target corpus may be used to evaluate or compare (used interchangeably herein) the boosting model, hybrid model or the native model for the boosted code. Thus, each existing model for the boosted code may be evaluated (e.g., using an inference procedure or the like) and the best model according to this evaluation is selected to generate predictive coding scores for the documents of the target corpus such that, for example, the predictive coding scores produced by the model with the better evaluation are the scores presented to the user for the documents of the target corpus relative to the boosted code.</p><p id="p-0040" num="0039">In one embodiment, in order to maintain consistency, the models produced by each training process (e.g., boosting, hybrid or native) may be archived or otherwise stored. A training process may be initiated, for example, when a test set of document (e.g., of the target corpora) changes. During one or more of the training processes, when a current model (e.g., boosting model, hybrid model or native model) is trained this currently trained model may be evaluated against at least one previously trained respective model (e.g., the best previously trained model) to determine which of the respective models is the best for those models. For example, a currently trained boosting model may be evaluated against a previously trained boosting model to determine which of the two is the best boosting model, a currently trained hybrid model may be evaluated against a previously trained hybrid model to determine which of the two is the best hybrid model or a previously trained native model may be evaluated against a previously trained native model to determine which of the two is the best native model. This evaluation for the respective model may be done, for example, on the new test set of documents. Each of the best of the respective models (e.g., boosting, hybrid or native) may then be evaluated against one another on the new test set of documents as discussed to select the best of these models (e.g., the boosting model, hybrid model or the native model) to provide predictive scores for the documents of the target corpora with respect to the boosted code.</p><p id="p-0041" num="0040">As a model (e.g., a boosting or hybrid model) may have been trained on data that is no longer available (e.g., the corpus comprising the dataset on which it was initially developed may no longer available), the evaluation of these previously generated model may allow previously generated models to be utilized in the document analysis system to perform predictive coding. Thus, although the old data may no longer be available for training, a model trained on the old data is still available, and if this model has better performance on the new test set, it may be used to generate predictive scores for the boosted code. This aspect of embodiments may be quite advantageous, as it allows the retention of quality models in an environment of changing or segregated data.</p><p id="p-0042" num="0041">To illustrate in more detail, a tag may be thought of as a subset of documents of a corpora that should be categorized together and assigned a specific code, name or label (used herein interchangeably). In practical terms however, in the realm of document analysis systems, the idea of a tag is a theoretical concept, as there is little to no way of fully and definitively determining or delineating this subset of documents with respect to a corpora without some degree of human intervention. While document analysis systems may classify or score documents with respect to the code to approximate which documents belong to the associated tag, unless a reviewer manually codes each document, there is no way for such document analysis systems to definitively determine if documents that are not manually coded belong to that tag.</p><p id="p-0043" num="0042">Embodiments of document analysis systems may thus give a prediction (or score) reflective of a determination of likelihood or confidence that a document belongs to a tag (and thus should be assigned the code for that tag). To generate such a prediction, embodiments of a document analysis may utilize a tag definition to allow a model for determining such predictions to be generated. This tag definition includes the code for the tag (e.g., the code to be assigned to documents that are part of that tag) and a target corpus. The tag definition may also include a dataset for the tag that includes positive signals for the tag and negative signals of the tag. The positive signals include a document (or reference thereto) and an annotation indicating that the document belongs to the tag (e.g., as indicated by the document being manually coded with the code by a human user). The negative signals include a document of (or reference thereto) and an annotation indicating that the document does not belong to the tag (e.g., as indicated by the document being manually coded as not being assigned the code by a human user). The dataset for a tag definition may thus be referred to as a coded dataset for the tag (or tag definition) as it comprises documents that have been assigned the code (and thus designated as belonging to the tag) or indicated as not being assigned the code (and thus designated as not belonging to the tag).</p><p id="p-0044" num="0043">Based on that tag definition, then, embodiments may train a model (e.g., a machine learning model) based on the positive and negative signals of the coded dataset of the tag definition and used the trained model to generate predictive scores for the documents of the target corpora with respect to the code of that tag definition. In other words, a predictive score may indicate a likelihood that an associated document may belong to the tag (and should thus be labeled with the associated code).</p><p id="p-0045" num="0044">Embodiments of document analysis systems and methods as disclosed herein may thus allow the reuse of a coded dataset for a tag definition associated with a particular code (the boosting code) and corpus by allowing the coded datasets of that boosting code to define a (boosting) dataset for another code, where that code (referred to as the boosted code) may be associated with a target corpus of documents. In particular, a user may define a code (e.g., label) in association with a particular target corpus using an interface. The definition of the code by the user may create an associated tag definition at the document analysis system with an identification of the target corpus of documents and the code. In some embodiments, the user may also use the interface to add a text annotation or other description (e.g., a natural language description or textual blurb) for the code being defined that may explain the code, its relevance or history, or any other information the user wishes to explicate regarding the code. This text description may be stored in association with the tag definition for the code.</p><p id="p-0046" num="0045">At some point (e.g., when the code is defined, or at a later point) a user (the user who defined the code or another user) can then boost this code (the boosted code) by associating another code (the boosting code) with the boosted code using an interface of the document analysis system. Specifically, in certain embodiments, an interface may be provided by the document analysis system that allows a user to interact with (e.g., search for, review, sort, etc.) the codes defined in the document analysis system. This interface may allow users to select codes to use for boosting, including codes defined for different corpora. This ability is quite advantageous for users of such document analysis systems in many context. For example, typically in a litigation context, codes defined for one corpus of document (e.g., a particular matter) and the associated datasets will not be available for reuse as matters must be kept private and confidential. Moreover, in these settings there is usually no user who has the knowledge of, or access to, codes across these different corpora.</p><p id="p-0047" num="0046">By providing a document analysis system where datasets associated with codes across these distinct corpora may be bundled without allowing cross-pollination of the documents of those corpora (or access to these documents), these document analysis systems provide a mechanism where the knowledge and insight gained with respect to a code associated with one corpus may be leveraged to improve the predictive coding associated with another code in a completely different matter. Additionally, by providing the aforementioned textual description in association with codes, along with an interface to review and select such codes for boosting across corpora document analysis systems may open the door for provisioning of new roles within enterprises (e.g., law firms, accounting firms, etc.) that handle distinct corpora of documents. These roles may include, for example, a code or document analysis curator role that may be responsible for managing or evaluating the codes utilized by the enterprise across the various corpora being analyzed by the document analysis system, including the definition of codes and the selection of which codes should be used to boost other codes.</p><p id="p-0048" num="0047">The interfaces of embodiments may thus provide such a curator, or other users of the document analysis system, with highly effective interfaces for accomplishing these types of tasks, among others, by providing the ability to interact with (e.g., search for, review, sort, etc.) the codes defined in the document analysis system, where the textual descriptions of the codes may be presented in association with the various code if a textual description exists. These users can thus use such interfaces to select a code to be boosted (the boosted code) and the codes to use for boosting, including codes defined for different corpora, based on the textual description of the various codes, the knowledge of the curator, or some other criteria (e.g., a user who initially defined the code, the corpus associated with a code, etc.).</p><p id="p-0049" num="0048">In any event, once a code (e.g., the boosting code) is selected to boost a particular code (e.g., the boosted code), the boosted code is then &#x201c;boosted&#x201d; by the document analysis system by bundling the dataset associated with boosting code with the dataset associated with the boosted code. Specifically, in one embodiment, the dataset of the tag definition associated with the boosted code is updated to include the dataset of the tag definition associated with the boosted code. Thus, the positive signals and the negative signals of the dataset associated with the boosting code may be included in the set of positive and negative signals included in the tag definition associated with the boosted code. The union of these datasets is thus the boosting dataset associated with the boosted code. As the boosting code may be associated with a different corpus than the target corpus associated with the boosted code, by boosting the boosted code with the boosting code, the positive and negative signals associated with documents of this different corpus (associated with the boosting code) may be included in the boosting dataset of the boosted code. In one embodiment, when creating the boosting dataset, any negative signals that are also in the set of positive signals may be removed from the set of negative signals of the dataset.</p><p id="p-0050" num="0049">Thus, as the boosting dataset may be used to train one or more models that may be used to generate predictive codes for the target codes, the bundling of datasets associated with different codes and corpora for use in training models for use on a different target corpus may provide an extremely powerful mechanism for generating high quality predictive scores for that boosted code for the target corpus, allowing &#x201c;knowledge&#x201d; gained from other corpus of document to be bundled together and applied to completely new set of documents even in the absence of any human coding of documents of the target corpus. In this manner, these composite or boosted codes serve as classifiers that exist above the corpus level and can be re-purposed or reused across corpora of documents in a document analysis system. Moreover, the power of such bundling may be more fully realized when it is understood that that boosting code may itself be a boosted code, such that the dataset of a boosting code may itself include positive and negative signals associated with multiple (e.g., boosting) codes and multiple corpora. In this manner, by boosting a code with another code that is itself a boosted code, large and informative dataset may be aggregated and used to generate highly effective predictive models. Furthermore, a boosted code can be applied not just to the target corpus of documents, but additionally can be applied against any other corpora, including corpora from which boosting codes were originally associated with, or corpora that may be accessed at a future point.</p><p id="p-0051" num="0050">Looking now at <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, a block diagram of one embodiment of a document analysis system that allows the use of boosted codes for predictive coding is depicted. The document analysis system <b>101</b> is part of computing environment <b>100</b> including one or more repositories <b>105</b>, document analysis system <b>101</b>, and one or more client computers <b>130</b>. Repository <b>105</b> may comprise a file server or database system or other storage mechanism remotely or locally accessible by document analysis system <b>101</b> which, according to an embodiment, may be almost any SQL or NoSQL platform such as MongoDB, Elasticsearch or the like. Repository <b>105</b> may store documents <b>107</b> that document analysis system <b>101</b> may allow users accessing the document analysis system <b>101</b> to review (e.g., navigate, search, code, etc.). The documents <b>107</b> of the repository <b>105</b> may comprise one or more corpora <b>109</b>. Thus, documents <b>107</b><i>a </i>may comprise one corpus <b>109</b><i>a</i>, documents <b>107</b><i>b </i>may comprise another corpus <b>109</b><i>b</i>, documents <b>107</b><i>n </i>another corpus <b>109</b><i>n</i>, etc. The corpora may correspond to, for example, different matters in a litigation context, different divisions or corporate entities in a business instance, or some other division or documents <b>107</b>.</p><p id="p-0052" num="0051">In the depicted embodiment document analysis system <b>101</b> may include one or more (virtual or physical) servers or other type of computing device utilizing a central processing unit <b>112</b> connected to a memory and a data store <b>118</b> (e.g., via a bus). Central processing unit <b>112</b> may represent a single processor, multiple processors, a processor(s) with multiple processing cores and the like. Data store <b>118</b> may include a volatile or non-volatile non-transitory storage medium such as RAM, hard disk drives, flash memory devices, optical media or the like. Document analysis system <b>101</b> may be connected to a data communications network such as the Internet, a local area network (LAN), a wide area network (WAN), a cellular network or some other network or combination of networks.</p><p id="p-0053" num="0052">Data store <b>118</b> stores computer executable instructions <b>119</b>. Computer executable instructions <b>119</b> can represent one or more multiple programs or operating system instructions. In one embodiment, instructions <b>119</b> are executable to provide document analysis application <b>122</b>. Predictive coder application <b>122</b> may be implemented on the same computing systems or can be distributed across multiple computing systems, platforms or physical or virtual server. Again, it will be noted here that while embodiments described and depicted herein may include a deployment of a document analysis system on a physical computing device other embodiments may include the document analysis system deployed as a service on, for example, a cloud computing environment or otherwise deployed without loss of generality.</p><p id="p-0054" num="0053">Client computer systems <b>130</b> may include components similar to those of the server of document analysis system <b>101</b>, such as CPU <b>138</b> and data store <b>140</b>. Additionally, client computer system <b>130</b> may include executable instructions <b>132</b> to provide user interface <b>134</b> that allows a user to interact with document analysis system <b>101</b> to review the documents <b>107</b> of the corpus <b>109</b>. These instructions <b>132</b> may have, for example, been provided by document analysis system <b>101</b> in response to an access by client computer <b>130</b>. For example, user interface <b>134</b> may be provided through a web browser, file system interface or other method without loss of generality. Moreover, user interface <b>134</b> may interact (e.g., issue requests, receive responses, etc.) with a corresponding interface <b>136</b> of the predictive coder <b>122</b>, which may be, for example a web services interface, an Application Programming Interface (API) or another type of interface.</p><p id="p-0055" num="0054">Again, those skilled in the art will appreciate that document analysis system <b>101</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is merely an example of a computing system and embodiments of a document analysis system that may be implemented using other computing systems (e.g., desktop computers, laptops, mobile computing devices, services platforms, cloud computing platforms or other computing devices or platforms with adequate processing and memory) including multiple computers acting together to provide a document analysis system (e.g., a cluster of servers or other computing devices connected by a network acting together to provide the document analysis system). Similarly, client computer <b>130</b> may include any suitable desktop computer, laptop, mobile device, server or other computing system.</p><p id="p-0056" num="0055">The document analysis system <b>101</b> may provide predictive coding capabilities to users reviewing the documents <b>107</b> of a corpus <b>109</b> through predictive coder <b>122</b> (e.g., through interface <b>134</b>). Using an interface (e.g., interface <b>134</b>) a user at a client device <b>130</b> may define a code in association with an associated (target) corpus <b>107</b><i>a</i>, <b>107</b><i>b</i>, <b>107</b><i>n</i>. When a user defines such a code for a corpus <b>109</b>, there is a subset of documents of this target corpus that should be categorized together and assigned this code. The subset of documents <b>107</b> of the target corpus can be referred to as a tag.</p><p id="p-0057" num="0056">Predictive coder <b>122</b> may thus be adapted to generate a score reflective of a determination of a likelihood or confidence that a document of the target corpus <b>109</b> belongs to a tag (and thus should be assigned the code). To generate such a prediction, embodiments of a document analysis may utilize a tag definition to allow a model for determining such predictions to be generated. Accordingly, when a user defines such a code, a tag definition <b>111</b> for that code may be created by the predictive coder <b>122</b> in association with the target corpus <b>109</b>. The tag definition <b>111</b> may include a corpus identifier <b>151</b> identifying the target corpus and the defined code (or label) <b>153</b> for the tag (e.g., the code assigned to documents that are part of that tag). The tag definition <b>111</b> may also include a dataset <b>155</b> that includes positive signals <b>157</b> for the tag and negative signals <b>159</b> for the tag. The dataset may also include a dataset identifier <b>165</b> that allows the dataset (e.g., set of positive and negative signals <b>157</b>, <b>159</b> to be identified as a group. The positive signals <b>157</b> include a document <b>107</b> (or reference thereto) and an annotation <b>161</b> indicating that the associated document <b>107</b> belongs to the tag (e.g., as indicated by the document <b>107</b> being manually coded with the code <b>153</b> by a human user). The negative signals <b>159</b> also include a document <b>107</b> of the (or reference thereto) and an annotation <b>163</b> indicating that the associated document <b>107</b> does not belong to the tag (e.g., as indicated by the document being manually coded as not being assigned the code <b>153</b> by a human user).</p><p id="p-0058" num="0057">To illustrate, tag definition <b>111</b><i>a </i>may be defined for code <b>153</b><i>a </i>and may be associated with corpus <b>109</b><i>a </i>though target corpus identifier <b>151</b><i>a</i>. The tag definition <b>111</b><i>a </i>may also include dataset <b>155</b><i>a </i>identified by dataset identifier <b>165</b><i>a </i>and having positive signals <b>157</b><i>a </i>and negative signals <b>159</b><i>a </i>associated with documents <b>107</b><i>a </i>of the target corpus <b>109</b><i>a</i>. Each positive signal <b>157</b><i>a </i>may include an associated document <b>107</b><i>a </i>(or a reference to such a document <b>107</b><i>a</i>) and a (positive) annotation <b>161</b> that the associated document belongs to the defined tag (e.g., has been associated with the code <b>153</b><i>a</i>, such as by a human code or otherwise). Conversely, each negative signal <b>159</b><i>a </i>may include an associated document <b>107</b><i>a </i>(or a reference to such a document <b>107</b><i>a</i>) and a (negative) annotation <b>163</b> that the associated document <b>107</b><i>a </i>does not belong to the defined tag (e.g., has been indicated as not being associated with the code <b>153</b><i>a</i>, such as by a human code or otherwise).</p><p id="p-0059" num="0058">ML model builder <b>124</b> may generate or train an ML model <b>103</b> for to generate predictive scores for each tag definition <b>111</b>. Specifically, for each tag definition <b>111</b> the ML model builder <b>124</b> may train an associated ML model <b>103</b> based on the dataset <b>155</b> associated with that tag definition <b>111</b> (e.g., the positive signals <b>157</b> and negative signals <b>159</b> of the dataset <b>155</b>). For example, ML model builder <b>124</b> may train ML model <b>103</b><i>a </i>for tag definition <b>111</b><i>a </i>based on positive signals <b>157</b><i>a </i>and negative signals <b>159</b><i>a </i>of dataset <b>155</b><i>a</i>. The training of such ML models <b>103</b> may, for example, occur at different intervals based on criteria that may be associated with the corpus <b>109</b> with which the tag definition <b>111</b> is associated (such as newly received positive or negative signals being added to a dataset <b>155</b> of the tag definition <b>111</b> (e.g., a human has coded additional document), timing criteria (e.g., every day, weekly, etc.), or some other criteria. The training of such ML models is described in U.S. patent application Ser. No. 16/167,205 entitled &#x201c;Methods and Apparatus for Asynchronous and Interactive Machine Learning Using Attention Selection Techniques&#x201d; by Lockett incorporated herein by reference in its entirety for all purposes.</p><p id="p-0060" num="0059">Thus, tag definition <b>111</b> can be associated with a corpus <b>109</b> and ML model <b>103</b> trained on the dataset of the tag definition <b>111</b> to generate predictive scores for the documents <b>107</b> of the associated target corpus <b>109</b>, with respect to the code <b>153</b> of that tag definition <b>111</b>. In other words, a generated predictive score may indicate a likelihood that an associated document <b>107</b> may belong to the tag (and should thus be assigned the associated code <b>153</b>). The predictive scores for the tag definitions <b>111</b> for the documents <b>107</b> of the associated corpus <b>109</b> can be presented to the user through the interface (e.g., interface <b>134</b>) and feedback (e.g., coding decisions, including additional positive or negative signals) may also be received through interface <b>134</b> and used to, for example, update the datasets <b>155</b> associated with each tag definition <b>111</b>.</p><p id="p-0061" num="0060">As discussed, embodiments of document analysis system <b>101</b> may also allow the reuse of the coded dataset for a tag definition <b>111</b> associated with a particular code <b>153</b> (the boosting code) and corpus <b>109</b> by allowing the coded datasets <b>155</b> of that boosting code to define a (boosting) dataset <b>155</b> for another code <b>153</b>, where that code <b>153</b> (referred to as the boosted code) may be associated with a target corpus <b>109</b> of documents <b>107</b>.</p><p id="p-0062" num="0061">Moving on to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> (where clients <b>130</b> have been removed from the diagram for ease of depiction), predictive coder <b>122</b> may also provide predictive coding functionality by which users can define a reusable code associated with one or more corpora <b>109</b>. In particular, a user may define a code (e.g., label) in association with a particular target corpus <b>109</b> using an interface <b>136</b>. The definition of the code by the user may create an associated tag definition <b>111</b> at the document analysis system with an identification of the target corpus <b>109</b> of documents <b>107</b> and the defined code.</p><p id="p-0063" num="0062">Specifically, the interface <b>136</b> that may be provided to a client <b>130</b> may allow a user to may designate a code and &#x201c;boost&#x201d; this code with one or more previously generated codes (e.g., which may themselves be boosted codes). The interface <b>136</b> may also allow a user to select one or more (e.g. native) corpora <b>109</b> to which the code should be applied (e.g., where predictive scores for the code should be generated for the documents <b>107</b> of that corpus), including a new corpus <b>109</b><i>x </i>of documents.</p><p id="p-0064" num="0063">In some embodiments, the interface <b>136</b> offered by the predictive coder <b>122</b> may be different, or have different functionality, based on a type of the user (e.g., based on a user's access rights or credentials). For example, types of users may include a curator (e.g., a curator persona) or a reviewer (e.g., a reviewer persona). For example, in the legal context, at present most legal matters are managed separately, and there is no one person who understands which codes can be bundled. As part of embodiments, a curator interface may allow a curator to supervise and manage the bundling of codes (e.g., the creation of boosted codes and the application of those tag bundles across corpora).</p><p id="p-0065" num="0064">For example, certain embodiments of the interface may allow a user, such as a curator or a creator of a code, to add a text annotation or other description (e.g., a natural language description or textual blurb) for the code being defined that may explain the code, its relevance or history, or any other information the user wishes to explicate regarding the code. This text description may be stored in association with the tag definition for the code. In many systems at present, user codes are named or labeled in a manner that is opaque to the digital systems in which those codes are applied. Review teams typically write down a set of rules or specification for how codes are to be applied, but these rules are stored separately and cannot be accessed and used by machine code. To remedy this situation, embodiments of interfaces utilized herein may allow users to enter in a freeform text description of a code being created in the interface or to upload the review instructions for use by machine learning or other programmatic methods. These descriptions or instructions provided by the user through the interface <b>136</b> may be used to assist curators in deciding which codes can be bundled (e.g., to form a new dataset).</p><p id="p-0066" num="0065">Additionally, in one embodiment, an interface for a curator persona (e.g., a user who is a curator) may provide a list of available codes, their general statistics including the number of positive and negative signals (e.g., from a particular corpus <b>109</b>) for the code, the textual descriptions for each code, if available, or an accuracy metric that can be used to judge the quality of the code and its relevance. The curator can then create new codes (e.g., boosted codes) from one or more extant codes and manage existing boosted codes by adding or removing codes (e.g. boosting codes) from this boosted code. The curator can also be provided with alerts when codes are selected to boost a code or when the document review system detects a potential match for a boosted code. These features make it easy to reuse existing datasets by selecting appropriate codes for boosting. Moreover, when a dataset (or portion thereof, or corpus) associated with a code that is used to boost another code is removed from the system, the curator may be notified that this alteration has occurred (e.g., through a user interface or via electronic mail, etc.). The curator then has the option to add new (e.g., boosting) codes to the boosted code or to review metrics and statistics generated based on the new dataset for the code.</p><p id="p-0067" num="0066">A reviewer interface may also be provided by interface <b>136</b> for a reviewer or review manager. These review managers may be people who (e.g., during legal discovery or the like) may be responsible for organizing and supervising the review process whose outcome is the application of a set of codes to a corpus. The review manager may make or implement decisions about whether and how to utilize machine predictions during the review process.</p><p id="p-0068" num="0067">Accordingly, embodiments of interfaces <b>136</b> of predictive coder <b>122</b> may offer a code management panel on which a review manager (or other user) can view and edit the codes for the review process. In particular, the review manager can add or modify the description of the code and can determine whether or not machine learning will be applied to a particular code. During the process of this latter decision, the review manager (or a curator) may be provided the opportunity to &#x201c;boost&#x201d; a code based with another currently existing code. The review manager is shown a list of available codes together with, for example, their names, descriptions, signal statistics and current test accuracy metrics. This interface may also indicate which of these codes are themselves boosted codes and the composition of the datasets of such codes (e.g., the corpora with which they are associated). This list can be searched, sorted, or filtered in order to help the review manager find the most relevant code or codes. When the review manager selects a code to use for boosting a particular code, that boosting code is associated with the boosted code such that the dataset of the boosting code is bundled with the dataset of the boosted code, as described. A curator can be notified by a user interface or by electronic mail of the decision.</p><p id="p-0069" num="0068">In one embodiment, when a reviewer or other user selects a boosting code for a boosted code, that dataset of that boosting code does not automatically become part of the dataset of the boosted code. A curator is, however, notified of the boosting decision and has the opportunity to include the selected code as a boosting code, in which case the dataset for the selected boosting code becomes part of the dataset of the boosted code as will be described.</p><p id="p-0070" num="0069">As discussed then, using the interface <b>136</b> of the predictive coder <b>122</b> a user at a client device <b>130</b> may create a code and associate the code with a corpus (e.g., a new corpus <b>109</b><i>x</i>). The definition of the code by the user may create an associated tag definition <b>111</b> at the document analysis system <b>101</b> with an identification of the target corpus <b>109</b> of documents <b>107</b> and the code <b>153</b>. At some point (e.g., when the code is defined, or at a later point) a user (the user who defined the code or another user) can then boost this code (the boosted code) by associating another code (the boosting code) with the boosted code using the interface <b>136</b> of the document analysis system <b>101</b>.</p><p id="p-0071" num="0070">Once a code is selected to boost the defined code, this boosted code is then &#x201c;boosted&#x201d; by the document analysis system <b>101</b> by bundling the dataset <b>155</b> associated with boosting code with the dataset <b>155</b> associated with the boosted code. Specifically, in one embodiment, the dataset <b>155</b> of the tag definition <b>111</b> associated with the boosted code <b>153</b> is updated to include the dataset <b>155</b> of the tag definition <b>111</b> associated with the boosted code <b>153</b>. Thus, the positive signals <b>157</b> and the negative signals <b>159</b> of the dataset <b>155</b> associated with the boosting code <b>153</b> may be included in the set of positive and negative signals <b>157</b>, <b>159</b> included in the tag definition <b>111</b> associated with the boosted code <b>153</b>.</p><p id="p-0072" num="0071">The union of these datasets <b>155</b> is thus the boosting dataset <b>155</b> associated with the boosted code <b>153</b>. As the boosting code <b>153</b> may be associated with a different corpus <b>109</b> than the target corpus <b>109</b> associated with the boosted code <b>153</b>, by boosting the boosted code <b>153</b> with the boosting code <b>153</b>, the positive and negative signals <b>157</b>, <b>159</b> associated with documents <b>107</b> of this different corpus <b>109</b> (associated with the boosting code) may be included in the boosting dataset <b>155</b> of the boosted code <b>153</b>. It will be understood that the dataset <b>155</b> of a boosting code may not be static (e.g., documents associated with one or more corpora <b>109</b> associated with boosting code may be predictively coded at a future point). As such the dataset <b>155</b> of the booted code can similarly be updated in the same manner as the dataset <b>155</b> of the boosting code is updated (e.g., as documents of these different corpora <b>109</b> are coded according to the boosting code). In one embodiment, when creating the boosting dataset, any negative signals that are also in the set of positive signals may be removed from the set of negative signals of the dataset.</p><p id="p-0073" num="0072">In the example depicted, a user has defined a code <b>153</b><i>x </i>in association with corpus <b>109</b><i>x</i>, causing document analysis system <b>101</b> to create tag definition <b>111</b><i>x </i>in association with corpus <b>109</b><i>x </i>(e.g., having a target corpus identifier <b>151</b><i>x </i>identifying target corpus <b>109</b><i>x</i>) and having the code <b>153</b><i>x</i>. The user has also selected code <b>153</b><i>a </i>associated with corpus <b>107</b><i>a </i>and code <b>153</b><i>b </i>associated with corpus <b>107</b><i>b </i>to be used as boosting code for the defined (boosted) code <b>153</b><i>x</i>. Based on the selection of codes <b>153</b><i>a </i>and <b>153</b><i>b </i>as boosting codes <b>153</b> for code <b>153</b><i>x</i>, the dataset <b>155</b><i>x </i>of code <b>153</b><i>x </i>is boosted with the dataset <b>155</b><i>a </i>associated with the tag definition <b>111</b><i>a </i>associated with code <b>153</b><i>a </i>and the dataset <b>155</b><i>b </i>associated with the tag definition <b>111</b><i>b </i>associated with code <b>153</b><i>b. </i></p><p id="p-0074" num="0073">More specifically, when code <b>153</b><i>x </i>is boosted with boosting code <b>153</b><i>a</i>, the predictive coder <b>122</b> may access tag definition <b>111</b><i>a </i>associated with the boosting code <b>153</b><i>a </i>to access the positive signals <b>157</b><i>a </i>and negative signals <b>159</b><i>a </i>of the dataset <b>155</b><i>a </i>associated with the boosting code <b>153</b><i>a</i>. These positive signals <b>157</b><i>a </i>and negative signals <b>159</b><i>a </i>of the dataset <b>155</b><i>a </i>associated with the boosting code <b>153</b><i>a </i>are then added to the dataset <b>155</b><i>x </i>of the boosted code <b>153</b><i>x </i>to update the dataset <b>155</b><i>x </i>of the boosted code <b>153</b><i>x</i>. Similarly, when code <b>153</b><i>x </i>is boosted with boosting code <b>153</b><i>b</i>, the predictive coder <b>122</b> may access tag definition <b>111</b><i>b </i>associated with the boosting code <b>153</b><i>b </i>to access the positive signals <b>157</b><i>b </i>and negative signals <b>159</b><i>b </i>of the dataset <b>155</b><i>b </i>associated with the boosting code <b>153</b><i>b</i>. These positive signals <b>157</b><i>b </i>and negative signals <b>159</b><i>b </i>of the dataset <b>155</b><i>b </i>associated with the boosting code <b>153</b><i>b </i>are then added to the dataset <b>155</b><i>x </i>of the boosted code <b>153</b><i>x </i>to update the dataset <b>155</b><i>x </i>of the boosted code <b>153</b><i>x</i>. These positive signals <b>157</b><i>a</i>, <b>157</b><i>b </i>and negative signals <b>159</b><i>a</i>, <b>159</b><i>b </i>added from boosting codes <b>153</b><i>a</i>, <b>153</b><i>b </i>to the dataset <b>155</b><i>x </i>of the boosted code <b>153</b><i>x </i>thus form a boosting dataset for the boosted code <b>153</b><i>x</i>. Moreover, in one embodiment, as the datasets <b>155</b><i>a</i>, <b>155</b><i>b </i>of the boosting codes <b>153</b><i>a</i>, <b>153</b><i>b </i>may not be static (e.g., documents associated with corpora <b>109</b><i>a</i>, <b>109</b><i>b </i>associated with boosting codes <b>153</b><i>a</i>, <b>153</b><i>b </i>may be predictively coded at a future point), the dataset <b>155</b><i>x </i>of the booted code <b>153</b><i>x </i>can similarly be updated in the same manner as the datasets <b>155</b><i>a</i>, <b>155</b><i>b </i>of the boosting codes <b>153</b><i>a</i>, <b>153</b><i>b </i>are updated (e.g., as documents of these different corpora <b>109</b><i>a</i>, <b>109</b><i>b </i>are coded according to the respective boosting code <b>153</b><i>a</i>, <b>153</b><i>b</i>). In one embodiment, any negative signals <b>159</b><i>a</i>, <b>159</b><i>b </i>that are also in the set of positive signals <b>157</b><i>a</i>, <b>157</b><i>b </i>of the dataset <b>155</b><i>x </i>may be removed from the set of negative signals <b>159</b><i>a</i>, <b>159</b><i>b </i>of the dataset <b>155</b><i>x. </i></p><p id="p-0075" num="0074">At some point then, ML model builder <b>124</b> may train boosting ML model <b>103</b><i>x</i><b>1</b> based on the dataset <b>155</b><i>x </i>of the tag definition <b>111</b><i>x </i>for the code <b>153</b><i>x</i>. Note that in this example, the dataset <b>155</b><i>x </i>of the tag definition <b>111</b><i>x </i>comprises only a boosting dataset that includes signals <b>157</b>, <b>159</b> (e.g., <b>157</b><i>a</i>, <b>157</b><i>b</i>, <b>159</b><i>a</i>, <b>159</b><i>b</i>) obtained from boosting codes <b>153</b> (e.g., <b>153</b><i>a</i>, <b>153</b><i>b</i>). Thus, in this example, the boosting model <b>103</b><i>x</i><b>1</b> for code <b>153</b><i>x </i>associated with corpus <b>109</b><i>x </i>is generated based on signals <b>157</b><i>a</i>, <b>157</b><i>b</i>, <b>159</b><i>a</i>, <b>159</b><i>b </i>from boosting tags <b>153</b><i>a</i>, <b>153</b><i>b </i>associated with documents <b>107</b><i>a </i><b>107</b><i>b </i>of corpora <b>109</b><i>a </i>and <b>109</b><i>b</i>. Again, this boosting ML model <b>103</b><i>x</i><b>1</b> may be generated in a similar manner to that described in U.S. patent application Ser. No. 16/167,205 entitled &#x201c;Methods and Apparatus for Asynchronous and Interactive Machine Learning Using Attention Selection Techniques&#x201d; by Lockett, however, it will also be noted generally that in various embodiments ML models may be generated by different methodology, or that ML models generated from different datasets (or the same datasets) may be generated by differing methodologies, without loss of generality,</p><p id="p-0076" num="0075">In one embodiment, in order to ensure that a boosting ML model is not overly influenced (e.g., overweighed) by a particular boosting code <b>153</b> (e.g., a dataset <b>155</b> associated with that boosting code <b>153</b>), the ML model builder <b>124</b> may sample positive signals <b>157</b> and negative signals <b>159</b> from each dataset <b>155</b> associated with each boosting code according to a balancing methodology. This balancing methodology may include for example, a logarithmic differential balancing method, a round robin selection process whereby a positive and negative signal is selected from each of the composite dataset <b>155</b> of the boosting dataset <b>155</b>, or by some other balancing method. For instance, in the example depicted, when training boosting ML model <b>103</b><i>x</i><b>1</b> the ML model builder <b>124</b> may sample (e.g., randomly) one or more positive signals <b>157</b><i>a </i>and one or more negative signals <b>159</b><i>a </i>from dataset <b>155</b><i>a </i>associated with boosting code <b>153</b><i>a</i>, followed by selecting (e.g., randomly) one or more positive signals <b>157</b><i>b </i>and one or more negative signals <b>159</b><i>b </i>from dataset <b>155</b><i>b </i>associated with boosting code <b>153</b><i>b</i>, until a sufficient amount of training examples are obtained.</p><p id="p-0077" num="0076">Specifically, according to certain embodiments, to generate a ML model <b>103</b> a neural networks model may be trained with minibatch gradient descent. The formation of the minibatches for the training may be done in a manner that ensures that the model <b>103</b> is not overly driven by a single code whose datasets <b>155</b> comprise the boosting dataset <b>155</b> of the code <b>153</b> but balances information across the corpora <b>109</b> of documents <b>109</b> associated with the bundled codes. In certain embodiments therefore, minibatches are formed two examples at a time by first randomly sampling data (e.g., documents <b>107</b>) from the corpus <b>109</b> associated with a code <b>153</b> and then randomly sampling one positive signal <b>157</b> and one negative signal <b>159</b> of the dataset <b>155</b> for the code <b>153</b>. The result is that minibatches are balanced having equal representation from each code <b>153</b> included in the boosting dataset <b>155</b> and equal representation between positive and negative signals <b>157</b>, <b>159</b>, on average.</p><p id="p-0078" num="0077">In certain embodiments, the formation of minibatches may account for the number of documents <b>109</b> in the corpus associated with a particular code <b>153</b>, in which corpora <b>109</b> with more data have a greater opportunity to provide examples to a minibatch. For example, a probability vector may be generated wherein each element of the probability vector is proportional to the logarithm of the number of signals <b>157</b>, <b>159</b> in the dataset <b>155</b> corresponding to each code <b>153</b> comprising the boosting dataset <b>155</b> and then this probability vector can be used to choose a constituent code <b>153</b> such that the probability of choosing each a constituent code <b>153</b> (and signal <b>157</b>, <b>159</b> of the dataset <b>155</b> associated with that constituent code <b>153</b>) is logarithmically higher for codes based on larger datasets <b>155</b>.</p><p id="p-0079" num="0078">In certain embodiments, ML models <b>103</b> are trained to generate predictive scores for codes <b>153</b> by training a parameterized model on the combined dataset <b>155</b>. In embodiments of machine learning, signals <b>157</b>, <b>159</b> may be separated into three sets: (1) a training set used to set the parameters of a parameterized model through a training process; (2) a validation set used to manage the training process, including most commonly to decide when to exit the process; and (3) a test set used to measure the accuracy or quality of a model generated by the training process. The formation of these sets is referred to herein as signal set allocation.</p><p id="p-0080" num="0079">This process is complicated by the fact that correlations exist among the various codes <b>153</b> applied to a particular corpus <b>109</b>, and it may be desired that this set formation process guarantee that each tag (e.g., each boosting code <b>153</b> and the corpus <b>109</b> associated with the boosted code <b>153</b>) is well represented within each of the training, validation, and test sets. In particular, the test set should contain substantially many examples to generate statistically valid accuracy metrics, and the training set should include enough examples to train a model with good generalization performance. In one embodiment, the algorithm utilized may be similar to that described in Sechidis, Tsoumakas, and Vlahavas, &#x201c;On the Stratification of Multi-Label Data&#x201d; (D. Gunopulos et al. (Eds.): ECML PKDD 2011, Part III, LNAI 6913, pp. 145-158, 2011; Springer-Verlag: Berlin, Heidelberg) hereby incorporated by reference in its entirety.</p><p id="p-0081" num="0080">In one embodiment, each dataset <b>155</b> is associated with a signal set allocation that is lifted from the datasets <b>155</b> of the constituent codes <b>153</b>. Thus for a given dataset <b>155</b>, its training set is the aggregation of the training set for each code <b>153</b>, and similarly for the validation set and testing set. The accuracy of the data sets is measured after each training process based on the test set for the dataset <b>155</b>. Thus the meaning of the accuracy test changes as new datasets <b>155</b> for new boosting codes <b>153</b> are added to the dataset <b>155</b> of the boosted code <b>153</b> and old boosting codes <b>153</b> and their datasets <b>155</b> are removed from the dataset <b>155</b> of the boosted code <b>153</b>.</p><p id="p-0082" num="0081">Once boosting ML model <b>103</b><i>x</i><b>1</b> is trained based on the dataset <b>155</b><i>x </i>of the tag definition <b>111</b><i>x </i>for the code <b>153</b><i>x</i>, it can then be used to predictively code (e.g., score in association with the code <b>153</b><i>x</i>) documents <b>107</b><i>x </i>from the corpus <b>109</b><i>x </i>(e.g., document from a new corpus <b>107</b><i>x</i>) and such predictive scores may be returned to a user. In this manner, an ML model <b>103</b><i>x</i><b>1</b> may be trained using a dataset <b>155</b><i>x </i>comprising signals associated with different corpora <b>109</b><i>a</i>, <b>109</b><i>b </i>and applied to a new corpus <b>109</b><i>x </i>to predictive coding decisions (e.g., scores for the boosted code <b>153</b><i>x</i>) on the documents <b>107</b><i>x </i>of the new corpus <b>109</b><i>x </i>substantially immediately, even sometimes in the absence of any human input or coding input with respect to documents of the new corpus <b>109</b><i>x</i>. This allows &#x201c;knowledge&#x201d; gained from other corpora <b>109</b><i>a</i>, <b>109</b><i>b </i>of documents to be bundled together and applied to a completely new set of documents <b>109</b><i>x </i>through ML model <b>103</b><i>x</i><b>1</b>, even in the absence of any coding or human input with respect to those new documents. In this manner, these boosted codes <b>153</b><i>x </i>serve as classifiers that exist above the corpus level and can be re-purposed or reused across corpora <b>109</b> of documents <b>107</b> in a document analysis system <b>101</b>.</p><p id="p-0083" num="0082">Specifically, when a code <b>153</b> (e.g., boosted code <b>153</b><i>x</i>) is created and associated with a target corpus <b>109</b>, there is no associated training dataset of that corpus <b>109</b> associated with that tag <b>153</b>, and hence no native ML model for the tag <b>153</b>. In other words, no coding decisions or decisions have been received in association with that code <b>153</b> for documents <b>107</b> of the target corpus <b>109</b> and thus no positive or negative signals to constitute a native dataset (e.g., signals associated with documents <b>107</b> of the target corpus <b>109</b>) for that code <b>153</b>. However, if the code <b>153</b><i>x </i>is boosted with a other codes <b>153</b><i>a</i>, <b>153</b><i>b </i>such that a boosting dataset <b>155</b> for that code <b>153</b><i>x </i>may be created, where that boosting dataset <b>155</b> comprises the datasets <b>155</b> of those boosting codes <b>153</b><i>a</i>, <b>153</b><i>b </i>before a native model is created for the code <b>153</b><i>x</i>, such a boosting ML model <b>103</b><i>x</i><b>1</b> may be used to generate predictive scores for that code <b>153</b><i>x </i>and corpus <b>109</b><i>x </i>that are shown to a user, potentially before the user has ever applied a single code to any of the documents <b>107</b><i>x </i>of the target corpus <b>109</b><i>x</i>. This provides substantial value to a user, who gains the benefit of machine-generated insights without the effort of labeling a dataset. In addition, embodiments may provide accelerated pathways for applying predictive scores to documents that allow these predictive scores to be shown to the user at a much faster rate than they would normally propagate through typical predictive coding systems.</p><p id="p-0084" num="0083">Moreover, as an additional advantage, the boosting ML model <b>103</b><i>x</i><b>1</b> may be utilized even when the data (e.g., the corpus of documents) associated with a particular boosting tag <b>153</b><i>a</i>, <b>153</b><i>b </i>is unavailable for use with that model. Furthermore, the boosting ML model <b>103</b><i>x</i><b>1</b> may continue to evolve (e.g., be trained and tested) in the absence of such data. This capability is advantageous in a number of contexts where data (e.g., corpora <b>109</b> or documents <b>107</b> thereof) may have a limited time of access or lifetime, or ownership issues exist with respect to the data, such as in the legal context. For example, corpus <b>109</b><i>a </i>may be removed from the repository <b>105</b> of the document analysis system <b>101</b>. However, the boosting ML model <b>103</b><i>x</i><b>1</b> may include (and still utilize and apply) the knowledge gained from documents <b>107</b><i>a </i>of that corpus <b>109</b><i>a</i>, even in the absence of the corpus <b>109</b><i>a. </i></p><p id="p-0085" num="0084">To illustrate in more detail, a particular point of differentiation between data warehousing in the legal industry as opposed to data warehousing across industries in general is that in the legal industry control over and availability of data are ephemeral. The data belongs to clients and not the legal firms or the third party services with which they share that data. This addition and removal of data presents a management challenge for maintaining the consistency and reliability of the bundles, which continue to be used even as the constituent codes change</p><p id="p-0086" num="0085">Consequently, codes <b>153</b> within an aggregation context (e.g., a legal firm) and the associated data (e.g., the documents <b>107</b> or corpora <b>109</b>) may come and go. Permission to access the documents in a dataset underlying a boosting code may be withdrawn, or the documents <b>107</b> or corpora <b>109</b> may be removed from the system <b>101</b>. According to embodiments therefore, a boosted tag (and the associated boosting ML model) exists separately from the boosting codes that are bundled to form it; new codes and their target corpora or datasets may be added while old codes and their target corpora or datasets may be removed.</p><p id="p-0087" num="0086">Referring now to <figref idref="DRAWINGS">FIG. <b>1</b>C</figref> (again where clients <b>130</b> have been removed from the diagram for ease of depiction), as the documents <b>107</b> of a target corpus <b>109</b><i>x </i>are coded with respect to the boosted code <b>153</b><i>x</i>, or other input is received with respect to the boosted code <b>153</b><i>x</i>, a native dataset <b>155</b><i>x</i><b>2</b> comprising documents <b>107</b><i>x </i>of the target corpus <b>109</b><i>x </i>that have been coded with respect to the boosted code <b>153</b><i>x </i>may be determined. Thus, the dataset <b>155</b><i>x </i>for the boosted code <b>1053</b><i>x </i>may now include a boosting dataset <b>155</b><i>x</i><b>1</b> and a native dataset <b>155</b><i>x</i><b>2</b>. Boosting dataset <b>155</b><i>x</i><b>1</b> comprises dataset <b>155</b><i>a </i>(positive signals <b>157</b><i>a </i>and negative signals <b>159</b><i>a</i>) associated with code <b>153</b><i>a </i>and dataset <b>155</b><i>b </i>(positive signals <b>157</b><i>b </i>and negative signals <b>159</b><i>b</i>) associated with code <b>153</b><i>b</i>, while native dataset <b>155</b><i>x</i><b>2</b> comprises positive signals <b>157</b><i>x </i>and negative signals <b>159</b><i>x</i>, each of these signals <b>157</b><i>x </i>and <b>159</b><i>x </i>including a document <b>107</b><i>x </i>(or reference to a document <b>107</b><i>x</i>) of the target corpus <b>109</b><i>x </i>and a corresponding annotation <b>161</b>, <b>163</b>.</p><p id="p-0088" num="0087">Therefore, in some embodiments, ML model builder <b>124</b> may train multiple ML models <b>103</b> using the dataset <b>155</b><i>x </i>associated with the boosted tag <b>153</b><i>x </i>using different subsets or permutations of the datasets <b>155</b> comprising the dataset <b>155</b><i>x </i>associated with that code <b>153</b><i>x</i>. In one embodiment, for example, the ML model builder <b>124</b> may train a boosting ML model <b>103</b><i>x</i><b>1</b> in a boosting training process, a native ML model <b>103</b><i>x</i><b>2</b> in a native training process and a hybrid ML model <b>103</b><i>x</i><b>3</b> in a hybrid training process. Alternatively, in another embodiment, a native ML model <b>103</b><i>x</i><b>2</b> may be trained in a native training process and either the boosting ML model <b>103</b><i>x</i><b>1</b> may be trained in the boosting training process or the hybrid ML model <b>103</b><i>x</i><b>3</b> may be trained in the hybrid training process.</p><p id="p-0089" num="0088">In certain embodiments, each code <b>153</b><i>x </i>that is activated for machine learning (e.g., by the user who defined the code) is associated with a series of training processes (e.g., that may be turned on or off by the user with respect to the code). The associated training processes for the code may be initiated after an incrementally growing number of tagging decisions (e.g., by a human coder) in order to generate models in that training process that account for the increasing amount of signal emanating from the review.</p><p id="p-0090" num="0089">Again, the boosting training process may train the boosting ML model <b>103</b><i>x</i><b>1</b> based on boosting dataset <b>155</b><i>x</i><b>1</b> comprising datasets <b>155</b><i>a </i>and <b>155</b><i>b </i>including signals <b>157</b><i>a</i>, <b>157</b><i>b</i>, <b>159</b><i>a</i>, <b>159</b><i>b </i>from boosting tags <b>153</b><i>a</i>, <b>153</b><i>b </i>associated with documents <b>107</b><i>a </i><b>107</b><i>b </i>of corpora <b>109</b><i>a </i>and <b>109</b><i>b </i>the dataset <b>155</b><i>x </i>of the tag definition <b>111</b><i>x </i>for the code <b>153</b><i>x. </i></p><p id="p-0091" num="0090">The native training process may train native ML model <b>103</b><i>x</i><b>2</b> based on native dataset <b>155</b><i>x</i><b>2</b> comprising positive signals <b>157</b><i>x </i>and negative signals <b>159</b><i>x</i>, each associated with a document <b>107</b><i>x </i>of the target corpus <b>109</b><i>x</i>. The hybrid training process employed by ML model builder <b>124</b> may train a hybrid ML model <b>103</b><i>x</i><b>3</b> based on both the boosting dataset <b>155</b><i>x</i><b>1</b> and the native dataset <b>155</b><i>x</i><b>2</b>. In other words, the boosting dataset <b>155</b><i>x</i><b>1</b> and the native dataset <b>155</b><i>x</i><b>2</b> may be composted to form a hybrid dataset and this hybrid dataset used to train the hybrid ML model <b>103</b><i>x</i><b>3</b>. The generation of each of the boosting ML model <b>103</b><i>x</i><b>1</b>, native ML model <b>103</b><i>x</i><b>2</b> and hybrid ML model <b>103</b><i>x</i><b>3</b> may be trained in a similar manner to that described above.</p><p id="p-0092" num="0091">Thus, hybrid ML model <b>103</b><i>x</i><b>3</b> is informed by coding decisions (e.g., signals) with respect to corpus <b>109</b><i>a </i>and code <b>153</b><i>a</i>, corpus <b>107</b><i>b </i>and code <b>153</b><i>b</i>, and coding decision with respect to corpus <b>109</b><i>x </i>and code <b>153</b><i>x</i>. Native ML model <b>103</b><i>x</i><b>2</b> is informed by coding decisions (e.g., signals) with respect to documents <b>107</b><i>x </i>of corpus <b>109</b><i>x</i>, while boosting ML model <b>103</b><i>x</i><b>1</b> is informed by coding decisions (e.g., signals) with respect to corpus <b>109</b><i>a </i>and code <b>153</b><i>a </i>and corpus <b>107</b><i>b </i>and code <b>153</b><i>b. </i></p><p id="p-0093" num="0092">As can be seen then, at this point there may be multiple ML models <b>103</b><i>x </i>(e.g., native model <b>103</b><i>x</i><b>2</b>, boosting ML model <b>103</b><i>x</i><b>1</b>, or hybrid ML model <b>133</b><i>x</i><b>3</b>) that may be used to generate predictive scores for code <b>153</b><i>x </i>with respect to the documents <b>107</b><i>x </i>of the corpus <b>109</b><i>x</i>. As it is desired to generate the most accurate predictive score possible, predictive coder <b>122</b> may perform A/B testing on any available models <b>103</b> for a code <b>153</b> to select the best model to use for generating predictive codes for the documents <b>107</b><i>x </i>of the corpus <b>109</b><i>x </i>with respect to the boosted code <b>153</b><i>x</i>. This testing may occur, for example, at the time the documents of the corpus <b>107</b><i>x </i>are scored with respect to the code <b>153</b><i>x </i>(e.g., for presentation to the user). Thus, each code <b>153</b> may have its own tests performed to see which of the available models <b>103</b> for that tag <b>153</b> is best for that particular tag <b>153</b>. For example, with respect to code <b>153</b><i>x</i>, predictive coder <b>122</b> may test hybrid ML model <b>103</b><i>x</i><b>3</b>, native ML model <b>103</b><i>x</i><b>2</b> and boosted ML model <b>103</b><i>x</i><b>1</b> to evaluate these models against one another to select a best one of the models to utilize.</p><p id="p-0094" num="0093">The predictive coder <b>122</b> may test each of these models <b>103</b><i>x </i>using a test set of data derived from corpus <b>107</b><i>x </i>(e.g., a &#x201c;native test set&#x201d;) to obtain an accuracy metric for each model <b>103</b><i>x </i>such that the model with the highest accuracy may be selected to generate predictive codes for the code <b>153</b><i>x </i>for display to the user. In particular, each of the hybrid ML model <b>103</b><i>x</i><b>3</b>, native ML model <b>103</b><i>x</i><b>2</b> and boosting ML model <b>103</b><i>x</i><b>1</b> may be tested on the test set for the corpus <b>107</b><i>x</i>. The model <b>103</b> (e.g., hybrid ML model <b>103</b><i>x</i><b>3</b>, native ML model <b>103</b><i>x</i><b>2</b> or boosting ML model <b>103</b><i>x</i><b>1</b>) that is selected based on the evaluation, may be used to generate predictive scores for the documents of the corpus <b>109</b><i>x </i>that are shown to the users. In this way, the user always obtains the best predictive scores that can be obtained using the models generated by the multiple training processes.</p><p id="p-0095" num="0094">In one embodiment, in order to maintain consistency, the models <b>103</b> produced by each training process (e.g., the boosting training process, the hybrid training process or the native training process) for a boosted code <b>153</b><i>x </i>may be archived, or the selected best model used to generate scores may be archived. When the test set changes, the training set also changes and a training process will be initiated. A set of models including at least the best of the previous models and the outcome of the most recent training processes (e.g., hybrid ML model <b>103</b><i>x</i><b>3</b>, native ML model <b>103</b><i>x</i><b>2</b> and boosting ML model <b>103</b><i>x</i><b>1</b>) are evaluated on the new test set. Here, the model <b>103</b> (e.g., hybrid ML model <b>103</b><i>x</i><b>3</b>, native ML model <b>103</b><i>x</i><b>2</b> and boosting ML model <b>103</b><i>x</i><b>1</b> or a previous model <b>103</b>) that is selected based on the evaluation, may be used to generate predictive scores for the documents of the corpus <b>109</b><i>x </i>that are shown to the users. In this way, the user always obtains the best predictive scores that can be offered by either a current training process or a previous training process.</p><p id="p-0096" num="0095">These previously trained models may have been trained on data that is no longer available (e.g., the corpus on which it was initially developed may no longer available). By archiving and testing previously generated models a model trained on the old data may be still available, and if this model has better performance on the new test set, this previously trained model may be used to generate predictive scores for the code <b>153</b><i>x</i>. Thus embodiments may allow the retention of quality models in an environment of changing data.</p><p id="p-0097" num="0096">Specifically, in embodiments the predictive coder <b>122</b> may determine and track metrics regarding the performance of the predictive coder <b>122</b> within each corpus <b>109</b> and across corpora <b>109</b> and makes these metrics available for display to a user in order to guide decisions on which codes should be bundled together or where codes may be boosted (or used to boost another code) to augment existing predictions. Accuracy metrics include but are not limited to precision, recall, F1-score, raw accuracy, weighted accuracy, perceived accuracy, enrichment, etc.</p><p id="p-0098" num="0097">Such an accuracy metric may be determined for each existing model associated with a code <b>153</b> (e.g., code <b>153</b><i>x</i>) to determine the best model to utilize to generate predictive scores for documents of corpus <b>109</b> with respect to the code <b>153</b>. The selection of the accuracy may be of particular importance in the context of boosted tags according to embodiments. To explain in more detail, within information retrieval, it is commonly the case that one wishes to locate some small fraction of items within a much larger corpus. As a consequence, if a decision process is measured according to how well it makes decisions generally about which items should be located, then one high accuracy decision process is simply to locate nothing. If 1% of the documents are relevant, then this nothing-locator yields 99% accuracy. As a consequence, within information retrieval, the metrics of precision and recall are used to measure, respectively, how accurate decisions are for proposed items and what percentage of actual items have been located; the F-score is the harmonic mean of these two metrics.</p><p id="p-0099" num="0098">In the current context of embodiments including boosted codes, however, precision and recall no longer have the same meaning, because a bundled dataset for a boosted code is extracted from its natural context and corpus, and the basic statistics governing an information retrieval task are disrupted. Consequently, in order to measure the quality of decision processes over bundled datasets for boosted codes <b>153</b>, a metric is needed that remains insensitive to the rarity of a class. The weighted accuracy is the average of the precision for the class and the precision for the negated class on the bundled dataset. This metric balances the prevalence of positive and negative identification of relevant objects, and hence can be used across multiple datasets to assess quality.</p><p id="p-0100" num="0099">As such, in certain embodiments, predictive coder <b>122</b> may test each of the models (e.g., hybrid ML model <b>103</b><i>x</i><b>3</b>, native ML model <b>103</b><i>x</i><b>2</b> and boosting ML model <b>103</b><i>x</i><b>1</b>) using a test set of data derived from corpus <b>107</b><i>x </i>(e.g., a &#x201c;native test set&#x201d;) to obtain an weighted accuracy for each model such that the model with the highest weighted accuracy may be selected to generate predictive codes for the code <b>153</b><i>x </i>for display to the user.</p><p id="p-0101" num="0100">It will be noted here that the selected model <b>103</b><i>x </i>(or any of the models generated through the training processes described) for generating scores for predictive codes for code <b>153</b><i>x </i>can be applied not just to the target corpus <b>109</b><i>x </i>of documents <b>107</b>, but additionally can be applied against any other corpora <b>109</b> of the document analysis system <b>101</b>, including corpora <b>109</b> (e.g., corpus <b>109</b><i>a </i>or <b>109</b><i>b</i>) from which the boosting codes <b>153</b> (e.g., code <b>153</b><i>a </i>or <b>153</b><i>b</i>) were originally obtained or associated with. Thus, if code <b>153</b><i>x </i>is applied to an available corpora <b>109</b> (e.g., corpora <b>109</b><i>a </i>or <b>109</b><i>b</i>) in the future, the same process as described with respect to the training and selection of that model <b>103</b><i>x </i>with respect to corpus <b>109</b><i>x </i>may occur. As may be realized, in such a case, the documents <b>107</b> selected for a test set of documents for the model <b>103</b><i>x </i>as applied to that corpora <b>109</b> may be selected from that corpora <b>109</b>. For example, if the selected model <b>103</b><i>x </i>is to be applied to corpus <b>109</b><i>a</i>, the test set of documents <b>107</b> for testing the models <b>103</b><i>x </i>generated by each training process may be selected from that particular corpus <b>109</b><i>a </i>to determine a best model for generating predictive scores for code <b>153</b><i>x </i>with respect to that corpus <b>109</b><i>a</i>. Thus, for example, the set of trained models <b>103</b><i>x </i>associated with a tag <b>153</b><i>x </i>may be evaluated using documents <b>107</b><i>a </i>and a best model <b>103</b><i>x </i>selected for generating predictive scores for code <b>153</b><i>x </i>with respect to that corpus <b>109</b><i>a </i>and the set of trained models <b>103</b><i>x </i>associated with a tag <b>153</b><i>x </i>may be evaluated using documents <b>107</b><i>x </i>and a best model <b>103</b><i>x </i>selected for generating predictive scores for code <b>153</b><i>x </i>with respect to that corpus <b>109</b><i>x</i>. Accordingly, in some instances, the selected models <b>103</b><i>x </i>for different corpora <b>109</b> may be different models <b>109</b><i>x </i>(e.g., produced by different training processes). In this manner, a best model <b>103</b><i>x </i>for generating predictive scores may be selected for a combination of a code <b>153</b><i>x </i>and the particular corpus <b>109</b> to which it is to be applied. Accordingly, these composite or boosted tags may be reused and serve as classifiers that may be reused across corpora of documents in a document analysis system.</p><p id="p-0102" num="0101">Moreover, in those instances, when coding decisions are made with respect to that code <b>153</b><i>x </i>and documents <b>107</b> of a particular corpus <b>109</b> that is not the (e.g., original) target corpus <b>109</b><i>x </i>of the code <b>153</b><i>x</i>, those coding decisions may be added to the dataset <b>155</b><i>x </i>associated with the code <b>153</b><i>x</i>. Specifically, the coding decisions with respect to the code <b>153</b><i>x </i>and a particular <b>107</b> associated with a particular corpus <b>109</b> may be added as signal to the portion of the dataset <b>155</b><i>x </i>of the code <b>153</b><i>x </i>comprising documents <b>107</b> of that corpus <b>109</b><i>a</i>. Thus, for example, if a coding decision for tag <b>153</b><i>x </i>is received with respect to a document <b>107</b><i>a </i>from corpus <b>107</b><i>a</i>, it may be added to boosting dataset <b>155</b><i>x</i><b>1</b> in association with signals <b>157</b><i>a</i>, <b>159</b><i>a </i>associated with that corpus <b>109</b><i>a. </i></p><p id="p-0103" num="0102">It may now be useful for an understanding of embodiments to discuss an embodiment of a method that may be employed by document analysis systems for reuse of data, models or coding decisions developed with respect to one corpus in the coding of another corpus. <figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts one embodiment of just such a methodology. With reference to the flow diagram of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, initially then, a definition of a code and a target corpus may be received by the document analysis system (STEP <b>202</b>). This definition may, for example, include the associated code (e.g., label) and one or more target corpora against which are to be predictively coded according to the code. Once the code is defined, this code may be boosted by selecting one or more boosting codes for the defined code (the boosted code) that are received by the document analysis system (STEP <b>204</b>). The definition of the boosted code and the selection of the one or more boosting codes may be done using an interface at the document analysis system by one or more different types of users.</p><p id="p-0104" num="0103">For example, the types of users may include a curator (e.g., a curator persona) or a reviewer (e.g., a reviewer persona). Certain embodiments of the interface may allow a user, such as a curator or a creator of a code, to add a text annotation or other description (e.g., a natural language description or textual blurb) for the code being defined that may explain the code, its relevance or history, or any other information the user wishes to explicate regarding the code. The interface for a curator persona (e.g., a user who is a curator) may provide a list of available codes, their general statistics including the number of positive and negative signals (e.g., from a particular corpus <b>109</b>) for the code, the textual descriptions for each code, if available, or an accuracy metric that can be used to judge the quality of the code and its relevance. The curator can then create new codes (e.g., boosted codes) from one or more extant codes and manage existing boosted codes by adding or removing codes (e.g. boosting codes) from this boosted code. The curator can also be provided with alerts when codes are selected to boost a code or when the document review system detects a potential match for a boosted code. These features make it easy to reuse existing datasets by selecting appropriate codes for boosting.</p><p id="p-0105" num="0104">A reviewer interface may also be provided for a reviewer or review manager. These review managers may be people who (e.g., during legal discovery or the like) may be responsible for organizing and supervising the review process whose outcome is the application of a set of codes to a corpus. The review manager may make or implement decisions about whether and how to utilize machine predictions during the review process. Accordingly, embodiments of interfaces may offer a code management panel on which a review manager (or other user) can view and edit the codes for the review process. In particular, the review manager can add or modify the text description and can determine whether or not machine learning will be applied to a particular code. During the process of this latter decision, the review manager (or a curator) may be provided the opportunity to &#x201c;boost&#x201d; a code based with another currently existing code. This interface may also indicate which of these codes are themselves boosted codes and the composition of the datasets of such codes (e.g., the corpora with which they are associated). This list can be searched, sorted, or filtered in order to help the review manager find the most relevant code or codes.</p><p id="p-0106" num="0105">Once the boosting codes are selected for the boosted code, the datasets (e.g., the positive and negative signals) associated with each of the boosting codes can be added to the dataset of the boosted code as a boosting dataset of the boosted code (STEP <b>206</b>). Once the boosted code is defined and the boosting codes associated such that the boosting dataset of the boosted code has been created (or updated), at some time interval then, a determination may be made by the document analysis system to train models for the boosted code (STEP <b>208</b>). This determination may be made on a criterion such as a size of datasets of one or more of the boosting codes, the size of the boosting dataset of the boosted code, a change in a training, testing or evaluation dataset, an elapsed time interval, or almost any criteria desired.</p><p id="p-0107" num="0106">When it is determined that models are to be trained (Y Branch of STEP <b>208</b>), the document analysis system can then determine whether a boosting model, a hybrid model or a native model is to be trained (e.g., using an associated training process) (STEP <b>210</b>). This determination can be made based on a setting of user who defined the boosted code or a user who boosted the code by selecting the boosting tags, such a reviewer or a curator; based on data availability such as the size of the boosting dataset or a native dataset; based on hard coded settings or administrator configurable settings of the document analysis system, or based on some other criterion.</p><p id="p-0108" num="0107">In one embodiment, either a hybrid or a boosting model may be trained by the document analysis system (STEP <b>212</b>), while a native model may always be trained if a size of a native dataset exceeds a certain threshold number of positive or negative signals (STEP <b>214</b>). If a boosting model is to be created (Boosting Branch of STEP <b>212</b>), the document analysis system may train a boosting model in a boosting training process by training a model using only the boosting dataset of the boosted code (e.g., the dataset formed of positive and negative signals obtained from the datasets of each of the boosting codes selected for the boosted code) to yield the boosting model (STEP <b>216</b>). If a hybrid model is to be created (Hybrid Branch of STEP <b>212</b>), the document analysis system may train a hybrid model in a hybrid training process by training a model using the boosting dataset of the boosted code (e.g., the dataset formed of positive and negative signals obtained from the datasets of each of the boosting codes selected for the boosted code) and the native data set (e.g., the dataset formed of positive and negative signals associated with documents of the target corpus coded according to the boosted code) to yield the hybrid model (STEP <b>218</b>). If a native model is to be created (e.g., if there is a sufficient native dataset) (Y Branch of STEP <b>214</b>) the document analysis system may train a native model in a native training process by training a model using only the native dataset of the boosted code (e.g., the dataset formed of positive and negative signals associated with documents of the target corpus coded according to the boosted code) (STEP <b>220</b>).</p><p id="p-0109" num="0108">These models may be generated in a similar manner to that described in U.S. patent application Ser. No. 16/167,205 entitled &#x201c;Methods and Apparatus for Asynchronous and Interactive Machine Learning Using Attention Selection Techniques&#x201d; by Lockett, however, it will also be noted generally that in various embodiments ML models may be generated by different methodology, or that ML models generated from different datasets (or the same datasets) may be generated by differing methodologies, without loss of generality,</p><p id="p-0110" num="0109">In one embodiment, in order to ensure that a boosting or hybrid ML model is not overly influenced (e.g., overweighed) by a particular boosting code, a boosting or hybrid model may be trained using a balancing methodology to sample positive signals and negative signals from each composite dataset of the boosting dataset associated with each boosting code (or native data set) according to a balancing methodology. This balancing methodology may include for example, a logarithmic differential balancing method, a round robin selection process whereby a positive and negative signal is selected from each of the composite dataset, or by some other balancing method</p><p id="p-0111" num="0110">Thus, at the end of the model training process there are a set of current models trained for the boosted code. If there are multiple models, these trained models may be evaluated against one another to select the current best model (STEP <b>222</b>). Specifically, in one embodiment, A/B testing on any available models (e.g., if more than one has been trained) may be performed to select the best of these currently trained models. These models may be tested using a test set of data derived from the target corpus to obtain an accuracy metric for each model such that the model (e.g., boosting, hybrid or native) with the highest accuracy may be selected.</p><p id="p-0112" num="0111">Once a current best model of the trained models has been selected it can be determined if there are any previously trained best models (STEP <b>224</b>). If a previously generated best model exists (e.g., a model has been previously trained for the boosted code and used to generate predictive scores for the boosted code) (Y Branch of STEP <b>224</b>), the best of the currently trained models may be evaluated against the previous best model (e.g., using A/B testing based on the test set of data of the target corpus). The best of the currently trained model or the previous best model may then be selected as the current best model (STEP <b>226</b>) and applied to the target corpus to generate scores for the boosted code (STEP <b>228</b>). In this way, the user always obtains the best predictive scores that can be offered by either a current training process or a previous training process. The predictive coding scores for the documents and the boosted code can be presented to the user through an interface of the document analysis system. Other information can also be displayed to a user of the system through such an interface, such as a display of a history of accuracy or quality of models or training process (e.g., on a particular corpus) to lend confidence to the determinations of the document analysis system. Coding decisions with respect to the documents of the target corpus and the boosted code can be received (e.g., through the interface) (STEP <b>230</b>) and added to the native dataset associated with the boosted code (STEP <b>232</b>).</p><p id="p-0113" num="0112">Embodiments as described herein may be understood with reference to embodiments of interfaces such embodiments. <figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>C</figref> depict embodiments of a reviewer interface for use with a document analysis system. In the depicted example interfaces, in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, the user has defined a code (e.g., referred to in the interface as a &#x201c;tag&#x201d;) called &#x201c;Bribes&#x201d; and assigned it a textual description of &#x201c;Docs about illegal persuasion&#x201d;. In <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, a user is presented with a list of codes that may be used to boost the &#x201c;Bribes&#x201d; code being defined, along with the textual descriptions of such potential boosting codes. Here, the user has selected another &#x201c;Bribes&#x201d; code as a boosting code for the &#x201c;Bribes&#x201d; code being defined. In <figref idref="DRAWINGS">FIG. <b>3</b>C</figref>, the &#x201c;Bribes&#x201d; code being defined is now shown as being boosted by the &#x201c;Bribes&#x201d; code and the &#x201c;AI Predictions&#x201d; slider is set to &#x201c;on&#x201d;, denoting that the &#x201c;Bribes&#x201d; code being defined will be boosted using the selected &#x201c;Bribes&#x201d; boosting code and that predictive codes will be generated based on a boosting dataset derived from this boosting code.</p><p id="p-0114" num="0113"><figref idref="DRAWINGS">FIGS. <b>4</b>A-<b>4</b>C</figref> depict embodiments of a curator interface for a document analysis system. As can be seen in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, a user (e.g., a curator) can be presented with a list of boosted codes for which predictive coding is enabled where at least one boosting code for each of the boosted code having a different target corpus (e.g., referred to as &#x201c;cross-matter&#x201d;). A text description may encompass each of the codes listed in the interface. Here, a &#x201c;Bribes&#x201d; code is listed as a cross-matter boosted code. <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> depicts an interface that may be displayed to the user when the user interacts with a boosted code, allowing a user to see all the boosting codes utilized by that boosted code, or alternatively, the other codes being boosted by that boosted code (e.g., where that boosted code is itself, used as a boosting code). <figref idref="DRAWINGS">FIG. <b>4</b>C</figref> depicts an interface that allows code to be selected for removal or addition to the boosting codes utilized by that boosted code, or alternatively, the other codes being boosted by that boosted code.</p><p id="p-0115" num="0114">Although the invention has been described with respect to specific embodiments thereof, these embodiments are merely illustrative, and not restrictive of the invention. The description herein of illustrated embodiments of the invention, including the description in the Abstract and Summary, is not intended to be exhaustive or to limit the invention to the precise forms disclosed herein. Rather, the description is intended to describe illustrative embodiments, features and functions in order to provide a person of ordinary skill in the art context to understand the invention without limiting the invention to any particularly described embodiment, feature or function, including any such embodiment feature or function described in the Abstract or Summary. While specific embodiments of, and examples for, the invention are described herein for illustrative purposes only, various equivalent modifications are possible within the spirit and scope of the invention, as those skilled in the relevant art will recognize and appreciate. As indicated, these modifications may be made to the invention in light of the foregoing description of illustrated embodiments of the invention and are to be included within the spirit and scope of the invention. Thus, while the invention has been described herein with reference to particular embodiments thereof, a latitude of modification, various changes and substitutions are intended in the foregoing disclosures, and it will be appreciated that in some instances some features of embodiments of the invention will be employed without a corresponding use of other features without departing from the scope and spirit of the invention as set forth. Therefore, many modifications may be made to adapt a particular situation or material to the essential scope and spirit of the invention.</p><p id="p-0116" num="0115">Reference throughout this specification to &#x201c;one embodiment&#x201d;, &#x201c;an embodiment&#x201d;, or &#x201c;a specific embodiment&#x201d; or similar terminology means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment and may not necessarily be present in all embodiments. Thus, respective appearances of the phrases &#x201c;in one embodiment&#x201d;, &#x201c;in an embodiment&#x201d;, or &#x201c;in a specific embodiment&#x201d; or similar terminology in various places throughout this specification are not necessarily referring to the same embodiment. Furthermore, the particular features, structures, or characteristics of any particular embodiment may be combined in any suitable manner with one or more other embodiments. It is to be understood that other variations and modifications of the embodiments described and illustrated herein are possible in light of the teachings herein and are to be considered as part of the spirit and scope of the invention.</p><p id="p-0117" num="0116">In the description herein, numerous specific details are provided, such as examples of components or methods, to provide a thorough understanding of embodiments of the invention. One skilled in the relevant art will recognize, however, that an embodiment may be able to be practiced without one or more of the specific details, or with other apparatus, systems, assemblies, methods, components, materials, parts, and/or the like. In other instances, well-known structures, components, systems, materials, or operations are not specifically shown or described in detail to avoid obscuring aspects of embodiments of the invention. While the invention may be illustrated by using a particular embodiment, this is not and does not limit the invention to any particular embodiment and a person of ordinary skill in the art will recognize that additional embodiments are readily understandable and are a part of this invention.</p><p id="p-0118" num="0117">Embodiments discussed herein can be implemented in a computer communicatively coupled to a network (for example, the Internet), another computer, or in a standalone computer. As is known to those skilled in the art, a suitable computer can include a CPU, at least one read-only memory (&#x201c;ROM&#x201d;), at least one random access memory (&#x201c;RAM&#x201d;), at least one hard drive (&#x201c;HD&#x201d;), and one or more input/output (&#x201c;I/O&#x201d;) device(s). The I/O devices can include a keyboard, monitor, printer, electronic pointing device (for example, mouse, trackball, stylus, touch pad, etc.), or the like.</p><p id="p-0119" num="0118">ROM, RAM, and HD are computer memories for storing computer-executable instructions executable by the CPU or capable of being compiled or interpreted to be executable by the CPU. Suitable computer-executable instructions may reside on a computer readable medium (e.g., ROM, RAM, and/or HD), hardware circuitry or the like, or any combination thereof. Within this disclosure, the term &#x201c;computer readable medium&#x201d; is not limited to ROM, RAM, and HD and can include any type of data storage medium that can be read by a processor. For example, a computer-readable medium may refer to a data cartridge, a data backup magnetic tape, a floppy diskette, a flash memory drive, an optical data storage drive, a CD-ROM, ROM, RAM, HD, or the like. The processes described herein may be implemented in suitable computer-executable instructions that may reside on a computer readable medium (for example, a disk, CD-ROM, a memory, etc.). Alternatively, the computer-executable instructions may be stored as software code components on a direct access storage device array, magnetic tape, floppy diskette, optical storage device, or other appropriate computer-readable medium or storage device.</p><p id="p-0120" num="0119">Any suitable programming language can be used to implement the routines, methods or programs of embodiments of the invention described herein, including C, C++, Java, JavaScript, HTML, or any other programming or scripting code, etc. Other software/hardware/network architectures may be used. For example, the functions of the disclosed embodiments may be implemented on one computer or shared/distributed among two or more computers in or across a network. Communications between computers implementing embodiments can be accomplished using any electronic, optical, radio frequency signals, or other suitable methods and tools of communication in compliance with known network protocols.</p><p id="p-0121" num="0120">Different programming techniques can be employed such as procedural or object oriented. Any particular routine can execute on a single computer processing device or multiple computer processing devices, a single computer processor or multiple computer processors. Data may be stored in a single storage medium or distributed through multiple storage mediums, and may reside in a single database or multiple databases (or other data storage techniques). Although the steps, operations, or computations may be presented in a specific order, this order may be changed in different embodiments. In some embodiments, to the extent multiple steps are shown as sequential in this specification, some combination of such steps in alternative embodiments may be performed at the same time. The sequence of operations described herein can be interrupted, suspended, or otherwise controlled by another process, such as an operating system, kernel, etc. The routines can operate in an operating system environment or as stand-alone routines. Functions, routines, methods, steps and operations described herein can be performed in hardware, software, firmware or any combination thereof.</p><p id="p-0122" num="0121">Embodiments described herein can be implemented in the form of control logic in software or hardware or a combination of both. The control logic may be stored in an information storage medium, such as a computer-readable medium, as a plurality of instructions adapted to direct an information processing device to perform a set of steps disclosed in the various embodiments. Based on the disclosure and teachings provided herein, a person of ordinary skill in the art will appreciate other ways and/or methods to implement the invention.</p><p id="p-0123" num="0122">It is also within the spirit and scope of the invention to implement in software programming or code any of the steps, operations, methods, routines or portions thereof described herein, where such software programming or code can be stored in a computer-readable medium and can be operated on by a processor to permit a computer to perform any of the steps, operations, methods, routines or portions thereof described herein. The invention may be implemented by using software programming or code in one or more general purpose digital computers, by using application specific integrated circuits, programmable logic devices, field programmable gate arrays, optical, chemical, biological, quantum or nanoengineered systems, components and mechanisms may be used. In general, the functions of the invention can be achieved by any means as is known in the art. For example, distributed or networked systems, components and circuits can be used. In another example, communication or transfer (or otherwise moving from one place to another) of data may be wired, wireless, or by any other means.</p><p id="p-0124" num="0123">A &#x201c;computer-readable medium&#x201d; may be any medium that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, system or device. The computer readable medium can be, by way of example only but not by limitation, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, system, device, propagation medium, or computer memory. Such computer-readable medium shall generally be machine readable and include software programming or code that can be human readable (e.g., source code) or machine readable (e.g., object code). Examples of non-transitory computer-readable media can include random access memories, read-only memories, HDs, data cartridges, magnetic tapes, floppy diskettes, flash memory drives, optical data storage devices, CD-ROMs, and other appropriate computer memories and data storage devices. In an illustrative embodiment, some or all of the software components may reside on a single server computer or on any combination of separate server computers. As one skilled in the art can appreciate, a computer program product implementing an embodiment disclosed herein may comprise one or more non-transitory computer readable media storing computer instructions translatable by one or more processors in a computing environment.</p><p id="p-0125" num="0124">A &#x201c;processor&#x201d; includes any hardware system, mechanism or component that processes data, signals or other information. A processor can include a system with a general-purpose CPU, multiple processing units, dedicated circuitry for achieving functionality, or other systems. Processing need not be limited to a geographic location, or have temporal limitations. For example, a processor can perform its functions in &#x201c;real-time,&#x201d; &#x201c;offline,&#x201d; in a &#x201c;batch mode,&#x201d; etc. Portions of processing can be performed at different times and at different locations, by different (or the same) processing systems.</p><p id="p-0126" num="0125">It will also be appreciated that one or more of the elements depicted in the drawings/figures can also be implemented in a more separated or integrated manner, or even removed or rendered as inoperable in certain cases, as is useful in accordance with a particular application. Additionally, any signal arrows in the drawings/figures should be considered only as exemplary, and not limiting, unless otherwise specifically noted.</p><p id="p-0127" num="0126">As used herein, the terms &#x201c;comprises,&#x201d; &#x201c;comprising,&#x201d; &#x201c;includes,&#x201d; &#x201c;including,&#x201d; &#x201c;has,&#x201d; &#x201c;having,&#x201d; or any other variation thereof, are intended to cover a non-exclusive inclusion. For example, a process, product, article, or apparatus that comprises a list of elements is not necessarily limited only those elements but may include other elements not expressly listed or inherent to such process, product, article, or apparatus.</p><p id="p-0128" num="0127">Furthermore, the term &#x201c;or&#x201d; as used herein is generally intended to mean &#x201c;and/or&#x201d; unless otherwise indicated. For example, a condition A or B is satisfied by any one of the following: A is true (or present) and B is false (or not present), A is false (or not present) and B is true (or present), and both A and B are true (or present). As used herein, that follow, a term preceded by &#x201c;a set&#x201d;, &#x201c;a&#x201d; or &#x201c;an&#x201d; (and &#x201c;the&#x201d; when antecedent basis is &#x201c;a&#x201d; or &#x201c;an&#x201d;) includes both singular and plural of such term, unless clearly indicated otherwise (i.e., that the reference &#x201c;a set&#x201d;, &#x201c;a&#x201d; or &#x201c;an&#x201d; clearly indicates only the singular or only the plural). Also, as used in the description herein the meaning of &#x201c;in&#x201d; includes &#x201c;in&#x201d; and &#x201c;on&#x201d; unless the context clearly dictates otherwise.</p><p id="p-0129" num="0128">Although the foregoing specification describes specific embodiments, numerous changes in the details of the embodiments disclosed herein and additional embodiments will be apparent to, and may be made by, persons of ordinary skill in the art having reference to this disclosure. In this context, the specification and figures are to be regarded in an illustrative rather than a restrictive sense, and all such modifications are intended to be included within the scope of this disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for document analysis comprising:<claim-text>a processor;</claim-text><claim-text>a data store, comprising a first corpus of electronic document and a second corpus of electronic documents; and</claim-text><claim-text>a non-transitory computer readable medium comprising instructions for:<claim-text>receiving an indication that a first code is to be boosted with a second code, wherein the first code is associated with first documents from the first corpus and the second code is associated with a boosting dataset comprising positive signals or negative signals from the second corpus, each positive or negative signal associated with second documents of the second corpus, wherein each positive signal indicates the associated second document belongs to the second code and each negative signal indicates the associated second document does not belong to the second code; and</claim-text><claim-text>training a boosting machine learning model adapted to generate predictive scores for the first code based on the boosting dataset including the second dataset comprising positive signals or negative signals from the second corpus, including training the first machine learning model based on each positive signal or negative signal from the second corpus, such that the first machine learning model is trained on the second documents of the second corpus associated with each of the positive or negative signals of the second dataset.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the instructions are further for:<claim-text>comparing the boosting machine learning model to a previous boosting machine leaning model for the first code previously trained on a previous boosting dataset comprising positive signals or negative signals from the second corpus; and</claim-text><claim-text>selecting, based on the comparison, the better of the boosting learning model or the previous boosting machine learning model as a current boosting machine learning model.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the instructions are further for:<claim-text>training, on a native dataset, a native machine learning model adapted to generate predictive scores for the first code;</claim-text><claim-text>comparing the native machine learning model to the current boosting machine learning model; and</claim-text><claim-text>selecting, based on the comparison, the better of the first machine learning model or the second machine learning model to generate predictive scores for the first code.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the indication is received through an interface through which a user defines the first code.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the interface allows the user to search for codes utilized for different corpora of documents and the second code is selected for boosting the first code using the interface.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the user does not have access to the second corpus of documents.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first target corpus is different than the second target corpus.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method for document analysis comprising:<claim-text>receiving an indication that a first code is to be boosted with a second code, wherein the first code is associated with first documents from a first corpus of electronic documents and the second code is associated with a boosting dataset comprising positive signals or negative signals from a second corpus of electronic documents, each positive or negative signal associated with second documents of the second corpus, wherein each positive signal indicates the associated second document belongs to the second code and each negative signal indicates the associated second document does not belong to the second code; and</claim-text><claim-text>training a boosting machine learning model adapted to generate predictive scores for the first code based on the boosting dataset including the second dataset comprising positive signals or negative signals from the second corpus, including training the first machine learning model based on each positive signal or negative signal from the second corpus, such that the first machine learning model is trained on the second documents of the second corpus associated with each of the positive or negative signals of the second dataset.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>comparing the boosting machine learning model to a previous boosting machine leaning model for the first code previously trained on a previous boosting dataset comprising positive signals or negative signals from the second corpus; and</claim-text><claim-text>selecting, based on the comparison, the better of the boosting learning model or the previous boosting machine learning model as a current boosting machine learning model.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>training, on a native dataset, a native machine learning model adapted to generate predictive scores for the first code;</claim-text><claim-text>comparing the native machine learning model to the current boosting machine learning model; and</claim-text><claim-text>selecting, based on the comparison, the better of the first machine learning model or the second machine learning model to generate predictive scores for the first code.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the indication is received through an interface through which a user defines the first code.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the interface allows the user to search for codes utilized for different corpora of documents and the second code is selected for boosting the first code using the interface.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the user does not have access to the second corpus of documents.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the first target corpus is different than the second target corpus.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer readable medium, comprising instructions for:<claim-text>receiving an indication that a first code is to be boosted with a second code, wherein the first code is associated with first documents from a first corpus of electronic documents and the second code is associated with a boosting dataset comprising positive signals or negative signals from a second corpus of electronic documents, each positive or negative signal associated with second documents of the second corpus, wherein each positive signal indicates the associated second document belongs to the second code and each negative signal indicates the associated second document does not belong to the second code; and</claim-text><claim-text>training a boosting machine learning model adapted to generate predictive scores for the first code based on the boosting dataset including the second dataset comprising positive signals or negative signals from the second corpus, including training the first machine learning model based on each positive signal or negative signal from the second corpus, such that the first machine learning model is trained on the second documents of the second corpus associated with each of the positive or negative signals of the second dataset.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising instructions for:<claim-text>comparing the boosting machine learning model to a previous boosting machine leaning model for the first code previously trained on a previous boosting dataset comprising positive signals or negative signals from the second corpus; and</claim-text><claim-text>selecting, based on the comparison, the better of the boosting learning model or the previous boosting machine learning model as a current boosting machine learning model.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising instructions for:<claim-text>training, on a native dataset, a native machine learning model adapted to generate predictive scores for the first code;</claim-text><claim-text>comparing the native machine learning model to the current boosting machine learning model; and</claim-text><claim-text>selecting, based on the comparison, the better of the first machine learning model or the second machine learning model to generate predictive scores for the first code.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the indication is received through an interface through which a user defines the first code.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the interface allows the user to search for codes utilized for different corpora of documents and the second code is selected for boosting the first code using the interface.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the user does not have access to the second corpus of documents.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first target corpus is different than the second target corpus.</claim-text></claim></claims></us-patent-application>