<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230003886A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230003886</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364958</doc-number><date>20210701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>66</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>66</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0248</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0212</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR TEMPORAL DECORRELATION OF OBJECT DETECTIONS FOR PROBABILISTIC FILTERING</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Argo AI, LLC</orgname><address><city>Pittsburgh</city><state>PA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Wyffels</last-name><first-name>Kevin Lee</first-name><address><city>Livonia</city><state>MI</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for tracking an object. The method comprising: receiving, by a processor, a series of observations made over time for the object; selecting, by the processor, a plurality of sets of observations using the series of observations; causing, by the processor, the plurality of sets of observations to be used by at least one filter to generate a track for the object (wherein the at least one filter uses sensor data associated with each of a plurality of frames of sensor data only once during generation of the track); and causing, by the processor, operations of an autonomous robot to be controlled based on the track for the object.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="90.34mm" wi="158.75mm" file="US20230003886A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="223.77mm" wi="146.13mm" orientation="landscape" file="US20230003886A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="165.10mm" wi="157.06mm" file="US20230003886A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="236.39mm" wi="131.15mm" orientation="landscape" file="US20230003886A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="225.38mm" wi="151.81mm" file="US20230003886A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="235.71mm" wi="146.56mm" file="US20230003886A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="172.21mm" wi="134.87mm" orientation="landscape" file="US20230003886A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="237.41mm" wi="160.95mm" orientation="landscape" file="US20230003886A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="219.12mm" wi="165.02mm" file="US20230003886A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="216.83mm" wi="139.19mm" orientation="landscape" file="US20230003886A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">Statement of the Technical Field</p><p id="p-0003" num="0002">The present disclosure relates generally to object detection systems. More particularly, the present disclosure relates to implementing systems and methods for temporal decorrelation of object detections for probabilistic filtering.</p><heading id="h-0002" level="1">DESCRIPTION OF THE RELATED ART</heading><p id="p-0004" num="0003">Object detection algorithms based on deep learning may operate on a temporal window of sensor data rather than on a single frame of sensor data. These object detection algorithms suffer from certain drawbacks. For example, object detections output from such algorithms are correlated over time which directly violates a foundational assumption of efficient estimators operating on those object detections.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">The present disclosure concerns implementing systems and methods for tracking an object and/or controlling an autonomous robot. The methods comprise performing the following operations by a processor: receiving a series of observations made over time for the object; selecting a plurality of sets of observations using the series of observations; causing the plurality of sets of observations to be used by at least one filter to generate a track for the object (wherein the at least one filter uses sensor data associated with each of a plurality of frames of sensor data only once during generation of the track); and causing operations of an autonomous robot (e.g., an autonomous vehicle) to be controlled based on the track for the object.</p><p id="p-0006" num="0005">In some scenarios, each observation is defined by a cuboid comprising LiDAR data points and/or is associated with a single frame of sensor data. Each set may be selected to comprise observations that are different from observations contained in all other sets. The filter can include, but is not limited to, a Kalman filter.</p><p id="p-0007" num="0006">In those or other scenarios, the methods comprise causing the plurality of sets of observations to be used by a first filter and a second filter to generate the track for the object. Each of the first and second filters uses sensor data associated with each of the frames of sensor data only once during generation of the track. The track may comprise a sequence of object states sequentially generated by the first and second filters using respective ones of the plurality of sets of observations. At least two of the different sets of observations may comprise at least one observation in common. The sets of observations are selected from the series of observations to ensure that the filter(s) use(s) sensor data associated with each of the frames of sensor data only once during generation of the track.</p><p id="p-0008" num="0007">The implementing systems comprise a processor and a non-transitory computer-readable storage medium comprising programming instructions that are configured to cause the processor to implement the above described methods.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008">The present solution will be described with reference to the following drawing figures, in which like numerals represent like items throughout the figures.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> provides an illustration of a LiDAR system.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> provides an illustration of a LiDAR device.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> provides an illustration of a tracker.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. <b>4</b>-<b>5</b></figref> provides tables that are useful for understanding operations of the tracker shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> provides a flow diagram of an illustrative method for tracking objects.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b></figref> provides an illustration of a system implementing the LiDAR system described in relation to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>6</b></figref>.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is an illustration of an architecture for a vehicle.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an illustration of a computing device.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> provides a block diagram that is useful for understanding how control of a vehicle is achieved in accordance with the present solution.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">As used in this document, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; include plural references unless the context clearly dictates otherwise. Unless defined otherwise, all technical and scientific terms used herein have the same meanings as commonly understood by one of ordinary skill in the art. As used in this document, the term &#x201c;comprising&#x201d; means &#x201c;including, but not limited to.&#x201d; Definitions for additional terms that are relevant to this document are included at the end of this Detailed Description.</p><p id="p-0020" num="0019">An &#x201c;electronic device&#x201d; or a &#x201c;computing device&#x201d; refers to a device that includes a processor and memory. Each device may have its own processor and/or memory, or the processor and/or memory may be shared with other devices as in a virtual machine or container arrangement. The memory will contain or receive programming instructions that, when executed by the processor, cause the electronic device to perform one or more operations according to the programming instructions.</p><p id="p-0021" num="0020">The terms &#x201c;memory,&#x201d; &#x201c;memory device,&#x201d; &#x201c;data store,&#x201d; &#x201c;data storage facility&#x201d; and the like each refer to a non-transitory device on which computer-readable data, programming instructions or both are stored. Except where specifically stated otherwise, the terms &#x201c;memory,&#x201d; &#x201c;memory device,&#x201d; &#x201c;data store,&#x201d; &#x201c;data storage facility&#x201d; and the like are intended to include single device embodiments, embodiments in which multiple memory devices together or collectively store a set of data or instructions, as well as individual sectors within such devices.</p><p id="p-0022" num="0021">The terms &#x201c;processor&#x201d; and &#x201c;processing device&#x201d; refer to a hardware component of an electronic device that is configured to execute programming instructions. Except where specifically stated otherwise, the singular term &#x201c;processor&#x201d; or &#x201c;processing device&#x201d; is intended to include both single-processing device embodiments and embodiments in which multiple processing devices together or collectively perform a process.</p><p id="p-0023" num="0022">The term &#x201c;vehicle&#x201d; refers to any moving form of conveyance that is capable of carrying either one or more human occupants and/or cargo and is powered by any form of energy. The term &#x201c;vehicle&#x201d; includes, but is not limited to, cars, trucks, vans, trains, autonomous vehicles, aircraft, aerial drones and the like. An &#x201c;autonomous vehicle&#x201d; is a vehicle having a processor, programming instructions and drivetrain components that are controllable by the processor without requiring a human operator. An autonomous vehicle may be fully autonomous in that it does not require a human operator for most or all driving conditions and functions, or it may be semi-autonomous in that a human operator may be required in certain conditions or for certain operations, or a human operator may override the vehicle's autonomous system and may take control of the vehicle, or it may be a human-operated vehicle equipped with an advanced driver assistance system.</p><p id="p-0024" num="0023">In this document, when terms such as &#x201c;first&#x201d; and &#x201c;second&#x201d; are used to modify a noun, such use is simply intended to distinguish one item from another, and is not intended to require a sequential order unless specifically stated. In addition, terms of relative position such as &#x201c;vertical&#x201d; and &#x201c;horizontal&#x201d;, or &#x201c;front&#x201d; and &#x201c;rear&#x201d;, when used, are intended to be relative to each other and need not be absolute, and only refer to one possible position of the device associated with those terms depending on the device's orientation.</p><p id="p-0025" num="0024">Conventional object detection algorithms based on deep learning may operate on a temporal window of sensor data rather than on a single frame of sensor data. These object detection algorithms suffer from certain drawbacks. For example, object detections output from such algorithms are correlated over time which directly violates a foundational assumption of efficient estimators operating on those object detections. The present solution solves this drawback of convention object detection algorithms.</p><p id="p-0026" num="0025">The present solution concerns implementing systems and methods for tracking an object and/or controlling an autonomous robot. The methods generally involve performing the following operations by a processor or circuit: receiving a series of observations made over time for the object; selecting a plurality of sets of observations using the series of observations; causing the plurality of sets of observations to be used by at least one filter to generate a track for the object (wherein the at least one filter uses sensor data associated with each of a plurality of frames of sensor data only once during generation of the track); and/or causing operations of an autonomous robot (e.g., an autonomous vehicle) to be controlled based on the track for the object.</p><p id="p-0027" num="0026">The present solution can be used in various applications. Such applications can include, but are not limited to, radar system applications, LiDAR system applications, camera system applications, robotic applications, autonomous vehicle applications, and/or semi-autonomous vehicle applications. The present solution will be described below in relation to LiDAR system applications and autonomous vehicle. The present solution is not limited in this regard.</p><p id="p-0028" num="0027">Illustrative LiDAR Systems</p><p id="p-0029" num="0028">The present solution concerns a LiDAR system for detecting the presence, direction, distance and speed of objects, which may include moving actors, stationary objects, or both. The objects can include vehicles, ships, aircrafts, pedestrians, animals, trees and/or buildings. An illustration of an illustrative LiDAR system <b>100</b> is provided in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the LiDAR system <b>100</b> comprises LiDAR device(s) <b>102</b>. An illustrative architecture for a LiDAR device will be discussed below in relation to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The LiDAR system <b>100</b> also comprises an object detector <b>106</b>, a data processor <b>110</b> and a tracker <b>114</b>. The object detector <b>106</b>, data processor <b>110</b> and/or tracker <b>114</b> can include, but is(are) not limited to, a computing device such as that shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0030" num="0029">During operation, the LiDAR device(s) <b>102</b> perform(s) operations to generate frames of sensor data (e.g., point cloud data) f<sub>1</sub>, f<sub>2</sub>, . . . , f<sub>n</sub>. Techniques for generating frames of sensor data are well known. The frames are passed to the object detector <b>106</b>. The object detector <b>106</b> processes the frames of sensor data to (i) identify object(s) in proximity to the LiDAR device(s) and (ii) generate a temporal sequence of observations or a series of observations made over time c<sub>1</sub>, c<sub>2</sub>, . . . , c<sub>n </sub>for each detected object. The observations can include, but are not limited to, cuboids comprising data points in the LiDAR point clouds. Techniques for object detection based on LiDAR datasets are well known. Techniques for generating cuboids and temporal sequences of cuboids are well known. Each cuboid specifies a position, size, orientation and/or velocity of a given object. For example, a cuboid may be defined by an x-axis coordinate for a centroid, a y-axis coordinate for the centroid, a z-axis coordinate for the centroid, a roll value, a pitch value, a yaw value, an x-axis coordinate for a linear velocity, a y-axis coordinate for the linear velocity, and a z-axis coordinate for the linear velocity.</p><p id="p-0031" num="0030">The temporal sequence of observations c<sub>1</sub>, c<sub>2</sub>, . . . , c<sub>n </sub>for each detected object is passed to the data processor <b>110</b>. The data processor <b>110</b> performs operations to determine whether the detected object has or has not been previously detected. This determination can be made based on information stored in a datastore indicating that prior observations were associated with a previously detected object (e.g., a pedestrian, a vehicle, an animal, etc.). If such information is not present in the datastore, then a determination is made that the detected object has not been previously detected. If the object has been previously detected, then the temporal sequence of observations is associated with the previously detected object. Once this object association is made, the temporal sequence of observations is passed to tracker <b>114</b>.</p><p id="p-0032" num="0031">Tracker <b>114</b> uses the temporal sequence of observations to determine a track T for the given detected object. The manner in which the track is determined will be discussed in detail below in relation to <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>5</b></figref>. The track T provides a spatial description for the given object. The track (or spatial description) includes a temporal sequence of states x<sub>1</sub>, x<sub>2</sub>, . . . , x<sub>t </sub>for the given object. Each state may be defined by a centroid location (e.g., an x-coordinate, a y-coordinate, a z-coordinate), a roll value, a pitch value, a yaw value, and/or a velocity (i.e., speed plus a direction of travel).</p><p id="p-0033" num="0032">Referring now to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, there is provided an illustration of an illustrative LiDAR device <b>200</b>. LiDAR device <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is the same as or similar to LiDAR device <b>200</b>. Thus, the following discussion of LiDAR device <b>200</b> is sufficient for understanding LiDAR device <b>102</b>.</p><p id="p-0034" num="0033">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the LiDAR system <b>200</b> includes a housing <b>206</b> which may be rotatable 360&#xb0; about a central axis such as hub or axle <b>216</b>. The housing may include an emitter/receiver aperture <b>212</b> made of a material transparent to light. Although a single aperture is shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the present solution is not limited in this regard. In other scenarios, multiple apertures for emitting and/or receiving light may be provided. Either way, the LiDAR system <b>200</b> can emit light through one or more of the aperture(s) <b>212</b> and receive reflected light back toward one or more of the aperture(s) <b>211</b> as the housing <b>206</b> rotates around the internal components. In alternative scenarios, the outer shell of housing <b>206</b> may be a stationary dome, at least partially made of a material that is transparent to light, with rotatable components inside of the housing <b>206</b>.</p><p id="p-0035" num="0034">Inside the rotating shell or stationary dome is a light emitter system <b>204</b> that is configured and positioned to generate and emit pulses of light through the aperture <b>212</b> or through the transparent dome of the housing <b>206</b> via one or more laser emitter chips or other light emitting devices. The light emitter system <b>204</b> may include any number of individual emitters (e.g., 8 emitters, 64 emitters, or 128 emitters). The emitters may emit light of substantially the same intensity or of varying intensities. The individual beams emitted by the light emitter system <b>204</b> will have a well-defined state of polarization that is not the same across the entire array. As an example, some beams may have vertical polarization and other beams may have horizontal polarization. The LiDAR system will also include a light detector <b>208</b> containing a photodetector or array of photodetectors positioned and configured to receive light reflected back into the system. The light emitter system <b>204</b> and light detector <b>208</b> would rotate with the rotating shell, or they would rotate inside the stationary dome of the housing <b>206</b>. One or more optical element structures <b>210</b> may be positioned in front of the light emitter system <b>204</b> and/or the light detector <b>208</b> to serve as one or more lenses or wave plates that focus and direct light that is passed through the optical element structure <b>210</b>.</p><p id="p-0036" num="0035">One or more optical element structures <b>210</b> may be positioned in front of a mirror <b>212</b> to focus and direct light that is passed through the optical element structure <b>210</b>. As shown below, the system includes an optical element structure <b>210</b> positioned in front of the mirror <b>212</b> and connected to the rotating elements of the system so that the optical element structure <b>210</b> rotates with the mirror <b>212</b>. Alternatively or in addition, the optical element structure <b>210</b> may include multiple such structures (for example lenses and/or wave plates). Optionally, multiple optical element structures <b>210</b> may be arranged in an array on or integral with the shell portion of the housing <b>206</b>.</p><p id="p-0037" num="0036">Optionally, each optical element structure <b>210</b> may include a beam splitter that separates light that the system receives from light that the system generates. The beam splitter may include, for example, a quarter-wave or half-wave wave plate to perform the separation and ensure that received light is directed to the receiver unit rather than to the emitter system (which could occur without such a wave plate as the emitted light and received light should exhibit the same or similar polarizations).</p><p id="p-0038" num="0037">The LiDAR system will include a power unit <b>218</b> to power the light emitter system <b>204</b>, a motor <b>216</b>, and electronic components. The LiDAR system will also include an analyzer <b>214</b> with elements such as a processor <b>222</b> and non-transitory computer-readable memory <b>220</b> containing programming instructions that are configured to enable the system to receive data collected by the light detector unit, analyze it to measure characteristics of the light received, and generate information that a connected system can use to make decisions about operating in an environment from which the data was collected. Optionally, the analyzer <b>214</b> may be integral with the LiDAR system <b>200</b> as shown, or some or all of it may be external to the LiDAR system and communicatively connected to the LiDAR system via a wired or wireless communication network or link.</p><p id="p-0039" num="0038">Referring now to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, there is provided an illustration of a tracker architecture <b>300</b>. Tracker <b>112</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can be the same as or similar to tracker <b>300</b>. Thus, the discussion of tracker <b>300</b> is sufficient for understanding tracker <b>112</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0040" num="0039">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, tracker <b>300</b> comprises an observation selector <b>302</b>, switches <b>304</b>, <b>308</b>, filters <b>306</b><sub>1</sub>, <b>306</b><sub>2</sub>, . . . , <b>306</b><sub>N</sub>, a track generator <b>310</b>, and a controller <b>312</b>. Controller <b>312</b> is operative to control operations of components <b>302</b>, <b>304</b>, <b>308</b>. For example, controller <b>312</b> provides: control signals to observation selector <b>302</b> to facilitate selection of observation (e.g., cuboids) from a temporal sequence of observations or series of observations made over time c<sub>1</sub>, c<sub>2</sub>, . . . , c<sub>n </sub>input into the tracker <b>300</b>; provides control signals to switch <b>304</b> for selectively coupling the observation selector <b>302</b> to filters <b>306</b><sub>1</sub>, <b>306</b><sub>2</sub>, . . . , <b>306</b><sub>N</sub>; and provides control signals to switch <b>308</b> for selectively coupling <b>306</b><sub>1</sub>, <b>306</b><sub>2</sub>, . . . , <b>306</b><sub>N </sub>to the track generator <b>310</b>.</p><p id="p-0041" num="0040">During each cycle of operations performed by tracker <b>300</b>, switch <b>304</b> is controlled such that select observations (e.g., cuboids) are provided to and processed by filters <b>306</b><sub>1</sub>, <b>306</b><sub>2</sub>, . . . , <b>306</b><sub>N </sub>in a sequential manner. For example, filter <b>306</b><sub>1 </sub>is provided with a first set of observations (e.g., cuboids) from observation selector <b>302</b> via switch <b>304</b> during a first cycle. Filter <b>306</b><sub>1 </sub>processes the observations to generate an object state x<sub>t </sub>at time t, which is provided to the track generator <b>310</b> via switch <b>308</b>. Next, filter <b>306</b><sub>2 </sub>is provided a second different set of observations (e.g., cuboids) from observation selector <b>302</b> via switch <b>304</b> and processes the same to generate an object state x<sub>t+1 </sub>at time t+1, which is provided to the track generator <b>310</b> via switch <b>308</b>. This process is repeated until track generator <b>310</b> has received an object state x<sub>t+N </sub>from filter <b>306</b><sub>N</sub>. A next cycle of operations for the given object or another object is performed by tracker <b>300</b>. The track generator <b>310</b> generates a track T by arranging the object states as a sequence of object states {x<sub>t</sub>, x<sub>t+1</sub>, . . . , x<sub>t+N</sub>}.</p><p id="p-0042" num="0041">Notably, the observations (e.g., cuboids) are selected by the observation selector <b>302</b> such that each filter does not process LiDAR data from the same frame twice. This is different than what is done in conventional trackers. In conventional trackers, the filters process LiDAR data from the same frame multiple times which results in the above-mentioned drawbacks which are solved by the present solution.</p><p id="p-0043" num="0042">The observation selection process of the present solution will now be described in detail in relation to a scenario of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In this scenario, the tracker <b>300</b> comprises three filters <b>306</b><sub>1</sub>, <b>306</b><sub>2</sub>, <b>306</b><sub>3</sub>. Thus, switch <b>304</b> has three positions: a first position connecting observation selector <b>302</b> to filter <b>306</b><sub>1</sub>; a second position connecting observation selector <b>302</b> to filter <b>306</b><sub>2</sub>; and a third position connecting observation selector <b>302</b> to filter <b>306</b><sub>3</sub>.</p><p id="p-0044" num="0043">The switch <b>304</b> is cycled through these three positions. Each cycle C<sub>1</sub>, C<sub>2</sub>, . . . , C<sub>M </sub>referenced in table <b>400</b> refers to an iteration of the switch <b>304</b> being cycled through the three positions.</p><p id="p-0045" num="0044">During a first cycle C<sub>1</sub>, observation selector <b>302</b> selects a first set of observations (e.g., cuboids) c<sub>1</sub>, c<sub>2</sub>, c<sub>3 </sub>which are associated with frames of sensor data f<sub>1</sub>, f<sub>2</sub>, f<sub>3</sub>. The selected observations (e.g., cuboids) c<sub>1</sub>, c<sub>2</sub>, c<sub>3 </sub>are provided to filter <b>306</b><sub>1 </sub>via switch <b>304</b>. Filter <b>306</b><sub>1 </sub>processes the observations (e.g., cuboids) c<sub>1</sub>, c<sub>2</sub>, c<sub>3 </sub>to generate an object state x<sub>t </sub>at time t, which is provided to the track generator <b>310</b> via switch <b>308</b>. Thereafter, observation selector <b>302</b> selects a second different set of observations (e.g., cuboids) c<sub>2</sub>, c<sub>3</sub>, c<sub>4 </sub>which are associated with frames of sensor data f<sub>2</sub>, f<sub>3</sub>, f<sub>4</sub>. The selected observations (e.g., cuboids) c<sub>2</sub>, c<sub>3</sub>, c<sub>4 </sub>are provided to filter <b>306</b><sub>2 </sub>via switch <b>304</b>. Filter <b>306</b><sub>2 </sub>processes the observations (e.g., cuboids) c<sub>2</sub>, c<sub>3</sub>, c<sub>4 </sub>to generate an object state x<sub>t+1 </sub>at time t+1, which is provided to the track generator <b>310</b> via switch <b>308</b>. Subsequently, observation selector <b>302</b> selects a third different set of observations (e.g., cuboids) c<sub>3</sub>, c<sub>4</sub>, c<sub>5 </sub>which are associated with frames of sensor data f<sub>3</sub>, f<sub>4</sub>, f<sub>5</sub>. The selected observations (e.g., cuboids) c<sub>3</sub>, c<sub>4</sub>, c<sub>5 </sub>are provided to filter <b>306</b><sub>3 </sub>via switch <b>304</b>. Filter <b>306</b><sub>3 </sub>processes the observations (e.g., cuboids) c<sub>3</sub>, c<sub>4</sub>, c<sub>5 </sub>to generate an object state x<sub>t+2 </sub>at time t+2, which is provided to the track generator <b>310</b> via switch <b>308</b>.</p><p id="p-0046" num="0045">During a second cycle C<sub>2</sub>, observation selector <b>302</b> selects a fourth set of observations (e.g., cuboids) c<sub>4</sub>, c<sub>5</sub>, c<sub>6 </sub>which are associated with frames of sensor data f<sub>4</sub>, f<sub>6</sub>. The selected observations (e.g., cuboids) c<sub>4</sub>, c<sub>5</sub>, c<sub>6 </sub>are provided to filter <b>306</b><sub>1 </sub>via switch <b>304</b>. Filter <b>306</b><sub>1 </sub>processes the observations (e.g., cuboids) c<sub>4</sub>, c<sub>5</sub>, c<sub>6 </sub>to generate an object state x<sub>t+3 </sub>at time t+3, which is provided to the track generator <b>310</b> via switch <b>308</b>. Thereafter, observation selector <b>302</b> selects a fifth different set of observations (e.g., cuboids) c<sub>5</sub>, c<sub>6</sub>, c<sub>7 </sub>which are associated with frames f<sub>5</sub>, f<sub>6</sub>, f<sub>7</sub>. The selected observations (e.g., cuboids) c<sub>5</sub>, c<sub>6</sub>, c<sub>7 </sub>are provided to filter <b>306</b><sub>2 </sub>via switch <b>304</b>. Filter <b>306</b><sub>2 </sub>processes the observations (e.g., cuboids) c<sub>5</sub>, c<sub>6</sub>, c<sub>7 </sub>to generate an object state x<sub>t+4 </sub>at time t+4, which is provided to the track generator <b>310</b> via switch <b>308</b>. Subsequently, observation selector <b>302</b> selects a sixth different set of observations (e.g., cuboids) c<sub>6</sub>, c<sub>7</sub>, c<sub>8 </sub>which are associated with frames f<sub>6</sub>, f<sub>7</sub>, f<sub>8</sub>. The selected observations (e.g., cuboids) c<sub>6</sub>, c<sub>7</sub>, c<sub>8 </sub>are provided to filter <b>306</b><sub>3 </sub>via switch <b>304</b>. Filter <b>306</b><sub>3 </sub>processes the observations (e.g., cuboids) c<sub>6</sub>, c<sub>7</sub>, c<sub>8 </sub>to generate an object state x<sub>t+5 </sub>at time t+5, which is provided to the track generator <b>310</b> via switch <b>308</b>.</p><p id="p-0047" num="0046">Notably, each of the filters processes the observations (e.g., cuboids) associated with different frames of point cloud data during cycles C<sub>1 </sub>and C<sub>2</sub>. For example, filter <b>306</b><sub>1 </sub>processes observations (e.g., cuboids) that are associated with frames f<sub>1</sub>, f<sub>2</sub>, f<sub>3 </sub>during cycle C<sub>1 </sub>and processes observations (e.g., cuboids) that are associated with frames f<sub>4</sub>, f<sub>5</sub>, f<sub>6 </sub>during cycle C<sub>2</sub>. This novel feature of the present solution provides more accurate object tracks as compared to that of conventional trackers in which each filter processes sensor data from the same frame during multiple cycles (i.e., point cloud data associated with frame f<sub>2 </sub>is processed by a particular filter during both cycles C<sub>1 </sub>and C<sub>2</sub>).</p><p id="p-0048" num="0047">In some scenarios, the filters <b>306</b><sub>1</sub>, <b>306</b><sub>2</sub>, . . . , <b>306</b><sub>N </sub>comprise Kalman filters. Kalman filters are well known as implementing an algorithm that provide estimates of unknown variables given observations made over time. In object detection applications, the unknown variables can include, but are not limited to, an object state x<sub>t </sub>(position and velocity) at a time t, an object position p<sub>t </sub>at a time t, and/or an object velocity v<sub>t </sub>at a time t. The object position and velocity are defined in an x-axis, a y-axis, and a z-axis. The object state may be defined as a cuboid with a greater degree of accuracy as compared to the cuboids input into the tracker <b>300</b>. Thus, each object state may include, but is not limited to, an x-axis coordinate for a centroid, a y-axis coordinate for the centroid, a z-axis coordinate for the centroid, a roll value, a pitch value, a yaw value, an x-axis coordinate for a linear velocity, a y-axis coordinate for the linear velocity, and a z-axis coordinate for the linear velocity.</p><p id="p-0049" num="0048">The object state at time t can be predicted based on the previous object state at time t&#x2212;1. This prediction of object state x<sub>t </sub>can be made in accordance with the following mathematical equations (1) and (2).</p><p id="p-0050" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i><sub>t|t-1</sub><i>=F</i><sub>t-1</sub><i>*x</i><sub>t-1</sub>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0051" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i><sub>t</sub><i>=K</i><sub>t</sub><i>c</i><sub>n</sub>+(1&#x2212;<i>K</i><sub>t</sub>)<i>x</i><sub>t|t-1</sub>&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0052" num="0000">where x<sub>t|t-1 </sub>denotes a predicted object state x<sub>t </sub>conditioned on measurements or observations through time t&#x2212;1, F<sub>t-1 </sub>denotes a matrix that encodes a linear function of a dynamics model (e.g., position p plus velocity v times delta t, i.e., p+v&#x394;t), x<sub>t-1 </sub>denotes an estimated object state at time t&#x2212;1, x<sub>t </sub>denotes an estimated object state at time t, K<sub>t </sub>denotes a Kalman gain, and c<sub>n </sub>denotes an observation (e.g., cuboid). The present solution is not limited to the mathematical equations (1) and (2). Other mathematical equations can be employed here for implementing a Kalman filter or other filter operative to predict or estimate an object state using sensor data.</p><p id="p-0053" num="0049">In the scenario of <figref idref="DRAWINGS">FIGS. <b>4</b>-<b>5</b></figref>, each filter <b>306</b><sub>1</sub>, <b>306</b><sub>2</sub>, <b>306</b><sub>3 </sub>could recursively perform mathematical equations (1) and (2) using the respective observations (e.g., cuboids) in sequence during each cycle. For example, filter <b>306</b><sub>1 </sub>sequentially performs three iterations of mathematical equations (1, 2) during cycle C1 to generate (i) object state x<sub>t-2 </sub>based on observation (e.g., cuboid) c<sub>1 </sub>and an initial object state x<sub>0</sub>, (ii) object state x<sub>t-1 </sub>based on observation (e.g., cuboid) c<sub>2 </sub>and previously computed object state x<sub>t-2</sub>, and (iii) object state x<sub>t </sub>based on observation (e.g., cuboid) c<sub>3 </sub>and previously computed object state x<sub>t-1</sub>. The present solution is not limited to the particulars of this scenario.</p><p id="p-0054" num="0050">An additional benefit of the present solution is that the tracker <b>300</b> still outputs state updates at the same frequency of the sensor, i.e. there is an updated x output for every f/c input. An alternative approach to &#x201c;decorrelate&#x201d; the input would be to have a single filter that only processes every N<sup>th </sup>set of cuboids (where N is 3 in the examples demonstrated in <figref idref="DRAWINGS">FIGS. <b>3</b>, <b>4</b>, <b>5</b></figref>). The downside to this alternative approach is that the frequency of updates would drop to 1/N times the sensor frequency, which would increase the reaction time of the autonomous vehicle to a potentially dangerous level.</p><p id="p-0055" num="0051">Referring now to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, there is provided an illustrative method <b>600</b> for operating an object tracking system (e.g., LiDAR system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). Method <b>600</b> begins with <b>602</b> and continues with <b>604</b> where frames of sensor data (e.g., frames of point cloud data f<sub>1</sub>, . . . , f<sub>n </sub>of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) are received by an object detector (e.g., object detector <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) from sensor device(s) (e.g., LiDAR device(s) <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and/or other sensor device(s) such as radar system(s)). The sensor data can include, but is not limited to, frames of point cloud information defining point clouds. Each point cloud comprises a plurality of data points plotted on a 3D graph. The object detector can include, but is not limited to, a computing device (such as that described below in relation to <figref idref="DRAWINGS">FIG. <b>9</b></figref>).</p><p id="p-0056" num="0052">In <b>606</b>, the object detector performs operations to detect one or more objects based on the sensor data. Techniques for detecting objects based on sensor data are well known. The object detector also generates a temporal sequence of observations (e.g., cuboids) for each detected object, as shown by <b>608</b>. Techniques for generating observations (e.g., cuboids) from sensor data are well known. The observations (e.g., cuboids) can be optionally be associated with a previously detected object in <b>610</b>. Each observation may be associated with a single frame of sensor data.</p><p id="p-0057" num="0053">Next in <b>612</b>, the temporal sequence or series of observations (e.g., cuboids) is provided to a tracker (e.g., tracker <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). At the tracker, sets of observations are selected in <b>614</b> using the temporal sequence or series of observations. The sets of observations are selected from the series of observations to ensure that each filter uses sensor data associated with each frames of sensor data only once during generation of the track for the given object. When a single filter is used to generate the track, each set may be selected to comprise observations that are different from observations contained in all other sets. When two or more filters are used to generate the track, at least two of the sets of observations may comprise at least one observation in common.</p><p id="p-0058" num="0054">In <b>616</b>, the sets of observations are used by filter(s) (e.g., a Kalman filter) to generate the track for the object. The filter(s) use(s) sensor data associated with each of the frames of sensor data only once during generation of the track. For example, the sets of observations are used by a first filter and a second filter to generate the track for the object. Each of the first and second filters uses sensor data associated with each frame of sensor data only once during generation of the track. The track may comprise a sequence of object states sequentially generated by the first and second filters using respective ones of the sets of observations. The present solution is not limited in this regard.</p><p id="p-0059" num="0055">In <b>618</b>, operations of an autonomous robot (e.g., an autonomous vehicle) may be optionally controlled based on the track for the object. For example, an articulating arm on the autonomous robot may be caused to move in a given way so as to capture the object or avoid the object based on the object's track. Additionally or alternatively, the autonomous robot is caused to travel along a given trajectory which avoids collision with the object based on the object's track. Subsequently, <b>620</b> is performed where method <b>600</b> ends or other operations are performed.</p><p id="p-0060" num="0056">Illustrative Vehicle Based Systems</p><p id="p-0061" num="0057">The above described LiDAR system <b>100</b> can be used in a plurality of applications. Such applications include, but are not limited to, vehicle based applications. The following discussion is provided to illustrate how the LiDAR system <b>100</b> of the present solution can be used to facilitate control of a vehicle (e.g., for collision avoidance and/or autonomous driving purposes). The vehicle can include, but is not limited to, an autonomous vehicle.</p><p id="p-0062" num="0058">Referring now to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, there is provided an illustration of an illustrative system <b>700</b>. System <b>700</b> comprises a vehicle <b>702</b><sub>1 </sub>that is traveling along a road in a semi-autonomous or autonomous manner. Vehicle <b>702</b><sub>1 </sub>is also referred to herein as an Autonomous Vehicle (&#x201c;AV&#x201d;). The AV <b>702</b><sub>1 </sub>can include, but is not limited to, a land vehicle (as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>), an aircraft, a watercraft, or a spacecraft.</p><p id="p-0063" num="0059">AV <b>702</b><sub>1 </sub>is generally configured to detect objects <b>702</b><sub>2</sub>, <b>704</b>, <b>706</b> in proximity thereto. The objects can include, but are not limited to, a vehicle <b>702</b><sub>2</sub>, cyclist <b>704</b> (such as a rider of a bicycle, electric scooter, motorcycle, or the like) and/or a pedestrian <b>706</b>. When such a detection is made, AV <b>702</b><sub>1 </sub>performs operations to: generate one or more possible object trajectories for the detected object; and analyze at least one of the generated possible object trajectories to determine a vehicle trajectory for AV <b>702</b><sub>1</sub>. The AV <b>702</b><sub>1 </sub>is then caused to follow the vehicle trajectory.</p><p id="p-0064" num="0060">Referring now to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, there is provided an illustration of an illustrative system architecture <b>800</b> for a vehicle. Vehicles <b>702</b><sub>1 </sub>and/or <b>702</b><sub>2 </sub>of <figref idref="DRAWINGS">FIG. <b>7</b></figref> can have the same or similar system architecture as that shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Thus, the following discussion of system architecture <b>800</b> is sufficient for understanding vehicle(s) <b>702</b><sub>1</sub>, <b>702</b><sub>2 </sub>of <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0065" num="0061">As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the vehicle <b>800</b> includes an engine or motor <b>802</b> and various sensors <b>804</b>-<b>818</b> measuring various parameters of the vehicle. In gas-powered or hybrid vehicles having a fuel-powered engine, the sensors may include, for example, an engine temperature sensor <b>804</b>, a battery voltage sensor <b>806</b>, an engine rotations per minute (RPM) sensor <b>808</b>, and a throttle position sensor <b>810</b>. If the vehicle is an electric or hybrid vehicle, then the vehicle may have an electric motor, and accordingly will have sensors such as a battery monitoring system <b>812</b> (to measure current, voltage and/or temperature of the battery), motor current <b>814</b> and temperature <b>816</b> sensors, and motor position sensors such as resolvers and encoders <b>818</b>.</p><p id="p-0066" num="0062">Operational parameter sensors that are common to both types of vehicles include, for example: a position sensor <b>836</b> such as an accelerometer, gyroscope and/or inertial measurement unit; a speed sensor <b>838</b>; and an odometer sensor <b>840</b>. The vehicle also may have a clock <b>842</b> that the system uses to determine vehicle time during operation. The clock <b>842</b> may be encoded into the vehicle on-board computing device, it may be a separate device, or multiple clocks may be available.</p><p id="p-0067" num="0063">The vehicle also will include various sensors that operate to gather information about the environment in which the vehicle is traveling. These sensors may include, for example: a location sensor <b>860</b> (e.g., a Global Positioning System (GPS) device); object detection sensors such as one or more cameras <b>862</b>; a LiDAR sensor system <b>866</b>; and/or a radar system <b>864</b>. LiDAR system <b>866</b> is the same as or similar to LiDAR system <b>100</b> discussed above in relation to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>7</b></figref>. As such, the above discussion of LiDAR system <b>100</b> is sufficient for understanding LiDAR system <b>866</b>. The sensors also may include environmental sensors <b>868</b> such as a precipitation sensor and/or ambient temperature sensor. The object detection sensors may enable the vehicle on-board computing device <b>820</b> to detect objects that are within a given distance range of the vehicle <b>800</b> in any direction, while the environmental sensors collect data about environmental conditions within the vehicle's area of travel.</p><p id="p-0068" num="0064">During operations, information is communicated from the sensors to the on-board computing device <b>820</b>. The on-board computing device <b>820</b> analyzes the data captured by the sensors, and optionally controls operations of the vehicle based on results of the analysis. For example, the on-board computing device <b>820</b> may control: braking via a brake controller <b>822</b>; direction via a steering controller <b>824</b>; speed and acceleration via a throttle controller <b>826</b> (in a gas-powered vehicle) or motor speed controller <b>828</b> (such as a current level controller in an electric vehicle); a differential gear controller <b>830</b> (in vehicles with transmissions); and/or other controllers.</p><p id="p-0069" num="0065">Geographic location information may be communicated from the location sensor <b>860</b> to the on-board computing device <b>820</b>, which may then access a map of the environment that corresponds to the location information to determine known fixed features of the environment such as streets, buildings, stop signs and/or stop/go signals. Captured images from the camera(s) <b>862</b> and/or object detection information captured from sensors such as LiDAR is communicated to the on-board computing device <b>820</b>. The object detection information and/or captured images are processed by the on-board computing device <b>820</b> to detect objects in proximity to the vehicle <b>800</b>. Any known or to be known technique for making an object detection based on sensor data and/or captured images can be used in the embodiments disclosed in this document.</p><p id="p-0070" num="0066">When such an object detection is made, the on-board computing device <b>820</b> performs operations to: generate one or more possible object trajectories for the detected object; and analyze at least one of the generated possible object trajectories to determine if there is a risk of a collision in a threshold period of time (e.g., 1 minute). If so, the on-board computing device <b>820</b> performs operations to determine whether the collision can be avoided if a given vehicle trajectory is followed by the vehicle <b>800</b> and any one of a plurality of dynamically generated emergency maneuvers is performed in a pre-defined time period (e.g., N milliseconds). If the collision can be avoided, then the on-board computing device <b>820</b> takes no action to change the vehicle trajectory or optionally causes the vehicle <b>800</b> to perform a cautious maneuver (e.g., mildly slows down). In contrast, if the collision cannot be avoided, then the on-board computing device <b>820</b> causes the vehicle <b>800</b> to immediately take an emergency maneuver (e.g., brakes and/or changes direction of travel).</p><p id="p-0071" num="0067">Referring now to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, there is provided an illustration of an architecture for a computing device <b>900</b>. The object detector <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, data processor <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, tracker <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, LiDAR system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, observation selector <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, controller <b>312</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, filters <b>306</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, track generator <b>310</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, LiDAR system <b>866</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, and/or vehicle on-board computing device <b>820</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref> is(are) at least partially the same as or similar to computing device <b>900</b>. As such, the discussion of computing device <b>900</b> is sufficient for understanding the listed components.</p><p id="p-0072" num="0068">Computing device <b>900</b> may include more or less components than those shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. However, the components shown are sufficient to disclose an illustrative solution implementing the present solution. The hardware architecture of <figref idref="DRAWINGS">FIG. <b>9</b></figref> represents one implementation of a representative computing device configured to operate a vehicle, as described herein. As such, the computing device <b>900</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref> implements at least a portion of the method(s) described herein.</p><p id="p-0073" num="0069">Some or all components of the computing device <b>900</b> can be implemented as hardware, software and/or a combination of hardware and software. The hardware includes, but is not limited to, one or more electronic circuits. The electronic circuits can include, but are not limited to, passive components (e.g., resistors and capacitors) and/or active components (e.g., amplifiers and/or microprocessors). The passive and/or active components can be adapted to, arranged to and/or programmed to perform one or more of the methodologies, procedures, or functions described herein.</p><p id="p-0074" num="0070">As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the computing device <b>900</b> comprises a user interface <b>902</b>, a Central Processing Unit (CPU) <b>906</b>, a system bus <b>910</b>, a memory <b>912</b> connected to and accessible by other portions of computing device <b>900</b> through system bus <b>910</b>, a system interface <b>960</b>, and hardware entities <b>914</b> connected to system bus <b>910</b>. The user interface can include input devices and output devices, which facilitate user-software interactions for controlling operations of the computing device <b>900</b>. The input devices include, but are not limited to, a physical and/or touch keyboard <b>950</b>. The input devices can be connected to the computing device <b>900</b> via a wired or wireless connection (e.g., a Bluetooth&#xae; connection). The output devices include, but are not limited to, a speaker <b>952</b>, a display <b>954</b>, and/or light emitting diodes <b>956</b>. System interface <b>960</b> is configured to facilitate wired or wireless communications to and from external devices (e.g., network nodes such as access points, etc.).</p><p id="p-0075" num="0071">At least some of the hardware entities <b>914</b> perform actions involving access to and use of memory <b>912</b>, which can be a Random Access Memory (RAM), a disk drive, flash memory, a Compact Disc Read Only Memory (CD-ROM) and/or another hardware device that is capable of storing instructions and data. Hardware entities <b>914</b> can include a disk drive unit <b>916</b> comprising a computer-readable storage medium <b>918</b> on which is stored one or more sets of instructions <b>920</b> (e.g., software code) configured to implement one or more of the methodologies, procedures, or functions described herein. The instructions <b>920</b> can also reside, completely or at least partially, within the memory <b>912</b> and/or within the CPU <b>906</b> during execution thereof by the computing device <b>900</b>. The memory <b>912</b> and the CPU <b>906</b> also can constitute machine-readable media. The term &#x201c;machine-readable media&#x201d;, as used here, refers to a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions <b>920</b>. The term &#x201c;machine-readable media&#x201d;, as used here, also refers to any medium that is capable of storing, encoding or carrying a set of instructions <b>920</b> for execution by the computing device <b>900</b> and that cause the computing device <b>900</b> to perform any one or more of the methodologies of the present disclosure.</p><p id="p-0076" num="0072">Referring now to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, there is provided a block diagram that is useful for understanding how vehicle control is achieved in accordance with the present solution. All of the operations performed in blocks <b>1002</b>-<b>1010</b> can be performed by the on-board computing device (e.g., vehicle on-board computing device <b>820</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) of a vehicle (e.g., AV <b>702</b><sub>1 </sub>of <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0077" num="0073">In block <b>1002</b>, a location of the vehicle is detected. This detection can be made based on sensor data output from a location sensor (e.g., location sensor <b>860</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) of the vehicle. This sensor data can include, but is not limited to, GPS data. The detected location of the vehicle is then passed to block <b>1006</b>.</p><p id="p-0078" num="0074">In block <b>1004</b>, an object is detected within proximity of the vehicle. This detection is made based on sensor data output from a LiDAR system (e.g., LiDAR system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and/or LiDAR system <b>866</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>), a radar system (e.g., radar system <b>864</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>), and/or a camera (e.g., camera <b>862</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) of the vehicle. The sensor data output from the LiDAR system includes, but is not limited to, a track (or spatial description) <b>1050</b> for the object. The track (or spatial description) <b>1050</b> is the same as or similar to track (or spatial description) T of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The sensor data is also used to determine one or more possible object trajectories for the detected object. The possible object trajectories can include, but are not limited to, the following trajectories:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0075">a trajectory defined by the object's actual speed (e.g., 1 mile per hour) and actual direction of travel (e.g., west);</li>        <li id="ul0002-0002" num="0076">a trajectory defined by the object's actual speed (e.g., 1 mile per hour) and another possible direction of travel (e.g., south, south-west, or X (e.g., 40&#xb0;) degrees from the object's actual direction of travel in a direction towards the AV) for the object;</li>        <li id="ul0002-0003" num="0077">a trajectory defined by another possible speed for the object (e.g., 2-10 miles per hour) and the object's actual direction of travel (e.g., west); and/or</li>        <li id="ul0002-0004" num="0078">a trajectory defined by another possible speed for the object (e.g., 2-10 miles per hour) and another possible direction of travel (e.g., south, south-west, or X (e.g., 40&#xb0;) degrees from the object's actual direction of travel in a direction towards the AV) for the object.<br/>The one or more possible object trajectories <b>1012</b> is(are) then passed to block <b>1006</b>.</li>    </ul>    </li></ul></p><p id="p-0079" num="0079">In block <b>1006</b>, a vehicle trajectory <b>1020</b> is generated using the information from blocks <b>1002</b> and <b>1004</b>. Techniques for determining a vehicle trajectory are well known in the art, and therefore will not be described herein. Any known or to be known technique for determining a vehicle trajectory can be used herein without limitation. In some scenarios, the vehicle trajectory <b>1020</b> is determined based on the location information from block <b>1002</b>, the object detection/trajectory information from block <b>1004</b>, and map information <b>1028</b> (which is pre-stored in a data store of the vehicle). The vehicle trajectory <b>1020</b> represents a smooth path that does not have abrupt changes that would otherwise provide passenger discomfort. The vehicle trajectory <b>1020</b> is then provided to block <b>1008</b>.</p><p id="p-0080" num="0080">In block <b>1008</b>, a steering angle and velocity command is generated based on the vehicle trajectory <b>1020</b>. The steering angle and velocity command is provided to block <b>1010</b> for vehicle dynamics control. The vehicle dynamics control cause the vehicle to decelerate, cause the vehicle to accelerate, and/or cause the vehicle to change its direction of travel.</p><p id="p-0081" num="0081">Although the present solution has been illustrated and described with respect to one or more implementations, equivalent alterations and modifications will occur to others skilled in the art upon the reading and understanding of this specification and the annexed drawings. In addition, while a particular feature of the present solution may have been disclosed with respect to only one of several implementations, such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Thus, the breadth and scope of the present solution should not be limited by any of the above described embodiments. Rather, the scope of the present solution should be defined in accordance with the following claims and their equivalents.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for tracking an object, comprising:<claim-text>receiving, by a processor, a series of observations made over time for the object;</claim-text><claim-text>selecting, by the processor, a plurality of sets of observations using the series of observations;</claim-text><claim-text>causing, by the processor, the plurality of sets of observations to be used by at least one filter to generate a track for the object, wherein the at least one filter uses sensor data associated with each of a plurality of frames of sensor data only once during generation of the track; and</claim-text><claim-text>causing, by the processor, operations of an autonomous robot to be controlled based on the track for the object.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each observation of said series of observations is defined by a cuboid comprising LiDAR data points.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each observation of said series of observations is associated with a single frame of the plurality of frames of sensor data.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each said set is selected to comprise observations that are different from observations contained in all other said sets.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one filter comprises a Kalman filter.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising causing the plurality of sets of observations to be used by a first filter and a second filter to generate the track for the object, wherein the at least one filter comprises the first and second filters and each of the first and second filters uses sensor data associated with each of the plurality of frames of sensor data only once during generation of the track.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the track comprises a sequence of object states sequentially generated by the first and second filters using respective ones of the plurality of sets of observations.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein at least two of the plurality of different sets of observations comprise at least one observation in common.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said plurality of sets of observations are selected from the series of observations to ensure that the at least one filter uses sensor data associated with each of a plurality of frames of sensor data only once during generation of the track.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the autonomous robot comprises an autonomous vehicle.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A system, comprising:<claim-text>a processor; and</claim-text><claim-text>a non-transitory computer-readable storage medium comprising programming instructions that are configured to cause the processor to implement a method for controlling an autonomous robot, wherein the programming instructions comprise instructions to:<claim-text>receive a series of observations made over time for the object;</claim-text><claim-text>select a plurality of sets of observations using the series of observations;</claim-text><claim-text>cause the plurality of sets of observations to be used by at least one filter to generate a track for the object, wherein the at least one filter uses sensor data associated with each of a plurality of frames of sensor data only once during generation of the track; and</claim-text><claim-text>cause operations of the autonomous robot to be controlled based on the track for the object.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein each observation of said series of observations is defined by a cuboid comprising LiDAR data points.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein each observation of said series of observations is associated with a single frame of the plurality of frames of sensor data.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein each said set is selected to comprise observations that are different from observations contained in all other said sets.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the at least one filter comprises a Kalman filter.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the programming instructions further comprise instructions to cause the plurality of sets of observations to be used by a first filter and a second filter to generate the track for the object, wherein said at least one filter comprises said first and second filters and each of the first and second filters uses sensor data associated with each of the plurality of frames of sensor data only once during generation of the track.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the track comprises a sequence of object states sequentially generated by the first and second filters using respective ones of the plurality of sets of observations.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein at least two of the plurality of different sets of observations comprise at least one observation in common.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein said plurality of sets of observations are selected from the series of observations to ensure that the at least one filter uses sensor data associated with each of a plurality of frames of sensor data only once during generation of the track.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A computer program product comprising a memory and programming instructions that are configured to cause a processor to:<claim-text>receive a series of observations made over time for an object;</claim-text><claim-text>select a plurality of sets of observations using the series of observations;</claim-text><claim-text>cause the plurality of sets of observations to be used by at least one filter to generate a track for the object, wherein the at least one filter uses sensor data associated with each of a plurality of frames of sensor data only once during generation of the track; and</claim-text><claim-text>cause operations of the autonomous robot to be controlled based on the track for the object.</claim-text></claim-text></claim></claims></us-patent-application>