<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000344A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000344</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17854607</doc-number><date>20220630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>TW</country><doc-number>110124054</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>113</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>136</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>13</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>225</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>113</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>136</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>13</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>002</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>2256</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23212</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>0008</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20032</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20036</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30041</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">OPHTHALMOLOGY INSPECTION DEVICE AND PUPIL TRACKING METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Medimaging Integrated Solution, Inc.</orgname><address><city>Hsinchu</city><country>TW</country></address></addressbook><residence><country>TW</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Lin</last-name><first-name>Yu Chian</first-name><address><city>Hsinchu</city><country>TW</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Li</last-name><first-name>Jyun-Hong</first-name><address><city>Hsinchu</city><country>TW</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Kuo</last-name><first-name>Yung-En</first-name><address><city>Hsinchu</city><country>TW</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Lee</last-name><first-name>Yu-Tsung</first-name><address><city>Hsinchu</city><country>TW</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A pupil tracking method includes: retrieving an external eye image of a subject, wherein the external eye image includes a pupil of the subject; performing an image preprocessing on the external eye image, wherein the image preprocessing includes performing a binary conversion on the external eye image to obtain a binary image; finding out a contour boundary of each feature in the binary image, and finding out a pupil feature based on a variance of a distance from the contour boundary of each feature to a corresponding reference point; fitting the contour boundary of the pupil feature by a boundary fitting method to find a center coordinate of the pupil feature. The abovementioned pupil tracking method can track the pupil of the subject's eyeball without using a stereo camera. An ophthalmology inspection device using the abovementioned pupil tracking method is also disclosed.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="80.60mm" wi="158.75mm" file="US20230000344A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="228.01mm" wi="131.74mm" orientation="landscape" file="US20230000344A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="221.49mm" wi="93.90mm" file="US20230000344A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="211.16mm" wi="96.18mm" file="US20230000344A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="173.31mm" wi="74.93mm" file="US20230000344A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="181.19mm" wi="63.92mm" file="US20230000344A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="228.01mm" wi="131.49mm" orientation="landscape" file="US20230000344A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="228.01mm" wi="130.89mm" orientation="landscape" file="US20230000344A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="228.01mm" wi="132.16mm" orientation="landscape" file="US20230000344A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading><heading id="h-0002" level="1">1. Field of the Invention</heading><p id="p-0002" num="0001">The present invention relates to an inspection device, particularly to an ophthalmology inspection device and a pupil tracking method.</p><heading id="h-0003" level="1">2. Description of the Prior Art</heading><p id="p-0003" num="0002">An ophthalmology inspection device is used to obtain the physiological information of the eyes of a subject. The physician will diagnose the disease based on the physiological information of the eyes. For example, a fundus camera can capture the images of a fundus; a tonometer can measure intraocular pressure; a corneal topography device can measure the topography of the surface of an eyeball; a refractometer can measure the diopter of an eyeball. While inspecting, the abovementioned ophthalmology inspection devices need to be aligned to the eyeball of the subject to acquire better inspection results. For example, a fundus camera needs to project the illumination light to the fundus through the pupil of the eyeball of a subject; the light reflected from the fundus enters the imaging system through the pupil to form images of the fundus. Therefore, the imaging system of a fundus camera needs to be exactly aligned to the pupil so as to acquire a larger view field and a better fundus image.</p><p id="p-0004" num="0003">A conventional fundus camera uses a stereo camera to recognize the pupil and acquire the coordinates of the center of the pupil. Then, a motor drives the optical lens to make the optical axis of the imaging system be coaxial with the pupil. Because of the stereo camera, the conventional fundus camera is more expensive, more bulky and heavier. Thus, the conventional fundus camera can only be used as a desk-top apparatus.</p><p id="p-0005" num="0004">Accordingly, it has become a target the concerned fields are eager to achieve: using a simple and compact design to enable ophthalmology inspection devices to track the pupil of a subject.</p><heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0006" num="0005">The present invention provides an ophthalmology inspection device and a pupil tracking method, wherein a pupil feature is found out from the external eye image, and a boundary fitting method is used to find a center coordinate of the pupil feature, whereby the present invention can track the pupil of the eyeball of a subject without using a stereo camera.</p><p id="p-0007" num="0006">In one embodiment, the pupil tracking method of the present invention comprises steps: using an ophthalmology inspection device to acquire an external eye image of a subject, wherein the external eye image includes a pupil of the subject; using the ophthalmology inspection device to perform an image preprocessing on the external eye image, wherein the image preprocessing includes performing a binary conversion on the external eye image to obtain a binary image; finding out a contour boundary of each feature in the binary image, and finding out a pupil feature based on a variance of a distance from the contour boundary of each feature to a corresponding reference point; fitting the contour boundary of the pupil feature in a boundary fitting method to find a center coordinate of the pupil feature.</p><p id="p-0008" num="0007">In one embodiment, the ophthalmology inspection device of the present invention comprises an illumination element, an image sensor, an imaging lens assembly and a signal processing element. The illumination element generates an illumination light beam to illuminate an external eye region of a subject. The image sensor receives the light beam reflected from the external eye region to generate an external eye image, wherein the external eye image includes a pupil of the subject. The imaging lens assembly is disposed at a light-input side of the image sensor to condense the reflected light and form an image to the image sensor. The signal processing element is electrically connected with the image sensor and performs a pupil tracking method. The pupil tracking method comprises steps: acquiring the external eye image output by the image sensor; performing an image preprocessing on the external eye image, wherein the image preprocessing includes performing a binary conversion on the external eye image to obtain a binary image; finding out a contour boundary of each feature in the binary image, and finding out a pupil feature based on a variance of a distance from the contour boundary of each feature to a corresponding reference point; fitting the contour boundary of the pupil feature in a boundary fitting method to find a center coordinate of the pupil feature, and calculating the deviation between an optical axis of the imaging lens assembly and the pupil of the subject according to the center coordinate of the pupil feature.</p><p id="p-0009" num="0008">The objective, technologies, features and advantages of the present invention will become apparent from the following description in conjunction with the accompanying drawings wherein certain embodiments of the present invention are set forth by way of illustration and example.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">The foregoing conceptions and their accompanying advantages of this invention will become more readily appreciated after being better understood by referring to the following detailed description, in conjunction with the accompanying drawings, wherein</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram schematically showing a fundus camera according to one embodiment of the present invention;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of a pupil tracking method according to one embodiment of the present invention;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of an image preprocessing of a pupil tracking method according to one embodiment of the present invention;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIGS. <b>4</b><i>a</i>-<b>4</b><i>d </i></figref>are images relating to a pupil tracking method according to one embodiment of the present invention;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIGS. <b>5</b><i>a </i>and <b>5</b><i>b </i></figref>are indication signals of a pupil tracking method according to one embodiment of the present invention;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram schematically showing a tonometer according to one embodiment of the present invention;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram schematically showing a corneal topography device according to one embodiment of the present invention; and</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram schematically showing an automatic refractometer according to one embodiment of the present invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading><p id="p-0019" num="0018">Various embodiments of the present invention will be described in detail below and illustrated in conjunction with the accompanying drawings. In addition to these detailed descriptions, the present invention can be widely implemented in other embodiments, and apparent alternations, modifications and equivalent changes of any mentioned embodiments are all included within the scope of the present invention and based on the scope of the Claims. In the descriptions of the specification, in order to make readers have a more complete understanding about the present invention, many specific details are provided; however, the present invention may be implemented without parts of or all the specific details. In addition, the well-known steps or elements are not described in detail, in order to avoid unnecessary limitations to the present invention. Same or similar elements in Figures will be indicated by same or similar reference numbers. It is noted that the Figures are schematic and may not represent the actual size or number of the elements. For clearness of the Figures, some details may not be fully depicted.</p><p id="p-0020" num="0019">The ophthalmology inspection device of the present invention uses an imaging system to acquire an external eye image of a subject and uses a pupil tracking method to find out a pupil feature from the external eye image and the center coordinate of the pupil, whereby the ophthalmology inspection device may be exactly aligned to the pupil of the subject. Below, a fundus camera is used to demonstrate the ophthalmology inspection device and the pupil tracking method of the present invention.</p><p id="p-0021" num="0020">Refer to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In one embodiment of the present invention, a fundus camera <b>10</b><i>a </i>comprises an illumination element <b>101</b>, an imaging lens assembly <b>103</b>, an image sensor <b>104</b> and a signal processing element <b>106</b>. The illumination element <b>101</b> generates an illumination light beam to illuminate an external eye region of an eyeball <b>20</b> of a subject. In the embodiment of the fundus camera <b>10</b><i>a</i>, the illumination element <b>101</b> generates an illumination light beam. The illumination light beam is condensed by an object lens <b>102</b>, then passing through the eyeball <b>20</b> and illuminating a fundus <b>21</b>. In the embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the illumination element <b>101</b> is deviated from an optical axis O<b>1</b> of the imaging lens assembly <b>103</b>. However, the present invention is not limited by <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In other embodiments, the illumination element may provide annular illumination light. In one embodiment, the illumination element includes a condenser lens, a baffle board with an annular opening, a relay lens, and a reflector with a circular opening, whereby to provide annular illumination light. In one embodiment, the illumination element <b>101</b> includes at least one visible light-emitting element and at least one infrared light-emitting element. The infrared light generated by the infrared light-emitting element may be used to search the fundus. The visible light generated by the visible light-emitting element nay be used as the light source for photography. Besides, the visible light or the infrared light may be used as the illumination light for photographing the external eye region.</p><p id="p-0022" num="0021">The imaging lens assembly <b>103</b> is disposed at the light-input side of the image sensor <b>104</b>, condensing the light reflected by the external eye region and forming an image to the image sensor <b>104</b>. The image sensor <b>104</b> receives the light reflected by the external eye region to generate an external eye image, wherein the external eye image includes a pupil of a subject, as shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>a</i></figref>. In one embodiment, the fundus camera <b>10</b><i>a </i>of the present invention further comprises an imaging focal length-adjusting element <b>105</b>. In cooperation with the imaging focal length-adjusting element <b>105</b>, the fundus camera may be operated to make the reflected light L<b>1</b> of the external eye region or the fundus of the eyeball <b>20</b> image on the image sensor. For example, the imaging focal length-adjusting element <b>105</b> may use an electric motor or a mechanical structure to drive the image sensor <b>104</b> to move physically along the optical axis O<b>1</b>, whereby to adjust the focal length. Alternatively, the imaging focal length-adjusting element <b>105</b> may use an electric motor or a mechanical structure to drive at least one lens of the imaging lens assembly <b>103</b> to move physically along the optical axis O<b>1</b>, whereby to adjust the focal length. In one embodiment, the imaging lens assembly <b>103</b> includes at least one liquid-state lens; the imaging focal length-adjusting element <b>105</b> may adjust the curvature of the liquid-state lens of the imaging lens assembly <b>103</b> to adjust the focal length.</p><p id="p-0023" num="0022">The signal processing element <b>106</b> is electrically connected with the image sensor <b>104</b> and executes a pupil tracking method according to the external eye image output by the image sensor <b>104</b> to find out the pupil and the center coordinate of the pupil. Refer to <figref idref="DRAWINGS">FIG. <b>2</b></figref> for a flowchart of a pupil tracking method according to one embodiment of the present invention. In Step S<b>21</b>, the signal processing element <b>106</b> acquires an external eye image output by the image sensor <b>104</b>. In one embodiment, the features, such as the pupil and the sclera, would not be underexposed or overexposed under an appropriate illumination condition or an appropriate exposure condition, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref><i>a. </i></p><p id="p-0024" num="0023">Next, in Step S<b>22</b>, perform an image preprocessing, such as a binary conversion, to acquire a binary image. In one embodiment, the image preprocessing step (Step S<b>22</b>) further comprises a plurality of image processing steps, whereby to reduce the computation amount of the image processing or raise the quality of the binary image. Refer to <figref idref="DRAWINGS">FIG. <b>3</b></figref> for a flowchart of an image preprocessing according to one embodiment of the present invention. Firstly, in Step S<b>221</b>, compress the size of the external eye image to reduce the computation amount of the succeeding image processing. For example, each of the length and width of the external eye image is reduced to a half of the original size, whereby the computation amount may be reduced to a quarter of the original computation amount. Next, in Step S<b>222</b>, perform a noise reduction treatment of the external eye image, whereby to eliminate the grain-like noises of the external eye image. The grain-like noises mainly originate from the image sensor. For example, while the image sensor is affected by the ambient temperature, the image is likely to have grain-like noises. In one embodiment, the noise reduction device may be an average filter, a Gaussian filter, a median filter, or a bilateral filter. The algorithms of all the abovementioned noise reduction devices perform a smooth operation of the signals of the image. Among them, the bilateral filter and the median filter can preserve the boundaries of the image. Thus, the bilateral filter and the median filter are suitable to be used in the images whose boundary information is important. For example, the median filter uses the center of a window having a size of m&#xd7;n to scan all the pixels of the external eye image; for each pixel, the median of the gray levels of the corresponding m&#xd7;n window is output to function as a new gray level of the pixel, whereby the gray levels of neighboring pixels are similar, wherefore is achieved the effect of eliminating noises.</p><p id="p-0025" num="0024">Next, in Step S<b>223</b>, perform an image enhancing treatment of the external eye image to enhance the boundary of the image. The image enhancing treatment may enhance the difference of the gray levels of the image boundary. In other words, the image enhancing treatment may enhance the boundary of the pupil feature. Thereby, while the pupil feature is extracted, the complete information of the boundary of the pupil can be acquired. Further, the image enhancing treatment may adapt to the variance resulting from different photographing environments or different races. In one embodiment, the image enhancing treatment may be realized by a gamma correction technology, a histogram equalization technology, a homomorphic filter technology, an unsharp masking technology, or a combination thereof. For example, the unsharp masking technology performs a Gaussian filtering treatment or a low-pass filtering treatment on the external eye image to acquire a blurred image, subtracts the blurred image from the original image to acquire a boundary image, and performs a linear combination of the boundary image and the original image to acquire a boundary-enhanced image.</p><p id="p-0026" num="0025">Next, in Step S<b>224</b>, perform a binary conversion on the external eye image to acquire a binary image. According to the extent of boundary enhancement by the unsharp masking technology, select an optimized value as the threshold t of the binary conversion. If the gray level p of a pixel after boundary enhancement is greater than the threshold t, assign a value of 255 to the gray level of the pixel. If the gray level p of a pixel after boundary enhancement is smaller than the threshold t, assign a value of 0 to the gray level of the pixel. After the binary conversion of all the pixels is completed, a binary image is obtained.</p><p id="p-0027" num="0026">It is easily understood: after the binary conversion, defects (such as boundary discontinuities) or small-area noises may appear. In one embodiment, an opening operation of morphology (Step S<b>225</b>) is performed on the binary image generated by Step S<b>224</b> to compensate for defects or small-area noises, which are generated in the binary conversion. The opening operation of morphology firstly performs erosion on the binary image to remove small-area noises and then performs dilation on the binary image to restore the shape, whereby to acquire a noise-filtered and revamped binary image, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref><i>b. </i></p><p id="p-0028" num="0027">Return to the description of the pupil tracking method shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. After the binary image has been acquired, the process proceeds to finding out the boundary contour of each feature and finding out a pupil feature in Step S<b>23</b>. It is easily understood: the binary image includes non-pupil features, such as eyelashes, an eyelid, and shadows, as shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>b</i></figref>. Therefore, it is necessary to find out a pupil feature from the contour boundaries of the features of the binary image. In one embodiment, the pupil feature is found out according to the variance of the distance between the boundary contour of each feature and a corresponding reference point. In one embodiment, a contour searching method is used to acquire the contour boundary of each feature from the binary image, i.e. the information of the coordinates of the boundary contour, and stores the information in a vector-type point set of a 2D coordinate system. Next, a minimum enclosing circle method is used to perform circle fitting on the contour shape of each feature. For example, an iteration method is used to find a minimum circle able to enclose a given 2D-coordinate point set, whereby each feature may acquire a minimum enclosing circle covering the contour of the feature, and whereby the center and radius of the minimum enclosing circle are also acquired. Then, use the center of the minimum enclosing circle as a reference point, and calculate the variance of the distance between the reference point and the contour boundary of the corresponding feature. It is easily understood: the more the contour boundary of a feature approaches a circle, the more the variance of the distance between the reference point and the contour boundary of the feature approaches zero. Therefore, an appropriate preset value may be used to filter out non-pupil features and find out a pupil feature, as shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>c</i></figref>. For example, while the variance of the distance between the reference point and the contour boundary of a feature is smaller than or equal to 3, the feature is a pupil feature.</p><p id="p-0029" num="0028">Next, in Step S<b>24</b>, use a boundary fitting method to fit the contour boundary of the pupil feature found in Step S<b>23</b> to find out a center coordinate of the pupil feature, as shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>d</i></figref>. In one embodiment, the boundary fitting method may be a circle fitting method, an ellipse fitting method, or a minimum enclosing circle method. After the fitting shape of the contour boundary of the pupil feature is acquired, the center of the fitting shape can be worked out to obtain the center coordinate of the pupil feature. In one embodiment, a least square method is used to calculate the center of the fitting shape. In the example using the ellipse fitting method, calculating the least sum of the square distances between the center of the ellipse and the coordinate points of the boundary can obtain a set of center and radius, which have the minimum error, whereby the pupil can be precisely tracked. According to the center coordinate of the pupil feature, the signal processing element <b>106</b> may work out the deviation between an optical axis of the imaging lens assembly and the eyeball <b>20</b> of a subject. According to the deviation, a motor (not shown in the drawing) automatically drives the imaging system to be exactly aligned to the pupil of the eyeball <b>20</b>.</p><p id="p-0030" num="0029">Refer to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In one embodiment, the signal processing element <b>106</b> generates an indication signal according to the deviation (Step S<b>25</b>). According to the indication signal, the operator may adjust the relative position of the ophthalmology inspection device and the subject to make the ophthalmology inspection device be exactly aligned to the pupil of the eyeball of the subject. Refer to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, <figref idref="DRAWINGS">FIG. <b>5</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>5</b><i>b</i></figref>. In one embodiment, the fundus camera <b>10</b><i>a </i>further comprises an external display device <b>108</b>. The external display device <b>108</b> is coupled to the signal processing element <b>106</b> through a communication interface <b>107</b> in a wired or wireless way. According to the calculated deviation, the signal processing element <b>106</b> presents an indication signal <b>51</b> on the external display device <b>108</b>. The indication signal <b>51</b> will guide the operator to adjust the relative position of the ophthalmology inspection device and the subject. As shown in <figref idref="DRAWINGS">FIG. <b>5</b><i>b</i></figref>, while the indication signal <b>51</b> coincides with a preset position <b>50</b>, it means that the ophthalmology inspection device is exactly aligned to the pupil of the eyeball <b>20</b> of the subject. In one embodiment, while the indication signal <b>51</b> coincides with the preset position <b>50</b>, the indication signal <b>51</b> changes its color to remind the operator to capture the image of the fundus. Through the abovementioned structure, an inexperienced operator can also operate the fundus camera <b>10</b><i>a </i>of the present invention to capture the image of the fundus of the subject. Even though the fundus camera <b>10</b><i>a </i>is in a handheld form, it can also acquire superior fundus images.</p><p id="p-0031" num="0030">In one embodiment, the subject himself may operate the ophthalmology inspection device of the present invention according to the indication signal <b>51</b>. Refer to <figref idref="DRAWINGS">FIG. <b>1</b></figref> once again. In one embodiment, the fundus camera <b>10</b><i>a </i>of the present invention further comprises a display lens assembly <b>109</b>, an internal display device <b>110</b>, a display focal length-adjusting element <b>111</b>, and a light splitter <b>112</b>. The internal display device <b>110</b> is electrically connected with the signal processing element <b>106</b>. According to the calculated deviation, the signal processing element <b>106</b> presents the indication signal <b>51</b> on the internal display device <b>110</b>. The display lens assembly <b>109</b> is disposed at the light-output side of the internal display device <b>110</b>. The display focal length-adjusting element <b>111</b> is connected with the internal display device <b>110</b> or the display lens assembly <b>109</b>. The display focal length-adjusting element <b>111</b> may drive the internal display device <b>110</b> to move along an optical axis O<b>2</b> of the display lens assembly <b>109</b> or adjust the position or curvature of the lens of the display lens assembly <b>109</b>, whereby to adjust the focal length of the indication signal <b>51</b> to make the light beam L<b>2</b>, which is generated by the internal display device <b>110</b>, form an image on the fundus <b>21</b> of the tested eyeball <b>20</b>. The light splitter <b>112</b> is optically coupled to the internal display device <b>110</b> and the imaging lens assembly <b>103</b>, making the indication signal <b>51</b> be imaged on the fundus <b>21</b> of the tested eyeball <b>20</b>. According to the indication signal <b>51</b> presented by the internal display device <b>110</b>, the subject may adjust the relative position of the subject and the fundus camera <b>10</b><i>a </i>by himself. For example, the subject may move his own position or the position of the fundus camera <b>10</b><i>a </i>to make the fundus camera <b>10</b><i>a </i>be exactly aligned to the pupil of the subject. Through the abovementioned structure, a subject himself can also operate the fundus camera <b>10</b><i>a </i>of the present invention. Even though the fundus camera <b>10</b><i>a </i>is in a handheld form, it can also acquire superior fundus images.</p><p id="p-0032" num="0031">The pupil tracking method of the present invention is applicable to different ophthalmology inspection devices. Refer to <figref idref="DRAWINGS">FIG. <b>6</b></figref> for a fundamental structure of a tonometer <b>10</b><i>b</i>. The main functional elements of the tonometer <b>10</b><i>b </i>include a chamber <b>121</b>, a pressure source <b>122</b>, an air injector <b>123</b> and a pressure sensor <b>124</b>. The pressure source <b>122</b> supplies air to the chamber <b>121</b> to make the chamber have an appropriate pressure. The air injector <b>123</b> spurts air to the tested eyeball <b>20</b>. The pressure sensor <b>124</b> measures the pressure inside the chamber <b>121</b>, whereby to calculate the intraocular pressure of the tested eyeball <b>20</b>. The other elements of the tonometer <b>10</b><i>b </i>shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, which have the same numeral symbols as the elements in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, also have the same functions as these elements in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Since the technical contents thereof has been described before, it will not repeat herein. The persons having ordinary knowledge of the art should be able to modify the pupil tracking method and the ophthalmology inspection devices without departing from the spirit of the present invention.</p><p id="p-0033" num="0032">Refer to <figref idref="DRAWINGS">FIG. <b>7</b></figref> for a fundamental structure of a corneal topography device <b>10</b><i>c</i>. The main functional elements of the corneal topography device <b>10</b><i>c </i>include a patterning illumination element <b>131</b> and an object lens <b>132</b>. The patterning illumination element <b>131</b> is used to generate a patterned illumination light beam having a specified pattern and project the patterned illumination light beam to a tested eyeball <b>20</b>. The surface of the tested eyeball <b>20</b> reflects the light beam. The reflected light beam passes through the object lens <b>132</b> and the imaging lens assembly <b>103</b> to the image sensor <b>104</b> and form an image of the eyeball surface. The defects of the eyeball surface may be identified according to the deformation of the pattern. The other elements of the corneal topography device <b>10</b><i>c </i>shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, which have the same numeral symbols as the elements in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, also have the same functions as these elements in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Since the technical contents thereof has been described before, it will not repeat herein. The persons having ordinary knowledge of the art should be able to modify the pupil tracking method and the ophthalmology inspection devices without departing from the spirit of the present invention.</p><p id="p-0034" num="0033">Refer to <figref idref="DRAWINGS">FIG. <b>8</b></figref> for a fundamental structure of an automatic refractometer <b>10</b><i>d</i>. The main functional elements of the automatic refractometer <b>10</b><i>d </i>include an object lens <b>141</b> and a feature optical element <b>142</b>. The feature optical element <b>142</b> generates an annular feature-type light source. While the annular light source is projected onto the fundus, the automatic refractometer <b>10</b><i>d </i>can estimate the diopter of the tested eyeball <b>20</b> according to the shape of the annular light source. The internal display device <b>110</b> may present an optometric pattern. Adjusting the focal length between the internal display device <b>110</b> and the display lens assembly <b>109</b> may induce the eyeball of the subject to have accommodation relax. For example, the focal length may be changed from 10 cm equivalently to infinity equivalently via using the display focal length-adjusting element <b>111</b> to adjust the internal display device <b>110</b>. A fogging lens may be used to make the subject view a blurred optometric image and induce the eyeball of the subject to relax. Then, the diopter of the tested eyeball <b>20</b> is inspected. The other elements of the automatic refractometer <b>10</b><i>d </i>shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, which have the same numeral symbols as the elements in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, also have the same functions as these elements in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Since the technical contents thereof has been described before, it will not repeat herein. The persons having ordinary knowledge of the art should be able to modify the pupil tracking method and the ophthalmology inspection devices without departing from the spirit of the present invention.</p><p id="p-0035" num="0034">In conclusion, the ophthalmology inspection device and the pupil tracking method of the present invention may find out a pupil feature from the external eye image and may use a boundary fitting method to find out a center coordinate of the pupil feature. Therefore, the ophthalmology inspection device can track the pupil of the eyeball of a subject without using a stereo camera. In a preferred embodiment, the ophthalmology inspection device of the present invention can work out the deviation between the imaging optical axis of the ophthalmology inspection device and the pupil of the subject according to the center coordinate of the pupil feature. Then, the ophthalmology inspection device generates an indication signal to assist the operator or the subject in adjusting the relative position of the ophthalmology inspection device and the pupil of the subject. Thereby, the ophthalmology inspection device is exactly aligned to the pupil of the subject to acquire a better inspection result.</p><p id="p-0036" num="0035">While the invention is susceptible to various modifications and alternative forms, a specific example thereof has been shown in the drawings and is herein described in detail. It should be understood, however, that the invention is not to be limited to the particular form disclosed, but to the contrary, the invention is to cover all modifications, equivalents, and alternatives falling within the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A pupil tracking method comprising steps:<claim-text>using an ophthalmology inspection device to acquire an external eye image of a subject, wherein the external eye image includes a pupil of the subject;</claim-text><claim-text>using the ophthalmology inspection device to perform an image preprocessing on the external eye image, wherein the image preprocessing includes performing a binary conversion on the external eye image to obtain a binary image;</claim-text><claim-text>finding out a contour boundary of each feature in the binary image, and finding out a pupil feature based on a variance of a distance from the contour boundary of each feature to a corresponding reference point; and</claim-text><claim-text>fitting the contour boundary of the pupil feature in a boundary fitting method to find a center coordinate of the pupil feature.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The pupil tracking method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising a step:<claim-text>the ophthalmology inspection device generating an indication signal to remind the subject or an operator to adjust a relative position of the ophthalmology inspection device and the subject to make the ophthalmology inspection device be exactly aligned to the pupil of the subject.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The pupil tracking method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image preprocessing further includes a step: reducing a size of the external eye image before the binary conversion to reduce a computation amount of succeeding image processing steps.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The pupil tracking method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image preprocessing further includes a step: performing a noise reduction treatment of the external eye image before the binary conversion to eliminate grain-like noises of the external eye image.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The pupil tracking method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the noise reduction treatment is realized by an average filter, a Gaussian filter, a median filter, or a bilateral filter.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The pupil tracking method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image preprocessing further includes a step: performing an image enhancing treatment of the external eye image before the binary conversion to enhance image boundaries.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The pupil tracking method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the image enhancing treatment is realized by a gamma correction technology, a histogram equalization technology, a homomorphic filter technology, an unsharp masking technology, or a combination thereof.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The pupil tracking method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image preprocessing further includes a step: performing an opening operation of morphology on the binary image to compensate for defects or small-area noises generated in the binary conversion.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The pupil tracking method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the reference point is a center of a minimum enclosing circle fitting the contour boundary of each feature.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The pupil tracking method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein while the variance of a feature is smaller than or equal to a preset value, the feature is the pupil feature.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The pupil tracking method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the boundary fitting method may be a circle fitting method, an ellipse fitting method, or a minimum enclosing circle method.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The pupil tracking method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a least square method is used to find out the center coordinate of the pupil feature.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An ophthalmology inspection device comprising<claim-text>an illumination element, generating an illumination light beam to illuminate an external eye region of a subject;</claim-text><claim-text>an image sensor, receiving a light beam light reflected from the external eye region to generate an external eye image, wherein the external eye image includes a pupil of the subject; and</claim-text><claim-text>an imaging lens assembly, disposed at a light-input side of the image sensor to condense the light beam reflected from the external eye region and form an image to the image sensor; and</claim-text><claim-text>a signal processing element, electrically connected with the image sensor, wherein the signal processing element performs a pupil tracking method, which comprises steps:<claim-text>acquiring the external eye image output by the image sensor;</claim-text><claim-text>performing an image preprocessing on the external eye image, wherein the image preprocessing includes performing a binary conversion on the external eye image to obtain a binary image;</claim-text><claim-text>finding out a contour boundary of each feature in the binary image, and finding out a pupil feature based on a variance of a distance from the contour boundary of each feature to a corresponding reference point; and</claim-text><claim-text>fitting the contour boundary of the pupil feature in a boundary fitting method to find a center coordinate of the pupil feature, and calculating a deviation between an optical axis of the imaging lens assembly and the pupil of the subject according to the center coordinate of the pupil feature.</claim-text></claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref> further comprising<claim-text>an internal display device, electrically connected with the signal processing element, wherein the signal processing element presents an indication signal on the internal display device according to the deviation;</claim-text><claim-text>a display lens assembly, disposed at a light-output side of the internal display device;</claim-text><claim-text>a display focal length-adjusting element, connected with the internal display device or the display lens assembly to adjust a focal length of the indication signal;</claim-text><claim-text>a light splitter, optically coupled to the internal display device and the imaging lens assembly, imaging the indication signal on a fundus of the subject to remind the subject to adjust a relative position of the ophthalmology inspection device and the subject to make the ophthalmology inspection device be exactly aligned to the pupil of the subject.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref> further comprising<claim-text>an external display device, coupled to the signal processing element, wherein according to the deviation, the signal processing element presents an indication signal on the external display device to remind an operator to adjust a relative position of the ophthalmology inspection device and the subject to make the ophthalmology inspection device be exactly aligned to the pupil of the subject.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref> further comprising<claim-text>an imaging focal length-adjusting element, connected with at least one of the image sensor and the imaging lens assembly to physically move the image sensor or at least one lens of the imaging lens assembly or adjust a curvature of a liquid-state lens of the imaging lens assembly to make the light beam reflected from the external eye region be imaged on the image sensor.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, which is a fundus camera, a tonometer, a corneal topography device, or an automatic refractometer.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the image preprocessing further includes a step: reducing a size of the external eye image before the binary conversion to reduce a computation amount of succeeding image processing steps.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the image preprocessing further includes a step: performing a noise reduction treatment of the external eye image before the binary conversion to eliminate grain-like noises of the external eye image.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the noise reduction treatment is realized by an average filter, a Gaussian filter, a median filter, or a bilateral filter.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the image preprocessing further includes a step: performing an image enhancing treatment of the external eye image before the binary conversion to enhance image boundaries.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the image enhancing treatment is realized by a gamma correction technology, a histogram equalization technology, a homomorphic filter technology, an unsharp masking technology, or a combination thereof.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the image preprocessing further includes a step: performing an opening operation of morphology on the binary image to compensate for defects or small-area noises generated in the binary conversion.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the reference point is a center of a minimum enclosing circle fitting the contour boundary of each feature.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein while the variance of a feature is smaller than or equal to a preset value, the feature is the pupil feature.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the boundary fitting method may be a circle fitting method, an ellipse fitting method, or a minimum enclosing circle method.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The ophthalmology inspection device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein a least square method is used to find out the center coordinate of the pupil feature.</claim-text></claim></claims></us-patent-application>