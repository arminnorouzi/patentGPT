<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005108A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005108</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363293</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>32</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>194</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>272</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>005</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>3258</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>003</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>194</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>272</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>2209</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20201</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30168</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHOD AND SYSTEM FOR REPLACING SCENE TEXT IN A VIDEO SEQUENCE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Palo Alto Research Center Incorporated</orgname><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Gopalkrishna</last-name><first-name>Vijay Kumar Baikampady</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Bala</last-name><first-name>Raja</first-name><address><city>Pittsford</city><state>NY</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">To replace text in a digital video image sequence, a system will process frames of the sequence to: define a region of interest (ROI) with original text in each of the frames; use the ROIs to select a reference frame from the sequence; select a target frame from the sequence; determine a transform function between the ROI of the reference frame and the ROI of the target frame; replace the original text in the ROI of the reference frame with replacement text to yield a modified reference frame ROI; and use the transform function to transform the modified reference frame ROI to a modified target frame ROI in which the original text is replaced with the replacement text. The system will then insert the modified target frame ROI into the target frame to produce a modified target frame. This process may repeat for other target frames of the sequence.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="91.36mm" wi="158.75mm" file="US20230005108A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="188.81mm" wi="120.82mm" orientation="landscape" file="US20230005108A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="192.19mm" wi="95.25mm" file="US20230005108A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="166.71mm" wi="89.07mm" file="US20230005108A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="188.72mm" wi="124.71mm" file="US20230005108A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="171.03mm" wi="132.50mm" orientation="landscape" file="US20230005108A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="175.60mm" wi="129.54mm" orientation="landscape" file="US20230005108A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="163.83mm" wi="128.61mm" file="US20230005108A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">There are many situations in which the removal of scene text from sequence of video images is desirable. For example, video sequences may contain images of storefronts with names, street signs, license plates, and other scenes that contain text. In some situations, it is desirable to remove the scene text from the video to prevent disclosure of personally identifiable information, such as street names or vehicle plate numbers, to others who view the images. In other situations, it is desirable to remove scene text to avoid creating a video with a logo or product name that conflicts with obligations to advertisers or sponsors of the video. In other situations, it is desirable to remove scene text and replace it with personalized messages or messages that suit the storyline that the video is presenting. Other applications, such as translation of scene text, can also benefit from text removal and replacement processes.</p><p id="p-0003" num="0002">Manual text replacement is very time consuming, and it can be imprecise. Therefore, several automated text replacement methods are known. However, the existing methods often yield results in which the tone or texture of the replacement text is still distinguishable from the original background, and/or results in which the background tone or texture has been altered to match the area around the replacement text.</p><p id="p-0004" num="0003">This is an especially challenging problem in a video sequence, in which text replacement must be performed for a large number of frames. Text replacement can involve several steps including removal of the original text (i.e., background inpainting), estimation of style properties of the original text (such as color, font, texture), and insertion of the new text into the background according to the original text's style. Modern video standards can support frame rates of 120, 240 or 300 frames per second. Even standard definition video, which has a nominal frame rate of 24 frames per second, yields over 86,000 frames in a one-hour video sequence. If all steps required for text replacement are applied to such a large number of frames, a significant amount of processing capacity and time can be required.</p><p id="p-0005" num="0004">In addition, in a video sequence the color appearance of a text object can change from frame to frame due to differences in lighting that is incident on the camera as it captures the scene, camera properties such as color/white balance, changes in focus, camera blur, varying distance between the text and the camera, and other variables. Thus, the results of applying existing text replacement methods to video sequences can be temporally inconsistent.</p><p id="p-0006" num="0005">This document describes methods and systems that are directed to addressing the issues listed above.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0007" num="0006">Methods and systems for replacing text and/or other objects in digital image sets are disclosed in this document. The system may include a processor and a memory device containing programming instructions that are configured to cause the processor to receive a sequence of digital image frames, and process the digital image frames to define a region of interest (ROI) that contains original text in each of the digital image frames. The system will use the ROIs of the digital image frames to select a reference frame from the sequence of digital image frames. The system also will select a target frame from the sequence of digital image frames, wherein the target frame is different from the reference frame, The system will determine a transform function between the ROI of the reference frame and the ROI of the target frame. The system will replace the original text in the ROI of the reference frame with replacement text to yield a modified reference frame ROI. The system will use the transform function to transform the modified reference frame ROI to a modified target frame ROI in which the original text is replaced with the replacement text. The system will then insert the modified target frame ROI into the target frame to produce a modified target frame. Optionally, instead of scene text, other objects such as images that an object detection and classification system may be replaced in digital image sets using the same processes described in this document,</p><p id="p-0008" num="0007">In some embodiments, for each of one or more additional target frames in the sequence, the system may determine an additional transform function between the reference frame ROI and the additional target frame ROI. The system may then use the additional transform function to transform the modified reference frame's ROI to a modified additional target frame's ROI in which the original text is replaced with the replacement text.</p><p id="p-0009" num="0008">In some embodiments, when using the ROIs to select the reference frame, the system may analyze the digital image frames of the sequence to select a frame in which the original text satisfies a function of one or more of the following criteria: a threshold optical character recognition confidence score; a sharpness criterion; a size criterion; a contrast criterion; or a pose criterion.</p><p id="p-0010" num="0009">In some embodiments, to determine the transform function between the reference frame ROI and the target frame ROI, the system may determine at least two of the following: (a) a lighting correction transform element, which comprises a function that represents changes in illumination between the reference frame ROI and the target frame ROI; (b) a sharpness transform element, which comprises a function that represents differences in camera focus, capture resolution, or both between the reference frame ROI and the target frame ROI; or (c) a motion blur transform element, which comprises a function that represents differences in motion blur between the reference frame ROI and the target frame ROI. To determine the lighting correction transform element, the sharpness transform element and/or the motion blur transform element, the system may train a machine learning model using training samples comprising pairs of reference frames and target frames. To use the transform function to transform the modified reference frame ROI to the modified target frame ROI, the system may apply the determined transform elements to the modified reference frame ROI to yield the modified target frame ROI.</p><p id="p-0011" num="0010">In some embodiments, before using the transform function to transform any of the target frame ROIs, the system may normalize and align the original text in the ROIs for each of the target frames to a consistent pose.</p><p id="p-0012" num="0011">In some embodiments, to replace the original text in the reference frame ROI with replacement text to yield the modified reference frame ROI, the system may: (i) process the ROI of the reference frame through a multitask machine learning model to determine a foreground image of the ROI, a background image of the ROI, and a binary mask that distinguishes foreground image pixels from background image pixels in the ROI; (ii) receive a new binary mask that contains replacement text; and (iii) apply the new binary mask to blend the background image with the foreground image and yield a modified reference frame ROI.</p><p id="p-0013" num="0012">In some embodiments, to process the digital image frames to define the ROI in each frame the system may apply a text detector to each of the digital image frames of the input sequence to return bounding box coordinates. The system may then define the ROI of each of the digital image frames according to the bounding box coordinates.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example text replacement process for a video sequence.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates the steps of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in flowchart format.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example text infilling process for a reference frame of a video sequence.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example text propagation process according to the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates example hardware elements of a video processing system.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>6</b></figref> compares example images, before and after processing, using the methods described in this document.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0020" num="0019">As used in this document, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; include plural references unless the context clearly dictates otherwise. Unless defined otherwise, all technical and scientific terms used in this document have the same meanings as commonly understood by one of ordinary skill in the art. As used in this document, the term &#x201c;comprising&#x201d; (or &#x201c;comprises&#x201d;) means &#x201c;including (or includes), but not limited to.&#x201d; When used in this document, the term &#x201c;exemplary&#x201d; is intended to mean &#x201c;by way of example&#x201d; and is not intended to indicate that a particular exemplary item is preferred or required.</p><p id="p-0021" num="0020">In this document, when terms such &#x201c;first&#x201d; and &#x201c;second&#x201d; are used to modify a noun, such use is simply intended to distinguish one item from another and is not intended to require a sequential order unless specifically stated.</p><p id="p-0022" num="0021">The term &#x201c;approximately,&#x201d; when used in connection with a numeric value, is intended to include values that are close to, but not exactly, the number. For example, in some embodiments, the term &#x201c;approximately&#x201d; may include values that are within +/&#x2212;10 percent of the value.</p><p id="p-0023" num="0022">In this document, the term &#x201c;substantially every&#x201d;, when referring to a particular parameter, means that a great majority of the instances or values such parameters will satisfy a condition. In various embodiments, the majority may be at least 80%, at least 85%, at least 90%, or at least 95% of the parameters.</p><p id="p-0024" num="0023">In this document, the term &#x201c;text&#x201d; will be used to refer to a sequence of one or more characters, and it may include alphanumeric characters such as letters and numbers, characters that represent words or portions of words in various languages such as kanji and hanzi, as well as punctuation marks, mathematical symbols, emojis, and other symbols.</p><p id="p-0025" num="0024">Additional terms that are relevant to this disclosure will be defined at the end of this Detailed Description section.</p><p id="p-0026" num="0025">As noted in the Background section above, the removal of scene text from digital images is sometimes referred to as &#x201c;text infilling&#x201d; or &#x201c;text inpainting&#x201d;. There are several known methods of text infilling, including methods that are based on principles of deep style transfer.</p><p id="p-0027" num="0026">To address the problems of the prior art described above, this document describes a novel method of applying text replacement to a digital image or a video sequence of digital image frames. The process may be understood as including four general steps: In one step, text in all frames of the sequence is normalized to a frontal pose. In another step, a reference frame is selected from the video sequence. In another step, the original text of the reference frame is replaced with new text using any suitable text infilling process. Then, after the three steps above are performed, then instead of applying the same text replacement process to the remaining frames of the video sequence, a novel pairwise image transformation network is used to transfer the new text is transferred to the remaining frames. This approach disentangles style transfer from geometric and photometric variations encountered in video, and it applies temporal consistency restraints to yield results of good quality, with less processing time and resources than would be required by traditional frame-by-frame text infilling processes.</p><p id="p-0028" num="0027">Certain elements of this workflow are shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates the steps of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in flowchart format, with some additional details that are not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. At <b>102</b>, an image processing system that includes a processor and programming instructions will receive a video that includes a sequence of digital image frames. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates this by way of example in which the image frames include a sign on which the word &#x201c;orange&#x201d; appears.</p><p id="p-0029" num="0028">At <b>103</b> the system will identify and extract a region of interest (ROI) from each of the image frames. The ROI in each frame will include text that is to be replaced. To extract the ROI, the image frames received at step <b>102</b> may be pre-processed, in that they have been cropped to define the ROI. Alternatively, then at <b>103</b> the system may process the image frames with a text detector that returns bounding box coordinates for cropping the text ROI in each frame. A text detector is a set of programming instructions that a computing device uses to process an image with a feature detector such as the tesseract algorithm, a maximally stable extremal regions (MSER) algorithm detector, a convolutional neural network (CNN) or any now or hereafter known optical character recognition (OCR) process.</p><p id="p-0030" num="0029">Text objects in videos can undergo diverse geometric distortions from frame to frame due to variations in the object pose relative to the position of the camera that captured the video. To reduce the effect of this distortion on text style transfer, at <b>104</b> the system may normalize and align the source text in all ROIs to a canonical frontal pose before beginning text replacement operations. This enables both robust text style transfer on the reference frame, and the propagation of the replaced text onto the remaining frames via learned image transforms. In step <b>104</b> the system may use an algorithm that makes a simplifying assumption that the scene text is on a planar surface. This covers many common cases such as street and store signage, banners, and the like. Under the planar assumption, pose correction is accomplished via a perspective transform. The system may do this via a machine learning model such as a spatio-temporal transformer network (STTN) that has been trained to perform for perspective correction in videos. A trained STTN predicts the parameters &#x3b8; of a geometric correction transform via a localization network, and it applies the transform to the image via grid generation and resampling operators. The STTN may trained as part of a supervised classification task at step <b>101</b>. Examples of suitable networks include those described in Jaderberg et al., &#x201c;Spatial Transformer Networks&#x201d; (arXiv:1506.02025v3 [cs.CV] 2016). The system may use a network architecture similar to that disclosed in the prior art, but it would be trained to produce temporally consistent frontal ROIs. The training samples used to train the STTN may include a set of distorted input ROIs and a frontal ROI of synthetic text serving as the target label. The network is trained on the following loss:</p><p id="p-0031" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>L</i><sub>STTN</sub><i>=L</i><sub>&#x3b8;</sub>+&#x3bb;<sub>1</sub><i>L</i><sub>pix</sub>+&#x3bb;<sub>2</sub><i>L</i><sub>t </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0032" num="0030">in which L<sub>&#x3b8; </sub>is the mean-squared error (MSE) between the true and predicted homography parameter vector &#x3b8;, and L<sub>pix </sub>is the pixelwise MSE between predicted and true text ROIs. L<sub>t </sub>is the temporal consistency loss, defined as the MSE between &#x3b8; for adjacent video frames: L<sub>t</sub>=&#x3a3;<sub>j</sub>|&#x3b8;<sub>i</sub>&#x2212;&#x3b8;<sub>j</sub>|<sup>2 </sup>, where index I denotes the current frame and the summation is over a number of neighboring frames in which j&#x2260;i. This term improves smoothness in variation of perspective correction parameters over adjacent frames. Alternatively, the normalization step <b>104</b> may be done after the reference frame selection step <b>105</b>, or after the text replacement step <b>106</b>.</p><p id="p-0033" num="0031">At <b>105</b> the system selects a reference frame, and at <b>106</b> the system replaces the original text of the reference frame with replacement text. These steps may be performed after the normalization step <b>104</b> as shown in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>.</p><p id="p-0034" num="0032">To select the reference frame at <b>105</b>, the system will analyze frames in the sequence and select one in which the text of the ROI exhibits a suitable appearance and/or geometry. To help ensure a successful style transfer, the source text in the reference frame should be legible, sharp, of high contrast, and maximally frontal in pose. The system may therefore use any or all of the following criteria to determine a frame's suitability for use as a reference frame: a requirement that the ROI in the reference frame have a threshold optical character recognition (OCR) confidence score; and/or a requirement that the ROI satisfy a sharpness criterion, a size criterion, a contrast criterion, and/or a pose criterion.</p><p id="p-0035" num="0033">For example, for a frame to qualify as a reference frame the system may require that the ROI in the reference frame have an OCR confidence score greater than 0.99, or that the reference frame have the highest (or one of the highest) OCR confidence scores of all frames in the sequence. The OCR score may be determined using existing processes such as the Amazon Rekognition image analysis module that is available from Amazon Web Services. Selecting an image with a high OCR score helps to eliminate excessively blurry, distorted or occluded text objects. The system may measure image sharpness of each frame's ROI as, for example, the variance of the Laplacian of the luminance image, and the sharpness criterion may require that the reference frame have one of the n highest sharpness scores of all images in the sequence. To measure contrast of each frame's ROI, the system may binarize the ROI into foreground regions and using Otsu's algorithm (as published in Otsu, &#x201c;A threshold selection method from gray-level histograms,&#x201d; <i>IEEE Trans. System. Man. Cybernetics </i>9(1):62-22 (1979)), and it may determine the contrast as normalized interclass variance s<sub>1 </sub>between the foreground and background regions. To estimate frontal pose of the text in each ROI the system may, for example, calculate the frontal pose as the ratio s<sub>2 </sub>of the area of the ROI' s bounding box to the area of a subsuming axis-aligned rectangle. The system may then calculate a composite text quality score for each frame as a function of any or all of these criteria, and it may select the frame with the highest composite text quality score as the reference frame. An example function for determining a composite text quality score is &#x3b1;<sub>1</sub>s<sub>1</sub>+&#x3b1;<sub>2</sub>s<sub>2</sub>, in which &#x3b1;<sub>1 </sub>and &#x3b1;<sub>2 </sub>are heuristically chosen based on visual evaluation of replaced text in random videos.</p><p id="p-0036" num="0034">As noted above, after the system selects a reference frame at <b>105</b>, the system will replace the original text of the reference frame with replacement text at <b>106</b>. An example text replacement process is illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in which steps <b>105</b> and <b>106</b> from <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> are consolidated as step <b>301</b>. The system processes the ROI of the reference frame in two parallel paths. In a first path, at <b>303</b> the system will apply a text infilling function to the ROI by processing the reference frame's ROI and modifying text-containing pixels in the ROI so that those pixels exhibit a background color of the ROI instead of the text color. In some embodiments, the text infilling function may be the pix2pix model, with the function applied to each image of the sequence, or to a subset of the images in the sequence (such as a group of images over a specified time period, or every other image). The pix2pix model is a model in which conditional generative adversarial network (GAN) transforms a first image to a second image. The pix2pix method is described in numerous publications, including Wang et al., &#x201c;High Resolution Image Synthesis and Semantic Manipulation with Conditional GANs&#x201d; (arxiv:1711.11585 [cs.CV], 2018). Alternatively, a style retention network such as that known as SRNet or another GAN may be used to process the image and replace text with background color.</p><p id="p-0037" num="0035">For example, at <b>303</b> the vid2vid method of video image synthesis may be used. The vid2vid method is disclosed in, for example, Wang et al., &#x201c;Video-to-Video Synthesis&#x201d; (arXiv:1808.06601v2 [cs:CV 2018). After step <b>303</b> pixels that were within the ROI will no longer exhibit a color that corresponds to text but instead will exhibit a color that appeared in the background of the ROI. The output of step <b>303</b> will be an image frames in which the text pixels have been infilled with the background color.</p><p id="p-0038" num="0036">In a parallel path, at <b>314</b> the system will separate foreground (text) from background (non-text) in the ROI. For example, the system may pass the ROI through a foreground-background segmentation module such as Mask-RCNN. The foreground-background segmentation module will generate a binary mask in which each pixel in the ROI is assigned a value of either zero (0) or one (1), wherein one of the values corresponds to foreground (text) and the other value corresponds to background (non-text). Each pixel of the ROI in the original reference image will have a color value, which in an RGB color model will include three channels (i.e., three channels, one for a red component R, one for a blue component G, and one for a blue component B). At <b>315</b> the system will then determine an input background color value for each channel in the ROI. For example, if the mask assigns background pixels a mask value of 1, the system may multiply each ROI pixel's color value by its binary mask value, and then determine a mean value of all pixels in each channel for the background of the ROI after the multiplication. Optionally, the input background color may be a weighted average of the pixel values in the ROI. For example, RGB channels may be given equal weights, but other channels that use characteristics such as luminance and chrominance, the luminance channel may be given more weight than that of the chrominance channel.</p><p id="p-0039" num="0037">At <b>306</b> the system will also determine, for each color channel, an average of the color values of all pixels in the preliminary output of the text infilling function. As with step <b>315</b>, the average of step <b>306</b> may actually be a mean, or a weighted average. At <b>307</b> the system will then compare the color value of each channel returned at step <b>306</b> to the color value of each corresponding channel returned at step <b>315</b> to generate a residual signal. The residual signal will be a C-dimensional vector, in which C represents the number of output channels of the network. The value of each channel in the residual signal may be a positive number or a negative number, depending on the results of the comparing step <b>317</b>.</p><p id="p-0040" num="0038">At <b>308</b> the residual signal is applied to the preliminary output set so that, for each pixel in the ROI of the preliminary output signal, the color value for each channel of the pixel is summed with the corresponding value of the residual signal for that channel.</p><p id="p-0041" num="0039">At <b>309</b> a target mask that is a binary mask with replacement text may then be blended (using a foreground color) with the blank reference frame to yield the modified reference frame with replacement text. The blending process may be performed by simple binary switching, in which background pixels of the target mask are assigned color values of the predicted background image while foreground pixels of the target mask are assigned color values of the predicted foreground image. Alternatively, the blending may be performed using any now or hereafter known blending process such as those that use a generative adversarial network. (See, for example, Zhang et. Al., &#x201c;Deep Image Blending&#x201d;, arXiv: 1910.11495 [cs.cv] 2020.) In the process described above, &#x201c;color&#x201d; has been described as a single value for the purpose of describing the process. However, in practice, each pixel of the ROI will have a color value in an RGB color model, which means that the color value will include three channels (one for a red component R, one for a blue component G, and one for a blue component B). Pixels may include additional channels for additional characteristics, such as luminance or chrominance. Thus, the process described above may be performed for each available channel. In such situations, the average foreground color, average background color, and residual signal will be C-dimensional vectors, in which C represents the number of available channels of the network. When determining averages, or when adding or blending, some channels may be given different weights than others. For example, the system may assign equal weights to RGB channels, but other channels such as those for characteristics such as luminance and chrominance may have unequal weights in various processes.</p><p id="p-0042" num="0040">Alternatively a number of state of art methods may be used for replacing text in the reference frame, including SR-Net (include reference).</p><p id="p-0043" num="0041">The process above yields a final edited ROI, which may be applied to the input video frame at <b>310</b> as a replacement ROI, thus resulting in a blank reference frame.</p><p id="p-0044" num="0042">Returning to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, after the text replacement in the reference frame is complete at <b>106</b>, the system will propagate that text replacement to other frames in the sequence at <b>115</b>. The text propagation step <b>115</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> includes several sub-steps, including (as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) determining a transform function between the ROI of the reference frame and the ROIs of each target frame at <b>108</b>, and using the transform function to transform generate a modified target frame ROI at <b>109</b> by transforming the modified reference frame ROI into the modified target frame ROI. The system will then insert the modified target frame ROI into the ROI at <b>110</b>.</p><p id="p-0045" num="0043">To insert the modified target frame ROI into the ROI at <b>110</b>, the system then applies the ROI to an encoder of a neural network or other machine learning model which converts the ROI image into a format that the network may then process, such as a multidimensional tensor. The encoder may be a single, deep neural encoder that is shared by multiple decoders. The output of the encoder is then simultaneously passed to three decoders: a foreground image decoder, a background image decoder, and a binary image decoder, which perform a parallel process to simultaneously predict the three constituent image that make up the single ROI input. Optionally, some or all of the decoders may be deep neural network decoders. The foreground image decoder of the neural network will predict a foreground image of the ROI, in which the foreground image includes color values for the text of the ROI. The background image decoder of the network will determine a background image of the ROI, in which the background image includes color values for areas of the ROI that do not contain the text of the ROI. The binary mask decoder of the network will predict a binary mask that distinguishes the foreground pixels from the background pixels by assigning each pixel a value of either 0 or 1, or by predicting the probability of each pixel being a foreground color, taking on values between 0 and 1. This probability map is then converted to a binary mask with a suitable thresholding operator. In the binary mask, foreground pixels have a value of 1 while background pixels have a value of 0, but in practice this assignment could also be reversed such that foreground pixels have a value of 0 while background pixels have a value of 1. To reduce errors, a residual post-correction step may be used. In residual post-correction the system applies the binary mask to the input ROI to extract the average color of the input foreground pixels and the average color of the input background pixels, respectively. The system may use the binary mask to extract a foreground average color by averaging the color values of pixels of the input image that correspond to foreground pixels in the mask. Similarly, the system may use the binary mask to extract a background average color by averaging the color values of input image pixels that correspond to background pixels in the mask. Optionally, the average value may be a weighted average. If the mask is a continuous-valued map indicating probability of foreground, then these probabilities are treated as foreground weights. A weighted foreground average may be computed by performing pixelwise multiplication of foreground weights and foreground colors, summing the products, and dividing by the sum of foreground weights. The process is repeated to compute a weighted background average, except that background weight is computed as 1&#x2212;{foreground weight}. Next the system determines an average predicted background color from the output image predicted by the background decoder using the same set of steps described above. This average predicted background color is subtracted channelwise for each of R, G, B from the average input background color to form a residual correction signal [&#x394;R, &#x394;G, &#x394;B]. The system will then add the residual signal to substantially every pixel of the predicted background image. This will result in a predicted background image in which pixels of the ROI that contain text have been infilled with the predicted background color. The system will add the average input foreground color computed from the input ROI to substantially every pixel in the predicted foreground image. This will result in a corrected foreground image in which pixels of the ROI that do not contain text have been infilled with the predicted foreground color.</p><p id="p-0046" num="0044">Step <b>111</b> indicates that this process will repeat until the replacement text has been propagated to all other target frames in the video. Upon completion the system will output a final video sequence with the modified target frames at <b>112</b>. Accordingly, rather than repeatedly performing text replacement on every frame of the video, the system will instead learn changes in text appearance over the frames of the video, and it will use the transform to propagate the edits that it applies to the reference frame to some or all of the other frames in the video.</p><p id="p-0047" num="0045">The transform function may be a function that includes two or more of the following elements: (a) a lighting correction transform element, which includes a function that represents changes in illumination between the ROI of the reference frame and that of the target frame; (b) a sharpness transform element, which includes a function that represents differences in camera focus, capture resolution, or both between the ROI of the reference frame and that of the target frame; and/or (c) a motion blur transform element, which includes a function that represents differences in motion blur between the ROI of the reference frame and that of the target frame. These and/or other elements of the transform function may be determined by training a machine learning model on pairs of reference frames and target frames.</p><p id="p-0048" num="0046"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates example machine learning models that the system's text propagation module may employ. The module's design presumes that image transformation between two video frames within a ROI can be adequately modeled in terms of simple parametric changes in lighting and image sharpness due to changes in camera, target object, and/or lighting conditions over a sequence of frames. The system may use self-supervision to learn the parameters of the model without relying on a large set of paired videos and labels. For the purpose of this discussion, consider I<sub>R </sub><b>401</b> and I<sub>i </sub><b>405</b> respectively to be the ROIs from the reference frame and the i<sup>th </sup>additional frame in the input video. Similarly let I<sub>R</sub>&#x2032; <b>411</b> and I<sub>i</sub>&#x2032; <b>415</b> be corresponding ROIs of the target text in the modified reference frame and the modified additional frame in the output video. The ROIs may be normalized (pose-corrected) and aligned by a pose alignment module such as the STTN as discussed above in the context of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>. The system uses a text replacement process <b>404</b> such as those described above to modify the original reference frame I<sub>R</sub><b>401</b> and yield the modified reference frame I<sub>R</sub>&#x2032; <b>411</b>.</p><p id="p-0049" num="0047">The system learns a parametric transform between the ROI of original reference frame IR <b>401</b> and the ROIs of each additional target frame <b>405</b> of the video in which text is to be edited. For each target frame, the system then applies the target frame's transform to the modified reference frame I<sub>R</sub>&#x2032; <b>411</b> to predict the modified target frame's ROI. The transform may include at two of the following stages, a lighting correction module (LCM) <b>421</b>, a differential blur prediction network (BPN) <b>422</b> as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0050" num="0048">The LCM <b>421</b> captures appearance differences between the reference image ROI <b>401</b> and the target image ROI <b>405</b> due to changes in illumination, including shadows and shading. Since the color of an object is a product of its reflectance and the illumination, the system may presume that changes in light reflected off a fixed text object can be modeled by independent channel-wise scaling of R, G, and B channels in a spatially varying manner. In particular, changes in lighting between two aligned ROIs can be obtained from their ratio I<sub>R</sub>/I<sub>i </sub>which is then multiplied (at <b>425</b>) by I<sub>R</sub>&#x2032; <b>411</b> as part of the transform function to obtain the lighting-corrected output for I<sub>i</sub>&#x2032; <b>415</b>. In practice, the ratio I<sub>R</sub>/I<sub>i </sub>may produce sharp discontinuities at the text boundaries. To avoid this, the system first removes the text in I<sub>R </sub>and I<sub>i </sub>via text inpainting (see <figref idref="DRAWINGS">FIG. <b>3</b></figref>) and then computes ratios of the inpainted versions. An implicit assumption with the ratio model is that the reflective properties of the foreground text and background in any ROI are similar. Further, to ensure temporal robustness the system may compute a weighted average of inpainted ROIs over N neighboring frames in the vicinity of I<sub>i </sub>before computing the ratio. The latter may then be multiplied by both the original reference frame I<sub>R </sub><b>401</b> (at <b>424</b>) and the modified reference frame I<sub>R</sub>&#x2032; <b>411</b> (at <b>425</b> ) to produce lighting-corrected versions that are then passed to the BPN <b>422</b>.</p><p id="p-0051" num="0049">Blur prediction by the BPN <b>422</b> may be a CNN-based method for predicting a transformation between a pair of images that can result from spatial aberrations, including motion blur, out-of-focus blur, and resolution differences due to varying distances between camera and text object. The system may identify differences in blur at a first order by spatial filtering. In addition or alternatively, the system model possible frame-to-frame distortions within a frame's text ROI using the following transformation:</p><p id="p-0052" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i><sub>i</sub>(<i>x,y</i>)=(1+<i>w</i>)<i>I</i><sub>R</sub>(<i>x,y</i>)&#x2212;<i>wI</i><sub>R</sub>(<i>x,y</i>)*G<sub>&#x3c3;,&#x3c1;</sub>(<i>x,y</i>) &#x2003;&#x2003;(Equation 1),<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0053" num="0050">where w&#x2208;[&#x2212;1, 1] and G<sub>&#x3c3;,&#x3c1;</sub> is an oriented 2D Gaussian filter rotated by angle &#x3c1;:</p><p id="p-0054" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <msub>        <mi>G</mi>        <mrow>         <mi>&#x3c3;</mi>         <mo>,</mo>         <mi>&#x3c1;</mi>        </mrow>       </msub>       <mo>(</mo>       <mrow>        <mi>x</mi>        <mo>,</mo>        <mi>y</mi>       </mrow>       <mo>)</mo>      </mrow>      <mo>=</mo>      <msup>       <mi>Ke</mi>       <mrow>        <mo>-</mo>        <mrow>         <mo>(</mo>         <mrow>          <mfrac>           <msup>            <mi>x</mi>            <mi>&#x2032;2</mi>           </msup>           <msubsup>            <mi>&#x3c3;</mi>            <mi>x</mi>            <mn>2</mn>           </msubsup>          </mfrac>          <mo>+</mo>          <mfrac>           <msup>            <mi>y</mi>            <mi>&#x2032;2</mi>           </msup>           <msubsup>            <mi>&#x3c3;</mi>            <mi>y</mi>            <mn>2</mn>           </msubsup>          </mfrac>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </msup>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mn>2</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0055" num="0051">where K is a normalizing constant, x&#x2032;=x cos &#x3c1;+y sin &#x3c1; and y&#x2032;=&#x2212;x cos &#x3c1;+y sin &#x3c1;. The case of &#x3c3;x&#x2248;&#x3c3;y yields an isotropic point spread function (PSF) modeling out-of-focus blur and resolution differences, while a significant difference between these two parameters models an anisotropic blur in direction &#x3c1;, encountered with typical camera or object motion. As w varies from &#x2212;1 to 0 to 1, the transformation varies from image sharpening to identity to image blurring. (Image sharpening with w&#x3c;0 by be done by a sharpness correction module (SCM) if the current frame is sharper than the reference frame. The SCM function may be implemented by the BPN, or it may be a separate function.) BPN takes the reference frame I<sub>R </sub><b>401</b> and a sliding window of N frames I<sub>i </sub>around the current (i<sup>th</sup>) time instance, and predicts N sets of parameters &#x3c8;=[&#x3c3;<sub>x</sub>, &#x3c3;<sub>y</sub>, &#x3c1;,w] that best transforms I<sub>R </sub>to the N output frames (with respect to spatial frequency characteristics) via the blur model described above. The network thus may take in N+1 image inputs and predict 4N parameters. Predicting transforms on frame stacks ensures temporal consistency. The network architecture for implementing the BPN <b>422</b> and/or SCM may include a ResNet18 backbone, an average pooling layer and two fully connected layers. The training loss for the BPN may be defined as:</p><p id="p-0056" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>L</i><sub>BPN</sub>=&#x3bb;<sub>&#x3c8;</sub><i>L</i><sub>&#x3c8;</sub>+&#x3bb;<sub>RLR</sub>+&#x3bb;<sub>T</sub><i>L</i><sub>T</sub>&#x2003;&#x2003;(Equation 3),<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0057" num="0052">where L<sub>&#x3c8; </sub>is the squared error loss between the true and predicted parameter vectors &#x3c8;; L<sub>R </sub>is the mean-squared image reconstruction error between the predicted and true ROIs, and L<sub>T</sub>is a temporal consistency loss that discourages large fluctuations in &#x3c8; over time: L<sub>T</sub>=&#x3a3;<sub>j</sub>|&#x3c8;<sub>i</sub>&#x2212;&#x3c8;<sub>j</sub>|<sup>2</sup>, where i denotes the current frame, the summation is over N&#x2212;1 neighboring frames, and j&#x2260;i. The predicted I<sub>i </sub>is obtained by applying the blur model described above with parameters &#x3c8; to I<sub>R</sub>. Note that the blur model is differentiable with respect to w and thus can be applied within a training loop, which will be described below.</p><p id="p-0058" num="0053">The transform function that transforms the modified reference frame <b>411</b> into a modified target frame <b>415</b> for each target frame therefore may include a ratio image from the LCM at <b>425</b>, and a set of blur parameters <b>426</b>.</p><p id="p-0059" num="0054">As noted above in the discussion of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, before the text replacement process described above begins, any or all of the networks used by the system will be trained (at <b>101</b>). The training may be periodically updated or supplemented at any point in time. For example, the BPN may be trained in two stages. In the first stage, Equation 1 above with known parameters &#x3c8; may be applied to reference ROIs I<sub>R </sub>from synthetic videos to obtain training pairs (I<sub>R</sub>, I<sub>i</sub>). As part of augmentation, inputs I<sub>i</sub>, may be warped with translational jitter in the x and y directions to make the network less prone to minor misalignments between I<sub>R </sub>and I<sub>i</sub>that may be encountered in real video frames. In the second stage of training, the BPN may be fine-tuned via self-supervision with (I<sub>R</sub>, I<sub>i</sub>) tuples extracted from real-world videos. Here, only L<sub>R </sub>and L<sub>T </sub>are minimized since the true &#x3c8; are unknown. During inference, the ROI pair (I<sub>R</sub>, I<sub>i </sub>) from the original video is passed through the BPN, and the predicted parameters are used to apply Equation 1 above to the altered ROI I<sub>R</sub>&#x2032; to produce the ROI I<sub>R</sub>&#x2032; for the current (i<sup>th</sup>) frame.</p><p id="p-0060" num="0055">Thus, during training the system may sample the transform parameters (such as w, &#x3c3;, length and angle) from corresponding distributions and use a reference frame to generate synthetic modified target frames. The system also may use a pre-trained network and fine-tune it with actual video frames. <figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts an example of internal hardware that may be included in an image processing system, such as a local or remote computing device that processes image frames as described above. An electrical bus <b>500</b> serves as an information highway interconnecting the other illustrated components of the hardware. Processor <b>505</b> is a central processing device of the system, configured to perform calculations and logic operations required to execute programming instructions. As used in this document and in the claims, the terms &#x201c;processor&#x201d; and &#x201c;processing device&#x201d; may refer to a single processor or any number of processors in a set of processors that collectively perform a set of operations, such as a central processing unit (CPU), a graphics processing unit (GPU), a remote server, or a combination of these. The programming instructions may be configured to cause the processor to perform any or all of the methods described above. Read only memory (ROM), random access memory (RAM), flash memory, hard drives and other devices capable of storing electronic data constitute examples of memory devices <b>525</b>. A memory device may include a single device or a collection of devices across which data and/or instructions are stored.</p><p id="p-0061" num="0056">An optional display interface <b>530</b> may permit information from the bus <b>500</b> to be displayed on a display device <b>535</b> in visual, graphic or alphanumeric format. An audio interface and audio output (such as a speaker) also may be provided. Communication with external devices may occur using various communication devices <b>540</b> such as a wireless antenna, a radio frequency identification (RFID) tag and/or short-range or near-field communication transceiver, each of which may optionally communicatively connect with other components of the device via one or more communication system. The communication device <b>540</b> may be configured to be communicatively connected to a communications network, such as the Internet, a local area network or a cellular telephone data network.</p><p id="p-0062" num="0057">The hardware may also include a user interface sensor <b>545</b> that allows for receipt of data from input devices <b>550</b> such as a keyboard, a mouse, a joystick, a touchscreen, a touch pad, a remote control, a pointing device and/or microphone. Such devices may be used to help label images in training the model. Digital image frames also may be received from a camera <b>520</b> that can capture video and/or still images.</p><p id="p-0063" num="0058"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates example applications of the methods described in this document to actual images. In particular, <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example comparison of an original image sequence <b>601</b> that includes scene text (the word &#x201c;coffee&#x201d;) and a modified image sequence <b>602</b> in which the scene text has been replaced (with the word &#x201c;robert&#x201d;) using the methods described above.</p><p id="p-0064" num="0059">The description above explains that the processes of this document may apply to text, which is defined about. However, in additional embodiments the same processes may be applied to alter, or remove and replace, other objects in an image. Object may include, for example, vehicles, people, animals, buildings, signs (including both text and the substrate on which text is printed) or any other object that appears in a picture. The object may be detected by any classifier (i.e., a machine learning model such as an artificial neural network that has been trained on a set of training images which have been labeled with objects that appear in the images). Alternatively, the object may be identified via a human operating a user interface, such as an interface that defines a bounding box around the object and receives a label for the object.</p><p id="p-0065" num="0060">Terminology that is relevant to this disclosure includes:</p><p id="p-0066" num="0061">An &#x201c;electronic device&#x201d; or a &#x201c;computing device&#x201d; refers to a device or system that includes a processor and memory. Each device may have its own processor and/or memory, or the processor and/or memory may be shared with other devices as in a virtual machine or container arrangement. The memory will contain or receive programming instructions that, when executed by the processor, cause the electronic device to perform one or more operations according to the programming instructions. Examples of electronic devices include personal computers, servers, mainframes, virtual machines, containers, gaming systems, televisions, digital home assistants and mobile electronic devices such as smartphones, personal digital assistants, cameras, tablet computers, laptop computers, media players and the like. Electronic devices also may include components of vehicles such as dashboard entertainment and navigation systems, as well as on-board vehicle diagnostic and operation systems. In a client-server arrangement, the client device and the server are electronic devices, in which the server contains instructions and/or data that the client device accesses via one or more communications links in one or more communications networks. In a virtual machine arrangement, a server may be an electronic device, and each virtual machine or container also may be considered an electronic device. In the discussion above, a client device, server device, virtual machine or container may be referred to simply as a &#x201c;device&#x201d; for brevity. Additional elements that may be included in electronic devices are discussed above in the context of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0067" num="0062">The terms &#x201c;processor&#x201d; and &#x201c;processing device&#x201d; refer to a hardware component of an electronic device that is configured to execute programming instructions. Except where specifically stated otherwise, the singular terms &#x201c;processor&#x201d; and &#x201c;processing device&#x201d; are intended to include both single-processing device embodiments and embodiments in which multiple processing devices together or collectively perform a process.</p><p id="p-0068" num="0063">The terms &#x201c;computer-readable medium,&#x201d; &#x201c;memory,&#x201d; &#x201c;memory device,&#x201d; &#x201c;data store,&#x201d; &#x201c;data storage facility,&#x201d; and the like each refer to a non-transitory device on which computer-readable data, programming instructions or both are stored. Except where specifically stated otherwise, the terms &#x201c;memory,&#x201d; &#x201c;memory device,&#x201d; &#x201c;data store,&#x201d; &#x201c;data storage facility&#x201d; and the like are intended to include single device embodiments, embodiments in which multiple memory devices together or collectively store a set of data or instructions, as well as individual sectors within such devices. A &#x201c;computer program product&#x201d; is a computer-readable medium with programming instructions stored thereon.</p><p id="p-0069" num="0064">The terms &#x201c;machine learning model&#x201d; or &#x201c;model&#x201d; refers to a set of algorithmic routines and parameters that can predict an output(s) of a real-world process (e.g., replacement of scene text) based on a set of input features, without being explicitly programmed. A structure of the software routines (e.g., number of subroutines and relation between them) and/or the values of the parameters can be determined in a training process, which can use actual results of the real-world process that is being modeled. Such systems or models are understood to be necessarily rooted in computer technology, and in fact, cannot be implemented or even exist in the absence of computing technology. While machine learning systems utilize various types of statistical analyses, machine learning systems are distinguished from statistical analyses by virtue of the ability to learn without explicit programming and being rooted in computer technology. Example machine learning models include neural networks such as convolutional neural networks (CNNs), generative adversarial networks (GANs), and other trained networks that exhibit artificial intelligence.</p><p id="p-0070" num="0065">The term &#x201c;bounding box&#x201d; refers to a rectangular box that represents the location of an object. A bounding box may be represented in data by x- and y-axis coordinates [x<sub>max</sub>, y<sub>max</sub>] that correspond to a first corner of the box (such as the upper right corner), along with x- and y-axis coordinates [x<sub>min</sub>, y<sub>min</sub>] that correspond to the corner of the rectangle that is opposite the first corner (such as the lower left corner). It may be calculated as the smallest rectangle that contains all of the points of an object, optionally plus an additional space to allow for a margin of error. The points of the object may be those detected by one or more sensors, such as pixels of an image captured by a camera.</p><p id="p-0071" num="0066">The features and functions described above, as well as alternatives, may be combined into many other different systems or applications. Various alternatives, modifications, variations or improvements may be made by those skilled in the art, each of which is also intended to be encompassed by the disclosed embodiments.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005108A1-20230105-M00001.NB"><img id="EMI-M00001" he="7.03mm" wi="76.20mm" file="US20230005108A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A digital image frame editing method comprising, by a processor:<claim-text>receiving a sequence of digital image frames;</claim-text><claim-text>processing the digital image frames to define a region of interest (ROI) that contains original text in each of the digital image frames;</claim-text><claim-text>using the ROIs of the digital image frames to select a reference frame from the sequence of digital image frames;</claim-text><claim-text>selecting a target frame from the sequence of digital image frames, wherein the target frame is different from the reference frame;</claim-text><claim-text>determining a transform function between the ROI of the reference frame and the ROI of the target frame;</claim-text><claim-text>replacing the original text in the ROI of the reference frame with replacement text to yield a modified reference frame ROI;</claim-text><claim-text>using the transform function to transform the modified reference frame ROI to a modified target frame ROI in which the original text is replaced with the replacement text, and</claim-text><claim-text>inserting the modified target frame ROI into the target frame to produce a modified target frame.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising, for each of a plurality of additional target frames in the sequence:<claim-text>determining an additional transform function between the reference frame ROI and the additional target frame ROI; and</claim-text><claim-text>using the additional transform function to transform the modified reference frame ROI to a modified additional target frame ROI in which the original text is replaced with the replacement text.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which using the ROIs to select the reference frame comprises analyzing the digital image frames of the sequence to select a frame in which the original text satisfies a function of one or more of the following criteria: a threshold optical character recognition confidence score; a sharpness criterion; a size criterion; a contrast criterion; or a pose criterion.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the transform function between the reference frame ROI and the target frame ROI comprises determining at least two of the following:<claim-text>a lighting correction transform element, which comprises a function that represents changes in illumination between the reference frame ROI and the target frame ROI;</claim-text><claim-text>a sharpness transform element, which comprises a function that represents differences in camera focus, capture resolution, or both between the reference frame ROI and the target frame ROI; or</claim-text><claim-text>a motion blur transform element, which comprises a function that represents differences in motion blur between the reference frame ROI and the target frame ROI.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein determining the lighting correction transform element, the sharpness transform element, and/or the motion blur transform element comprises training a machine learning model using training samples comprising pairs of reference frames and target frames.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein using the transform function to transform the modified reference frame ROI to the modified target frame ROI comprises applying the determined transform elements to the modified reference frame ROI to yield the modified target frame ROI.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> further comprising, before using the transform function to transform any of the target frame ROIs, normalizing and aligning the original text in the ROIs for each of the target frames to a consistent pose.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which replacing the original text in the reference frame ROI with replacement text to yield the modified reference frame ROI comprises:<claim-text>processing the ROI of the reference frame through a multitask machine learning model to determine a foreground image of the ROI, a background image of the ROI, and a binary mask that distinguishes foreground image pixels from background image pixels in the ROI;</claim-text><claim-text>receiving a new binary mask that contains replacement text; and</claim-text><claim-text>applying the new binary mask to blend the background image with the foreground image and yield a modified reference frame ROI.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the digital image frames to define the ROI in each frame comprises:<claim-text>applying a text detector to each of the digital image frames of the sequence to return bounding box coordinates; and</claim-text><claim-text>defining the ROIs of each of the digital image frames according to the bounding box coordinates.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A system for editing digital image frames, the system comprising:<claim-text>a processor; and</claim-text><claim-text>a memory device containing programming instructions that are configured to cause the processor to:<claim-text>receive a sequence of digital image frames,</claim-text><claim-text>process the digital image frames to define a region of interest (ROI) that contains original text in each of the digital image frames,</claim-text><claim-text>use the ROIs of the digital image frames to select a reference frame from the sequence of digital image frames,</claim-text><claim-text>select a target frame from the sequence of digital image frames, wherein the target frame is different from the reference frame,</claim-text><claim-text>determine a transform function between the ROI of the reference frame and the ROI of the target frame,</claim-text><claim-text>replace the original text in the ROI of the reference frame with replacement text to yield a modified reference frame ROI,</claim-text><claim-text>use the transform function to transform the modified reference frame ROI to a modified target frame ROI in which the original text is replaced with the replacement text, and</claim-text><claim-text>insert the modified target frame ROI into the target frame to produce a modified target frame.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising additional programming instructions to, for each of a plurality of additional target frames in the sequence:<claim-text>determine an additional transform function between the reference frame ROI and the additional target frame ROI; and</claim-text><claim-text>use the additional transform function to transform the modified reference frame ROI to a modified additional target frame ROI in which the original text is replaced with the replacement text.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, in which the instructions to use the ROIs to select the reference frame comprise instructions to analyze the digital image frames of the sequence to select a frame in which the original text satisfies a function of one or more of the following criteria: a threshold optical character recognition confidence score; a sharpness criterion; a size criterion; a contrast criterion; or a pose criterion.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the instructions to determine the transform function between the reference frame ROI and the target frame ROI comprise instructions to determine at least two of the following:<claim-text>a lighting correction transform element, which comprises a function that represents changes in illumination between the reference frame ROI and the target frame ROI;</claim-text><claim-text>a sharpness transform element, which comprises a function that represents differences in camera focus, capture resolution, or both between the reference frame ROI and the target frame ROI; or</claim-text><claim-text>a motion blur transform element, which comprises a function that represents differences in motion blur between the reference frame ROI and the target frame ROI.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instructions to determine the lighting correction transform element, the sharpness transform element, and/or the motion blur transform element comprise instructions to train a machine learning model using training samples comprising pairs of reference frames and target frames.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the instructions to use the transform function to transform the modified reference frame ROI to the modified target frame ROI comprise instructions to apply the determined transform elements to the modified reference frame ROI to yield the modified target frame ROI.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref> further comprising additional programming instructions that are configured to cause the processor to, before using the transform function to transform any of the target frame ROIs, normalize and align the original text in the ROIs for each of the target frames to a consistent pose.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the programming instructions to replace the original text in the reference frame ROI with replacement text to yield the modified reference frame ROI comprise instructions to:<claim-text>process the ROI of the reference frame through a multitask machine learning model to determine a foreground image of the ROI, a background image of the ROI, and a binary mask that distinguishes foreground image pixels from background image pixels in the ROI;</claim-text><claim-text>receive a new binary mask that contains replacement text; and</claim-text><claim-text>apply the new binary mask to blend the background image with the foreground image and yield a modified reference frame ROI.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the instructions to process the digital image frames to define the ROI in each frame comprise instructions to:<claim-text>apply a text detector to each of the digital image frames of the sequence to return bounding box coordinates; and</claim-text><claim-text>define the ROIs of each of the digital image frames according to the bounding box coordinates.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A computer program product comprising a computer-readable medium containing programming instructions that are configured to cause the processor to:<claim-text>receive a sequence of digital image frames,</claim-text><claim-text>process the digital image frames to define a region of interest (ROI) that contains original text in each of the digital image frames,</claim-text><claim-text>use the ROIs of the digital image frames to select a reference frame from the sequence of digital image frames,</claim-text><claim-text>select a target frame from the sequence of digital image frames, wherein the target frame is different from the reference frame,</claim-text><claim-text>determine a transform function between the ROI of the reference frame and the ROI of the target frame,</claim-text><claim-text>replace the original text in the ROI of the reference frame with replacement text to yield a modified reference frame ROI;</claim-text><claim-text>use the transform function to transform the modified reference frame ROI to a modified target frame ROI in which the original text is replaced with the replacement text, and</claim-text><claim-text>insert the modified target frame ROI into the target frame to produce a modified target frame.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer program product of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising additional programming instructions to, for each of a plurality of additional target frames in the sequence:<claim-text>determine an additional transform function between the reference frame ROI and the additional target frame ROI; and</claim-text><claim-text>use the additional transform function to transform the modified reference frame ROI to a modified additional target frame ROI in which the original text is replaced with the replacement text.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A digital image frame editing method comprising, by a processor:<claim-text>receiving a sequence of digital image frames;</claim-text><claim-text>processing the digital image frames to define a region of interest (ROI) that includes an object in each of the digital image frames;</claim-text><claim-text>using the ROIs of the digital image frames to select a reference frame from the sequence of digital image frames;</claim-text><claim-text>selecting a target frame from the sequence of digital image frames, wherein the target frame is different from the reference frame;</claim-text><claim-text>determining a transform function between the ROI of the reference frame and the ROI of the target frame;</claim-text><claim-text>replacing the object in the ROI of the reference frame with a replacement object to yield a modified reference frame ROI;</claim-text><claim-text>using the transform function to transform the modified reference frame ROI to a modified target frame ROI in which the object is replaced with the replacement object, and</claim-text><claim-text>inserting the modified target frame ROI into the target frame to produce a modified target frame.</claim-text></claim-text></claim></claims></us-patent-application>