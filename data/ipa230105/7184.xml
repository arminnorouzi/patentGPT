<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007185A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007185</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364552</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>7</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>7</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232127</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0002</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>7</main-group><subgroup>1413</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>7</main-group><subgroup>10722</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30168</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHOD OF DETECTING AND CORRECTING FOCUS DRIFT OF VARIABLE FOCUS LENS FOR FIXED FOCUS APPLICATIONS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>ZEBRA TECHNOLOGIES CORPORATION</orgname><address><city>Lincolnshire</city><state>IL</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kuchenbrod</last-name><first-name>Harry E.</first-name><address><city>Kings Park</city><state>NY</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for correcting focus drift of an imaging system. The method includes an imaging system obtaining a plurality of images of an object of interest with each image obtained at a different focus of the imaging system. A processor then determines image property values of each image of the plurality of images and determines an image quality metric from each image of the plurality of images. The processor then compares the image quality metric to a reference metric and determines, based on the comparison, if a focus drift has occurred, and further adjusts a focus of the imaging system if a focus drift has occurred.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="101.60mm" wi="158.75mm" file="US20230007185A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="111.34mm" wi="160.19mm" file="US20230007185A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="245.19mm" wi="160.53mm" orientation="landscape" file="US20230007185A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="191.09mm" wi="140.21mm" orientation="landscape" file="US20230007185A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="231.31mm" wi="154.09mm" file="US20230007185A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="210.48mm" wi="138.51mm" file="US20230007185A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="247.23mm" wi="153.92mm" file="US20230007185A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="89.58mm" wi="132.76mm" file="US20230007185A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0002" num="0001">Devices such as barcode scanners are used in various inventory applications. In some configurations, these barcode scanners use internal auto-focus cameras to capture images of barcodes and other scannable indicia. Electrically tunable variable focus lenses allow for auto-focusing of a barcode scanner on objects of interest (OOI) in machine vision and bar-code scanning applications. One drawback of electrically tunable variable focus lenses is that they do not provide any feedback signal indicative of a current focus, or current optical power, which can change over time due to ambient temperature variation, lens aging, voltage drift, and other factors. The change in the camera focus results in misreads or the inability of an autofocus camera to scan an OOI at a designated distance or range of distances. Also, the location of an OOI in a field of view may change, such as on a conveyer belt, and an autofocusing scanner Is unable to determine if an OOI is out of focus due to a change in the distance of the OOI, or if the imaging system has undergone focus drift. Furthermore, the need for accurate and fast focusing on varied objects adds considerable processing complexity and mechanical componentry to autofocusing camera systems, all of which increase costs.</p><p id="p-0003" num="0002">Autofocus imaging systems with liquid lenses typical perform any type of focus tuning at job setup and require the focus to be held indefinitely. During operation the focus of the autofocus system may change or shift due to age, humidity, and temperature among other factors. The inability of systems to determine whether the focus of the system has shifted, or if an object distance has changed may cause a system to perform a focus shift when it is unnecessary which may cause the OOI to be further out of focus. Additionally, any focus compensation or tuning of a system causes a reduction in scanning rates which reduces the efficiency of the imaging system.</p><p id="p-0004" num="0003">Accordingly, there is a need for determining if a focus drift has occurred, and also for correcting for any potential focus drift for imaging capture systems such as barcode readers.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0005" num="0004">In an embodiment, the present invention is a method for correcting focus drift of an imaging system. The method includes obtaining, by the imaging system, a first image of an object of interest, the first image obtained with the imaging system having a focus at a near focal plane distance; obtaining, by the imaging system, a second image of a second object of interest, the second image obtained with the imaging system having a focus at a far focal plane distance, and wherein the near focal plane distance and far focal plane distance bound a focus distance band which includes a central focal plane distance; determining, by a processor, a first image property value of the first image and a second image property value of the second image; determining, by a processor, an image quality metric from the first and second image property values; comparing, by the processor, the image quality metric to a reference metric; and based on the comparison, (i) determining, by the processor, that a focus drift has not occurred when the image quality metric is within a threshold of the reference metric, or (ii) determining, by the processor, that a focus drift has occurred when the image quality metric is greater than the threshold of the reference metric, and adjusting, by the imaging system, a central focal plane distance of a variable focus lens of the imaging system according to the focus drift.</p><p id="p-0006" num="0005">In a variation of this embodiment, the method may further include obtaining, by the imaging system, a first reference image of a reference object, the first reference image obtained with the imaging system having a focus at the near focal plane distance; obtaining, by the imaging system, a second reference image of the reference object, the second reference image obtained with the imaging system having a focus at the far focal plane distance; determining, by a processor, a first reference image property value of the first reference image and a second reference image property value of the second reference image; and determining, by a processor, the reference metric from the first and second reference image property values.</p><p id="p-0007" num="0006">In yet another variation of the current embodiment, determining the image quality metric includes determining one of a difference between the first and second image property values, an addition of the first and second image property values, a quotient of the first and second image property values, or an average of the first and second image property values. In further variations, determining the image quality metric includes determining one of a difference between the first and second image property values, an addition of the first and second image property values, a quotient of the first and second image property values, or an average of the first and second image property values.</p><p id="p-0008" num="0007">In another embodiment, the present invention is a computer-implemented method for differentiation focus drift of an imaging system from position changes of an object of interest. The method includes obtaining, by the imaging system, an image of an object of interest; identifying, by the processor, a region of interest in the image, wherein the region of interest contains an indicia indicative of the object of interest; determining, by the processor, an image quality of the image; analyzing, by the processor, the indicia and determining a pixel measurement of the indicia, and comparing, by the processor, the pixel measurement of the indicia to a reference pixel measurement; and based on the comparison, determining, by the processor, that the image quality of the image results from a difference in a distance of the object from the imaging system and a reference distance from the imaging system when the pixel measurement is outside of a threshold of the reference pixel measurement, and determining that the image quality of the image results from focusing drift of the imaging system when the pixel measurement is within a threshold of the reference pixel measurement.</p><p id="p-0009" num="0008">In a variation of the current embodiment, method further includes obtaining, by the imaging system, an image of the reference indicia; and performing, by the processor, the reference pixel measurement on the reference indicia. In further variations of the current embodiment, the method includes determining, by the processor and from the pixel measurement, a distance of the object of interest away from the imaging system; and wherein comparing the pixel measurement to the reference pixel measurement includes comparing, by the processor, the distance of the object of interest with a reference distance, the reference distance determined from the reference pixel measurement.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS</heading><p id="p-0010" num="0009">The accompanying figures, where like reference numerals refer to identical or functionally similar elements throughout the separate views, together with the detailed description below, are incorporated in and form part of the specification, and serve to further illustrate embodiments of concepts that include the claimed invention, and explain various principles and advantages of those embodiments.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a variable focus scanning station, in accordance with an embodiment of the present invention.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an exemplary block diagram schematic of an imaging reader of the variable focus scanning station of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in accordance with an embodiment of the present invention.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a perspective view of another variable focus scanning station with variable determined imaging planes, in accordance with an embodiment of the present invention.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart representative of a method for differentiating focus drift of an imaging system from a position change of an object of interest.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is an image of an example indicia as a barcode scanned by an imaging reader.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is an example image of an indicia as a QR code scanned by an imaging reader.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart representative of a method for correcting focus drift in an imaging system.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a variable focus scanning station imaging reader having multiple focal planes for performing the method of <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0019" num="0018">Skilled artisans will appreciate that elements in the figures are illustrated for simplicity and clarity and have not necessarily been drawn to scale. For example, the dimensions of some of the elements in the figures may be exaggerated relative to other elements to help to improve understanding of embodiments of the present invention.</p><p id="p-0020" num="0019">The apparatus and method components have been represented where appropriate by conventional symbols in the drawings, showing only those specific details that are pertinent to understanding the embodiments of the present invention so as not to obscure the disclosure with details that will be readily apparent to those of ordinary skill in the art having the benefit of the description herein.</p><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">Electrically controlled variable focus (VF) lenses are convenient, low-power solutions for fast auto-focus (AF) on objects of interests (OOI) in machine vision and bar-code applications. However, when location of objects of interest in the field of view varies (e.g., parts moving on conveyor, a person moving an object across a scanner field of view, etc.) an AF imaging system may be unable to determine the OOI, or if the OOI is at a different distance from the imaging system. In many scanning applications, it is desirable to keep focus plane constant. VF lenses do not provide any feedback signal indicating a current optical power or focus which can change after initial settings due to ambient temperature variation, aging, and other factors. The proposed system and method allow for the system to determine if a focus shift has occurred, or if the OOI is at a different distance from the imaging system. The disclosed system and methods also enable the correction of focus drift of an imaging system utilizing a VF lens.</p><p id="p-0022" num="0021">The disclosed system and methods propose to obtain images of one or more OOI's and determine whether an image quality of the obtained images is due to a difference in OOI distance, or due to focus drift of the imaging system. The method may use a pixel measurement of one or more of the images to determine whether a focus drift has occurred. Further, the system and methods disclosed enable correcting for focus drift of an imaging system. The methods include obtaining images of an OOI at different focuses of the imaging system, and determining image property values for the different images. The image property values are compared, a focus drift is determined, and the focus of the imaging system is adjusted according to the determined focus drift.</p><p id="p-0023" num="0022">Referring now to the drawings, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an exemplary environment where embodiments of the present invention may be implemented, including the processes described and illustrated herein. In the present example, the environment is provided in the form of a scanning station <b>100</b> where goods <b>102</b> are moved across or along a scanning surface <b>104</b> and are scanned by an imaging reader <b>106</b> to identify the goods <b>102</b>. In some embodiments, the scanning station is a point-of-sale (POS) station, which may have a computer system and an interface, not shown, for optically scanning goods and identifying the goods and characteristics of the goods for affecting a transaction. In some embodiments, the scanning station <b>100</b> is part of an inventory delivery system, where goods are conveyed by the scanning surface or across the scanning surface to monitor and control delivery of the goods, for example, shipping goods from a facility or receiving shipped goods to a facility.</p><p id="p-0024" num="0023">The scanning surface <b>104</b> may be a stationary surface, such that the goods <b>102</b> are manually moved relative to the surface <b>104</b>. In embodiments, the scanning surface <b>104</b> may move the goods <b>102</b> or be moved by another automated means. In other embodiments, the scanning surface <b>104</b> may be a moving surface, such as by a conveyor system such as a conveyer belt, pneumatic conveyer, wheel conveyer, roller conveyer, chain conveyer, flat conveyer, vertical conveyer, trolley conveyer, or another conveyer. In any case, the goods <b>102</b> may be moved continuously relative to the imaging reader <b>106</b>, such that the goods <b>102</b> are constantly moving through a working (or scanning) range <b>108</b> of the station <b>100</b>. In some examples, the goods <b>102</b> move in a discretized manner, where, at least part of the time the goods <b>102</b> are maintained fixed on the surface <b>104</b> relative to the imaging reader <b>106</b> for a period of time, sufficient to allow one or more images to be captured of the goods <b>102</b>.</p><p id="p-0025" num="0024">The goods <b>102</b> may move along different substantially linear paths <b>110</b>A, <b>110</b>B, etc. each path traversing the working range <b>108</b> but at a different distance from the imaging reader <b>106</b>. Indeed, the paths <b>110</b>A, <b>110</b>B are for illustration purposes, as the goods <b>102</b> may traverse across the surface <b>104</b> at any distance from the imaging reader <b>106</b>.</p><p id="p-0026" num="0025">In some exemplary embodiments, the imaging reader <b>106</b> includes a variable focus imaging system, in which the reader <b>106</b> continuously scans for an object of interest (OOI) (such as the goods <b>102</b>), in its field of view until the object, or a region of interest of the OOI (e.g., a barcode, serial number, other identifiers, etc.) is located and then brought sufficiently into focus on the imaging sensor. With at least some embodiments of the present invention, the imaging reader scans for the object (e.g., goods <b>102</b>) only at discretized, determined distances, corresponding to imaging planes of the imaging reader <b>106</b>. Instead of continuous scanning, the imaging reader <b>106</b> more quickly captures images at one or more predetermined imagining planes. The imaging planes are defined relative to the imaging reader <b>106</b>. For illustration purposes, in <figref idref="DRAWINGS">FIG. <b>1</b></figref> imaging planes happen to coincide with paths (e.g., <b>110</b>A, <b>110</b>B, etc.) over which the goods <b>102</b> traverse. Although, it is more likely the case that the imaging planes will not exactly coincide with the scan path of goods. While, in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the imaging reader <b>106</b> is depicted as being to the side of the goods <b>102</b>, in embodiments, the imaging reader <b>106</b> may be positioned directly above the goods <b>102</b>, above the goods <b>102</b> in front of or behind the goods <b>102</b> configured to image the OOI, or at another positon for imaging a region of interest of the goods <b>102</b> or any OOI. The imaging reader <b>106</b> captures images at each of the imaging planes, where the captured images of the good will vary in focus depending on where the good (and its scan path) is relative to the imaging reader <b>106</b>. That is, the good will appear more in focus at some imaging planes in comparison to others. By capturing images of the goods at only certain imaging planes, the imaging reader <b>106</b> is able to identify the goods <b>102</b> much faster than a conventional autofocus system. Indeed, the imaging reader <b>106</b> can be configured such that if it has an autofocus operation that operation is disabled, and instead images are captured at specific imaging planes irrespective of which scan path the good traverses and without needing to continuously detect the good and autofocus onto the good. This operation greatly reduces power consumption demands on the imaging reader <b>106</b>.</p><p id="p-0027" num="0026">As previously described, electrically tunable AF lenses may undergo focus plane drift due to environmental and other factors, which causes the defocusing of images of OOI reducing the efficacy of the VF imaging reader <b>106</b>. As discussed further herein, the identification and scanning efficiencies can be increased having the imaging reader <b>106</b> actively determine if a focus drift has occurred, and actively determine and correct for the focus drift. The disclosed systems and methods enable the increased efficiency, and therefore reduced time required, for reading identifiers on an OOI, e.g., to identify an indicia or other barcode on the good. At least some of the image quality metrics and parameters, scanning parameters, and/or calibration parameters described further herein, may be stored on a server <b>112</b> communicatively coupled to the imaging reader <b>106</b>, and the imaging reader may retrieve the image quality metrics and parameters, scanning parameters, and/or calibration parameters, from the server or another memory or form of storage.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a block connection diagram of system <b>200</b> including an imaging reader <b>106</b>. In <figref idref="DRAWINGS">FIG. <b>2</b></figref> the imaging reader <b>106</b> may have one or more processors and one or more memories storing computer executable instructions to perform operations associated with the systems and methods as described herein. The imaging reader <b>106</b> includes a network input/output (I/O) interface for connecting the reader to the server <b>112</b>, an inventory management system (not shown), and other imaging readers. These devices may be connected via any suitable communication means, including wired and/or wireless connectivity components that implement one or more communication protocol standards like, for example, TCP/IP, WiFi (802.11b), Bluetooth, Ethernet, or any other suitable communication protocols or standards. The imaging reader <b>106</b> further includes a display for providing information such as visual indicators, instructions, data, and images to a user.</p><p id="p-0029" num="0028">In some embodiments, the server <b>112</b> (and/or other connected devices) may be located in the same scanning station <b>100</b>. In other embodiments, server <b>112</b> (and/or other connected devices) may be located at a remote location, such as on a cloud-platform or other remote location. In still other embodiments, server <b>112</b> (and/or other connected devices) may be formed of a combination of local and cloud-based computers.</p><p id="p-0030" num="0029">Server <b>112</b> is configured to execute computer instructions to perform operations associated with the systems and methods as described herein. The server <b>112</b> may implement enterprise service software that may include, for example, RESTful (representational state transfer) API services, message queuing service, and event services that may be provided by various platforms or specifications, such as the J2EE specification implemented by any one of the Oracle WebLogic Server platform, the JBoss platform, or the IBM WebSphere platform, etc. Other technologies or platforms, such as Ruby on Rails, Microsoft .NET, or similar may also be used.</p><p id="p-0031" num="0030">In the illustrated example, the imaging reader <b>106</b> includes a light source <b>202</b>, which may be a visible light source (e.g., a LED emitting at 640 nm) or an infrared light source (e.g., emitting at or about 700 nm, 850 nm, or 940 nm, for example), capable of generating an illumination beam that illuminates the working range <b>108</b> for imaging over an entire working distance of that working range <b>108</b>. That is, the light source <b>202</b> is configured to illuminate over at least the entire working range <b>108</b>. The illumination intensity of the light source <b>202</b> and the sensitivity of an imaging reader can determine the further and closest distances (defining the distance of the working range, also termed the scanning range) over which a good can be scanned, and a barcode on the good can be decoded. The light source <b>202</b> is controlled by processor and may be a continuous light source, an intermittent light source, or a signal-controlled light source, such as a light source trigged by an object detection system coupled (or formed as part of though not shown) to the imaging reader <b>106</b>. The light source may be an omnidirectional light source.</p><p id="p-0032" num="0031">The imaging reader <b>106</b> further includes an imaging arrangement <b>204</b> having an imaging sensor <b>206</b> positioned to capture images of an illuminated target, such as the goods <b>102</b> or another OOI, within the working range <b>108</b>. In some embodiments, the imaging sensor <b>206</b> is formed of one or more CMOS imaging arrays. A variable focusing optical element <b>208</b> is positioned between the imaging sensor <b>206</b> and a window <b>210</b> of the imaging reader <b>106</b>. A variable focus imaging controller <b>214</b> is coupled to the variable focusing optical element <b>208</b> and controls the element <b>208</b> to define one or more discrete imaging planes for the imaging sensor. The one or more discrete imaging planes may be considered one or more central focal planes as described here. As previously described, a central focal plane is the imaging plane that is expected to result in the greatest image edge sharpness value.</p><p id="p-0033" num="0032">In the illustrated example, the controller <b>214</b> is coupled to the variable focusing optical element <b>208</b> through an actuator control unit <b>215</b> and bypasses an optional autofocus control unit <b>217</b>, thereby providing faster image capture at the desired imaging planes by overriding the slower autofocus control units of conventional systems. In exemplary embodiments, the imaging reader <b>106</b> does not have an autofocus control unit or any autofocus functionality. The actuator <b>215</b> may include a focusing lens drive, a shift lens drive, a zoom lens drive, an aperture drive, angular velocity drive, voice coil motor drive, and/or other drive units for controlling operation of the optical element <b>208</b>, which itself may comprise multiple lens, lens stages, etc.</p><p id="p-0034" num="0033">The VF optical element <b>208</b> may be a deformable lens element, a liquid lens, a T-lens or another VF optical element. In some embodiments, the optical element includes a voice coil actuator motor in the actuator <b>215</b> that is controllably adjusted by the controller <b>214</b>. In exemplary embodiments, such as some barcode scanning applications, the VF optical element <b>208</b> has an aperture from 1.5 mm to 3 mm. In some embodiments, the image stage <b>204</b> is implemented as part of a VF camera assembly.</p><p id="p-0035" num="0034">In embodiments, the VF imaging controller <b>214</b> is configured to access one or more scanning parameters <b>216</b> stored in the imaging reader <b>106</b>, stored on the server <b>112</b>, or stored on another medium. From these scanning parameters <b>216</b>, the controller <b>214</b> determines the number of discrete central focus imaging planes at which the imaging reader <b>106</b> scans for and captures images of the target or an OOI (such as goods <b>102</b>). The controller <b>214</b> further determines the distance of each of those central focus imaging planes, as measured from the imaging sensor <b>206</b>. The controller <b>214</b>, for example, may determine the number and distance of the central focus imaging planes so that the entire working range <b>108</b> is covered by five (5) or fewer imaging planes. In some examples, depending on the scanning parameters <b>216</b>, the number of central focus imaging planes may be three (3) or fewer.</p><p id="p-0036" num="0035">The imaging controller <b>214</b> converts these determined central focus imaging planes and central focal distances into parameters or instructions for controlling the actuator <b>215</b> for controlling the variable focus optical element <b>208</b>.</p><p id="p-0037" num="0036">In exemplary embodiments, the variable focus imaging controller <b>214</b> has hands-free mode in which the variable focus optical element <b>208</b> and the imaging sensor <b>206</b> are controlled to capture an image of the target at each of the central focus imaging planes within the working range in an ordered manner to form a set of captured images of the target, stored in the memory. In some embodiments, that ordered manner is a sequential order, such as from nearest central focus imaging plane to farthest or vice versa, as measured from the imaging sensor <b>206</b>.</p><p id="p-0038" num="0037">In some exemplary embodiments, the imaging reader <b>106</b> is implemented in a handheld bar code scanner device. When the handheld scanner is placed within a stationary cradle thereby establishing an upright scanning position, the handheld scanner may automatically sense that placement and enter the hands-free mode. In other exemplary embodiments, the imaging reader <b>106</b> is implemented as a multi-plane scanner, such as a bioptic scanner as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0039" num="0038">In exemplary embodiments, the variable focus optical element <b>208</b> is discretely controlled to hop to each central focus imaging plane, avoiding a sweeping operation between imaging planes.</p><p id="p-0040" num="0039">In embodiments, the imaging sensor <b>112</b> may be a charge coupled device, or another solid-state imaging device. The imaging sensor <b>112</b> may be a one megapixel sensor with pixels of approximately three microns in size. In embodiments, the imaging sensor <b>112</b> includes a sensor having an active area of 3 millimeters, 4.5 millimeters, 5 millimeters, 6.8 millimeters, 7.13 millimeters, less than 5 millimeters, less than 10 millimeters, or less than 50 millimeters. The imaging sensor <b>112</b> may have a total of about 1 megapixels, 2 megapixels, 2.3 megapixels, 5 megapixels, 5.1 megapixels or more than 5 megapixels. Further, the imaging sensor <b>112</b> may include sensors with pixels having dimensions of less than 10 microns, less than 5 microns, less than 3 microns, or less than 2 microns in size in at least one dimension of the pixel. In embodiments, the lens assembly is configured to capture images with a modulation transfer function of 40% at 160 line pairs per millimeter.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an exemplary scanning station <b>300</b> having an imaging reader <b>302</b>, in the form of a bioptic scanner, having a housing <b>304</b> and a first scanning window <b>306</b> behind which is an illumination source (not shown) and an imaging stage (not shown) like that of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The imaging reader <b>302</b> is positioned adjacent a scanning surface <b>308</b> and defines a horizontally and vertically extending working range <b>310</b> illuminated by the imaging reader <b>302</b> and having defined therein 3 defined middle imaging planes <b>312</b>, <b>314</b>, and <b>316</b> at which the imaging reader <b>302</b> captures images of an object for identification and imaging. For clarity and simplicity, a single central imaging plane will be further used in descriptions of the systems and methods for determining if a focus shift has occurred, and for correcting the focus shift. It should be understood that the systems and methods are not limited to a single central focus imaging plane, and that the disclosed systems and methods may be applied for determining and correcting focus shift for any number of focus imaging planes as practicable by the limitations (i.e., imaging camera resolution, imaging system working range, illumination capabilities, etc.) of the imaging system.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a method <b>400</b> of differentiating focus drift of an imaging system from a position change of an object of interest. The method <b>400</b> may be implemented in systems exhibiting a small image sharpness tolerance (i.e. a required minimum sharpness is less than 20% of the maximum image sharpness), systems with a relatively a slow conveyor speeds, and/or relatively low image frame rates wherein several non-decoded frames are acceptable while maintaining a 100% decode rate, resulting in identification and decoding of indicia indicative of all OOIs. The method <b>400</b> may be implemented by the imaging readers <b>106</b> and <b>302</b> of any of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>3</b></figref>, for example. In the example of a handheld scanner, initially, the imaging reader enters a hands-free mode indicating that swipe scanning is to be performed. In some examples, the handheld scanner includes a trigger on or near a handle, and the trigger, when depressed, enters the scanner into the hands-free mode for swipe scanning. In some embodiments, the imaging scanner is part of a scanning station of an inventory system, where goods are conveyed by the scanning surface or across the scanning surface to monitor and control delivery of the goods, for example, shipping goods from a facility or receiving shipped goods to a facility, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Therefore, the imaging scanner of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may operate mainly in the hands-free mode, and a manual mode may be activated for manual scanning and/or calibration purposes. Further, the methods described herein may be implemented with the scanner of <figref idref="DRAWINGS">FIG. <b>1</b></figref> being an inspection station where on OOI moves into a field of view of the scanner, the OOI pauses momentarily in the field of view to be imaged, and then the OOI moves out of the field of view.</p><p id="p-0043" num="0042">For simplicity the method <b>400</b> will be described in reference to components of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, but it should be understood that the method <b>400</b> may be performed for any imaging system for scanning a target or object of interest. The imaging reader <b>106</b> obtains scanning parameters at block <b>402</b>. In exemplary embodiments, the scanning parameters include frame rate of the image sensor of the imaging reader, exposure time of that image sensor, the aperture or aperture range of the variable focus optical element, the swipe speed of movement of the target, an automatic movement speed of a conveyer belt or other speed of a means for automatic swiping, a size of the working distance, a nearest distance of the working distance, a farthest distance of the working distance, and/or focusing time of the variable focus optical element. Such parameters may be stored within the imaging reader <b>106</b>, for example, in memory of the imaging reader, and/or one or more of the scanning parameters may be stored in another system such as at the server <b>200</b>. Further, one or more of the scanning parameters may be stored in individual elements of the imaging reader <b>106</b> such as in the variable focus optical element <b>208</b>, variable focus imaging controller <b>214</b>, and/or other elements of the imaging reader <b>106</b>.</p><p id="p-0044" num="0043">In some embodiments, the scanning parameters are target specific parameters, such as the types and/or sizes of the targets to be scanned. In some embodiments, the scanning parameters include the types of indicia on the targets, such as whether the targets contain 1D or 2D barcodes, QR codes, UPC codes, or other identifying indicia.</p><p id="p-0045" num="0044">In some embodiments, the obtaining the scanning parameters may include obtaining calibration parameters. The calibration parameters may include one or more defocus parameters, near focus plane parameters, central focus plane parameters, and far focus plane parameters. In some embodiments, the calibration parameters may include scanning parameters for determining the near, center, and far focus planes. In some examples, some scanning parameters and calibration parameters are obtained from the imaging reader communicating with a server, such as the server <b>112</b>, which may include an inventory control manager that access information on targets and OOIs.</p><p id="p-0046" num="0045">The variable focus imaging controller <b>214</b>, at <b>404</b>, controls the imaging sensor <b>206</b> to obtain an image of a target OOI, such as the goods <b>102</b>. The variable focus imaging controller <b>214</b> may assess the obtained scanning parameters and control the image sensor, variable focus optical element, and other components of the imaging reader <b>106</b> according to the scanning parameters to obtain the image. The processor of the imaging reader <b>106</b>, at <b>406</b>, identifies a region of interest in the captured image. The region of interest in the image may include a barcode, a serial number, alphanumeric, a graphic, or another indicia indicative of the target or OOI.</p><p id="p-0047" num="0046">At <b>408</b>, the processor of the imaging reader <b>106</b> analyzes the image and determines an image quality of the image. For example, at <b>408</b>, the processor may analyze the image of the OOI by performing image processing on the image. The image processing may include applying a spatial lowpass filter, spatial highpass filter, Fourier lowpass or highpass filter, performing a noise reduction, a scaling, rotation, shearing, reflection, or another image filtering or image processing technique. Additionally, analyzing the image, and determining image quality value for the image may include determining a sharpness value, a contrast value, an image resolution, a spatial frequency content value, a noise measurement value, a dynamic range value, a measurement of image distortion, a blur value, a pixels per module value, a modulation transfer function, or another value associated with an image or image quality.</p><p id="p-0048" num="0047">The description of method <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> is continued with additional reference to <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref>. <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is an example image of an example indicia <b>500</b> as a barcode scanned by the imaging reader <b>106</b>, and <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is an example image of the indicia <b>500</b> as a QR code scanned by the imaging reader <b>106</b>. While illustrated as an example barcode and QR code, the indicia <b>500</b> may be a 1D or 2D barcode, another barcode, a static !R code, a dynamic QR code, a UPC code, a serial number, an alphanumeric code, or another indicia indicative of an OOI. At block <b>410</b>, the processor analyzes the indicia <b>500</b> in the region of interest and at block <b>412</b>, the processor determines a pixel measurement of the indicia <b>500</b>. To analyze the indicia <b>500</b>, the processor may perform image processing on the region of interest to determine features of the indicia <b>500</b> for determining the pixel measurement. For example, the processor may sharpen the image and perform edge detections on the image to determine pixels of the image of the indicia <b>500</b> for performing the pixel measurement. While the example of sharpening and performing edge detection is one example, the processor may perform other image processing and analysis of the image of the indicia <b>500</b> for further determining the pixel measurement. For example, the pixel measurement may include one or more of measuring a pixel sharpness, contrast, edge detection, size, pixel resolution, spatial frequency content, image or spatial noise, dynamic range, measurement of image distortion, blur, pixels per module, rotation normalization, or another value associated with pixels of an image or image quality.</p><p id="p-0049" num="0048">In determining the pixel measurement, the processor may determine a pixel distance that is measured in a pixels per module (PPM). The PPM value of the image of the indicia <b>500</b> may be approximately 0.7 to tens of pixels per module. In embodiments, the described methods are applied with values of approximately 5&#xb1;0.5 PPM, or with values of less than 5 PPM. To perform the pixel measurement, the processor may determine a pixel distance between subgroups of bars of the barcode, e.g., the pixel distance <b>504</b><i>a </i>and <b>504</b><i>b, </i>the pixel distance may be a pixel distance across an entire dimension of the image of the barcode, e.g. the pixel distances <b>504</b><i>c </i>and <b>504</b><i>d, </i>or may be another pixel distance in the image of the barcode of <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. In the example of the QR code illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, the pixel distance may be determined to be the pixel distance <b>508</b><i>a </i>along a dimension of a position detection pattern, a distance <b>508</b><i>b </i>of a data area between position detection patterns <b>510</b>, an entire distance <b>508</b><i>c </i>between position detection patterns, a distance <b>508</b><i>d </i>from a position detection pattern <b>510</b> to an edge of the QR code, a distance <b>508</b><i>e </i>diagonally across data between position detection patterns <b>510</b>, a distance <b>508</b><i>f </i>across the entire length of a dimension of the QR code, a distance (not illustrated) diagonally across the entirety of the QR code, a distance across a subregion of the barcode, or another distance of the image of the barcode that is measurable in a number of PPM. Further, while not illustrated, the processor may perform a pixel measurement that may determine a pixel distance between or across multiple modules, a timing pattern, one or more alignment patterns, regions of code format information, across error correction patterns, format information, version information, or other data patterns of the QR code.</p><p id="p-0050" num="0049">At <b>414</b>, the processor of the imaging reader <b>106</b> compares the pixel measurement to a reference pixel measurement. For example, the reference pixel measurement may be a scanning parameter obtained at block <b>402</b>, and the reference pixel measurement may be indicative of a pixel measurement of an indicia that is focused and at a focal plane of the imaging system. The reference pixel measurement may be a reference pixel measurement of a barcode, QR code, UPC codes, or other identifying indicia. The reference pixel measurement may be stored in a memory of the imaging reader <b>106</b>, or another memory wherein the reference pixel measurement is retrievable by the imaging system. The reference pixel measurement may be a numerical value, image data indicative of a reference image of indicia, or another form of data for comparing the pixel measurement to. In embodiments, the method <b>400</b> may further include determining the reference pixel measurement. For example, the imagining reader <b>106</b> may obtain a reference image of a reference indicia. The reference indicia may be placed at a reference or calibrated distance from the imaging reader <b>106</b>, for example, the reference indicia may be placed at one of the linear paths <b>110</b>A or <b>110</b>B. The processor may then perform image processor and analysis of the image of the reference indicia, and the processor may perform the reference pixel measurement and determine a reference pixel distance.</p><p id="p-0051" num="0050">Based on the comparison of the pixel measurement with the reference pixel measurement, at <b>416</b>, the processor determines if the image quality of the image results from a difference in a distance of the OOI (e.g., the goods <b>102</b>) from the imaging reader <b>106</b> (e.g., from the imaging sensor <b>206</b> of the imaging system), or from a focus drift of the imaging reader <b>106</b>. The reference pixel measurement may be a pixel distance with a PPM value of less than 1, of less than 10, or on the order of tens of PPM. Further the tolerance of the PPM may be less than &#xb1;1, less than &#xb1;5, or less than &#xb1;10 PPM. At block <b>418</b>, the processor determines that the image quality is due to a change in the distance of the OOI from the imaging reader <b>106</b> when the pixel measurement is outside of the range of the threshold of the reference pixel measurement. At block <b>420</b> the processor determines that the image quality of the image is due to a focus drift of the imaging reader <b>106</b> when the pixel measurement is within the threshold range of the reference pixel measurement. Further, in embodiments wherein the pixel measurement includes a pixel distance between pixels of the image, or the region of interest of the image, at block <b>418</b> the processor may determine that the image quality is due to a change in the distance of the OOI from the imaging reader <b>106</b> when the pixel distance is outside of the range of the threshold of the reference pixel distance, and at block <b>420</b> the processor determines that the image quality of the image is due to a focus drift of the imaging reader <b>106</b> when the pixel distance is within the threshold range of the reference pixel distance.</p><p id="p-0052" num="0051">In embodiments, the method <b>400</b> may further determine a physical distance of the OOI from the imaging reader <b>106</b>. For example, one or more of the obtained scanning parameters may include a reference distance as a calibration parameter. The reference distance may be a known or calibrated distance of reference indicia used to determine the reference pixel measurement. The processor, may compare the pixel measurement to the reference pixel measurement and the distance of the OOI may be determined from the comparison. The processor may determine that the OOI is closer to or further from the imaging reader <b>106</b> by an amount of distance based on a difference between the pixel measurement and the reference pixel measurement (e.g., a difference between PPM values). The processor may determine that the OOI may be further to the imaging reader <b>106</b> if the pixel measurement results in a lower PPM value than the reference pixel measurement, or that the OOI is closer from the imaging system in the pixel measurement is greater than the reference pixel measurement. The distance of the OOI may be determined without the use of a reference distance. For example, by knowing specific parameters of optical elements of the imaging reader <b>106</b>, such as lens focuses in diopters, angles of fields of view, and other geometries and optical parameters the distance of the OOI can be determined.</p><p id="p-0053" num="0052">Further, it should be noted that the distance of the OOI from the imaging reader <b>106</b> is also dependent on where the OOI is in the FOV of the imaging reader <b>106</b>. For example, the OOI may be further from the imaging reader <b>106</b> near edges of the FOV, while the OOI may be closest to the imaging reader <b>106</b> at a center of the FOV of the imaging reader <b>106</b>. The position of the OOI and distance from the imaging reader <b>106</b> may be considered when determining if a distance to the OOI has changed, or if a focus drift has occurred in the imaging reader <b>106</b>.</p><p id="p-0054" num="0053">Further, at blocks <b>414</b> and <b>416</b>, to compare the pixel measurement and reference pixel measurement, the processor may compare a determined distance of the OOI and a reference distance. The processor may then determine, at block <b>418</b>, if the image quality is due to a change in the distance from the OOI to the imaging reader <b>106</b> when the determined distance of the OOI is outside of a threshold of the reference distance. Alternatively, the processor may determine that the image quality is due to a focus drift of the imaging reader <b>106</b> if the distance of the OOI is within the threshold of the reference distance.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart representative of a method <b>600</b> for correcting focus drift in an imaging system. The method <b>600</b> may be implemented in systems exhibiting a small image sharpness tolerance (i.e. a required minimum sharpness is less than 20% of the maximum image sharpness), systems with relatively slow conveyor speeds, and/or relatively low image frame rates while maintaining a 100% decode rate, resulting in identification and decoding of indicia indicative of all OOIs. The method <b>600</b> may be implemented by the imaging readers <b>106</b> and <b>302</b> of any of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>3</b></figref>, for example. <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a variable focus scanning station imaging reader <b>106</b> and multiple focal planes for imaging of goods <b>102</b> and for performing the method <b>600</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. For simplicity the method <b>600</b> will be described in reference to components of <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b> and <b>7</b></figref>, but it should be understood that the method <b>600</b> may be performed for any imaging system for scanning a target or object of interest.</p><p id="p-0056" num="0055">The imaging reader <b>106</b> obtains scanning parameters at block <b>602</b>. Such parameters may be stored within the imaging reader <b>106</b>, for example, in memory of the imaging reader, and/or one or more of the scanning parameters may be stored in another system such as at the server <b>200</b>. Further, one or more of the scanning parameters may be stored in individual elements of the imaging reader <b>106</b> such as in the variable focus optical element <b>208</b>, variable focus imaging controller <b>214</b>, and/or other elements of the imaging reader <b>106</b>. Various components including the variable focus imaging controller <b>214</b>, AF control <b>217</b>, actuator <b>215</b>, and processor of the imaging reader <b>106</b> may obtain and/or assess the scanning parameters for performing image scans and obtaining images of an OOI. The processor assesses the scanning parameters and determines a center focus plane A along the substantially linear path <b>110</b>A, a near focus plane A<sub>n</sub>, and a far focus plane A<sub>f</sub>, each having its own respective optical focus determined by the controller and each at a corresponding distance from the imaging reader <b>106</b>. The near focus plane is closer to the imaging reader <b>106</b> than the center focus plane, and the far focus plane is further from the imaging reader <b>106</b> than the center focus plane. In examples, the near focus plane and far focus plane may be equally distance from the center focus plane. In other examples the near focus plane and far focus plane may be at different distances from the center focus plane. In the described methods, the near focus plane and far focus planes are two imaging planes at which the imaging reader <b>106</b> will capture images of the target for detecting and correcting focus drift imaging reader <b>106</b>. In some embodiments, the scanning parameters may include parameters for determining more than two or three focus planes for obtaining images for detecting and correcting focus drift of the imaging reader <b>106</b>.</p><p id="p-0057" num="0056">The scanning station further has a near focus limit A<sub>nl </sub>and a far focus limit A<sub>fl</sub>. The near and far focus limits are predetermined minimum focal plane and maximum focal plane distance thresholds that are limits of the focus of the imaging reader <b>106</b>, outside of which the imaging and decoding of indicia of the imaging reader degrades. For example, the imaging reader <b>106</b> may be able to image and decode images of indicia captured between the near and far focus limits, while the imaging reader <b>106</b> is unable to efficiently decode images of indicia obtained outside of the near and far focus limits. As described herein, efficient decoding of images of indicia may include a decode efficiency of 100%, of greater than 90%, or of greater than 75%. In embodiments, the near focus plane and the far focus plane are within the bounds of the near focus plan limit and far focus plane limit as illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref> (i.e., the near focus limit is at a distance from the imaging reader <b>106</b> that is equal to or greater than the distance of the near focus limit, and the far focus plane is at a distance less than or equal to the far focus limit away from the imaging reader <b>106</b>). Having the near and far focus planes within the boundaries of the near and far focus plan limits allows for efficient decoding of indicia obtained in images of the indicia for identifying information pertaining to an OOI associated with the indicia.</p><p id="p-0058" num="0057">At block <b>604</b>, the imaging reader <b>106</b> obtains an first image of the OOI or target with the focus of the imaging read <b>106</b> set to a first focal distance being the near focal plane distance. The variable focus imaging controller <b>214</b> may control the actuator <b>215</b> and/or the variable focus optical element <b>208</b> to set the focal distance of the imaging reader <b>106</b> to the near focal plane for the imaging sensor <b>206</b> to obtain the image of the OOI. The imaging reader <b>106</b> then obtains a second image of an OOI with the focus of the imaging reader <b>106</b> set to a second focal distance being the far focal distance at block <b>606</b>. The first and second images may be obtained from a single OOI in the field of view, or the first image may be obtained for a first OOI, and the second image may be obtained for a second OOI. In examples, the first focal distance may be the far focal distance, and the second focal distance may be the near focal distance. Additionally, the first and second focal distances may be any focal distances that are on either side of the center focal distance, and that bound a focus distance band that includes the central focal plane A (i.e., the band of distance between the near and far focal planes A<sub>n </sub>and A<sub>f </sub>respectively).</p><p id="p-0059" num="0058">At <b>608</b>, the processor determines a first image property value of the first image, and at <b>610</b> the processor determines a second image property value of the second image. For example, the first and second image property values may include one or more of an image contrast, resolution, sharpness, edge detection, frequency domain analysis, spatial Fourier transformation, a normalized sharpness, or another value associated with a property of an image or of image data. In a specific example, to determine the first and second image property values, the processor may determine a contrast value of every fourth pixel of the images obtained by the imaging reader <b>106</b>. In embodiments, each pixel may be used, every other pixel, or any number of pixels for determining the first and second image property values. In the specific example, a Sobel operator is applied to determine the first derivative of the gradient of the pixels, with the resultant value being the image property value. The contrast of each image is normalized to ensure that the image property values are independent of any brightness or exposure differences between the obtained first and second images. The contrast is normalized by first subtracting the minimum pixel value from the mean pixel value, and then dividing all pixel values by the resulting difference.</p><p id="p-0060" num="0059">In embodiments, the normalized sharpness may be used as the first and second image property values. The processor may calculate the normalized sharpness by</p><p id="p-0061" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>S</mi>    <mi>n</mi>   </msub>   <mo>=</mo>   <mfrac>    <mrow>     <mi>mean</mi>     <mo>(</mo>     <mrow>      <mi>grad</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>I</mi>      <mo>)</mo>     </mrow>     <mo>)</mo>    </mrow>    <mrow>     <mi>std</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mi>I</mi>     <mo>)</mo>    </mrow>   </mfrac>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0062" num="0000">where I denotes all pixel values in a region of interest of an image, and S<sub>n </sub>is the normalized sharpness value. Using the normalized sharpness as the first and/or second image property values may be advantageous because the value of a minimal black pixel value is not required as it is removed in the calculation of the standard deviation. While described above as a contrast value or a normalized sharpness value, the first and second image property values may be any value indicative of a property of an image or of image data. Further, a normalized value or contrast value may be calculated by methods and calculations that are different than the explicit examples described herein.</p><p id="p-0063" num="0060">At <b>612</b>, the processor determines an image quality metric from at least one of the first and/or second image property values. The image quality metric may be an indication of an image quality of the first image, an indication of image quality of the second image, or an indication of a comparison of the first and second image property values. For example, the image quality metric may be calculated directly from the first image property value, directly from the second image property value, include one or more of a difference between the first and second image property values, a summation of the first and second image property values, a quotient of the first and second image property values, an average of the first and second image property values, or the image quality metric may be based on a threshold or limit of the first and/or second image property values. In embodiments, it may be determined that a focus drift has not occurred if the first and second image property values are equal, or substantially equal (e.g., within less than 15% of each other, within 10% of each other, or within 5% of each other). In such an example a difference between the image property values results in an image quality metric equal to 0, or substantially close to 0 within a tolerance of 0, and a quotient of the first and second image property values would result in a value of 1, or substantially close to 1 within a tolerance.</p><p id="p-0064" num="0061">At <b>614</b>, the processor compares the image quality metric to a reference metric to determine if a focus drift occurred and, if so, how much of a focus drift occurred. The reference metric may be retrieved as a scanning parameter or as a calibration parameter as part of the scanning parameters at block <b>602</b>. Further, the reference metric may be stored in the memory of the imaging reader <b>106</b>, or may be retrieved by the imaging reader <b>106</b> from the memory of the server <b>112</b> or from another network or device.</p><p id="p-0065" num="0062">To determine if a focus drift occurred and, if so, how much focus drift has occurred, the processor may compare the image quality metric directly to the reference metric, or to a threshold of the reference metric. The threshold of the reference metric may be indicative of a location of a focus plane that is between the near focus limit A<sub>nl </sub>and the near focus plane A<sub>n</sub>, between the far focus limit A<sub>fl </sub>and the far focal plane A<sub>f</sub>, between the near focus plane A<sub>n </sub>and the central focus plane A, or between the far focus plane A<sub>f </sub>and the central focal plane A. Therefore, the reference metric may be indicative of one or more of a drift of the near focus plane toward the near focus limit, a drift of the far focus plane toward the far focus limit, a drift of the near focus plane toward the central focal plane, or a drift of the far focus plane toward the central focal plane.</p><p id="p-0066" num="0063">The threshold of the reference metric may be a distance threshold relative to the far focal plane with the threshold of the reference metric being indicative of the distance of (i) a focal plane between the far focal plane distance and the far focus limit, (ii) a focal plane between the far focal plane distance and the central focus plane distance, or (iii) a band of focal plane distances that includes the far focal plane distance. Similarly, the threshold of the reference metric may be a distance threshold relative to the near focal plane with the threshold of the reference metric being indicative of the distance of (i) a focal plane between the near focal plane distance and the near focus limit, (ii) a focal plane between the near focal plane distance and the central focus plane, or (iii) a band of focal plane distances that includes the near focal plane distance. Further, the threshold of the reference metric may be a distance threshold relative to the central focal plane with the threshold of the reference metric being indicative of the distance of (i) a focal plane between the central focal plane distance and the near focal plane distance, (ii) a focal plane between the central focal plane distance and the far focus plane distance, (iii) a focal plane between the central focal plane distance and the far focus limit, (iv) a focal plane between the central focal plane and the near focus limit, or (v) a band of focal plane distances that includes the central focal plane distance. At <b>616</b> the processor may determine whether the focus drift has occurred based on the comparison of the image quality metric to one or more example reference metrics.</p><p id="p-0067" num="0064">In embodiments, the method <b>600</b> may include determining the reference metric. The imaging system may obtain a first reference image of a reference object, with the reference object placed at a pre-determined distance from the imaging reader <b>106</b>. The reference object may be placed at a central focal plane distance A to provide a reference for objects that may be scanned at the central focal plane distance A. In embodiments, a single image of the reference object at the central focal plane distance A may be used to further determined the reference metric. In other embodiments, a first image is obtained with the imaging reader <b>106</b> having a focal distance at the near focal distance A<sub>n</sub>, and the imaging reader <b>106</b> then obtains a second image of the reference object, with the second image being obtained at the far focal plane distance A<sub>f</sub>. The processor determines a first reference image property value from the first image and a second reference image property value of the second image, and the processor then determines the reference metric from the first and second image property values. For example, the first and second image property values may each be an image sharpness value and the reference metric may be a difference between the first and second image property values. The first and second image property values may include one or more of a sharpness value, a contrast value, an image resolution, a spatial frequency content value, a noise measurement value, a dynamic range value, a measurement of image distortion, a blur value, a pixels per module value, or another value associated with an image or image quality. The reference metric may be a difference between the first and second image property values, a summation of the first and second image property values, a quotient of the first and second image property values, an average of the first and second image property values, the reference metric may be based on a threshold or limit of the first and second image property values. In embodiments, it may be determined that a focus drift has not occurred if the first and second image property values are equal, or substantially equal (e.g., within less than 15% of each other, within 10% of each other, or within 5% of each other). In such an example a difference between the image property values results in an image quality metric equal to 0, or substantially close to 0 within a tolerance of 0, and a quotient of the first and second image property values would result in a value of 1, or substantially close to 1 within a tolerance.</p><p id="p-0068" num="0065">At <b>616</b> the processor determines whether a focus drift has occurred based on the comparison of the image quality metric to the reference metric. At <b>618</b> the processor may determine that a focus drift has not occurred when the image quality metric is within a threshold of the reference metric and ends the method at <b>620</b>. For example, the image quality metric may be a sharpness value. At <b>618</b> the processor may determine that a focus drift has occurred when the image quality metric is outside of a threshold of the reference metric. Following the example above, if the image quality metric indicates that one of the image property values has increased while the other image property value has decreased than the focus has shifted toward the plane indicated by the larger of the two image property values. In some examples, one of the image property values may increase or decrease while the other image property value remains substantially the same which may indicate that the substantially similar image property value has remained within an image property value threshold, while the other image property value has increased or decreased beyond an image property threshold. For example, the far focal plane of the system may have increased beyond the far focus limit, while the near focal plane of the system remains within the near focal plane limit.</p><p id="p-0069" num="0066">The reference metric may be indicative of a focus drift of the near focal plane toward the central focal plane, or of a drift of the far focal plane toward the central focal plane. For example, the reference metric may be indicative of a threshold of focus drift having a plane between the far focal plane and the central focal plane, and the processor may determine that the far focal plane has drifted toward the central focal plane by comparing the image quality metric with the reference metric. Similarly, the reference metric may be indicative of a focus plane that is between the near focus plan and the central focus plane, and the processor may determine that the near focus plane has drifted toward the central focus plane by comparing the image quality metric and the reference metric. The threshold of the reference metric may be a percentage of the reference metric, for example, the threshold may be &#xb1;1%, &#xb1;5%, &#xb1;10%, &#xb1;15%, &#xb1;20%, or less than &#xb1;50% of the reference metric.</p><p id="p-0070" num="0067">The imaging reader <b>106</b> then adjusts the central focal plane A of the variable focus optical element <b>208</b> according to the determined focus drift. For example, the processor may analyze the comparison of the image quality metric and the reference metric to determine an amount of focus tuning to apply to the variable focus optical element <b>208</b>. The processor may provide a signal and/or data indicative of a an amount of focus tuning to the variable focus imaging controller <b>214</b>, the actuator <b>215</b>, or directly to the variable focus optical element <b>208</b> to tune adjust the focus of the central focal plane A. Tuning the central focal plane may include providing a DC voltage to the actuator <b>215</b> or the variable focus optical element <b>208</b> (e.g., providing a voltage to a liquid lens, applying a voltage to another variable focus lens, or tuning another variable focus optical element), providing a control signal to the actuator <b>215</b> to reposition optical elements of the imaging stage <b>204</b> of the imaging reader <b>106</b>, or another means for tuning the central focal plane.</p><p id="p-0071" num="0068">Once the focus drift is determined, the processor may store the determined amount of focus drift, or a tuning amount, in the memory of the imaging reader <b>106</b>, the memory of the server <b>112</b>, or provide the amount of focus drift and/or tuning amount to another network or system. During the execution of the method <b>600</b> detectors (not illustrated) may determine different environmental factors and measure environmental variables such as an ambient temperature, a temperature of optics of the imaging reader <b>106</b>, an environmental pressure, a humidity, an age of optics of the imaging reader <b>106</b>, or other measurable factors. Values associated with the environmental factors may then be associated with a determined focus drift and/or tuning amount and the values associated with the environmental factors may also be stored in the memory of the imaging reader <b>106</b>, the memory of the server <b>112</b>, and/or provided to another network or system. When operating the imaging reader <b>106</b>, sensors may measure current environmental conditions such as a current temperature of the imaging reader or current temperature of the environment, and the processor may retrieve a focus drift and/or tuning amount associated with that temperature. The processor may then provide the retrieved focus drift and/or tuning amount to the variable focus imaging controller <b>214</b>, the actuator <b>215</b>, the variable focus optical element <b>208</b>, or other elements of the imaging reader <b>106</b> to tune the focus of the imaging reader <b>106</b> according to the previously determined focus drift.</p><p id="p-0072" num="0069">While the steps in the blocks of the methods <b>400</b> and <b>600</b> are described as performed by various elements in the imaging reader device, any of these steps may be performed in a controller or a processor, whether through one or more dedicated controllers or processors or one or more elements that are configured to perform other processes described herein. Additionally, the controller <b>214</b> may include one or more processors and one or more memoires.</p><p id="p-0073" num="0070">In the foregoing specification, specific embodiments have been described. However, one of ordinary skill in the art appreciates that various modifications and changes can be made without departing from the scope of the invention as set forth in the claims below. Accordingly, the specification and figures are to be regarded in an illustrative rather than a restrictive sense, and all such modifications are intended to be included within the scope of present teachings.</p><p id="p-0074" num="0071">The benefits, advantages, solutions to problems, and any element(s) that may cause any benefit, advantage, or solution to occur or become more pronounced are not to be construed as a critical, required, or essential features or elements of any or all the claims. The invention is defined solely by the appended claims including any amendments made during the pendency of this application and all equivalents of those claims as issued.</p><p id="p-0075" num="0072">Alternative implementations of the examples represented by the block diagram of the system <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> includes one or more additional or alternative elements, processes and/or devices. Additionally or alternatively, one or more of the example blocks of the diagram may be combined, divided, re-arranged or omitted. Components represented by the blocks of the diagram are implemented by hardware, software, firmware, and/or any combination of hardware, software and/or firmware. In some examples, at least one of the components represented by the blocks is implemented by a logic circuit. As used herein, the term &#x201c;logic circuit&#x201d; is expressly defined as a physical device including at least one hardware component configured (e.g., via operation in accordance with a predetermined configuration and/or via execution of stored machine-readable instructions) to control one or more machines and/or perform operations of one or more machines. Examples of a logic circuit include one or more processors, one or more coprocessors, one or more microprocessors, one or more controllers, one or more digital signal processors (DSPs), one or more application specific integrated circuits (ASICs), one or more field programmable gate arrays (FPGAs), one or more microcontroller units (MCUs), one or more hardware accelerators, one or more special-purpose computer chips, and one or more system-on-a-chip (SoC) devices. Some example logic circuits, such as ASICs or FPGAs, are specifically configured hardware for performing operations (e.g., one or more of the operations described herein and represented by the flowcharts of this disclosure, if such are present). Some example logic circuits are hardware that executes machine-readable instructions to perform operations (e.g., one or more of the operations described herein and represented by the flowcharts of this disclosure, if such are present). Some example logic circuits include a combination of specifically configured hardware and hardware that executes machine-readable instructions. The above description refers to various operations described herein and flowcharts that may be appended hereto to illustrate the flow of those operations. Any such flowcharts are representative of example methods disclosed herein. In some examples, the methods represented by the flowcharts implement the apparatus represented by the block diagrams. Alternative implementations of example methods disclosed herein may include additional or alternative operations. Further, operations of alternative implementations of the methods disclosed herein may combined, divided, re-arranged or omitted. In some examples, the operations described herein are implemented by machine-readable instructions (e.g., software and/or firmware) stored on a medium (e.g., a tangible machine-readable medium) for execution by one or more logic circuits (e.g., processor(s)). In some examples, the operations described herein are implemented by one or more configurations of one or more specifically designed logic circuits (e.g., ASIC(s)). In some examples the operations described herein are implemented by a combination of specifically designed logic circuit(s) and machine-readable instructions stored on a medium (e.g., a tangible machine-readable medium) for execution by logic circuit(s).</p><p id="p-0076" num="0073">As used herein, each of the terms &#x201c;tangible machine-readable medium,&#x201d; &#x201c;non-transitory machine-readable medium&#x201d; and &#x201c;machine-readable storage device&#x201d; is expressly defined as a storage medium (e.g., a platter of a hard disk drive, a digital versatile disc, a compact disc, flash memory, read-only memory, random-access memory, etc.) on which machine-readable instructions (e.g., program code in the form of, for example, software and/or firmware) are stored for any suitable duration of time (e.g., permanently, for an extended period of time (e.g., while a program associated with the machine-readable instructions is executing), and/or a short period of time (e.g., while the machine-readable instructions are cached and/or during a buffering process)). Further, as used herein, each of the terms &#x201c;tangible machine-readable medium,&#x201d; &#x201c;non-transitory machine-readable medium&#x201d; and &#x201c;machine-readable storage device&#x201d; is expressly defined to exclude propagating signals. That is, as used in any claim of this patent, none of the terms &#x201c;tangible machine-readable medium,&#x201d; &#x201c;non-transitory machine-readable medium,&#x201d; and &#x201c;machine-readable storage device&#x201d; can be read to be implemented by a propagating signal.</p><p id="p-0077" num="0074">In the foregoing specification, specific embodiments have been described. However, one of ordinary skill in the art appreciates that various modifications and changes can be made without departing from the scope of the invention as set forth in the claims below. Accordingly, the specification and figures are to be regarded in an illustrative rather than a restrictive sense, and all such modifications are intended to be included within the scope of present teachings. Additionally, the described embodiments/examples/implementations should not be interpreted as mutually exclusive, and should instead be understood as potentially combinable if such combinations are permissive in any way. In other words, any feature disclosed in any of the aforementioned embodiments/examples/implementations may be included in any of the other aforementioned embodiments/examples/implementations.</p><p id="p-0078" num="0075">The benefits, advantages, solutions to problems, and any element(s) that may cause any benefit, advantage, or solution to occur or become more pronounced are not to be construed as a critical, required, or essential features or elements of any or all the claims. The claimed invention is defined solely by the appended claims including any amendments made during the pendency of this application and all equivalents of those claims as issued.</p><p id="p-0079" num="0076">Moreover in this document, relational terms such as first and second, top and bottom, and the like may be used solely to distinguish one entity or action from another entity or action without necessarily requiring or implying any actual such relationship or order between such entities or actions. The terms &#x201c;comprises,&#x201d; &#x201c;comprising,&#x201d; &#x201c;has&#x201d;, &#x201c;having,&#x201d; &#x201c;includes&#x201d;, &#x201c;including,&#x201d; &#x201c;contains&#x201d;, &#x201c;containing&#x201d; or any other variation thereof, are intended to cover a non-exclusive inclusion, such that a process, method, article, or apparatus that comprises, has, includes, contains a list of elements does not include only those elements but may include other elements not expressly listed or inherent to such process, method, article, or apparatus. An element proceeded by &#x201c;comprises . . . a&#x201d;, &#x201c;has . . . a&#x201d;, &#x201c;includes . . . a&#x201d;, &#x201c;contains . . . a&#x201d; does not, without more constraints, preclude the existence of additional identical elements in the process, method, article, or apparatus that comprises, has, includes, contains the element. The terms &#x201c;a&#x201d; and &#x201c;an&#x201d; are defined as one or more unless explicitly stated otherwise herein. The terms &#x201c;substantially&#x201d;, &#x201c;essentially&#x201d;, &#x201c;approximately&#x201d;, &#x201c;about&#x201d; or any other version thereof, are defined as being close to as understood by one of ordinary skill in the art, and in one non-limiting embodiment the term is defined to be within 10%, in another embodiment within 5%, in another embodiment within 1% and in another embodiment within 0.5%. The term &#x201c;coupled&#x201d; as used herein is defined as connected, although not necessarily directly and not necessarily mechanically. A device or structure that is &#x201c;configured&#x201d; in a certain way is configured in at least that way, but may also be configured in ways that are not listed.</p><p id="p-0080" num="0077">The Abstract of the Disclosure is provided to allow the reader to quickly ascertain the nature of the technical disclosure. It is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims. In addition, in the foregoing Detailed Description, it can be seen that various features are grouped together in various embodiments for the purpose of streamlining the disclosure. This method of disclosure is not to be interpreted as reflecting an intention that the claimed embodiments require more features than are expressly recited in each claim. Rather, as the following claims reflect, inventive subject matter may lie in less than all features of a single disclosed embodiment. Thus, the following claims are hereby incorporated into the Detailed Description, with each claim standing on its own as a separately claimed subject matter.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007185A1-20230105-M00001.NB"><img id="EMI-M00001" he="6.01mm" wi="76.20mm" file="US20230007185A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method for correcting focus drift of an imaging system, the method comprising:<claim-text>obtaining, by the imaging system, a first image of a first object of interest, the first image obtained with the imaging system having a focus at a near focal plane distance;</claim-text><claim-text>obtaining, by the imaging system, a second image of a second object of interest, the second image obtained with the imaging system having a focus at a far focal plane distance, and wherein the near focal plane distance and far focal plane distance bound a focus distance band which includes a central focal plane distance;</claim-text><claim-text>determining, by a processor, a first image property value of the first image and a second image property value of the second image;</claim-text><claim-text>determining, by a processor, an image quality metric from at least one of the first and second image property values;</claim-text><claim-text>comparing, by the processor, the image quality metric to a reference metric; and</claim-text><claim-text>based on the comparison, (i) determining, by the processor, that a focus drift has not occurred when the image quality metric is within a threshold of the reference metric, or (ii) determining, by the processor, that a focus drift has occurred when the image quality metric is greater than the threshold of the reference metric, and adjusting, by the imaging system, a central focal plane distance of a variable focus lens of the imaging system according to the focus drift.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the near focal plane distance is equal to or greater than a minimum focal plane threshold.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the minimum focal plane threshold is at a predetermined distance smaller than the central focal plane distance.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the far focal plane distance is equal to or less than a maximum focal plane threshold.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the maximum focal plane threshold is at a predetermined distance greater than the central focal plane distance.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the reference metric is stored in a memory of the imaging system.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>obtaining, by the imaging system, a first reference image of a reference object, the first reference image obtained with the imaging system having a focus at the near focal plane distance;</claim-text><claim-text>obtaining, by the imaging system, a second reference image of the reference object, the second reference image obtained with the imaging system having a focus at the far focal plane distance;</claim-text><claim-text>determining, by a processor, a first reference image property value of the first reference image and a second reference image property value of the second reference image; and</claim-text><claim-text>determining, by a processor, the reference metric from the first and second reference image property values.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first image property value includes one or more of an image contrast, resolution, sharpness, edge detection, or modulation transfer function.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second image property value includes one or more of an image contrast, resolution, sharpness, edge detection, or modulation transfer function.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the image quality metric comprises determining one of a difference between the first and second image property values, an addition of the first and second image property values, a quotient of the first and second image property values, or an average of the first and second image property values.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the central focal plane distance is half way between the near focal plane distance and the far focal plane distance.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein adjusting the central focal plane distance of the variable focus lens comprises adjusting a voltage applied to the variable focus lens.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein adjusting the central focal plane of the variable focus lens comprises adjusting a position of an optical element of the imaging system.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the variable focus lens comprises a liquid lens.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first object of interest and second object of interest are a same object of interest.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the threshold of the reference metric is a threshold relative to the near plane focal distance.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the threshold of the threshold of the reference metric is a threshold relative to the far plane focal distance.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the threshold of the threshold of the reference metric is a threshold relative to the central plane focal distance.</claim-text></claim></claims></us-patent-application>