<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005270A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005270</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17931261</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>13</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>64</class><subclass>C</subclass><main-group>39</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>64</class><subclass>D</subclass><main-group>47</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>19</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>13</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>185</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>005</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23218</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>64</class><subclass>C</subclass><main-group>39</main-group><subgroup>024</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>64</class><subclass>D</subclass><main-group>47</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200501</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>19</main-group><subgroup>485</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>89</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">UNCREWED AERIAL VEHICLE SHARED ENVIRONMENT PRIVACY AND SECURITY</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17107736</doc-number><date>20201130</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11443518</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17931261</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>AT&#x26;T  Intellectual Property I, L.P.</orgname><address><city>Atlanta</city><state>GA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>AT&#x26;T  Mobility II LLC</orgname><address><city>Atlanta</city><state>GA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Cui</last-name><first-name>Zhi</first-name><address><city>Sugar Hill</city><state>GA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Dowlatkhah</last-name><first-name>Sangar</first-name><address><city>Cedar Hill</city><state>TX</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Khan</last-name><first-name>Sameena</first-name><address><city>Peachtree Corners</city><state>GA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Paige</last-name><first-name>Troy</first-name><address><city>Buford</city><state>GA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Craine</last-name><first-name>Ari</first-name><address><city>Marietta</city><state>GA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Koch</last-name><first-name>Robert</first-name><address><city>Peachtree Corners</city><state>GA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A processing system including at least one processor may capture at least one image via at least one imaging sensor of an uncrewed aerial vehicle in a shared environment, detect a window within the at least one image, determine a location of the window in the shared environment, based upon a position of the uncrewed aerial vehicle and a distance between the uncrewed aerial vehicle and at least a portion of the window that is calculated from the at least one image, and record the location of the window in a map of the shared environment as a prohibited imaging zone.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="100.67mm" wi="158.75mm" file="US20230005270A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="237.57mm" wi="155.53mm" orientation="landscape" file="US20230005270A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="233.00mm" wi="158.41mm" orientation="landscape" file="US20230005270A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="233.76mm" wi="158.58mm" orientation="landscape" file="US20230005270A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="233.51mm" wi="158.92mm" orientation="landscape" file="US20230005270A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="147.07mm" wi="139.28mm" orientation="landscape" file="US20230005270A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/107,736, filed on Nov. 30, 2020, now U.S. Pat. No. 11,443,518, which is herein incorporated by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0003" num="0002">The present disclosure relates generally to uncrewed aerial vehicle operations, and more particularly to methods, computer-readable media, and apparatuses for determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone, methods, computer-readable media, and apparatuses for determining by an uncrewed aerial vehicle based upon a map of a shared environment that at least one image includes an image portion that captures a location of at least one window and altering the at least one image to exclude the image portion, and methods, computer-readable media, and apparatuses for disabling at least one imaging sensor of an uncrewed aerial vehicle when it is determined, based upon a map of a shared environment, a location of the uncrewed aerial vehicle, and an orientation of at least one imaging sensor of the uncrewed aerial vehicle, that a location of a window is within a field of view of the at least one imaging sensor.</p><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Current trends in wireless technology are leading towards a future where virtually any object can be network-enabled and addressable on-network. The pervasive presence of cellular and non-cellular wireless networks, including fixed, ad-hoc, and/or or peer-to-peer wireless networks, satellite networks, and the like along with the migration to a 128-bit IPv6-based address space provides the tools and resources for the paradigm of the Internet of Things (IoT) to become a reality. In addition, drones or autonomous aerial vehicles (AAVs) (broadly, &#x201c;uncrewed aerial vehicles&#x201d; (UAVs)) are increasingly being utilized for a variety of commercial and other useful tasks, such as package deliveries, search and rescue, mapping, surveying, and so forth, enabled at least in part by these wireless communication technologies.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0005" num="0004">In one example, the present disclosure describes a method, computer-readable medium, and apparatus for determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone. For instance, in one example, a processing system including at least one processor may capture at least one image via at least one imaging sensor of an uncrewed aerial vehicle in a shared environment, detect a window within the at least one image, determine a location of the window in the shared environment, based upon a position of the uncrewed aerial vehicle and a distance between the uncrewed aerial vehicle and at least a portion of the window that is calculated from the at least one image, and record the location of the window in a map of the shared environment as a prohibited imaging zone.</p><p id="p-0006" num="0005">In another example, the present disclosure describes a method, computer-readable medium, and apparatus for determining by an uncrewed aerial vehicle based upon a map of a shared environment that at least one image includes an image portion that captures a location of at least one window and altering the at least one image to exclude the image portion. For instance, in one example, a processing system of an uncrewed aerial vehicle including at least one processor may obtain a map of a shared environment, the map including a location of at least one window that is defined as a prohibited imaging zone, capture at least one image via at least one imaging sensor of the uncrewed aerial vehicle, and determine that the at least one image includes an image portion that correlates to the location of the at least one window, alter the at least one image to exclude the image portion. The processing system may then perform at least one of: providing the at least one image that is altered to exclude the image portion to at least one recipient device or storing the at least one image that is altered to exclude the image portion.</p><p id="p-0007" num="0006">In still another example, the present disclosure describes a method, computer-readable medium, and apparatus for disabling at least one imaging sensor of an uncrewed aerial vehicle when it is determined, based upon a map of a shared environment, a location of the uncrewed aerial vehicle, and an orientation of at least one imaging sensor of the uncrewed aerial vehicle, that a location of a window is within a field of view of the at least one imaging sensor. For instance, in one example, a processing system of an uncrewed aerial vehicle including at least one processor may obtain a map of a shared environment, the map including a location of a window that is defined as a prohibited imaging zone, navigate the uncrewed aerial vehicle in the shared environment, determine based upon the map, a location of the uncrewed aerial vehicle, and an orientation of at least one imaging sensor of the uncrewed aerial vehicle, that the location of the window is within a field of view of the at least one imaging sensor, and disable or reorient the at least one imaging sensor when the location of the window is within the field of view of the at least one imaging sensor.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007">The teaching of the present disclosure can be readily understood by considering the following detailed description in conjunction with the accompanying drawings, in which:</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example system related to the present disclosure;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a flowchart of an example method for determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone, in accordance with the present disclosure;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a flowchart of an example method for determining by an uncrewed aerial vehicle based upon a map of a shared environment that at least one image includes an image portion that captures a location of at least one window and altering the at least one image to exclude the image portion;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a flowchart of an example method for disabling at least one imaging sensor of an uncrewed aerial vehicle when it is determined, based upon a map of a shared environment, a location of the uncrewed aerial vehicle, and an orientation of at least one imaging sensor of the uncrewed aerial vehicle, that a location of a window is within a field of view of the at least one imaging sensor; and</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example high-level block diagram of a computing device specifically programmed to perform the steps, functions, blocks, and/or operations described herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0014" num="0013">To facilitate understanding, identical reference numerals have been used, where possible, to designate identical elements that are common to the figures.</p><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0015" num="0014">Examples of the present disclosure describe methods, computer-readable media, and apparatuses for determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone, methods, computer-readable media, and apparatuses for determining by an uncrewed aerial vehicle based upon a map of a shared environment that at least one image includes an image portion that captures a location of at least one window and altering the at least one image to exclude the image portion, and methods, computer-readable media, and apparatuses for disabling at least one imaging sensor of an uncrewed aerial vehicle when it is determined, based upon a map of a shared environment, a location of the uncrewed aerial vehicle, and an orientation of at least one imaging sensor of the uncrewed aerial vehicle, that a location of a window is within a field of view of the at least one imaging sensor. In particular, examples of the present disclosure provide an uncrewed aerial vehicle (UAV)-based system for security and privacy in a shared environment (where &#x201c;uncrewed&#x201d; may mean devoid of an onboard operator of the vehicle, also referred to as &#x201c;unmanned&#x201d;). In accordance with the present disclosure, an uncrewed vehicle (such as an uncrewed aerial vehicle (UAV)) may be remotely controlled by a human or an autonomous system, or may be self-operating or partially self-operating (e.g., a combination of on-vehicle and remote computing resources), such as an autonomous aerial vehicle (AAV). In one example, the present disclosure enables video surveillance to be performed in a shared area, or shared environment, in such a way that privacy is ensured within the content produced by the video surveillance for people who share the area. The shared area may be a residential neighborhood, a business community, or an educational community, for example, including but not limited to an apartment complex, a gated community of homes or buildings, an office complex, a business complex, e.g., a mall or a strip mall, buildings located on one or more public streets, an educational complex, e.g., schools, universities, colleges, campuses, and so forth. Examples of the present disclosure define and enforce prohibited imaging areas (and/or viewing areas) for which UAV image, video, and other sensor data capture is not permitted.</p><p id="p-0016" num="0015">A shared environment or area may be defined using geographic coordinates, e.g., a global or spherical coordinate system. The coordinates may be stored by a security system and/or a UAV fleet management system (e.g., including a security service database (SSDB)). The coordinates stored may be, for instance, latitude and longitude, or other geographic coordinates that delineate a shared area e.g., geocentric coordinates (x,y,z), and the like. The coordinates may be stores as x-y points (or x-y-z points), or as ranges of points. This shared area may be served by one or more security UAVs that fly one or more flight paths to cover the area on a periodic or on-demand basis.</p><p id="p-0017" num="0016">A mapping UAV may be used to identify areas that should be blocked from any image, video, and other sensor data capture, or &#x201c;prohibited imaging zones.&#x201d; In particular, it may be decided by an entity controlling the shared area or among stakeholders of the shared area (e.g., homeowners, tenants, landlords, property managers, local town officials, etc.) that the security UAV(s) are not permitted to capture image or video content that may potentially contain image or video content captured from inside windows of houses or other buildings. In one example, one or more mapping UAVs may execute a mapping run to cover all of the geographic bounds of the shared area. A mapping UAV may be equipped with sensors including a video camera and a light detection and ranging (LiDAR) unit. A mapping UAV may also be equipped with location sensing capabilities (such as a Global Positioning System (GPS) unit, an altimeter, etc.) so that it is aware of its x-y-z location coordinates (for example, latitude-longitude-altitude in a world reference system). The mapping UAV may capture both optical images (e.g., including video and/or still images) via an optical camera and LiDAR readings, or images/renderings, in the same orientation as the optical camera during the mapping run. The data is then analyzed using one or more object detection models to identify windows of buildings.</p><p id="p-0018" num="0017">To illustrate, the mapping UAV may track its x-y-z coordinates throughout the mapping run. In addition, using LiDAR data recorded throughout the mapping run, the UAV may determine the distance from the UAV (e.g., from the LiDAR unit) to various objects. A combination of the video and LiDAR data may be used to map window bounds within the space (e.g., the four corners of a rectangular window, or the circular boundary of a circular window, and the like). At the same time t1, LiDAR data provides the location in x-y-z coordinates of the four points in relation to the position of the mapping UAV. Since the mapping UAV tracks its own location in space (e.g., in x-y-z of a particular reference system), the location coordinates (e.g., in x-y-z) of the four points in the same reference system may be calculated. The x-y-z location coordinates of the four corners of the window may be stored in the SSDB with an indication that the points define a boundary of a prohibited imaging zone. The prohibited imaging zone may also be stored in a database, or map accessible to a wider scope of security UAVs, delivery UAVs, commercial UAVs, private UAVs, and others.</p><p id="p-0019" num="0018">In an illustrative example, a neighborhood security UAV may be deployed within the shared area with a map containing one or more prohibited imaging zones (e.g., including at least one window as detected and defined above). As the security UAV traverses a flight path, the security UAV may approach such a prohibited imaging zone. The security UAV has access to the no-video zone coordinate data in the SSDB. The security UAV may be equipped with an optical camera and/or an infrared camera, or the like. The security UAV may be configured and/or calibrated to have awareness of the field-of-view and range of these sensors as they relate to the security UAV.</p><p id="p-0020" num="0019">When any prohibited imaging zone enters the field of view and range of one or more of the imaging sensors, the imaging sensors may be temporarily disabled, or the recorded image(s) may be altered to exclude the captured data content for the prohibited imaging zone, e.g., overwritten with blank data, pixelated or otherwise obscured, replaced with a different data content (e.g., a generic window, an image of the actual window previously obtained and stored, e.g., with consent of a property owner or other(s) with an interest in the property), etc. In another example, the security UAV imaging sensors may be oriented in any direction other than that of the prohibited imaging zone.</p><p id="p-0021" num="0020">A prohibited imaging zone may also be defined by specification of coordinates without particular detection of a window or other object. For example, if a homeowner wants to opt-out (e.g., withholding consent) of the security service, the entire property bounds may be defined as a prohibited imaging zone. Prohibited imaging zones may also be activated and deactivated. For instance, prohibited imaging zone data in the SSDB may have an associated owner, e.g., identified by a user ID and password or other credentials. The owner may permit the owner's own UAV to record images (e.g., video, LiDAR images/renderings, etc.) within a prohibited imaging zone, while all other UAVs may be subject to the no recording restrictions.</p><p id="p-0022" num="0021">In one example, when traversing a flight path, a security UAV may use onboard object detection models to detect conditions in images that are not expected (e.g., with respect to expected conditions that may be stored in the SSDB). For instance, within a neighborhood, the residents may opt-in (e.g., providing consent) to providing data to the SSDB regarding expected package deliveries or service technician calls. In addition, unexpected conditions may include changes to structures within the shared environment, e.g., a tree falling on a house, a wall collapse, etc., dangerous conditions, such as fire or flooding, and so on. These and other aspects of the present disclosure are discussed in greater detail below in connection with the examples of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>5</b></figref>.</p><p id="p-0023" num="0022">To aid in understanding the present disclosure, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example system <b>100</b>, related to the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the system <b>100</b> connects user device <b>141</b>, server(s) <b>112</b>, server(s) <b>125</b>, uncrewed aerial vehicles (UAVs <b>160</b>-<b>161</b>), remote control device <b>169</b>, and so forth, with one another and with various other devices via a core network, e.g., a telecommunication network <b>110</b>, a wireless access network <b>115</b> (e.g., a cellular network), and Internet <b>130</b>.</p><p id="p-0024" num="0023">In one example, the server(s) <b>125</b> may each comprise a computing device or processing system, such as computing system <b>500</b> depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, and may be configured to perform one or more steps, functions, or operations for determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone. For instance, an example method <b>200</b> for determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone is illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> and described below. In addition, it should be noted that as used herein, the terms &#x201c;configure,&#x201d; and &#x201c;reconfigure&#x201d; may refer to programming or loading a processing system with computer-readable/computer-executable instructions, code, and/or programs, e.g., in a distributed or non-distributed memory, which when executed by a processor, or processors, of the processing system within a same device or within distributed devices, may cause the processing system to perform various functions. Such terms may also encompass providing variables, data values, tables, objects, or other data structures or the like which may cause a processing system executing computer-readable instructions, code, and/or programs to function differently depending upon the values of the variables or other data structures that are provided. As referred to herein a &#x201c;processing system&#x201d; may comprise a computing device, or computing system, including one or more processors, or cores (e.g., as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> and discussed below) or multiple computing devices collectively configured to perform various steps, functions, and/or operations in accordance with the present disclosure.</p><p id="p-0025" num="0024">In one example, server(s) <b>125</b> may comprise a security system for a shared environment. In one example, the security system may comprise a UAV fleet management system. For instance, server(s) <b>125</b> may receive and store information regarding UAVs, such as (for each UAV): an identifier of the UAV, a maximum operational range of the UAV, a current operational range of the UAV, capabilities or features of the UAV, such as maneuvering capabilities, payload/lift capabilities (e.g., including maximum weight, volume, etc.), sensor and recording capabilities, lighting capabilities, visual projection capabilities, sound broadcast capabilities, and so forth. In one example, server(s) <b>125</b> may manage or support UAVs that are deployed for performing tasks within shared environment <b>190</b>, e.g., security surveillance or other image gathering tasks. For instance, server(s) <b>125</b> may obtain requests to perform tasks from personnel of the facility <b>190</b>, other automated systems, etc., may assign AAVs to particular tasks, may track task completions, and so forth. Server(s) <b>125</b> may also store a map or mapping data of shared environment <b>190</b>, and provide the map or mapping data to UAVs, may update the map or mapping data as new information is collected from UAVs, and so forth. The shared environment may comprise, for example, a residential community, an apartment complex, an office complex, and so forth.</p><p id="p-0026" num="0025">In addition, server(s) <b>125</b> may store detection models that may be applied to sensor data from UAVs, e.g., in order to detect items or objects (which may specifically include windows (broadly including any building structures having see-through glass), and which may also include humans or animals, and so on). For instance, in one example, UAVs may include on-board processing systems with one or more detection models for detecting items or objects. However, as an alternative, or in addition, UAVs may transmit sensor data to server(s) <b>125</b>, which may apply detection models to the sensor data in order to similarly detect items or objects.</p><p id="p-0027" num="0026">The MLMs, or signatures, may be specific to particular types of visual/image and/or spatial sensor data, or may take multiple types of sensor data as inputs. For instance, with respect to images or video, the input sensor data may include low-level invariant image data, such as colors (e.g., RGB (red-green-blue) or CYM (cyan-yellow-magenta) raw data (luminance values) from a CCD/photo-sensor array), shapes, color moments, color histograms, edge distribution histograms, etc. Visual features may also relate to movement in a video and may include changes within images and between images in a sequence (e.g., video frames or a sequence of still image shots), such as color histogram differences or a change in color distribution, edge change ratios, standard deviation of pixel intensities, contrast, average brightness, and the like. For instance, these features could be used to help quantify and distinguish shimmering water, a flag on a flagpole, an animate object, such as a human or animal, a vehicle, and so forth from other types of images/object and/or other features.</p><p id="p-0028" num="0027">As noted above, in one example, MLMs, or signatures, may take multiple types of sensor data as inputs. For instance, MLMs or signatures may also be provided for detecting particular items based upon LiDAR input data and/or optical camera input data. In accordance with the present disclosure, a detection model may comprise a machine learning model (MLM) that is trained based upon the plurality of features available to the system (e.g., a &#x201c;feature space&#x201d;). For instance, one or more positive examples for a feature may be applied to a machine learning algorithm (MLA) to generate the signature (e.g., a MLM). In one example, the MLM may comprise the average features representing the positive examples for an item in a feature space. Alternatively, or in addition, one or more negative examples may also be applied to the MLA to train the MLM. The machine learning algorithm or the machine learning model trained via the MLA may comprise, for example, a deep learning neural network, or deep neural network (DNN), a generative adversarial network (GAN), a support vector machine (SVM), e.g., a binary, non-binary, or multi-class classifier, a linear or non-linear classifier, and so forth. In one example, the MLA may incorporate an exponential smoothing algorithm (such as double exponential smoothing, triple exponential smoothing, e.g., Holt-Winters smoothing, and so forth), reinforcement learning (e.g., using positive and negative examples after deployment as a MLM), and so forth. It should be noted that various other types of MLAs and/or MLMs may be implemented in examples of the present disclosure, such as k-means clustering and/or k-nearest neighbor (KNN) predictive models, support vector machine (SVM)-based classifiers, e.g., a binary classifier and/or a linear binary classifier, a multi-class classifier, a kernel-based SVM, etc., a distance-based classifier, e.g., a Euclidean distance-based classifier, or the like, and so on. In one example, a trained detection model may be configured to process those features which are determined to be the most distinguishing features of the associated item, e.g., those features which are quantitatively the most different from what is considered statistically normal or average from other items that may be detected via a same system, e.g., the top 20 features, the top 50 features, etc.</p><p id="p-0029" num="0028">In one example, detection models (e.g., MLMs) may be deployed in UAVs, and/or in a network-based processing system to process sensor data from one or more UAV sensor sources (e.g., cameras, LiDAR, or the like), and to identify patterns in the features of the sensor data that match the detection model(s) for the respective item(s). In one example, a match may be determined using any of the visual features mentioned above, e.g., and further depending upon the weights, coefficients, etc. of the particular type of MLM. For instance, a match may be determined when there is a threshold measure of similarity among the features of the sensor data streams(s) and an item signature, e.g., a window.</p><p id="p-0030" num="0029">In one example, server(s) <b>125</b> may store detection models that may be provided to UAVs and/or applied by server(s) <b>125</b>, in-network, to input data from cameras and/or LiDAR units to detect various &#x201c;conditions,&#x201d; e.g., unexpected conditions, such as &#x201c;fire,&#x201d; &#x201c;flooding,&#x201d; &#x201c;fight,&#x201d; &#x201c;break-in,&#x201d; &#x201c;stalking,&#x201d; or the like. For instance, MLM detection models may be similarly trained to detect these types of conditions which may be present in visual and/or spatial sensor data, but which may not necessarily comprise &#x201c;objects.&#x201d; In this regard, it should be noted that as referred to herein, an &#x201c;unexpected condition&#x201d; may comprise a presence of an unknown item or object (which may include unknown humans or animals) within the shared environment <b>190</b>, or may include defined conditions that may be found in the shared environment that do not necessarily comprise &#x201c;objects&#x201d; (e.g., as defined by at least one detection model for such a condition).</p><p id="p-0031" num="0030">In one example, the system <b>100</b> includes a telecommunication network <b>110</b>. In one example, telecommunication network <b>110</b> may comprise a core network, a backbone network or transport network, such as an Internet Protocol (IP)/multi-protocol label switching (MPLS) network, where label switched routes (LSRs) can be assigned for routing Transmission Control Protocol (TCP)/IP packets, User Datagram Protocol (UDP)/IP packets, and other types of protocol data units (PDUs), and so forth. It should be noted that an IP network is broadly defined as a network that uses Internet Protocol to exchange data packets. However, it will be appreciated that the present disclosure is equally applicable to other types of data units and transport protocols, such as Frame Relay, and Asynchronous Transfer Mode (ATM). In one example, the telecommunication network <b>110</b> uses a network function virtualization infrastructure (NFVI), e.g., host devices or servers that are available as host devices to host virtual machines comprising virtual network functions (VNFs). In other words, at least a portion of the telecommunication network <b>110</b> may incorporate software-defined network (SDN) components.</p><p id="p-0032" num="0031">In one example, one or more wireless access networks <b>115</b> may each comprise a radio access network implementing such technologies as: global system for mobile communication (GSM), e.g., a base station subsystem (BSS), or IS-95, a universal mobile telecommunications system (UMTS) network employing wideband code division multiple access (WCDMA), or a CDMA3000 network, among others. In other words, wireless access network(s) <b>115</b> may each comprise an access network in accordance with any &#x201c;second generation&#x201d; (2G), &#x201c;third generation&#x201d; (3G), &#x201c;fourth generation&#x201d; (4G), Long Term Evolution (LTE), &#x201c;fifth generation&#x201d; (5G), or any other existing or yet to be developed future wireless/cellular network technology. While the present disclosure is not limited to any particular type of wireless access network, in the illustrative example, base stations <b>117</b> and <b>118</b> may each comprise a Node B, evolved Node B (eNodeB), or gNodeB (gNB), or any combination thereof providing a multi-generational/multi-technology-capable base station. In the present example, user device <b>141</b>, UAV <b>160</b>, UAV <b>161</b>, and remote control device <b>169</b> may be in communication with base stations <b>117</b> and <b>118</b>, which provide connectivity between user device, <b>141</b>, UAVs <b>160</b>-<b>161</b>, remote control device <b>169</b> and other endpoint devices within the system <b>100</b>, various network-based devices, such as server(s) <b>112</b>, server(s) <b>125</b>, and so forth. In one example, wireless access network(s) <b>115</b> may be operated by the same service provider that is operating telecommunication network <b>110</b>, or one or more other service providers.</p><p id="p-0033" num="0032">For instance, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, wireless access network(s) <b>115</b> may also include one or more servers <b>112</b>, e.g., edge servers at or near the network edge. In one example, each of the server(s) <b>112</b> may comprise a computing device or processing system, such as computing system <b>500</b> depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref> and may be configured to provide one or more functions determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone. For example, one or more of the server(s) <b>112</b> may be configured to perform one or more steps, functions, or operations in connection with the example method <b>200</b> described below. In one example, server(s) <b>112</b> may perform the same or similar functions as server(s) <b>125</b>. For instance, telecommunication network <b>110</b> may provide a security system for shared environment(s) and/or a UAV fleet management system, e.g., as a service to one or more subscribers/customers, in addition to telephony services, data communication services, television services, etc. In one example, server(s) <b>112</b> may operate in conjunction with server(s) <b>125</b> to provide an UAV fleet management system and/or a network-managed, or network-supported UAV-based security service. For instance, server(s) <b>125</b> may provide more centralized services, such as UAV authorization and tracking, maintaining user accounts, creating new accounts, tracking account balances, accepting payments for services, etc., while server(s) <b>112</b> may provide more operational support to UAVs, such as deploying MLMs/detection models for detecting objects and/or conditions, for obtaining location information of user devices (e.g., from a cellular/wireless network service provider, such as an operator of telecommunication network <b>110</b> and wireless access network(s) <b>115</b>), and providing such information to UAVs, for updating maps of shared environments with prohibited imaging zones and/or providing such maps to UAVs, and so on. It is noted that this is just one example of a possible distributed architecture for UAV-based security system and/or UAV fleet management system. Thus, various other configurations including various data centers, public and/or private cloud servers, and so forth may be deployed. For ease of illustration, various additional elements of wireless access network(s) <b>115</b> are omitted from <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0034" num="0033">As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, user device <b>141</b> may comprise, for example, a wireless enabled wristwatch. In various examples, user device <b>141</b> may comprise a cellular telephone, a smartphone, a tablet computing device, a laptop computer, a head-mounted computing device (e.g., smart glasses), or any other wireless and/or cellular-capable mobile telephony and computing devices (broadly, a &#x201c;mobile device&#x201d; or &#x201c;mobile endpoint device&#x201d;). In one example, user device <b>141</b> may be equipped for cellular and non-cellular wireless communication. For instance, user device <b>141</b> may include components which support peer-to-peer and/or short range wireless communications. Thus, user device <b>141</b> may include one or more radio frequency (RF) transceivers, e.g., for cellular communications and/or for non-cellular wireless communications, such as for IEEE 802.11 based communications (e.g., Wi-Fi, Wi-Fi Direct), IEEE 802.15 based communications (e.g., Bluetooth, Bluetooth Low Energy (BLE), and/or ZigBee communications), and so forth. In another example, user device <b>141</b> may instead comprise a radio frequency identification (RFID) tag that may be detected by UAVs or other RFID reader-equipped device or systems.</p><p id="p-0035" num="0034">In accordance with the present disclosure, UAV <b>160</b> may include a camera <b>162</b> and one or more radio frequency (RF) transceivers <b>166</b> for cellular communications and/or for non-cellular wireless communications. In one example, UAV <b>160</b> may also include one or more module(s) <b>164</b> with one or more additional controllable components, such as one or more: microphones, loudspeakers, infrared, ultraviolet, and/or visible spectrum light sources, projectors, light detection and ranging (LiDAR) unit(s), temperature sensors (e.g., thermometers), and so forth. In addition, UAV <b>160</b> may include a cargo handling element <b>167</b>. As illustrated, cargo handling element <b>167</b> may comprise a lift hook or clamp for engaging a cargo carrier, e.g., a basket and the like. However, in another example, cargo handling element <b>167</b> may alternatively or additionally comprise an internal cargo compartment in which to receive and transport an item or object. It should be noted that UAV <b>161</b> may be similarly equipped. However, for ease of illustration, specific labels for such components of UAV <b>161</b> are omitted from <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0036" num="0035">In addition, each of the UAVs <b>160</b> and <b>161</b> may include on-board processing systems to perform steps, functions, and/or operations for controlling various components of the respective UAVs, and for determining by an uncrewed aerial vehicle based upon a map of a shared environment that at least one image includes an image portion that captures a location of at least one window and altering the at least one image to exclude the image portion and/or for disabling at least one imaging sensor of an uncrewed aerial vehicle when it is determined, based upon a map of a shared environment, a location of the uncrewed aerial vehicle, and an orientation of at least one imaging sensor of the uncrewed aerial vehicle, that a location of a window is within a field of view of the at least one imaging sensor. For instance, UAVs <b>160</b> and <b>161</b> may each comprise all or a portion of a computing device or processing system, such as computing system <b>500</b> as described in connection with <figref idref="DRAWINGS">FIG. <b>5</b></figref> below, specifically configured to perform various steps, functions, and/or operations of the example method <b>300</b> and/or the example method <b>400</b> discussed below and illustrated in <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>, respectively.</p><p id="p-0037" num="0036">In an illustrative example, the shared environment <b>190</b> may comprise a residential community having several buildings <b>195</b>-<b>197</b> therein. The buildings <b>195</b>-<b>197</b> may comprise houses, apartment buildings, or the like. Continuing with the present example, the shared environment <b>190</b> may utilize UAVs (including AAVs or remote controlled UAVs) to perform various tasks therein, such as navigating flight paths within or over the shared environment <b>190</b>, capturing images of various areas within the shared environment <b>190</b>, searching for and detecting any unexpected conditions, reporting and investigating any unexpected conditions that are detected, and so forth. In one example, the assignment of tasks to UAVs and the management of the airspace of the shared environment <b>190</b> may be provided by a UAV-based security management system, which may comprise server(s) <b>125</b> and/or server(s) <b>112</b>. In this case, the security management system may assign to UAV <b>160</b> a task of generating a map of the shared environment (e.g., a three-dimensional (3D) map), or collecting mapping data for generating and/or updating such a map. In particular, UAV <b>160</b> may be assigned to detect windows within the shared environment <b>190</b>, to determine the locations of such windows and to record the locations of one or more windows in the map as prohibited imaging zones.</p><p id="p-0038" num="0037">In the present example, the UAV <b>160</b> may comprise an AAV (e.g., without remote control) that is provided with a flight path and/or an assigned area of the shared environment <b>190</b>, with instructions to traverse the area and capture images (e.g., including still images, video and/or LiDAR sensor data (e.g., LiDAR images/renderings), or the like). However, it should be understood that in other examples, UAV <b>160</b> may be controlled by a human operator, such as via remote control device <b>169</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, UAV <b>160</b> may have at least one imaging and/or spatial sensor having a field-of-view <b>150</b> from which at least one image may be captured. In one example, UAV <b>160</b> may capture image and/or spatial sensor data comprising optical image(s) and/or LiDAR images/renderings. In an example, where both optical camera and LiDAR images/renderings are both obtained, the field-of-view <b>150</b> may represent the fields-of-view of both imaging sensors. However, in another example, the fields-of-view may be different.</p><p id="p-0039" num="0038">As noted above, UAVs, such as UAV <b>160</b> may obtain object or condition detection models (e.g., MLMs) from server(s) <b>125</b> and/or server(s) <b>112</b>, e.g., including at least one detection model for detecting windows from within one or more images. In the present example, UAV <b>160</b> may capture one or more images within field-of-view <b>150</b>, which may include a window <b>192</b>. The window <b>192</b> may be detected by UAV <b>160</b> by applying the one or more captured images as input(s) to a window detection model (and in one example, to other detection models for detecting other conditions and/or objects of interest). As such, UAV <b>160</b> may distinguish window <b>192</b> from the surrounding visual data captured in the one or more images. In one example, the detection model for detecting windows may additionally be configured to detect and output boundaries of windows that are determined. In one example, the one or more images may include LiDAR images/renderings. In one example, the one or more images may additionally include images from an optical camera of UAV <b>160</b>.</p><p id="p-0040" num="0039">In addition, UAV <b>160</b> may also track its own position within a three-dimensional space. For example, the position of UAV <b>160</b> may be determined from a GPS unit of UAV <b>160</b> and an altimeter, or a calculation of a distance to at least one reference point (e.g., one of reference points <b>199</b> within the shared environment <b>190</b>). For instance, reference points <b>199</b> may each comprise a reflector or a wireless beacon at a known geographic position. In one example, the calculation of the distance to the at least one reference point <b>199</b> may be based upon the at least one image that is captured (e.g., a LiDAR image/rendering that may include the one of the reference points <b>199</b>). In another example, UAV <b>160</b> may capture a different set of one or more images (and/or obtain LiDAR sensor data) to determine a range/distance to one or more of the reference points <b>199</b> and then capture the one or more images within field-of-view <b>150</b> that include the window <b>192</b>. In one example, UAV <b>160</b> may translate its position in one reference coordinate system into a position in another reference coordinate system (e.g., translating from a local coordinate system for shared environment <b>190</b> to a global/world reference coordinate system, or vice versa). In one example, UAV <b>160</b> may confirm its position (e.g., in x-y-z, or latitude-longitude-elevation/altitude) via ranging to reference point(s) <b>199</b> and via location detection via a GPS unit and altimeter. In still another example, UAV <b>160</b> may determine its location via a GPS unit in conjunction with LiDAR sensing of a height/altitude over ground. Thus, various combinations of different technologies may be deployed in different examples, for different UAVs having different components and capabilities, and so forth.</p><p id="p-0041" num="0040">The UAV <b>160</b> may determine the distance from the UAV <b>160</b> (e.g., from the LiDAR unit) to different objects, surfaces, etc., in the shared environment. For instance, image recognition analysis of a video frame or still image x at time t1 may identify the portion of the image bounded by points P<b>1</b>, P<b>2</b>, P<b>3</b>, and P<b>4</b> as window <b>192</b> (e.g., the four corners of a rectangular window). At the same time t1, LiDAR data provides the location in x-y-z coordinates of the four points P<b>1</b>-P<b>4</b> in relation to the position of UAV <b>160</b> (e.g., a distance to the LiDAR unit of UAV <b>160</b>). Since the UAV <b>160</b> tracks its own location in space (e.g., in x-y-z of a particular reference system), the location coordinates (e.g., in x-y-z) of the four points P<b>1</b>-P<b>4</b> in the same reference system may be calculated. As such, the x-y-z location coordinates of the space defined by the points P<b>1</b>-P<b>4</b> (the four corners of the window <b>192</b>) may be added to a map of the shared environment <b>192</b> with an indication that the points P<b>1</b>-P<b>4</b> define a boundary of a prohibited imaging zone. In one example, UAV <b>160</b> may store the points P<b>1</b>-P<b>4</b> with an indication that these points comprise a window, or prohibited imaging zone. Alternatively, or in addition, at a same time, or at a later time, UAV <b>160</b> may provide the points P<b>1</b>-P<b>4</b>, along with an indication that these points comprise a window, or prohibited imaging zone, to server(s) <b>125</b> and/or server(s) <b>112</b>. The server(s) <b>125</b> and/or server(s) <b>112</b> may then update a stored map of the shared environment <b>190</b> with these points and the indication of the prohibited imaging zone defined by such points P<b>1</b>-P<b>4</b> (i.e., window <b>192</b>).</p><p id="p-0042" num="0041">In a same or similar manner, UAV <b>160</b> and/or other UAVs may capture images from within/above shared environment <b>190</b>, may detect other windows, define the bounds thereof, may determine the positions of such windows in space, and may likewise provide notification to server(s) <b>125</b> and/or server(s) <b>112</b> to include these windows as prohibited imaging zones in the map. It should be noted that not all windows will be designated permanently as prohibited imaging zones after being detected as such and/or added to the map. For instance, property owners or others with interest in various properties of the shared environment <b>190</b> may designate entire properties as acceptable imaging zones, e.g., a store owner may want all of its windows including glass doors to be monitored. Similarly, property owners or others with interest in various properties of the shared environment <b>190</b> may designate entire properties as acceptable imaging zones as prohibited imaging zones, such as prohibited imaging zone <b>198</b> around building <b>197</b>. In one example, property owners or other entities may utilize an interface that provides access to the map, with indications of designated prohibited imaging areas, and the property owners or other entities may de-select certain windows or other designated prohibited imaging areas as areas of allowed recording (e.g., upon confirming a legitimate property interest or other rights to the property). Alternatively, or in addition, certain windows or other prohibited imaging zones may be designated as allowed imaging zones at certain times of day, days of the week, etc. (e.g., during business hours, while there is daylight, and so on) and prohibited imaging zones at others (e.g., during non-business hours, while there is no daylight, and so on).</p><p id="p-0043" num="0042">The server(s) <b>125</b> and/or server(s) <b>112</b> may provide the map of shared environment <b>190</b> to any UAVs operating therein, which may be required to abide by the restrictions set forth in the map, including designated prohibited imaging areas (e.g., windows, entire properties, etc.). In one example, the map may include other restrictions, such as no flyover zones, quiet zones, etc. In the example, of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, UAV <b>161</b> may be tasked with surveilling shared environment <b>190</b> (e.g., autonomously, or under the control of a remote operator, such as via remote control device <b>169</b>). The UAV <b>161</b> may therefore load the map of shared environment <b>190</b>. As such, as UAV <b>161</b> traverses/navigates along a flight path within shared environment <b>190</b>, it may determine its own location (e.g., in a same or a similar manner as discussed above with regard to UAV <b>160</b>), may capture at least one image via at least one imaging sensor of UAV <b>161</b> (e.g., LiDAR, optical camera, infrared camera, etc.), and may determine that the at least one image includes an image portion that correlates to the location of the at least one window.</p><p id="p-0044" num="0043">For example, UAV <b>161</b> may be aware of the field-of-view and ranges of onboard imaging sensors (e.g., optical camera(s), LiDAR unit(s), etc.) as well as the orientations of such imaging sensor(s). Combined with its own position information, UAV <b>161</b> may determine when any points or prohibited imaging zones defined by a set of points are within such field-of-view. To illustrate, in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, any portion or all of the prohibited imaging zone defined by points P<b>1</b>-P<b>4</b> (window <b>192</b>) may enter the field-of-view of one or more imaging sensors of UAV <b>160</b> as UAV <b>160</b> performing surveillance of the share environment <b>190</b>. Upon detecting that any of the prohibited imaging zone is in the field-of-view, in one example, UAV <b>160</b> may continue to capture at least one image via the at least one imaging sensor, but may alter the at least one image to exclude the image portion that correlates to the location of the window <b>192</b>. It should be noted that UAV <b>160</b> need not detect or recognize that the image portion corresponds to a window. Rather, UAV <b>160</b> may determine that the image portion corresponds to the prohibited imaging zone and may alter the at least one image accordingly. The altering may include overwriting an original content of the at least one image portion of the at least one image such that the original content is irretrievably lost, i.e., the portion of the image containing the window cannot be recovered due to the overwriting image processing operation. For instance, the original content may be overwritten with blank data, pixelated or otherwise obscured, replaced with a different data content (e.g., a generic window, an image of the actual window <b>192</b> previously obtained and stored, e.g., with consent of a property owner or other(s) with an interest in the property), etc.</p><p id="p-0045" num="0044">In another example, imaging sensors of UAV <b>160</b> may be oriented in any direction other than that of the prohibited imaging zone when it is approaching the prohibited imaging zone or when the prohibited imaging zone is detected to be within the field-of-view of at least one imaging sensor of AAV <b>160</b>. In still another example, one or more of the imaging sensor(s) may be disabled based upon the same or similar criteria. For instance, an optical camera and/or infrared camera may be disabled, while a LiDAR unit may remain activated. However, in another example, it is specifically the case that imaging sensors remain active and recording/capturing one or more images, while altering the at least one image to exclude the image portion that correlates to the prohibited imaging zone (e.g., window <b>192</b>). For instance, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, there may be a would-be burglar <b>145</b> lurking outside the window <b>192</b>. Thus, while it may be generally desirable to not capture visual data of the window <b>192</b>, it may be preferred to capture visual data from nearby (and directly adjacent to the window <b>192</b>), to help detect any unusual conditions (such as burglar <b>145</b>).</p><p id="p-0046" num="0045">In this regard, it should be noted that UAV <b>160</b> may have various detection models for various objects or other conditions, which UAV <b>160</b> may apply on an ongoing basis to captured image sensor data (e.g., at least one image) to detect such things as any humans present, animals, vehicles, conditions such as fire, flooding, fighting, break-in, stalking, etc., and so on. It should also be noted that in one example, stakeholders of the shared environment, such as owners, tenants, property managers, security personnel, employees, guests, delivery services, property maintenance services, etc. may register various activities or presences with server(s) <b>125</b> and/or server(s) <b>112</b>, such as a scheduled delivery, an intention to go for a walk, an expected arrival of a guest, etc. Thus, a guest's vehicle may be registered upon or before arrival at the shared environment <b>190</b>. If and when the guest's vehicle may be detected by UAV <b>160</b> and/or others, it may either not be detected as an unexpected condition, or if notified by UAV <b>160</b> to server(s) <b>125</b> and/or server(s) <b>112</b>, the server(s) <b>125</b> and/or server(s) <b>112</b>, such reporting may be ignored.</p><p id="p-0047" num="0046">In still another example, UAVs in shared environment, such as UAV <b>161</b>, may track authorized people via RFID or other wireless sensing mechanisms, such as detecting user device <b>141</b> of person <b>140</b> via Institute of Electrical and Electronics Engineers (IEEE) 802.11 based communications (e.g., Wi-Fi Direct) Long Term Evolution (LTE) Direct, a 5G device-to-device (D2D) sidelink, such as over a P5 interface, and so forth), via Dedicated Short Range Communications (DSRC), e.g., in the 5.9 MHz band, or the like, and so on. Thus, for example, UAV <b>161</b> may capture one or more images in the shared environment <b>190</b>, may apply the one or more images as input(s) to one or more detection models, and may detect a &#x201c;person&#x201d; or &#x201c;human&#x201d; when the images include person <b>140</b>. However, upon detection of user device <b>141</b>, UAV <b>161</b> may match the location of the detected &#x201c;person&#x201d; or &#x201c;human&#x201d; with a location of user device <b>141</b> as determined via communication with and/or wireless sensing of such user device <b>141</b>. For example, the user device <b>141</b> may be registered with the server(s) <b>125</b> and/or server(s) <b>112</b>, which may be provided to UAV <b>161</b>. For instance, the shared environment <b>190</b> may require that all personnel, residents, visitors, etc. carry RFID tags that may be sensed by UAVs operating therein. As such, UAV <b>161</b> may determine that this is not an unexpected condition, and may not send a notification to the server(s) <b>125</b> and/or server(s) <b>112</b>. In an example, where UAV <b>161</b> is controlled by a human operator via remote control device <b>169</b>, the UAV <b>161</b> may not provide an enhanced notification to the remote control device <b>169</b>.</p><p id="p-0048" num="0047">On the other hand, burglar <b>145</b> may be detected in the at least one image captured by UAV <b>160</b> that includes the image portion that correlates to the location of the window <b>192</b>. However, there may be no detection of any identification of such person (e.g., an authorized RFID tag, an authorized mobile phone or other user device, etc.). As such, the presence of burglar <b>145</b> may be reported as an unexpected condition. In one example, UAV <b>160</b> may forward one or more images that identify the unexpected condition (e.g., the burglar <b>145</b>) to server(s) <b>125</b> and/or server(s) <b>112</b>, to the remote control device <b>169</b> with a highlighting of the unexpected condition, and/or to one or more other devices, such as a monitoring station of a security guard of the shared environment <b>190</b>. Notably, however, the portion(s) of the image(s) corresponding to the window <b>192</b> may still be overwritten to maintain the privacy of any persons or other entities having an interest in the property, e.g., with respect to any other person who may review the image(s) in real time or upon retrieval from a storage repository.</p><p id="p-0049" num="0048">In various examples, UAV <b>160</b> and/or UAV <b>161</b> may provide additional operations. For instance, UAV <b>161</b> may record any changes to the map and/or mapping data that it detects. UAV <b>161</b> may also notify server(s) <b>125</b> and/or server(s) <b>112</b> of the change(s) so that server(s) <b>125</b> and/or server(s) <b>112</b> may update the stored map and/or mapping data with the most recent information. In addition, UAV <b>161</b> may be an AAV that operates autonomously, and which may record footage for temporary storage, e.g., for two weeks, one month, etc. UAV <b>161</b> may transmit a live video feed to the monitoring station on demand or upon detection of unexpected condition. For an unexpected condition, UAV <b>161</b> may be configured to maintain view and contact, e.g., by circling overhead or maintaining position, any may wait until an operator at a monitoring station releases the UAV <b>161</b> back to continuing surveillance.</p><p id="p-0050" num="0049">In addition, the foregoing illustrates just one example of a system in which examples of the present disclosure for determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone, for determining by an uncrewed aerial vehicle based upon a map of a shared environment that at least one image includes an image portion that captures a location of at least one window and altering the at least one image to exclude the image portion, and or for disabling at least one imaging sensor of an uncrewed aerial vehicle when it is determined, based upon a map of a shared environment, a location of the uncrewed aerial vehicle, and an orientation of at least one imaging sensor of the uncrewed aerial vehicle, that a location of a window is within a field of view of the at least one imaging sensor may operate. It should also be noted that the system <b>100</b> has been simplified. In other words, the system <b>100</b> may be implemented in a different form than that illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For example, the system <b>100</b> may be expanded to include additional networks, and additional network elements (not shown) such as wireless transceivers and/or base stations, border elements, routers, switches, policy servers, security devices, gateways, a network operations center (NOC), a content distribution network (CDN) and the like, without altering the scope of the present disclosure. In addition, system <b>100</b> may be altered to omit various elements, substitute elements for devices that perform the same or similar functions and/or combine elements that are illustrated as separate devices.</p><p id="p-0051" num="0050">As just one example, one or more operations described above with respect to server(s) <b>125</b> may alternatively or additionally be performed by server(s) <b>112</b>, and vice versa. In addition, although server(s) <b>112</b> and <b>125</b> are illustrated in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in other, further, and different examples, the same or similar functions may be distributed among multiple other devices and/or systems within the telecommunication network <b>110</b>, wireless access network(s) <b>115</b>, and/or the system <b>100</b> in general that may collectively provide various services in connection with examples of the present disclosure. In still another example, severs(s) <b>112</b> may reside in telecommunication network <b>110</b>, e.g., at or near an ingress node coupling wireless access network(s) <b>115</b> to telecommunication network <b>110</b>, in a data center of telecommunication network <b>110</b>, or distributed at a plurality of data centers of telecommunication network <b>110</b>, etc. Additionally, devices that are illustrated and/or described as using one form of communication (such as a cellular or non-cellular wireless communications, wired communications, etc.) may alternatively or additionally utilize one or more other forms of communication. For instance, in one example, server(s) <b>125</b> may communicate with UAV <b>160</b>, such as for assigning tasks to UAV <b>160</b>, monitoring for task completion, etc. via a wireless access point (AP) <b>122</b>. For instance, server(s) <b>125</b> may be owned or operated by the same entity owning or controlling the facility <b>190</b>, and may have one or more wireless access points, such as AP <b>122</b>, deployed throughout the facility <b>190</b>. Thus, communications between server(s) <b>125</b> and UAV <b>160</b> may not traverse any networks external to the entity. For instance, AP <b>122</b> and UAV <b>160</b> may establish a session via Wi-Fi Direct, LTE Direct, a 5G D2D sidelink, a DSRC session/pairing, etc.</p><p id="p-0052" num="0051">As noted above, the server(s) <b>125</b> and/or server(s) <b>112</b> may provide the map of shared environment <b>190</b> to any UAVs operating therein, which may be required to abide by the restrictions set forth in the map, including designated prohibited imaging areas (e.g., windows, entire properties, etc.). This may apply to all UAV managed by or via the server(s) <b>125</b> and/or server(s) <b>112</b>, as well as any guest or visitor UAVs. For instance, delivery UAVs may enter the shared environment <b>190</b> and may not be performing tasks under the direction of server(s) <b>125</b> and/or server(s) <b>112</b>. However, such delivery UAVs or others may be required to obtain the map (with restrictions) and to abide by such restrictions, e.g., associated with prohibited imaging zones.</p><p id="p-0053" num="0052">In one example, server(s) <b>125</b> and/or server(s) <b>112</b> may enforce prohibited imaging zones via a wireless channel sensing technique. For instance, prohibited imaging zones may be illuminated periodically or on some other basis, e.g., randomly or semi-randomly, when UAV presence is detected nearby via rough RF sensing, etc. For example, visible or other lighting options (e.g., various colored lights, pulsating lights, etc.) may be directed to illuminate a window. If any UAV is recording and streaming images/video of this window at such time, detectable pattern changes in the wireless downlink signals from the UAV to a remote control device or other ground-based recipients may occur in response to the change in illumination. Furthermore, a pattern of pulsed light illuminating the window may result in a corresponding pattern change in the wireless signals emanating from the UAV. Thus, non-compliance with the prohibited imaging zone may be detected in at least this way. In one example, server(s) <b>125</b> and/or server(s) <b>112</b> may engage in one or more remedial actions, such as notifying authorities, notifying a monitoring station of security personnel of the shared environment <b>190</b>, dispatching another UAV (e.g., an AAV) to intercept and capture, if necessary, the non-compliant UAV, and so forth. Thus, these and other modifications are all contemplated within the scope of the present disclosure.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a flowchart of an example method <b>200</b> for determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone. In one example, steps, functions and/or operations of the method <b>200</b> may be performed by a device or apparatus as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, e.g., by one or more of server(s) <b>125</b> and/or server(s) <b>112</b>, or any one or more components thereof, or by server(s) <b>125</b> or servers <b>112</b>, and/or any one or more components thereof in conjunction with one or more other components of the system <b>100</b>, such as elements of wireless access network <b>115</b>, telecommunication network <b>110</b>, mobile device <b>141</b>, UAVs <b>160</b>-<b>161</b>, and so forth. In another example, steps, functions and/or operations of the method <b>200</b> may be performed by a UAV, such as UAV <b>160</b>, and/or UAV <b>160</b> in conjunction with other components of the system <b>100</b>, such as server(s) <b>125</b> and/or server(s) <b>112</b>, etc. In one example, the steps, functions, or operations of method <b>200</b> may be performed by a computing device or processing system, such as computing system <b>500</b> and/or hardware processor element <b>502</b> as described in connection with <figref idref="DRAWINGS">FIG. <b>5</b></figref> below. For instance, the computing system <b>500</b> may represent any one or more components of the system <b>100</b> that is/are configured to perform the steps, functions and/or operations of the method <b>200</b>. Similarly, in one example, the steps, functions, or operations of the method <b>200</b> may be performed by a processing system comprising one or more computing devices collectively configured to perform various steps, functions, and/or operations of the method <b>200</b>. For instance, multiple instances of the computing system <b>500</b> may collectively function as a processing system. For illustrative purposes, the method <b>200</b> is described in greater detail below in connection with an example performed by a processing system. The method <b>200</b> begins in step <b>205</b> and proceeds to step <b>210</b></p><p id="p-0055" num="0054">At step <b>210</b>, the processing system captures at least one image via at least one imaging sensor of an uncrewed aerial vehicle (UAV) in a shared environment. For instance, as discussed above, the at least one imaging sensor may comprise at least one of an optical camera or a LiDAR unit of the UAV. As also discussed above, the shared environment may comprise a residential complex (e.g., individual homes, condos, townhouses, apartments, etc. serving as residences), an educational complex (e.g., universities, colleges, schools, campuses, etc.), a business complex (e.g., office buildings, retail establishments, malls, strip malls, etc.), or the like. In one example, a notification may be provided to entities associated with the shared environment of a scheduled time of the capturing of the at least one image. For instance, preparations may be made to anticipate a one-time recording of images for locations that will become prohibited imaging zones (e.g., windows of buildings in the shared environment). The notified entities may include homeowners, tenants, landlords, property managers, local town officials, etc. In one example, the UAV may fly a predefined route to cover the shared environment from all vantages or from a sufficient sampling of vantages. In one example, multiple UAVs may be deployed such that the UAV of the subject method <b>200</b> may be assigned a portion of the shared environment. In one example, the UAV may be an AAV, or could be controlled remotely by a human operator.</p><p id="p-0056" num="0055">At step <b>220</b>, the processing system detects a window within the at least one image. For instance, the window may be identified by applying the at least one image as an input to at least one object detection model (e.g., an object detection model that is trained to detect windows or building structures with see-through glass panels within the captured images). In one example, the at least one object detection model is further trained to determine boundaries of windows within the captured images.</p><p id="p-0057" num="0056">At optional step <b>230</b>, the processing system may determine the position of the UAV when the at least one image is captured. For instance, the position of the uncrewed aerial vehicle may be determined from at least one of a GPS unit of the UAV or a calculation of a distance to at least one reference point within the shared environment based upon the at least one image. To illustrate, the at least one reference point may comprise a reflector or a wireless beacon at a known geographic position. In one example, optional step <b>230</b> may include translation from GPS coordinates to coordinates in a local coordinate system for the shared environment (or vice versa). In addition, in one example, the shared environment may be defined by a series of geographic coordinates.</p><p id="p-0058" num="0057">At step <b>240</b>, the processing system determines a location of the window in the shared environment, based upon a position of the uncrewed aerial vehicle and a distance between the uncrewed aerial vehicle and at least a portion of the window that is calculated from the at least one image. As noted above, in one example, the at least one object detection model is trained to determine boundaries of windows within the images. As such, in one example, the determining of the location of the window further comprises determining locations of a plurality of points of a boundary of the window.</p><p id="p-0059" num="0058">At step <b>250</b>, the processing system records the location of the window in a map of the shared environment as a prohibited imaging zone. In an example, where the processing system comprises the UAV, the processing system may record the change in a map stored locally at the UAV. Alternatively, or in addition, the recording the location of the window in the map may comprise providing a set of coordinates defining the boundary of the window, e.g., to a security system and/or fleet management system that maintains the map of the shared environment. In one embodiment, any captured images are detected by the UAVs during or after the mapping operation is completed. In another example in which the processing system comprises a network-based processing system, step <b>250</b> may comprise obtaining the location of the window in the shared environment from the UAV. In this regard, it should be noted that each of the steps <b>210</b>-<b>230</b> may include instructing the UAV and/or obtaining intermediate results/data from the UAV, such as obtaining the at least one image, obtaining the position of the UAV, etc., and calculating the location of the window therefrom. The coordinates may be coordinates in a <b>3</b>D space and can be coordinates in a world reference system (e.g., GPS coordinates including elevation), or can be coordinates in a local coordinate system for the shared environment.</p><p id="p-0060" num="0059">Following step <b>250</b>, the method <b>200</b> proceeds to step <b>295</b>. At step <b>295</b>, the method <b>200</b> ends.</p><p id="p-0061" num="0060">It should be noted that the method <b>200</b> may be expanded to include additional steps, or may be modified to replace steps with different steps, to combine steps, to omit steps, to perform steps in a different order, and so forth. For instance, in one example the processing system may repeat one or more steps of the method <b>200</b>, such as steps <b>210</b>-<b>250</b> to detect additional windows and to record additional prohibited imaging zones in the shared environment during a same or a different flight session. In one example, the method <b>200</b> may include obtaining an opt-out (e.g., withholding consent to allow the UAV to capture images at specific locations) from one or more of the entities of one or more properties, wherein the one or more properties are recorded as additional prohibited imaging zones. In still another example, the method <b>200</b> may include obtaining window(s) may be changed from prohibited imaging zones to allowed recording zones depending upon time of day, day of week, etc. Thus, these and other modifications are all contemplated within the scope of the present disclosure.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a flowchart of an example method <b>300</b> for determining by an uncrewed aerial vehicle based upon a map of a shared environment that at least one image includes an image portion that captures a location of at least one window and altering the at least one image to exclude the image portion. In one example, steps, functions and/or operations of the method <b>300</b> may be performed by an UAV, such as UAV <b>161</b> or UAV <b>160</b> or any one or more components thereof, or by UAV <b>161</b>, and/or any one or more components thereof in conjunction with one or more other components of the system <b>100</b>, such as server(s) <b>125</b>, server(s) <b>112</b>, elements of wireless access network <b>115</b>, telecommunication network <b>110</b>, one or more other UAVs (such as UAV <b>160</b>), and so forth. In one example, the steps, functions, or operations of method <b>300</b> may be performed by a computing device or processing system, such as computing system <b>500</b> and/or hardware processor element <b>502</b> as described in connection with <figref idref="DRAWINGS">FIG. <b>5</b></figref> below. For instance, the computing system <b>500</b> may represent any one or more components of the system <b>100</b> (e.g., UAV <b>161</b>) that is/are configured to perform the steps, functions and/or operations of the method <b>300</b>. Similarly, in one example, the steps, functions, or operations of the method <b>300</b> may be performed by a processing system comprising one or more computing devices collectively configured to perform various steps, functions, and/or operations of the method <b>300</b>. For instance, multiple instances of the computing system <b>500</b> may collectively function as a processing system. For illustrative purposes, the method <b>300</b> is described in greater detail below in connection with an example performed by a processing system. The method <b>300</b> begins in step <b>305</b> and may proceed to step <b>310</b>.</p><p id="p-0063" num="0062">At step <b>310</b>, the processing system (e.g., of an uncrewed aerial vehicle (UAV)) obtains a map of a shared environment, the map including a location of at least one window that is defined as a prohibited imaging zone.</p><p id="p-0064" num="0063">At optional step <b>320</b>, the processing system may navigate the UAV in the shared environment. For example, the processing system may navigate the UAV using the previously obtained map. In one example, the processing system may cause the UAV to fly a defined route in the shared environment, e.g., for security surveillance, or the like.</p><p id="p-0065" num="0064">At step <b>330</b>, the processing system captures at least one image via at least one imaging sensor of the UAV. For instance, the at least one imaging sensor may comprise at least one of an optical camera or a LiDAR unit. In one example, the at least one imaging sensor may also comprise an infrared camera, or the like.</p><p id="p-0066" num="0065">At step <b>340</b>, the processing system determines that the at least one image includes an image portion that correlates to the location of the at least one window. For instance, the prohibited imaging zone may be defined by a series of points of a boundary of the window. Thus, the processing system may determine if any of the boundary or the area within that is defined by the points is within the image. For example, the processing system may know the position of the UAV via a GPS unit or via ranging/sensing of reference points in the shared environment. In addition, the processing system may have awareness of the orientation, field-of-view, and range of the imaging sensor(s). As such, the processing system may calculate that the prohibited imaging zone is captured within the at least one image.</p><p id="p-0067" num="0066">At step <b>350</b>, the processing system alters the at least one image to exclude the image portion, e.g., the original content of the at least one image portion of the at least one image may be overwritten with blank data, pixelated or otherwise obscured, replaced with a different data content (e.g., a generic window, an image of the actual window previously obtained and stored, e.g., with consent of a property owner or other(s) with an interest in the property), etc.</p><p id="p-0068" num="0067">At optional step <b>360</b>, the processing system may determine, based upon the map of the shared environment that the at least one image includes at least one unexpected condition. For instance, the processing system of the UAV may have various detection models for detecting various objects or other conditions, which the processing system may apply on an ongoing basis to captured image sensor data to detect such things as any humans present, animals, vehicles, conditions such as fire, flooding, fighting, break-in, stalking, etc. Thus, for example, an unexpected condition may comprise a human presence outside of the prohibited imaging zone corresponding to the window (where the human is unknown to the processing system, or may comprise a known human whose presence is not authorized), a fire detected within the same image(s) that include the prohibited imaging zone, etc.</p><p id="p-0069" num="0068">At step <b>370</b>, the processing system performs at least one of: (1) providing the at least one image that is altered to exclude the image portion to at least one recipient device, or (2) storing the at least one image that is altered to exclude the image portion. The at least one recipient device may comprise, for example, an operator device that controls the UAV and/or a premises monitoring system, or monitoring station. In one example, the at least one recipient device may comprise a storage system from which stored images (including video and/or LiDAR images/renderings) may be retrieved and reviewed by humans or automated systems. In one example, the at least one recipient device may further comprise an artificial intelligence (Al) and/or machine learning (ML)-based system that further analyzes stored images. In an example where the unexpected condition is determined at optional step <b>360</b>, the at least one image that is altered to exclude the image portion may further include a highlighting of the at least one unexpected condition (e.g., for forwarding to the at least one recipient device, where a human monitoring the at least one recipient device may have his or her attention further drawn to the detected unexpected condition).</p><p id="p-0070" num="0069">Following step <b>370</b>, the method <b>300</b> proceeds to step <b>395</b>. At step <b>395</b>, the method <b>300</b> ends.</p><p id="p-0071" num="0070">It should be noted that the method <b>300</b> may be expanded to include additional steps, or may be modified to replace steps with different steps, to combine steps, to omit steps, to perform steps in a different order, and so forth. For instance, in one example, the processing system may repeat one or more steps of the method <b>300</b> for additional images relating to the same prohibited imaging zone, for different prohibited imaging zones, and so forth. In another example, the method <b>300</b> may include summoning another UAV or non-aerial AV to provide assistance in obtaining additional images relating an unexpected condition. In one example, steps <b>350</b> and <b>360</b> may comprise creating an altered version of the at least one image and then deleting an original copy of the image after storing the altered image or providing the altered image to the at least one recipient device. Thus, these and other modifications are all contemplated within the scope of the present disclosure.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a flowchart of an example method <b>400</b> for disabling at least one imaging sensor of an uncrewed aerial vehicle when it is determined, based upon a map of a shared environment, a location of the uncrewed aerial vehicle, and an orientation of at least one imaging sensor of the uncrewed aerial vehicle, that a location of a window is within a field of view of the at least one imaging sensor. In one example, steps, functions and/or operations of the method <b>400</b> may be performed by a UAV, such as UAV <b>161</b> or UAV <b>160</b> or any one or more components thereof, or by UAV <b>161</b> or UAV <b>160</b>, and/or any one or more components thereof in conjunction with one or more other components of the system <b>100</b>, such as server(s) <b>125</b>, server(s) <b>112</b>, elements of wireless access network <b>115</b>, telecommunication network <b>110</b>, one or more other UAVs (such as UAV <b>160</b>), and so forth. In one example, the steps, functions, or operations of method <b>400</b> may be performed by a computing device or processing system, such as computing system <b>500</b> and/or hardware processor element <b>502</b> as described in connection with <figref idref="DRAWINGS">FIG. <b>5</b></figref> below. For instance, the computing system <b>500</b> may represent any one or more components of the system <b>100</b> (e.g., UAV <b>161</b>) that is/are configured to perform the steps, functions and/or operations of the method <b>400</b>. Similarly, in one example, the steps, functions, or operations of the method <b>400</b> may be performed by a processing system comprising one or more computing devices collectively configured to perform various steps, functions, and/or operations of the method <b>400</b>. For instance, multiple instances of the computing system <b>500</b> may collectively function as a processing system. For illustrative purposes, the method <b>400</b> is described in greater detail below in connection with an example performed by a processing system. The method <b>400</b> begins in step <b>405</b> and may proceed to step <b>410</b>.</p><p id="p-0073" num="0072">At step <b>410</b>, the processing system (e.g., of an uncrewed aerial vehicle (UAV)) obtains a map of a shared environment, the map including a location of at least one window that is defined as a prohibited imaging zone. For instance, step <b>410</b> may comprise the same or similar operations as step <b>310</b> discussed above.</p><p id="p-0074" num="0073">At step <b>420</b>, the processing system may navigate the UAV in the shared environment. For example, the processing system may navigate the UAV using the previously obtained map. In one example, the processing system may cause the UAV to fly a defined route in the shared environment, e.g., for security surveillance, or the like. For instance, step <b>420</b> may comprise the same or similar operations as optional step <b>320</b> discussed above.</p><p id="p-0075" num="0074">At step <b>430</b>, the processing system determines based upon the map, a location of the UAV, and an orientation of at least one imaging sensor of the UAV, that the location of the window is within a field of view of the at least one imaging sensor. For instance, as discussed in the examples above, the processing system of the UAV may be aware of the field-of-view and ranges of onboard imaging sensors (e.g., optical camera(s), LiDAR unit(s), etc.) as well as the orientations of such imaging sensor(s). Combined with its own position information of the UAV, the processing system may determine when any points or prohibited imaging zones defined by a set of points are within such field-of-view.</p><p id="p-0076" num="0075">At step <b>440</b>, the processing system disables or reorients the at least one imaging sensor when the location of the window is within the field of view of the at least one imaging sensor.</p><p id="p-0077" num="0076">Following step <b>440</b>, the method <b>400</b> proceeds to step <b>495</b>. At step <b>495</b>, the method <b>400</b> ends.</p><p id="p-0078" num="0077">It should be noted that the method <b>400</b> may be expanded to include additional steps, or may be modified to replace steps with different steps, to combine steps, to omit steps, to perform steps in a different order, and so forth. For instance, in one example, the processing system may repeat one or more steps of the method <b>400</b> for the same prohibited imaging zone, for different prohibited imaging zones, and so forth. In another example, prior to or in connection with step <b>430</b>, the processing system may calculate the position of the UAV, e.g., in the same or a similar manner as discussed above in connection with optional step <b>230</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In another example, the method <b>400</b> may include detecting that the prohibited imaging area is no longer within the field of view of the at least one imaging sensor, and re-activating or reorienting the at least one imaging sensor when it is detected that the prohibited imaging area is no longer within the field of view of the at least one imaging sensor. Thus, these and other modifications are all contemplated within the scope of the present disclosure.</p><p id="p-0079" num="0078">In addition, although not expressly specified above, one or more steps of the method <b>200</b>, the method <b>300</b>, and/or the method <b>400</b> may include a storing, displaying and/or outputting step as required for a particular application. In other words, any data, records, fields, and/or intermediate results discussed in the method can be stored, displayed and/or outputted to another device as required for a particular application. Furthermore, operations, steps, or blocks in <figref idref="DRAWINGS">FIGS. <b>2</b>-<b>4</b></figref> that recite a determining operation or involve a decision do not necessarily require that both branches of the determining operation be practiced. In other words, one of the branches of the determining operation can be deemed as an optional step. However, the use of the term &#x201c;optional step&#x201d; is intended to only reflect different variations of a particular illustrative embodiment and is not intended to indicate that steps not labelled as optional steps to be deemed to be essential steps. Furthermore, operations, steps or blocks of the above described method(s) can be combined, separated, and/or performed in a different order from that described above, without departing from the example embodiments of the present disclosure.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a high-level block diagram of a computing system <b>500</b> (e.g., a computing device or processing system) specifically programmed to perform the functions described herein. For example, any one or more components, devices, and/or systems illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> or described in connection with <figref idref="DRAWINGS">FIGS. <b>2</b>-<b>4</b></figref>, may be implemented as the computing system <b>500</b>. As depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the computing system <b>500</b> comprises a hardware processor element <b>502</b> (e.g., comprising one or more hardware processors, which may include one or more microprocessor(s), one or more central processing units (CPUs), and/or the like, where the hardware processor element <b>502</b> may also represent one example of a &#x201c;processing system&#x201d; as referred to herein), a memory <b>504</b>, (e.g., random access memory (RAM), read only memory (ROM), a disk drive, an optical drive, a magnetic drive, and/or a Universal Serial Bus (USB) drive), a module <b>505</b> for determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone, for determining by an uncrewed aerial vehicle based upon a map of a shared environment that at least one image includes an image portion that captures a location of at least one window and altering the at least one image to exclude the image portion, or for disabling at least one imaging sensor of an uncrewed aerial vehicle when it is determined, based upon a map of a shared environment, a location of the uncrewed aerial vehicle, and an orientation of at least one imaging sensor of the uncrewed aerial vehicle, that a location of a window is within a field of view of the at least one imaging sensor, and various input/output devices <b>506</b>, e.g., a camera, a video camera, storage devices, including but not limited to, a tape drive, a floppy drive, a hard disk drive or a compact disk drive, a receiver, a transmitter, a speaker, a display, a speech synthesizer, an output port, and a user input device (such as a keyboard, a keypad, a mouse, and the like).</p><p id="p-0081" num="0080">Although only one hardware processor element <b>502</b> is shown, the computing system <b>500</b> may employ a plurality of hardware processor elements. Furthermore, although only one computing device is shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, if the method(s) as discussed above is implemented in a distributed or parallel manner for a particular illustrative example, e.g., the steps of the above method(s) or the entire method(s) are implemented across multiple or parallel computing devices, then the computing system <b>500</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> may represent each of those multiple or parallel computing devices. Furthermore, one or more hardware processor elements (e.g., hardware processor element <b>502</b>) can be utilized in supporting a virtualized or shared computing environment. The virtualized computing environment may support one or more virtual machines which may be configured to operate as computers, servers, or other computing devices. In such virtualized virtual machines, hardware components such as hardware processors and computer-readable storage devices may be virtualized or logically represented. The hardware processor element <b>502</b> can also be configured or programmed to cause other devices to perform one or more operations as discussed above. In other words, the hardware processor element <b>502</b> may serve the function of a central controller directing other devices to perform the one or more operations as discussed above.</p><p id="p-0082" num="0081">It should be noted that the present disclosure can be implemented in software and/or in a combination of software and hardware, e.g., using application specific integrated circuits (ASIC), a programmable logic array (PLA), including a field-programmable gate array (FPGA), or a state machine deployed on a hardware device, a computing device, or any other hardware equivalents, e.g., computer-readable instructions pertaining to the method(s) discussed above can be used to configure one or more hardware processor elements to perform the steps, functions and/or operations of the above disclosed method(s). In one example, instructions and data for the present module <b>505</b> for determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone, for determining by an uncrewed aerial vehicle based upon a map of a shared environment that at least one image includes an image portion that captures a location of at least one window and altering the at least one image to exclude the image portion, or for disabling at least one imaging sensor of an uncrewed aerial vehicle when it is determined, based upon a map of a shared environment, a location of the uncrewed aerial vehicle, and an orientation of at least one imaging sensor of the uncrewed aerial vehicle, that a location of a window is within a field of view of the at least one imaging sensor (e.g., a software program comprising computer-executable instructions) can be loaded into memory <b>504</b> and executed by hardware processor element <b>502</b> to implement the steps, functions or operations as discussed above in connection with the example method(s). Furthermore, when a hardware processor element executes instructions to perform operations, this could include the hardware processor element performing the operations directly and/or facilitating, directing, or cooperating with one or more additional hardware devices or components (e.g., a co-processor and the like) to perform the operations.</p><p id="p-0083" num="0082">The processor (e.g., hardware processor element <b>502</b>) executing the computer-readable instructions relating to the above described method(s) can be perceived as a programmed processor or a specialized processor. As such, the present module <b>505</b> for determining via at least one imaging sensor of an uncrewed aerial vehicle a location of a window in a shared environment and recording the location of the window in a map of the shared environment as a prohibited imaging zone, for determining by an uncrewed aerial vehicle based upon a map of a shared environment that at least one image includes an image portion that captures a location of at least one window and altering the at least one image to exclude the image portion, or for disabling at least one imaging sensor of an uncrewed aerial vehicle when it is determined, based upon a map of a shared environment, a location of the uncrewed aerial vehicle, and an orientation of at least one imaging sensor of the uncrewed aerial vehicle, that a location of a window is within a field of view of the at least one imaging sensor (including associated data structures) of the present disclosure can be stored on a tangible or physical (broadly non-transitory) computer-readable storage device or medium, e.g., volatile memory, non-volatile memory, ROM memory, RAM memory, magnetic or optical drive, device or diskette and the like. Furthermore, a &#x201c;tangible&#x201d; computer-readable storage device or medium may comprise a physical device, a hardware device, or a device that is discernible by the touch. More specifically, the computer-readable storage device or medium may comprise any physical devices that provide the ability to store information such as instructions and/or data to be accessed by a processor or a computing device such as a computer or an application server.</p><p id="p-0084" num="0083">While various examples have been described above, it should be understood that they have been presented by way of example only, and not limitation. Thus, the breadth and scope of a preferred example should not be limited by any of the above-described examples, but should be defined only in accordance with the following claims and their equivalents.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>capturing, by a processing system including at least one processor, at least one image via at least one imaging sensor of an uncrewed aerial vehicle in a shared environment, wherein the shared environment comprises at least one of: a residential complex, an educational complex, or a business complex;</claim-text><claim-text>detecting, by the processing system, a window within the at least one image;</claim-text><claim-text>determining, by the processing system, a location of the window in the shared environment, based upon a position of the uncrewed aerial vehicle and a distance between the uncrewed aerial vehicle and at least a portion of the window that is calculated from the at least one image;</claim-text><claim-text>recording, by the processing system, the location of the window in a map of the shared environment as a prohibited imaging zone; and</claim-text><claim-text>storing, by the processing system, the map of the shared environment with the location of the window recorded as the prohibited imaging zone into a database that is accessible by at least one other uncrewed aerial vehicle.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one imaging sensor comprises an optical camera.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one imaging sensor comprises a light detection and ranging unit.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining the position of the uncrewed aerial vehicle when the at least one image is captured.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the position of the uncrewed aerial vehicle is determined from a global positioning system unit of the uncrewed aerial vehicle.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the position of the uncrewed aerial vehicle is determined from a calculation of a distance to at least one reference point within the shared environment based upon the at least one image.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the window is identified by applying the at least one image as an input to at least one object detection model.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the at least one object detection model is trained to detect one or more windows within an image.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the at least one object detection model is further trained to determine boundaries of the one or more windows within the image.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining the location of the window further comprises determining locations of a plurality of points of a boundary of the window.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the recording the location of the window in the map comprises providing a set of coordinates defining the boundary of the window.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a notification is provided to at least one entity associated with the shared environment of a scheduled time of the capturing of the at least one image.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A non-transitory computer-readable medium storing instructions which, when executed by a processing system including at least one processor, cause the processing system to perform operations, the operations comprising:<claim-text>capturing at least one image via at least one imaging sensor of an uncrewed aerial vehicle in a shared environment, wherein the shared environment comprises at least one of: a residential complex, an educational complex, or a business complex;</claim-text><claim-text>detecting a window within the at least one image;</claim-text><claim-text>determining a location of the window in the shared environment, based upon a position of the uncrewed aerial vehicle and a distance between the uncrewed aerial vehicle and at least a portion of the window that is calculated from the at least one image;</claim-text><claim-text>recording the location of the window in a map of the shared environment as a prohibited imaging zone; and</claim-text><claim-text>storing the map of the shared environment with the location of the window recorded as the prohibited imaging zone into a database that is accessible by at least one other uncrewed aerial vehicle.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the at least one imaging sensor comprises at least one of:<claim-text>an optical camera; or</claim-text><claim-text>a light detection and ranging unit.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, the operations further comprising:<claim-text>determining the position of the uncrewed aerial vehicle when the at least one image is captured.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the position of the uncrewed aerial vehicle is determined from at least one of:<claim-text>a global positioning system unit of the uncrewed aerial vehicle; or</claim-text><claim-text>a calculation of a distance to at least one reference point within the shared environment based upon the at least one image.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the window is identified by applying the at least one image as an input to at least one object detection model.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the at least one object detection model is trained to detect one or more windows within an image.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the at least one object detection model is further trained to determine boundaries of the one or more windows within the image.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A device comprising:<claim-text>a processing system including at least one processor; and</claim-text><claim-text>a computer-readable medium storing instructions which, when executed by the processing system, cause the processing system to perform operations, the operations comprising:<claim-text>capturing at least one image via at least one imaging sensor of an uncrewed aerial vehicle in a shared environment, wherein the shared environment comprises at least one of: a residential complex, an educational complex, or a business complex;</claim-text><claim-text>detecting a window within the at least one image;</claim-text><claim-text>determining a location of the window in the shared environment, based upon a position of the uncrewed aerial vehicle and a distance between the uncrewed aerial vehicle and at least a portion of the window that is calculated from the at least one image;</claim-text><claim-text>recording the location of the window in a map of the shared environment as a prohibited imaging zone; and</claim-text><claim-text>storing the map of the shared environment with the location of the window recorded as the prohibited imaging zone into a database that is accessible by at least one other uncrewed aerial vehicle.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>