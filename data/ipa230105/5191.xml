<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005192A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005192</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17866183</doc-number><date>20220715</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>80</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>222</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>81</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>854</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>2343</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>80</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>2224</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>0037</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>8146</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>854</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>234336</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>816</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>11</class><subclass>B</subclass><main-group>27</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR CREATING A 2D FILM FROM IMMERSIVE CONTENT</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17078062</doc-number><date>20201022</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11403787</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17866183</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62925710</doc-number><date>20191024</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Baobab Studios Inc.</orgname><address><city>Redwood City</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Solovykh</last-name><first-name>Mikhail Stanislavovich</first-name><address><city>Redwood City</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Wei</first-name><address><city>Redwood City</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Dirksen</last-name><first-name>Nathaniel Christopher</first-name><address><city>Redwood City</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Cutler</last-name><first-name>Lawrence David</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Lerios</last-name><first-name>Apostolos</first-name><address><city>Austin</city><state>TX</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems, methods, and non-transitory computer-readable media can obtain data associated with a computer-based experience. The computer-based experience can be based on interactive real-time technology. At least one virtual camera can be configured within the computer-based experience in a real-time engine. Data associated with an edit cut of the computer-based experience can be obtained based on content captured by the at least one virtual camera. A plurality of shots that correspond to two-dimensional content can be generated from the edit cut of the computer-based experience in the real-time engine. Data associated with a two-dimensional version of the computer-based experience can be generated with the real-time engine based on the plurality of shots. The two-dimensional version can be rendered based on the generated data.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="93.81mm" wi="158.75mm" file="US20230005192A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="227.67mm" wi="145.37mm" orientation="landscape" file="US20230005192A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="222.59mm" wi="146.05mm" orientation="landscape" file="US20230005192A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="216.75mm" wi="145.29mm" orientation="landscape" file="US20230005192A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="209.89mm" wi="140.29mm" file="US20230005192A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="228.60mm" wi="137.24mm" file="US20230005192A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="204.98mm" wi="152.57mm" orientation="landscape" file="US20230005192A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="209.89mm" wi="140.29mm" file="US20230005192A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="195.75mm" wi="137.24mm" file="US20230005192A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="205.82mm" wi="141.73mm" file="US20230005192A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="239.35mm" wi="137.16mm" file="US20230005192A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="238.34mm" wi="137.16mm" file="US20230005192A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="228.52mm" wi="136.57mm" file="US20230005192A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="241.55mm" wi="136.57mm" file="US20230005192A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="228.01mm" wi="158.16mm" orientation="landscape" file="US20230005192A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="186.44mm" wi="153.67mm" orientation="landscape" file="US20230005192A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="235.29mm" wi="141.22mm" file="US20230005192A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="235.88mm" wi="141.31mm" file="US20230005192A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="236.39mm" wi="141.22mm" file="US20230005192A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="122.51mm" wi="141.22mm" file="US20230005192A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="217.25mm" wi="146.73mm" file="US20230005192A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/078,062, filed on Oct. 22, 2020, and entitled &#x201c;SYSTEMS AND METHODS FOR CREATING A 2D FILM FROM IMMERSIVE CONTENT&#x201d;, which claims priority to U.S. Provisional Patent Application No. 62/925,710, filed on Oct. 24, 2019 and entitled &#x201c;SYSTEMS AND METHODS FOR CREATING A 2D FILM FROM IMMERSIVE CONTENT,&#x201d; all of which are incorporated in their entireties herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE INVENTION</heading><p id="p-0003" num="0002">The present technology relates to the field of digital animation. More particularly, the present technology relates to techniques for generating 2D content from AR/VR content.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Virtual Reality (VR) and Augmented Reality (AR) are new mediums for entertainment and storytelling that enable content creators to immerse viewers in ways that are not possible in other mediums. VR and AR are powerful immersive platforms to tell engaging stories with characters with which audiences can interact and empathize. A user (e.g., a viewer) is viscerally connected to the world around them. The user can be immersed, can have agency, and can look anywhere. The user can also have a role to play and can be inspired to act. Characters can acknowledge the user exists in their world and can respond to user actions in real-time. By contrast, 2D content (e.g., a movie, film, TV show) is a passive and cinematic medium that can elicit empathy with characters, but there is, of course, no interaction.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">Various embodiments of the present technology can include systems, methods, and non-transitory computer readable media configured to obtain data associated with a computer-based experience. The computer-based experience can be based on interactive real-time technology. At least one virtual camera can be configured within the computer-based experience in a real-time engine. Data associated with an edit cut of the computer-based experience can be obtained based on content captured by the at least one virtual camera. A plurality of shots that correspond to two-dimensional content can be generated from the edit cut of the computer-based experience in the real-time engine. Data associated with a two-dimensional version of the computer-based experience can be generated with the real-time engine based on the plurality of shots. The two-dimensional version can be rendered based on the generated data.</p><p id="p-0006" num="0005">In an embodiment, the computer-based experience is based on immersive real-time technology.</p><p id="p-0007" num="0006">In an embodiment, the data associated with the two-dimensional version is interactive 2D content.</p><p id="p-0008" num="0007">In an embodiment, obtaining the data associated with the edit cut further includes importing data describing a set of edits reflected in the edit cut from non-linear video editing software and into the real-time engine.</p><p id="p-0009" num="0008">In an embodiment, generating the plurality of shots that correspond to the two-dimensional content further includes applying additional set dressing and layout data to one or more frames associated with at least one shot included in the plurality of shots with the real-time engine; and applying lighting and one or more media effects to the one or more frames, wherein the lighting and the one or more media effects are added on top of the lighting and the one or more media effects applied to the computer-based experience in the real-time engine.</p><p id="p-0010" num="0009">In an embodiment, generating data associated with the two-dimensional version of the computer-based experience from the real-time engine further includes generating at least one new shot for the two-dimensional version in an animation creation application, or applying one or more animation fixes to at least one shot for the two-dimensional version in the animation creation application.</p><p id="p-0011" num="0010">In an embodiment, the systems, methods, and non-transitory computer readable media are configured to create a copy of the at least one shot prior to application of the one or more animation fixes, wherein the one or more animation fixes are applied to the copy.</p><p id="p-0012" num="0011">In an embodiment, generating data associated with the two-dimensional version further includes rendering the two-dimensional version with the real-time engine.</p><p id="p-0013" num="0012">In an embodiment, the rendering is performed in-editor through the real-time engine.</p><p id="p-0014" num="0013">In an embodiment, the systems, methods, and non-transitory computer readable media are configured to determine a timeline associated with the two-dimensional version; determine a region marked to be rendered in the timeline; and render a portion of the two-dimensional version that corresponds to the region with the real-time engine.</p><p id="p-0015" num="0014">Various embodiments of the present technology can include systems, methods, and non-transitory computer readable media configured to obtain data associated with a computer-based experience. The computer-based experience can be based on interactive real-time technology. At least one virtual camera can be configured within the computer-based experience in an animation creation application. A plurality of shots that correspond to two-dimensional content can be generated from an edit cut of content captured by the at least one virtual camera in the animation creation application. Data associated with a two-dimensional version of the computer-based experience can be generated in a real-time engine based on the plurality of shots. The two-dimensional version can be rendered based on the generated data.</p><p id="p-0016" num="0015">In an embodiment, the systems, methods, and non-transitory computer readable media are configured to render the two-dimensional version in the real-time engine.</p><p id="p-0017" num="0016">In an embodiment, generating the plurality of shots that correspond to the two-dimensional content further includes obtaining data associated with the edit cut of the computer-based experience from non-linear video editing software.</p><p id="p-0018" num="0017">In an embodiment, generating the plurality of shots that correspond to the two-dimensional content further includes importing data associated with the plurality of shots generated in the animation creation application into the real-time engine.</p><p id="p-0019" num="0018">In an embodiment, the systems, methods, and non-transitory computer readable media are configured to create a first animation layer associated with the two-dimensional version that is separate from a second animation layer associated with the computer-based experience, wherein adjustments made to a shot in the plurality of shots are applied to the first animation layer.</p><p id="p-0020" num="0019">In an embodiment, the systems, methods, and non-transitory computer readable media are configured to create a first animation layer associated with a timeline corresponding to the two-dimensional version that is separate from a second animation layer associated with the computer-based experience, wherein adjustments made to the timeline corresponding to the two-dimensional version are applied to the first animation layer.</p><p id="p-0021" num="0020">In an embodiment, generating the plurality of shots that correspond to the two-dimensional content further includes applying one or more animation fixes to at least one shot for the two-dimensional version in the animation creation application.</p><p id="p-0022" num="0021">In an embodiment, applying the one or more animation fixes to at least one shot for the two-dimensional version in the animation creation application further includes creating a copy of the at least one shot prior to application of the one or more animation fixes, wherein the one or more animation fixes are applied to the copy.</p><p id="p-0023" num="0022">In an embodiment, the systems, methods, and non-transitory computer readable media are configured to apply lighting and one or more media effects to frames associated with the computer-based experience in the real-time engine; and apply lighting and one or more media effects to frames associated with the two-dimensional version in the real-time engine, wherein the lighting and the one or more media effects to frames associated with the two-dimensional version are added on top of the lighting and the one or more media effects applied to the computer-based experience.</p><p id="p-0024" num="0023">In an embodiment, the systems, methods, and non-transitory computer readable media are configured to apply additional set dressing and layout data to one or more frames associated with at least one shot included in the plurality of shots in the real-time engine.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>B</figref> illustrate conventional workflows for generating AR/VR content and 2D content.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example system for generating 2D content, according to an embodiment of the present technology.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref> illustrate example methods, according to an embodiment of the present technology.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates another example system for generating 2D content, according to an embodiment of the present technology.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> illustrate example methods, according to an embodiment of the present technology.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIGS. <b>6</b>-<b>17</b></figref> illustrate various interfaces and related outputs, according to embodiments of the present technology.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIGS. <b>18</b>-<b>24</b></figref> illustrate various improvements to 2D content derived from AR/VR content, according to embodiments of the present technology.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates an example of a computer system or computing device that can be utilized in various scenarios, according to an embodiment of the present technology.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0033" num="0032">The figures depict various embodiments of the disclosed technology for purposes of illustration only, wherein the figures use like reference numerals to identify like elements. One skilled in the art will readily recognize from the following discussion that alternative embodiments of the structures and methods illustrated in the figures can be employed without departing from the principles of the disclosed technology described herein.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0034" num="0033">Creating a 2D Film from Immersive Content</p><p id="p-0035" num="0034">Virtual Reality (VR) and Augmented Reality (AR) are new mediums for entertainment and storytelling that enable content creators to immerse viewers in ways that are not possible in other mediums. VR and AR are powerful real-time immersive platforms to tell engaging stories with characters with which audiences can interact and empathize. A user (e.g., a viewer) is viscerally connected to the world around them. The user can be immersed, can have agency, and can look anywhere. The user can also have a role to play and can be inspired to act. Characters can acknowledge the user exists in their world and can respond to user actions in real-time. By contrast, 2D content (e.g., a movie, film, TV show) is a passive and cinematic medium that can elicit empathy with characters, but there is typically no interaction. More recently, interactive 2D content (e.g., Interactive TV shows) allows users to make choices at specific moments in a story which then branch into alternate 2D storylines and alternate 2D content for each branch of the narrative.</p><p id="p-0036" num="0035">A storytelling project may rely on a traditional computer-animated movie pipeline (or process) to produce 2D content for a non-interactive medium, such as movies or TV. Further, a storytelling project may rely on an interactive real-time pipeline (or process) to produce AR/VR content for an interactive medium, such as a computer-animated real-time experience based on VR or AR technology. A storytelling project may rely on an interactive real-time pipeline (or process) to produce interactive game content, such as a computer-animated real-time experience based on mobile, console, or PC technology. Sometimes, a storytelling project can require content to be produced for both interactive and non-interactive mediums. For example, the storytelling project can require an interactive version of a story for viewers that prefer an immersive experience based on VR or AR technology and a non-interactive version of the story for viewers that prefer a traditional 2D movie experience. Under conventional approaches, the AR/VR version will be created based on the interactive real-time process and the 2D version will be created separately based on the traditional computer-animated movie process, as discussed in relation to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> illustrates an example conventional interactive real-time pipeline <b>100</b> for creating AR/VR content for a story. As shown, the interactive real-time pipeline <b>100</b> begins with concept development using various two-dimensional (2D) tools <b>102</b>. Initially, at block <b>104</b>, a story and script can be created for interactive content, such as an interactive story or game. The story and script may consider the role of a user (or viewer), interactive mechanics, and non-linearity. For example, there may be alternate dialogue lines based on user choice. At block <b>106</b>, as the story is refined, concept artwork can be created. Environmental concept art and character designs can be created to explore the world and the characters of the story. At block <b>108</b>, the ways in which the user interacts with the characters, the story, and the environment may be designed. For example, this can involve designing the core mechanic, interactive elements, and non-linear story flow. At block <b>110</b>, the script can be translated into 2D storyboard drawings that visually bring scenes to life. The 2D storyboards are first pitched to the director, iterated on, and then edited into a video story reel. The 2D storyboards provide a quick and cheap way to watch the entire film doing no more expensive production work. At block <b>114</b>, the real-time engine <b>112</b> generates 3D storyboards based on the 2D storyboards produced by the 2D tools <b>102</b>. Storyboards can be created in 3D so they can be placed and viewed in a three-dimensional environment. 3D storyboards can be created as 2D sprites that are moved around in 3D space or drawn directly in 3D in the real-time engine <b>112</b>, in other 3D drawing/storyboarding tools, or in VR drawing/storyboarding tools. Next, at block <b>116</b>, previz can be done in the real-time engine <b>112</b> to visualize scenes before going into animation production. At block <b>118</b>, editorial will add dialogue, music, audio, and edit shot sequences for timing. In some cases, a movie edit may not be generated since the content changes based on user input. At block <b>120</b>, set dressing, final environment models, and final characters can be incorporated into the scenes. At block <b>124</b>, an animation creation tool <b>122</b> is used to create all the character and environment assets. At block <b>126</b>, scenes and shots can be animated. In interactive games and AR/VR content, one shot typically represents one character action (called an animation cycle or cycle animation). At block <b>128</b>, the real-time engine <b>112</b> can combine and/or procedurally generate animation through non-linear animation frameworks, AI systems, and other procedural methods. At block <b>130</b>, the real-time engine <b>112</b> can create FX and simulation elements. At block <b>132</b>, the real-time engine <b>112</b> can apply lighting effects to the animation in real-time. This is often achieved through a few dynamic lights calculated in real-time on dynamic, non-static elements (i.e. characters) of the scene combined with pre-baked global illumination calculated for static elements (i.e. non-moving parts of the environment) in the scene. At block <b>134</b>, the real-time engine <b>112</b> outputs an interactive application. The interactive application may be a computer-animated real-time experience based on VR or AR technology or an interactive game.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates an example conventional computer-animated movie pipeline <b>150</b> for creating non-interactive 2D content (e.g., a 2D computer-animated movie). As shown, the computer-animated movie pipeline <b>150</b> begins with concept development using various two-dimensional (2D) tools <b>152</b>. At block <b>154</b>, a story can be created for the 2D computer-animated movie. The story can be used to create a screenplay or script. At block <b>156</b>, as the story is refined, concept artwork can be created to define the look of the movie. Environmental concept art and character designs can be created to explore the world and the characters of the story. At block <b>158</b>, the script can be translated into 2D storyboard drawings that visually bring the scenes to life. The 2D storyboards are first pitched to the director, iterated on, and then edited into a video story reel. The 2D storyboards provide a quick and cheap way to watch the entire film without doing any more expensive production work. At block <b>160</b>, previz artists translate the storyboard reels into 3D pre-visualizations of sequences. The previz artists create mocks of environment models and characters models. Then artists can place virtual cameras in scenes to establish cinematography and provide editorial footage to cut shots together. At block <b>162</b>, the editor may create an edit cut in non-linear video editing software that combines the camera shots into a single sequence movie. This edit cut can be reviewed by the director, which often requires adjustments to existing cameras and and/or additional cameras to be created by the previz artist. At block <b>164</b>, the characters and environments can be translated into final 3D models with textures. Character assets are rigged as digital puppets so animators can make these characters move and emote in a believable way. At block <b>166</b>, final shots are created, camera cinematography is refined, and set dressing is adjusted per shot. At block <b>168</b>, the animation can be broken up into a series of shots or scenes. For a given shot, either an animator will hand-animate or keyframe characters over time or the animation may be motion captured from an actor's live performance. At block <b>170</b>, FX elements (e.g., water, fire, fog, etc.) can be added to shots. Further, clothing, hair, and other dynamic components are simulated. These elements rarely are applied in real-time. At block <b>172</b>, lighting artists may apply virtual lights to shots to create final images of the movie. Master lighting often is first done to establish the appearance for an entire scene and/or for a smaller number of key shots. Shot lighting can then be applied to fine-tune lighting on characters and environments on a per-shot basis. At block <b>174</b>, scene data can be rendered with an offline, non-real-time renderer. Rendering can occur at an artist's desk or on render farm machines either on-premise or in the cloud. Rendering often generates multiple render layer passes (e.g., depth pass, matte pass, etc.) per frame for a compositing package. At block <b>176</b>, final combined render frames can be generated using compositing software to produce the non-interactive 2D content.</p><p id="p-0039" num="0038">Thus, under conventional approaches, an entity will need to use an interactive real-time process, such as the process illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, to create an AR/VR version of a story. Further, under conventional approaches, the entity will need to use a separate computer-animated movie process, such as the process illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, to create a 2D version of the story. The need for different processes to generate interactive and non-interactive versions of the same story can cause duplicative work, increased costs, and increased production complexity.</p><p id="p-0040" num="0039">An improved approach rooted in computer technology overcomes the foregoing and other disadvantages associated with conventional approaches specifically arising in the realm of computer technology. The present technology provides the ability to create interactive and non-interactive versions of a story based on a single process (or pipeline). The present technology allows non-interactive 2D content for a story to be generated from an existing interactive AR/VR version of the story. For example, the present technology allows a 2D computer-animated movie to be generated from an existing computer-animated real-time experience based on VR or AR technology. As a result, the present technology provides a comprehensive toolset that can enable filmmakers to meticulously craft a 2D computer-animated movie without compromising features available in conventional processes. The present technology also enables simultaneous development of both 2D- and AR/VR-based projects where changes to the AR/VR-based project can be propagated to the 2D-based project and vice-versa. The present technology can also utilize AR/VR as a content creation medium for developing the cinematography and look for a 2D project. The present technology can also handle different pipeline architectures. More details relating to the present technology are provided below.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example system <b>200</b>, according to an embodiment of the present technology. The example system <b>200</b> can include an animation creation module <b>202</b>, a real-time engine module (or real-time engine) <b>212</b>, and a 2D editorial module <b>232</b>. The example system <b>200</b> can be implemented to generate non-interactive 2D content from AR/VR content. The example system <b>200</b> can be implemented to generate non-interactive 2D content from interactive game content developed for mobile, console, and PC technology. For example, the example system <b>200</b> can generate a non-interactive 2D computer-animated film or TV show from an interactive computer-animated real-time experience based on VR or AR technology. In another example, the example system <b>200</b> can generate a first-person 2D computer-animated film or TV show from a character-based computer-animated real-time experience based on VR or AR technology. The character-based computer-animated real-time experience may involve an immersive experience in which a viewer can participate in a story as a character. In another example, the example system <b>200</b> can generate an interactive 2D computer-animated film or TV show from a character-based computer-animated real-time experience based on VR or AR technology. The example system <b>200</b> will generate a set of 2D content paths representing different branches of the story. Many variations are possible. In contrast to the example system <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> that is discussed in more detail herein, the example system <b>200</b> allows camera placement and shot creation to occur in relation to the real-time engine module <b>212</b> instead of the animation creation module <b>202</b>. The example system <b>200</b> can be well-suited for interactive AR/VR content where the final real-time content is procedurally generated from a set of animation clips created based on the animation creation module <b>202</b>. The animation creation module <b>202</b>, the real-time engine module <b>212</b>, and the 2D editorial module <b>232</b> can be implemented in one or more software applications running on one or more computing devices. The components (e.g., modules, elements, etc.) shown in this figure and all figures herein are exemplary only, and other implementations may include additional, fewer, integrated, or different components. Some components may not be shown so as not to obscure relevant details. In various embodiments, one or more of the functionalities described in connection with the animation creation module <b>202</b>, the real-time engine module <b>212</b>, and the 2D editorial module <b>232</b> can be implemented in any suitable sequences and combinations.</p><p id="p-0042" num="0041">In some embodiments, the various modules and/or applications described herein can be implemented, in part or in whole, as software, hardware, or any combination thereof. In general, a module and/or an application, as discussed herein, can be associated with software, hardware, or any combination thereof. In some implementations, one or more functions, tasks, and/or operations of modules and/or applications can be carried out or performed by software routines, software processes, hardware, and/or any combination thereof. In some cases, the various modules and/or applications described herein can be implemented, in part or in whole, as software running on one or more computing devices or systems, such as on a user or client computing device or on a server. For example, one or more modules and/or applications described herein, or at least a portion thereof, can be implemented as or within an application (e.g., app), a program, or an applet, etc., running on a user computing device or a client computing system. In another example, one or more modules and/or applications, or at least a portion thereof, can be implemented using one or more computing devices or systems that include one or more servers, such as network servers or cloud servers. It should be understood that there can be many variations or other possibilities. In an example embodiment, the animation creation module <b>202</b> can be implemented in or with animation creation software, such as Autodesk&#xa9; Maya, and the real-time engine module <b>212</b> can be implemented in or with a real-time engine, such as the Unity&#xae; game engine.</p><p id="p-0043" num="0042">The animation creation module <b>202</b> can include a VR animation module <b>204</b> and a shot fix module <b>206</b>.</p><p id="p-0044" num="0043">The VR animation module <b>204</b> can be configured to animate an interactive version of a story. For example, the interactive version of the story can be a computer-animated real-time experience based on VR or AR technology. The interactive version of the story can be composed of sequences of shots. A shot can represent a number of frames captured by a virtual camera positioned within the computer-animated real-time experience in three-dimensional space. In some embodiments, a shot can represent a single character action, such as an animation cycle or cycle animation. Further, a sequence of shots can correspond to a number of related shots. For instance, a sequence can include shots captured at a particular location within the computer-animated real-time experience. The VR animation module <b>204</b> can store data associated with the interactive version of the story (e.g., shots, sequences of shots, etc.) in a data store <b>208</b>. In one embodiment, the data store <b>208</b> resides on a remote server. In another embodiment, a web-based application interfaces with various modules in the animation creation module <b>202</b>, the real-time engine module <b>212</b>, and the 2D editorial module <b>232</b> to synchronize the data in the data store <b>208</b>.</p><p id="p-0045" num="0044">The shot fix module <b>206</b> can be configured to apply various fixes to shots associated with a non-interactive 2D version of the story generated from the interactive version of the story. For example, the shot fix module <b>206</b> can apply fixes to shots generated for the non-interactive 2D version of the story by the real-time engine module <b>212</b>. In general, shots associated with the non-interactive 2D version of the story can be derived from shots associated with the interactive version of the story. As a result, updates to a shot associated with the interactive version of the story may propagate to a corresponding shot associated with the non-interactive 2D version of the story. In some instances, specific adjustments that apply only to shots associated with the non-interactive 2D version of the story may be required. For example, character eyelines represented in a shot may be correct in the interactive version of the story but may appear to look in the wrong direction in a shot associated with the non-interactive 2D version of the story. In such instances, the shot fix module <b>206</b> can be configured to bi-furcate shots that require specific adjustments to the non-interactive 2D version of the story. When a shot is bi-furcated, separate copies of the shot are maintained for the interactive version and the non-interactive 2D version. As a result, adjustments to the shot can be made to the non-interactive 2D version of the story without affecting the interactive version of the story. In the foregoing example, adjustments can be made to character eyelines in shots associated with the non-interactive 2D version of the story without affecting the character eyelines in corresponding shots associated with the interactive version of the story. In some instances, the non-interactive version of the story may require new shots to be created. In one example, one shot in the interactive version of the story will be split into a number of smaller and potentially overlapping shots in the non-interactive 2D version which correspond to different camera angles (e.g. for close-ups, medium shots, and establishing shots). These new shots may correspond to computer-animation that is created specifically for the non-interactive 2D version. For example, a visual quality associated with a character may not be acceptable in a shot associated with the non-interactive 2D version of the story. In another example, a shot associated with the interactive version of the story can involve some interaction by a viewer. While such interaction is permissible in the interactive version of the story, the interaction is not suitable for reproduction in the non-interactive 2D version of the story. In such instances, the shot fix module <b>206</b> can permit creation of new shots for the non-interactive 2D version of the story which revise or replace existing shots associated with the interactive version of the story. The shot fix module <b>206</b> can store data associated with shots fixed or added in relation to the non-interactive 2D version of the story in the data store <b>208</b>.</p><p id="p-0046" num="0045">The real-time engine module <b>212</b> can include a VR interactivity module <b>214</b>, a VR lighting &#x26; FX module <b>216</b>, a 2D previz module <b>218</b>, a 2D shot create module <b>220</b>, a 2D layout module <b>222</b>, and a 2D lighting &#x26; FX module <b>224</b>.</p><p id="p-0047" num="0046">The VR interactivity module <b>214</b> can be configured to combine and/or procedurally generate animation using non-linear animation frameworks, artificial intelligence (AI) systems, and other generally known procedural methods.</p><p id="p-0048" num="0047">The VR lighting and FX module <b>216</b> can be configured to apply lighting and media effects (FX) elements used in the interactive version of the story. The lighting and FX elements can be created using generally known techniques.</p><p id="p-0049" num="0048">The 2D previz module <b>218</b> can be configured to position a set of virtual cameras in the interactive version of the story as animated by the VR animation module <b>204</b>. For example, an artist may instruct the 2D previz module <b>218</b> to position the set of virtual cameras in the interactive version of the story based on a 2D film script <b>210</b>. For example, the 2D film script <b>210</b> can be written based on a script associated with the interactive version of the story. The set of virtual cameras can be placed within the interactive version of the story to capture animation footage from a variety of different camera angles. In some embodiments, the set of virtual cameras and their associated parameters can be created using AR/VR technology. For example, an artist can place and manipulate a virtual camera by moving a 6DOF hand controller with their hands in VR. The artist can then record footage from the original VR/AR experience with this virtual camera in VR. This can be used to create hand-held camera effects. In one embodiment, multiple artists can use VR/AR technology to place and record different virtual cameras, thus creating distinct sets of camera data. The 2D previz module <b>218</b> can also be configured to export 2D &#x201c;playblast&#x201d; feeds (or movies) for each virtual camera. In some embodiments, the 2D previz module <b>218</b> can insert reticle and slate information to each frame of an exported 2D &#x201c;playblast&#x201d; feed, as illustrated in example <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The reticle and slate information can be provided as frame overlays. In some embodiments, the reticle and slate information can include reticle data (e.g., camera boundaries), sequence data, a filename, camera lens, camera/shot name, timestamp, take or iteration number, and animation frame number.</p><p id="p-0050" num="0049">The 2D shot create module <b>220</b> can be configured to create new shots and update existing shots. In general, the 2D shot create module <b>220</b> can create shots from an edit cut. The edit cut can be produced by the 2D editorial module <b>232</b>, as discussed below. In one embodiment, every virtual camera represents a single shot. In another embodiment, multiple shots may be constructed from a single virtual camera. The 2D shot create module <b>220</b> can store each shot and its related metadata information in the data store <b>208</b>. An example of shot metadata information associated with a virtual (or animation) camera is illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. For example, the shot metadata information can include a shot name, duration, animation file, frame range, characters included in a shot, and a publish version. Many variations are possible. In one embodiment, the 2D shot create module <b>220</b> can update a shared file that stores shot data. In one embodiment, the 2D shot create module <b>220</b> updates the data store <b>208</b>. In another embodiment, the 2D shot create module <b>220</b> updates the data store <b>208</b> through the web application API (application programming interface).</p><p id="p-0051" num="0050">The 2D layout module <b>222</b> can be configured to enhance shots created by the 2D shot create module <b>220</b>. For example, the 2D layout module <b>222</b> can apply additional set dressing and layout on a per-shot basis using generally known approaches.</p><p id="p-0052" num="0051">The 2D lighting &#x26; FX module <b>224</b> can be configured to provide various options to apply features to shots. For example, the 2D lighting &#x26; FX module <b>224</b> can apply lights; shadows; contact shadows; FX elements; and post-processing effects, such as motion blur and depth of field. These features can be applied based on instructions from lighting and FX artists, for example. Applying such features can augment the appearance of each shot.</p><p id="p-0053" num="0052">The 2D compositor <b>242</b> can generate final render frames based on generally known approaches. For example, the 2D compositor <b>242</b> can generate the final render frames based on final frames and layer passes as rendered by the real-time engine module <b>212</b>. The 2D compositor <b>242</b> can also generate the non-interactive 2D version of the story, such as a 2D computer-animated movie <b>244</b>, based on the final render frames.</p><p id="p-0054" num="0053">The 2D editorial module <b>232</b> can provide information describing an edit cut that combines various camera shots into a single sequence movie. The edit cut can be produced using non-linear video editing software. Information describing the edit cut and related editorial timing and track information can be imported into the real-time engine module <b>212</b>. The real-time engine module <b>212</b> permits further adjustments to existing virtual cameras and addition of new virtual cameras as needed to refine the edit cut. In some embodiments, the 2D editorial module <b>232</b> can read editing choices created by the non-linear video editing software in relation to the edit cut. The 2D editorial module <b>232</b> can recreate (or provide) the editing choices in the real-time engine module <b>212</b>. In an embodiment, the 2D editorial module <b>232</b> can read the following data from the non-linear editing software project: the location of a source video and audio clips; timecodes of clips from video and audio tracks; and effects, such as cross-fade and audio level curves that can be applied to video and audio clips. In an embodiment, the 2D editorial module <b>232</b> can use such data to identify assets in the real-time engine module <b>212</b> that correspond to video and audio clips edited in the non-linear video editing software. In an embodiment, the 2D editorial module <b>232</b> can use such data to create animation tracks and audio tracks with the identified assets in the real-time engine module <b>212</b>. In an embodiment, the 2D editorial module <b>232</b> can use such data to apply editing decisions to these assets in the real-time engine module <b>212</b>. In an embodiment, the 2D editorial module <b>232</b> can create a single timeline that represents all shots so a user can scrub back and forth between shots in the real-time engine module <b>212</b>. The single timeline can help facilitate real-time editing with the context of surrounding shots. Further, the 2D editorial module <b>232</b> supports updating existing cameras and shot information that already exists in the real-time engine module <b>212</b>. In one embodiment, camera/shot tracks from non-linear video editing software are associated with tracks in the real-time engine module <b>212</b> by naming convention. In another embodiment, the tracks can be correlated by metadata information that is passed to the non-linear video editing software and then round-tripped back to the real-time engine module <b>212</b>. In another embodiment, the tracks can be associated by using open source file formats, such as OpenTimeline. In yet another embodiment, tracks can be correlated using data comparison heuristics. Many variations are possible.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> illustrates an example method <b>300</b>, according to an embodiment of the present technology. In some embodiments, the method <b>300</b> can be performed by the system <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. At block <b>302</b>, sequences and shots can be animated for an AR/VR animation or experience, as described above in reference to the VR animation module <b>204</b>. At block <b>304</b>, animation can be combined and/or procedurally generated through non-linear animation frameworks, as described above in reference to the VR interactivity module <b>214</b>. At block <b>306</b>, lighting and FX elements can be created for the AR/VR animation or experience, as described above in reference to the VR lighting &#x26; FX module <b>216</b>. At block <b>308</b>, virtual cameras can be placed in a real-time engine, as described above in reference to the 2D previz module <b>218</b>. At block <b>310</b>, an edit cut can be produced in non-linear video editing software, as described above in reference to the 2D editorial module <b>232</b>. At block <b>312</b>, once the edit cut is approved, 2D shots are created from camera and timing metadata, as described above in reference to the 2D shot create module <b>220</b>. At block <b>314</b>, set dressing and layouts may be applied to shots, as described above in reference to the 2D layout module <b>222</b>. At block <b>316</b>, additional lights, shadows, contact shadows, FX elements, and post-processing effects can be applied, as described above in reference to the 2D lighting &#x26; FX module <b>224</b>. At block <b>318</b>, fixes can be applied to 2D shots and additional 2D shots can be created, as described above in reference to the shot fix module <b>206</b>.</p><p id="p-0056" num="0055">Many variations to the example methods are possible. It should be appreciated that there can be additional, fewer, or alternative steps performed in similar or alternative orders, or in parallel, within the scope of the various embodiments discussed herein unless otherwise stated.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> illustrates an example method <b>350</b>, according to an embodiment of the present technology. For example, the method <b>350</b> can be performed by the system <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. At block <b>352</b>, data associated with a computer-based experience can be obtained. The computer-based experience can be based on interactive real-time technology. For example, the interactive real-time technology can apply augmented reality (AR) and/or virtual reality (VR) to provide interactive content. At block <b>354</b>, at least one virtual camera can be configured within the computer-based experience in a real-time engine. At block <b>356</b>, data associated with an edit cut of the computer-based experience can be obtained based on content captured by the at least one virtual camera. At block <b>358</b>, a plurality of shots that correspond to two-dimensional content can be generated from the edit cut of the computer-based experience in the real-time engine. At block <b>360</b>, data associated with a two-dimensional version of the computer-based experience can be generated with the real-time engine based on the plurality of shots. The two-dimensional version can be rendered based on the generated data.</p><p id="p-0058" num="0057">Many variations to the example methods are possible. It should be appreciated that there can be additional, fewer, or alternative steps performed in similar or alternative orders, or in parallel, within the scope of the various embodiments discussed herein unless otherwise stated.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example system <b>400</b>, according to an embodiment of the present technology. The example system <b>400</b> can include an animation creation module <b>402</b> and a real-time engine module (or real-time engine) <b>422</b>. The example system <b>400</b> can be implemented to generate a non-interactive 2D version of a story from an interactive version of the story. For example, the example system <b>400</b> can generate a non-interactive 2D computer-animated film from an interactive computer-animated real-time experience based on VR or AR technology. In another example, the example system <b>400</b> can generate a first-person 2D computer-animated film from a character-based computer-animated real-time experience based on VR or AR technology. The character-based computer-animated real-time experience may involve an immersive experience in which a viewer can participate in a story as a character. Many variations are possible. In contrast to the example system <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the example system <b>400</b> allows camera placement and shot creation to occur in relation to the animation creation module <b>402</b> instead of the real-time engine module <b>422</b>. As such, the example system <b>400</b> can be well-suited for content that is primarily linear in nature where the straight-ahead animations in the animation creation module <b>402</b> match the animations in the real-time engine module <b>422</b>. The animation creation module <b>402</b> and the real-time engine module <b>422</b> can be implemented in one or more software applications running on one or more computing devices. The components (e.g., modules, elements, etc.) shown in this figure and all figures herein are exemplary only, and other implementations may include additional, fewer, integrated, or different components. Some components may not be shown so as not to obscure relevant details. In various embodiments, one or more of the functionalities described in connection with the animation creation module <b>402</b> and the real-time engine module <b>422</b> can be implemented in any suitable sequences and combinations.</p><p id="p-0060" num="0059">In some embodiments, the various modules and/or applications described herein can be implemented, in part or in whole, as software, hardware, or any combination thereof. In general, a module and/or an application, as discussed herein, can be associated with software, hardware, or any combination thereof. In some implementations, one or more functions, tasks, and/or operations of modules and/or applications can be carried out or performed by software routines, software processes, hardware, and/or any combination thereof. In some cases, the various modules and/or applications described herein can be implemented, in part or in whole, as software running on one or more computing devices or systems, such as on a user or client computing device or on a server. For example, one or more modules and/or applications described herein, or at least a portion thereof, can be implemented as or within an application (e.g., app), a program, or an applet, etc., running on a user computing device or a client computing system. In another example, one or more modules and/or applications, or at least a portion thereof, can be implemented using one or more computing devices or systems that include one or more servers, such as network servers or cloud servers. It should be understood that there can be many variations or other possibilities. In an example embodiment, the animation creation module <b>402</b> can be implemented in or with animation creation software, such as Autodesk&#xa9; Maya, and the real-time engine module <b>422</b> can be implemented in or with a real-time engine, such as the Unity&#xae; game engine.</p><p id="p-0061" num="0060">The animation creation module <b>402</b> can include a VR animation module <b>404</b>, a 2D previz module <b>406</b>, a 2D shot create module <b>408</b>, and a shot fix module <b>410</b>.</p><p id="p-0062" num="0061">The VR animation module <b>404</b> can be configured to animate an interactive version of a story. For example, the interactive version of the story can be a computer-animated real-time experience based on VR or AR technology. The interactive version of the story can be composed of sequences of shots. A shot can represent a number of frames captured by a virtual camera positioned within the computer-animated real-time experience in three-dimensional space. In some embodiments, a shot can represent a single character action, such as an animation cycle or cycle animation. Further, a sequence of shots can correspond to a number of related shots. For instance, a sequence can include shots that were captured at a particular location within the computer-animated real-time experience.</p><p id="p-0063" num="0062">The 2D previz module <b>406</b> can be configured to position a set of virtual cameras in the interactive version of the story as animated by the VR animation module <b>404</b>. For example, an artist may instruct the 2D previz module <b>406</b> to position the set of virtual cameras in the interactive version of the story based on a 2D film script <b>412</b>. For example, the 2D film script <b>412</b> can be written based on a script associated with the interactive version of the story. The set of virtual cameras can be placed within the interactive version of the story to capture animation footage from a variety of different camera angles. The 2D previz module <b>406</b> can also be configured to export 2D &#x201c;playblast&#x201d; feeds (or movies) for each virtual camera. For example, the 2D previz module <b>406</b> can provide a camera recording interface that provides options to record data captured by each camera. In some embodiments, the 2D previz module <b>406</b> can insert reticle and slate information to each frame of an exported 2D &#x201c;playblast&#x201d; feed, as illustrated in example <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The reticle and slate information can be provided as frame overlays. In some embodiments, the reticle and slate information can include reticle data (camera boundaries), sequence data, a filename, camera lens, camera/shot name, timestamp, take or iteration number, and animation frame number. The 2D previz module <b>406</b> can interact with a 2D editorial process <b>414</b>. For example, the 2D editorial process <b>414</b> can provide an edit cut that combines various camera shots from exported &#x201c;playblast&#x201d; feeds into a single sequence movie. The edit cut can be produced using non-linear video editing software. The edit cut can be reviewed and, if needed, new virtual cameras can be introduced and/or further adjustments can be made to existing virtual cameras. In one embodiment, every camera represents a single shot. In another embodiment, multiple shots may be constructed from a single camera.</p><p id="p-0064" num="0063">The 2D shot create module <b>408</b> can be configured to create new shots and update existing shots. In general, the 2D shot create module <b>408</b> can create shots from the edit cut provided by the 2D editorial process <b>414</b>. The 2D shot create module <b>408</b> can store each shot and its related metadata information in a data store <b>416</b>. An example of shot metadata information associated with a virtual (or animation) camera is illustrated in example <figref idref="DRAWINGS">FIG. <b>7</b></figref>. For example, the shot metadata information can include a start frame, an end frame, a production name, a sequence, version, and rendition. Many variations are possible. In one embodiment, the 2D shot create module <b>408</b> can update a shared file that stores shot data. For example, the 2D shot create module <b>408</b> can export shot data associated with one or more identified virtual cameras to a specified output location, such as the data store <b>416</b>. In one embodiment, the data store <b>416</b> resides on a remote server. In another embodiment, a web-based application interfaces with various modules in the animation creation module <b>402</b>, the real-time engine module <b>422</b>, and the 2D editorial module <b>414</b> to synchronize the data in the data store <b>416</b>. <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example export interface for exporting shot data. In another embodiment, the 2D shot create module <b>408</b> can update a show database accessible through the data store <b>416</b>. The 2D shot create module <b>408</b> can also export virtual camera information, for example, to a file system or cloud-based storage system.</p><p id="p-0065" num="0064">The shot fix module <b>410</b> can be configured to apply various fixes to shots associated with a non-interactive version of the story generated from the interactive version of the story. For example, the shot fix module <b>410</b> can apply fixes to shots generated for the non-interactive version of the story by the real-time engine module <b>422</b>. In general, shots associated with the non-interactive version of the story can be derived from shots associated with the interactive version of the story. As a result, updates to a shot associated with the interactive version of the story may propagate to a corresponding shot associated with the non-interactive version of the story. In some instances, specific adjustments that apply only to shots associated with the non-interactive version of the story may be required. For example, character eyelines represented in a shot may be correct in the interactive version of the story but may appear to look in the wrong direction in a shot associated with the non-interactive version of the story. In such instances, the shot fix module <b>410</b> can be configured to bi-furcate shots that require specific adjustments to the non-interactive version of the story. When a shot is bi-furcated, separate copies of the shot are maintained for the interactive and non-interactive versions of the story. Adjustments to the shot can be made to the non-interactive version of the story without affecting the interactive version of the story. In the foregoing example, adjustments can be made to character eyelines in shots associated with the non-interactive version of the story without affecting the character eyelines in corresponding shots associated with the interactive version of the story. In some instances, the non-interactive version of the story may require new shots to be created. In one example, one shot in the interactive version of the story will be split into a number of smaller and potentially overlapping shots in the non-interactive 2D version which correspond to different camera angles (e.g. for close-ups, medium shots, and establishing shots). These new shots may correspond to computer-animation that is created specifically for the non-interactive version. For example, a visual quality associated with a character for an interactive version of a story may be not be acceptable in a shot associated with the non-interactive version of the story. In another example, a shot associated with the interactive version of the story can involve some interaction by a viewer. While such interaction is permissible in the interactive version of the story, the interaction is not suitable for reproduction in the non-interactive version of the story. In such instances, the shot fix module <b>410</b> can permit creation of new shots for the non-interactive version of the story which revise or replace existing shots associated with the interactive version of the story. The shot fix module <b>410</b> can store data associated with shots that were fixed or added in relation to the non-interactive version of the story in the data store <b>416</b>.</p><p id="p-0066" num="0065">The real-time engine module <b>422</b> can include a VR lighting &#x26; FX module <b>424</b>, a 2D shot import module <b>426</b>, a 2D layout module <b>428</b>, and a 2D lighting &#x26; FX module <b>430</b>.</p><p id="p-0067" num="0066">The VR lighting &#x26; FX module <b>424</b> can be configured to apply lighting and media effects (FX) elements for the interactive version of the story. The lighting and FX elements can be created using generally known techniques.</p><p id="p-0068" num="0067">The 2D shot import module <b>426</b> can be configured to import shot data into the real-time engine module <b>422</b>. For example, once the shots and cameras have been added to the data store <b>416</b> (e.g., a show database), the 2D shot import module <b>426</b> can provide an interface that provides options to import virtual camera feeds and 2D shot data into the real-time engine module <b>422</b>, as illustrated in the example of <figref idref="DRAWINGS">FIG. <b>9</b></figref>. New shot timelines can be setup for each 2D shot the camera data is imported. For example, a directory that stores 2D shot data (e.g., virtual camera feeds) can be identified through the interface. The interface can also provide options to view shot information (e.g., frame range, path, etc.) for a given shot, reimport the shot, and delete the shot. When importing a shot, the 2D shot import module <b>426</b> can create additional 2D data for each shot. The additional 2D data can layer on top of existing AR/VR content. This enables per-shot adjustments to be made additively while using the existing AR/VR content as basis. In one embodiment, the 2D data can be represented as additional timelines that can be adjusted per department and applied on top of AR/VR content timelines.</p><p id="p-0069" num="0068">The 2D layout module <b>428</b> can be configured to enhance shots. For example, the 2D layout module <b>222</b> can apply additional set dressing and layout on a per-shot basis.</p><p id="p-0070" num="0069">The 2D lighting &#x26; FX module <b>430</b> can be configured to provide options to add additional features to shots. For example, the 2D lighting &#x26; FX module <b>430</b> can apply lights; shadows; contact shadows; FX elements; and post-processing effects, such as motion blur and depth of field. The additional features can be applied as instructed by lighting and FX artists, for example. Applying the additional features can augment the appearance of each shot. In one embodiment, the 2D lighting &#x26; FX module <b>430</b> represents VR content as a set of master timelines representing character animation, lighting, and FX elements. In this embodiment, the content runs primarily in linear fashion. In another embodiment, the content is non-linear and uses additional systems in addition to timelines, such as finite state machines, blend trees, and custom AI systems.</p><p id="p-0071" num="0070">The 2D compositor <b>432</b> can generate final render frames based on generally known approaches. For example, the 2D compositor <b>432</b> can generate the final render frames based on final frames and layer passes as rendered by the real-time engine module <b>422</b>. The 2D compositor <b>432</b> can also generate the non-interactive version of the story, such as a 2D computer-animated movie <b>434</b>, based on the final render frames.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> illustrates an example method <b>500</b>, according to an embodiment of the present technology. For example, the method <b>500</b> can be performed by the system <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. At block <b>502</b>, sequences and shots can be animated for an AR/VR animation or experience in an animation creation tool, as described above in reference to the VR animation module <b>404</b>. At block <b>504</b>, virtual cameras can be placed in the animation creation tool, as described above in reference to the 2D previz module <b>406</b>. At block <b>506</b>, an edit cut can be produced in non-linear video editing software, as described above in reference to the 2D editorial process <b>414</b>. At block <b>508</b>, once the edit cut is approved, 2D shots are created from camera and timing metadata, as described above in reference to the 2D shot create module <b>408</b>. At block <b>510</b>, fixes can be applied to 2D shots and additional 2D shots can be created, as described above in reference to the shot fix module <b>410</b>. At block <b>512</b>, lighting and FX elements can be created for the AR/VR animation or experience, as described above in reference to the VR lighting &#x26; FX module <b>424</b>. At block <b>514</b>, 2D shot data can be imported into a real-time engine, as described above in reference to the 2D shot import module <b>426</b>. At block <b>516</b>, set dressing and layouts may be applied to shots, as described above in reference to the 2D layout module <b>428</b>. At block <b>518</b>, additional lights, shadows, contact shadows, FX elements, and post-processing effects can be applied, as described above in reference to the 2D lighting &#x26; FX module <b>430</b>.</p><p id="p-0073" num="0072">Many variations to the example methods are possible. It should be appreciated that there can be additional, fewer, or alternative steps performed in similar or alternative orders, or in parallel, within the scope of the various embodiments discussed herein unless otherwise stated.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates an example method <b>550</b>, according to an embodiment of the present technology. For example, the method <b>550</b> can be performed by the system <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. At block <b>552</b>, data associated with a computer-based experience can be obtained. The computer-based experience can be based on interactive real-time technology. For example, the interactive real-time technology can apply augmented reality (AR) and/or virtual reality (VR) technology to provide interactive content. At block <b>554</b>, at least one virtual camera can be configured within the computer-based experience in an animation creation application. At block <b>556</b>, a plurality of shots that correspond to two-dimensional content can be generated from an edit cut of content captured by the at least one virtual camera in the animation creation application. At block <b>558</b>, data associated with a two-dimensional version of the computer-based experience can be generated in a real-time engine based on the plurality of shots. The two-dimensional version can be rendered based on the generated data.</p><p id="p-0075" num="0074">Many variations to the example methods are possible. It should be appreciated that there can be additional, fewer, or alternative steps performed in similar or alternative orders, or in parallel, within the scope of the various embodiments discussed herein unless otherwise stated.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example shot picker interface that can be used to select shots within a real-time engine (e.g., the real-time engine module <b>212</b>, the real-time engine module <b>422</b>), according to an embodiment of the present technology. The shot picker interface may be provided by the 2D shot create module <b>220</b> or the 2D shot create module <b>408</b>. The interface can provide a list of all shots available for a given scene. Upon shot selection, the scene can be configured to use the corresponding shot timelines. As a result, an artist is able to scrub the shot in a timeline window of the real-time engine. Additionally, the shot picker interface can provide options for accessing useful workflow objects, such as a component that hides geometry on a per-shot basis and game objects where additive 2D lights exist. Adjustments made to lighting in 2D shots can be additive on top of AR/VR lighting. Further, any changes to the AR/VR lighting will propagate to corresponding 2D shots. In some embodiments, a lighting manager interface can provide options that allow an artist to sever the link between an AR/VR scene and a 2D shot. Once the link is severed, changes to the AR/VR lighting will not propagate to corresponding 2D shots.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates an example camera recording interface that can provide options to record shots captured by virtual cameras, according to an embodiment of the present technology. For example, the camera recording interface may be provided by the 2D shot create module <b>220</b> or the 2D shot create module <b>408</b>. The camera recording interface can be customized to record a variety of content with different configurations in a single batch. Further, reticle and slate information can be applied to frames, as described above. In one embodiment, the following information is included: reticle (camera boundaries), sequence, filename, camera lens, camera/shot name, date, and animation frame number.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a real-time shot renderer interface, according to an embodiment of the present technology. For example, the real-time shot renderer interface may be provided by real-time engine module <b>212</b> or the real-time engine module <b>422</b>. The real-time shot renderer interface can provide options to render out final frames using the real-time engine module <b>212</b> and the real-time engine module <b>422</b>. For example, the real-time shot renderer interface can provide options to render out previz or intermediate frames. In some embodiments, the real-time shot renderer interface can be used in conjunction with the camera recording interface. In such embodiments, the camera recording interface can be used to identify virtual cameras and slate information to be included in frames and the real-time shot renderer interface can be used to render the frames with the slate information. The real-time shot renderer interface can provide options to select different resolutions to render. The real-time shot renderer interface can also provide options to create EXRs and produce multiple render layer passes (e.g., depth, matte, and beauty) for a 2D compositor. In various embodiments, frames can be rendered using the real-time engine module <b>212</b> or the real-time engine module <b>422</b>. In one embodiment, rendering software may activate selected shots and render them one at a time. In one embodiment, the rendering software may render the shots in parallel on one or more remote machines. The rendering can be done in-editor. That is, instead of rendering shots from the build, the rendering software puts the real-time engine module <b>212</b> or the real-time engine module <b>422</b> into &#x201c;play-mode&#x201d; and then proceeds to scrape the shot camera's render texture from shot cameras on a per-frame basis. In an embodiment, the rendering software can operate in conjunction with a recording system built into the real-time engine module <b>212</b> or the real-time engine module <b>422</b>. The rendering can be performed in-editor. In this embodiment, a user can specify recorder clips on a timeline which defines regions of rendering. These clips also define which Arbitrary Output Variables (AOV) are to be rendered. AOVs or render passes provide a way to render any arbitrary shading network component into different images. For example, an artist can render out separate depth, matte, and beauty passed and later recombine them in a compositing process. The chosen AOVs function as additional render passes to a forward render pipeline. Notably, these passes render to user defined render textures instead of to the camera's render texture. The recorders for these passes then write these user defined render textures to disk. To facilitate the compositing process, the AOV passes for a single frame are written into one EXR file, for example, using the SimpleImageIO library.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an example rendered frame. The frame may be rendered by the real-time engine module <b>212</b> or the real-time engine module <b>422</b>. The frame can include metadata information, such as an animation name, sequence, filename, timestamp, an identification number associated with the frame in an animation creation application, and identification number associated with the frame in a real-time engine.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a timeline interface that can be used to create additive 2D shot timelines that layer on top of VR master timelines. For example, the timeline interface can be provided by the 2D shot import module <b>426</b>.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates another timeline interface that allows lighting artists to make shot-specific additive improvements without having to modify the underlying VR lighting. In some embodiments, making these additive changes does not require re-baking expensive global illumination calculations.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates a combined lighting interface that incorporates lighting interfaces described above. The combined lighting interface can be provided by the real-time engine module <b>212</b> or the real-time engine module <b>422</b>.</p><p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates an example contact sheet that can be generated based on the present technology. The contact sheet can include frames associated with one or more shots.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIGS. <b>18</b>-<b>24</b></figref> illustrate various improvements that can be made based on the present technology. For example, the present technology can improve shot lighting in 2D content. In general, lighting for interactive and VR projects often appears correct, but this lighting may not produce intended effects when composed for a specific shot angle. For example, <figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates 2D content with lighting from an AR/VR version. In this example, a crow character <b>1802</b> is intended to be a focal point. However, the lighting from the AR/VR version does not show the crow character <b>1802</b> as the focal point. As discussed above, the present technology allows an artist to layer on shot lighting in the 2D content to emphasize the crow character <b>1802</b> as the focal point, as illustrated in the example of <figref idref="DRAWINGS">FIG. <b>19</b></figref>. The present technology allows an artist to add contact shadows to a turtle character <b>2002</b>, as illustrated in the example of <figref idref="DRAWINGS">FIG. <b>20</b></figref>. Further, the present technology allows an artist to apply post processing (e.g., depth of field) to focus a viewer's attention and to make the shot cinematic, as illustrated in the example of <figref idref="DRAWINGS">FIG. <b>21</b></figref>. The present technology also allows an artist to add additional FX elements to 2D content, which would not be suitable for AR/VR content. For example, <figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates an original shot <b>2202</b> from AR/VR content. <figref idref="DRAWINGS">FIG. <b>22</b></figref> also illustrates an improved 2D shot <b>2212</b> derived from the original shot <b>2202</b>. In the improved 2D shot <b>2212</b>, FX elements corresponding to snow footsteps have been added to ground the turtle character in the environment. The present technology also allows an artist to apply animation fixes to improve 2D content. For example, <figref idref="DRAWINGS">FIG. <b>23</b></figref> illustrates a frame from AR/VR content in which characters are looking at a viewer playing a character in a computer-animated real-time experience. In <figref idref="DRAWINGS">FIG. <b>23</b></figref>, the characters are looking at a virtual camera associated with the viewer playing a character in the computer-animated real-time experience. While acceptable for the computer-animated real-time experience, the character behavior reflected in <figref idref="DRAWINGS">FIG. <b>23</b></figref> would not translate to 2D content, since the characters would be looking in the wrong direction. To address this, the present technology allows a new animation to be created and used as 2D content. For example, <figref idref="DRAWINGS">FIG. <b>24</b></figref> illustrates a new animation in which a crow character <b>2402</b> is looking at an ant character <b>2404</b> while the ant character's <b>2404</b> animation hooks up smoothly with the previous shot in <figref idref="DRAWINGS">FIG. <b>23</b></figref>. The new animation reflected in <figref idref="DRAWINGS">FIG. <b>24</b></figref> can be better suited for viewing as 2D content than the animation corresponding to the AR/VR content in <figref idref="DRAWINGS">FIG. <b>23</b></figref>. Many variations are possible.</p><heading id="h-0007" level="2">Hardware Implementation</heading><p id="p-0085" num="0084">The foregoing processes and features can be implemented by a wide variety of machine and computer system architectures and in a wide variety of network and computing environments. <figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates an example machine <b>2500</b> within which a set of instructions for causing the machine to perform one or more of the embodiments described herein can be executed, in accordance with an embodiment of the present technology. The embodiments can relate to one or more systems, methods, or computer readable media. The machine may be connected (e.g., networked) to other machines. In a networked deployment, the machine may operate in the capacity of a server or a client machine in a client-server network environment, or as a peer machine in a peer-to-peer (or distributed) network environment.</p><p id="p-0086" num="0085">The computer system <b>2500</b> includes a processor <b>2502</b> (e.g., a central processing unit (CPU), a graphics processing unit (GPU), or both), a main memory <b>2504</b>, and a nonvolatile memory <b>2506</b> (e.g., volatile RAM and non-volatile RAM, respectively), which communicate with each other via a bus <b>2508</b>. The processor <b>2502</b> can be implemented in any suitable form, such as a parallel processing system. In some cases, the example machine <b>2500</b> can correspond to, include, or be included within a computing device or system. For example, in some embodiments, the machine <b>2500</b> can be a desktop computer, a laptop computer, personal digital assistant (PDA), an appliance, a wearable device, a camera, a tablet, or a mobile phone, etc. In one embodiment, the computer system <b>2500</b> also includes a video display <b>2510</b>, an alphanumeric input device <b>2512</b> (e.g., a keyboard), a cursor control device <b>2514</b> (e.g., a mouse), a drive unit <b>2516</b>, a signal generation device <b>2518</b> (e.g., a speaker) and a network interface device <b>2520</b>.</p><p id="p-0087" num="0086">In one embodiment, the video display <b>2510</b> includes a touch sensitive screen for user input. In one embodiment, the touch sensitive screen is used instead of a keyboard and mouse. The disk drive unit <b>2516</b> includes a machine-readable medium <b>2522</b> on which is stored one or more sets of instructions <b>2524</b> (e.g., software) embodying any one or more of the methodologies or functions described herein. The instructions <b>2524</b> can also reside, completely or at least partially, within the main memory <b>2504</b> and/or within the processor <b>2502</b> during execution thereof by the computer system <b>2500</b>. The instructions <b>2524</b> can further be transmitted or received over a network <b>2540</b> via the network interface device <b>2520</b>. In some embodiments, the machine-readable medium <b>2522</b> also includes a database <b>2525</b>.</p><p id="p-0088" num="0087">Volatile RAM may be implemented as dynamic RAM (DRAM), which requires power continually in order to refresh or maintain the data in the memory. Non-volatile memory is typically a magnetic hard drive, a magnetic optical drive, an optical drive (e.g., a DVD RAM), or other type of memory system that maintains data even after power is removed from the system. The non-volatile memory <b>2506</b> may also be a random access memory. The non-volatile memory <b>2506</b> can be a local device coupled directly to the rest of the components in the computer system <b>2500</b>. A non-volatile memory that is remote from the system, such as a network storage device coupled to any of the computer systems described herein through a network interface such as a modem or Ethernet interface, can also be used.</p><p id="p-0089" num="0088">While the machine-readable medium <b>2522</b> is shown in an exemplary embodiment to be a single medium, the term &#x201c;machine-readable medium&#x201d; should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions. The term &#x201c;machine-readable medium&#x201d; shall also be taken to include any medium that is capable of storing, encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present technology. The term &#x201c;machine-readable medium&#x201d; shall accordingly be taken to include, but not be limited to, solid-state memories, optical and magnetic media, and carrier wave signals. The term &#x201c;storage module&#x201d; as used herein may be implemented using a machine-readable medium.</p><p id="p-0090" num="0089">In general, routines executed to implement the embodiments of the invention can be implemented as part of an operating system or a specific application, component, program, object, module or sequence of instructions referred to as &#x201c;programs&#x201d; or &#x201c;applications&#x201d;. For example, one or more programs or applications can be used to execute any or all of the functionality, techniques, and processes described herein. The programs or applications typically comprise one or more instructions set at various times in various memory and storage devices in the machine and that, when read and executed by one or more processors, cause the computing system <b>2500</b> to perform operations to execute elements involving the various aspects of the embodiments described herein.</p><p id="p-0091" num="0090">The executable routines and data may be stored in various places, including, for example, ROM, volatile RAM, non-volatile memory, and/or cache memory. Portions of these routines and/or data may be stored in any one of these storage devices. Further, the routines and data can be obtained from centralized servers or peer-to-peer networks. Different portions of the routines and data can be obtained from different centralized servers and/or peer-to-peer networks at different times and in different communication sessions, or in a same communication session. The routines and data can be obtained in entirety prior to the execution of the applications. Alternatively, portions of the routines and data can be obtained dynamically, just in time, when needed for execution. Thus, it is not required that the routines and data be on a machine-readable medium in entirety at a particular instance of time.</p><p id="p-0092" num="0091">While embodiments have been described fully in the context of computing systems, those skilled in the art will appreciate that the various embodiments are capable of being distributed as a program product in a variety of forms, and that the embodiments described herein apply equally regardless of the particular type of machine- or computer-readable media used to actually effect the distribution. Examples of machine-readable media include, but are not limited to, recordable type media such as volatile and non-volatile memory devices, floppy and other removable disks, hard disk drives, optical disks (e.g., Compact Disk Read-Only Memory (CD ROMS), Digital Versatile Disks, (DVDs), etc.), among others, and transmission type media such as digital and analog communication links.</p><p id="p-0093" num="0092">Alternatively, or in combination, the embodiments described herein can be implemented using special purpose circuitry, with or without software instructions, such as using Application-Specific Integrated Circuit (ASIC) or Field-Programmable Gate Array (FPGA). Embodiments can be implemented using hardwired circuitry without software instructions, or in combination with software instructions. Thus, the techniques are limited neither to any specific combination of hardware circuitry and software, nor to any particular source for the instructions executed by the data processing system.</p><p id="p-0094" num="0093">For purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the description. It will be apparent, however, to one skilled in the art that embodiments of the disclosure can be practiced without these specific details. In some instances, modules, structures, processes, features, and devices are shown in block diagram form in order to avoid obscuring the description or discussed herein. In other instances, functional block diagrams and flow diagrams are shown to represent data and logic flows. The components of block diagrams and flow diagrams (e.g., modules, engines, blocks, structures, devices, features, etc.) may be variously combined, separated, removed, reordered, and replaced in a manner other than as expressly described and depicted herein.</p><p id="p-0095" num="0094">Reference in this specification to &#x201c;one embodiment&#x201d;, &#x201c;an embodiment&#x201d;, &#x201c;other embodiments&#x201d;, &#x201c;another embodiment&#x201d;, &#x201c;in various embodiments,&#x201d; or the like means that a particular feature, design, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the disclosure. The appearances of, for example, the phrases &#x201c;according to an embodiment&#x201d;, &#x201c;in one embodiment&#x201d;, &#x201c;in an embodiment&#x201d;, &#x201c;in various embodiments,&#x201d; or &#x201c;in another embodiment&#x201d; in various places in the specification are not necessarily all referring to the same embodiment, nor are separate or alternative embodiments mutually exclusive of other embodiments. Moreover, whether or not there is express reference to an &#x201c;embodiment&#x201d; or the like, various features are described, which may be variously combined and included in some embodiments but also variously omitted in other embodiments. Similarly, various features are described which may be preferences or requirements for some embodiments but not other embodiments.</p><p id="p-0096" num="0095">Although embodiments have been described with reference to specific exemplary embodiments, it will be evident that the various modifications and changes can be made to these embodiments. Accordingly, the specification and drawings are to be regarded in an illustrative sense rather than in a restrictive sense. The foregoing specification provides a description with reference to specific exemplary embodiments. It will be evident that various modifications can be made thereto without departing from the broader spirit and scope as set forth in the following claims. The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense.</p><p id="p-0097" num="0096">Although some of the drawings illustrate a number of operations or method steps in a particular order, steps that are not order dependent may be reordered and other steps may be combined or omitted. While some reordering or other groupings are specifically mentioned, others will be apparent to those of ordinary skill in the art and so do not present an exhaustive list of alternatives. Moreover, it should be recognized that the stages could be implemented in hardware, firmware, software or any combination thereof.</p><p id="p-0098" num="0097">It should also be understood that a variety of changes may be made without departing from the essence of the invention. Such changes are also implicitly included in the description. They still fall within the scope of this invention. It should be understood that this disclosure is intended to yield a patent covering numerous aspects of the invention, both independently and as an overall system, and in both method and apparatus modes.</p><p id="p-0099" num="0098">Further, each of the various elements of the invention and claims may also be achieved in a variety of manners. This disclosure should be understood to encompass each such variation, be it a variation of an embodiment of any apparatus embodiment, a method or process embodiment, or even merely a variation of any element of these.</p><p id="p-0100" num="0099">Further, the use of the transitional phrase &#x201c;comprising&#x201d; is used to maintain the &#x201c;open-end&#x201d; claims herein, according to traditional claim interpretation. Thus, unless the context requires otherwise, it should be understood that the term &#x201c;comprise&#x201d; or variations such as &#x201c;comprises&#x201d; or &#x201c;comprising&#x201d;, are intended to imply the inclusion of a stated element or step or group of elements or steps, but not the exclusion of any other element or step or group of elements or steps. Such terms should be interpreted in their most expansive forms so as to afford the applicant the broadest coverage legally permissible in accordance with the following claims.</p><p id="p-0101" num="0100">The language used herein has been principally selected for readability and instructional purposes, and it may not have been selected to delineate or circumscribe the inventive subject matter. It is therefore intended that the scope of the invention be limited not by this detailed description, but rather by any claims that issue on an application based hereon. Accordingly, the disclosure of the embodiments of the invention is intended to be illustrative, but not limiting, of the scope of the invention, which is set forth in the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. (canceled)</claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. A computer-implemented method comprising:<claim-text>generating, by a computing system, a set of two-dimensional content paths associated with an interactive version of a computer-based experience;</claim-text><claim-text>configuring, by the computing system, at least one virtual camera within the computer-based experience in a real-time engine;</claim-text><claim-text>generating, by the computing system, sets of shots that correspond to a two-dimensional content path of the set of two-dimensional content paths based on content captured by the at least one virtual camera within the computer-based experience; and</claim-text><claim-text>determining, by the computing system, at least one set of shots for a non-interactive version of the computer-based experience.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the at least one virtual camera is configured based on a script for a two-dimensional film, and wherein the sets of shots that correspond to the two-dimensional content path are associated with camera angles based on the script.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>receiving, by the computing system, input associated with a hand controller, wherein the at least one virtual camera is configured within the computer-based experience based on the input.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>determining, by the computing system, an update to the at least one set of shots; and</claim-text><claim-text>determining, by the computing system, at least one set of update shots based on the update, wherein the at least one set of update shots includes overlapping shots.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>determining, by the computing system, an update to the interactive version of a computer-based experience; and</claim-text><claim-text>propagating, by the computing system, the update to the at least one set of shots for the non-interactive version of the computer-based experience.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>receiving, by the computing system, information associated with an edit to be applied to the at least one set of shots;</claim-text><claim-text>editing, by the computing system, an asset in the interactive version of the computer-based experience based on the information; and</claim-text><claim-text>applying, by the computing system, the edit to the at least one set of shots based on the edited asset.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>providing, by the computing system, an overlay for the at least one set of shots, wherein the overlay includes at least one of: reticle data, sequence data, a filename, camera lens information, a camera name, a shot name, a timestamp, a take number, an iteration number, or an animation frame number.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the at least one set of shots for the non-interactive version of the computer-based experience replaces existing shots associated with the interactive version of a computer-based experience.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the set of two-dimensional content paths represent branches of a story associated with the computer-based experience.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the at least one set of shots is associated with shot metadata information comprising at least one of: a shot name, a duration, an animation file, a frame range, an included character, or a publish version.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A system comprising:<claim-text>at least one processor; and</claim-text><claim-text>a memory storing instructions that, when executed by the at least one processor, cause the system to perform operations comprising:<claim-text>generating a set of two-dimensional content paths associated with an interactive version of a computer-based experience;</claim-text><claim-text>configuring at least one virtual camera within the computer-based experience in a real-time engine;</claim-text><claim-text>generating sets of shots that correspond to a two-dimensional content path of the set of two-dimensional content paths based on content captured by the at least one virtual camera within the computer-based experience; and</claim-text><claim-text>determining at least one set of shots for a non-interactive version of the computer-based experience.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the at least one virtual camera is configured based on a script for a two-dimensional film, and wherein the sets of shots that correspond to the two-dimensional content path are associated with camera angles based on the script.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, the operations further comprising:<claim-text>receiving input associated with a hand controller, wherein the at least one virtual camera is configured within the computer-based experience based on the input.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, the operations further comprising:<claim-text>determining an update to the at least one set of shots; and</claim-text><claim-text>determining at least one set of update shots based on the update, wherein the at least one set of update shots includes overlapping shots.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, the operations further comprising:<claim-text>determining an update to the interactive version of a computer-based experience; and</claim-text><claim-text>propagating the update to the at least one set of shots for the non-interactive version of the computer-based experience.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A non-transitory computer-readable storage medium including instructions that, when executed by at least on processor of a computing system, cause the computing system to perform operations comprising:<claim-text>generating a set of two-dimensional content paths associated with an interactive version of a computer-based experience;</claim-text><claim-text>configuring at least one virtual camera within the computer-based experience in a real-time engine;</claim-text><claim-text>generating sets of shots that correspond to a two-dimensional content path of the set of two-dimensional content paths based on content captured by the at least one virtual camera within the computer-based experience; and</claim-text><claim-text>determining at least one set of shots for a non-interactive version of the computer-based experience.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the at least one virtual camera is configured based on a script for a two-dimensional film, and wherein the sets of shots that correspond to the two-dimensional content path are associated with camera angles based on the script.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, the operations further comprising:<claim-text>receiving input associated with a hand controller, wherein the at least one virtual camera is configured within the computer-based experience based on the input.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, the operations further comprising:<claim-text>determining an update to the at least one set of shots; and</claim-text><claim-text>determining at least one set of update shots based on the update, wherein the at least one set of update shots includes overlapping shots.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, the operations further comprising:<claim-text>determining an update to the interactive version of a computer-based experience; and</claim-text><claim-text>propagating the update to the at least one set of shots for the non-interactive version of the computer-based experience.</claim-text></claim-text></claim></claims></us-patent-application>